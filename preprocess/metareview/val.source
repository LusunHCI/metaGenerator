Reject; rating score: 10; rating score: 3; rating score: 5; rating score: 6; rating score: 8; rating score: 8; An intuitive overview of the fundamental ideas:• The fundamental problem that this work is concerned with is finding a suitable, “optimal”, way to adaptively set the learning rate for theNeural Net setting (whose fundamental challenge, as the paper points out, is it’s ’over parametrization’)• The approach adopted in the paper is the proposal of a framework (WITHIN the existing GD oriented ways in which we train our NN’s),wherein we consider the Learning Rate as a learnable parameter • The real novelty of this approach shows itself in how elegantly the mathematical formalism that follows from this approach, works out,I try to briefly cover the interpretation of the obtained mathematical results in the paper below:– Since we are trying to treat α as a learnable parameter, and want to treat it on the same footing as any other NN parameter, thiswould mean that we ought to write a GD update for it (which would involve the gradient of an augmented loss, say: Lα(αt,wt)wrt the learning rate, which we would have to define), and would also involve an auxiliary parameter (say ηt) as a sort of metaLR for α – this is where a straightforward pursuit of the LR as a learnable parameter would lead to. – The real strength of the above approach, shines through when the relevant gradients are actually computed, namely the quantity:∂Lα/∂α |α αt , for which we end up with the really powerful expression: −⟨gt, gt−1⟩, which, in addition to being an extraordinarilycompact, for such a general gradient computation (probably giving us a nudge that our initial definition of the augmented lossL was good) also affords to us a really intuitive (but robust and complete) qualitative interpretation of how we should update ourLR per iteration, the reason this expression is so significant, is because so far, we haven’t even considered the GD styled update forthe learning rate, and already, the expression for this newly defined gradient (computing which is O(1), since the dimensionalityof our spaces are fixed when training) provides us with enough information to be able to make better LR related decisions per iteration than using a constant LR/some weighting method to reduce our LR with progressing EPOCHS (either of which mightbe schemes that do not accurately reflect the topology of the loss landscape)– After the above gradient computation, we look towards the second order gradient – both in hopes of obtaining a similarly elegantmathematical expression for the same and also with the aim that we might be able to leverage second order information to devise astronger update rule for the LR, and as it so happens, we end up getting a really elegant expression for the second order derivativeas well, namely: ∂2L/∂2α |α αt   4/αt⟨gt, gt − gt−1⟩   4/αt(||gt||2 −⟨gt, gt−1⟩), which can also be computed along with the abovefirst order derivative in O(1) time. – The fact that we have effectively O(1) access to relevant second order information for the framework that we have built for theLR, is very significant, making available all the power associated with second order methods, for basically free! The above strategy is precisely what is adopted in the paper, the Newton direction is given by: pNk  −(∇2fk)−1∇fk, which may be re created in the Naïve update rule: αt+1   αt−η.∇Lα(αt;wt)  by setting η to −(∇2 αLα)−1 – in addition to this, the term is slightly altered to ensure numerical stability. A comparison with surrounding literature: The two specific papers that I am interested in talking about (wrt the current work), are: item 1 and item 3, since they are similar in spirit to the current work (in that, they also try to suggest ’optimal’, formalised methods foradaptively setting the LR) and have both been published in good conferences (the former in NeuriPS,19 and the latter in ICLR,18)2.1 Painless stochastic gradient: Interpolation, line search, and convergence rates: This paper tries to solve fundamentally the same problem – i.e.it tries to find an ’optimal’ way to adaptively compute step size during training, though the approach adopted in their paper is fundamentally different from the current work, but with some interesting takeaways that could’ve influencedthe current work. I first summarize their approach:The fundamental idea of the paper is to use line search on each iteration for computing the step size, specifically, Armijo LS, which chooses it’s step size based off of the solution to the following optimization problem (the following is the expression suggested in the paper for the adaption of the problem to the stochastic setting). Since the starting point of both agree to a certain extent. • A clear strength of the current work over item 1 is the lack of requirement of the interpolation condition for results to hold good, and thesignificantly easier implementation of the suggested methods 2.2 Online learning rate adaptation with hypergradient descent: In terms of the fundamental structure of the work, the current paper isincredibly similar to item 3. They also arrive at the same expression for the gradient, but that is where they decide to stop (and instead, focus their time towards the general formulation of the same update). There aren’t any ’obvious’ ways to extend the fundamental theoretical framework of the paper, and any kind of work that takes motivation from the structure of this one, would have to start with first choosing the broader framework to embed this optimization of the LR in (in this case, it was GD wrt the NN loss)<|endoftext|>The authors propose an adaptive learning rate strategy that relies on treating the learning rate as a learnable parameter. The results are validated on common datasets and architectures such as CIFAR 10/100 and ImageNet. Namely, Amid et al., 2020 already propose a very similar approach based on the inner product of the current and past gradient to update the per layer learning rate. Amid et al., 2020 extend this to coordinate wise learning rates and use exponentiated gradient updates, which is better suited than gradient descent for updating non negative values. They even consider adding momentum, EMA of the past gradients, normalized updates, etc. Methods that rely on updating the learning rate based on the interaction of the consecutive gradients date back to the Delta Bar Delta (DBD) method (Jacobs, 1988) and have been repeatedly proposed in different forms (Minai & Williams, 1990; Sutton, 1995; Schraudolph, 1999; Baydin et al., 2018). So in a sense, the paper has nothing new to offer. The only minor change that the current submission considers is a second order update on the learning rate, which I do not expect to have a major impact, as discussed next. Another important point missed by the authors is the problem of short horizon bias associated with consecutive gradient based adaptive learning rate methods. Specifically, Wu et al., 2018 show that all such approaches suffer from the effect of a short time horizon where the method punishes for the variability in the loss surface (due to stochasticity and other factors) in the early stages of optimization. Thus, these methods usually do not allow a large enough learning rate initially to escape bad local minima. The paper does not acknowledge this problem nor provides any improvements over the earlier approaches to address this issue. (Conf.Publ.No.470), volume 2, pp. Baydin, A. G., Cornish, R., Martínez Rubio, D., Schmidt, M., and Wood, F. Online learning rate adaptation with hypergradient descent. Step size Adaptation Using Exponentiated Gradient Updates. In Workshop on "Beyond first order methods in ML systems" at the 37th International Conference on Machine Learning (ICML), 2020. Overall, I believe the current submission misses multiple related work and has not much to offer in terms of novelty or addressing the issues associated with the gradient based adaptive learning rate methods. Therefore, I vote for a reject. Update: I forgot to add pointers to the missing references that I mentioned in my initial review.<|endoftext|>In this paper the authors derive updates to the learning rate (LR) that minimize the loss function that can be computed as functions of neural network gradients. These updates to the LR can be used to compute an adaptive LR schedule in an online, hyperparameter free (or at least, less hyperparameter sensitive) fashion. They derive first and second order updates for the LR and then run experiments with it on various deep learning workloads. *Strengths:*  Your Figures 1, 2 are great in that they actually showed what the computed LR schedule looked like. *Weaknesses:*Overall I believe most of my issues stem from lack of comparisons to baselines with nontrivial LR schedules. The authors do not seem to include any LR schedules that include a warmup in their baselines, which is surprising given LR warmup is often common on the workloads considered in the experiments. Most of the computed TLR schedules look like a linear warmup followed by exponential or quadratic decay, so I would recommend trying this sort of schedule as a baseline, even just with default hyperparameter choices (warmup for 10% of training?). This to me indicates that by trying even a few more baselines LR schedules, the proposed method may not be competitive. While there is definitely appeal in the proposed method because it mostly does not require tuning, many heuristics used today do not actually require much guess/check work either beyond choosing the initial LR (which all proposed methods must do), that is they work “out of the box” (such as warmups or other popular decay schedules) or with very minimal tuning. As another example of this concern, it is surprising in Figure 3 that Adam does worse than SGD with momentum, given that on CIFAR WRN Adam almost always outperforms other optimizers; what LR schedule was used for Adam? It seems unfair to compare to any optimizer with a knowingly poor LR schedule, especially given that commonly used ones (sgd_mstep) seem to outperform the proposed method. For example in Figure 4 it is unclear how the optimizers perform towards the end of training, because of the vertical axes (what were the final accuracies achieved for each optimizer?). With any paper that introduces a training algorithm technique, I think it is critical to highlight if the goal of the proposed technique is to 1. improve training speed 2. improve generalization performance (or both). If any part of the goal is to improve training speed, then I believe that a better method of comparing performance is to instead report the number of steps (or total wallclock time) required for each technique to reach a competitive validation performance (for example, how many steps did it take for each optimizer to reach a competitive ImageNet validation accuracy (75.9%)?). In Figure 3, it would be much more interesting to see different LRs instead of batch sizes (you have plots of the losses on CIFAR100 in the appendix for different LRs, but as we saw in other figures the loss does not tell the whole story in the case of overfitting), because it seems like an unfair comparison to use an untuned LR for the baseline methods (each baseline only gets a single initial LR) when the proposed method can change its LR online to correct for a poor choice of initial LR. As a minor point, I believe citing this paper https://openreview.net/forum?id H1MczcgR  would be useful, as it surfaces concerns with learning the LR. Overall, while the paper is very well written and formatted, I do not believe I can recommend this paper for acceptance without seeing better comparisons to more competitive baseline LR schedules (such as warm up followed by an exponential or quadratic decay, even with all the hyperparameters set to default values). The figures could use some cleanup as described above. However, *I do believe that the proposed method could be of significant value, but I would want to see it demonstrated on workloads where common LR schedules/heuristics do not hold up*; could the authors find or design an experiment where the dataset/model is too difficult to optimize with traditional LR schedules, but the proposed method succeeds?<|endoftext|>More specifically, the learning rate is optimized via the extra gradient descent step during the training. Then based on that, the authors give an expression of both the first and second derivatives of the learning rate where the neural network structure is taken into consideration. They also show that such implementation can be extended to the case where a different learning rate is applied for each layer. Pros:* The paper is well written and easy to follow. The logic structure of the paper is clear and necessary explanation is given. * The idea of incorporating the second order gradient is novel and interesting. Most hypergradient descent methods require to introduce several additional hyper paramters and carefully fine tune them, but with a Newton Raphson formulation, the update of learning rate become more concise. * The derivation of the algorithm is clear and the method is easy to implement. Furthermore, the per layer learning rate scheme seems a natural extension to the original algorithm where a global learning rate is applied to all layers. * Both the experiment setup and the empirical results look good. The new algorithm outperforms the state of art methods in most cases. There are lots of papers about hypergradient descent which consider hyper paramters (including the learning rate) to be trainable. However, I have not seen a decent discussion about the connection between the proposed method and existing work in that direction. * The expression of derivatives of the learning rate with standard neural networks may not be applicable to more complex architectures (e.g.ResNet).Furthermore, I somehow feel $\hat{y}$ is used as more like an intermediate variable so I am not sure if it is necessary to formulate the problem in this way. [1] has a similar conclusion for calculating the gradient of the learning rate. Is there any specific reason not to cite their conclusion? * I found the figures from the experiment sections extremely hard to read. For example, in Figure 1 the curves overlap with each other during training and I could not distinguish any one of them. If possible, please provide a table so the results will be much more readable. Comments:* On page 4 there is a footnote explaining how to formulate the layer with bias term. I might miss something, but shouldn t $W   (W\ b)$ instead? Online learning rate adaptation with hypergradient descent. In International Conference on Learning Representations, 2018. Overall this is a good paper and I tend to vote for acceptance. However, I do hope that the authors could provide some feedbacks on the reviews I made and polish the paper more in details.<|endoftext|>In this paper, the authors propose a novel adaptive learning rate method where the learning rate is learned along with the weights through gradient descent. The main intuition is that the learning rate is part of the loss function (along with the weight gradients) and this combined loss is minimized through gradient descent (naive GD TLR). To mitigate this, the authors propose a variant of the method that updates the learning rate based upon a single loss (efficient GD TLR). The efficient GD TLR has the intuition that if 2 consecutive losses are in the same direction, the learning rate is increased (move to goal faster). If the 2 consecutive losses are opposite direction, decrease the learning rate (bouncing around the well). In addition, this method can use different learning rates for different layers in the network. This paper provides a useful formulation of the problem. Posing the learning rate as a learnable parameter does appear to be well founded theoretically, but the results are a bit lackluster for the networks that were tested. After the description of the technique, the authors do a good job of comparing it to existing techniques. However, I would like to a concise summary of how GD TLR improves on the shortcomings of those techniques. A single sentence making this explicit would be helpful for the reader. For the simplified variant, efficient GD TLR, the authors indicate that the technique is reactive to either too high or too low of a learning rate. This should have been the first hint that it will at best perform as well as a prospective technique that considers a known gradient profile. The authors should relate this in the intuition under equation 6 to the expected performance. The experimental portion was a good representation of networks for computer vision classification. But GD TLR is likely most applicable to other problems where there might be more of an advantage. The authors  final comments do indicate that they believe an important result is that GD TLR can work in scenarios where learning rate hasn t been optimized much. It would be nice for the authors to actually demonstrate this rather than simply stating it. I suspect this is true, especially for large transformer models, but the paper provides no evidence to this end. Overall a good paper with an interesting technique. Showing that the technique really can work in scenarios where learning rate hasn t been optimized could make this a very important result. As written, there is much work to prove this out, so the idea remains a curious hypothesis.<|endoftext|>The authors proposed an approach to optimize the learning rate of deep learning methods during gradient descent training. The approach uses first and second order gradients concerning learning rate as functions of consecutive weight gradients. The gradient based method suggests no manual tuning is required and considers the learning rate itself as a learnable parameter. The authors also investigate the use of the approach per layer. The ideas of the paper are promising and have the potential to open discussion to a different direction of how to adjust the learning rate on iterative learning algorithms. I have the following considerations and suggestions on how this paper can be improved:  The authors build the method in the context of gradient descent for optimizing a neural network loss function, and I’m wondering about the impact on different loss functions. The paper suggests robustness to user defined hyper parameters such as batch size and initial learning rate, but no consideration is made about the number of epochs. Considering the comparison with literature approaches in a long term training regime the methods tend to be equivalent at some point of the optimization. If the authors consider fast convergence as a feature of the method and suggest ignoring the number of epochs, experiments and comments in this direction are needed. Besides the considerations about the impact of the user defined hyperparameter p, which controls the frequency of learning rate updates. It is unclear if the suggested 0.33 value is valid for different datasets, considering small/complex/sparse or other datasets characteristics. Are all the methods starting from the same initial weights? When considering the Time Requirements experiments, why are only experiments with CIFAR100 and the WRNET_16_4 architecture presented? Did the authors identify any phase shift regime during the training process? Some experiments suggest the proposed method with better performance at larger batch sizes. What is the impact of using bs minor than 64? Could the method generate overfitting on small datasets? There has been little discussion on the limitations of the proposed approach. What are the possible technical limitations? The paper presents an interesting approach with promising results, quantitative and analysis. However, the authors need to provide more comments and address some concerns about the experimental methodology and experiments before publication.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; rating score: 3; The main technical idea in this paper is searching for (mining) roughly parallel pairs in the unpaired corpora, to create a "pseudo parallel dataset". The part of the approach using imitation learning to refine the pseudo parallel data is also new and effective as confirmed by the ablation studies. The authors have also carefully curated the dataset to remove hate speech, and used a filtering method (Appendix B) to ensure mined sentences are faithful to the attribute. 3.The authors show improvements in three different text rewriting tasks over several baselines on both automatic metrics and human evaluations. I suggest the authors try style transfer tasks where the content distributions will be very different across attributes, such as Shakespeare < > Tweets from [6].<|endoftext|>The paper proposes to transform the popular unsupervised style transfer to a semi supervised task. The other metrics might not be the focus of the paper and achieving high measures on them is an added bonus. The authors assume that  parallel sentences inherently exist between the source and the target style corpus. However, the existing unsupervised text style transfer methods do not make such assumptions. Here are my concerns with respect to this. Also some of highlight boxes are missing in the Sentiment dataset1. Please mention that you are highlighting the best performing method for that metric. The adoption of these measures would have made the paper stronger.<|endoftext|>The paper addresses a text style transfer task based on non parallel datasets. Finally, to use imitation learning to enforce the loss contrast between the best target candidate and all others. The results of such a combined approach are competitive with previous works. Weaknesses and questions:  The authors claim "existing methods learn a mapping without considering the self parallelism, ... they tend to learn the mapping between source and target style by randomly mapping sentence pairs." The first step of the proposed approach (matching sentences from the source and target corpora) implies that both corpora have a similar distribution. However, it is not the case for some definitions of "styles." This assumption possibly limits the method s applicability, but it wasn t addressed in the motivation or discussion sections. Thus, to show the improvement of the new model, one needs to demonstrate Pareto improvement both of them (in this case, this means the improvement both in terms of ACC and BLEU metrics).<|endoftext|>This work has proposed a new method to textual style transfer, which is based on the assumption that there exist some pseudo parallal sentences pairs between two styles. It first construct synthetic parallel corpora by using two similarity measures: semantic similarity based on larger LMs and scene graph similarity. Then it trains the generation model via reinforced imitation learning. 3.The empirical results establish the effectiveness of this method. This method is based on the assumption that there are parallel pairs in the original corpora of two styles so that sentences of the same content and different styles can be found out, which limits the use of the method to cases where abundant corpora are existing and there are parallel sentences there in the corpora. S Emb are very small for BLEU and PPL of w/.<|endoftext|>This paper proposes an approach to addressing text style transfer with non parallel data. The basic idea is to follow three steps: (1) for a given source sentence, mine some nearly parallel sentences from the target domain; (2) perform an MLE learning; and (3) augmented with an imitation learning. The proposed method was evaluated on three text style transfer tasks: (1) sentiment transfer; (2) formality transfer; and (3) a new task called political stance transfer. Weakness  First, I am not sure about the novelty of the proposed method. About table 1, I am not sure how the highlights were selected in this table. Apparently, not all the highlights are the best results.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; The authors have considered the problem of imitation learning using only visual observations, and they ve explored the paradigms of using methods based on adversarial learning and optimal transport to solve this problem. In particular, the authors propose two new methods P SIL and P DAC presumably as a way to explore design choices and tradeoffs in this space. Moreover, with respect to the experiment depicted in Figure 7, it does not seem to me that the presented data matches up with the conclusion. STRENGTHS(S1) The stated thrust of the paper that of exploring the design choices and tradeoffs within the space of algorithms designed for imitation learning from visual observations is of great importance to the community studying these methods.<|endoftext|>Representation from the RL encoder are adopted for calculation imitation rewards. The key component of this work is the RL encoder used in both P SIL and P SAC. Pros: Two imitation learning methods based on RL encoder are proposed. The experimental results show that both of the methods outperform the DAC and SIL baselines. However, it is not clear how to augment the data in practice.<|endoftext|>This paper presents two algorithms for imitation learning P SIL and P DAC which are built on top of SIL (Papagiannis & Li, 2020) and DAC (Kostrikov et al, 2019). The first one is an algorithm based on the Sinkhorn distance and the second one uses an adversarial approach, by training a discriminator. in equation 3, $\Psi$ and $\psi$ are not defined.<|endoftext|>This paper focuses on learning visual policies by imitating video data without observing the expert actions. Strength:+ The paper is well motivated. However, such an RL expert is unavailable in most video demos. + Two approaches (P SIL and P DAC) are proposed based on stinkhorn imitation learning (SIL) and discriminator actor critic (DAC), respectively.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 8; The paper argues pruning a pretrained network and finetuning is better than training a sparse network from scratch and tries to connect the fine tuning phase properties to dynamical isometry (DI) literature. The empirical observation makes a case for inheriting the pretrained weights. The empirical observation that inheriting pretrained weights is better than training a sparse network from scratch if the fine tuning phase is carefully tuned. 2.The hypothesis on page 7 connecting larger LR in fine tuning and DI is dubious and not backed by any theory or experiments. As DI theory suggests to have mean JSV around 1. Usually, fine tuning phase is much shorter compared to training from scratch but in this case fine tuning is done for almost the same no of epochs as the scratch version. There are many unsubstantiated claims (or weak statements) in the paper.<|endoftext|>This paper challenges an existing argument that “inheriting the weights of the pruned network is not necessary for fine tuning”. The authors conjecture that fine tuning pruned network requires larger learning rates and longer training epochs because its dynamical isometry is broken. 4) This paper is easy to follow. Although the author claims that OrthP can “complete” resolve the dynamical isometry issue, the pruned network converges still only after prolonged training epochs (90 and 900 epochs). Does that means the pruned networks weights are more difficult to optimize than random initialized ones, which contradicts the "the value of weights" claim? 3) Table 5: Can the authors also provide the “Scratch” results and their hyper parameters? 4) Related to 3), from Figure 2 and Table 5, it looks like OrthP takes the role of a large training epochs to recover dynamical isometry. I like this paper in that it thoroughly investigates existing arguments on the value of pruning weights.<|endoftext|>Post rebuttal update: The discussions did not change my opinion of the paper. This work claims that these previous works do not set learning rates during fine tuning correctly while performing their experiments and hence these claims are invalid. The paper also offers an explanation for this phenomenon via dynamical isometery theory. However I appreciate the efforts taken by the authors to update the paper in light of reviewer suggestions, particularly to make the core argument more rigorous. Is the paper s hypothesis a claim about the speed of optimization, that pruning+fine tuning is a fast way to obtain a small model? It is thus not clear why mean JSV (or any measure of dynamical isometry) can be used to measure model convergence (ref.to boxed explanation in page 7). For example, if gradient norm regularization is used, then the model would converge and yet have small JSVs. For all these reasons, I would recommend rejection.<|endoftext|>They show finetuning with pruned weights actually outperforms training from scratch, when larger learning rates and longer training epochs are adopted. They show that weight pruning breaks dynamical isometry and finetuning can recover it and a larger LR can recover faster. The effect of learning rate on fine tuning stage of pruning is not thoroughly investigated and this paper provides an in time and thorough study. A comparison with other metrics for generalization could be more persuasive. The authors propose strong regularization helps, however, no explanation on why this method helps and how it connects with previous analysis. The paper carefully studies recent thoughts on the meaning of pruning and made some rigorous investigations. Therefore I recommend to accept.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; rating score: 6; This paper presents a method to construct a molecular graph, which is inspired by a spanning tree. Given the discussion below, all of my concerns have been addressed. Nov 24.Since all of my concerns have been addressed in the current version, I would recommend to accept this paper.<|endoftext|>This paper proposes a new paradigm of graph generation, a transformer network sequentially generates a sequence of decisions to generate a spanning tree for a bipartite graph. The authors also have a focus on generating valid graphs (which is seen in the results) in which they mask out invalid decisions during the generation process itself. Strengths:I think this work has a lot of novelty and merit.<|endoftext|>The paper proposes a novel spanning tree based graph generation (STGG) framework. The experiments of the work are comprehensive and well designed. In particular, for PLOGP molecular optimization, they adopt an offline optimization algorithm, which is interesting to me. However, several concerns remain to be addressed.<|endoftext|>This paper proposes a spanning tree based generative model (STGG) for molecular graphs. * STGG is a node by node graph generation method following the depth first order (DFS) of a graph. I vote for weak accept because the empirical results seem convincing.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; Therefore, we think ChIRo has not yet been proved to be a SOTA molecular representation model. And implementation details are presented in Appendix. [Theory]ChIRo uses a series of trigonometric functions to encode the torsion angles. To demonstrate that such encoding is invariant to bond torsion but sensitive to chirality, this paper gives a proof in Appendix.<|endoftext|>It is a bit sceptical that they can actually distinguish this geometry chirality (i.e, considering coupled torsions in the message updates). The distinguishing features are mainly based on the sinusoid transformations of torsion angles, which are invariant to the bond rotations.<|endoftext|>This work focuses on representation learning for molecules with stereochemistry and designs an SE(3) invariant model that processes torsion angles of a 3D molecular conformer. Overall the model sounds reasonable and the results on multiple tasks are promising. Why not sufficient? SphereNet is comparable to Chiro in Table 1, but much worse in Table 2. Why is Chiro much better in the second task?<|endoftext|>To handle the chirality, the most important aspect would be torsion angles of (3), but comparisons to the original GeoMol (Ganea+ 2021) are presented only in "A.8 Additional Ablations" with a confusing statement of "It is not strictly necessary that ChIRo learn the weight coefficients or phase shifts in order to distinguish enantiomers". This paper presents a novel SE(3) invariant GNN model for predicting 3D geometry dependent physicochemical properties of molecules. The developed method proposed a torsion angle encoder having 1) an invariance to rotations about internal molecular bonds and 2) the ability to learn molecular chirality. Overall I liked the paper s idea.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; In the first part of this work, a range of existing attacks is unified in a Bayesian setting. The order of gradient computation and noise addition differs, but it is not clear to me if this is a meaningful distinction in the Bayesian sense. The experiments in Sec.6 then show that this viewpoint can have tangible benefits. None of the insights from the Bayesian framework seem to help in creating these adaptive attacks.<|endoftext|>This paper introduces a framework to address issues in the leakage of information through gradients. This paper claims to provide Bayes optimal adversary as an optimization problem which other attacks can be considered as an approximation of this framework. The experiment section is written poorly and the importance of each section is not justified. + Linking the existing attacks to their framework is interesting.<|endoftext|>This paper provides a gradient leakage attack via Bayes optimal adversary. The authors also demonstrate that existing attacks can be seen as approximations of Bayes optimal adversary. The proposed method is simple and empirically effective.<|endoftext|>The paper proposes a theoretical Bayesian framework for the problem of gradient leakage in federated learning. Since the authors have managed to show that the proposed attacks can be derived as approximations of the Bayes optimal adversary and are not purely custom designed, I have updated the rating.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; The paper introduces a representation consolidation method to properly aggregate the pre trained knowledge from multiple teachers for transfer learning. The introduced method performs on par with multi task training while neglecting the need for teacher datasets. The problem of how to properly use multiple pre trained models for downstream transfer learning is interesting and practical in real life. 2.The introduced method achieves better performance than distillation from a single ImageNet pretrained teacher or task specfic teacher. 2.The authors claim that the generalist teacher is important to the transferability of distilled representations. I did not find anything about it. 3.Similarly, how to properly select task specific teachers for corresponding downstream tasks? How much does it affect when choosing different numbers of task specific teachers? In the experimental part, the authors only show the comparisons when adopting ad hoc teachers, which is less convincing. But unfortunately, I choose to maintain my original score considering the other reviewers  concerns and the overclaiming problem (the method and experiments fail to support the motivation) as I mentioned in the initial review. The motivation of this paper (life long meta learning) is attractive, but the method and experiments fail to well support it. I tend to reject this paper, and recommend the authors refine it and submit it to the next conference.<|endoftext|>  The authors propose a method of learning a consolidated image feature representation from a collection of related task specific teachers that transfer well on novel recognition tasks. To achieve it, the authors utilize multi teacher multi task model distillation framework that jointly distills one or several task specific teachers with a generalist one (trained with Imagenet dataset). Each teacher is set to operate on a different set of classes, and a multi head student is trained to emulate all teachers. Experimental results show that the proposed method doesn’t need to revisit original training data of each teacher, but gains better performance. In this paper, the authors have stated their key difference from the traditional distillation loss item is the generalist teacher item. In addition, the generalist teacher item is just the model pretrained on imagenet dataset. The work here aims at transfer learning, but the experiments are set with the classes in the same domain. Some advice in the experiments, the chart figures are not very suitable here. Besides the generalist teacher, what is the other contribution? Discuss more about the experiments.<|endoftext|>The paper proposes a task agnostic Knowledge Distillation method that includes a generalist model as an additional teacher, to limit student forgetting and representation collapse. The authors call this method representation consolidation and claim it can benefit both related and unrelated downstream tasks. The main contribution of the paper is the  _representation consolidation_ method to consolidate knowledge from multiple task specific teacher models into a single expert student, given a generalist teacher is available. The main findings are that consolidating a task specific teacher with a generalist teacher is sufficient to rescue the student and consolidation performs only slightly worse/similar to a multi task joint training oracle, at least on image classification tasks. Which brings about one of the main issues of this paper   despite proposing a general framework, only the image classification problem has been addressed. The main insight here is   if we can get a generalist that works well on most scenarios, could we squeeze additional performance from the proposed method or not? Furthermore, most of the times few shot learning is illustrated, the few examples of full fine tuning are less favorable to the proposed method. Strenghts:  general method for a better distilled student  easy to implement, provided a generalist is available for the proposed task  no data replaying for the generalist network during fine tuningWeaknessses:  simple training scenario, mostly classification, few shot learning  where fine tuning is employed (Fig 4/ 10) the results are less convincingI lean towards weakly rejecting the paper, based on the limited applicability/experiments mentioned above. If there is a general claim to be made, I would like to see more evidence and other domains tested, beyond image classification. As it stands, I do not find the technical contribution sufficient for acceptance.<|endoftext|>This paper studies the problem of multi model consolidation, which aims to merge (consolidate) the knowledge from multiple models into one model so that we can better transfer this merged model for downstream tasks. The key technique (technical contribution) is to add a generalized model during the knowledge distillation, which can be regarded as s "special" teacher model. I think the study multi teacher consolidation problem is interesting and important. In some practical domains (like healthcare), it is difficult to construct a large dataset to train a model,  how to conduct transfer learning from multiple datasets (models) is critical. Different from previous multi teacher distillation works, the authors add a generalized model (ImageNet model) during knowledge distillation to prevent representation collapse. 2.As the technical contribution is limited, I would appreciate the merits of experiments. For the few shot learning setting, the performance of different experimental runs has large perturbations, it is truly difficult to determine the effectiveness of the proposed method. Are the results in the tables the average values of different runs or single runs? The studied problem is interesting and important and the whole paper is easy to follow, while the technical contribution is limited and the experiments are not very strong to support the advantage of the proposed method.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; rating score: 6; This can cast the downstream task as a sort of fine tuning experiment, whereby the results are heavily influenced by conditioning on the starting point of a single pre trained version of an LLM. While I m not sure that ICLR is the best venue for this work, it is clearly important and deserves to be showcased.<|endoftext|>This paper releases MultiBERTs, a set of 25 BERT base checkpoints to facilitate studies of robustness to parameter initialization and order of training examples. It also proposes the Multi Bootstrap method to quantify the uncertainty of experimental results based on multiple pre training seeds. The paper provides adequate models and a good estimation method for future researches on robustness.<|endoftext|>The paper presents MultiBERTs, a set of 25 model checkpoints to support robust research on BERT, and the Multi Bootstrap, a non parametric statistical method to estimate the uncertainty of model comparisons across multiple training seeds. It seems to be a novel approach, it s hard to assess its significance but the presented application suggests that the impact might be important. The paper presents an interesting approach, a novel contribution that might have some applications.<|endoftext|>The method in this paper is novel and theoretically sound, which is helpful for understanding the large models. Even though the techniques are simple, but they can be categorized into three designs and formulated as a formal problem. This paper is different from classical analysis paper in NLP. Paper Strengths:  It is great to see the models and statistical library are available online (165 checkpoints).
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; However, for images the authors try to explain why CNN+ReLU is isometric by considering images as objects with large monochromatic images. The paper is not a typical paper that we read today. The formulation is simple, and the paper is easy to read (in the main part, which does not contain proofs, only formulation). Some results have doubtful meaning, such as concentration of measure result (see 22e) for example, which contains very unrealistic constants. What is the effect of deeper layers on the isometry property? Some proofs are not needed.<|endoftext|>The authors consider angle propagation in randomly initialized convolutionalneural network layers   given two input images that have cosine angle $\rho$,what is the cosine angle of their feature embeddings when propagated throughthe random layer? The authors give some measure concentration results, inthe spirit of Daniely et al.s results for feedforward networks, that specifythe behavior in the convolutional case, and present examples of different typesof images (unstructured/gaussian; cartoons; CIFAR 10 data) where differentangle propagation behaviors are observed. ### Strengths  The paper is well written and enjoyable to read. There is a good mix of  theory and intuition. For example, can we understand when  this bound is loose/tight based on the structure of the inputs across  patches?<|endoftext|>This paper studies the random initialized CNNs and analyzes the geometry preservation. For linear CNNs, the authors show the JL lemma type results hold. Strengths:The paper is well written. For the linear CNNs with random Gaussian weights, the underlying procedure is a sum of product between the inputs and random Gaussian random variables. For the CNNs+ ReLU, since part of the information can be truncted by ReLU, the contraction behavior is also straightforward. My major concern is the technical significance. I agree the JL type results for CNNs are different from those in the literature. I change my score accordingly.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 8; Will the distillation approach also work for diffusion models in other domains? Diffusion models have recently emerged as a promising class of generative models. The paper proposes a method to progressively distill this slow iterative generation process into much faster synthesis relying on fewer denoising steps, while almost preserving generation quality as measured by metrics quantifying perceptual image quality. The quantitative results are promising, for example achieving FID scores $\leq3.0$ on CIFAR 10 in as few as 4 steps. If we distill the slow iterative sampling from diffusion models into these few step samplers, one may intuitively expect that we pay a price in terms of requiring bigger network capacity to make up for the fewer synthesis steps. This is a problem of very high relevance. The idea to use the deterministic DDIM [1] sampling scheme and distill 2 teacher steps into a single student step in each round of distillation is interesting and novel. Why not?Note that this question is also related to the first point under *Weaknesses* above (i.e., maybe we don t use a bigger model, but have to pay a price with regards to diversity and mode coverage?). However, I certainly acknowledge that this work shows how to do it in an elegant, scalable and high performance manner using a progressive protocol. I would be willing to raise my score, if the weak points were addressed in a satisfactory manner.<|endoftext|>This work studies the problem of improving the sampling speed of diffusion models, which is currently a timely and challenging topic. (2) The proposed progressive distillation is novel and interesting. In particular, the comparison with previous fast sampling methods on CIFAR 10 demonstrates that it largely outperforms these strong methods regarding FID scores with a few model evaluations. Weaknesses:(1) The progressive distillation process seems to need a much larger computational cost than many previous fast sampling methods, such as DDIM and DDPM respacing. As stated in the paper, its training budget is almost the same as training a diffusion model from scratch. I wonder how this concern can be addressed in practice? 1) Typos.For example, in the abstract, “as little as 4 steps”  > “as few as 4 steps”. Still, I have some concerns about the method regarding computational cost and generalization to larger datasets. Thus, my initial rating is a weak accept.<|endoftext|>The paper presents an approach to improve sampling speed of diffusion models. The idea is to iteratively learn integrators for the probability flow ODE which, given a diffusion model, maps noise to data samples. Strengths  An important topic given that diffusion models achieve very good results but that their sampling time is a barrier to wider adoption in practice. Overall, there are quite a few design choices for diffusion models which haven t been evaluated exhaustively and many of the results in this paper will help towards a better understanding on these choices (e.g.parameterization, weighting function, using signal noise ratio of 0 at t 1, different integrators evaluated in the supplementary). It also shows a nice interpretation of DDIM as an integrator of the probability flow ODE. Weaknesses  The effect of distilling iteratively/progressively is not fully explored. This could have effects on both the time required for the distillation process as well as the quality achieved by the distilled models. Given the impressive results obtained with diffusion models, the paper addresses an important and timely topic.<|endoftext|>This work proposes to speed up diffusion models by progressively distilling a score network with deterministic dynamics. Authors also provide a proof that DDIM is a special numerical integrator for the probability flow ODE, connecting two existing approaches. In addition, empirical studies on the weighting function and parameterization of score functions are also discussed. Their impact on model performance is also investigated rigorously in Table 1. It is unclear how the distillation approached introduced in this paper can be applied to other SDEs such as VE SDEs and subVP SDEs, as well as how to choose the weighting function and score function parameterization in those cases. Well written paper with impressive experimental results and clear description of design choices.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; The paper has missed important existing works. I think that the paper has missed important existing works that are closely related to this paper. "Badencoder: Backdoor attacks to pre trained encoders in self supervised learning." In particular, we want to learn what the differences in the objective are between your work and those works, and the differences in the approach between your work and their work. If those works are relevant, the authors should add comparative studies comparing their approach with the approach in those works. I think that it is important to evaluate the approach for one PTM against multiple tasks, as this is a very common scenario for PTMs. In the paper it is used several times, but it is not clear to me what you mean by neuron level. Since this paper misses important existing works and baseline comparisons, I don t think the current version can be accepted to ICLR.<|endoftext|>This paper proposes a framework to inject backdoor into pre trained models sothat the backdoor can be inherited by different downstream student models. I appreciate the paper evaluating on both CV and NLP tasks. Moreover, similar works have been proposed, yet not compared by thispaper. There are existing attacks that  injects backdoors to pretrained models, such as [0,2]. What is the difference  between the proposed attack and existing work? What is the key  contribution of this work? The threat model of the attack is unclear. * For the blending ratio of blending backdoor attack, why the blending ratio for  VGGNet is 1:4, while the ratio for ViT is 3:7?<|endoftext|>This paper proposed backdoor attacks to pre trained models (PTM), in which an attacker can train a PTM such that it outputs pre defined representations for triggered inputs. Backdoor attacks to pre trained models is an important security problem. 2.The paper studied the attacks for both NLP and CV domains. 4.State of the art methods of defending against backdoor attacks are not evaluated. Moreover, the proposed defense can be largely defended by Fine Pruning. are insufficient. 1) The attack may be impractical in realistic settings and 2) comparison with existing work is not sufficient.<|endoftext|>Experimental results show that the proposed attack method can work well after fine tuning and induce the target labels successfully in most cases, revealing the backdoor security threat of PTMs. 5.Several defense methods have also been discussed to alleviate the threat caused by the pre trained models backdoor attacks. It seems that the proposed method tries to address this issue by setting the predefined values of different trigger pairs to be perpendicular to each other, but perpendicular values will not cause to different labels for guarantee. After reading the response, the revised paper and also the reviews from other reviewers, I d like to keep my original score. This paper is trying to deal with a quite interesting and practical problem, and extensive experiments (both NLP and CV tasks) have been conducted to demonstrate the threat of the pre trained models backdoor attacks.
Reject; rating score: 3; rating score: 3; rating score: 3; Could the authors please elaborate on why these theorems are useful or have any relevance? The submitted code does not contain the data, e.g., learned models & datasetsIn summary, the paper should be rejected as the contribution on physics inspired networks is marginal and the claims of the paper are not backed up by experiments.<|endoftext|>The idea of modeling the Hamiltonian in a latent is appealing and the best way of doing this is still largely unsolved in the community. This paper suggests a way to do this that augments existing RL methods (Fig 3/Table 1).<|endoftext|>However, due to the concerns on technical soundness, I think this paper is not ready for ICLR yet. I m very surprised that the authors didn t comment on this gap between their motivation/models and the systems in the experiments. The q and p learned in these systems have no physical meaning at all.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; The authors of this paper propose a GAN based method for the anomaly detection task. Their method relies on so called Bad GANs (that uses the trained GAN discriminator to distinguish between inliers and pseudo anomalies), a new orthogonal loss (that favorises generated samples to cover a larger angular space) and eventually a so called Good GAN (which generator is shared with the Bad GAN; and discriminator is trained on "real" anomalies). In the current state and given all the concerns on experimental methodology, I would recommend the rejection of the paper with regards to the standards of the ICLR conference. This point is not discussed in the main paper but different hyperparameters are used for different datasets (Appendix A).<|endoftext|>In this paper the authors propose a method for image anomaly detection (AD) based on GANs. The use the GAN discriminator based on a method called "Bad GAN" where one trains a GAN to produce low likelihood samples (trained using the nominal data) and then use the discriminator, which has been generalized to a large class of samples, to determine if a test sample is anomalous. In the proposed method the authors include $L_{orth}$ to enforce angular diversity of generated samples, a "good" gan discriminator that is trained to correctly determine if a sample is generated, and a bad gan discriminator inducing the generator to make points near the boundary of the nominal set distribution. * It would be interesting to see how the method performs on CIFAR10 one v rest, since that seems to be a bit of a standard dataset and fairly challenging for generative model based AD. * The paper lacks any theory. Lacking any theory a paper should have very strong experimental support. * Good lit review. Overall the paper is okay, I don t think the results are impressive enough, or that the proposed method is novel enough for a top tier conference.<|endoftext|>The authors propose the coupling of coupling two GANs for anomaly detection: one which focuses on generating examples at the periphery of the distribution (hence the name BadGAN) and one that generates reference examples (the GoodGAN portion). For one, the experimental evaluation is somewhat limited, in that it focuses on predominantly simple distributions and benchmarks (e.g.MNIST, FashionMNIST). Aligning experiments on MNIST with this challenging setting could potentially improve the paper s contribution, and make it a bit clearer how it compares to existing work. In addition, I would however strongly recommend that the proposed method is also evaluated on more complex benchmarks, e.g.CIFAR 10.Next, the proposed method seems to require the availability of labeled anomalies. This is a very strong assumption for AD, and as "Rethinking Assumptions in Deep Anomaly Detection" of Ruff et al.(2021) clearly shows, often very simple classification based methods that require only very few labeled anomalies achieve very robust performance. Two GAN based anomaly detection works which seem to be missing from this manuscript are:  A. Berg, M. Felsberg, and J. Ahlberg. Concerns remain however with regards to performance versus competitors (such as F AnoGAN), and because recent benchmarks like semantic AD have not been incorporated fully.<|endoftext|>The paper proposes a new interesting method for anomaly detection. In particular, to overcome the limitations of existing Bad GAN, the authors introduce the orthogonal loss to regularize the generation of anomaly samples to be distributed evenly at the periphery of the training data. Furthermore, in the scenario of available anomalies, the authors combine Bad GAN and Good GAN together, in which Good GAN learns to generate the anomalies while Bad GAN reguralizes the anomaly pseudo anomalies at the boundary of inlier distribution. **Pros** 1. 3.This paper provides comprehensive studies and experiments, including both qualitative analysis and quantitative results, to show the effectiveness of the proposed framework.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 8; In this paper, the authors propose to improve the vanilla greedy Core set active learning algorithm by (1) weighting the distance with uncertainty (measured by doubt, $1 \max_yP(y|x)$) and (2) use beam search instead of greedy search where the beams are selected by average uncertainty. The reasoning that “original core set algorithm improved significantly from those baselines, we expect improvement over the original core set algorithm to imply similar or greater improvement as well” does not make sense as the paper was published quite early and only compared to less advanced algorithms at the time. The authors provide thorough theoretical analysis of potential rationales behind the improved performance.<|endoftext|>This paper proposes to improve the core set approach to active learning of Sener and Savares by a probabilistic extension. The paper is not very well structured. As  memory efficient  and  run on GPU  is stated in the paper, is computation efficiency a part of motivation? What is more favorable than the original method? This paper needs major revision for judgement.<|endoftext|>This paper attempts to improve upon the greedy core set for active learning (Sener and Savarese) by employing distances scaled by uncertainty. The proposed method then leverages a beam search algorithm to identify the best core set configuration among candidates with the lowest log confidence to yield further improvements. No comparison to other active learning methods. Computational cost of these additional steps were analysed. Not much analysis here on that. would be useful? If using VGG16 for uncertainty: why?<|endoftext|>In this paper, the authors enhance the original core set algorithm proposed in Sener & Savarese (2018) by incorporating distance measures that are weighted by confidence/uncertainty levels. Based on CIFAR10/100 and SVHN image classification benchmarks, the authors show that the proposed "doubt weighted" core set algorithm can improve the active learning performance compared to the original greedy core set algorithm. Of special interest is the comparison against recent Bayesian core set algorithms, which are capable of naturally incorporating uncertainties in the predictions into the selection scheme based on a Bayesian paradigm. It would be meaningful to provide some additional justifications of the choice made in the proposed algorithm and discuss limitations (if any) of the current scheme, as well as alternative ways for quantifying uncertainty and their potential pros/cons. This paper presents an extension of the original greedy core set algorithm.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; The paper proposes a test time batch normalization method for domain adaptation. They present a formulation $\alpha$ BN to calibrate the batch statistics by mixing up the source and target statistics with a fixed hyper parameter of alpha. There is no discussion on how to select $\alpha$. The main concern is that the proposed Core loss in EQ 5,  $L_{core}   1   \sum_j p_j p_j$,  is actually very similar to the entropy loss $L_{ent}     \sum_j p_j log(p_j)$. The proposed method can be considered as an extension of Tent. The proposed alpha BN actually provides a better initialization of the BN parameters for DA, which is why it can obtain better performance than T BN. I consider the proposed method is incremental. The authors should provide more explanation & discussion of the $L_{core}$ in the rebuttal.<|endoftext|>Also, it is unclear how the $\alpha$ parameter can be set in practice. Thus, while this work presents an interesting empirical study, I think it is borderline and I lean towards rejection. A major contribution in the paper is the $\alpha$ BN method of combining source and target batch statistics, which the paper argues improves upon T BN. However, they do not modify the model parameters unlike the proposed method and other test time adaptation approaches. Moreover, the method of Pandey et al 2021 was not included in the comparisons. Overall this paper proposes a simple and effective method for test time adaptation that seems to outperform the previously described TENT method. The additional ablation studies address my concerns about the effect of the individual components, but as pointed out by the other reviewers as well, technical novelty is somewhat limited as both components have existed in the literature although they are not applied to this particular setting.<|endoftext|>This paper presents several modifications over test time normalization and test time adaptation, which swap the statistics of the BN layer during test time to handle the distributional difference. Specifically, they propose alpha BN to balance the source statistics and target statistics, and CORE to further refine the transformation parameters of BN, which is similar to prior test time adaptation methods. 3.This paper is well written and easy to follow. The proposed method is technically sound but not a revolutionary idea. Regarding CORE, difference from (Jin et al., 2020) is not clear from the manuscript. What is the batch size in the experiment? Besides I am suspicious that the performance gain is caused by the hyperparameter selection. It should be better to include the hyperparameter sensitivity of Tent and CORE to fully show the CORE is indeed superior one. Overall I think this is well written and an acceptable paper with solid empirical results. I am happy to increase my score if the response clarifies my concerns or I misunderstood some points.<|endoftext|>This paper presents a method to calibrate batch normalization statistics at test time to improve a model’s cross domain generalization ability under covariate shifts. The main concern is the lack of technical novelty. 2.The analysis in Figure 1 is inspiring and interesting. The idea of mixing source and target statistics has been proposed by Schneider et al.(2020).I don’t see how the so called alpha BN differs from it. It seems that the class correlation minimization loss is directly taken from Jin et al.(2020).I think the authors should not claim to use it as a contribution. I think we need more analysis to understand this phenomenon.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; rating score: 6; TLDR: The authors theoretically study convergence properties of the Noise Contrastive Estimation (NCE) loss, identify issues that make its optimisation difficult in some practical scenarios, propose a method to resolve this and provide theoretical guarantees that their proposal works. There is not a great deal of work theoretically analysing the NCE objective, and so the reasons for why choice of Q is so important is not rigorously understood. This is an interesting and seemingly poorly studied area. Also, what is the impact of dimension on your results?<|endoftext|>This paper examines Noise Contrastive Estimation (NCE) from an optimization perspective. The NCE task consists in training a Discriminator to distinguish between data and noise samples. 2.What is the impact of the proportion of noise samples hyperparameter on this paper s analysis?<|endoftext|>This paper constructs a Gaussian case to illustrate why NCE doesn t work. To overcome this problem, the author proposes a normalized GD, which improves the convergence rate. The author also introduces a modification of the NCE loss. Pros:The author constructs a bad case, and improves the NCE by fixing problems in the bad case. It would be better if more experimental results on more complex data sets are provides.<|endoftext|>This paper studies the noise contrastive estimation (NCE) method for the exponential family of probability distribution, which is usually used to learn probabilistic models. Based on the above comment, I believe this paper gives interesting theoretical results on NCE and would vote for accept. Strength 	Theoretically understanding of NCE method is an important direction and I believe this paper gives some interesting results in this direction.
Reject; rating score: 3; rating score: 6; rating score: 8; rating score: 8; rating score: 8; Different from the previous module networks, the authors propose to learn the middle level goals first and decompose the step by step action predictions into two modules, master policy for navigation and interaction policy for object interaction. The method overall achieves slightly better performance than the previous method. **3.Concerns**The reviewer has many concerns about the method which the reviewer thinks resolving them will make it a better paper. At least, the reviewer thinks that a discussion of the usage of visual input for PCC and modules are needed. Also, it adopts a low level text instruction as input when compared with HLSM. It s OK to use these additional signals as input but the reviewer thinks that it will be better to clarify them in the comparison. We care more about the unseen testing split and so does the official Alfred challenge. The authors of HLSM recently release their code publicly on github (https://github.com/valtsblukis/hlsm.git) and achieve much better performance (20.27) with better parameters fine tuning. It is OK not to compare with the updated performance in this submission since the code has not been released at that time. However, a discussion about how HACR is better and complementary to HLSM will be more convincing. ** The performance of HACR seems to suffer from strong overfitting in table 1 (from seen to unseen and from validation to test). More analysis for such a performance gap is needed and makes the paper stronger.<|endoftext|>This paper proposes a hierarchical approach for policy learning in interactive instruction following for the ALFRED task. Strengths:  In one of their experiments, they show that training a hierarchical policy learns faster and more efficient action sequences as opposed to their flat policy counterparts. Also, hierarchical policy produces subgoals which leads to a more interpretable and transparent approach. The paper is well written and easy to follow with proper figures and with explanations of all the notations used in various modules. I believe it is not mentioned in the paper. How was it decided and how does it impact the usability of Loop Escape module for the ALFRED task? For the sub instruction (3) in language instruction in Figure (2) _“Turn around, bring the potato to the microwave on the right.”_, this corresponds to two subgoals “GoToLocation” and “HeatObject”. How are they different from each other? It might be good to clarify. Overall, I feel that this is a good paper with good results and clear presentation.<|endoftext|>The main contribution claimed in the paper is the decomposition of the high level goal into subgoals (which makes it interpretable), adding OEM and LEM to boost performance, and the SoTA performance on ALFRED benchmark. See the questions below for concrete examples. The above two examples are not mentioned in the main claims but they obscure the main points as I’m not sure if they are technical details or a contribution. The paper ablated on aspects that are not mentioned in the main claims such as DA, NIH, and OCMP. It’s unclear what conclusion the readers should draw from the analysis. Lacks analysis compared with previous best models. E.g., comparison in Table 2 is too weak (flat policy is a weak baseline); should compare to similar hierarchical approaches referenced above. As far as I understand, prior work does generate subgoals which group low level actions. If it’s pointing at the 3 layer modular structure, then I’d like to understand how having architectural hierarchy is useful in addition to action hierarchy   Q2: How is encoding the instructions with Bi LSTM a novel approach? Is this designed by the authors? Shouldn’t this be for the PCC to decide? I would be happy to raise the scores if the authors could clarify more on the above two points as well as answering my questions.<|endoftext|>The paper looks at the problem of instruction following (using language) in a navigation+interaction task setting, encapsulated by the ALFRED benchmark. In a general set of environments, this value could vary drastically and there could be a better way to handle this (if at all such a module is necessary). ## Short versionOverall, I think this paper presents an effective hierarchical approach to instruction following that’s tailored a bit too tightly to the ALFRED benchmark. I respect the effort put by the authors in carefully designing each component and engineering an effective algorithm for the ALFRED benchmark but have concerns regarding the generality of the findings. 3.The task that the paper addresses is extremely visual and the authors do a good job of visually showing rollouts and their method in action. 4.The implementation and experimental details, while not exhaustive, feel sufficient to understand and replicate the settings in which the results are presented. Don’t worry, this does not affect my impression of the otherwise well written paper :)  [Section 1] “To evaluate our proposal in a challenging scenario*s*”  [Figure 2, caption] “comprises of *tree* main components” *Update*: Updating my recommendation to reflect the discussion by the others. My biggest concern with the method is that it seems overfit to the ALFRED benchmark and a lot of the design decisions were motivated by common failure cases in the benchmark. I am on the fence regarding acceptance and look forward to a response from the authors. The paper presents an effective algorithm for instruction following within the ALFRED benchmark but the means and insights seem overfit to the metrics. This feels more like a by product and not requirement. It’s not clear to me how the single policy metrics imply this.<|endoftext|>This paper proposes a hierarchical and modularized framework for addressing the ALFRED language guided task completion problem. The hierarchical and modularized approach seems promising for addressing the ALFRED problem, which is a mixture of navigation and multiple interactive actions. 2.Comprehensive experiments in the main paper and in appendix nicely support the proposed methods and arguments in this paper. The performance of the agent on ALFRED has been improved by a significant margin. 3.The overall paper is nicely written, tables, figures are clearly presented. 2.Following the previous comment, I believe the learning of some subtask can benefit each other, a simple example might be: heat and cool both requires the agent to put an object in to something, which such information is not shared across specialized policies. I would like to learn about the authors’ thoughts on this point. (2) Following previous point, it is unclear how does each step by step instruction x^hat_i translate to multiple subgoals. E.g.Figure 2 Instruction 3 contains navigation action and object interaction, but from Equation 1, it seems that each x^hat_i only translates into a single action. I hope the authors can elaborate more on this point. The paper is nicely written, it presents interesting and valuable ideas, and the experiments are comprehensive. I am happy to accept the paper if the authors can clarify my questions.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 8; This paper proposes a theoretical bound on inductive biases self attention module can represent. I think this paper proposes an interesting conclusion from its derivation of the theoretical bound on self attention modules.<|endoftext|>The paper provides a theoretical analysis of the functioning of the self attention modules.<|endoftext|>3 The theoretical analysis is rigorous with assumptions and detailed proofs. With this bound, the paper analyses the inductive biases of self attention modules; it investigates which function and the long range dependencies the self attention modules represent. This paper aims to answer this new question.<|endoftext|>e.g."Untangling tradeoffs between recurrence and self attention in artificial neural networks. This paper adresses an important gap in theoretical understanding of attention based systems that are widely used in state of the art models. Is this a typo ? I believe there are two main points that could be improved to further strengthen this contribution.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; This paper proposes to solve the long tailed recognition by aligning the predicted label distribution with the test label distribution via Optimal Transport (OT). The sentence seems to be unfinished. after reading author s latest revisions I find the limitations of the methods adequately addressed in the latest revision. 3.A convergence analysis for the proposed OTLM is provided (Sect 4.5), making the paper easier to be reimplemented and reproduced.<|endoftext|>General comments:The overall idea proposed by the paper is interesting and seems well motivated.<|endoftext|>This paper proposes a new method for post hoc correction in long tailed recognition. The introduction of W also seems abruptly. Such an idea is well supported by the existing theory guarantees.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6;   This paper is established by a discovered phenomenon that in distillation, not all the classes’ performance is improved although the student is improved significantly. Thus, for me, this observation is not surprising enough. So in section 3 (are distillation’s gains uniform), the authors use big space of the main paper to tell us the conclusions, which are already known by the researchers in the field of distillation. In this way, this paper is more or less like an experimental report. So, the contribution of the main method is limited.<|endoftext|>The paper proposes to have a closer look at (logit matching) knowledge distillation, and explores whether the gains are uniform across all portions of the test data or if instead there are gains on some portions of the data but damaged performance on others. This is justified by the potential of KD to amplify biases. Issue 2: The main results, shown in Table 3, are not very impressive. The score would be around 4 if that existed.<|endoftext|>This paper is clearly written and well organized. **Weaknesses**The improvements of AdaAlpha and AdaMargin on ImageNet LT, a larger dataset for long tailed classification, are marginal. This marginal improvement limits the contribution of the proposed methods. The contribution of AdaMargin is somehow limited. These results show that AdaMargin is not a good choice in practice, which limits its technical contribution. Overall, this paper proposed an interesting and important perspective on the fairness of distillation, which will inspire future works to pay more attention to the fairness of distillation.<|endoftext|>Furthermore, it is also demonstrated that these phenomena can also impact the fairness of the model. + The paper is well written and easy to follow and brings out an interesting issue with distillation+ An easy to implement solution is proposed for addressing the issue highlighted. AdaAlpha/AdaMargin are not compared with more advanced distillation methods. as well or if it only concerns vanilla distillation. Overall, I enjoyed reading this paper, which brings out an important issue in distillation. Furthermore, there is no discussion on why distillation brings out this behavior from a theoretical standpoint.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; The paper introduces a novel mechanism to add early exiting on existing networks without re training the original network. This could work just as well, and make it easier to  retrain . I think the idea of the paper is cute, and it surprised me that this worked. However, when reading the paper there are some glaring holes and potential problems with the method that the authors don t comment on. First of all is the actual computational overhead. The method is not parameter free. Likely far from it. Given the author s focus on mobile devices, this doesn t seem very feasible. The authors seem to avoid this problem by not considering datasets/tasks with a lot of class labels. The second reason why these drawbacks seem to not show up in the paper is the usage of really really large models for MNIST and CIFAR10. I wonder what happens to the gains of this method when comparing on such more efficient models.<|endoftext|>The paper proposes a method for early exit based conditional computation for reducing the inference time of a given neural network. Although the main idea of the paper is intriguing, I think the empirical evaluation needs to be extended and the possible application scenarios should be more thoroughly discussed. Strengths:  The proposed approach is novel, and, to the best of my knowledge, early exits have not been explored from the perspective of non gradient training, which is a new research direction. The authors show how their method performs in an unsupervised learning setting, which is an interesting new application of early exits, which were so far mostly considered for classification problems. Weaknesses:  The empirical evaluation, in general, is lacking and should be extended, as in the current form the performance and use cases of the proposed method are not clear. The most complex dataset used in this paper is CIFAR 10, which is currently not considered particularly challenging by the community. Although the authors consider the additional memory footprint, they do so for the case of CIFAR 10, where the number of classes is fairly low (10). In practice, many real world datasets consist of a larger number of classes. The authors consider adding early exits only at specific points to reduce the memory overhead, but do not show experiments on well it would work in practice.<|endoftext|>This paper proposes a new method for performing early exits on a pre trained architecture. Results on small datasets (cifar mnist) using very deep networks show that proposed method achieve better results compared to some of the previous work. I think studying early exit is a very important research direction and the focus on simplicity is important (if not more than final performance) for achieving real world impact. As far as I am aware using nearest centroid classification (NCC) for early exit classification is novel (though not super original as NCC is used widely for classification) and the experimental study provided can be useful for future research. Is it a typo? I recommend authors to have (at least some) matching experiments. However the potential impact of this experimental work is currently limited due to its choice of datasets/architectures and lacks in depth investigations/ablations. Specially there are 2 ideas (1) concat of class means with activations (2) using 2 stage early exit strategy.<|endoftext|>The paper proposes a method for early exit during inference of DNNs. Rather, it uses the learned embeddings from intermediate model layers to decide where to terminate the execution. Specifically, for a trained model, the method calculates the per class means of intermediate activation. The proposed method is very simple and intuitive. The evaluations in the non constrained training time mode show that the proposed method cannot compete with prior methods unless it is combined with them. The evaluated datasets are fairly simple, therefore it is not clear how well the method can generalize in face of more complicated data patterns such as ImageNet.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 5; This is not clear in the paper. Therefore, the authors propose their method, DPLC, to tackle this problem. The final rating will depend on the authors’ feedback. + The face recognition performance benefits from the supervision combination of local data and online cluster.<|endoftext|>This work proposes a novel framework to train a face recognition network for the federated learning setting. I will adjust my rating according to the feedback from the authors. The details of these setting and other experimental results and discussion are totally missing from the paper.<|endoftext|>This paper proposes a FL strategy for face recognition. (3) The authors mentioned privacy cost and the proposed method is efficient. I think this work is inspiring for both industry and academia. (1) The authors do not compare with any existing methods.<|endoftext|>Moreover, the face images in the other local datasets must not have large overlap with the local face images in the feature space. During the rebuttal process, concerns 2 and 4 have been addressed to a great extent. 3) The paper appears to analyze the differential privacy guarantees of revealing the cluster centers and the model parameter updates of the feature embedding separately. Overall, I would like to upgrade my rating by one level based on all the discussion.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; The paper introduces a new transformer architecture, called CoBERL, targeting reinforcement learning. The key ingredients of the proposed architectures are the use of LSTMs to reduce the need of large transformers and a contrastive learning procedure that doesn t require of human data augmentation. However, the paper has clarity issues and doesn t seem completely self contained. before entering in the details.<|endoftext|>The paper introduces a contrastive learning objective to learn representations for an RL agent. The proposed approach seems like a novel combination of existing methods with the advantage of requiring less prior knowledge and higher sample efficiency. Using time steps from other trajectories in the buffer seems well motivated and avoids the need for designing augmentations. A minor gripe is the use of Y for input and X for output of the transformer. The proposed approach is a novel combination of existing methods that is well motivated. Experiments are comprehensive, including an ablation study.<|endoftext|>The paper replaces the recurrent architecture of R2D2 with a Transformer LSTM hybrid, and introduces a bi directional contrastive loss that will be combined with the RL loss for overall optimization. Yet in this work it is being claimed that a combination of these data hungry methods results in higher data efficiency that with other methods. The results on improved data efficiency come in a for of AUC curves as compared to GTrXL. This makes it a bit hard to distill the core idea and important parts of the contribution from non essential implementational details. Questions * In the Rainbow paper (https://arxiv.org/pdf/1710.02298.pdf) the reported improvement over human is ~200%+ (Figure 1), in the current work it is reported as 874%, where does this difference come from?
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 6; In this paper, the transfer of a reinforcement model (RL) setting from an idealized (training) environment to a more realistic environment with distractors in the observations is considered. Instead of augmenting the training environment with more data so as to make the system more resilient to variations and distractors, the system is adapted at test time to be invariant to the specific distractors found in the environment. Experiments in simulation show the benefits of the proposed approach. Crucially, the agent is not able to access any reward data at test time. I do believe that this setup is relevant in practice in robotics applications. There are indeed many cases in which sim2real faces generalization problems. While the goal of the paper seems very worthy, and the proposed algorithm seems to provide empirical benefits, the theoretical presentation and the justification of the approach are severely lacking, and I would say incorrect. In particular:  Why is a distinction between S and O even made? Why is Eq.(5) just adding the two losses? Is this dimensionally correct? How do we know they are balanced? It seems that the relative scaling could be arbitrary by simply changing the scale of the actions. However, this frozen generator is put inside the expectation over "outside of the distribution samples". The need for the GAN: It is unclear what the GAN portion of the architecture is bringing to the table. Since the inverse dynamics head is locked and learned on the training data, making it work on the test data should result in a z that aligns well with the latent space discovered at training time. The paper contains an ablation section showing that removing the GAN makes the system not work at all. But the authors had said that, after training the inverse dynamics head on the training data, this head is frozen and remains locked during adaptation to test data. How can it possibly memorize the test dynamics then? And, even though I don t understand how this overfitting can happen, if it did, shouldn t the solution be regularization of the inverse dynamics network? What is it? This paper tackles an important problem, but contains technical inconsistencies, lacks rigor and provides very little detail about the proposal as a whole. The use of a GAN, a major part of their architecture, is completely unjustified, and we are told that it s necessary because the system doesn t work if we remove it... but there s no explanation as to why. Empirical results seem good, but there s little support as to why this is the case.<|endoftext|>The inference in this generative model is accomplished using a GAN framework with auxiliary losses for the inverse dynamics of the system. As far as I am aware the authors propose a novel method for solving the problem of generalisation to new (but similar) domains in RL. This is an important problem to solve for the real world deployment of reinforcement learning agents. The results provided indicate that the authors  method performs well. I list them here:    1. A more commonly used term would be better. 2.The statement "... with different forward dynamics, so long as the distinct dynamics maintain *some semblance of similarity*" is very vague. Could it not be replaced with something more concrete? A.In some cases G_\theta has two arguments, the observation and action, and in other cases it only has the observation. B.(Related to A), should the text after eqn 2, also mention a mapping similar observations *and actions* to close latents? 4.\tilde{z} and D are not introduced before eqn 3. 6.In eqn 5, it is not clear why J_inv is within the arg max over D, given that it does not depend on D (i.e.D does not appear in eqn 2)? 7.Section 4.2 would be much more clear if written in an algorithm environment rather than as paragraphs of text. I would expect to see a wider range of experiments in an ICLR paper introducing a new method. Otherwise, how can readers know whether the method is generally applicable? 3.Why was the comparison to PAD not also made in Figure 5? 4.How many repetitions were made to get the confidence intervals in Table 1? 5.The proposed future work of exploring the effect of exploration strategy seems like it should be included in this paper, in order to paint a full picture of the proposed method. 2.(nit) Regarding the slight abuse notation for eqn 1, there seems to be enough space to write things correctly. 3.Regarding the discussion of data augmentation techniques in the paragraph above Sec 4. While it is true that even extreme data augmentation techniques would fail to cover the test MDP observation space, it is not clear that data augmentation does not ultimately play the same role as *invariance through inference* albeit in a less direct manner. That is, data augmentation should force the policy learned to be invariant to the noise from the data augmentations and thus be more likely to generalise to the test MDP setting. 5.How much of a challenge was it to get the adversarial training to converge in the various experiments presented in the text. This was not touched on in the manuscript and seems an important detail for the general applicability of this work. CoRR abs/1803.10122 (2018)While the authors  proposed method seems like a promising solution to a challenging and important problem, this paper is let down by a lack of clarity in presentation, and a narrow range of experimental evaluation. They have also expanded on their experimental results and clarified that the scope of their experimental work is larger than I initially understood. With these two points in mind, I have increased my score from 5 to 6. Unfortunately, I do still feel that the experimental evaluation could be more comprehensive, which is why I have not increased my score further. Additionally, I have not had the time I would like to go through the heavily revised paper, thus I have decreased my confidence.<|endoftext|>**Introduction:**In this paper the authors aim to tackle the problem of learning a model that generalizes well on a test distribution that samples outside of the training data distribution. If the target domain is not know a priori things like domain transfer, domain randomization and meta learning techniques may fail. The agent has no access to the reward function at test time. That is, the state and reward structures between the train and test MDPs are quite similar, but the observation spaces between the two MDPs are significantly different. **Invariance Through Inference and Algorithm**The goal here is to learn an encoder mapping semantically similar states in the train and test distribution to latent vectors. The approach is defined by two objectives: 1. a distribution matching objective encourages the latent distribution induced by both MDPs to match. 2. a GAN style loss to help ensure that the latent code is not using distracting information that would help distinguish the train and test MDPs. Results are presented showing that baseline methods augmented with ITI show much slower performance degradation as the distraction intensity is increased. They compare to ITI on DMC for fixed distraction intensity. In particular, we posit that the signal from PAD’s inverse dynamics head does not encourage the latent train and test distributions to match, which is a feature specifically baked into Invariance through Inference. This provides a more principled way to model environment invariance and can possibly scale well in comparison to data driven approaches. However, it is unclear whether this is typically the case for the other baselines. * The results showing decreased degradation for distraction on DMC (Fig.5) look good and are consistent across baselines and distraction dimensions. * A nice ablation demonstrating the effect of the adversarial objective. Are there any theoretical bounds or existing results that might give us confidence that this approach scales well across any out of distribution evaluation scenario? Are there other approaches that might be considered? Could combinations of this approach and data augmentation lead to even more robust adaptation to out of distribution estimation? What if we simply have access to large amounts of more varied data, do the gains of ITI remain strong in such scenarios? Do we use training samples only to train the encoder? It would be helpful if these conclusions were better supported across more variation and/or datasets. Do they have access to reward of the target distribution? * Table 1 doesn t seem to describe what is being compared. * When discussing the comparison to PAD, it  s stated that the hypothesis is that it does not encourage latent train and test distributions to match as ITI does, could this be described in more detail wrt to PAD itself? While I believe that the ideas of this work are novel and very interesting and provide a potential pathway away from data augmentation method for out of distribution generalization I have two issues with the paper: 1) the evaluation domain may be too narrow to provide confirmation of the approach with confidence over other methods, 2) more clarity and details of the approach would be helpful in understanding this work, how it contrasts with others, and where it exceeds those. Post Rebuttal:My initial two main concerns with this work: 1) how well the approach generalizes and 2) the clarity of the paper. The updated draft addresses the second concern very well and so I consider that resolved. For the first point, the authors have carried out a large number of new experiments which strengthens the case for control domains but to me the overall claims of the paper seems to be more general than that. That said, I believe this is a worthy contribution given the efforts put forth by the authors and will also increase my score to a 6.<|endoftext|>The paper proposes a method for learning invariant latent representation of the observations from MDP processes that share some aspects of their dynamics but differ in states. This is useful for generlising reinforcement learning agents to wider range of variability of test conditions (i.e.test data can be out of distribution of the train data). The invariant latent representation is derived through minimisation of mutual information between latent encoding of the in distribution and the out of distribution experiences by "fooling" a GAN like discriminator tasked at differentiating the two. I quite like the aim of the proposed method, and the results are impressive. The fundamental principle of using the discriminator to obtain invariant latent representation of an out of distribution state space of the MDP seems like a very sensible and practical idea. The only problem with the paper is that is very high level and I can t quite figure out how it all works and comes together. Seems there are different phases to training...but I can t quite understand what happens when. What is a_t in Equation 2 when training on MDP_test? What s z and \tilde{z} in 3? Does it store all training data? I don t think it would require a huge amount of work, but a bit more meticulous explanation of the loss functions and the training process is needed. At the moment it seems that only those intimately familiar with the method and code (that is not provided) would be able to implement it. The direction seems good, results are very encouraging, just need to flesh out the actual method a bit more, otherwise it feels there isn t enough information to reproduce.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 5; This paper presents a method to train diverse policies. New policies are trained to maximize the original reward while also optimized to be distinctive compared to previous policies. This is achieved by switching between optimizing for original reward and diversity reward. 2.The method is clearly described. The idea of trajectory filtering based on the sum of cross entropy and how to make use of rejected trajectories with the intrinsic reward to promote diversity is new to me. Issue:1.Some result indicates that the additional intrinsic reward can lead to suboptimal behaviors in terms of the extrinsic result. e.g., in the mujco environments, the 1st policy always demonstrates the best performance, and the subsequent policies can either perform worse or better than previous ones . It will be nice to provide a discussion about this. This paper presents a simple and effective approach to generating policies with diverse behaviors. I believe this will be an important contribution to the field.<|endoftext|>The main idea is that at each iteration of the algorithm, a new policy is optimised so that it maximises the extrinsic reward and satisfy the condition of being different enough from the policy already in the collection. In order to solve this constrained reinforcement learning problem, the authors propose a new loss that effectively rejects the strategies that are similar to already discovered ones and provide an intrinsic reward for pushing these strategies as far as possible before considering the extrinsic reward. The experimental evaluation is appropriate and demonstrates the benefits of the proposed method in a large variety of domains. These approaches can generate collections of hundreds of NN policies and would appear as suitable baselines for this paper. An aspect of the algorithm that remains unclear to me after looking at the experimental results is if every iteration necessarily leads to a policy added to the collection. Figures 1.a, and 7.b show that it is possible to have fewer strategies than the number of iterations. Quality diversity: A new frontier for evolutionary computation." [2] Nilsson, O., & Cully, A. I believe the paper could be improved by also considering recent work from the QD literature which could be applied in similar domains while scaling to larger collections.<|endoftext|>This paper presents a Reward Switching Policy Optimization (RSPO) that discovers diverse policies that are locally optimal and sufficiently different from the existing ones. This is done by adaptive switching among extrinsic and intrinsic rewards that are used for policy learning. RSPO makes use of both the trajectory filtering objective and the exploration objective. The overall idea of the proposed method makes sense to me. I have the following concerns and questions regarding this paper. I can adjust my score accordingly after the authors reply to these questions:(1) My first concern is regarding $\delta$. (2) My second concern is regarding using the reward prediction error as your reward driven intrinsic reward. I believe authors can fix this issue as it affects the readability of paper. E.g.Figure 4 in the main text has been referred to only in the Appendix. The location of Table 1 doesn’t match with the corresponding text. Figure 7 (a) does not clearly show other approaches (PG,DIPG,RND) due to the overlay.<|endoftext|>If the results in Tables 2 and 3 are the mean values over multiple trials, the standard deviations should also be reported. In the proposed method, policies are sequentially trained with the constraint that encourages to find different behaviors. Although the paper reports some interesting results, some of statements regarding the proposed algorithm do not seem well supported. During the training process, the extrinsic and intrinsic rewards are switched based on a metric for quantifying the novelty of trajectories. Comments after the author response  I appreciate the authors  effort to address my concerns. The experimental results show that the proposed method discovers diverse strategies in these domains. In my understanding, the policy update is simply PPO, and the only difference from PPO is the use of the specific form of the reward function. In addition, the reward function in Eq.(9) also has the intrinsic reward which does not have the indicator function. In my understanding, policies are trained sequentially in the proposed method, and I guess that a policy is optimized in each iteration. Minor comments  The proposed method is evaluated based on diversity metric, but the results of the average return based on the extrinsic reward should also be reported. As the goal of the proposed algorithm is to obtain the diverse policies that solves given tasks, the average return based on the extrinsic reward is also an important metric.
Reject; rating score: 3; rating score: 5; rating score: 8; The paper derives the p Laplacian message passing formula under the p Laplacian based regularization framework and further proposes p GNN architecture. Experiments show the superiority of p GNNs on both heterophilic and homophilic settings. However, we still have some concerns for the paper before further evaluation. The paper is well written and the theoretical part is wellorganized, though not significant. Strength:(1) The author proposes a new general GNN architecture based on p Laplacian message passing,which claims to work on both heterophily and homophily settings. The authors seem not to have given a good discussion on this aspect. So I do not see how that theorem can be applied here. Can the authors also provide the training curve for the p 1 case?<|endoftext|>The authors propose a new message passing design based on p Laplacian. They demonstrate that their method works better in heterophilic graphs. ## Pros:(+) The idea of designing new message passing based on p Laplacian is interesting. (+) The empirical evaluation of cSBM is great. ( ) Some concerns regarding the choice of hyperparameters (see detail below). I think the paper has great potential but requires a major revision. This is because the analysis does not take the correlation of graphs into account. I feel like the analysis and result here can be further improved for meaningful bound. However, according to their definition, it should depend on the input feature matrix. Please elaborate more on this point and improve the clarity of the text.<|endoftext|>This work proposes a discrete p Laplacian message passing scheme which is derived from a discrete regularization framework. The authors do a spectral analysis of their novel p Laplacian message passing scheme and show that it works as both a low pass and high pass filter, which is then applicable to both homophilic and heterophilic graphs. The empirical results support the theoretical justifications and outperform the baselines quite significantly especially on heterophilic and non informative topology bearing graphs. The paper is very well written and easy to follow. Can the authors please elaborate on this point in their rebuttal?
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; rating score: 6; The approach is based on set of conditional flows that take the image generated from previous stage as conditioning factor and generate the image with the higher resolution. It is also very beneficial for that work that the authors analyse also training time and complexity of the model during the experimental stage. Even for the third contribution I have the problem of understanding what components of the model are responsible for time and model capacity reduction. In my opinion the contribution of the paper is based on the different base flow model (CNF) and using unimodular transformation in eq.(7).This contribution is rather incremental with respect to the WaveletFlow. Using CNF seems to be a good direction but is not motivated taking into account that any conditional flow can be used instead. The selection of BPD criterion for quantitative evaluation is ok, but the goal of multi resolution generative model is to generate good quality images for larger dimensions. Therefore, my current recommendation is to reject this work.<|endoftext|>The authors note a few key differences with previous work (WaveletFlow)   using CNF instead of realnvp / glow based architectures, modeling noise at multi resolution, and utilizing a volume and range preserving formulation, Overall, my general take is that the paper is relatively weak on both conceptual novelty as well as application performance. Cons:Conceptual Novelty:Though the paper tried to differentiate from prior work (WaveletFlow) in various aspects (using CNF rather than RealNVP, utilizing a volume and range preserving formulation), the main idea of multi resolution conditional modeling of images at the next resolution based on the previous, is very much the same. Experiment Results:From the experiment results (Table 1), it s not clear that (1) the multiresolution formulation is more efficient, or (2) the generative modeling quality is improved, both of which I think are central to the claims of the paper. Therefore I cannot recommend acceptance at ICLR.<|endoftext|>The paper proposes a multi resolution variant of continuous normalizing flows for images. The key proposed benefit seems to be in number of parameters and training times. Comments/ questions:   The paper is written well and easy to understand. Perhaps more insight into why flows are worse on OOD from a multiscale perspective would be a good addition. Can we check if the problem is worse or better at coarser scales? Are constant images consistently assigned higher likelihoods at all image scales? I think these would be interesting and insightful additions to the paper. I think that the current draft of this work makes incremental contributions to the area of normalizing flows for image synthesis.<|endoftext|>If so, what were the exact architectural changes? The authors seem not to have modified (or even reproduced) WaveletFlow themselves, seeing as they report the exact same empirical results from the paper [1] and no mention of it in the provided code. Strengths: The authors provide a faster multi resolution strategy for normalizing flows. The paper is clearly written and easy to read. Weaknesses: Comparisons to some of the benchmarked methods are not fair/relevant (details below). Glow: Generative flow with invertible 1x1 convolutions. Ffjord: Free form continuous dynamics for scalable reversible generative models. I would be willing to update my recommendation based on the authors’ response to my clarification requests listed above. The ablations section does not seem relevant either (more on that below). This needs to be extremely clear.<|endoftext|>The authors introduce a new (invertible) transformation to go from a coarse image to a finer image which preserves the range of the image while having unit determinant (implying the transformation leaves the log likelihood unchanged). However, as described in the weakness sections I believe there are still some aspects of the model and experiment discussion that need to be clarified. However, I have a few concerns about the discussion of some of the results in the paper as well as questions around some aspects of the model. The paper is well written and the figures are nice. Experimental results on low resolution datasets are not very good.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 3; This paper investigates the problem of periodic client distribution shift in cross device FL, as previously investiged by Eichner et al (semi cyclic SGD). The authors propose to split the NN model in a shared part, with different heads for each component. A GMM model is added on top of the output of the shared part to allocate each client to a given head during training, with a temporal prior for the shift between classes. This GMM is fit via a federated EM scheme, in addition to the federation of the NN weights themselves. The results demonstrate the superiority of the proposed method with respect to baseline approaches, including the addition of the temporal prior. # Strengths  Well written and clear paper, with a good motivation (fig 1)  The proposed method is novel to the best of my knowledge, and provides an empirical solution to the problem initially tackled by Eichner et al.Although the authors focus on the case of 2 components in the experiments, the method is quite generic and could be applied in different settings  The experiments are run on different datasets, with multiple relevant ablation baselines, and the results demonstrate the impact of the proposed method as well as the resilience to errors in modelisation (e.g.cosine instead of linear)# Weaknesses  In the experiments, the baselines are not that natural insofar as they also have multiple heads. The temporal prior is not learnt but fixed. Only 3 seeds are used: in addition to the means and stds, it would be great to provide the individual points, or, alternatively, to use more seeds. Given the technical contributions as well as the well conducted experiments and its overall quality, I am in favour of accepting this paper.<|endoftext|>This paper handles the distributional shift observed in data in clients in a federated learning production setting. In particular, the distributional shift is modeled as a mixture of distributions.Furthermore, a multi branch network is used to encapsulate the shifting distribution and a Federated Expectation Maximization algorithm enhanced by Temporal priors of the shifting distribution (FedTEM) is proposed. Strengths:  The paper proposes a flexible, intuitive and practical algorithm for being able to model distributional shift and incorporate the temporal information through a mixture distribution. While the paper is centered around two different distributions, it can be easily extended to multiple distributions to finer granularities depending on the need. Cons:  It seems the authors make an implicit assumption that a client at any given time consists of data from either of the distributions at any given time. As with FL, one can t have any information about the distribution of data on one client. What other functions were considered? Finally, misspecification of the prior at each step can lead to the algorithm underperforming severely. However, there are some gaps in the paper which require additional clarification and experiments. Upon addition of the clarifications and experiments, the paper will be a strong paper to be accepted.<|endoftext|>The paper is to solve the distribution shift between daytime modes and nighttime modes in federated learning with a mixture of distributions. The proposed method could be viewed as a two group clustered federated learning method. The authors model the distribution shift with the prior daytime and nighttime modes in daily life. 3.The authors propose a novel federated expectation maximization algorithm (FedTEM) to learn a branched neural network model. Experiments on simulated environments show the proposed method achieves better performance when distribution shifts in the federation. The periodic distribution shift problem in FL is very interesting. However, the targeting problem in this paper is a shift between two fixed distributions in a periodic manner. From an application perspective, there are many straightforward solutions to solve the proposed problem in training two models for “Diurnal or Nocturnal” respectively. I am not convinced that the proposed solution is necessary to solve the targeting problem. 2.Could you please provide more details about the difference between a daytime distribution and nighttime distribution? For example, how the distribution changes with respect to p(y), p(x) or p(y|x) ? However, a similar strategy has been reported in the clustered FL framework as mentioned in A.4   Clustering and mixture model, and other related methods, such as the below papers. Please pick up some paper as baseline methods to compare with. However, the claim is not well supported by the contents.<|endoftext|>his paper proposes a federated learning method to address a specific non IID challenge over clients, i.e., when some clients are only available during the daytime and others are only available at the night, and these two types of clients  data are drawn from a mixture of two corresponding distributions. They study a multi branch model, similar to multi task learning, such that all clients share the same backbone network to extract data representations but a different branch is applied on top of the backbone network for different distributions. The proposed heuristics are not sufficiently justified whether they are compatible with the EM algorithm and the Gaussian mixture model. In experiments, they simulate the day night distribution shift on EMNIST, CIFAR, and Stackflow datasets in FL by splitting each dataset into two parts with different data types. They evaluate the proposed method under different shifting settings (different p) with a few baselines proposed in this paper. Detailed comments:(1) The contribution is incremental and lacks sufficient novelty. The idea of client clustering has also been studied by several recent works in FL. These works should be discussed and compared in the paper. Moreover, the performance seems to be sensitive to the application of these heuristics. However, none of them has been compared in the experiments. "min loss" is a baseline modified by this paper. Hence, it is hard to justify the advantage of the proposed method. The problem of FL with periodically shifting distributions is well motivated by practical needs. The idea of tackling the non IID problem in this case by assigning different clients to different branches also makes sense. Given these works, the EM algorithm in this paper is not novel.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; rating score: 5; The authors identify conspicuous patterns from adversarial examples (of the denoised classifier) with the aid of human judgment, and extract those patterns and modify them in the hope that they will match the original backdoor trigger patches. Overall, the paper is an empirical work with little justification for the proposed method. It is not clear when it works and when it fails.<|endoftext|>3.The proposed method achieved a higher or similar attack success rate compared with the original backdoor trigger. The authors showed that other possible backdoor triggers can be generated without access or knowledge about the training set and original trigger. For example, in Fig.6, the sequence of color is not aligned with adversarial examples’ color sequence. Therefore, backdoored networks are fundamentally working.<|endoftext|>The authors can also show experimentally that automating this procedure might not lead to good results. ### **Strengths**:— The idea of creating an attacker without access to the original trigger can create an alternative trigger is a novel threat model and can raise awareness in the community about the severity of the problem. It would also be interesting to see if adversarially trained classifiers (without the denoiser) also exhibit such patterns when poisoned. — Manual inspection is required for creating the alternative trigger.<|endoftext|>This question was not addressed. I think this weakness needs to be addressed. I am sufficiently convinced that there is novelty in the method and the results show that other techniques would fail in this scenario. If my understanding is accurate, perhaps the authors can make a stronger case for the importance of this method. Minor errors:(i) I would like more comparison of the extracted triggers to the original ones used when training the classifiers. The epsilon values are given without context.<|endoftext|>The authors observe that they can find highly effective backdoor triggers for poisoned classifiers which are not the ones that were originally introduced during poisoning. If the authors want to claim that their method is useful for determining whether or not a model is poisoned, they should compare to a variety of state of the art systems for this task.
Reject; rating score: 1; rating score: 3; rating score: 5; At present, the paper is not ready for publication and needs substantial work. The authors present preliminary experimental results showing that the proposed method has very slight gains on the VIST (Visual Storytelling) dataset. How is this process differentiable? Other portions of the paper are incomplete. First, the authors method of using "contextual sentence encoder" is as far as I can tell essentially identical to the Chen et al.2019 reference. The "graph encoding" portion is very light on details, but also appears to primarily rely on off the shelf methods methods (e.g.Faster R CNN, other cited works   e.g.Zellers, et al.).<|endoftext|>The complex designs here may be redundant. Based on unfair comparisons, the strength of the proposed model is quite limited. Considering both graph space and joint space is interesting but not novel enough and its effectiveness is not well supported due to the missing ablation studies. + The proposed method shows some strength compared with some existing methods.<|endoftext|>Strengths:  For the some part, the paper is well written and easy to read, especially the 1st 2 sections, I felt that the motivations are strong. The proposed system outperforms other method on the VIST testset. (3) It s not clear what is the goal of the retrieval loss in section 3.5 where VGG/ResNet is being used as image encoder. Weaknesses:I have the following troubles understanding the proposed system:  (1) The images retriever part is figure 1 is not clear (or was not explained at all).
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; rating score: 8; The paper presents Speech MLP, an architecture based on Mixer MLP but specific for speech signals. The Mixer MLP architecture is argued to be appropriate for the particular structure of speech. The performance is good, and that is in itself interesting. Although the evaluation on speech enhancement produces good objective results, the measures are hardly significant. I find the method part less persuasive, beginning with the claim that the contribution is threefold. Given the speech recognition is *the* main application of machine learning in speech processing, Speech MLP is not really a good name. The authors introduce their architecture simply by describing it. Further, where in the introductory material, the authors claim that their solution is based on domain knowledge, I do not see how the proposed architecture addresses this where Mixer MLP would not.<|endoftext|>This paper proposes an MLP based neural network, which is designed for speech processing. The novel Split & Glue layer is used to capture multi resolution speech characteristics. The method achieved state of the art performance in both command recognition and speech enhancement tasks. weaknesses  the effectiveness of the Split & Glue layer is similar to the convolution operation with different kernel/stride sizes. although the performance is strong, the task is rather simple and limited. the paper needs more surveysThis paper proposes a novel neural network architecture based on MLP.<|endoftext|>The authors have proposed a new general purpose MLP architecture for speech processing and learning speech representation for tasks such as keyword spotting and speech enhancement. The proposed architecture is similar to a convolutional neural network. It would be interesting to see how the  split and glue  output performs on the relevant tasks without any residual connection. The authors demonstrate the performance of the proposed mlp architecture on two problems, namely, keyword spotting, and speech enhancement. The speech enhancement experiment has been carried out on VoiceBank+Demand dataset and the authors show improvement by proposed technique across multiple metrics and baseline models. Strengths: The proposed model is relatively simple and easy to implement. Weaknesses: This model appears to be convolution in disguise. Additionally, the baseline methods in the speech enhancement task are mostly generative models which are perhaps not trained in a supervised setting.<|endoftext|>The paper proposes a new architecture for speech processing, dubbed "speech MLP". Speech MLP is tested on keyword spotting (with 2 datasets: Google Speech Commands and LibriWords) as well as speech enhancement (voicebank + demand). With these (and other more minor) concerns, it is hard to evaluate the novelty and impact of the paper, but these issues would be fixed with improved writing, literature review, and hyperparameter search / comparison. Weaknesses:  One main contribution of the paper, the "split and glue" layer, seems like a groupwise convolution layer. An unfold followed by a linear layer *is* a convolution. This significantly reduces the novelty and undermines the claim that the "architecture involves simple linear transformations only". Although Google Speech Commands is a well known dataset for KWS, LibriWords is not commonly used and as far as I can tell has only ever been evaluated with once in a single paper that proposed this dataset. In comparison to KWT2, which achieves similar / better results, it is much smaller.<|endoftext|>The paper proposes a simple architecture based on the multi layer perceptron for extracting information from speech signals. The architecture is based on a new layer called split and glue and can capture multi scale local temporal dependecies. The ablation study is useful but it would be more complete if additional parameters were investigated, e.g., number of blocks. It is also not clear how the number of chunks impacts performance in Table 3. Some typosAppendix B.3 SpecificaalyA.2.2 closing quotation marks are used as opening quotation marksOverall, this is an interesting study, the paper is clearly written and motivated and presents a novel architecture which leads to state of the art results for keyword spotting and speech enhancement.
Reject; rating score: 3; rating score: 3; rating score: 8; rating score: 8; The authors identify some missing information from the original paper and they provide some extra analyses. This paper presents a thorough re evaluation of the word mover s distance paper by Kusner et al.(2015), focusing on a number of points that are considered misleading. The paper has a point about the fact that the normalization of the word vectors used in WMD by Kusner et al.was not mentioned in the paper, even though it was possible to find out in the code released by the authors (misleading point 2). The paper then criticises Kusner et al.for not using such a nornalization for the document vectors obtained by BOW and TFIDF, which they find improves their results (misleading point 3). However, it should be noted that the original paper included other methods for comparison, which performed rather competitive to WMD, so while I agree with this paper that Kusner et al.didn t do justice to BoW and TFIDF, they did show that other methods were competitive to WMD but not as good. This paper also admits this too. Thus I don t think there is much to see here, unless the paper would like to be more of a case of how to make the most of token matching distance metrics. As for the misleading point 4, I have to disagree with this paper. On the whole I believe that much of the criticism of this paper is not justified. The main claims of the Kusner et al.paper still hold, and the experiments in this paper confirm them in my mind. On the whole, while I appreciate the work to bring to the community s attention some extra information and analyses of the Word Mover s Distance paper, the paper is not very informative.<|endoftext|>This paper shows that when standard baseline methods are carefully evaluated, such as using L1 normalization of the tf idf/bow vector, then the performance difference between the baselines and the WMD distance based methods shrinks considerably. The authors also claim that the findings in the paper are more general in the sense, that the distance between the embeddings of two words behaves more and more like a delta function, which is bimodal : close to one when the words are distinct, and close to 0 when the words are the same. This corrects some misconceptions that may have arisen in a particular sub field of NLP. I believe that this paper may be well suited to conferences devoted to the particular application area of document classification or NLP in general, but as it is the paper s methodological contribution, or technical contribution is too little. The paper presents a sub section on the "re evaluation of existing methods" in section 2 to answer some of this criticism, but all the papers in that section were much more wide ranging in their focus area. paper was not focused on the results of a single paper but on multiple papers, and the paper by (Arora et al.2017) presented a novel method. The paper by (Shen et al.2018) was accepted at ACL which is an NLP conference more focused on NLP tasks. The paper corrects some errors made in an earlier paper by (Kusner et al.2015) and presents a careful evaluation of baselines for document classification applications.<|endoftext|>This paper re evaluates WMD and identifies issues with the original paper. It shows that the gain from the original paper is not the product of WMD but the normalization. When the normalization is controlled, WMD performs similarly to baseline. Finally, it shows WMD resembles classic BOW when normalization is controlled. It offers detailed analysis and experiments to support its claim. # Weaknessesn/aThis paper revisits the original WMD paper and offers a detailed evaluation showing what contributes to the performance gain: normalization instead of WMD.<|endoftext|>This paper empirically shows that the performance of WMD is not as high as initially reported, and the real performance is comparable to  L1 normalized BOW, which can be formulated as a specific case of WMD. The authors also find that WMD resembles BOW in high dimensional spaces. The core claims in this paper include: 1. In the original study of WMD, many duplicate samples exist in the datasets, and applying L2 norm to word embeddings is not explicitly stated. 2.The superiority of WMD over BOW and TF IDF will weaken enormously by normalizing the BOW and TF IDF. 3.Both the normalization of word vectors and document level distance metric (L1 or L2) will impact performance. 4.WMD coincides with L1/L1 BOW empirically and theoretically, consistent with the two modalities characteristic of high dimensional word embeddings. By designing extensive experiments, the authors present the above observations and corresponding suggestions. Following are some of my questions. In figure 4, what metric is used for word level distance? Experiments show that L1 document level distance with L1 normalized BOW/TF IDF will generate the best performance. For example, WMD uses L2 word level distance, then what about using other metrics? Besides, what if we use L1 normalization for word embeddings in WMD? This paper only mentions the stop word strategy on page 7. Is there any explanation for this observation? I am more inclined to accept this paper.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; rating score: 8; This paper proposes using pretrained language models for planning actions. They achieve this by providing an example task sequence as a prompt to generate an action description for each step autoregressively. This paper shows promising results on extracting executable actions using pretrained language model. The results show that there is a trade off between executability and correctness. This paper shows that the pretrained language model contains knowledge that is useful for action planning. However, there are major concerns about how this relates and contributes to planning and grounding. When we do grounding or planning, the agent’s actions may change given the environment they are put in. While the available actions in VirtualHome might be different from other virtual environments (therefore, this paper proposed a translation step), the generated action plans don’t change if we change the environment from one home to another. So, the generated plans are not really grounded in the domain or environment the agent is planning for. The tasks are mainly around the daily home tasks like setting up a dinner table. However, procedure knowledge and the translation approach proposed by this paper cannot plan for the task like “stack two plates on the right of a cup” because the language model doesn’t ground its knowledge to the environment so the agent cannot tell the difference between putting down on the left or right of the cup. This is the main limitation to using the proposed approach as a planner. However, most of the tables in the evaluation only show one or another. If we compute the success rate for Translated Codex 12B, it is 15/88 0.17 and would be lower for other models. This reflects the performance of the proposed method better and the overall success rate is quite low. 4.Missing related work. Just to list a few below. The authors will need to discuss how the proposed approach is different from other knowledge mining approaches compared to prior work.<|endoftext|>This paper takes advantage of the pre trained large language models (LLMs) and assesses whether they can be helpful for embodied agents. I like the idea of prompting LLMs to decompose high level instructions and extract actionable knowledge, which is a novel perspective for embodied agents. However, the execution of the idea and the experiments are not right there to support the claim. It would be great if the authors can show its effectiveness on "actual" embodied agents in different environments instead of text games. Prompting LLMs for actionable knowledge extraction is new and interesting. Different LLMs including GPT 3 and Codex are used and evaluated here. The prompt and translation method is reasonable on the VirtualHome environment. It would be great if the authors can show its generalization to other environments such as Room2Room and ALFRED. Moreover, it seems there is no embodied agent to execute those instructions. All experiments are conducted on the text level only, but we all know that there is a huge gap between text instructions and actual actions taken by the agent. So it would be much more convincing if the authors can show an embodied agent can actually follow the generated instructions and complete tasks in the environment. There seems to be a tradeoff between executability and correctness for the current approach. Why?It is actually pretty easy to improve the executability by finding the most similar actions from the action set of the environment. Many technical details are missing in the paper, for example,     It is unclear how Executability is calculated. Do you sample multiple ones and then rely on human evaluation to select the best one?<|endoftext|>This paper explores the ability of pre trained language models to generate plans or action sequences from a text instruction. The in context learning ability of language models is used where the model is prompted with an example instruction and corresponding action sequence and the query instruction. Since text sequences generated by the language model may not be directly usable in the agent environment, the closest valid text actions are identified using a retrieval approach. Experiments compare performance of different language models on tasks from the VirtualHome benchmark. However, I feel ambivalent about the paper due to concerns about the evaluation (See detailed comments in discussion thread). * The pipelined approach considered here seems fairly limited* Limited technical novelty* Weak experimentsThe paper should provide more details about the data. I would encourage the authors to provide a summary of the tasks. The metrics considered in the paper are less ideal and do not provide a holistic view of model performance. This makes it very hard to interpret these results and I am not sure how meaningful the results are. Furthermore, it’s hard to say if the Translated variants in Table 1 are better than the plain language models. While these models are better in terms of executability and LCS, they are inferior in terms of correctness. How were the human annotators instructed to label model generated plans? Is it possible to define a straightforward metric that identifies whether a given task was successfully completed or not? The proposed approach should be compared against other baselines.<|endoftext|>This paper studies left to right language models for on the VirtualHome planning task. In this task, a model is given a high level goal like "get glass of milk" and it must generate a sequence of lower level steps to execute that goal ( walk to kitchen ,  open fridge , etc.). These steps correspond to the annotated scratch programs built by crowd workers as part of VirtualHome, so as such they are executable plans to achieve the given goal. This paper uses a left to right LM to generate those lower level steps, given one prompted example. LMs do better, but frequently deviate from the set of valid actions, so the authors introduce a Translation LM that measures the similarity between any "action" that the LM generates and the set of valid actions. Strengths:* The paper studies an idea that seems relatively novel to this reviewer   how well can models generate action plans in an embodied environment (not just code, or language)? * The paper presents a simple idea   using a translation LM to constrain the generated actions   that greatly improves performance, by ensuring that the actions generated by the LM are valid within the environment. * The paper has results that consider LMs of a variety of different sizes, from 1.5B to 175B, including both GPT 3 and codex families. * This paper presents analysis about the kinds of programs generated by LMs which I think might help future work build off of this direction. In the VirtualHome setup, the action space is composed of mid level actions (like "walk to home office") versus low level actions ("walk forward 1m" ,etc), so the promise of LMs for embodied understanding might not generalize to more difficult environents (like Alfred; Shridhar et al 2020a). That said, I think this limitation is discussed well in the limitations section (Sec7). * Though I appreciate the authors  work on making the evaluation robust, it is not clear to this reviewer that it is a good measure of correctness. The details of the human evaluation are a bit unclear, but it seems like humans are just given the list of actions rather than the world state after executing those actions. In this setup, it seems like there is a very strong bias towards language that  looks like  real action plans, rather than language that is truly actionable. The result is that models outperform humans at the task. To this reviewer, the paper could be improved if the correctness measured something grounded, rather than just something that looks grounded.<|endoftext|>This paper investigates whether large language models (LLMs) are capable, without additional training, of decomposing high level tasks into a sequence of instructions (i.e., a plan) and grounding them in an embodied environment. The authors also suggest using another language model to translate generated instructions into actions that are parsable by the embodied environment. Through a series of experiments, the authors empirically show that larger LLMs are able to produce sensible plans (according to human annotators) that can also be mapped to executable actions within an embodied environment. Part of the paper is also dedicated to answering several hypotheses related to the investigation. ## What I like about this paper  Rather than trying to solve a particular task using large language models, this paper focuses on analyzing what is the capability of pre trained language models in terms of decomposing high level tasks into a sequence of instructions. In other words, what kind of actionable knowledge, if any, is stored in those large language models? Using two axes of evaluation   executability, and correctness   better characterize the strengths and limitations of the language models studied in this paper. Looking at the discrepancy between the correctness, as reported by the human annotators, and the executability of a plan motivates the need for a translation LM. It can overcome some of the restrictions interactive environments have, i.e.their action space contains instructions with a specific syntax. I like the suggestion of using another language to translate generated instructions to grounded actions to boost executability. Also, it makes sense to me to use an autoregressive trajectory correction approach to improve the executability of a plan. Is it that other large language models (e.g., T5) are less amenable to prompt engineering? To me, the main message/claims of the paper seem to be about large language models in general. It would be interesting to know if actionable knowledge is only present in GPT * models or not. p.7: "... by [our  > the] human annotators"   Suggested change. The proposed paper is exactly about doing such an investigation.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; The aim of this work is to integrate the Stein variational inference methods in the existing probabilistic programming language NumPyro. The implemented methods include variants of Stein variational gradient descent algorithms with a range of kernel functions, non linear scaling of update terms, and matrix valued kernels. Empirical results with existing baselines for real world problems are provided. [Strength]The paper is overall well written and the method is clearly explained. The literature review is thorough. The originality is low since all the Stein variational inference algorithms implemented in NumPyro in this work are proposed by prior work and the authors do not propose any new algorithm.<|endoftext|>The paper presents the implementation of a framework to work with different "Stein VI" methods in NumPyro. A quick introduction to the different algorithms is given as well as an evaluation of a series of examples. Although I am fully supportive of having source code presented in academia, I do not think this paper brings anything of particular interest, in particular for this venue. Although the introduction to the different Stein VI methods is nicely written and very understandable, this paper does not aim to be a review which is already done in Anastasiou 21  as the authors point out. What the paper could bring, would be novel techniques or implementations to improve speed, but unfortunately only using `vmap` to solve things seems a bit light.<|endoftext|>This paper proposed a new probabilistic programming framework for stein variational gradient descent and its variants. Moreover, using this framework, the authors developed a new Stein mixture algorithm for deep Markov models, which shows better performance than existing methods. # Strength  Integration of SteinVI into numpyro seems very useful. Users can easily take advantage of the state of the art SteinVI algorithms for their own Bayesian modelings. The concept of ELBO within Stein is innovative and seems interesting to be explored. Extending the stein mixture method to deep Markov models is a novel application. No experiments to support the usefulness of EinSteinVI for Non linear Stein VI, Matrix valued kernel stein VI, and message passing stein VI.<|endoftext|>  This paper describes an implementation of EinsteinVI, an improvement over Stein VI, a method that can be used to apply VI in prob. The paper describes how this method has been implemented in NumPyro, a software for prob. programming. The method is evaluated and compare to other alternative prob. It is not clear what is the difference between EinSteinVI and the Stein VI methods included in Edward and PyMC3. Th experiments are extensive, considering different prob. models and problems. However, it does not compare results with Edward. The novelty of the paper is low since it does not describe any new method. It only consists in an implementation of an already known method to be used within NumPyro for prob. My main concern with the paper hence the lack of novelty, although some practitioners may consider the implementation interesting and useful.
Reject; rating score: 1; rating score: 3; rating score: 3; rating score: 3; I believe the purpose of this paper is to try to understanding programs by analyzing assembly code using graph neural networks. The authors test their system against two program level tasks and claim an accuracy of 82.58% and 83.25%. Sadly, I m not convinced this approach is the right one for "general intelligence" on program analysis. I really like the area of research the authors are exploring, but their approach is not something I am convinced has novel contributions nor am I convinced that the experimental results are compelling. TL;DR: Great area of research, but I found little to no novelty in the paper. Moreover, as a scientist, I believe this approach is likely the opposite of what we should actually be doing for program reasoning.<|endoftext|>This paper presents a graph neural network based approach to solve two binary analysis tasks (program classification and binary similaritydetection). The key idea of the paper is to merge different forms of representation of binary code (compiler IR, assembly code, etc.). Strengths The paper presents an interesting idea. No comparison across different  hardware architectures   Scalabailtiy of the technique for large real world programs is not clear Overall, even though the idea is interesting, the evaluation section is very weak in the current version of the paper. The ideas are interesting but the evaluation is subpar.<|endoftext|>This work proposes a new approach to solving semantics based problems and compilation related problems in program analysis by leveraging graph neural networks from the level of assembly code. ### NoveltyThe idea of fusing feature dimensions in machine learning for code is not new. See https://arxiv.org/ftp/arxiv/papers/2109/2109.03341.pdf on composite program graphs, or https://arxiv.org/abs/1907.02136 on different types of program features. ### TerminologyAuthors need to be more precise about the terms they use. I know this is a learning conference, still, it is imprecise to call program classification, duplicate code detection or the other kinds of applications program analysis tasks. Program analysis in PL refers to data flow analysis, pointer analysis, etc. (4) In the program classification task, the author proves that he can better solve semantics based problems by comparing with Ins2vec. (8) Clone detection is usually performed at a function or a more detailed level, and what is the practical significance of detecting the binary similarity of the entire program. What is the difficulty of learning semantic information from assembly code compared to source code? Overall, I think the paper does not pass the novelty test.<|endoftext|>The goal is to design a generalized model that can solve both source code level tasks (e.g., program classification) and compilation related task (e.g., vulnerability analysis). Program embedding based on the assembly code allows the model to learn compilation specific features and the use of multiple graphs reflect semantic and structural information. The contribution of the proposed modeling approach seems to be the addition of CG and DFG for program level analysis, which could be considered as very incremental. What are the factors contributing to this change in performance? Learning to represent programs with graphs. ICLR, 2018. A very relevant work that addresses similar challenges and applies similar approach is not discussed.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 5; rating score: 8; First, I am a bit surprised that this is even sufficient. The paper is well structured, despite some issues with clarity. The writing style makes it understandable. The experiments are sensible albeit simplistic. If the steerability were to be introduced into the geometric neurons, what benefit do we get? The same also holds for limitations of the approach. In fact, at the moment the paper lacks any sort of a comparison to the state of the art except the  ancestor  baseline, which is not rotation invariant anyway. In general, I believe that the writing style of this paper is a bit convoluted. Could section 4.1.2 or any other preceding section explain why spherical filter banks are necessary and how they would be used? or if this is to be relegated to experimentation, what follows after Eq.13 could say so. Can the paper speak more about why a hypersphere can be seen as a generalization of a hyperplane in the context of the paper?<|endoftext|>This paper introduces steerable 3D neurons which are equivariant to 3D rotations for point cloud classification. Two main strengths of this paper are as follows:1, The method proposed in the paper is very novel. 3.The contribution of the paper is list very clear. I agree with other reviewer that experiments of comparison with other equivariant model on more challenging and mearningful tasks should be provided. In addition, from my perspective, steerable constraint shoud be followed by a method to solve it, while I can not find it in the paper(please correct me if I miss something).<|endoftext|>Authors propose 3D "spherical neurons" leading to rotationally equivariant layers. The authors then solve for the steerability constraint for this neuron and empirically show that the proposed approach overperforms Melnyk et al.(2021) on rotated 3D data. Is there one parametrised spherical neuron for each point? For instance, the Background section feels a bit like a sequence of definitions with no obvious direction or motivation for the reader. Also, I would argue that framing the steerability condition within the framework given by representation theory & equivariance could help the reader understanding the motivation and the narrative of the submission better. Equations 1 and 2:      Is Eq 1 necessary / helping the reader as this paper is focused on the 3D setting? Why not framing this in a representation theory setting?<|endoftext|>The paper aims to derive a steerability constraint for spherical neurons (3D point classifiers with spherical decision boundary). When input rotation perturbations are unknown, the authors propose a method to recover the unknown rotations and therefore make rotation invariant predictions. The experiments on a few small scale datasets verifies some of the claims. Some technical terms and notations could be explained better (see minor comments). **Weakness**  The motivation of designing rotation equivariant (or steerable) networks is only briefly mentioned in introduction, and seems rather weak. From reading, one important motivation for steerability is to make rotation invariant predictions for point cloud classification tasks. However, it is intuitively unclear how to achieve it by steering the pre trained filters. Given the dense notations, adding a table of notations would help readers keep track. Such as in Fig.1, it s not immediately clear what S is.<|endoftext|>This construction is at the heart of the geometric neuron (illustrated in Fig.1).The approach taken in the present paper is to build a fully steerable version of this geometric neuron. [Note: I did not check all the math, but to me the reasoning and reliance on past work seems sound.] Weaknesses: The present experiments demonstrate a type of proof of concept. The actual potential of steerable spherical neurons could be better shown in future work, particularly given the relevance of the contribution to ICLR in terms of new ideas for "representation" learning. Whereas this might by some be considered by some to be a niche contribution, and others might argue that the experimental results could be strengthened, I for one appreciate the geometric reasoning and the relevance to ICLR themes. However, I consider these issues to be somewhat minor. This is not primarily pitches as a benchmark oriented or empirical results derived paper.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; rating score: 6; This paper studies the problem of unsupervised object centric learning in the context of point clouds. Specifically, the goal is to learn a generative model of 3D objects from a collection of untextured point clouds of multi object scenes, and this is achieved via a VAE based generative framework. The resulting model seems to work well on simple scenes with small isolated objects on a plane. Overall, I think the problem is interesting, and the proposed framework is technical sound, but the results presented on the simplistic datasets cannot sufficiently verify the effectiveness of the method.<|endoftext|>This paper introduces a framework, SPAIR3D, to decompose a 3D point cloud into several objects (point cloud segmentation). The framework is unsupervised, driven by a Chamfer Mixture Loss. Firstly the problem setting is a little confusing, since the framework is unsupervised, I thought that the training data would be captured point clouds or those from other various datasets, since GT is not needed. However, the authors still make a lot of efforts to render UOR and UOT. If so, the GT is easy to get, why do we need an unsupervised framework for these synthetic datasets? On the other hand, the synthesized datasets look relatively simple and objects are placed sparsely and at similar sizes. I would like to see results on more challenging data, and I would like to hear the reason why the training is not performed on real point clouds or existing datasets. 2.In a real scene, objects are often stacked. Would SPAIR3D work and how is it work?<|endoftext|>This paper proposes a VAE based unsupervised generative model, to achieve 3D object centric learning as well as 3D scene decomposition. 2.There is no ablation study to validate the effectiveness of the proposed components and the loss. The idea to decompose a scene in an unsupervised way is novel. However, the proposed method is mainly based on SPAIR and the only technical contribution seems to be Chamfer Mixture loss.<|endoftext|>Their variational training pipeline utilizes a VAE based generative model that jointly considers the global 3D background and local individual objects, and a novel chamfer mixture loss is proposed for irregular 3D point clouds. The method is validated on two synthetic datasets, and the performance is demonstrated to be close to a supervised SOTA method. Page 2.Section 3, 2nd paragraph should be ‘However, point cloud data is ...’This paper is a valuable pioneering work on unsupervised 3D scene segmentation, with a full variational training pipeline built up for irregular 3D point cloud data. One quick related question, is it possible for the reconstruction to come in the form of regular voxels? A simple way to verify that is trying to tilt the point cloud data, so the desk plane would not be flat. 3.Following the previous question, is it possible for the proposed method to test on S3DIS dataset as well?<|endoftext|>The paper tackles unsupervised 3D scene decomposition/object discovery from point clouds. It proposes SPAIR3D, inspired by the 2D counterpart SPAIR, to factorize a 3D point cloud into a spatial object centric mixture model. It introduces two customized point cloud datasets, and compares with a supervised baseline PointGroup. # Strengths  The overall writing is clear. How important is VAE in the object centric representation learning framework? It might be a general question (also for 2D counterparts). However, the baselines are not carefully chosen, and the scalability of the proposed SPAIR3D is questionable.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; 2.As for the results, I would consider the voxel wise statistical comparisons on the real data a rather definitive result demonstrating that Surreal GAN was able to isolate two major sources/locations of atrophy in Alzheimer’s disease. As many as six regularization terms, while I stated as a “strong point”, can also been viewed as a sign of over engineering. Overall, I would recommend this paper to be accepted. Personally I find the problem the authors are tackling legitimate and usually neglected, and I am persuaded that the proposed solution has some generalizability.<|endoftext|>Surreal GAN aims to create fake pathological data with a latent variable and a healthy input and includes an inverse function that predicts the latent variable from a fake/real pathological data. It also shows that the best number of patterns for the model to predict is two, and it indeed has positive correlation to the severity of the disease. The model proposed provides an interesting idea of using GAN to create an inverse function to detect the pattern and severity of disease from real data. The reasoning behind the regularizations is overall reasonable. Also, since the regularizations are the major extensions of the model from Smile GAN, it would be more comprehensible for the reader if the paper had cases where some parameters were set to 0 (where some regularizations were not used). Overall, the paper is convincing and has reasonable suggestions in creating a correlated continuous latent space for pathological data. However, there could be more comparisons important in proving their case.<|endoftext|>In this paper, the authors present a method for learning a representation of disease related image patterns from regional volume information generated from structural MRI images. Modelling a disease as a continuous process2. Inverse Mapping to ensure that disease patterns synthesized are capturedThe evaluation of the method is performed on semi synthetic data sets and an actual Alzheimer’s disease data set. The results show that the method can identify two clinically informative patterns. However, the selection method of the hyperparameter for the loss function is not given.<|endoftext|>This paper proposes a Surreal GAN method for learning representations of underlying disease related imaging patterns. Strengths:(1) The proposed method models disease as a continuous process and learning infinite transformation directions from CN to PT. (5) The authors state that there are four novelties in this study, however, some of them only introduce the existing technologies into the proposed framework. Weaknesses:(1) The overall objective function includes several hyper parameters, and the authors set fixed values for some parameters.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 3; The paper proposes a domain adaptation method that is guided by the reverse Kullback Leibler (KL) divergence between the target and source domains in the representation space. The authors derive a bound on the test loss based on the training loss and a quantity based on the divergence of the joint distribution (y,z) between the target and source domain (Proposition 1). The proposed method and optimization procedure are evaluated on multiple data sets and compared with multiple baselines. The problem is well motivated, and the derived bound and optimization procedure seem to be reasonable. Why don t you show the different combinations?<|endoftext|>this paper proposed a method for domain adaption. Especially, KL divergence on representation alignment is used. Strength:1. the idea is simple and the paper is easy to follow. But in training, we sample a small mini batch for training, and this term could be changed and unstable. Why not perform experiments on Office 31 and Office Home datasets? Also, the results in this paper on VisDA 18 are not consistent with existing work such as [Zhu 20]. In addition, [Zhu 20] is based on marginal MMD and you need to compare it. 3. the novelty. The paper is easy to follow, and the idea is easy and effective.<|endoftext|>This paper proposes a generalization bound and a reverse KL term to match the marginal distribution. This paper proposes an reverse KL which can be optimized together with the source classification loss directly. 2.This paper points out a deficiency of Ben David s bound on domain adaption: it fails to consider the problistic labeling mechanism. Cons:1.My major concern is that : the implementation is inconsistent with the derived generalization bound. It would be better if authors could present baseline method (e.g., $\lambda_{aux} 0$) results and show the reverse KL is effective. This paper proposes an reverse KL term to match the marginal distributions in domain adaptation. The results are promising.<|endoftext|> In this paper, the authors propose a generalization bound for domain adaptation where discrepancy between distributions (i.e.marginals and class conditionals) is estimated using the reverse KL divergence. Inspired by this result, the authors then proposed a learning algorithm and show  that this outperforms some existing methods. **  The idea of a probabilitisc interpretation of DA is interesting. **Weakness:**  **Major Concern #1** Novelty of the theoretical result. **Major Concern #2** Significance of the derived bound. One of the major motivation for the H divergence introduced in [1] vs L1 and then several authors using hypot based divergences such as [2,3,4]. **Major Concern #3** Comparison of the algorithm with modern baselines. Why there is no comparison to recent work in domain adaptation? In my opinion, they look more as a way to motivate an algorithm which could be done following the results from either [1,2,3,4] rather than a new generalization bound as claimed.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; rating score: 6; This paper proposes to implicitly model label correlations in multi label text classification. The authors conduct experiments on two benchmark datasets, AAPD and RCV1. Experimental results show that their proposed method outperforms several baselines for multi label text classification. Technically, the idea of appending multiple "latent tokens" to the beginning of each document is not new. Note that in both papers, this is just a small trick instead of their key novelty, so there may be even earlier work already proposing this trick. The key idea has been proposed in previous studies, so the novelty is rather limited.<|endoftext|>This paper addresses the task of multi label text classification by modeling the label correlations implicitly. Different from the previous works that explicitly model the label correlations, such as the label embedding methods, this paper proposes modeling the label correlations via latent labels. The proposed method outperforms the baselines on two multi label text classification benchmarks in the reported experimental results. Recent interest for multi label classification lies in how to model the label correlations. The reported experimental results show that the proposed LLEM outperforms the other baselines. Since the [CLS] token is intended to represent the sentence, pretraining the latent label embeddings may not capture the label correlations and lead to performance loss.<|endoftext|>The paper presents a method that uses latent label representations to model label correlations implicitly, for the multi label text classification (MLTC) task. The method concatenates a set of randomly generated latent labels to input text tokens. Then the method uses this as the input to the BERT model. ## Weaknesses:* The paper s overall contribution and impact seem quite limited. * Some questions on the algorithm itself:   * Label semantics is not utilized in the model design since the embeddings of the latent labels are randomly initialized. But how to choose K?<|endoftext|>This paper consider multi label text classification problem and propose a cross attention Transformer encoder to model the correlation between latent labels and input text sequence. ## Strengths:  encouraging experiment results that show marginal gains over baselines on two small scale XMTC datasets## Weakness  lack of complexity analysys. How scalable is the proposed method? Can it be used in extreme multi label classificiation problems where number of labels is million or more? The rationale of latent labels are not convincing. Does it serve as a semantic cluster that consists of multiple labels? This paper propose a cross attention Transfomrer architecture for the multi label text classification problem. Time complexity with respect to number of labels is also missing.<|endoftext|>This paper proposes a novel multi label text classification method named LLEM that jointly encodes a document and latent labels (with smaller number than actual labels), and tries to better model label correlations implicitly and impose less a priori limits compared with previous state of the art works. The method is conceptually simple but outperforms the state of the art results on two widely used benchmarks. The paper was well written and organized, and there is an extensive survey on recent work of multi label text classification. As mentioned by the authors, some further work are still required to "investigate how the latent label correlate with each other, with the actual labels and the context, and how can they be interpreted".
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 5; The measurement method depends on two key factors: Intrinsic Dimension (ID) and CLuster Learnability (CL). Inspired by the above observation, this paper proposes a modified DeepCluster algorithm to increase the final performance. Strengths:   Pearson correlation coefficient between the proposed factors (ID and CL) and final accuracy is surprisingly high. However, in the top left of Fig 1, DINO has both better ID and CL than resnet50, while supervised learned resnet50 should outperform DINO. Therefore, the motivation for seeking higher ID is not well supported. Furthermore, Pearson correlation coefficient as the only metric is not enough to conclude that ID and CL are better than other existing predictors. This paper is novel and the results are interesting.<|endoftext|>The authors collect 30 checkpoints of recent self supervised methods to validate the proposed metrics on two datasets, ImageNet and STL_10, and the results show a high correlation between the proposed metrics and the downstream classification performance. However, the complexity of the proposed metrics is not discussed in this paper. 1.Section 3.2 indicates that the estimator is exact for uniformly distributed data, and datasets applied in experiments are balanced. Is the proposed metrics only work for the balanced dataset?<|endoftext|>Finally, the authors argue that CL can also be incorporated into DeepCluster (an unsupervised method), as an auxiliary loss to further improve the prediction performance. For example, i) I don’t understand how language compositionality has anything related to the motivation of this work. The results in the original paper indicate that lower alignment and lower uniform would have a better prediction performance. But in the Fig 1., it seems to indicate otherwise, which is not further justified. However, the proposed frameworks are heavily identical to the existing works.<|endoftext|>(2) The experimental results demonstrate the effectiveness of the proposed metric CL and ID. Although the experimental results show some advantages, it is still hard to understand why should we use these two metrics. Hence, I think the comparison method is not a strong baseline. The modifications made on DeepCluster seems a little far fetched with proposed objective and the results are not convincing enough, especially on ImageNet. Although mentioned in the conclusion, I think the authors should make more efforts along this direction to make this work more complete. This paper is generally well written and the motivation is clear.
Reject; rating score: 1; rating score: 3; rating score: 5; rating score: 5; The paper proposed 6 rules for robust continual learning evaluation and conducted some experiments. Personally, I do not see the value of this with the current state of the art. I agree that some mixing of tasks will be useful in evaluation, but that is similar to a replay based method. Without the task identifier provided, how to classify it? It doesn t contain much more than what are already known to the research community. This reviewer also doesn t agree with some points.<|endoftext|>This work proposes 6 rules for continual learning experiments. this is not a new finding, it s well reported in the literature. Actually, lots of papers report both performance as class incremental vs task incremental learning. The work considers task transition to be model by a Markov chain and uses $\alpha$ parameter to decide how likely the next data will be drawn from the current task. I don t think it s top conference material in its current form, however.<|endoftext|>However, the flow of the experiments is not well justified and only limited hindsight analysis is provided (e.g.P7: In the unrestricted setting for splitMNIST, most regularisation results are around 20% which is a similar founding as Farqhuar Farquhar & Gal (2018) as their single head setting) where the previous findings seem to be aligned with the experimental results reported in the paper. Limited hindsight analysis of the existing work: Section 4 provides experimental results for a few scenarios including unrestricted and reappearing classes. The paper addresses an important problem in continual learning and the introduced rules are clear and intuitive. It would have been more useful to report cases of misalignment between the experimental results reported in the paper and the existing literature to highlight the importance of the experimental settings on the  previous bodies of works  conclusions.<|endoftext|>But a lot of the ground it covers has been covered in other papers. Although the present paper offers some further thoughts and some useful new benchmark figures, I don’t feel this is enough novelty to warrant an ICLR paper. However, papers of this sort are, in my view, not appropriate for a research focused conference such as ICLR, as they are purely methodological and don’t offer new techniques or results or analysis. It might be better suited as part of a longer journal paper whose main contribution is a new continual learning method.
Reject; rating score: 3; rating score: 3; rating score: 6; This paper considers the problem of data imputation, which is important in handling tabular data, and shows interesting results regarding imputation methods, with some potential practical implications regarding the efficient methods to use. They also do not provide reference results for any of the other classical algorithms that appeared in the original paper (such as MICE and MissForest). The paper uses a small number of datasets, and all the datasets used have a small number of features.<|endoftext|>The paper compares two generative deep learning based data imputation methods (GAIN and MisGAIN) with KNN based imputation. **List strong and weak points of the paper. The authors claim that they are the first to report these results, which is not exactly true. * Add information about the data split or cross validation used to find hyperparameters would increase the credibility.<|endoftext|>The results, overall, show that the KNN, despite its simplicity, provides very competitive results when compared to its more complex and computationally expensive alternatives. This paper has also an important message for the new method developers within the community suggesting to compare first their new methods with simple standard baselines. I wonder why the authors did not mention this study. This study presents a nice guideline for imputing missing values in tabular data. The experimental section can be enriched with a more in depth evaluation of the effect of hyperparameters on the performance of data imputation approaches. This is a nice study with important messages for the community. What about the effect of hyperparameters when imputing real data?
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; The goal of this paper is to train a Bayesian GAN on non i.i.d.federated data. Specifically, the authors propose to adopt the newly introduced expectation propagation (EP) prior, being partition invariant, to address the non i.i.d.federated challenge. The writing should be improved significantly; the current manuscript is quite challenging to understand what s really going on. What s the novelty over existing methods for training GAN under federated learning settings? e.g., [1 2]In the paragraph before Eq 1, $q(\boldsymbol \theta)$ is not a distribution, i.e., it s not normalized, right? How expensive is the proposed method, both in space and time? The notations starting from Eq 5 are quite confusing. In the paragraph following Eq 14, I cannot see clearly why ``Theorem 4.1 shows that we are able to analytically approximate the prior of the global data distribution with the datasets stored on different clients while following the cross silo federated learning settings?<|endoftext|>Specifically, the work is built on top of Bayesian GANs. In order to aggregate the information from different clients, the authors proposed to use expectation propagation (EP). The authors presented a low complexity solution. In order to apply EP, it requires to have Bayesian GAN as the basis model at each client. For instance, the oracle baseline model still has FID on CIFAR10 above 25 while the best performing GAN on CIFAR10 is below 6. For EP ProbGAN, how does the newly introduced EP prior affect the guarantee claimed by ProbGAN. In the experiment part, despite being introduced in the text, the performance of the baseline model BayesGAN (2) was actually not reported in Table 1. However, the Table 1 did not consider the top performing case. Furthermore, EP GAN variants on i.i.d.N 2 outperform oracle. Within the Bayesian framework, the use of EP for federated learning is reasonable. The authors proposed some low complexity solution and empirically showed the benefits of using EP over existing schemes, which however are not really developed for non i.i.d.scenarios. Furthermore, the baseline models do not take the top performance configuration, which lead to my general concern on how strong the baselines are. Therefore, having or adding an EP prior in Bayesian models seems to be straightforward. The closed form update is definitely interesting, but the authors shall (empirically) analyse its fidelity.<|endoftext|>This paper aims to construct a GAN that can be applied to non i.i.d.federated data. To achieve this aim, the authors propose an extension of Bayesian GAN called expectation propagation prior GAN (EP GAN), which obtains a partition invariant prior using expectation propagation. The effectiveness of the proposed method is demonstrated using non i.i.d data, including the toy data, image data, and speech data. However, its validity is not empirically demonstrated. In particular, I am curious whether the calculation in EP is dominant in the total framework, including the GAN training. 3.In practice, it is assumed that the number of clients is considerably large. However, in the experiments, the number of clients is relatively small (the order of 10). Therefore, in the current manuscript, I consider that the effectiveness in a practical setting is not sufficiently demonstrated, although some benchmark performance is provided. This paper addresses an interesting problem and proposes a reliable method that is mathematically grounded.<|endoftext|>This paper proposes a method for learning Generated Adversarial Networks (GANs) for non i.i.d data in a federated learning setting. This is accomplished with the use of a partition aware prior via an Expectation Propagation (EP) algorithm embedded into a Bayesian GAN setting. The claims are substantiated with experiments on both synthetic and real data. 1.While Figure 1 is a good way to motivate the problem, it would be good to supplement all figures with some quantifiable metrics (at least for a few ones). 2.In the Related work section, the two large paragraphs on Federated Learning and Bayesian GANs seem to be disconnected. What bearing does this have on the entire method? 9.The other important question I have is   It is not clear from the paper why should the proposed method aid in handing i.i.d data in the federated learning setting.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; The proposed architecture breaks the limitation of sequentially encoding the nodes of a DAG, and could parallelly encode the nodes of each graph so that the learning and inference speed could be much improved. Also, better downstream tasks should be added for validating the proposed method: (1) it’s better to combine the proposed method with advanced NAS baselines, the BO based method is well recognized in the AutoML domain but not for the latest NAS community. 2.Dataset with larger graphs (graph with more nodes) is needed to demonstrate the performance. Based on the above concern described above, I currently tend to weakly reject the paper but would like to hear more feedback from the author and discussion from other reviewers towards the final decision.<|endoftext|>While some of the above have only recently been published, they have been available on the arXiv for a significant time prior to the ICLR deadline. However the concerns I presented, especially the fact the evaluation does not completely match DAGNN s and the relevant graph Transformer literature is not compared against, means the paper could benefit from another iteration of improvements. The authors position most of their evaluation setup based on DAGNN s, and they focus primarily on evaluating it in the self supervised setting (e.g.with a VAE pipeline). However, DAGNN also evaluates their work on the ogbg code benchmark, which seems to be missing from this paper.<|endoftext|>The paper misses a significant body of related work on directed graphs and graph adaptations of Transformers, which would be necessary for a fair evaluation of their approach. (If some of these methods are added, I would be willing to raise my score.) Many of my questions about missing details have been answered in the discussion, although it is difficult to evaluate this fully without a revised version of the paper. I m also not sure if there is a fine tuning step for any of the experiments or not. Is this possible for all of the baselines?<|endoftext|>### StrengthsThe objective of the paper is clearly motivated and a need for a parallelizable architecture is apparent. ### Weaknesses  The explanation of the used GNN in Section 3.1 is not fully stand alone and misses out details. Was this done on number of parameters, computation time, etc.? __Post rebuttal update__: I appreciate the authors  rebuttal, and most of my questions have been sufficiently answered. In light of the fair concerns of other reviewers and the missing update of the main text, I will stay with my initial score, which was already weakly positive. The proposed method shows favorable benefits compared to previous work.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; The present paper proposes an importance weighting approach to address the issue of regression under conditional moment restrictions in the context of non parametric instrumental variables. The experiments do not support the authors  claim that their method outperforms other competitors. _Causal inference in statistics, social, and biomedical sciences_.<|endoftext|>The paper seems to provide a novel, interesting and applicable method that improves the accuracy of an important causal inference tool (instrumental variables). Improving non parametric instrumental variables methods is an important problem in causal effect estimation.<|endoftext|>The authors propose a novel method for conducting instrumental variable regression by learning the ratio of conditional and unconditional densities, and using this estimate when learning the causal relationship between cause and effect variables. The paper presents a method that is novel in terms of the estimand it proposes, the density ratio, in order to learn the structural function.
Reject; rating score: 5; rating score: 5; rating score: 6; However, my main concern is that the paper has no technical novelty and it also does not bring sufficiently new insights to the community. This paper presents an empirical study on the effect of face obfuscation in the ImageNet dataset. This paper would be more interesting (from a technical point of view) if the authors could additionally investigate into methods for alleviating such impact. Strengths:* The main contribution of this work is that it provided empirical evidence on the effect of face obfuscation on the ImageNet dataset.<|endoftext|>The authors propose a two step face filtering method. The accuracy of the two approaches was reduced by 0.9% on average compared to the original database on the ILSVRC classification challenge. Perhaps it would be better to include a brief description of Amazon Rekognition in the text. 3.The novelty of this paper is limited reference to the proposed method in the paper. There are many similar discussions, for example, there are some papers proposing to remove images associated with people from the database. This paper has some contributions in exploring the ethicality of datasets, especially in the current very popular ImageNet database, but it exists some flaws (see weakness). The solutions and results in this paper are open sources and feasible, and this work will inspire subsequent exploration of privacy protection in publicly available datasets.<|endoftext|>In addition, the authors conduct very thorough experiments across different tasks and different architecture for the study of the performance influence with the obfuscated dataset. However, the weakness is that the main part of the paper is to examine the performance influence of different settings and is of limited technical novelty. For the verification of some downstream tasks, the coverage of tasks is not enough. Although the paper mainly focuses on providing plenty of empirical results to evaluate the influence of using the face obfuscated ImageNet dataset and is of limited novelty, it provides a lot of insights to show the effectiveness and feasibility of privacy preserving ImageNet. I have few concerns of the selection of transferring tasks.
Accept (Poster); rating score: 8; rating score: 8; rating score: 5; This paper identifies that adversarial examples in the low density region of the groud truth distribution have much stronger transferability. The observation on the relationship between low density data and the high transferability of both random noise and adversarial noise is interesting, likely to inspire future works. ## WeaknessesMy major concern on this paper is on the practical significance of the proposed AAI metric and IAA algorithm. Besides, there are some other issues in this paper3. ## After rebuttalAfter discussing with the author I have a better understanding on the significance of this work. Although further experiments show it may not be the best objective for choosing the structural hyperparameters, I think it serves as a good theortical strating point for futher analysis of adversarial transferability.<|endoftext|>The proposed method is based on the observation that low density region of the training data is not well trained. To utilize this, the authors try to align the adversarial direction with the direction to decrease the ground truth density. I believe this could be a useful attack method for generating transferable adversarial examples, and providing a strong counterpart for future research on adversarial defense. Strengths:  The proposed method is well supported by theoretical analysis. The results is good. I feel there is a disconnection between the motivation and the actual proposed method. However, I did not see many connection between this observation and the proposed method. The only transition is that "The most efficient direction towards the low density region is...". Although there are some weaknesses of the paper, based on their experiments, I still believe the proposed method could be useful for generating strong adversarial examples and open the door for future research on defense. Post rebuttal:I appreciate the author s comments on my questions.<|endoftext|>This paper proposes Intrinsic Adversarial Attack (IAA), a transfer attack method based by jointly matching data distribution. The key assumption of this paper is that the DNN might not be well trained on low density regions (LDD). Therefore, taking data distribution into consideration during attack could potentially improve the transferability. Strength:  Important topic  Somehow intuitive and interesting idea  Sound and feasible solution  Promising resultsWeakness:  The assumption is intuitive but limited evidence is provided. Overall, this paper is well written and works on an important problem. The assumption is simple and intuitive, the proposed method is also feasible and sound. Even though, there are still quite a few concerns. I am on the borderline of this paper and could be easily flipped to the other side based on the authors  response.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 8; In this paper, the authors propose a closed form approximation for the Gaussian decoder distribution in VAE. The closed form is achieved by matching the mean and covariance matrix with the Taylor expansion of the nonlinear mapping. The closed form approximation reduces the computation cost for computing the ELBO of exact Gaussian VAE. Weakness.1.The proposed method is similar to the well known moment matching (MM) methods. 2.The proposed method is limited to Gaussian VAE. It may not apply to VAE with other distribution. Overall,  I think the proposed method is similar to the moment matching method. It is marginal novel and limited to the Gaussian case.<|endoftext|>The paper proposes an approximation that frees the calculation of ELBOs of Gaussian variational autoencoders from sampling. To achieve this, it utilizes Taylor expansion on the decoder networks. The proposed method was evaluated on three typical datasets. The former is a distribution but the latter is a sample. * Figure 5 ref before Figure 1Overall, I vote for rejecting. I like the idea that analytical approximate ELBO would help the computation. ######Pros:  + The analytical approximation would help with reducing the variance and computational expense of the ELBO. Instead, the claimed advantages are not well demonstrated. For example, the authors could show the wall time and variance of ELBO. Sec 4.4 is qualitative. The proposed approximation is limited to Gaussian encoders and ReLU decoder networks. The Taylor expansion itself should be applicable to other types of networks, but the performance then needs further evaluations. Concerns:* What s its connection to the Laplace method in variational inference.<|endoftext|>The paper proposes a sampling free approximation to the ELBO of a variational auto encoder with Gaussian likelihood and mean field Gaussian variational posterior. The approach relies on a Taylor series around the mean of the posterior, which allows for an evaluation of the ELBO s expectation, rather than relying on Monte Carlo Sampling. The authors clarify some of my concerns and include some minor improvements in the experimental comparisons, which is why I slightly increase the score (3 $\to$ 5). The computational cost introduced by the approach is explicitly discussed and not hidden in an appendix. ### Weaknesses  The contribution itself is very minor, as Taylor approximations to solve expectations are a common approach in the literature. Similarly, what does the number of iterations refer to, as the blue line seems to have converged from the beginning? Appendix B.1 switches to claiming to follow Higgins et al.instead but mentions again five convolutional layers, which is not what Higgins et al.report to be using in their appendix (they claim four convolutional layers). However, that in itself is of limited novelty, and the paper lacks a proper comparison to other methods and the broader literature to classify it properly.<|endoftext|>The paper proposes a learning method for Gaussian variational auto encoders based on Taylor expansion approximation. This allows an analytical formula for ELBO and therefore avoid the requirement of sampling during training. *Update after rebuttal* I have read the authors’ response and other reviews. Although, the related work section could still use some work in terms of writing (for example, to avoid presentation in a form of a list “A did this, B did that”, but it has been improved.I am therefore increasing my score. Motivation for the sampling free training for VAE in general as a concept is not very well provided considering that it comes with the computational overhead. The related work section in particular could see a lot of improvement. Considering the context of the paper, one would expect the related work section to focus on the discussion of the training methods for VAEs and on contrasting sampling vs sampling free methods. 3 layer network, specific number of hidden units and latent dimension, MNIST dataset   these may be too much details for related work description. "that works independentLY"The paper seems to provide an interesting idea for training a VAE that allows an analytical solution and therefore does not need sampling, but the empirical evaluation of the method is somewhat limited and presentation of the paper could be significantly improved. Therefore, I am voting for the weak acceptance as I believe the idea is worth to be known by the community but the existing drawbacks stop me from giving a higher score.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 8; It proposed spatial smoothing, a method that ensembles neighboring feature map points of CNNs to alleviate the computational issues with Bayesian NNs. + The paper empirically shows that spatial smoothing improves accuracy, uncertainty estimation, and robustness of BNNs across a whole range of ensemble sizes. While the theoretical analysis of the proposed spatial smoothing is interesting and valuable, the experimental evaluations are relatively weak or too some extent insufficient. Furthermore, the paper is not well self contained, which highly relies on the analysis, experiments in the appendices.<|endoftext|>The authors presented the proposed technique motivated in the context of Bayesian neural networks, which is not very strongly supported. Smooth  improves the accuracy and uncertainty of both deterministic CNN and a Bayesian NN approximated by MC dropout. Authors tried to justify how `smooth` improves the optimization of neural networks by 1. interpolating the `blur` operations as an ensemble of the neighboring features 2. showing `smooth` filter out the high frequency noises introduced by MC dropout and smoothen the loss landscapes perturbed by MC dropout. This paper proposed a spatial `smooth` layer including a feature range bounding layer `prob` and `blur` the intermediate feature map in a CNN. Authors also tried to connect common pieces in CNNs like global average pooling, ReLU + BN as special cases of `smooth`. This paper represents the spatial smooth design in the context of Bayesian neural network and claims that the smoothing e.g.averaging can be seen as an ensemble of neighboring features. However, only MC dropout approximated BNN is studied in the paper. The limitations of the proposed method should be further discussed.<|endoftext|>The paper provides an improvement to Bayesian NNs computation needs and accuracy by incorporating spatial smoothing (blur). The paper is, however, too difficult to follow since much of these results are put in an appendix which significantly increases the page numbers, and for which, without the appendix, the paper does not have enough evidence. It is also difficult to follow the paper with the appendix referred to every 3rd sentence. The structure of the paper also lends to difficult reading, with results included in the methodology, figures out of place, the literature review at the end, and a too brief conclusion. The language of the paper should also be improved. The authors do not provide enough evidence to prove this.<|endoftext|>The motivation of this work is on the computational cost of using BNNs in practice, where applications might require a large number of BNNs in an ensemble formation for achieving good performance. The work in this manuscript aims to reduce the computational cost ensemble. This is a similar approach used in convolutional neural networks. The work explores the idea of spatial consistency to improve the computational cost of posteriors. The main idea is well presented and edge cases are explored. Although this is not my area of expertise, the theoretical analysis seems relevant and sound. Some more discussion on applications beyond the imaging domain could provide stronger significance to this method. The unifying perspective, when compared to global average pooling, pre activation, and ReLU6, is a welcoming result. * Minor comments      Figure 1 appears on page 1 without being referenced in the textThe work presents a novel method for reducing the computational cost when using BNNs in real world image tasks.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 8; The authors propose a model of collaboration to improve outcomes for any participating agent. In their setting, the authors assume that every agent does not benefit from always collaborating with all other agents because of heterogeneity of the underlying data distributions. There is a lot of related work in the cooperative game theory and federated learning literature.<|endoftext|>This paper considers the problem of federated learning where different clients/entities share their datasets with other clients in order to obtain a model that performs best for the local loss functions. In particular, a coalition of agents belong to a collaboration equilibrium if each agent gets maximum benefit by collaborating with all the agents in the subset, and the subset is maximal. This paper talks about collaboration equilibrium but such notions of collaboration have been studied in cooperative game theory, probably in different context. Questions for the authors:  Regarding the example shown in figure 1, it is not clear that you will always be able to find an initial coalition from the graph. I thought the authors consider an interesting problem in this paper.<|endoftext|>The paper models this problem by introducing the benefit graph which models how each client may benefit another. Some theoretical guarantees are shown along with experimental results. I think the paper solves an interesting and important problem and that the model introduced is interesting and detailed. Here are some points:1 The authors mention that privacy concerns are the main reason why a client would not share his data. For example, the number of individuals the client has to share his data with. It is not clear to me however, that the Pareto solution gives the actual benefit graph. I realize that the exhaustive search for the benefit graph would be infeasible. The introduced model does not seem to be complicated enough to capture the real details of the problem.<|endoftext|>The paper considers the problem of agents sharing training data to improve the accuracy of the model obtained on their own population. The authors first consider an abstract formulation where each agent has a utility function over all sets of other agents to collaborate with. Together with the SPO part, this provides a quite complete solution for personalized federated learning problems, and in particular, the equilibrium solutions take into consideration the incentives of self interested agents, which is of potential practical importance. Is that right? Overall I think this paper proposes a novel theoretically principled method for computing stable outcomes of data sharing or collaboration in general, which can also be applied to personalized federated learning. Some of the theoretical results appear quite insightful.
Accept (Oral); rating score: 8; rating score: 8; rating score: 8; The paper treats the problem of unsupervised RL, which it defines as the problem of pretraining a system, without having access to a reward function, to learn a collection of policies, that are labeled skills. Using these analytical tools, the authors prove several interesting results. Overall, the paper presents an insightful analysis of some of the challenges of unsupervised RL. In general, I believe that the primary impact of this paper would be through its geometric approach of analyzing the space of learnable policies. The concrete theorems proven by the paper, while interesting, appear to have a relatively large set of conditions. The approach allows insights about the characteristics and certain optimality criteria that applies to skills learned through mutual information type approaches.<|endoftext|>The authors provide an in depth study of unsupervised skill discovery from the RL literature. In particular, the authors analyze such approaches from the perspective of "information geometry," and show that these algorithms _can not_ learn skills that are optimal for all possible reward functions, but _can_ provide a good initialization for online learning approaches that seek to adapt the initial skills to find optimal policies for new reward functions. WEAKNESSES(W1) The paper lacks a good empirical confirmation of the claims made here.<|endoftext|>From their analysis, the paper suggests ways to infer how many skills can be learned, and whether those skills are optimal with respect to some downstream tasks. It seems that these skills can be guaranteed to be the vertex of the above polytope, but not to be optimal with respect to all downstream reward functions. I am unsure how skills with different probabilities could lead to state marginals that have the same KL divergence from the average. * The reframing of the unsupervised skill discovery problem as a vertex discovery problem is very interesting. My questions in this regard are below.
Accept (Poster); rating score: 5; rating score: 5; rating score: 5; rating score: 5; This paper studies the generalization performance of learning algorithms. The basic idea is to decompose the training dynamics into noise and signal components, and then consider separately the behavior of models trained with noise and signals. The authors consider two specific algorithms: overparameterized linear regression and diagonal matrix recovery. The idea of decomposing excess risk dynamics seems to be novel and interesting. However, there are some issues. The advantage of the results is not quite clear. For example, the authors indicated that Thm 1 outperforms the existing stability analysis by replacing $B $ with $B$. The decomposition theorem requires Assumption 1 and Assumption 2, which may be restrictive. The analysis seems not rigorous. This is not quite intuitive since this result holds for all $t$. In particular, in the first few iterations the performance of gradient descent may not be good and it is unlikely that they have smaller training errors than $\theta^*$. It should be a vector according to the norm there. However, the advantage of the decomposing excess risk dynamics is not well justified. There are also some issues on the correctness of the deduction.<|endoftext|>This paper proposes a new approach to characterize the excess risk by decomposing the risk into two parts, the risk of learning the exact response and the risk of fitting the noise signal. Under the linear regression setting, the learning dynamic can be decomposed exactly into the proposed two parts. This paper is written clearly and easy to follow. The first concern is, how better is Theorem 1 compared to directly using uniform convergence to characterize the excess risk of linear regression, especially under the high signal noise ration scenario. It is argued that Theorem 1 is better than directly applying stability results under the high signal noise ration scenario, which makes sense. But I wonder if the proposed decomposition really improves the state of the art risk bounds. I also have concerns with whether this decomposition is meaningful for modern deep learning tasks. Therefore, even if the decomposition is approximately correct for deep learning tasks, it could be such a case that how to characterize the  bias training  dynamic is still unknown, while the  variance training  dynamic is not important at all. The decomposition proposed in this paper makes sense in some specific settings like linear regression. However, it is very unclear for me to see how this decomposition can be extended to a more general regime and how it helps understand the learning dynamics of deep learning. Given that I believe the decomposition s applicability and significance are questionable, and the technical contribution seems incremental, I recommend a weak reject.<|endoftext|>This work proposes a new bound for the excess risk based on a decomposition into a bias and variance term. Maybe: "we refer to the clean data without the output noise as the signal" would be clearer. They illustrate their framework in two contexts: overparametrised linear regression with SGD and low rank diagonal matrix recovery with gradient flow. Introduction, contributions: *provides interesting insights into the generalization community.*. Moreover, the quantity of typos and confusing sentences (some of which I list below) don t help. While in the linear regime it is very clear what is the model of data and the loss, there is a conceptual jump between Sec.3 and Sec.4 which makes it confusing to understand how general are the results in Sec 4. Does theorem 2 holds for any model and any data distribution as long as assumptions 1 and 2 are valid? The first is due to the norm of the gradient, but it is not clear what the second is due to. As it is, the sentence seems to suggest that almost all infinite width network is lazy. The authors speak about neural networks in the abstract, and present two plots in Fig.2 and Fig.3 related to neural networks. The authors should either detail the setting they are plotting in the main, or leave these results to the appendix. Introduction, second paragraph: *Stability based bound(s)* or *(The) Stability based bound*. Introduction, footnote: *In this paper, we refer the signal to the clean data without the output noise, and the noise to the output noise.<|endoftext|>Based on this decomposition, the authors derive the generalization bound for overparameterized linear regimes and matrix recovery regimes. I believe decomposing the excess risk into the variance excess risk and bias excess risk intuitively makes sense, which aligns well with the empirical observation that neural networks converge fast when fitting signal but converge relatively slowly when fitting noise. Even though, I have the following concerns:  Both the linear regime and diagonal matrix recovery regime are too different from the neural network case, as the optimization of neural networks is highly non convex. Although the NTK theory indicates that the neural network at the infinite width limit is equivalent to kernel methods, it is still far away from explaining the generalization/optimization property of practical neural networks. I would suggest the authors apply their framework to analyze, for example, two layer relu networks. It is unclear to me how does the new decomposition lead to improvements over the original stability bound quantitatively, or the improvement seems quite marginal? However, there is also a $\|θ^\star\|^2$ in Theorem 1, and both bounds are approximately $\tilde{\mathcal{O}}(1/\sqrt{n})$. Otherwise, I am not convinced that your decomposition will indeed improve the original bound. The following paper is very relevant to your work, it would be great if you can discuss it in the related work. Overall, the idea in the paper is well motivated. However, the results in the paper are mostly about overparameterized linear regression and matrix recovery, but the many of the claims in the paper are about understanding the generalization of neural networks.
Accept (Oral); rating score: 8; rating score: 8; rating score: 8; rating score: 8; The authors introduce Frame Averaging (FA), a general framework for adapting known architectures to become invariant or equivariant with respect to a general group by using group averaging operator. *Strengths*  The idea of replacing the averaging operator over the entire group by the FA is innovative. No comparison with the previous equivariant architectures are presented. In addition, I have some comments on the technical parts of the paper:  The definition of the frame $\mathcal{F}(\mathbf{X})$ for the case of point clouds and $G E(d)$ in page 5 is not clear. I think this remark is crucial for the proposed method as it affects the size of the FA framework.<|endoftext|>This paper proposes a method to transform a model into an invariant/equivariant model using the Reynolds operator. In GNN+FA, the problem seems to be that invariance is calculated redundantly as described above, and the contribution of this method is not clear. If this method is good enough, the point permutation action should also be subject to the frame averaging model. The authors should be cited for this paper.<|endoftext|>The paper provides a simple yet effective solution. These statements are clear and rigorous. Using the simple idea of frame averaging, the paper demonstrates state of the art results on several tasks. For example, it might be good to work through an example to make MLP translation equivariant (with the simplest possible construction of frames)? I might be wrong, but I think this simple construction is also equivariant? Are the number of elements output by frames the smaller the better, or is there a balance between performance and computational efficiency?<|endoftext|>The paper proposes to make any neural network equivariant by symmetrizing over a subset of the group, rather than over whole group. 3)	When using the symmetrization of a universal model, the resulting model class is universal in the class of equivariant functions. Strengths: 	The paper proposes a very practical strategy of building equivariant nets 	The universality proof helps convince the reader to use this method 	The paper considers and experiments on three different instantiations of their method, showing wide applicability. Is it the operator norm? Why have the authors chosen the name “frame” for F(X)? Which are the 2^d O(d) matrices?
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; This paper is well written and interesting, and the empirical analysis is impressive. The results speak for themselves by the end of section 4, but the takeaways could be more precise for the reader (note that this may take more analysis/experiments   but I m sure the authors can knock that out of the park).<|endoftext|>This paper empirically investigates how different models, trained with different methodologies should be ensembled to maximize the accuracy in image net classification task. Since the models are trained on different datasets and then tested on image net. Based on the analysis of training methodology can one further improve the ensembling by rescaling prediction confidences of certain models for certain test datasets? I have following questions for the authors though:  It might just be me, but how is the training methodology "Frameworks" is incorporated in this work?<|endoftext|>Strengths:The paper is well written, and has clearly very thorough experiments. I have not specialized in this research area and cannot speak well to the novelty of each claim, but believe that the paper deserves acceptance due to the extensive experiments, clear presentation, and interesting findings which open questions for future research.<|endoftext|>Overall I remain positive about this paper and vote for acceptance. The flow of the paper is well written, with each section addressing a one line of inquiry and naturally leading to the next.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 3; I am positive about this framework as it presents a better model for multi agent communication, especially enriching the communication among agents over the fixed, restricted reward based communication protocol in traditional RL. **Before rebuttal**: The paper proposes an architect builder problem setting where the architecture guides the builder to accomplish a goal by sending messages. This setting is distinct from traditional reinforcement learning and imitation learning. Drawing inspiration from cognitive science theories, the authors devise an algorithm for learning a communication protocol between the architect and the builder. On grid world tasks, they show that learned the communication protocol can generalize to previously unseen tasks. Overall, I find the setting and the proposed algorithm novel and interesting. However, I feel like this setting makes more sense if the architect were a human, as they may have implicit intent that the builder has to manage to interpret. I d encourage the authors to give a concrete example where this setting is useful in the agent agent setting. In general, what is the motivation of this work? The motivation should be stated more clear in the introduction. In addition, the description of the interaction needs to be more precise: right now, it does not specify whether the architect sends a message to the builder after every step or the builder can take multiple steps after a message. The main limitations of this work are (1) the architecture requires access to a simulator of the environment (which may not be a problem for a human) and (2) the simplicity of the experimented environment. The authors adequately acknowledge these limitations. Even though the formulation needs to be more rigorous and the experiment environments are simplistic, I think the contributions are interesting enough to draw attention of the community in the future. I recommend acceptance.<|endoftext|>The paper proposes an approach for interactive learning between a so called architect and builder agent, which is a different, but related protocol to RL or imitation learning. Under the assumption that the architect knows the target dynamics and reward function, the authors focus on learning the communication between the two parties, such that the builder can solve the MDP. After formalizing the setting in the multi agent paradigm, where two individual MDPs are defined, the authors propose an algorithm with so called modelling  and guiding phases. In each phase, architect and builder gather datasets, from which policies are extracted via behaviour cloning and planning. The results show that the proposed approach is superior wrt the baselines for solving the block environment. In addition, experiments show that the learned communication channel can potentially be reused for solving other tasks. Are any reductions possible? What would intermediate applications be before a perfect version of the approach is available? The related work is informative, but I am missing comparison to recent works on such learning protocols   e.g.[1].There are differences between the frameworks and [1] might work towards HRI (as also mentioned in the paper), but the direction seems quite related. It would be also interesting to dwell on the reusability of the proposed algorithm for the problem at hand. The authors themselves note in the discussion of the paper that the employed methods are suboptimal for the defined learning problem (i.e.using behavioral cloning and MCTS). I am also wondering to what extent it might be possible to reuse established approaches for emerging communication, as already cited in the paper (e.g.[2]).The empirical evaluation is informative, although the random building agent does not give too much additional insight. It would be good to show relevant ablations in the main part of the paper. A minor comment with respect to the figure of the approach: I find the pseudocode of the algorithm quite helpful (in my opinion, more helpful), but it is in the suppl. Learning to communicate with deep multi agent reinforcement learning. The paper proposes an interesting and relevant framework for interactive learning / teaching between two agents. The model choice and solution is sensible, and the empirical evaluation can shows that the initial approach for the problem works sufficiently well for the toy problem.<|endoftext|>The authors propose a setting in which two agents with asymmetric information collaborate to complete a goal. They demonstrate that learning in this setting results in the emergence of communication protocols that generalize to new tasks. ## Weaknesses* Several assumptions seem to be made which limit the approach s direct applicability to more complex environments  * Giving the architect access to the ground truth environment model seems to be a very strong assumption. * The heuristic used in MCTS is not described in detail, but, presumably, it uses a significant amount of domain knowledge. * It s unclear how the self imitation learning works (see "Questions" below for more detail). My main concern is that this only works due to the fact that the architect has access to ground truth environment transition models such that it can exploit spurious correlations between messages and behavior in an untrained builder model. This seems unlikely to work at all even in this simple environment without access to the environment model. * Lacking in experiments which attempt to understand the nature of the learned communication protocol. From section B.3 it seems like the agents simply learn to map messages directly to actions which is somewhat disappointing. If this is the case, then the generalization results may simply be down to the effectiveness of the MCTS procedure. It seems this would be crucial for the self imitation learning to work. In other words, an untrained builder agent will have no ability to interpret messages and modulate its behavior through them. As a result, the trajectories produced by the architect builder pair will be no better than random and self imitation of these trajectories shouldn t produce any meaningful results. This paper presents an interesting setting; however, it appears to rely too heavily on strong assumptions, and the communication protocols that emerge do not seem to exhibit any interesting properties (i.e.the communicating agent simply learns to output messages that correspond to the desired action for the acting agent).<|endoftext|>This paper presents an approach — Architect Builder Iterated Guiding — a method that tackles what is presented as an “Architect Builder” problem: a scenario in which two actors, an Architect, with knowledge of a high level goal, or reward function, must communicate over a discrete channel with a Builder who can take actions in the environment based on the Architect’s message. The paper presents a motivated, easy to understand algorithm for training both the Architect and the Builder, and evaluate on a series of “construction” based grid world tasks (resembling GridLU, MiniGrid, or Mazebase). Making this all possible is that the architect has full knowledge of the high level reward, **in addition to the ground truth state transition function**. The evaluation focuses on the proposed model, and two simple ablations: one where the architect has “no intent” at training, sending random messages, and one in which the builder takes random actions. The paper also presents (in the main body and appendix) a meaningful, thoughtful intuitive explanation of the learning dynamics of the architecture and the builder — more papers should dedicate portions of the main body to explanations such as this! I believe that this is a well written paper, with solid motivation, and thoughtful care in crafting the approach. Concretely, I  believe that providing the architect access to the transition function is an incredibly strong assumption that undercuts this work. I understand that full system evaluations are hard, but in this specific case, I think the current ablations trivially fail, and don’t provide much insight into the proposed approach. Instead, I would love to see other work that looks at more traditional HRL approaches, or that relaxes the some of the assumptions made in this work. Typos/Style/Questions:Broad Positioning Question: I love the Architect Builder problem statement, but it’s not immediately clear to me why we need to restrict the communication mechanism between the architect and the builder/what that gives us more broadly. Furthermore, the discussion and analysis of the learning dynamics of the various components of the approach should be a mainstay of future systems driven work in ML. However, I am deeply concerned with the assumptions made with this approach, stemming from the architect’s access to the ground truth state transition function. This coupled with a missing discussion of related work in hierarchical RL and Feudal RL specifically, and a more thorough evaluation including comparison to these methods and other relevant ablations inform my decision to lean towards rejecting this paper.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; Given that some of these filters might have very many parameters the resulting change can be quite big. I believe there are 2 clear limitations in the paper. I tend to vote for rejection of this paper. The fact that this is done for a limited number of parameters is similar to using L1 normed adversarial attacks. The evaluation of the proposed approach in terms of providing pixel wise interpretability section 3.4 is limited. The fact that this changes the confidence should not be surprising per se.<|endoftext|>This paper devises an analytic method for explainability based on the observation of filter wise parameter saliency distribution, and tests on several models. The motivation is straightforward and easy to understand. Does it mean a model of general performance could become a top performing model benefiting from the proposed fine tuning method? The contribution might be not enough for iclr.<|endoftext|>To support the proposed standpoints, the authors conduct a lot of quantitative and qualitative experiments. 2.Quantitative and qualitative experiments are conducted to verify the proposed standpoints. 3.The discoveries in the paper is interesting and inspired. I could not find any weakness in this paper. The contributions proposed in this paper are really interesting and are verified by the proposed experiments.<|endoftext|>+ Though the idea of parameter wise visualization is not new, and widely studied in other works, this paper provides an interesting perspective on the parameters responsible for misclassification. The authors would better claim the similarity and difference of these 2 topics. The proposed idea is somehow similar to the parameter space adversarial attacks.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 10; This paper presents novel and tighter information theoretic upper bounds for the generalization error of machine learning models, such as neural networks, trained with SGD. Following the same construction of the auxiliary weight process in Neu (2021), the upper bounds proposed in this paper improve upon Neu (2021) in two ways. This is a great paper that refines the information theoretical analysis of the generalization behavior of SGD using the HWI inequality and the ISMI bound. The organization of section 3 can be improved.<|endoftext|>This paper proposed a new generalization bound which is based on the recent construction by Neu (2021). The authors made further assumptions on the randomness of the batch sequence, and improved the results by Neu by achieving tighter bounds by tuning the trajectory term. A strength in this paper is that the proposed generalization bound yields a simple practical regularization technique, which can improve classification with VGG16. This is some novel bits as compared to (Neu 2021), which is mostly theoretical. However, the significance of (Neu, 2021) is not clearly written. Overall, the writing can be better polished to explain the background and provide intuitions.<|endoftext|>In particular, they drop a term called local gradient sensitivity, leading to significantly tighter bound in practical models. Based on this generalization bound, they study both linear model and two layer ReLU neural networks. This paper is solid and well structured. The author also admitted that their bounds are not new compared with the previous result. On the other hand, the authors argue that their bound decays with the increase of the model size. Is the label in testing instances also replaced by the random labels?<|endoftext|>The paper presents a new bound on the generalization error of models trained with stochastic gradient descent. Based on the insights obtained from the bounds, the authors propose two approaches to improve generalization. The distinction between sample level and instance level mutual information was not clear to me in the introduction, even though I am familiar with both types of bounds. The paper is exceptionally well written. In Fig.3, I suggest to use the same axes for both the gap and the bound. Even though the results may not look as impressive, it would be good to show a fair comparison. This is also seen in Fig.1.I assume that this is natural for the first few epochs, as initially the model performs equally badly on the training data and on the population.
Reject; rating score: 3; rating score: 3; rating score: 6; Strengths:* The theoretical result is interesting. * The novel algorithm is simple and effective at finding holes in the latent space. Weaknesses:* It is not clear how the theoretical result is important to the rest of the paper. To this end, they compare the quality of text generated from a hole to the quality of text generated from an untrained VAE.<|endoftext|>This paper studies the holes within the latent space of text VAEs. The major contribution is a hole detection algorithm, which firstly projects the latent representations to a principle subspace, then performs tree based BFS to detect holes. ICLR 2019 This paper discuss an interesting problem, the holes, in the representation space of text VAEs. My major concerns are the clarity of the algorithm, the scalability and sensitivity, and the experiment settings.<|endoftext|>This paper focuses on the discontinuities (aka.holes) in the latent space of VAE. In experiments, the proposed TDC algorithm takes less time to find a hole compared with previous methods. 2.The paper proposes an efficient tree based search algorithm for latent hole identification, and it is easy to parallelize.
Reject; rating score: 3; rating score: 3; rating score: 5; The authors propose a new meta learning framework to tackle the zero shot learning problem for time series data – through the combination of:1. This allows the model to be trained end to end on a source task, while automatically performing domain adaptation when applied to a new target task. The automatic output layer tuning they propose is an interesting idea. Weaknesses However, I do have some concerns with the paper in its current form, which has a couple of areas that need to be addressed. 3.How does the framework handle transfer learning between datasets with a different number of covariates? 4.Complexity of matrix inversion   assuming that h(t) has dimensions d, inverting the dxd matrix typically is O(d^3). While the raw idea has promise, many corrections/clarifications need to be made before it is ready for publication   specifically related to the terminology used, whether inputs between datasets can differ, and the complexity claims of their proposed layer.<|endoftext|>This work proposes a new forecasting method for jointly learning from a large pool of related time series. Instead of calling the method a meta forecasting method, which seems problematic. I was hoping to see a table similar to Table 1, but with training time for all the baseline methods across all the datasets. Nevertheless, they should also be included as baselines and discussed appropriately since both utilize global and local components and can be trained with a large corpus of time series. The method proposed is interesting (but somewhat incremental) and the problem is important. However, there are several issues that need to be addressed. Overall, the contribution and novelty of this work is limited.<|endoftext|>The authors propose an autoregressive framework, Meta GLAR, comprising of a global RNN model and a local linear model (learned using a closed form solution) for the task of meta forecasting of time series (TS). The linear model is the task specific model varies for each TS in the dataset, whereas the RNN model is the meta model that is shared across al time series. The authors perform comparisons with local models as well as NBEATS and DeepAR, and show that Meta GLAR is competitive with some of them on particular metrics. They also perform an ablation study to show that each component in their model is important to the overall zero shot transfer in TS forecasting. ## Strengths* The authors describe the problem setting for meta forecasting as well as the Meta GLAR approach quite clearly. ## Weaknesses* The novelty of the approach itself is quite limited. If all three metrics were calculated across all baselines and the authors  proposed models then these results need to be included (at least in the appendix).
Accept (Poster); rating score: 8; rating score: 8; rating score: 5; In particular, it proposes to combine slot attention mechanisms with conditional neural radiance fields to segment and render novel views of a scene from a single input view. The authors also address one apparent shortcoming in the slot attention paper: the background and foreground object latent codes are sampled from the same distribution, leading to breakdowns on scenes with complicated backgrounds. The paper proposes to learn two disjoint distributions, one for the background and one for the foreground, to alleviate this issue.<|endoftext|>This paper utilized the powerful NeRF. In particular, the authors showed additional results on a real world image. The authors showed success on three synthetic datasets and various applications. Additionally, a demonstration of the approach on real world scenes would be a strong result to show in the paper (either it is negative or positive).<|endoftext|>The author evaluted with 3 self created datasets and show several reasonable results by performing mentioned tasks such as 3D segmentation, rearrangement etc. This paper points out a good direction to dive into unsupervised learning of compositional scene representations. Finally,  the object clusters are discoverred.
Accept (Poster); rating score: 8; rating score: 8; rating score: 5; rating score: 3; Hiding the cost of environmental resets is unrealistic and hinders progress in embodied RL. Will following this methodology result in much fewer resets and superior learning if tasked to achieve the same level of performance as expensive and impractical full episodic reset methodology, at least for tasks of interest with real world robots? If total experimentation time is also a factor, then the trade off of autonomous operation in the real world vs. cost of human resets seems to be still an open problem in need of task specific trade offs. This paper gives two convincing definitions of autonomous reinforcement learning and introduces clear evaluation methodologies accessible for the community. Relevant baseline results are provided, such that the proposed benchmarks are good starting points for future research.<|endoftext|>The manuscript presents a concrete formalization of the "Autonomous Reinforcement Learning" (ARL) problem: this problem refers to a scenario where the agents has no (or very few) resets during learning of the task(s). This benchmark contains diverse environments from simple manipulation tasks to locomotion and complex manipulation. Using this benchmark the authors provide an analysis of the crucial factors that affect the performance of current state of the art algorithms for the ARL problem. The main contributions of the paper are:  Concrete formalization of the "Autonomous Reinforcement Learning" (ARL) problem  A novel benchmark for ARL with diverse tasks  Novel evaluation settings (Deployment Setting and Continuing Setting) for ARL  Insights on the important factors for effective ARLStrengths   The paper provides a nice formalization of an important problem (that of Autonomous Reinforcement Learning)  I like the distinction into "Deployment Setting" and the "Continuing Setting".<|endoftext|>Most reinforcement learning (RL) studies use episodic benchmark tasks. The paper argues that conducting studies on non episodic tasks is crucial. The strength of this paper is that they clearly argue that the studies on continual learning are important and give a clear notion about ARL. I totally agree with the authors  argument about the importance of continual reinforcement learning. The general and classical framework of RL does not necessarily have the assumption of episodic segmentation. This is because they only showed the pre existing methods tend to underperform in the ARL settings. The paper is very good from the viewpoint of position/short journal paper.<|endoftext|>It is quite difficult to see clear contributions in this paper since the theoretical framework does not really introduce novel concepts and the experimental framework is essentially a collection of existing environments. Even if I always commend papers that propose benchmarks and evaluation framework, I found this work quite problematic. Usually, the concept of autonomy is defined with respect to the concept of human intervention in setting goals, etc. At the end of the day, a situation where an agent is stuck in a certain situation might happen. The reviewer also wonders if there is any specific requirement for this assumption. The models used in the benchmarks are not novel (see Section 5), but they are essentially existing ones. Minor points:  The authors say: "Current episodic settings typically provide an environment reset every 100 to 1000 steps, corresponding to ε ∈ (1e 3, 1e 2) and an autonomous operation time of typically a few seconds to few minutes depending on the environment.".
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; rating score: 6; This then motivated a two stage training method that shows bits of information can be prioritized to capture more perceptible information during VAE training. In [1], a dedicated rate distortion analysis has been made to show the competition and balance between these two terms in ELBO. [2] was also not discussed in this paperInterestingly, in this paper, the authors shows that the second stage VAE decreases ELBO loss without changing perceptible quality change of the generated images. In [2], the point of the additional bits was to improve the quality of the generated images. If the second stage VAE does not improve the perception of the image, then what is the benefit of the second stage VAE? But for what actual benefits? There was also no consideration or comparison to related works, further obscuring the contribution of the work compared to the state of the arts. 159–168.PMLR, 2018. The paper can also strengthen discussion and comparison with related works that have examined the same topic.<|endoftext|>Authors explore a disconnect between likelihood and perceptual quality in likelihood based generative models   a well known phenomenon (Theis et al., 2015). In attempt to alleviate this authors propose a two stage method, where one VAE is trained to be low rate (i.e.to have high quality samples), and a second VAE is trained to be high rate and model the distribution of *imperceptible information* conditioned on the output from the first VAE. In the end, I consider the paper to be below the bar for acceptance. The notion of the rate distortion (RD) trade off/curve in relation to VAE learning (Alemi et al., 2018) is not properly introduced, even though what eqs.<|endoftext|>This paper studies the relationship between likelihood and sample quality in Gaussian VAEs. The authors argue that high likelihoods are obtained through high frequency signals which are not perceptible to humans, and that low likelihood models can actually produce good samples. [1] Fixing a Broken ELBO, Alemi et al.UPDATE 1 AFTER REBITTAL I have read the author s response, and while I agree that this paper more closely looks at sample quality than Alemi et al., I still believe that a much more careful discussion is needed before the paper is accepted. While this paper studies an interesting problem in a well presented way, I have doubts about both the novelty and the motivation of the proposed approach.<|endoftext|>The authors formulate a hypothesis and try to prove it experimentally deriving a novel two stage scheme for VAE that provides good ELBO values and generates high quality images. How the rate changes in the last experiment? Overall I find the paper interesting but in its current form it is more a workshop paper. Many important experiments are missing. All that is observed/reported can be explained by more simple and clear reason: overfitting on training data. So I would say that the main claim remains unproven if not wrong. It would nice to see the comparison between their quantity in initial VAE and in the proposed two stage model. The phenomena and suggested modifications of the model is important and should be studied in more details.<|endoftext|>This paper argues that much of the likelihood in a VAE is taken up by visually imperceptible information. The authors propose to address this by learning a multi stage model where the first stage learns coarse visual details and the second stage learns to produce original images. The improvement in visual samples is impressive, especially on CelebA. However this makes the presentation more concrete, so I suppose it s okay. I believe that this paper will have very high impact.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 3; rating score: 3; This paper proposes a new model architecture, along with a new loss, to solve the abductive natural language inference (aNLI) task. I would have liked to see some explanation for this choice, and why this method is expected to work well. ### Strengths of the paper  The paper is overall well written. The proposed model changes are easy to follow as well as the focal loss. This result shows there seems to be some value of the proposed approach in improving sample efficiency during learning. Specifically, it seems that ART, by itself, is not such a useful task. How much of the performance is degraded/improved by adding or removing the interaction layers? What is new is their application to the aNLI task.<|endoftext|>It would be really helpful to explain this in more detail. If not, why use a BiLSTM in the information interaction layer? Why use a focal loss? W4) I felt like the impact of the paper is likely to be limited, with the paper in its current form: it s a relatively complex model for one single task, with little analysis of why the method improves or indication that the approach could be more broadly useful, and it relies heavily on one feature of this dataset (having multiple annotated hypotheses for a single pair of observations). In particular I found it difficult to follow the description of the joint softmax layer in section 3.2 (see questions below), and (to a lesser extent) the information interaction layer. *Questions*Q1) How are predictions made from the model during evaluation?<|endoftext|>Specifically, they propose a novel loss function called the joint softmax focal loss function (based on Lin et al, 2017) that groups together a single correct hypothesis together with all the incorrect hypotheses. The first component is called the context coding layer and the second one is called an information interaction layer. The approach seems to be motivated well. Weaknesss:  Presentation: The paper is not written in a clear and concise manner. For example, there are some loose claims about abductive natural language inference in the first paragraph. Results: Some of the results are missing. I think the paper proposes a useful method for the task of abductive natural language inference based on good intuition.<|endoftext|>This paper proposes a new model for the abductive natural language inference (alphaNLI) task. The model extends a pretrained language model with a BiLSTM based information interaction layer and the joint softmax focal loss. Weaknesses:(1) The proposed joint softmax focal loss seems not applicable to the experimental ART dataset. In other words, this proposed loss is not evaluated in the experiments. This makes the contributions of the paper unconvincing at the current stage.<|endoftext|>This paper proposes a method for the abductive natural language inference task. Different from previous state of the art models that score and rank candidate hypotheses, the authors propose to model them together and compute a joint loss. The proposed method implemented on top of RoBERTa large shows improvement by 1% accuracy in the test set from the previous best method, and 3% improvement from the simple RoBERTa large. Weaknesses:  The writing is not clear and the paper has a lot of grammatical errors and stylistic issues in particular around citations. It seems that the proposed approach lacks novelty where the authors use bi directional modeling and focal loss that are common techniques in the field. Even if the improvement on the task is significant, its contribution might be incremental.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; The authors provide theoretical analysis on the lottery ticket hypothesis for convolutional neural networks. The techniques of the random subset sum are used to prove theorems. I am not an expert on theory nor random subset sum, but I can grasp some sense of the proof after reading the paper. Assumptions are too strong as indicated by the authors. Currently, I cannot clearly see the connection between the theorems and the LTH, so I would give a score of 5 at this time.<|endoftext|>This paper proves the Strong Lottery Ticket Hypothesis for convolutional neural networks by showing that given a target convolutional network, a logarithmically wider and twice deeper randomly initialized network can be pruned to approximate it. The overparameterization required is logarithmic in the approximation error and the network depth and number of parameters, which matches the existing results for fully connected networks by Pensia et al.(2020).The importance of this result is that this proves that (sufficiently large) randomly initialized CNNs can be  trained  solely by pruning, without actually updating the weights of the CNN. I have the following questions:1. It is not clear to me why the negative input situation cannot be handled similar to prior work. Could the authors please provide some more explanation or an example?<|endoftext|>The main theorem states that a CNN with any given weights can be approximated well with high probability with a larger CNN with a doubled number of layers and a logarithmic number of parameters, and a proper pruning mask on random weights. This paper is an extension of previous theoretical results on SLTH from fully connected layers to convolutional layers. The paper is well written and easy to follow. The main theorem is well supported by the empirical experiments. However, the review is not sure if the technical contribution is significant enough compared to previous theoretical work on neural networks with fully connected layers.<|endoftext|>The lottery ticket hypothesis is proven for 2 dimensional convolutional network layers and positive inputs. This is not a new insight. No neural networks are trained or actually pruned in the process. Why wouldn t the authors at least use the pruning algorithm edge popup to show that they can approximate a target network? Because of the lack of novel theoretical ideas and the fact that the proofs are limited to positive inputs and an unrealistic parameter initialisation approach, I recommend that the paper is not accepted for publication at ICLR at this stage.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; This is an interesting paper that presents a new and interesting problem. However, I felt some part of the paper could be clearer. I felt the experiments in the paper were quite weak. As in given the data, we would like to find X. Overall, I like the paper but it can improve on clarity and experiments.<|endoftext|>The authors  motivating example is liver donation acceptance decisions: a potential donor approaches with an organ they are offering to a particular recipient. I thank the authors for their response. The paper identifies an interesting problem that takes a different perspective than past work. The structure of the authors  model is based on three components:1. 2.The performance of the model "looks good" in the sense that it produces plausible explanations and has a better AUC than the baselines. IRL can produce non stationary policies if the state includes the appropriate observations that would induce non stationarity.<|endoftext|>The paper motivates an "inverse online learning" framework to learn non stationary policy online. 1.The setting and description is quite confusing. If I am missing something, I would ask authors to clarify. 5.Table 2 results are confusing especially comparison to CIRL, what are the "unrealistic" assumptions CIRL is making that is being relaxed in this work? Overall I believe the presentation of the technical problem is confusing, could be significantly more clear, cost function/inference better connected to the problem and validation improved. I have updated my score based on their response.
Reject; rating score: 1; rating score: 3; rating score: 5; rating score: 6; The paper has several limitations when in comes to experimental evaluation. Thus, by the hardness definition in the paper, the well trained model $B$ would find the same set of samples "harder" than an ill trained model. So concluding hardness based on the latest epoch number at which its label stabilizes is not indicative of its hardness. and Pal et al."Stateful detection of model extraction attacks". * To construct benign users, the authors take a split from the original test distribution.<|endoftext|>  The paper proposes a defense  HODA  against DNN model stealing attacks. 3\.Well written  The paper is well written and was easy to follow. Simulating benign users: How are benign users $S_u$ simulated   by using a held out fraction of the victim s test set? I appreciate the empirical study in Sec.4.3. that shows hardness captures OODness of samples.<|endoftext|>It is a detection method that determines if a given user is adversarial based on the hardness of users  queries. Additionally, the GPU load + energy cost is increased by X as well. The storage overhead is substantial if the method has to store checkpoints after every epoch. https://arxiv.org/abs/1910.05429The paper presents a new detection based defense against model extraction attacks that performs better than the previous (directly comparable) defense called PRADA.<|endoftext|>The authors investigated the hardness degree of samples and demonstrated that the hardness degree histogram of model extraction attack samples is different from the hardness degree histogram of normal samples. My comments are as follows. In the last paragraph of page 4, it is not lucid to me how the authors get the results based on this figure. Next, in section 4.3, the authors calculate the hardness degree of the samples generated by attacks. Finally, in section 5, the authors proposed the experiment setup and evaluation. In general, I think this paper has merit.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; The paper shows a detailed comparison of different graph generative modelevaluation metrics and highlights that current approaches for the evaluation ofGGMs are insufficient and perform poorly in terms of assessing diversity andfidelity of generated samples. The paper pinpoints these issues to the relianceof many metrics on a predefined set of features extracted from the generatedgraphs. Particularly, interesting would be how important   the concatenation of the layer readouts is. Finally, I wonder how fair the comparison between the static graph   featurizations and the newly proposed approach is. The paper is well written and contributes some new thoughts and approaches forthe evaluation of GGMs in an unbiased manner.<|endoftext|>This paper evaluates the effectiveness of different metrics for graph generative models from many perspectives. They thoroughly study the following factors: fidelity, diversity, sensitivity to node/edge features. They find that pre existing GGM metrics fail to capture the diversity of data and find several random GIN based metrics that are more expressive and have low computational costs. 2.I think the randomized GIN based metrics have some novelty there and I am surprised to see they are more robust to mode collapse and dropping. 3.In Fig3, what is $\hat{\rho}(S_g, S_r)$? The datasets provided are quite small. Besides, a few more concerns about this paper have been mentioned above.<|endoftext|>The paper proposes a scalar metric for evaluating Graph Generative Models (GGMs). The metric is based on computing the Maximum Mean Discrepancy (with an RBF kernel) between graph representations of the sampled and real graphs, as extracted from an untrained GIN model.<|endoftext|>The paper proposes the use of an untrained graph neural network (GNN) to generate a graph embedding which is used with other measures to evaluate Generative Graph Models (GGMs). The main advantages of this evaluation process are the use of a single score, the inclusion of node and edge features, and its empirical time complexity. For example, there is no analysis of the time complexity. For example, what does it mean a value of 0.97 in some of the metrics? While the presentation of a new method/framework is important for the GGM, the paper lacks theory, and the new metric is impossible to interpret.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; The authors address the task of concept based video classification: The resulting model is interpretable, as it first predicts a set of binary "concepts", and these concept features alone are then used for producing the final classification. The main novelty of this work is to propose a method for automatically generating these concepts from natural language descriptions (hence avoiding the need for expert annotations). They also show that their concept based model performs as well as a standard video classification model on these datasets (ie interpretability does not come at the cost of predictive accuracy). The authors address the timely problem of interpretable video classification. The paper is well written, and the authors have also described their method, and data collection process, thoroughly and clearly. Although the proposed concept extraction method is automatic, it still requires significant annotations in the form of natural language descriptions for each video in the dataset. And so the annotation time and effort of this needs to be contextualised.<|endoftext|> This paper proposed an automatic concept discovery and extraction module based on the label explanations and an end to end concept bottleneck model for video classification. 2) The pipeline can be easily applied to any supervised video classification architecture. 2) This work is more like a "label decomposition" based on natural language explanations and has low relevance to video modeling. It seems that the annotation cost of this pipeline is high in even normal size datasets. 4) In the extraction phase of CoDEx, a fixed set of rules is used to extract "raw concepts". This paper proposed an end to end pipeline to train an explainable video classification model. The paper is targeted to study three interesting research questions, and I particularly liked the discussion section at the end of the paper. However, I still find that the contribution is weak   experiments are done on only two video datasets with limited classes but given that the method itself is generic.<|endoftext|>For inference, these concepts are identified in a video and based on these identified concepts, an FC layer classifies a video into one of the target classes. The method is evaluated with two datasets MLB Youtube video activity classification dataset and MSR VTT dataset with extending them with textual descriptions, showing that the concept bottleneck does not impact a lot to the classification performance. 1.The experimental results are encouraging. I think the competing approach with this method is the one that uses a set of handcrafted concepts instead of automatically extracted concepts from textual descriptions, while according to the paper, the downside of such handcrafted concepts is that there is no guarantee of having a necessary and sufficient set of concepts. However, this is also the case for textual descriptions, and thus to demonstrate the merit of this method, I think comparison to the approach that uses a set of handcrafted concepts are necessary. I think the paper is well written, but the merit of the method is not very clear. Since textual descriptions may require a similar burden to handcrafting a set of concepts, the proposed method may just add noises in the set of concepts. Regarding this point, the merit of the method should be experimentally verified (w1).<|endoftext|>This paper works on the interpretability of video understanding problem. With a set of textual descriptions, the authors propose a pipeline called CoDex to extract the key concepts for explaining the classification, in contrast to previous methods which use the predefined classes. The trained final concept bottleneck model can obtain interpretable explanations aligned with human servey, while retaining the performanceHere are the pros and cons of the paper:Pros:1. This could benefit other interpretability models which have textual descriptions available. The proposed datasets could also benefit the related fields/applicationsCons:1. I think this paper overall contributes an interesting pipeline for automatically extracting relevant concepts for the concept bottleneck model. Overall I still feel the paper is addressing an interesting problem, but agree that the method might limit its applicability.
Reject; rating score: 3; rating score: 6; rating score: 8; Furthermore, using these interpretations, the authors aim to show that any neural induction method is interpretable. These ideas are illustrated in several experiments. The paper generally has a very relevant topic of generating interpretations from DAG models learned from data. It makes interestingobservations, drawing connections between causal inference literature, more traditional DAG learning, and mental models. Technically, I thought the paper was difficult to follow, in particular in the crucial parts. Similarly, a ruleset function has a range of { 1,0,1} but I do not see how it relates to the said properties. The main theorem (Theorem 2) I also found confusing. Theorem 3: again, I am not sure what is being proved here (and the proof itself does not help much). I also strongly disagree with the "big picture view" that is given. In my opinion, the main contribution of this paper is the approach to generate structural causal interpretations from CEMs. This is not completely new however, as there is quite some related work on generating explanations from Bayesian networks. I believe that only after this the significance of the results can be assessed much clearly. As a result, I do not recommend acceptance at this point.<|endoftext|>Several contributions are made by the paper, including:(1) The theory and method for automatic generation of human understandable interpretations (i.e.SCIs) from causal models, including deep causal induction models. (2) The theory and algorithm for using existing SCIs (e.g.obtained from human) to improve the learning of causality by deep causal induction models. (3) Experiments and the small human study for illustrating the theory and methods. Some grammar problem too. The research topic is important and new. The theory and method developed looks sound to me, and the findings are interesting and could have potential use. A main concern I have on the paper is the clarity of its presentation. Although the logic is rigorous, the writing of the paper is not so easy to follow. In many places, the authors have assumed that readers know the meaning of a term used or the purpose of a statement, which leaves readers a lot of guesswork; and some statements are misleading. Some examples:  Introduction: the (important) term "causal induction method" is used without being introduced. Theorem 1: what does "n SCMs" mean? 2.Related work is not complete or lack necessary details. Is there any link between SCI and the work presented in the following paper? More details should be provided on (Stammer et., 2021) and how it is related to and different from the work in this current paper.<|endoftext|>This paper investigates how interpretation arises within a DAG structure learned from data. The authors demonstrated the usefulness of SCI. These definitions lead to SCI theorem, and the authors further made a connection between Neural Induction Models and SCI interpretability. To begin with, the paper’s strong assumptions are not quite well articulated. Is this because this paper has very restricted assumptions on what an underlying SCM is (linear and Markovian)? In page 2, “product distribution over noise variables” and “independent, exogenous variables” imply that the SCM yields a Markovian model, meaning that we observe all the confounders among the measured variables. Input: Can you share some concrete examples of bold I? Some of the assumptions in the paper seemed too restrict at first, but I later found reasonable for the sake of generating interpretation. Interpretation provided by edge specific causal effects can be problematic in most situations (without the additivity assumption). It would be desired to have some discussion about the “probability of causation” in causal inference and its relevance to SCI / rule set. I find this paper contains several refreshing and interesting ideas, and the linear and Markovianity assumptions are reasonable for the sake of SCI. Given that the paper provides an interesting idea (which although I feel a little bit preliminary), I wish more researchers read, discuss the paper, and work on making the idea more concrete (e.g., connecting to path specific effect and counterfactuals). Similar to Prop 1, Is there any reason why it is a theorem rather than a definition for what the authors want to do with DAG in a specific way? I would like to point out a few more things with no particular order. In the middle of page 3.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; Focusing on LayerNorm, they further demonstrate combining LayerNorm tuning with existing fine tuning methods to improve performance. Overall, this paper seems to have limited contributions for adapting pre trained vision language models to downstream tasks. The depth of the analyses is limited and does not provide interesting and novel findings to the community.<|endoftext|>The paper investigates a range of techniques for adapting CLIP to different tasks. They find that only tuning the LayerNorm is effective and further combining it with other adapting approaches delivers better performance. Pro:It is good to know that LayerNorm tuning is quite effective for CLIP and combining it with other approaches gives even better performance. Con:1.Limited novelty. The methods used in this paper are existing methods and the authors do a simple combination and benchmark them with CLIP on various datasets. 2.Missing full model fine tuning. The paper presents an empirical study of how to fine tune CLIP for downstream tasks.<|endoftext|>This paper has extensively studied how to adapt the large scale pre trained vision language model CLIP for downstream tasks. The LayerNorm tuning for adapting CLIP to downstream tasks is shown to be effective. 2.A simple yet effective scheme that combines LayerNorm tuning with other fine tuning methods is proposed to obtain competitive performance across the board. 3.A thorough comparison of different adaptation methods is provided in four scenarios across two spectra. 2.The technical novelty of this paper seems very limited. The three methods for fine tuning new parameters have been extensively studied in existing works.<|endoftext|>This paper proposed a new method (LayerNorm tunning) for finetuning a pre trained vision and language model. Weaknesses:      While the authors claim that the proposed method is useful for vision and language models, only a single model (CLIP) is used for these studies. *arXiv 2015*The paper proposed a useful technique LN tunning for fine tuning the pre training CLIP model for downstream image classification tasks. However, the generalizability of this approach is not demonstrated, and the proposed recipe of combing LN tunning with other fine tuning methods seems also not useful as it typically shows even lower performance than only using LN tuning. Overall, this paper does not provide enough support for it to be published at ICLR.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; + The paper also presents a novel driving simulator with procedurally generated maps and active agents. The authors mention that the simulator will be released, which is a big plus. I would appreciate it if the authors can clarify this. This paper presents a simple yet very effective human in the loop learning algorithm. I recommend acceptance for this paper.<|endoftext|>This paper proposed a method for driving policy learning based on human AI copilot. The contributions are in the design of the copilot learning method. The proposed method is compared with several types of baselines including RL, safe RL, offline RL, and IL. The adding of intervention minimization seems to be a marginal contribution. The paper is solid and clear, with enough experimental support.<|endoftext|>The paper proposes HACO, a human in the loop reinforcement learning method that safely trains an agent to imitate expert behavior while minimizing the number of expert interventions required. HACO uses offline RL to train the agent to imitate the actions taken by the human during these interventions. Overall, I enjoyed reading this paper and found its results convincing. Missing related work: 1. Update Thank you to the authors for adding the comparison to IWR. I have increased my score.<|endoftext|>In this work, the authors propose a new algorithm for data efficient human in the loop learning, Human AI Copilot Optimization. While I realize that this is not something that can be addressed in the timeframe of rebuttals, it would significantly strengthen the paper. "We split the driving scenes into the training set and test set with 50 different scenes in each set. > The latter part of this phrase seems to suggest that you re training on the test set?
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; The estimation problem is formulated as the contained optimization problem (6), with both continuous and discrete parameters to optimize over. The proposed TELLER algorithm is not perfect, but the authors explain the idea very well and the algorithm makes heuristic sense. The problem this paper aims at solving is an important one and I can see that it will make a difference in many applications. The only concern I have is about the stability of the algorithm, which may be of critical importance in practice.<|endoftext|>The proposed method is evaluated using one synthetic and two real data sets. 1.Structure and presentationThe paper is not self contained on the theoretical basis. This is a key observation in evaluating TELLER. I am not sure whether this meets the ICLR format requirements because the content from Page 4 8 may exceed the limit. The first paragraph in Section 3.4 is a redundant summary that deserves to be simplified. The quality of the paper has been significantly improved and exceeded the acceptance threshold.<|endoftext|>This paper proposed a model for learning temporal logical rules for the temporal point process. It would be better to claim that the key difference between temporal logical rules and ordinary logical rules, and how temporal logical rules generalize them. * The following paper is also relevant, please discuss this paper in the related work section. 2019.In summary, it seems that this is a novel and interesting paper for learning temporal logical rules with solid experiment results. I am familiar with logical rule learning but not the temporal point process, so I am not sure about my evaluation of the temporal process part.<|endoftext|>The paper uses a method inspired by column generation to continuously add temporal logic rules to their model to maximize the likelihood of the data. The goal is to generate a model that is highly interpretable.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; The paper looks at  favorable properties of feature representations of an adversarially robust model. The paper only provides experimental demonstration of this phenomenon without going into a more detailed explanation of the phenomenon. In my opinion this is not enough when the observations, in question, are not very novel and have already been explored in various forms in past published literature. Thus I vote for rejection.<|endoftext|>Moreoever, They also conducts similar experiments to visualize robust model s features. To summary, the main contributions of this paper are as follows:  A comprehensive literature review about findings related to features of standard and robust models;  Train a $\ell_2$ norm robust neural network and visualize its features;  Leverage *representation inversion* and *feature visualization* to demonstrate the features in robust neural networks are more discernable. This paper s experiments are comprehensive. *Figure 5* is especially interesting since it demonstrates how the feature space of a robust network differs from that of a standard network.<|endoftext|>Thus I lean toward rejecting the paper. The observations are interesting and inspiring. My main concern is the novelty of the paper since the current submission does not introduce a new approach or algorithm or theoretical results. The paper also lacks comparison/discussion of recent works.<|endoftext|>This paper posits that adversarial training imposes a prior on intermediate representations of a classifier. Here are some of my thoughts/criticisms:1. In robust models (which have lower accuracy), the models seem to be retaining a lot of information not relevant to classification. My question is    is visualizing a representation useful if that representation /classifier is not very good? I really like the simple and elegant nature of the solution proposed in this paper but, overall, I’m not convinced about the novelty wrt prior work, so I’m going recommend a borderline reject (however, I’m willing to be persuaded)
Accept (Poster); rating score: 8; rating score: 5; rating score: 5; rating score: 5; This paper proposes a new information fusion approach to deal with the instance dependent label noise (IDN). Specifically, the authors claim that the essential problem caused by IDN is empirical, instead of underlying, data distribution mismatch during training, which I think is new and interesting. Therefore, the authors fuse posterior transition matrix (PTM) and noise transition matrix (NTM) and design an unbiased risk estimator. 2.The proposed estimation method for PTM and the fusion of PTM and NTM via Kalman filtering are new. 6.Some language issues, such as "a easy to compute" should be "an easy to compute"; "is the instance vector of n th sample" >"is the instance vector of the n th sample".<|endoftext|>This paper proposes to do loss correction by a posterior transition matrix (PTM). The information fusion approach, which should be the key novelty of this paper, is not demonstrated sufficiently. The paper defines a new loss correction approach, which uses the posterior transition matrix, to achieve the statistically consistent classifier. It is not clear why the proposed information fusion approach works.<|endoftext|>Strengths：1, I think the posterior noise transition matrix and information fusion (IF) approach are new and interesting. Thus the applicability of the posterior transition matrix may be limited. However, the authors only compare PTD in the experiments. I think more comparisons are needed to further validate the effectiveness of IF. The proposed approach is supported by the theorem.<|endoftext|>This paper proposes to estimate an input dependent noise transition matrix (PTM). I would be willing to upgrade my evaluation if the authors address my concerns during the rebuttal. The presentation and the writing of the actual method can be improved to make it easier for the reader to understand different steps of the pipeline. For example, what happens when the noise ratio is above 50%. The experiments would also benefit from additional baselines.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; The paper studies k means problem in a learning augmented setting. I would be glad to accept the paper if the authors could properly address my major concerns in the follow up discussions. The authors also experimented with their algorithm on three datasets, along with three different predictors. This is an interesting paper that has both conceptual and technical novelties. Instead, the focus of this algorithm seems to be “de noising” an already good but slightly noisy predictor. Nonetheless, I can still see that this “de noising” setting make sense, and it is relevant in dealing with adversarial data set. I also have other concerns about some proof details and the experiment results. The major ones are listed as follows.<|endoftext|>This paper considers the problem of k means clustering with the aid of a predictor which supplies a proxy to the optimal clustering subject to some possible errors. The problem is interesting. Given the fact that hard clustering is generally NP hard in the worst case, it make sense to try and mimic/find scenarios where this discouraging fact can be bypassed, e.g., by introducing reasonable side information. 2.I am not sure if it is a real weakness, but I find both the algorithm and (especially) the analysis quite standard or at least not surprising. Perhaps the authors could elaborate a bit more on the technical novelty in their proofs.<|endoftext|>To empirically evaluate the proposed method, the authors perform experiments on synthetic data and a few real datasets. The experiments demonstrate that the proposed method with $k$ means++ initialization achieves better performance than $k$ means++; moreover, the performance is competitive and robust even when the predictor labels are corrupted. For example, in the experiments, the initial solution are derived from $k$ means (Lloyd s) algorithm, which might require many initial seeds to attain a good solution. In the current experiment, only $k 10$ and $k 25$ are tested, and it is hard to see the comparison of algorithms when $k$ gets larger, which is a more challenging case for the $k$ means problem. A rigorous analysis of the algorithm has been provided.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; Their runtime is much faster. Following the invariance property, the paper introduces the novel graph matching network and avoids sampling step based on it. Overall we found the paper to be clearly written and offer a nice formulation with a matching modeling approach for the important problem of rigid body protein docking.<|endoftext|>In this paper, the authors propose a method for the “rigid body” docking of protein protein complexes, i.e., a complex in which conformational changes in protein structures are not allowed. This method, called independent E(3) equivariant graph matching networks (IEGMNs), finds the optimal rotation and translation to place proteins in a manner that the distances between residues in the binding site is minimized. The authors could use one of such methods, e.g., one based on the Voronoi procedure [PMID: 12376381], to significantly reduce the problem size and render identifying keypoints easier.<|endoftext|>The paper proposes a SE(3) equivariant graph matching network for end to end rigid protein docking. I vote for accepting this manuscript for the reasons listed above. The keypoint selection with optimal transport and rotation prediction with Kabsch algorithm are also interesting to me. I agree that surface contact modeling is very important for protein docking.
Reject; rating score: 5; rating score: 5; rating score: 6; This paper provides an empirical study on pretraining for lifelong learning. The model is evaluated on a range of datasets in CV and NLP to support the findings. Pros  The paper is very well written and easy to follow. Again, the coherent structure and detailed analysis are a plus. Limited setting.<|endoftext|>The authors present a comparative study on the impact of using pre trained models on task incremental learning problems. It is well known that model pre training provides significant performance boosts and robust transfer training solutions. While the proposed experiment are substantial and do provide additional confirmations, main conclusions are mostly well known (pre training achieves better performance, larger datasets and models are better), or already discussed in the lifelong learning community (flat loss landscape). Are there tasks where pre training is detrimental, or less beneficial?<|endoftext|>This paper explores the catastrophic forgetting in lifelong learning from the perspective of model initialization. However, larger models and models pre trained with diverse pre trained corpora undergoes less forgetting which shows that diversity of pre trained corpus is a relevant factor during lifelong learning. It further shows that large pre trained models have an implicit bias towards wider loss basin when fine tuning on sequential tasks which ultimately results in less forgetting of tasks when training on subsequent tasks.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 6; However, the local SGD may have a different regime on the dependency of $\tau$. The proposes algorithm is a hybrid method where decentralized method is applied within clusters (on edge devices) and federated average is applied between cluster and server. Empirical studies are conducted to demonstrate the improvement under synthetic federated learning environment. The presentation of the paper is very clear and easy to follow.<|endoftext|>This paper proposes a new Federated learning algorithm which can take advantage of the fast D2D (Device to Device) connections among the devices. In particular, it is shown that the proposed algorithm has better performance in terms of convergence rate and accuracy compared to the existing algorithms such as  Local SGD  based FL which rely only on the slow D2S connections for model updating. Numerical results are provided to show the improved performance compared to the local sgd based FL. The proposed algorithm is novel and the the performance is rigorously characterized. The improvement compared to existing algorithms is significant and hence I believe it should be accepted for publication.<|endoftext|>The paper introduces a new hybrid scheme combining local and decentralized SGD by considering a network with one central node (server) and multiple worker nodes (devices) grouped into clusters with fast intra cluster communication but slow device to server communication. The paper definitely adds to the discussion in Federated Learning and by introducing an interesting new setting of hybrid local SGD. My main concern is that the evaluation is a bit unfair to local SGD.<|endoftext|>Both theoretical analysis and empirical results are provided to show the effectiveness of HL SGD. The paper proposes to use a hybrid model aggregation to conduct federated learning leveraging both high speed D2D network and low speed D2S network. Cons: 1.The main concern I have for the paper is that the proposed idea lacks novelty. Hierarchical federated learning has been studied extensively in the literature [1,6] as well as hierarchical local SGD [3]. 2.The theoretical analysis also seems to make sense, but it would be valid to show the comparison among the convergence rates among this work and the prior methods. Does HL SGD come with any privacy guarantee?
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; The paper proposes and analyzes the convergence rate and optimality of a natural actor critic like actor critic with linear function approximation in linear MDPs. The most important discovery is that once the policy is within the ball of the maximum entropy optimal policy, it remains there forever provided that the critic is sufficiently accurate. I think the paper makes a reasonable contribution to the field. A possible improvement: I am not sure if linear MDP is a necessary ingredient for this work.<|endoftext|>Minor remarks:page 12 line 3 "is just ... and need not be"  > is just ... and needs not to be"A highly theoretical paper underlying an interesting result on the bias of actor critic toward high entropy policies. It is worth noting that this result is obtained without global mixing assumptions on the MDP (only with a mixing assumption on the target policy). But my bet is that it is a good and correct paper. The core of the paper is in the detailed proof which spans 11 pages of appendix.<|endoftext|>The paper provides a novel analysis to the linear MDP setting through an actor critic setup where the policy is softmax parameterized. The claim is that we can avoid mixing time and exploration based assumptions by using this analysis. How should one contrast this with the assumption that the softmax parameterization itself places a positive mass on each state (+ is that even true)? On the downside, there aren t any algorithmic insights I could gather from the paper.
Reject; rating score: 3; rating score: 3; rating score: 6; rating score: 6; This paper proposes a new method for class imbalanced semi supervised learning. However, in my view, the assumption of the proposal is too strong to satisfy and the novelty of the proposal is limited. Experiments show that the proposal can achieve performance improvement. Moreover, the new proposal is mainly based on existing techniques, although combining these techniques can achieve a performance improvement, the novelty and contribution are limited.<|endoftext|>This paper is well written and easy to understand;4. The novelty is limited. In the current version, this paper mainly compares its proposal with CReST+. I am aware that CReST+ is a strong baseline, but it would be more convincing if more baselines could be compared, such as [1,2,3];3. From Figure 3, it is observed that the performance of the model is quite sensitive to the choice of $\alpha_{min}$;4. In Table 4, the results for Supervised models are not informative.<|endoftext|>  This paper tackles the class imbalanced problem with a semi supervised learning scenario. This approach incurs no additional training time on top of the underlying semi supervised learner. Considering the class imbalance scenario is an essential step for applying SSL in a more realistic scenario but has been less explored. ** Although I agree that a considered scenario (i.e., distributions of both labeled and unlabeled are same) is most natural, I wonder that the proposed method can be applicable to more generic scenarios (i.e.,  distributions of both labeled and unlabeled are different). Also, is this necessary for empirical improvement?<|endoftext|>The paper is generally well structured and clearly written. As it stands, I do not see any major flaws with the approach. + In contrast to other papers addressing this problem, the proposed approach relies only on diistribution alignment without sampling based strategies. + Ablations are run to validate each component of the proposed approach. The proposed approach is a combination of two existing approaches, somewhat limiting novelty. Questions:  What happens if the assumption that the unlabeled set of examples is imbalanced in a different way than the training examples?
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; Compared with the cost of time, the improvement is trivial. Their novel approach learns representations via gradient descent directly at inference time after having pretrained feature extractors. weakness:The paper proposes a simple ensembling method that optimizes the self supervised pretrained model. This paper propose a new way to learn self supervised model ensembling.<|endoftext|>2) I have doubts about the comparison with the learned representation vectors with baselines. Pros1) The problem of model ensemble for self supervised learning studied in this paper is interesting and important. 3) Experimental results show that the proposed method can improve the performance of an existing representation from a pre trained self supervised learning model.<|endoftext|>This paper proposes a framework to perform self supervised model ensembling via a novel method of learning representations directly through gradient descent at inference time. The effectiveness of the proposed method is evaluated by k nearest neighbors accuracy.<|endoftext|>The paper proposes a new self supervised ensembling method to get a better feature representation. Below are several experiment that the reviewer is interested. For example, the details of the kNN evaluation. Why use $\psi $, what happens if $\bar{\theta}_{\ell}(x )$ is optimized and used for the inference.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; The authors analyze the known phenomenon that the optimization of the Gaussian likelihood of a probabilistic neural network with heteroscedastic output variance using gradient ascent can get stuck at solutions with a suboptimal mean fit, compensated by large output variance: the likelihood amplifies non uniform initial distributions of feature granularity due to variance weighting, i.e., the gradient w.r.t.the mean are scaled by the inverse variance leading to well fitted samples dominating the gradient. While I judge this to be an interesting contribution on its own, I would like to see an improved experimental evaluation. **Update after rebuttal:**I thank the authors for their effort to improve the experimental evaluation. They compare $\beta$ NLL with standard NLL, MSE, and a moment matching version on several datasets. I raise my score to 6. While the phenomenon of variance weighting is known, the authors provide a nice analysis with explanations, visualizations, and derivations of  high quality. The claims they make in Secs. 3 and 4 are all well supported and fleshed out nicely. The presentation is very clear and well organized. I really enjoyed reading the paper and found it quite insightful. The authors provide a pragmatic and easy to implement method to alleviate the identified problems in a situation specific manner (by adjusting $\beta$). 1 and Tab. S1 seem to be not significant according to the reported standard errors? It is optimizing directly for the mean and thus should yield better results, shouldn t it? But then, I m still wondering why the MSE in the dynamics experiments (Tab.2) is worse than $\beta$ NLL as here you train on the pure MSE loss (not MM). 1/S1?While I agree that the proposed method visually seems to yield better calibrated uncertainty estimates than MM in Fig.8, I would also like to see a quantitative evaluation here. I would be interested in results on more complex, higher dimensional problems.<|endoftext|>The author(s) attribute the under fitting of predictive means by neural networks that parameterize heteroscedastic Gaussian likelihoods to 1) initial inability to break symmetry and 2) Gaussian negative log likelihoods tendency to down weight poorly predicted points. ### Weakness:  Figure 1 attempts to illustrate the problem of fitting a neural network to a noisy sinusoid using a NLL loss. However, a few things could be improved. I understand that the the MM loss for the mean is the same as NLL, but the MM loss also has a variance loss term. I recommend moving discussion of results to appear after the experiments. 1) "For β   1 the gradient w.r.t.μ in Eq.10 is equivalent to the one of MSE. However, for the variance, the gradient in Eq.11 is a new quantity with 2σ2 in the denominator." The author(s) should have MSE in their loss table since I don t believe ($\beta   1$) is equivalent. The author(s) do not compare to ANY of the cited works, which often outperform them on the UCI regression tasks. However, their only support for this a contrived toy example over a single run. The paper would be much stronger if:  The author(s) could apply their analysis to identify either of the two conditions from section 3 in a real world data set.<|endoftext|>The paper identifies a problem with this approach: optimization can get stuck in configurations where the predicted mean is far from the true mean, as this is compensated for by a high predicted variance that also stalls learning. By adjusting the NLL objective, the authors compensate for the problem. **Strong Points**  The discussion of the optimization issue caused by the NLL is very clear with nice synthetic examples. The paper is well written and mostly easy to follow  Table 2 shows clear improvements in performance over the standard NLL approach**Weak Points**  Section 3.1 is difficult to follow. Figure 4 suggests the measurement in Equation (5) is important, but there is a lack of intuition for its significance. No ablation study comparing other approaches to solving this problem. Which optimizers were used and how were they tuned? I understand gradients are not taken with respect to the beta NLL, but I’m curious whether part of the issue is that the Gaussian assumption on the noise is inappropriate. How is the bolding chosen in Table 1. It seems like there’s significant overlap between the performance of the different methods. Currently, it does not meet the bar for acceptance.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; There are several significant strengths and weaknesses in this paper. (Neutral) The parameters of each convolution layer are generated by the generator. Could the authors discuss it with "Parameter prediction for unseen deep architecture"? (Positive) Though the novelty of this paper is limited, the proposed method achieves significant performance gain compared to the existing works. It would be good for the authors to rethink the practical use of the proposed method. (Negative) I understand that the proposed method shares some merits of convolution in weight sharing. But sharing weights in spatial is different from sharing weights in depths. But sharing weights in depths (i) will hurt model performances and (ii) cannot reduce computational cost. Therefore, the proposed method might not be able to be regarded as "Efficient." In other words, although the parameters are saved, the computational cost might increase. Actually, based on my rich experience, vision transformers might have a more severe problem in parameter redundancy in the axis of depth. Maybe the proposed method can do well in ViTs. It would be nice if the authors could provide such studies to prove the generability of the proposed method. (Negative) The inference time should also be added to Table 2, Table 3, and Figure 4 to show whether the proposed method is at a disadvantage. (Negative) From Table 7, we can see that even combined with the network pruning method, the proposed method does not hold an advantage, not to mention that SOTA network pruning methods are not compared with. Most typical network pruning methods focus on FLOPS pruning or the combination of FLOPs and parameters, but not merely parameters.<|endoftext|>In order to reduce size of deep models, this work propose one parameter sharing method for different convolutional layers. With the help of one sharing set of parameters, all convolutional kernels can be generated from the sharing parameters. They show the effeciveness of this method in classification, pose estimation tasks. This paper proposes a method of weight sharing. The authors show that re utilization of parameters generated by their recurrent parameter generator introduces diversity among kernel parameters within a single model. By reusing weights, the model size is reduced greatly. In contrast to conventional vector quantization or just quantization methods, the RPG is not better. However, the proposed method is not easy to do this. If use fusion method, extra computations will be executed. 3.In the paper "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", the compression ratio is neraly 2%. I can not see any comparison in the experiments.<|endoftext|>The paper proposes recurrent parameter generator (RPG) that is able to generate (ideally) arbitrarily large model based on a fixed set of inputs $\mathbf{W}$. There clearly are a lot of things that can be studied here, and mere empirical evidence may not be the only thing that we need. Also, while the results are good, the method is not too different from the canonical hypernetwork idea which is also trained end to end. Experiments show that this new way of generating model parameters is able to perform on par with, or better than, many existing compressive or pruning approaches. On the other hand, I feel the paper has a strong set of empirical evidence demonstrating the effectiveness of RPG and I m surprised by the results. The method itself is simple in nature, although the actual design, such as the motivation for using permutations and sign reflections, remains somewhat unclear (e.g., why not use other ways to create orthogonal matrix?). It is surprising to me how well this method works. 2.The empirical results are relatively thorough, and the improvement over baselines is substantial. Post rebuttal: See my comment below. [1] https://arxiv.org/pdf/1512.03385.pdfAs mentioned in the main review, I m not exactly satisfied with the lack of many potential discussions and further analysis of why and how RPG works. For example, compared to the conventional hypernetworks (which can be considered as a sort of input based parameter generator, and so somewhat more reasonable), what is the new and key ingredient that RPG brings? While this is an empirical paper that presents a surprising finding, I feel that some discussions of why RPG is sensible/reasonable is lacking in the current version of the paper. While the authors acknowledge that they found "this one [setting] to be the best for RPG", I wonder if we might want to do the same for the baseline models. After all, there is nothing "compressed" or "sparse" about the modeling. The fixed generating matrix, while unlearned, is actually still **parameters of the model**, which just happens can be efficiently stored using the seed trick because of the way pseudo random number generators work. 4.I like that the paper gets into a lot of discussions (e.g., quantization, security, log linear DoF accuracy relationship), but it mostly just briefly touched the surface of these topics.<|endoftext|>It is a simple technique that can be applied to any network architecture as the parameter generation is decoupled from the underlying architecture. Post rebuttal update  I thank the authors for their clarifications and additional experiments. The experiments show improved or competitive performance when trained with similar or fewer parameters compared to the baselines. As stressed by the authors, I agree that the contribution of this paper is not limited to compression. However, the experiments focus on the evaluation of the proposed method mainly in the parameter reduction tasks rather than providing insights on why and how it works. The authors addressed our concerns, yet it is still unclear why random sign flipping and even weight assignment are so effective. While the proposed model could be expected to achieve a lower gap between the training and the validation performance due to the smaller DoF, the out of distribution performance improves as well. The proposed technique also achieves better performance by using permutations of the weights in the subsequent iterations, if the base architecture follows an iterative refinement by passing the output of a layer/sub network to itself. The "LegoNet" paper proposes a more structured parameter sharing approach. In my opinion, a comprehensive comparison of the proposed destructive weight sharing approach with different concepts could improve the paper s contribution. Hence, this paper can be considered for acceptance. If my understanding is correct, the permutation P(M) results in a similar random assignment effect. The “even sampling” and the random sign reflections seem to be the main difference in this case. If so, why the proposed technique works significantly better than the random assignment? It is not discussed in the paper. The plot in Fig.1 is not clear.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; This paper focuses on the task of image retrieval using a sample image combined with text modifiers. The modules computing both these scores use attention mechanisms so as to be conditioned on the text modifier. Strengths * Extremely well written and well motivated* A simple but novel idea/architecture* Comprehensive experiments showing the proposed approach achieves state of the art performance on 3 datasets* The ablations clearly show the utility of each of the parts of the already simple architectureSuggestions and Questions * How were heat maps shown picked?<|endoftext|>The paper presents a method for the task special kind of multi modal retrieval: image search with free form text modifiers, where image is used as a query and accompanied text specifies the differences with respect to the given query image which target image should satisfy. It is practically important problem for which several benchmark datasets exist. The paper proposes simple yet effective and practically applicable method. The comparison with TIRG is valid, assuming that the protocol was the same for both ATERMIS and TIRG. The paper presents a simple method for important problem that outperforms recently proposed methods on three challenging benchmarks. The paper is clearly written, backed with experimental evaluation and ablations, where it shows good performance.<|endoftext|>This work combines the image text similarity and text guided image image similarity in a unified framework, and experiments on the Fashion IQ, Shoes and CIRR dataset demonstrates the effectiveness of the proposed strategy. It is somewhat interesting to consider the image retrieval with text modifiers in a unified framework, and the strategy of directly combining image image similarity and image text similarity seems easy but effective. 2.This paper is well presented, with clear organization and detailed visual demonstration in experiments. 3.Another concern might be the framework itself. Although simple and effective, the methodology may not be novel enough: it is more like a simple combination of matching scores and does not provide many insights.
Reject; rating score: 6; rating score: 6; rating score: 6; rating score: 8; They show upper and lower bounds for the risk of classifiers in terms of DB variability under a variety of assumptions, as well as experimentally verify some of their claims. Overall, I think the paper introduces a number of interesting potential ideas for explaining generalization in neural networks. This would allow for a fixed model but a change in other parameters that can affect test accuracy. (2) The usage of BigGAN images to estimate variability seems tolerable but it would be best if there were some additional confirmation that the obvious alternative   data subsetting, say keeping 5,000 random samples from the training set as a new test set and not using these in the training process   would result in similar trends. The authors seem to be referring to the margin at a point x in terms of the largest rho s.t. y * f(x ) has the same sign as y * f(x) for all ||x x || <  rho; these are closely related only if f has a small Lipschitz constant as a function of x. It is definitely the case that large margins in the former sense have good generalization performance (vs. authors  suggestion in p.1); this is shown in standard statistical learning theory. p.4, Sec.4.1: Soudry et al.(2018) isn t really about neural networks; better references would be Lyu & Li (2020) and Ji & Telgarsky (2020) not 2018a ("directional convergence and alignment"). Appendix C: the E^2() notation is weird; does this mean [E()]^2 or E[()^2]? How does one go from (50) (51)? Not clear on first read.<|endoftext|>This paper studies that the smaller variability of prediction can provide a better generalization in empirical and theoretical foundations. The paper considers two types of variability with respect to algorithm and training data. In any case, lower variability ensures higher performance in generalization. The proof and empirical results are sufficient to show the claim. All claims are clarified and clear to understand, the organization of the paper looks good. (1) I have some issues concerning the theoretical aspects. 1.Assumptions 2 3 can be sound. (2) In addition, simulations studies to validate the claims are limited since we cannot know the causality between prediction variability and generalization due to various confounding effects. In this case, I can only know the correlation, not causation. There is a valuable empirical analysis for the prediction variability. However, the more realistic cases should be considered.<|endoftext|>This paper proposes to characterize the generalizability with the variability of decision boundary instead of the margin. They show empirically that generalizability is negatively correlated with the variability and theoretically bounded by it. My main concern about this paper is how well the findings hold in a more realistic setting, given the strong assumptions made in the limited experiments and several confusions in the theoretical part. In this case, the variability does not well reflect the generalization   more experiments are needed to understand this. * Factors that may increase the DB variability while improving the generalization: there are cases where instabilities of the training process leading to better generalization, for example, dropout and large learning rate [3]. Do such instabilities also lead to larger DB variability? 2.The quantification of the DB variability depends on the data generating distribution, which is however vaguely defined in the paper. The authors motivate the data DB variability as "the algorithm DBvariability hardly shows the decision boundary variability caused by changes in training data". It is not clear to me when one is more useful / tighter than the other and should be used.<|endoftext|>Decision boundary variability is measured in two ways. One method depends on the algorithm, or the random seed and reflects how much the boundaries change wen retraining the same network on the same data. The other technique reflects the variability across different amounts of training data. The theoretical claims are supported by some empirical work as well. Several bounds are presented, and while it is stated that they do not depend on model size   implying other existing bounds do   there is no comparison presented. When would these bound be more useful than existing ones? If these questions seem misguided, perhaps a short discussion of why the bounds derived do not need to be compared to existing ones would be helpful. I admit I am not so familiar with this domain and may have overlooked something. The claims are supported by experimental results where appropriate. This paper would be even stronger with more detail/discussion about existing work.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; The authors present a new inference pipeline for neural networks $f$ trained to solve classification problems. Probably this subsection could be shorter, and at least part of the computational cost consideration could go in the appendix. Moreover, I think that some less relevant sections should be shortened and others more relevant should be deepened. Considering the outputs of a neural network as confidences is problematic, this point should be expanded. Yet, to me, the idea of enriching neural networks features with gradient information through a two stage inference process seems novel and sounds interesting. DNNs are known to have unreliable uncertainty estimates (MacKay, 1995, Szegedy et al., 2014, Nguyen et al., 2015)The core parts of the paper are not clear and do not allow a proper comprehension and evaluation of the scientific contribution of this work. The overall idea seems valuable and experimental results appear promising. Yet, with the current manuscript, I can only judge the potential of this work rather than the work itself, so at this stage my recommendation is a 3: reject, not good enough. Results suggest that this research direction can be relevant, so I encourage the authors to pursue this work and focus on communicating it properly. I do not understand why the authors use the projection concept in this context. $L$ seems a tag to indicate the final layer but then becomes a natural number ($L 1$). It is a textual string?<|endoftext|>The paper proposes an interesting and novel approach for improving robustness and calibration under distribution shift. Results on CIFAR10 Corrupted are very promising, and the authors further show the method is able to improve in active learning and OOD detection experiments when inputs are corrupted with noise. Regarding the reasonableness of the assumptions, a  "well trained" network might have high confidence predictions on input data it was trained on, but not necessarily on test data. Empirically, it has been observed that test data (particularly OOD test data) tends to have much higher entropy in predictions. I would also highly recommend experiments to see if introspection (and this approximation in particular) scale well to more complex tasks with more classes like CIFAR100 or Imagenet, as I believe the current set of experiments to be quite limited. I m also curious what would happen if the introspection network is trained on a separate validation set. Some missing related work:Gradients as features: [2] is a crucial piece of missing related work, which also considers using gradients of a network as features, and for example show some similar results in recovering the original classifier performance on the trianing set. **Misc comments:**There s a bit of a weird jump in the assumptions in the paper: lemma 1 assumes cross entropy loss, but in sec 3, we re assuming network is trained with MSE for the derivation.<|endoftext|>In particular, after the feedforward pass, a so called introspective stage occurs, the goal of which is to ascertain why the particular class label was provided rather than a different label. They finally demonstrate how the approach can be used in a number of different applications. Overall, the introspective approach is, to the reviewer s knowledge, novel and potentially of interest as a general method for improving the performance of feed forward networks. The authors provide the reader with intuition (e.g., Fig 2), rigor, and empirical data (Fig 3/4). If possible, I would suggest the authors find such an example, or, if they cannot, to perhaps indicate future work that might improve the technique in such a way that it could in fact do so. Beyond this, the paper is written and organized well and clearly. The authors present a new method, introspection, that can be added on top of feedforward networks to improve accuracy and generalization of the networks. While they have written a well structured paper that provides both intuition and rigor, the empirical results are somewhat lacking in that there are no statistically significant results that support the claims of accuracy improvements that introspection can obtain.<|endoftext|>In this paper, the authors propose a modification to standard neural networks used for object classification tasks to incorporate what they call “introspective learning”. It is not clear the motivation of why including introspective learning in artificial neural networks would improve their performance. Furthermore, standard neural networks are typically optimized to classify an object as belonging to one class and not belonging to the others (all classes are used in the cost function). In that sense, it could be argued that they also are trained to answer the questions illustrated in Figure 1. Because of this it was not clear where the computational complexity of the introspective networks come from. The MLP used is very standard and the introspective features are obtained from the model’s last layer gradients. ## Poor choice of controlsSince adding the 3 layers to the standard model made it considerably worse, the choice of using 3 additional layers on the control for the same number of parameters was a poor choice by the authors. A better comparison would be to make the last layer of the standard model with more features, keeping only 1 layer, or keeping the same penultimate layer and adding a new much wider layer to compensate the smaller number of parameters. Furthermore, the explanation of the method is unnecessarily confusing and better controls could have been chosen to make a more convincing case.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; This paper performs a large scale study of transfer learning in GANs. It proposes a way to understand the relevance of a pre trained generator and discriminator, as well as heuristics to select good source/initialization dataset and even a training snapshot. All of this is very valuable for the practitioners. Overall this feels like important work that should to be accepted. This paper answers important questions in GAN transfer learning, and therefore makes a solid contribution.<|endoftext|>This paper studies the problem: when, why, and which pretrained GANs are useful. In general, this paper has conducted a series of experiments with the stylegan2, and got some conclusions, which are helpful for subsequent papers and the community. This paper makes some conclusions, which could promote the development of adapting pre trained GANs to different domains.<|endoftext|>This paper aims to achieve a better understanding of GAN fine tuning. **Strengths:**    Well written and easy to understand. Transfer learning for GANs is an important subject. Why is this? Paper provides useful insights about the role of the generator and discriminator in GAN transfer learning, as well as the behaviour of transfer learning itself (i.e., improved coverage but not fidelity).
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; rating score: 3; 3.Developed a continual learning approach, trust region gradient projection(TRGP) based on the introduced Trust Region, scaled weight projection and a module to construct task input subspace. In this example, the only difference between task 1 and task 2 is the sign of the input. 3.As said in the paper, trust region aims to select strongly correlated tasks. 4.In the Session 4.2, this paper says that if task t is strongly correlated with task j, the weight projection on subspace of Sj is important for task t. This view seems like an assumption which is not proved. This paper put forward the main problem of existing methods and provides a detailed discussion.<|endoftext|>In that case, we know that the essential parameters for Task A are likely to be important for Task B and that the network could benefit by continuing to update the important parameters for Task A, which is not allowed in GP algorithms. The paper addresses this issue, with a simple, yet practical, solution. This would presumably lead to the least loss for the current task (as it provides the maximum capacity for the task). If so, why should one optimize $Q$ as opposed to simply fixing it to unfreeze all parameters in the trust region? This reduces both forward and backward transfer, especially when the tasks are similar (e.g., revisiting an old task). From an editorial point of view, the paper is well written, easy to follow, and it provides a good overview of the recent literature on the topic. Hence, I think this is a good paper and vote for its acceptance.<|endoftext|>In this paper, the problem of forward knowledge transfer in continual learning settings is explored. The idea is to measure correlations between the learned tasks based on the notion of "trust region" which helps to identify the most similar learned tasks to the current task. Experiments on four benchmarks are provided to demonstrate that the method is effective. 2.The paper reads well. Experiments are not extensive enough and important benchmarks are missing. 2.Thorough comparison with recent works is missing. Given the expectation in the recent literature on continual learning, only relatively simple datasets are considered in the experiments. For a realistic comparison against prior works, state of the art methods for each group of methods should be included.<|endoftext|>This paper proposes a continual learning method based on gradient projection memory (GPM) of Saha et al., which projects the gradient of each layer to be orthogonal to the input subspace of previous tasks. I was confused if they are going to allow or restrict the model update in the trust region. The authors claim that orthogonal projection is problematic because the optimal model for task 2 should be $W_2^l    W_1^l$. The proposed method is just one way of relaxing the orthogonal projection, and there is not enough justification for why this particular algorithm should be effective.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; ### Post discussion and updated versionI believe that authors have done a good work in the discussion and answering my concerns. I would have liked to see an analysis of the effect and equilibrium with systems of larger number of agents, but the current work has more than sufficient analysis and contributions on its own as reflected in the additional ablations and discussion incorporated in the new version.<|endoftext|>The authors propose a new curriculum generating algorithm for goal conditioned agents. The main question for me that arose was: Where does the difference in Alice and Bob s performance come from? Is it from their different initializations? It does a good job of explaining it s algorithmic choices with toy experiments and shows a reasonable bump over other curriculum methods on a series of goal conditioned tasks. However, I m left with the question of why the method works at all and if there s a better formulation that achieves the same thing (see my first question in the main review). If this were clarified and some analysis included in the paper I think it would be a much stronger submission and would be a clear accept; in this case I would be more than happy to increase my score.<|endoftext|>If would be better if the authors can provide some discussion about the general applicability of the proposed method. 2.The work is mostly based on empirical study, the paper could be strengthened if some theoretical analysis, such as sample/computation complexity, convergence etc. Other comments:1. In all, this paper is well written and provided a novel formulation for goal conditional automatic curriculum learning. The paper could be strengthened if its use cases can be extended to more general curriculum learning settings (beyond goal generation).<|endoftext|>(3) even if one solved a Nash of this game that wasn’t degenerate, it’s not clear to me why that would be a good curriculum. I don’t see how the given method satisfies the desiderata (progressive diversity, progressive feasibility and anti forgetting). The results look decent, but do not perform much better than uniform (domain randomization) on most tasks. I would be interested to see how well PAIRED does on these tasks. I like this direction but I am worried that the main objective has problems that can prevent it from being an effective curriculum, even if they don t show up in these particular results.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; The paper presents a strategy to solve the problem while the experimentation showcases different aspects of the solution. The authors present the inter functional relationship with a directed graph. The binary relationships are represented with a prior network. Processing all the objects results in a scene representation of a functional relationship. Their procedure for training an exploratory policy allows them to generate data to construct a posterior network of the functional relationship in the scene. 3.The paper introduces a strategy based on PointNet++ to infer the binary inter object functional relationship, a graph convolutional network to model the scene and predict the functional graph for the scene. How does one scene differ from the rest?<|endoftext|>Writing :S: The writing is good. The technical summary of the paper is given below. Goal : Learning inter object functional relationships, represented by scene graphs. Here S is the scene and R_s is the relationship scene graph for the scene, S. Assumptions :Object segmentation and state changes (by "state", the authors refer to the functional outcome of an object) are provided to the agent apriori. The proposed method is a modular approach with simple networks, which is quite easy to follow. Novelty :W: Problem statement   The claim is that the problem is new.<|endoftext|>Strengths: The task is interesting. Although I do not think exploring inter object functional relationships in 3D indoor scenes is a new task (some previous works about causality modeling proposed similar tasks, such as "Learning Perceptual Causality from Video"), I appreciate the collected dataset and the designed pipeline of learning the relationships for inter object. Weaknesses: Some details are missed which may affect the reproducibility o the work. I am not sure what "fast" refers to or what makes the framework fast. It is nice if the authors could clarify this point. For example, I am wondering what is the performance of the framework on the many to many relationship prediction. The paper is well organized.<|endoftext|>While the ablations are interesting and well done, the lack of other comparisons is a weakness. There also aren t comparisons in the accuracy of different types of relationships (toggleable, many to many, one to one, functional, inter object, etc). The experiments on transferring between environments are interesting and is an important ability for such an approach. The paper would benefit from some proofreading and editing, there are some sentences with grammar mistakes, especially in the introduction. Overall the paper is well motivated and the problem is interesting.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 8; This paper presents a new approach for detecting topological structuresin 2D and 3D imaging data. When discussing persistent homology, consider citing a survey or  a text book, such as the one by Edelsbrunner and Harer. I urge the authors  to reconsider the title of this paper. There are three main issuesI have with the current write up:1. I understand that it does not extend to 3D data, but if this is to     be the deciding property of the proposed algorithm, additional     experiments on 3D data sets are required. The DMT method is applicable to 2D and 3D and [code    appears to be available](https://github.com/HuXiaoling/DMT_loss),    so it should be part of this comparison, in particular since the    paper claims that the proposed algorithm outperforms *all* PH based    loss functions.<|endoftext|>However, the common filtrations proposed in the literature (pixel values, distance maps) for doing so have their own flaws and are not always suited for such images, so the authors propose a new filtration that combines those common filtration with filtrations based on heights, i.e., with the persistent homology transform. The proposed approach is definitely promising, but I think that the work is too incremental for now to be accepted as is. There is no real theoretical back up and the results, while being OKish, do not empirically justify by themselves the approach (to my opinion). Moreover, some parts of the writing could be improved.<|endoftext|>Relying on recent progress regarding the (automatic) differentiation of « Persistent Homology » (a tool of Topological Data Analysis (TDA) to extract topological descriptors on top of complex data), this paper proposes a new filtration (essentially, a sum of the distance filtration and the height filtration) to favor proper topological reconstruction in the context of image processing (segmentation in particular). The authors experimentally showcase their approach on different (2D and 3D) datasets and compare it with various competitors (from and outside TDA based techniques) and obtain very competitive results. Overall, the flow of the work is clear and the paper is pleasant to read. Simplifying this by instead looking at one (or many!) for the "matrix reduction algorithm" to compute PH is not very accurate as well. Calling "a homology $g \in H_{X_1}$..." a point in the persistence diagram is not accurate. However, it contains some inaccuracies regarding TDA concepts/literature that must be corrected/discussed.<|endoftext|>The proposes a localized version of topological loss. Current methods perform matching in the space (i.e.persistence diagrams) that does not account for the location of topological features. The method proposes to add to the likelihood maps some sort of positional embedding, i.e.a function of image pixel coordinates chosen at random during training. The experiments demonstrate improved performance. The paper addresses an important problem of training network producing segmentation with desired topology. The paper builds upon an established framework of persistent homologies (PH) and extends it by adding positional embedding. The topological loss based on PH is quite complex in the implementation. How does this compare to the proposed method? DO the authors explicitly compare to the patch based approach? The paper is addresses an important problem of topology aware segmentation learning. The experimental evaluation shows improvement upon global methods.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 6; rating score: 6; In this paper, the authors address the problem of fingerprinting GAN (StyleGAN2 in the proposed setting) in a scalable way, e.g.after initial processing, arbitrary fingerprints can be added. This paper presents a set of metrics for evaluating the tasks of GAN fingerprinting, which is important as such tasks are still new. The authors also provide a set of metrics to evaluate GAN fingerprinting techniques, and accordingly demonstrate the superiority of the proposed method. Overall, this paper is well structured and easy to follow.<|endoftext|>This paper addresses an interesting problem about fingerprinting data generated by GANs, in order to be able to trace any misuse of deepfake. The generative model is jointly trained with a decoder whose aim is, given a potentially fake data, to recognize which fingerprint has been used to generate the data. This paper addresses an interesting problem, grounded on concrete and impactful issues raised by the emergence of deep fakes.<|endoftext|>This work aims to provide a method to tackle deep fake by making the samples generated using the generative adversarial networks (GANs) contain a user specific fingerprint that can be accurately detected and attributed to each user. Theoretically, this model only needs to be trained once, and a large population of models can be instantiated, each of which has a generator modulated using a different fingerprint. The generator in the proposed GAN is modulated with different fingerprints on the fly.<|endoftext|>Aim of this work is to develop a method to fingerprint GAN models. In this way, images generated from that model can be detected and attributed to a specific GAN model. I cannot find an ablation study in the experimental section that shows the importance of such terms. This work aims at solving the problem of deepfake misuse. The concept of fingerprinting generative models is not new. Also note that in Yu et al.authors talk about a proactive and responsible disclosure of pre trained GAN models. Suppose for example that the bad actor builds its own generator without a fingerprint. Many important points have been clarifiedand the paper has been updated accordingly. In addition, the main contribution as well as the scenario of interest have been better explained.<|endoftext|>The authors demonstrated that the model fingerprint can be detected from its generated images with high confidence while retaining the synthesis fidelity. The paper is well written and easy to follow. Nevertheless, the authors carefully state a user case that motivates their approach in the Introduction section. Overall the paper introduce an interesting approach to address the problem of synthesised image detection/attribution. Although the proposed method certainly has merits and applications, I feel that the paper has slightly different terminology of image detection/attribution versus the literature.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; rating score: 5; In this paper the authors pose the problem of relational learning in the form of a Probabilistic graphical model and utilize variational inference for learning the relational property. The authors claim in the introduction that they formulate the problem of relational learning as a PGM which seems misleading. 2.The relational learning problem needs better discussion. In particular, the transition from the definition of relational learning to the probabilistic formulation could use an example. Other than this, I believe that this is an interesting paper with a novel formulation of the relational learning problem and extensive experimentation.<|endoftext|>### Strength  The relational learning problem is clearly formulated in the paper, and seems novel and interesting. It is quite surprising that it does so badly on Omniglot. This is basically directly learning the transformations. Depending on how the STN is specified, I can imagine different outputs of the trained localization network that lead to the same transformation. The VRL PGM is not symmetric w.r.t.a and b.Does that affect the model in any way? To make the comparison fair, for VAEs for example, in addition to training VAEs on the given training data, there should be additional supervision that the latent state for a pair of samples and the latent state for the pair of samples transformed by a relation preserving function should match. If VAEs perform well with this additional piece of supervision information, then the good performance of VRL is more from the way RPDA is used, and less from the method itself. I have some questions/concerns about the paper so recommend weak reject, but would be happy to bump up the score if my questions/concerns can be satisfactorily addressed. The authors have satisfactorily addressed my concerns, and I bumped up the score accordingly.<|endoftext|>The main contributions areA novel formulation of relational learning as a variational inference problemA novel augmentation method for relational dataEmpirical results that show the ability to learn relationships in images. A new relational learning approach is proposed based on variational learning. The main idea is to disentangle absolute and relational properties and force the learner to learn based on relationships rather than absolute properties of instances. The paper has an interesting idea and I liked the formulation of the variational inference problem for relational learning. On the other hand, the RPDA functions need to encode the rotational aspects so it does require some specialization. After author feedbackI thank the authors for their feedback. The idea definitely seems good, maybe with a bit stronger comparison/ empirical studies, this would be a stronger paper. However, the relationships in the experiments seem a bit forced and maybe simpler data augmentation methods could replace the relational augmentation methods used.<|endoftext|>This paper introduces a variational method for relational learning. Strengths:The problem set up is novel, and the paper addresses an important problem. Update:I have read the reply from the author as well as other reviewers  comments. It is understandable that the 4 conditions are hard to achieve simultaneously, and I m satisfied with the authors  response. The authors may need to improve its justification of the PGM omitting condition (ii), or find ways to make it consistent with the proposed 4 conditions of relational variable.<|endoftext|>This paper proposes a relational learning method based on variational Bayes. However, there are other approaches that are not mentioned here which try to find "motifs" in graphs or perform link prediction, where the objects (without their properties) are represented as nodes, relations are in the edges and new links are learned from the original graph. The model theory is discusses where authors define the problem and discuss about solutions for some limitations of the model when a relation between objects A and B can be found by only looking at one of the objects own properties.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; On the other hand, several issues with the clarity of the presentation (especially in Sec.4) obscure this main message. The paper presents a novel approach to analyzing and comparing feature manifolds, e.g.from a learned embedding. The authors addressed most of my concerns in their response and evidently took the time to implement many of the reviewers  suggestions in the updated draft of the paper. I believe that the method has a strong motivation and the presented results are encouraging.<|endoftext|>To that aid, the main method proposed in this paper proposes to alleviate these drawbacks and comprises of 3 steps: 1.) manifold estimation, 2.) The authors demonstrate their metric in three different setups: (A.) The complexity of constructing the delaunay graph in high dimensions can be a computationally expensive affair. All in all, I perceive this to be a good paper.<|endoftext|>This paper proposes a new method to assess the quality of learned data representations. The paper could improve on many aspects   presentation of the experiment section, intuition behind the proposed method, discussion on hyperparameters and most importantly complexity   however its core contributions are valid albeit with limited novelty. How much of an issue is that in general when we have high dimensional data compared to the complexity of the method?
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 8; This paper proposed a novel idea that generates imperceptible adversarial attacks for ASR by perturbing the phase information. Proof that phase perturbations reduce the magnitude of the spectrogram is provided. The main strength of the paper is the novelty of the energy dissipation. It is not shown in the paper what this claim is based on. The effectiveness of such adversarial attacks is certainly related to the detailed model architecture and how it was trained. ...In conclusion, the paper presents a novel idea to perform adversarial attacks for audio based systems, but the experiment setup and results need improvement.<|endoftext|>This paper proposes a phase oriented algorithm PhaseFool to efficiently construct imperceptible audio adversarial attacks with energy dissipation. The authors leverage the spectrogram consistency of STFT to adversarially transfer phase perturbations to the adjacent frames and dissipate the energy that is crucial for ASR systems. Empirical evaluations show that the attack effectiveness of the proposed attack is high. However, the evaluation of the proposed attack in the paper is a bit weak. Overall the paper is well organized and the proposed method is interesting.<|endoftext|>The experimental results show that the proposed phase oriented audio adversarial samples can be produced in reduced time as well as they are less perceivable by humans. 4.I am not fully convinced that an ASR model which does not use an STFT or a log mel representation would also be vulnerable to an attack such as the one proposed in this paper. Although the idea proposed in the paper is interesting and could have a great potential in the way that adversarial examples are generated for ASR systems. However, the current version of the paper displays only a limited experimental setup of the proposed algorithm that does not help to show the significance of the algorithm.<|endoftext|>This paper proposes a novel adversarial attack method that attacks ASR network. The perturbation on phase information influences the magnitude information which is known as energy dissipation. The advantage of the proposed method is that it can generate the imperceptible perturbation using fewer steps than the previous state of the art method. 2.The authors theoretically show that the phase based perturbation leads to energy dissipation. 3.The experiment results are good, and is even better than the magnitude based method. It could have been nicer if the authors had experimented with the raw waveform based ASR networks. 2.It could have been nicer if the authors had experimented the method on more diverse audio NNs. All in all, I recommend to accept the paper.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; I would recommend bolding important figures in the tables. I would also encourage the authors to repeat their experiments with different random seeds and report error bars. **Clarity and Writing** The introduction and background sections of the paper read very well. I would say that simple concepts are explained in an unnecessarily notation heavy way at times. I would suggest moving this information to the introduction or related work.<|endoftext|>Given that it seems like the authors have unfortunately implemented none of these suggestions yet, I will for now have to stick to my previous assessment of the work and recommend rejection again. In fact, the proposed method underperforms the baselines in many settings, which may or may not be significant.<|endoftext|>The authors propose an algorithm scheme that they prove provides probabilisitic asymptotic guarantees of `distribution  calibration for models. For UCI you say 25% is used for testing and the rest for training. I do not currently recommend acceptance. Overall, I liked this paper.<|endoftext|>Theoretical analysis and empirical experiments are conducted to support the claim that this is a way to calibrate "any model" at "almost no cost". The method is simple and generally applicable. Specific Questions/Comments:   I am confused by the definition of the check score in section 2.2. In summary, the paper provides a simple method that empirically generally improves model calibration.
Reject; rating score: 3; rating score: 3; rating score: 5; The paper proposes a linerization method for ricci flow that is used for manifold surgery which makes the process more robust. The main idea of linearizing the Ricci flow to increase the robustness sounds meaningful to me, and as far as I can tell the math behind it looks convincing (though I did not check all details). It is not clear to me what the method has to do with the topics of ICLR, there is no mention of which papers use Ricci flow for anything, and the results are also not clear in how this method is used on CIFAR exactly. I see that this is a very basic theoretical work, and I would have been fine with experiments on artifical and simplistic data but the complete lack of any positive results does not raise faith. However, the paper does not explain the impact of its contribution or its applications well, does not have a proper related work section, and the results are half hidden in the appendix and their positive aspects not explained.<|endoftext|>The paper concerns the Ricci flow and surgery to handle singularities occuring during the Ricci flow. The authors propose linearly nearly Euclidean metrics that they prove are stable under the Ricii DeTurck flow. They continue to define nearly Euclidean metrics and prove various properties of these. The sections on divergences, gradient flows and approximations with neural networks are largely unreadable. What do the authors mean by sentences such as "we dynamically consider the gradient flow followed with the optimal descent direction on this manifold" and "Therefore, this gradient flow is a weak approximation under the manifold micro surgery."? Because of this, it is not clear what is the contribution and point of the paper besides a survey and discussion of some very exciting mathematical topics. Unfortunately, while I very much appreciate the topic and the underlying mathematics, I find the paper and presentation in its present form to be too unclear that I can recommend acceptance.<|endoftext|>This paper investigates linearly nearly Euclidean metrics on Riemannian manifolds under the Ricci flow. In contrast, the utility of the analysis for training neural networks using gradient flow is of major importance to the community, but it is not well treated in the paper. The paper presents seemingly new theoretical results and sets the stage for the analysis of gradient flow used for training neural networks. On the one hand, this is a theoretical paper presenting new analysis, which is interesting and is a contribution by itself.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; rating score: 6; Minor：3) The table captions are confusing, especially those on Page 8 and 9. In my opinion, the proposed method is sound under the specific setting of this paper.<|endoftext|>I am interested to see how the performance changes with this "autolabeling" setting. The paper is well written and the experiments clearly show that each of the proposed components is effective (Table 5). This paper proposes an effective method for an important but under addressed issue. The proposed method is different from [Zakharov et al.2020], but it is unclear which is better.<|endoftext|>What is the time cost in the training period since it is involved with the large computation of ray tracing. + The writing and organization of this paper are good.<|endoftext|>6) An inherent limitation of the method is the handling of occlusion. Also, this dataset also comes with evaluation metrics for translation, orientation, separately, which could provide more insights. Therefore I am leaning a little positive despite there are several concerns that I invite the authors to address in the rebuttal. 5) The metrics used for experiments are rather inconsistent. I am confused about the switch from AP11 IoU 0.7 (Table 1) to AP40 IoU 0.7 (Table 2), and then to AP40 IoU 0.5 in Table 4 and the ablation study.<|endoftext|>The most critical concern is motivation. + Organization of the paper is clear and easy to follow. Overall, this paper is in good shape.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; The paper aims to boost randomized smoothing (RS). Specifically, the paper demonstrates that an ensemble of diverse base models can enhance RS both theoretically and empirically. The key insight is that reducing variance of ensembles over the introduced perturbations can lead to more consistent classifications for inputs. Extensive experiments are conducted to thoroughly evaluate the proposed boosted RS. However, training an ensemble of models requires far more computational resources and time than training an individual model. Or y_p and y_c are just for analysis? My another concern is that the experimental results are only for certifying L2 perturbation.<|endoftext|>In this paper, the authors propose using the aggregation of an ensemble of similar models as the base classifier in the randomized smoothing (RS). Both theoretical arguments and numerical experiments are included to support their idea. 3.They proposed Adaptive Sampling and K consensus algorithms to reduce the computational cost, making their method more practical. The underlying topic of constructing a certified robust classifier is important. The main idea of the paper is sound and is supported by extensive experiment investigations.<|endoftext|>This paper integrates model ensembles with randomized smoothing to improve the certified accuracy. The methodology is motivated theoretically by showing the effect of model ensemble on reducing the variance of smooth classifiers. Moreover, it proposes an adaptive sampling algorithm to reduce the computation required for certifying with randomized smoothing. Extensive experiments were conducted on CIFAR10 and ImageNet datasets. The Adaptive Sampling proposed in section 6 algorithm is practical. They cover several frameworks for training smooth classifiers and several datasets.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; With the above assumptions, in essence the authors explicitly "hand code"     specific "invariance" and "equivariance" properties, namely,    rotation/translation invariance and angle preserving     transformations as captured by Eucliean and conformal groups, in AGN. I presume that     they are not defined on per node or per edge basis. This may not hold in general at all. For     example, the nodes may lie in a sphere in an ambient 3 D     Euclidean space, and the "right" distance between the nodes should be the geodesic distance on the sphere, not the ambient Euclidean distance. The same comments also apply to other equations, such   as the transformation of the node coordinates will be based only   node coordinates, but not other node features.<|endoftext|>This paper proposes two kinds of equivariant GNNs, including the distance preserving graph network (DGN) that is equivariant to the Euclidean transformations and angle preserving graph network (AGN) that is equivariant to the Conformal group. This paper is well written. The authors have introduced the necessary conceptions in a comfortable way. The novelty is somehow limited. The authors attribute this detriment to the lack of symmetry in the data. Previous studies such as group CNN have also demonstrated further taking the rotation equivariance into account delivers desirable enhancement. The authors are suggested to provide more explanations on this issue.<|endoftext|>The authors propose distance preserving Graph Network and Angle Preserving Graph Networks which are equivariant to node permutation as well distance preserving transformations/ angle preserving transformations of the coordinates associated with the nodes. The authors generalize the E(N) GNN work to include group symmetries to the conformal group and to the case when not all nodes in the graph are connected. 2.The authors do not present other use cases (or motivations of this work) of the symmetries to the conformal group or distance preserving transformations of an object   for example results on point cloud datasets (where the objects are not rigid bodies) would do well for this work.<|endoftext|>This paper proposes two graph networks that are equivariant to distance and angle preserving transformations in graph coordinates. The authors conduct experiments on synthetic data to demonstrate the effectiveness of considering symmetries. The motivation of this work is clearly explained and convincing.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 8; Thus, if the authors can highlight the contribution of this work, especially compared with [1], I am willing to raise my rating. However, the black box defense setting considered in this paper has been studied in [1]. [1] Denoised Smoothing: A Provable Defense for Pretrained Classifiers. In fact, I have felt that.<|endoftext|>The authors formulate the problem of black box defense and propose a novel black box defense approach called the Zero Order AutoEncoder based Denoised Smoothing (ZO AE DS). 3.The paper is clearly written and easy to follow. After all, black box defense is designed to solve the problem in real world scenes. The experiments are all compared with DS and its variants. Hence, I am increasing my score to 8.<|endoftext|>This paper proposes a novel approach to robustify black box models to address the problem of black box defense, which arises due to the concerns of privacy. Although the method seems doesn’t outperform FO version with surrogates, I think it has broader applicability owning to its minimum assumptions. Compared to the competing method FO DS, the authors eliminate the need for white box model by applying zero order optimization, and also use an autoencoder to avoid dimension issues for calculation. Experiments are thorough and well organized.<|endoftext|>The argument for the use of the Autoencoder is intuitive and clearly works. I did not see this discussed in the paper. A comparison would be useful. Overall, I find the algorithm to make sense intuitively and the experiments are quite thorough. ~However, I have a few concerns and clarifications. If they are resolved, I would recommend the paper to be accepted.~[Update]: I am satisfied with the author s response.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; Based on these studies, several guidelines are proposed to design deep model named DGMLP. (3) The paper is well organized. I know the authors derive these guidelines based on formulating the concepts of D_p & D_t and conducting the extensive experiments to get Findings 1. (4) Some of the claims made in this paper is not correct. However, I cannot find much novelty from this paper, which seems to ensemble the existing knowledges.<|endoftext|>In this work, the authors performed an experimental evaluation on several GNNs in order to understand what aspects of the current architecture designs that leads to the compromised performance of deep GNNs. They claimed to find the root causes: large propagation depth leads to the over smoothing issue and large transformation depth leads to the model degradation issue. It would be more convincing if the performance of DGMLP improves with a larger D_t. Otherwise, why bother to increase D_t? The experiments, which compare various GNNs, provide useful information. The proposed DGMLP contains interesting ideas. However, experimental results are mixed and do not clearly support all claims made in this manuscript.<|endoftext|>The performance of DGMLP is not very strong. (+) The paper is well organized and easy to follow. I enjoy reading the design of the analysis. Concerns:* The authors claim model degradation is the main cause of performance degradation of deep GNNs. However, what is model degradation is not defined formally. * It seems the proposed Node Smoothing Level is a measure of **smoothness** instead of **over smoothness**. I think a better metric should be used. ( ) Some claims are not well justified.<|endoftext|>It isn’t clear why the authors don’t refer to these as hyper parameters (except buried in the appendix). The main body of the paper seems repetitive at times, stating repeatedly the basic points about the tradeoffs with Dp and Dt. In the reviewers opinion, the meaningful portions of the paper are in the appendices, especially A and B. It isn’t clear why these don’t make up the major portion of the main paper. However, the influence here wasn’t entirely clear. The paper would be significantly improved with a statement of the DGMLP in some kind of Table or Algorithm statement.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; The paper introduces an algorithm to defend against backdoor data poisoning attacks by leveraging noisy label defense algorithms. The motivation and the derivation of the algorithm seem reasonable, although the proposed algorithm has some parameters whose selection can be critical for the performance on the clean dataset and the backdoors. Perhaps the authors could shed a bit of light in this sense during the rebuttal. My main concerns about the paper are on the experimental evaluation: 1) The results in Tables 1 and 2 shows that the proposed method has a significant negative impact on the performance of the algorithm on the clean dataset, which limits the applicability of the method. Thus, setting the value of the parameters of the algorithm is not trivial. 5) The experiments only consider CIFAR datasets. The motivation for the proposed algorithm is interesting and promising.<|endoftext|>The authors propose a defense against backdoor data poisoning attacks by leveraging (reinforcing) existing defenses against noisy label attacks. 2.The connection between the noisy label attack and the backdoor attack has been well explained and later well exploited to arrive at the formulation and also the AT based minimax optimization. Cons:1.One of the main issues I have is with the evaluation. This also goes back to the assumption on the existence of at least one noisy label defense that the authors mention in Pg. In this regard, the numbers in Table 2 (for example) when $\epsilon 0.45$ are interesting. I could not understand the clean accuracy numbers in the evaluation.<|endoftext|>This mainly consists of two attacks, BadNets and Blending attack. ### **Strengths**:— The authors present a novel optimization based approach to defend against poisoning attacks. The proposed approach is not compared with previous defenses for noisy label attacks such as Spectral Signature defense. — The organization of the work can be improved. — **Minor**: Some typos are present in the submitted version which can be corrected. The authors presenting an interesting approach towards defending against backdoor attacks involving noisy label algorithm.<|endoftext|>The authors propose a meta algorithm by adding noisy labels to the training set and optimizing outer minimization and inner maximization of the model by the noisy label algorithm. Combined with adversarial training, the authors show their method could improve the robustness of models against backdoor attack. Pros:By adding noisy label to the training set and performing adversarial training at the same time, the noisy label defense algorithm can be directly utilized to defense backdoor attacks. The authors did not provide any discussion. 3.The organization of this paper should be improved. The analysis on experiments is short and incomplete. The range of the ∑ on the right hand side of the first equation in the derivation should be i∈G\R.
Reject; rating score: 3; rating score: 3; rating score: 6; rating score: 8; rating score: 8; The paper studies the problem of propagating adversarial robustness in federated learning. Please see the detailed comments above. Adversarial Training (AT) is typically used for training models that are robust to adversarial examples, however, AT requires computing resources that are not always available at all devices participating in FL. Pros:a.The authors propose Federated Batch Normalization (FedRBN) to transfer robustness between users participating in FL. d. I like the choice of datasets used by the authors to perform the evaluation. There are numerous typos, and certain parts of the paper could benefit immensely from more clarity in writing. 2.The contributions of this paper are marginal. This paper proposes a mechanism to transfer BN statistics between users. In this context, Algorithm 3 (in the appendix) is very important. However, confident detection of adversarial examples is still an area of active research. It would be great if there could be more consistency in notation.<|endoftext|>This paper studied the propagation of adversarial robustness among federated learning users, where some users had limited training data and computational budgets for affording the adversarial training. The rationality of technical details used in this paper is not well explained, thus I recommend the rejection of this work for ICLR. The experiments supported that this approach improved the adversarial robustness of federated learning. (2) Compared to FATBN, FedRBN used the robustness propagation from AT user to ST user. Or the performance improvement can be explained by the DBN and the noise detector in FedRBN? "Federated multi task learning."<|endoftext|>This work studies adversarial robustness in a federated learning context. Why do FATAvg and FATBN show lower robust performance than FedAvg and FedBN? Why do the numbers of FedRBN from Table 2 and Table 3 not match? I recommend the authors to include results at least for a ResNet architecture. * The paper is overall well written and easy to follow. * The authors provide extensive experimental results. Robustness is an important issue also in the federated learning context. In essence, the authors just apply previous insights, such as the distributional shift caused by BN and adversarial training to the context of federated learning. The proposed noise detector seems to be a DNN based binary classifier, similar to the approach in [5]. * The used baselines are not clear to me.<|endoftext|>(specifically, would it perform well in the presence of more devices?) The authors want to find a good way to propagate adversarial robustness from high resource users to low resource ones. Overall a good paper, but I am still giving the “marginally above” score due to some questions/concerns as given above; however, upon getting reasonably satisfactory answers, especially to the convergence analysis question, I will be happy to move higher to the positive side. Experimental results including various ablation studies confirm the advantage of FedRBN. The effectiveness of the proposed method is validated by extensive experiments and ablation studies. The effect of debiasing seems to be marginal in Table 1. While writing is good overall, certain points are vague. This is not clearly described, and I believe the authors assume all users participate in each round.<|endoftext|>Extensive experiments are conducted and the results show that the proposed method outperforms other baselines. This paper aims to improve the adversarial robustness of federated learning with heterogeneous users. The authors consider both the statistical heterogeneity and the computational heterogeneity in federated learning, which is a realistic scenario. Since robust training is often too expensive for edge users, the authors studied a novel setting of practical relevance: users with limited restricted budgets only afford cheaper standard local training and obtain robustness from other budget sufficient users who use adversarial training. I would like to know whether FedRBN still work under label heterogeneity, which could be a more challenging heterogeneous scenario.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 5; [3] Hu, Weihua, et al."Open graph benchmark: Datasets for machine learning on graphs." OOD generalization of the graph is not trivial because the graph has the interconnection among nodes and the existence of structural information. To solve the problems, this paper proposes a new method, multiple contexts explore that are adversarially trained to maximize the variance of risk. The proposed model is validated on many diverse datasets, Cora, Amazon Photo, Twitch explicit, and so on. [3] Krueger, David, et al."Out of distribution generalization via risk extrapolation (rex)." "Environment inference for invariant learning." OOD generalization for graph neural networks is important, but it is a relatively unexplored research area. This paper will be attractive for many graph related machine learning researchers. 2.OOD for graph dataset are not trivial because of their own unique characteristic of the graph structured dataset (for example, there are only one graph), but the proposed approach, "Explore to Extrapolate Risk Minimization",  that adopts K context generators solve the problems in a smart way. There are some related graph OOD works [1,2]. Especially, this paper seems similar to [1], and a detailed discussion will be needed. 2.The baseline methods for experimental results are not enough.<|endoftext|>If yes, can you provide some insight why? If not, is it an orthogonal issue such as not properly tuning the hyperparameters? The paper is well written, polished, and easy to follow. When training with multiple graphs is the assumption that we have a single environment? The problem that is studied is important and relevant for the community. The theory is sound, however, the technical novelty is limited. The proposed approach is well motivated. Once the input graph has been decomposed into a set of ego graphs the definitions, formulations and theory are all straightforward adaptions from the respective versions for IID data. Nonetheless, there is some value in doing this. It is not clear how much of the apparent benefit is due to the new objective (i.e.the addition of the variance term) vs. the (inherent) benefit of data augmentation. In other words, is there a price for using EERM?<|endoftext|>This paper studies the problem of distribution shifts as out of distribution generalization. Specifically, it formulates the OOD problem as invariant risk minimization under different environments. The relation between these two has been extensively discussed in the paper. Multiple environment is done by graph editing using policy gradient. ** Besides OOD on graph and other domain, there are couple of work [1,2] study the generalization from non I.I.D training data in GNNs. 1.**Technical writing is dense and hard to follow**In Figure 1, the context generator is actually not generating a new graph, which instead augment the input graph. 1.**Assumption 1 seems to be unrealistic. **In practice, do we really have the invariance property? I feel this assumption is the foundation of the paper that tries to bridge OOD and invariant risk minimization. For example, after graph editing, it would be hard to obtain invariant property. After reading this paper several times, I start to understand the underlying connection between OOD and invariant property.<|endoftext|>  This paper tackles the out of distribution problem for node level prediction on graphs from the invariance perspective. Although the presentation is well organized and easy to read, the limited novelty and the lack of excitement bring me to this recommendation. All the ego graphs can be treated as a set of IID. This paper developed a new approach to tackle a problem that has already been discussed. This work extends the discussion of the OOD problem to node level tasks on graphs. A new learning approach is proposed and theoretically proven to be correct. Strength:  Provided a new approach to retrieve invariance information among different environments. Weakness:  The contribution of adversarial attack or resilient GNN beyond prior work is not fully discussed. The OOD problem has been discussed in graph neural network adversarial robustness literature and causal inference literature. The experimental design looks good otherwise. I recommend a reject.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; In this paper, authors propose a framework for utilizing the contrastive learning to improve the feature distinctiveness for multi label classification. The main purpose is to address that the direct application of contrastive learning (similar to the single label classification case) is unhelpful for improving the multi label performance. However, the major concern is the overall novelty of proposed method. Overall, the idea of utilizing the contrastive loss during the training of multi label image classification network is appealing, however the limited novelty and experiments are the main concerns. Authors should carefully address above concerns (weaknesses) during their feedback, the final recommendation will be made upon the feedback.<|endoftext|>In this paper, the authors introduce the contrastive learning into multi label classification. Specifically, the multi label classification problem is first decomposed into a series of binary classification problems with label level features extracted by the attention mechanism. Then, label wise contrastive learning is performed on these binary classification problems respectively. Comparative experiment shows the proposed approach achieves the new state of the art performance in multi label image classification. The paper is written very well and easy to follow. It is an incremental work and the main focus, i.e.the adaptation of contrastive learning for multi label classification, is a trivial generalization of existing contrastive learning method. 3.According to the results reported in the ablation study (LLEN+BEN+LLCL vs. LLEN+BEN), it seems that the contrastive learning term may not be a good regularization for multi label learning. While the proposed MulCon trained with a two step policy has a much better performance, I am really confused about whether the performance gain comes from the contrastive learning term or the improved training procedure, since many additional training tricks are utilized.<|endoftext|>The framework adds an attention mechanism on top of the image encoder. + The proposed method seems to be an intuitive adaptation of the contrastive learning formula to multi label classification. + The appendix seems to consist of enough details to reproduce these results. Weaknesses and doubts:  The scope of the contribution is a bit limited, it seems to me that the main novelty in the area of multi label classification is proposed Label level Contrastive Loss. The motivation for the usage of contrastive learning is shallow. It is not clear from the paper. One of the authors  responses suggests that it was not.<|endoftext|>This paper presents a contrastive learning based method for multi label classification. In particular, authors propose to learn "label level embeddings" for an image, thereby the multi label classification problem can be transformed into a single label one where contrastive learning can be naturally adapted. Results were demonstrated by the experiments on two benchmark datasets (MS COCO and NUS WIDE). The proposed solution that learns label level embeddings is intuitive and reasonable, and the results are promising. The proposed label level embedding network is considered one of the main contributions in this work. For example, COCO contains some "small objects" (e.g., spoon, cell phone, etc.) Does the performance gain against existing methods come mainly from the attention module? I am not sure about the novelty at current point.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; The paper proposes to use amortized MCMC to learn latent space energy based model (EBM). The idea discussed is quite interesting and somewhat new. However, there are some downsides of the current submission: ( ) The presentation of the paper needs to be improved. Quite a few places are unclear which need to be better motivated or to be further described. 14?Any motivations/insights on why consider Langevin process on the parameter space (the parameter space usually has much higher dimensions compared to the latent codes itself)? The paper should also need to be well motivated on why performing Langevin on their parameter spaces (as in Eqn 10, 11) as well. The paper study an important problem, but the current presentation makes it hard to follow.<|endoftext|>If the dimension is large, then the communal MCMC chain is no cheaper than parallelized low dim chains across dataitems, if the dimension is too small, this imposes too much similarity across all the posteriors and the method suffers, this trade off is not clear. The authors describe application of this new trick to latent space of an autoencoder latent variable model and the sampling from an unconditioned density such as an energy based model. The MCMC algorithm of choice is the well known Langevin Dynamics solved by the Euler Maruyama method. However the rather chaotic numerical results are very disturbing. I enjoyed the overall paper. treating all dataitem posteriors as a linear projections of a single communal distribution seems risky, further comment would be appreciated. The theorem requires that all samples for all posteriors $z^{(i)}$ must be varying linear projections ($g(x)$) of a single communal chain $\Phi$. This sounds really cool! I enjoyed the theorem and the discussion of the rank of the G matrix. # Concerns  In algorithm 2, the generative model weights, $\Theta$, are also sampled using the Euler Maruyama method, however this does not correspond to the theorem 2.1 which requires that the generative model $\Theta$ is fixed (otherwise there is no true posterior to approximate). P4, line 3: Test time sampling: why not pregenerate a markov chain during/after training that can be reused again and again at test time? (I realise this may contradict other venues/reviewers that emphasise empirical results). in the experiments, I think it is valuable to know why LD and LD+EMB are worse than LAE? Surely LD + EMB is the gold standard (one MCMC chain per dataitem) that the LAE strives to achieve, while this may be computationally expensive, I believe that such a baseline should at least be implemented to outperform (or match) LAE, (i.e.agreeing with the theory) this could also show the MCMC amortization gap that this new amortization method introduces (again agreeing with the theory).<|endoftext|>This paper introduces amortized Langevin dynamics for latent variables and extends it to unconditional distributions. A Langevin autoencoder is further developed by using amortized Langevin dynamics for prior and posterior sampling. The proposed method has been tested on synthetic distributions and image generation. It is not clear to me how to get the function g in the inference network. It will be better to provide more explanation and details about it, since it seems to play an important role in approximating the posterior as shown in Theorem 1. How does the storage cost of the proposed method compared to LD without amortization and AVI? For the experiments, it will be better to add the results of SGALD in Figures 1 and 4 to check the posterior estimation when using minibatch of data. However in practice the dataset size will be much larger than the dimension of the linear layer (as what the authors did for the experiment on image generation). It will be much convincing to simulate this scenario on synthetic data and verify that with small dimension ALD/SGALD can still approximate posterior well. It will be very helpful to further show how the posterior estimation changes with respect to the dimension of the linear layer. In summary, I think the idea is reasonable and developing a cheap MCMC based autoencoder will be of interest to the community. However, I have some concerns with respect to the methodology and the experiments mentioned above.<|endoftext|>This paper proposes an amortized Langevin dynamics for sampling from a posterior distribution of a top down generative model or from an energy based model. The key is to recruit a sampler function that generates many samples directly in parallel, and run a single Langevin dynamics on the parameters of this sampler function. The method is illustrated by image generation. Strengths: (1) The idea of the paper is novel and interesting. The paper proposes a new idea on sampling from unnormalized densities. It can be useful for learning deep generative models.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; This allows a defender to recover the original image used to produce an adversarial example and may be an effective tool to mitigating adversarial example attacks. The proposed reverse engineering topic is interesting and novel. However, the experimental results are limited to the ImageNet dataset, it is difficult to be confident that the experimental results would generalize beyond that dataset without incorporating more datasets. The major strengths of this paper are:•	The paper is novel and could be have a considerable impact. •	The ability to reverse engineer adversarial perturbations could be a powerful tool for understanding adversarial examples better.<|endoftext|>The paper considers the problem of automatically reconstructing adversarial perturbations from examples in a post hoc manner. The authors argue that for an effective reconstruction, it is not sufficient to only minimize the reconstruction error but also it is essential to align the predictions of the original and their reconstructed versions. The paper is well organized, easy to read, and the algorithmic approach is sound. Overall the paper should be accessible to a wider audience. 2.The experimental results show that the new architecture is more effective than the baseline approaches for the RED problem considered in this paper.<|endoftext|>This paper defines a new problem, Reverse Engineering of Deceptions (RED), that aims to reconstruct the adversarial perturbation applied to a clean image based on the adversarial image. Finally, in table 2, the d(x, x_{RED}) is actually lowest for the DO baseline, and the DO baseline has comparable PA to CDD RED. What could these perturbations be used for?
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; This paper approximates the learning dynamics by an Ornstein Uhlenbeck process and uses some PAC Bayes results to obtain a functional form for the generalisation behaviour as a function of the ration of the learning rate to the batch size. It then uses these to develop a kernel function for hyper parameter optimisation using Gaussian Processes modelling. This provides a bound, but that bound could dramatically change the function form.<|endoftext|>The authors study the learning rate over batch size dependence of generalization bounds and try to use them for hyperparameter tuning. In section 5, the authors use the bounds to propose a functional approximation for the generalization based as a function of the n eta/|S| scaling of the fluctuations around the minimum, including for the possibility that n is time dependent. The generalization error is roughly a sum of integer powers of sqrt(n). > For the hyperparameter tuning comparison to be meaningful, I would recommend comparing with a grid search over parameters. Main concern is that their functional form only applies to linear regression with SGD.<|endoftext|>* Inspired by the generalization bound, the authors propose a heuristic functional form of the generalization error w.r.t.the (learning rate / batch size) ratio. * The authors propose a novel kernel function for Bayesian optimization in hyperparameter optimization. Empirical results show that the proposed kernel can match or outperform existing Bayes optimization baselines. * Proposes a novel functional form for the generalization error w.r.t.the (learning rate / batch size) ratio, and empirically show that it is empirically realistic for image classification tasks. Questions:* To approximate the learning dynamics as an SDE, the learning rate needs to be small. For now, I recommend weak rejection.<|endoftext|>The authors study the dependence of generalization error of trained neural networks on the batch size and the learning rate used in SGD. The authors present a functional form using PAC based generalization bounds to model the desired dependence. They conduct extensive experimentation to show that the functional form approximates the generalization error well. In addition, the authors show that a hyperparameter search based on the proposed model outperforms existing hyperparameter optimization libraries like Hyperopt and Optuna. Overall, I believe the primary strength of the paper lies in the extensive experimental study to corroborate their claim.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; This paper introduces a special loss formulation and training algorithm leveraging projected gradient ascent to build robustness to parameter noise into training for neural networks (both spiking and convolutional neural networks are tested). The authors show that the proposed method flattens the loss landscape for small parameter perturbations with a tradeoff of increased overall cross entropy (task) loss. The paper is presented well and provides sufficient evidence on the performance of the method. L_{cce} is referred to twice in the paper but is undefined   is this the same as L_{nat} task loss? Though I am not an expert in the specifics of this adversarial and noise related network training methods, the paper layout and methodology appear sound.<|endoftext|>In this paper, the authors proposed an adversarial training method that minimizes the robustness loss so that the network parameter can be trained to be robust to the parameter perturbation. The motivation of the work comes from the parameter mismatch that exists when using analog devices for computation. Experimental results with several benchmarks showed that minimizing such loss resulted in flattened weight loss landscape, and the proposed method outperformed existing methods in terms of the robustness to parameter mismatch. First of all, this paper is well written and easy to follow. The authors provided clear explanations on the prior works, and the proposed method is clearly described. The proposed method that uses the robustness loss seems like a simple idea, but the experimental results showed clear improvement compared to existing methods with several different benchmarks. However, the datasets used for experiments seem too small in terms of the network size and the number of parameters, so it is doubtful that the proposed method can be a scalable solution. Since there seems no reason for the proposed method to be limited to the spiking neural networks, it would be better if the authors can showed the improvement from the proposed method with larger scale datasets commonly used for non spiking neural networks.<|endoftext|>Authors investigate the problem of network robustness in the presence of parameter variations. To that end, authors propose an adversarial parameter perturbation based robust training method. Proposed training approach iteratively performs adversarial attacks on the parameter space during training to regularize the model by penalizing parameter vulnerability. Experiments are performed on F MNIST, ECG data and a speech command detection dataset, with both conventional CNNs and recurrent SNNs, where weight space perturbations during training showed generalization to weight perturbations at inference time. Experiments are well presented, including successful demonstrations of better flattening of the weight loss landscape with respect to the competitive methods. The novelty of this paper however, in my opinion, comes from the fact that the authors applied this idea for the first time to recurrent spiking neural networks (SNNs). So far the conventional CNN that the authors used only has 500k trainable parameters, and is not even demonstrated to operate on RGB images such as CIFAR 10.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; This paper considers a text style transfer task where no paired sentences are available. To avoid that, the paper proposes to learn an invariant style classifier (which can classify the sentiment regardless of the product category). Besides, the paper also proposes to learn an orthogonal classifier which can monitor any style independent changes (e.g.product category). The paper is tackling an important problem where we only want to change certain aspects of the text and maintains other characteristics. The main concern is about the effectiveness of the proposed method. I would also suggest adding human evaluation experiments on M w/C_ERM and M w / C_s. The proposed method is mainly built on top of Invariant Risk Minimization, thus lacks novelty.<|endoftext|>This paper presents a method that can work on a single text style transfer task with multiple datasets. One example scenario as exemplified in the paper is the sentiment transfer task with datasets on multiple product categories. The proposed methods were evaluated on the sentiment transfer task with two different setups to demonstrate the value. Strengths  The idea of using Invariant Risk Minimization is interesting. Comments/questions on the proposed methods and evaluation  In section 3.1.2, how to learn `an IRM classifier`? Additional evaluation methods are needed here. By constructing the `environments`, it can eliminate other factors, but there is no guarantee that style is the only factor left.<|endoftext|>This article deals with style transfer. The general framework has been defined by (Arjovsky et al., 2019, IRM) and the basic classifier is (Kim, 2014). Section 3.2 describes the authors  contribution where multiple orthogonal classifiers enables them to transfer style while preserving other attributes. The authors demonstrate that their approach is able to transfer sentiment while preserving domain or punctuation depending on the experiment. Pros:+ interesting application and synthetic datasetCons:  outdated encoder and text generator  less general framework than [Subramanian et al., 2018]  I don t understand the synthetic noise addition by punctuation change: what are the changes? One wonders what this approach would look like in the context of the experiments conducted in [Subramanian et al., 2018]. Even if GPT is very expensive, BART or T5 could have been investigated for such a task.<|endoftext|>Style transfer is a text generation task where a certain attribute of a sentence (like formality) is modified while preserving sentence content. Also it s hard to compare between rows in this table since it s missing an "overall score" / aggregation like you did for the human evaluation. This paper is an attempt to fix this issue using invariant risk minimization (IRM) [6]. To perform style transfer, the authors first train two orthogonal IRM classifiers   the first classifies the target attribute ("sentiment") irrespective of the confounder ("product category"), and the second for the confounder ("product category") irrespective of sentiment. Style transfer is an important problem in natural language generation, with several practical applications (as discussed in [1]). Experiments confirm the models trained by the model modify sentiment while keeping the confounder consistent. On both kinds of metrics the authors see improvements using their proposed approach. The current approach also needs an orthogonal classifier, which needs a well defined label space for the confounding factor.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; The authors work on the important problem of robustness guarantees in RL algorithms. The paper is generally well written. I assume all your integrations are done over the finite support of action space ? Do you mean a dynamical system controlled by an LQG controller ? Indeed, with the simple linear dynamics one can compute closed forms of the target reward \bar{r}, etc.but what happens when the dynamics are nonlinear and a closed from solution is not easily obtainable ?<|endoftext|>This work analyzes the implicit robustness of maximum entropy RL. Maximum entropy is a form of regularization that prioritizes higher entropy over optimality. Previous works have shown that such regularization may lead to faster learning and in general that there is a connection between generalization < > regularization < > robustness. Hence, while these results are not very surprising the clear connection is important and interesting. I find the theoretical results and the explanation very good. finally, I suggest the authors refer to prior work [1] that analyzes the connection between regularization and robustness in general MDPs (as maximum entropy is a special case of regularization).<|endoftext|>I find the paper very interesting and in general well written. Proving that policies learned with entropy regularization are robust in some well defined sense would be an important result in my opinion. Moreover, I believe that the experimental results would have been stronger if the authors had included some experiments with deep RL algorithms based on stochastic policy gradients in contrast to TD3, which yields a deterministic policy. Detailed comments: The paper should be checked for typos. Some technical results should be clarified and the experimental results could be strengthened.<|endoftext|>The authors investigate an interesting problem, which consists in asking whether maximum entropy reinforcement learning can be thought as a robust problem. The authors correctly describes the related works, which shows the relationship between MaxEnt RL and some kind of robustness w.r.t.the reward. The first one is related the clarity of the paper and the soundness of its claims. The second issue is related to relevance: to which extent the bound provided by the authors helps in shedding light to MaxEnt optimization? The clarity of the exposition can also be improved, together with the experimental analysis which struggles to effectively shedding light on the theoretical results.
Accept (Poster); rating score: 6; rating score: 6; rating score: 5; rating score: 5; rating score: 5; This paper considers the popular MAML algorithm, and provide more theoretical understanding of (1) the optimal inner loop learning rate and (2) how the MAML initialization compares to that learned by empirical risk minimization. Firstly, to estimate the optimal inner loop learning rate (adaptation rate), the authors pose this as a problem of minimizing MAML s population risk, and develop an estimator for this optimal learning rate. The contributions appear to develop on closely related studies such as Bernacchia et al, ICLR 2021. ## WeaknessesI am not familiar enough with the methods and related work to gauge the significance of the theoretical results. However, a couple points on clarity that came up when reading:  For clarity, the estimator of optimal LR in Eq 145 could be included in the main paper, since this is used in the experimental sections. Overall this work is well motivated and appears to provide new insight about the inner loop adaptation of MAML and the significance of the meta initialization learned by MAML, so I would consider recommending acceptance.<|endoftext|>The authors analyze the statistical properties of the optimal inner loop learning rate in order to try to obtain insights applicable to settings that go beyond restrictive assumptions in the theoretical analysis, finding an inverse relationship between the estimator and regressor variance. The theoretical results were validated by simulations (albeit toy), even when the assumptions were slightly violated. The large gap between the setting studied in the paper and meta learning in practice limits my confidence in how impactful this work will be. Questions  Can we not think of MAML in this mixed linear regression setting as learning the mean of a Gaussian prior over $w$, and the choice of $\alpha$ as implicitly specifying the variance of the prior? Why is ERM a valid algorithm to consider for mixed linear regression?<|endoftext|>The paper put effort to understand the adaptation learning rate alpha. It showed the relation between alpha* and data distribution (variance of features). It also showed that alpha in MAML, compared to ERM, helped to minimize the total distance to all the tasks’ optima. Intuitively, alpha has relation with data(or say task) distribution. I don t think those results well pass the bar of ICLR conference. According to the above, I would suggest a weak reject (barely below the bar). PS: I am not sure if result one (find the better alpha) could be applied in practical tasks. My confidence is mostly on the theoretical part of the paper<|endoftext|>This paper explores a question of how to theoretically derive a form of optimal MAML inner learning rate and how to interpret its meaning. In order to do this, the authors assumes that the given problem is linear regression with some possibly nonlinear feature transformations. Further, they demonstrate that MAML with proper inner learning rate stabilizes the meta learning of the shared initialization, such that the shared initialization can generally minimize the distance from all the given task predictors, whereas the baseline ERM is sensitive to the density of the task distribution. 2.Similarly, the experimental results are done only under the synthetic environment, which limits the practical impact of this paper. Can you extend the experiment to more realistic scenario such as miniImageNet few shot classification? Can you provide an intuition in relation to the derivation? However, I m not fully convinced if the finding is practically important, or something new (in terms of intuition) as well.<|endoftext|>More formally, they show that the optimal learning rate depends on the distribution of the feature vector. Second, the authors characterize the gain obtained from MAML in comparison with ERM. The obtained bound matches with basic intuitions. For instance, it increases with the test time learning rate up to some $\bar{\alpha}$ and also decreases as the number of deputation steps increases. The paper provides new insights into the theory of MAML. This is an unusual assumption to me: if you know the true label to update the model, then validating the model on it might not make sense. I appreciate it if the authors explain how much the analysis depends on this assumption and whether it can be relaxed or not. I appreciate it if the authors explain how their results differ from the results of that paper.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; The authors claim that this method captures both the uncertainty due to lack of data, as well as model misspecification in the process   while other/existing methods focus mostly on the variance of the posterior distribution (or its approximation) as a measure of epistemic uncertainty (and thereby implicitly assume the model is well specified.) While the approach bypasses some of the limitations of existing epistemic uncertainty techniques (e.g vulnerability to misspecification), the dependence on having an aleatoric uncertainty estimator or Oracle can be a major limitation as well. The generalization of the secondary model (the one that predicts total generalization error) itself feels to be on shaky grounds. It appears that the main difference in the secondary model (compared to similar works that employ a secondary model) is that it uses labels from out of sample examples. However those examples are (by definition) something already available at training time, and it s not clear how or why we somehow ended up with the ability to predict total error in OOD settings? The overall narrative / presentation is hard to follow.<|endoftext|>Using this definition, it develops an estimator: direct epistemic uncertainty prediction. The idea is to have a main predictor to learn the task, and an error predictor to predict the generalization error. Empirical studies show that their proposed estimator produces better estimation on downstream tasks such as sequential model optimization and reinforcement learning. It s possible that there are some interesting ideas in this paper, but at this stage, I find it difficult to understand this paper. The authors also seem to use out of sample generalization and out of distribution generalization interchangeability. Then on page 2, it says ``DEUP error predictors can be explicitly trained to care about, and calibrate for estimating the generalization error for examples which may come from a distribution different from the distribution of most of the training examples, i.e., an out of distribution (OOD) setting". Is it supposed to be an illustrative example? A reader should not have to go to the appendix to understand the algorithm.<|endoftext|>Minor comments:  Algorithm 1 requires an estimator of aleatoric uncertainty. Does the authors use that in any of the experiments and how do they define it? The proposed approach, DEUP, builds a new model (in addition to the original model) which predicts epistemic uncertainty, defined as generalization error minus aleatory uncertainty. In case there does not exist a hold out set or in interactive settings (like RL or active learning), DEUP is extended to be used in a cross validation setting and the features used to fit the error predictor is extended to include data density estimates and model variance. I like the approach of directly estimating uncertainty. I have some concerns on providing intuition on the estimated uncertainty, which I believe could be addressed by some additional discussion and some more analyses. 2.The proposed approach is practical, which is important for its adoption by others. For instance, in the case of fixed training set where the set used does not capture OOD dataset and density features are not used, is DEUP s error prediction able to generalize to OOD? I would be very interested to see how the method performs in such settings. 2.Section 5.1.3: The metrics used here can be improved (except log likelihood). For instance, coverage probabilities and CI widths would be great metrics to use here, which will also help with the intuition point made above.<|endoftext|>This paper provides a novel view on uncertainty estimation and is an interesting read. In turn, this held out used to estimate the primary model error comes from a k fold split. I currently can not recommend acceptance. What am I missing here? The authors provide a diverse range of experiments: OOD rejection in image classification, active learning for drug discovery, function optimisation and exploration in reinforcement learning. With a few exceptions, I found the paper to be well written. I found it to be an enjoyable read, as it made me think about and question some of the assumptions traditionally made in the deep learning uncertainty estimation literature. For this, I think that this work can present a relevant contribution to the ICLR community. Perhaps HMC is a better example. Does it make sense to have this diagram? Is it needed to understand the rest of the paper? I think this is due to there not being an explicit predictive distribution. I look forward to discussing this point with the authors and other reviewers. However, the rest of the examples in the paper use NNs. As stated by the authors these are asymptotically unbiased. In part, this is what makes Bayesian deep learning so difficult. I am somewhat surprised by this as I would have guessed the neural network based approach to uncertainty estimation allows for good scalability.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; The paper are well written and easy to understand. Therefore, I believe the exploration strategy is not theoretically convinced enough to obtain good performance on all possible RL tasks. For the latter, the authors only conduct experiments on 8 games. Discussions on the comparison with SAU method by Rigotti & Zhu. 2.Explanation of the insight behind the exploration strategy. 3.More experiments to support their conclusion.<|endoftext|>The method is simple and intuitive, and I think it s an interesting idea. I think the paper is not currently ready for publication. I found also limited the set of experiments in deepRL. ICML 1999Minor comments:  I would suggest adding a dependence on the current time $t$ in the definition of $\Delta^2(a)$ and providing an explicit definition of $\mathbb{T}_a$Overall, while I think the idea is interesting, it is necessary to provide a more detailed experimental evaluation of the method to support the claims.<|endoftext|>The paper is generally well written. The experiments are mainly compared against vanilla DQN. Since the authors  algorithm is a modified DQN, it would be fairer to compare their algorithm against other modified DQN variants, such as Rainbow, etc. **After Discussion:** I read other reviewers  reviews. I think the first concern downgrades my evaluation on the theoretical strength of this paper, and the second concern downgrades the empirical strength.<|endoftext|>The authors discussed the background and formulation of SAU. The work lacks theoretical support. Though incorporating SAU for exploration is promising (as I claimed in the strength part), the proposed incorporation method in this paper is not grounded by theory. In addition, some exploration strategies based on Q learning are missing in the literature review. (2019)The idea of incorporating SAU into RL exploration is promising.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; The paper introduces a particle based approach for model free RL. Results suggest that the proposed particle based approach improves on the prior work and shows different divergence measures yield different performance. Review of: PARTICLE BASED STOCHASTIC POLICY OPTIMIZATIONI found the paper very difficult to understand. The connection between probabilistic inference and RL makes it possible to leverage the advancements of probabilistic optimization tools. However, recent efforts are limited to the minimization of reverse KL divergence which is confidence seeking and may fade the merit of a stochastic policy." It is unclear what the goal and contribution of the paper is. To connect probabilistic methods with RL? Also, what does it mean to provide more flexible property? Is framework 1 meant to be a mathematical statement? Is remark 1 supposed to follow from the framework? Minor:  Stochastic policy has been <  policies have been? Also, I didn t understand it. The paper should state clearly what was done.   " Jensen Shannon divergence, though it is reported toshow some tendency of mode seeking (Theis et al., 2015), it also effectively ameliorates the problem as it consistently outperforms the exclusive KL" What does this mean? The paper proposes an interesting approach, but the writing and presentation were incomplete / unclear.<|endoftext|>There are a number of fundamental problems with the presentation and technical results. Building off of prior connections to probabilistic inference, the authors propose a framework for optimizing policies using general divergences/distances by way of particle based methods for sampling. This paper proposes a new method for policy optimization in reinforcement learning. What effect does this have on the objective or the regularized objective? Instead, I think this is an important claim that should be understood via proof or experiments. I have fundamental concerns about the paper. Very little notation is defined and there is inconsistent use of the notation that is defined, making it difficult to understand and evaluate technical correctness and significance. Some theory is presented. D[ .|| . ]notation is used inconsistently. What properties of the other discrepancy measures suggest that they could solve this problem? The technical novelty is not clear. “Thus minimizing D˜ F well serves as minimizing DF” I think formal justification of this is necessary. What are the updates? “Robustness” is cited but no robust problem is formulated. Additionally there is use of $\approx$ without further discussion of what is being approximated. One of the core problems that the paper tries to address is the fact that reverse KL is mode seeking.<|endoftext|>This work extends the popular RL as inference framework to accommodate for more general divergence measures such as the f divergence and Wasserstein distance. The author proposed a particle based optimization framework for learning stochastic policies where the policy is learned using samples generated via Langevin dynamics. The framework can be applied to both online and offline settings. However I found the presentation of the paper to be quite messy and insufficient, additional detailed comments are as follows:  One major problem I found with this work is that the overall motivation seems unclear. While I appreciate the detail provided by the authors in the paper, the overall big picture not clear. Is this a bias issue? I think this line of work has great potential in its current state, I do not believe this paper is ready for publication.<|endoftext|>This paper proposes a particle based sampling method for stochastic policy optimization in reinforcement learning. To mitigate this problem, the authors propose to use particle based sampling such that one can directly use KL for policy optimization. Strong points:The paper proposes a very interesting idea to solve an important problem in stochastic policy optimization. In supervised learning, we all know optimizing the reverse KL is a bad idea as it s mode seeking. The proposed method is empirically verified on both online and offline RL tasks. To compute the gradient of KL in policy optimization, one needs samples from the softmax policy defined by Q. In fact, the methods proposed in [2] can exactly to be used for Eq 7. I recommend rejecting this paper as important baselines are missed.
Reject; rating score: 3; rating score: 6; rating score: 6; rating score: 6; This paper proposes a learning based method to solve planning problems. Experiments show that the proposed method can achieve performance comparable to the traditional planners with lower planning time for smaller maps, but the performance drops significantly as the map sizes increase. The authors have made a few changes since the NeurIPS version and I have updated my review accordingly. The paper tackles an important problem of learning to plan which is of interest to the machine learning community and has important applications in robotics. The performance of the method drops significantly when map sizes are increased, which indicates that the method is not scalable. This makes the significance of the approach very low in my opinion. Traditional planners would be still be preferred over the proposed method when maps are large or when performance is more important than runtime. The use of Transformers for planning is not well motivated in the submission. The authors only conduct experiments for 2D navigation tasks. It is unclear whether the approach would scale to manipulation tasks with higher degrees of freedom.<|endoftext|>This paper proposes a transformer based trajectory estimator for 2D navigation. The proposed method is shown to not be limited by input size as other learning based approaches. In the experimental section, the authors show that their estimated trajectories result in speed up and performance boost for planning in comparison to traditional and other learning approaches. Strengths:+ Reduced planning time over traditional approaches (there are some issues here that I talk about in the weaknesses). + Method does not need fixed input size as other learning based approaches. The authors argue that this can be seen as a preprocessing step, however, I think this may not be a valid argument. If the baselines have pre processing steps, these times should be included as well for fairness. Using the hybrid approach with learning based baselines:This author proposes the use of a hybrid approach to addressed any trajectories that miss reaching the goal. Method marginally outperforms baselines in the "Dubins car model" experiments (Table 4). Is the baseline faster than the proposed method? I personally like this approach, but I feel that there are key missing questions that the paper does not answer in order to determine whether the claims are successfully substantiate or not.<|endoftext|>The paper presents a Transformer based planning approach that attempts to use an attention mechanism to reduce the search space of a traditional planner such as RRT*. In a similar way, the authors present an approach that uses attention on a 2D map to create a "mask" for a standard planner to draw samples from. The proposed Motion Planning Transformer (MPT) works by using sliding window on the original map, creating features that are then positionally encoded (which helps with generalisation) and then put through a transformer encoder and classified. When combined with a planner it is able to achieve state of the art results in a reduced time. The proposed approach combines learning based and sampling based planning approaches using the strength of each approach to solve the task it is best suited to. The authors present several important contributions, from using transformers to ensure the planner can attend the entire map in deciding which areas to sample from, to using positional encoding in a way that allows them to generalise to unseen maps. Furthermore, the approach is presented in a way that makes it reasonable to assume it will generalise to significantly larger maps. There are a couple of concerns. Firstly, the evaluation presents 2 planning scenarios: point robot and Dubins car. While I understand that this is a large domain in and of itself, it is important to notice that RRT* and its variants are designed for higher dimensional spaces and can be outperformed by simple A* in 2D grids. It would be interesting to see if the approach can generalise to higher dimensions, making it feasible for robotic arms and more complex planning spaces. Finally, the authors do not compare against learning based planners because they did not perform well, however it would be interesting to see the results for context.<|endoftext|>This paper proposes a transformer based method for 2D motion planning. The aim of the method is to work in any resolutions of the 2D map and the attention of the transformer enable to get the best possible regions through which the desired path should exist. A  traditional planners is then used given the extracted region to generate the final collision free path. The results shows the efficacy of the transformer guided path planning using RRT* in different resolution of maps. There are two important aspects of this method. First is to get a region so that a sampling based method can derive the path within that region to save time in the path planning. Second due to the transformer this method can scale to different resolution of map. The neural guided RRT is not new. It will be good to see the generalisation of this method as it uses a classifier to train. So a study on the generalisation ability out of distribution would be good given the IRRT* performance. Also the path length should also be compared across method. The authors mention about a few choices of the position encoding of the transformer. The method is good as a pipeline but it needs few careful experiments to position the claims with respect to state of the arts. While part of the method s philosophy is already existed like neural guided path planner, the other part where the resolution invariant is being claimed is good. Some of the comparisons are important to position the entire pipeline against SoA  and few metrics needs to be shown (like the time) to understand the computational complexity.
Accept (Poster); rating score: 8; rating score: 5; rating score: 5; rating score: 5; I m both excited and disappointed by this paper. Overall, I m now satisfied that the paper is presenting an empirically meaningful improvement over earlier works. I believe that this is a valuable contribution that should be published. On the other hand, the remainder of the paper has a number of substantial problems:1. the translation of the database of program facts into a binary graph is not described well:  * (Q1a) how do tuple attributes exactly contribute to the node features? While many open questions about design choices here remain, this is a substantial step forward that deserves publication at ICLR, as I expect it to be of significant interest to the ML4Code subcommunity at the conference. After all, you re using a similar attentive approach for the aggregation of paths by going through DeepSet. * (Q3a) The hypers described in App. * (Q3c) How sensitive are your results to different choices of the random walk specification? The paper presents a novel way of automatically extracting valuable information from programs before using standard ML tools to process them.<|endoftext|>The paper presents a framework called CodeTrek for processing and learning source code. The main idea is to represent code as a graph, manually design analyses to serve as additional relations, and perform a set of random walks on the graph. These walks can also be manually guided. + The system is compared to a variety of structural and textual baselines. ## Cons  The paper is very difficult to understand. The authors claim that "A task developer need not be a machine learning expert to bring in more semantic information about programs". However, there are many more aspects that the task developer needs to consider: which queries to write, how to write these queries, how to choose the starting node, how to bias the random walk, etc. These might make the proposed system to be very difficult to use for other tasks and languages. ### Clarity* The paper mentioned Semmle several times and mentioned that Semmle converts codebases into relational databases. How does Semmle work? How does it convert codebases into databases? Overall, the paper shows strong empirical results. However, I believe that this is an application with little novelty, and few lessons to be learned for the ICLR audience.<|endoftext|>This paper proposes a new program representation approach CodeTrek, which leverages a program analysis tool (Semmle) to produce the rich representation of the context for a program. Semmle is able to convert the program into a relational database that can capture the semantics behind the program, furthermore, it also supports extracting the task specific semantics with the query language CodeQL to represent new semantic information. Then CodeTrek utilizes a biased graph walk mechanism for pruning the context paths and feeds them to the transformer encoder to learn the path representations and followed by deepset to get a vector representation. This paper is well written and the analysis of the experimental results are sufficient. But I have some questions to discuss. (1) The novelty in the network architecture is not enough. Derived relations are customized by the program analysis queries on the base relations, so it can be regarded as another featuring engineering. Is there a way to only use the base relations to construct the graph and see the experimental results?<|endoftext|>The paper presents a framework for creating task specific program representations for deep learning based on program analysis and random graph walks. It shows that the use of the relational graph outperforms the common use of AST and data flow graphs. This graph based representation of the program is then encoded by embedding a set of (random) walks in the graph. for tackling a new task (it doesn’t mean that these steps are easy). Cons: * This paper uses standard neural architectures and the representation that is used is also rather low on novelty. * This paper has less of a contribution to the ML community but has a potential impact on the SE community. * Allamanis et al.have shown how to represent programs as graphs, including cases when the program information is enriched by a wide range of semantic information from program analysis. Converting the database into a graph, defining some user/task specific random walk bias and length, and user/task specific anchor nodes. Applying this random walk, encoding these using Transformers, and feeding the result to some task specific neural network. Overall I think it’s a good paper with a minor ML contribution.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 5; In this paper the authors present a method for performing Bayesian inference in likehood free settings where the data and parameters are jointly equivariant or approximately equivariant. are a bit unclear, but overall the paper is a good contribution. Overall I found the problem and approach to be interesting. Major comments:  Despite being central to the main application and some of the setup of the paper, it was not clear to me what "approximate equivariance" looks like in practice. The "Chained NPE" (equation (12) in Section 5.3) makes sense as a reasonable baseline, but it is unclear to me why it performs so much worse than the wide $\kappa$ GNPE, and it would be good to include more discussion of this point. Does this indicated that chained NPE is overly confident in its estimation of the pose? Is it sufficient to only consider uniform distributions, as in the present paper, or would other distributions result in better performance? I find the c2st metric a bit difficult to interpret. It would also be good to consider more interpretable metrics, like the MSE of the posterior mean for each method, as well as e.g., the coverage of credible intervals. Typo:  on p. 9 "GNPE achieves a scores"  > "GNPE achieves a score"The paper is well written and presents an interesting method for a methodologically interesting problem with a nice application.<|endoftext|>The authors propose group equivariant neural posterior estimation (GNPE), a posterior estimation method which can self consistently infer parameters and standardize the pose. For equivariant posterior distributions, the GNPE can achieve better performance than the traditional NPE. Moreover, GNPE can also be applied to cases where the equivariance of the posterior is approximately estimated. The major disadvantage of the paper is the writing, which makes the paper difficult to read and evaluate. But I cannot check the correctness of the conclusions and algorithm due to lack of necessary assumptions and derivation steps. If so, please provide all the related assumptions and the proof, which are not trivial for readers. The first two paragraph in Section 3.3 seems to discuss the necessity of introducing them, but it is really hard to understand. It is an interesting paper, and possibly make important contributions.<|endoftext|>This paper studies group equivariant neural posterior estimation which seeks to endow conventional NPE method with equivariance of both the data and parameters simultaneously. To test the efficacy of the proposed approach the authors experiment with gravitational wave data and show GNPE achieves considerable performance gains. First, there are a few clarifications that I d like the authors to address. This also brings a few points with regards to writing clarity and perhaps mathematical rigor. For the data $x$ this is not an issue as we take a representation of the group and act on the space, but how does one act on the parameters?<|endoftext|>Strengths:  The approach is independent of neural architectures and does not necessitate knowledge of exact equivariances. The method seems to be much better than regular NPE in cases where there are known equivariances. Is this a regular NPE trained on the same data used for training invariances?
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; The paper claims thatthe new machine learning model simplifies and improvesinterpretability of the SAT encodings. In my opinion, the paper s premises are not correct. SAT encodings are important because they enable efficient reasoning insome settings, but not because of issues with interpretability. Furthermore, any information resulting from reasoning about theoriginal machine learning model on such a representation can berelated with the original model. Another one is toassume a very restricted representation, one where at most ninefeatures can be considered. For example, I was unable to understand how one goesfrom equation (1) to an intermediate equation claiming that one cangenerate both CNF and DNF representations. I was unable to make senseof the subsequent example, and I have some experience with logicalencodings. Also, the integration of different blocks is unclear. However, I was unable to understand how this might bedone for a DNF intermediate representation. In its present form, I cannotrecommend acceptance of this paper.<|endoftext|>This paper proposes a new kind of SAT encodable Neural Network which has real valued weights and binary activations. The authors have conducted experiments to show that this method achieves a better performance in terms of verifiable accuracy and runtime than the existing methods (Table 1). **Weakness*** The models used in this paper are for very small sized neural networks trained on simple datasets like MNIST and CIFAR 10. The main reason being *grouping parameters* makes a trade off between the generalizability of the network and its scalability to SAT solvers. This compromises the overall accuracy of the model but prevents the number of clauses from exploding. Even with this simplification, this method performs marginally better than the real value based complete verification method for low noise is unable to perform better in high noise cases such as *MNIST epsilon_{test}   0.3* and *CIFAR 10 epsion_{test}   8/255*. * The statement *The only scalable method for global exact interpretability was proposed in (Granmo et al., 2019)* in sec.2  is somewhat inaccurate. The MIP solver then generates explanations by solving for the simplified formulation.<|endoftext|>These masks indicate certain interpretability, e.g., the set of variables actually contributed to the truth value. The experimental evaluation shows that model accucy on MNIST and CIFAR10 dataset is comparable with or slightly better than other state of the art approaches. missing important baselines, for instance, the authors are clearly aware of that Narodytska et al.2019b work is highly related, given it has been cited many times; however, it is a bit surprising to see the evaluation does not use it as a baseline. there are lots of conceptual claims, however, which are not backed up by experimental results. I like the idea presented in this paper and hope it could be backed up in a more systematic and scientific way. So I slightly prefer not to accept this paper, at least in its current state.<|endoftext|>These convolutions can be fully represented with truth tables, as long as the number of inputs in the filter is low dimensional. Explainability and formal verification of deep neural networks are vibrant research venues. I think that I got the intuitive idea behind TT DCNs, but the presentation of the architecture could be largely improved, possibly by adding figures and more examples. This of course comes at a cost in terms of expressiveness, and the trade off between interpretability and predictive performance is not very clear. The reader is assumed to be knowledgeable about formal verification of neural networks. Isn t this true for BNNs compiled to propositional formulas too?
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; These activation functions are based on the principle that neurons encode logits to represent presence of features in the log odds space (logit space). The formulations are simple and straightforward. I do not think the paper and the idea is well motivated. The paper states that sigmoid is less applied and that ReLU is now applied more. But, the introduction does not explain why or make the connection to why we need these Boolean logic based activation functions. There is mention of biological networks, but it is not well cited by the paper.<|endoftext|>This paper introduces three novel activation functions for neural networks, by modifying the logit space versions of AND, OR, and XNOR in order to only use addition and comparison operations. A large number of experiments show that in certain architectures and on certain tasks, some combinations of these activation functions deliver good performance. It would be helpful to have a more developed theory of these activation functions and more experiments in settings that are more standard. Can the authors explain why this is desirable? And in Section 3.5, it s unclear where to find the actual results supporting the claims in the second paragraph.<|endoftext|>This is motivated by the behavior of the operators under the logit space equivalence. Finally, the authors provide empirical results showing how the different potential combinations of logical activation functions behave for different datasets and architectures. The paper is technically sound and well written, with some very minor comments on the latter. There are many other alternatives and you even mention some of them in the introduction. However, you do not compare against them. Furthermore, the recent advances in learnable activation functions further improve the performance of DNNs beyond what ReLU can do. A parameterized activation function for learning fuzzy logic operations in deep neural networks. They seem like a nice extension to MaxOut with semantical meaning on the operations they carry.<|endoftext|>However, my concerns are not fully addressed in the response, and I also agree with some of other reviewers  concerns. I keep my score. ### Strengths  The paper is clearly written. 1) Does this mean that the tasks/datasets are too simple? Or is there another reason that linear layers work reasonably well? Could authors comment on this phenomenon? 2.The performance of MaxOut is comparable to ${OR_{AIL}}$ for image based tasks. Does this mean that most of the pre activations become negative? Could authors provide more analysis between Maxout and $OR_{AIL}$? Authors mainly applied the proposed activation functions for MLP and CNN based models. Minor:Table 1 exceeds the paper size. I generally like the idea of this paper and how it s written.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; # Theoretical BasisThere are some odd points in the theory given behind their algorithm and data structure:  Other than using non standard hashing functions, what distinguishes the STORM sketch from the RACE sketch? My *review of the theoretical results* and data structure design is that the results are believable and seem correct, but lack technical novelty. The data structure appears to be the same as in prior work on LSH. The experiments suggest that using the STORM estimator is only slightly better than returning the mean of your data. # Experimental EvidenceThe experimental evidence continues to be a bit confusing.<|endoftext|>This paper considers the online risk minimization setting. In particular, it provides two STORM surrogate losses for linear regression and classification, respectively. Here is the detailed comments. pros1.Overall, I think this paper is well written. cons1.The technique novelty is limited since previous work (Coleman & Shrivastava, 2020) has used LSH to approximate kernel density estimation on streaming setting. Minor comments This paper seems a bit rushed since there are many typos. For example, in Sec.2.1, the head of the second line should not be $\mathcal{D}$.<|endoftext|>The paper proposes an online sketching algorithm (STORM) for empirical risk minimization(particularly for linear regression loss functions), which only needs to save the integer count values in the sketch. The author also gives an empirical evaluation with ridge regression and classification. Particularly, it uses the local sensitive hash functions to divide the space into several regions and save the integer counts values over the regions for the data points. The paper is well written and easy to follow up. The empirical results also show the advantage of this method. However, I have the following concerns/questions:(1) The conclusion section and the experiments are focusing on the regularized ERM problems, while most of the theoretical analysis seems to be for the non regularized version. I think it should be added to the experiments as a baseline, if I do not have some misunderstandings. Hence the advantage of the STORM is for the case when $n << d$. As stated in the previous section, I think the method this paper proposes is interesting, while there are still some perspectives that can be further improved. I am willing to raise the score if I have some misunderstandings or some of the concerns can be addressed.<|endoftext|>Experiment: The author has done relatively sufficient comparative experiments, including classification and regression under linear and nonlinear conditions with several datasets. Writing: Overall, the manuscript is well written and addressing a relevant problem by proposing an interesting learning method suited to edge devices. Weekness: Experiment: Important details missing, which can help the paper quality significantly. During inference, is the proposed approach less computational expensive than baseline approaches? Parameters: The proposed approach STORM is heavily based on some of the hyper parameters, for instance, p, there is no empirical evidence that how the performance of the model will vary by changing p. Besides, the value of p is different for different datasets. Performance: Although this edge computing method reduces the dependence on hardware, the reduction of accuracy seems to have a great impact from the experimental results in this paper. Can it be compared with more similar methods to highlight the advantages of this paper? Some results in Figure 5 are worse than the benchmark method. The paper contributes some new ideas to reduce the compuation complexity of ERM.
Reject; rating score: 3; rating score: 3; rating score: 5; In this paper, the authors aim to improve previous works on randomized smoothing by extending the analysis to allow smoothing measures with bounded support. While the analytic efforts in this paper are both deep and comprehensive, the problem the authors try to solve has been studied with a quite decisive and general conclusion.<|endoftext|>Under this setting, much of the technical machinery describes which Wasserstein and TV balls are needed to cover these bounded support smoothing distributions. **Strengths:** The paper is well organized and clearly explains the approach considered. **Weaknesses:**A primary downfall of this work is the motivation. Additionally, the experiments section is unsatisfying. **Other notes:**  I ve never seen "total variation distance" referred to as "total variance distance."<|endoftext|>The paper proposed a framework based on Wasserstein distance and total variance distance relaxation as well as Lagrange duality to deal with the analysis of bounded support set smoothing measures. **Strengths**The theoretical analysis is comprehensive and sound. I think the current version of the paper is not well ready to ICLR quality, there are multiple missing parts in related work and experiments sections.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 5; This paper provides an empirically very strong method for task based continual learning, as measured by average task accuracy, alongside a solid theoretical derivation and justification for it, which will be of interest to the continual learning community and for these reasons I am recommending it for acceptance. The main weakness of the method is that it has a runtime that is quadratic in the number of parameters at each layer, as a result of the gradient projection at each update; the paper could greatly benefit from an empirical analysis of the runtime compared to competing methods in order to quantify this limitation. Positives:  Very strong empirical performance on standard CL benchmarks, exhibiting essentially no forgetting across 20 tasks. The method is simple to implement. It could do with a diagram.<|endoftext|>The network has to learn to cater for all the tasks without forgetting what it learnt for an older task as data from older tasks are no longer available. * The theoretical analysis of the problem (forgetting as increment of old task losses, its approximation to more tractable RLL and further to its upperbound) is done very good. The problem is formulated as tackling forgetting (increment of old task losses) with the constraint of learning optimally for the new tasks. Next the authors introduce a modification in the direction of gradient update of normal SGD that makes sure to minimize forgetting.<|endoftext|>This method modifies the direction of gradients on a new task in order to minimise forgetting on previous tasks, and unlike many previous works, does not require storing past raw data to do so. The authors can make this clear when discussing the Bayesian approaches. They are based off Bayes equation for continual learning. It is nice to be able to modify the gradient on a new task with a projection matrix, while minimising forgetting on past tasks.<|endoftext|>The authors are encouraged to define two new baselines each of which incorporates one of the aforementioned components. The paper proposes a continual learning approach based on recursive gradient optimization. This is as if we say that the optimal parameters for each task are in the vicinity of each other which would abolish the problem of catastrophic forgetting from the beginning. There is a discrepancy between Algorithm 1 and the previously derived quantities.
Reject; rating score: 3; rating score: 5; rating score: 5; The paper considers the problem of generating recourse actions that are robust to shifts in the parameters of the classifier. In particular, the authors do not compare their proposed method with previous approaches addressing the problem of generating robust recourse actions. Weaknesses: the experiments presented are rather limited and the value of the contributions is unclear. In particular, the problem of generating robust recourse was previously considered by Upadhyay et al., 2021, but the authors do not provide any evidence as to why their approach may be preferable, which is particularly concerning given that the experiments considered by the authors are heavily inspired in those of Upadhyay et al., 2021. The contribution of this paper would be much stronger if the authors compared the performance of their approach to that of Upadhyay et al., 2021, validated the claim that “robust optimization solutions can be overly conservative because it may hedge against a pathological parameter in the uncertainty set” in the context of algorithmic recourse, and showed that their proposed approach overcomes this issue. Detailed comments on the experiments section:* As previously mentioned, authors should compare their approach to Upadhyay et al., 2021. It seems like in practice many of the features of the real world data sets would be immutable (e.g.“Recession” in the SBA data set). This would also give the authors more space for the experiments section and a conclusion.<|endoftext|>The paper provides a framework for recourse (i.e.counterfactual explanations) that is robust to shifts in the model. They formulate the robustified recourse setup as a min max optimization problem, where the max is over a neighborhood around the distribution over model parameters. The model parameters are drawn from a mixture of K distributions, so that the neighborhood is specified by Gelbrich distance on each component. They propose a finite dimensional version of the robustified optimization problem, which can be optimized using projected gradient descent. They evaluate their approach on the German credit dataset, the Small Business Administration dataset, and the Student performance dataset, each of which demonstrates a different type of data distribution shift. One weakness of this paper is that the technical solution provided is somewhat limited. In particular, the formulation in (4) relies heavily on the structural properties of the mixture distribution and Gelbrich distance to reformulate the optimization problem, and is not surprising given these assumptions.<|endoftext|>DiRRAc adopts the distributionally robust optimization technique and the paper proposes a projected gradient descent method to solve the optimization problem. Most existing work on recourse actions do not consider model change, so the problem addressed by the paper is relatively new, and it is an important problem since model/data shifts are common in practice. 2.The idea of considering/modelling model shift as a mixture shift of model parameters, and formalizing the problem as a min max problem. 3.The experiment results demonstrate the superiority of DiRRAc over the methods compared. 2.From the paper, ROAR is a method for generating counterfactual explanations that are robust to model shifts, but the experiments conducted do not consider ROAR as a baseline. 4.The performance of the proposed method under no model shifts should be evaluated as well. The paper is also well written.
Reject; rating score: 3; rating score: 3; rating score: 6; The paper presents a simple method for constructing decision rules for classification problems through linear programming. In fact, the method rather uses rules generated by the standard DecisionTree algorithm but later selects the most useful rules, weights instances, and poses a new learning problem for the Decision Tree algorithm. Even though the idea in the paper is quite interesting and the paper is well written, I find the experimental section insufficient to show the method s superiority in comparison to related algorithms. Therefore, claims that the method obtains similar results as RF are   for the moment   not fully supported since the hyperparametrizatoin of RF is very much suboptimal. 2.The proposed method for extracting decision rules from ensembles only selects them and has no mechanism of simplifying them. The proposed method calculates the weights automatically, the weights from boosting are no measure of interpretability   therefore, the proposed method should be able to calculate the correct rule weights on its own. Moreover, the method for extraction rules is not compared against any other method of this type.<|endoftext|>From a conceptual point of view, it is unclear what is gained by using linear programming in the proposed manner: runtime is not shown to be improved compared to classic greedy rule learning algorithms and optimality is not preserved in either new algorithm variant because heuristic tree learning algorithms are used to provide the starting point for rule simplification and are also used within the algorithm in the second variant of the proposed approach. The main text of the paper only compares to decision tree learning, which is not the appropriate comparison. There are results for some datasets for other rule learners in an appendix, which are obtained from other publications, but these were almost certainly not obtained using exactly the same experimental methodology. Another rule learner that should be compared against is FURIA, which was shown to perform better than RIPPER:Hühn, J., Hüllermeier, E. FURIA: an algorithm for unordered fuzzy rule induction. * Table 2: "average performance is close to ... ADA"   no, for many datasets, accuracy is quite noticeably reduced and the average accuracy is substantially lower.<|endoftext|>The paper discusses how, from an existing set of rules or a procedure generating rules "on the fly", one can use linear programming to select a subset of rules that would increase "interpretability", where an interpretable system of rules is here understood as a system with few, short rules. How does the proposed method compare to such approaches? For instance, authors could compare to the work "A Bayesian Framework for Learning Rule Sets for Interpretable Classification" that they do cite in the paper. Is there a limit to the number of rules that can be activated by an example? In the experiment, only rule length is considered, and I am curious about how $c_i$ should be assessed when other aspects should be included in it? For instance, P5 algorithm 1 does not compile well (probably due to the floating environment).
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 8; This paper is in the realm of hierarchical reinforcement learning, and the aim is to employ a particular kind of affordance model: milestones of subtasks which can be achieved and a coupling between states and "affordable" subgoal completion (from that state). Not much is said in the paper about this. Overall, the approach is interesting, and valid, and significant. I wish more would be said about this tradeoff.<|endoftext|>This paper incorporates the concept of affordances (the idea that humans and agents perceive the world in terms of relevant action possibilities) from ecological psychology with hierarchical reinforcement learning. I think this is a good paper. # Strengths  The central idea of the paper is very good.<|endoftext|>Given a task hierarchy specified as a dependency graph of milestones, this work employs a two level hierarchical reinforcement learning method similar to Kulkarni et al.2016.The lower level learns to achieve a given milestone in a classical fashion, while the upper level learns to choose milestones such that ultimately the agent will achieve the overall goal. The paper addresses an important problem and proposes a novel and effective solution that is likely to be used and built upon.<|endoftext|>The affordance prediction can be used to prune impossible subtasks leading to a more efficient exploration. I like the idea of this paper, and the experiments also convinced me. 5.The overall writing is easy to follow.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; This work looks at defining role diversity as a means for analyzing multi agent dynamics. It proposes three different perspectives on analyzing roles in multiagent systems. The paper does not do a good job at motivating why we should care about analyzing multiagent systems from the perspective of roles, or the alternative role definitions they propose. * The paper does not do a convincing job at showing that the roles are core to “cooperative training strategies for multiagent RL”. It adds more confusion to their role diversity framework. Does it make sense to think about this minimization as reducing the error we incur when approximating a joint policy with independent policies? This seems contradictory to what the authors want to propose in their methods section. In fact, this is not really the investigation the paper conducts: The paper demonstrates there are certain correlations between their role definitions and coordination performance, but not that this understanding results in coordination improvements. and how can these challenges be addressed by understanding how groups/teams can be factorized into roles? This is arguable. If I’m finding a good position just like all other agents, shouldn’t this indicate that we have the same role? This doesn’t seem reflected in this metric. It’s not clear to me how this is being used in the KL metric.<|endoftext|>Positives    Role diversity is important in multi agent environments. Being able to quantify it might allow us to better understand how to improve multi agent reinforcement learning. Therefore I believe focusing on the trajectory as a whole makes much more sense and includes the frequency of the actions of this paragraph. The authors present an example for the SMAC task in the appendix, but SMAC happens to have a circular observation radius which is then easy to calculate. In addition, fully observable environments are not uncommon and, in my understanding, the rule breaks down in that case (the distance between observations is always 0). 3.In general, all those metrics seems to require that training has already been completed. In my understanding, this is not in line with the motivations and promises of this work, that a diversity metric can help with a better training strategy and performance in MARL. (5.1 parameter sharing): I would expect different roles to require different parameters and roles that are similar to benefit from shared parameters (which is also shown in "...selective parameter sharing..."[7]). While the writing is clear, and conveys the author s intentions, it could certainly be further polished. There are some sentences that are vague or that could benefit from rewriting (e.g.first page, "In other word, even adopting the ... performance", but there are other sentences as well).<|endoftext|>This paper study the relation between role diversity of tasks and the MARL model performance. The role diversity (difference among agents) is described from three perspectives: policy based, trajectory based, and contribution based. The authors analyze how the role diversity impacts the MARL performance in theory and experiments. The experimental results, measured by the three role diversity metrics, answered a common concern question that why the MARL model performance varies across different tasks. The observation overlap percentage can be easily defined and measured in SMAC, but how to measure it in other realistic scenarios, e.g., embodied vision robot? The contribution based role highly depends on the output of the RL model. The experimental results are unclear. Why not report the three metrics jointly for a more comprehensive analysis? 2 and .3.These can help us better understand the difference between the three metrics. But I am concerned about the generalization of the proposed metrics in other MARL environments.<|endoftext|>To address the problem of algorithm choosing in different MARL tasks, this paper proposes to use role diversity as a metric to describe MARL tasks. They also find that the error bound in MARL can be decomposed into three parts that have a strong relation to the role diversity. To evaluate the proposed method, they further conduct some experiments on MPE and SMAC environments. The reviewer finds the paper is rather timely and interesting. This paper proposes to use role diversity to describe the MARL tasks and the experiments strongly show the relationship between algorithm choosing and role diversity. (1) The paper does not provide enough discussion or experiment about the connections and differences between these three role diversity definitions. They might describe different diversity aspects. (2) Theoretical analysis is not clear enough. Specifically, more explanations about Eq.(5 6) are expected. (3) The definitions of trajectory based and contribution based diversities should be explained in Eq.(3 4).For example, what are the definitions of $d_T$ in these two equations? (4) The discussion of how to obtain an accurate measurement of the role diversity before training in Sec.6 could be moved to Sec.3.(5)  What do the figures in Fig.2 (b)(c) want to say? It should be $T$ instead of $t_0$ when describing the notations of Eq.(2).This paper is timely and interesting, but there are still some concerns, especially the the first one in the Main Review.
Reject; rating score: 1; rating score: 3; rating score: 3; rating score: 5; rating score: 6; rating score: 6;  This paper discuss the accelerated gradient based MCMC method and propose some rigorous proof. I do not think the problem addressed by this paper does not make sense. The gradient based algorithm added by the noise in the convex case does not play the essentially different role with that without the noise. Please the authors refer to Bin Shi, Weijie Su and Michael I. Jordan,  On Learning Rates and Schrödinger Operators, https://arxiv.org/abs/2004.06977Bin Shi, On the Hyperparameters in Stochastic Gradient Descent with Momentum, https://arxiv.org/abs/2108.03947<|endoftext|>This paper introduces Hessian Free High Resolution SDE inspired by Accelerated Gradient (NAG). The author shows that continuous solution achieves an acceleration over the underdamped Langevin. This paper has solid theoretical analysis. However, it seems Assumption A1 require $m > 0$. 3) On page 9, author wrote "(strongly) log concave assumptions required in Theorem 5.2", why there is a bracket? Which algorithm is the author referring to for "ULD". 5) In section 6.1, author proposed to use error of mean as a surrogate of 2 Wasserstein distance. Clearly one iteration is not enough for burn in.<|endoftext|>The paper is mostly clear and easy to follow. However it still achieve same asymptotic convergence as certain discretized ULD at the price of an extra condition on higher order derivative. 4.Iteration complexity is worse than certain discretization of ULDAlthough the authors cited [Shen & Lee, 2019], they didn t discuss methods in this paper at all. 5.The method is not well motivatedAccording to the paper, HFHR is motivated by one question "how to appropriately inject noise to NAG algorithm in discrete time". We should note that, although there are underlying connections between optimization and sampling, injecting noise into a good optimization method doesn t necessarily yield a good sampling algorithm. The method is novel in the sense that it is derived based on the idea "inject noise into Nesterov’s Accelerated Gradient".<|endoftext|>The paper proposes an accelerated MCMC method for sampling, motivated by Nesterov s Accelerated Gradient (NAG) method. The obtained first order ODE system serves as the backbone of the proposed diffusion process, Hessian Free High Resolution (HFHR) dynamics. Theoretical convergence are provided for both the continuous and the discretized variant, showing acceleration to existing underdamped Langevin method (ULD). The main contribution of the paper is to provide an accelerated first order diffusion process in continuous time, yielding a novel sampling method after discretization. Is it related to the time scaling? 4 ULD   HFHR(0,$\gamma$)I understand that the continuous ODE are equivalent by taking $\alpha   0$, is the discretized algorithm still follows the same equivalence? In Figure 3, why does some figure only has $\alpha 0.1$ and the others has $\alpha   0.1, 0.5, 1$Overall, I believe the theoretical result of the paper has merit but the presentation need to be improved<|endoftext|>Overall, the reviewer would like to recommend acceptance. This paper introduces a Hessian free high resolution Nesterov’s Acceleration approach which proves faster convergence than both Underdamped and Overdamped Langevin Dynamics. In addition, it also proves that the acceleration cannot be achieved by time rescaling. Experiments results support the argument in optimization acceleration as well as the theoretical results.<|endoftext|>The author proposed an accelerated gradient based MCMC method based on Nesterov s accelerated gradient (NAG). In continuous time, the algorithm is able to achieve a tremendous acceleration over the underdamped Langevin algorithm. Numerical schemes can propose a speed up with a constant factor. This paper builds a NAG based MCMC sampler, which is named as Hessian Free High Resolution (HFHR($\alpha, \gamma$)) and generalizes the underdamped Langevin (also can be written as HFHR($0, \gamma$)) with an additional gradient drift and Brownian motion for the update of the position variable. Although the acceleration becomes less significant in numerical algorithms due to an increase of discretization error, some speed up by a constant factor can be still achieved. ICLR 19[2]. On sampling from a log concave density using kinetic Langevin diffusions. I would recommend this paper to be accepted.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; The experiments are only conducted on the long range arena benchmark, and the performance improvement does not seem to be significant. Finally, the paper conduct experimental results on the long range arena benchmark, where most tasks are of very small scales.<|endoftext|>However for the other two, the results are quite mixed. Regarding Shift Invariance 1: The authors claim that the shift invariant property of the softmax function is important for Transformers performance. However, why this might be important is not very well motivated: what causes this property to help with the performance? However, a key difference between the previous approaches and the one presented in this paper is that of binning the large distances (done logarithmically in [1]).<|endoftext|>Thus it s not surprising that the speed and memory usage is close to Linear Transformer. The experiments show that both methods are important to improve Linear Transformer.<|endoftext|>The whole paper indeed explored two existing issues of the Transformer framework: inefficient scaling and weak performance on long sequence tasks. What is the relationship between inefficient scaling and weak performance on long sequence tasks? 2.Experimental results demonstrated that the proposed two methods cooperated with each other to improve the performance of Linear Transformer models on part of long sequence tasks. 2.The proposed approach does not achieve good results on all tasks in Tables 1 and 3.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 8; The authors prove such a method obtains the optimal tradeoff and results in convergence in settings where predecessor variants fails (TD and ETD). The paper proves these claims theoretically, and show an illustration on the Baird domain for empirical support. The paper discusses the issues with off policy evaluation, specifically the problem of distribution correction, for which emphatic algorithms have been introduced. $B_\phi$ is not introduced. In proposition 1, $\zeta$ should be $\zeta^b$ everywhere? How confident are we that these methods are scalable in the general spaces we use these methods in? The paper addresses a fundamental problem in RL is a simple way and proves that such a method achieves an optimal tradeoff between bias and variance in emphatic algorithms, leading to polynomial sample complexity. The paper could benefit from some more explanations and clarity.<|endoftext|>This paper proposed a new off policy evaluation successor method of ETD. The empirical results show that the proposed method could converge in the case that neither TD nor ETD does. Strengths:  The paper is well written and clear to follow.<|endoftext|>The authors propose an improved variant of emphatic temporal difference (ETD) aimed at addressing issues of high variance when faced with a large mismatch between behavior and target policies. This is used to derive a schedule that effectively minimizes the variance and bias to achieve polynomial sample complexity. The paper is well written and easy to follow. I think the experiments should have contained some results in settings that aren t constructed to diverge. A more complete comparison would have been nice, but, as it stands, this is likely to be of interest to parts of RL community and is worth sharing. I can understand keeping the same step size for different $b$ for the purpose of illustrating the theoretical contributions, but why is it fair to use a fixed step size across different algorithms?<|endoftext|>The variance of ETD can grow up exponentially so this paper proposes to use a periodically restarted variant (namely, the PER ETD) as an improvement. Pros:1.In my opinion, this paper does answer an important question in off policy value evaluation. Cons:1.It seems that the algorithm is not using all the data in an efficient manner. Maybe some variant similar to least square type methods? In addition, there are a few papers on how to remove projection for the analysis of stochastic approximation under Markovian noise which should be mentioned in this paper. The analysis is very interesting and insightful in characterizing the trade off between variance and bias. I think the authors  response to my comments are very reasonable. I have increased my score to 8.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; This paper proposes Physics Informed Neural Operator (PINO) by combining two previous methods, Fourier neural operators (FNO) and physics informed neural networks (PINNs). Update after author response: The author s responses do not address my concern on the novelty w.r.t.previous papers. This is not conclusive from this paper as well. This seems to be too incremental given the previous work and the experimental setups are not sufficient to support the claims of the paper. Also, the comparison should be done with the comparable previous methods as well as better variants of PINNs, as I mentioned in my review. Here, note that this can be viewed as a type of transfer “training” (which is to accelerate training), instead of transfer “learning” (which is designed to accelerate training and generalization). However, I have several doubts. First of all, this seems to be too incremental given the following previous work: [1] Sifan Wang, Hanwen Wang, Paris Perdikaris. The authors should elaborate the difference more.<|endoftext|>The paper proposes the physics informed neural operator (PINO). It combines the operating learning and function optimizationframeworks, which improves convergence rates and accuracy over traditional  methods. I have read the other reviews and responses from the authors. The paper is well written and organized. The experiments are convincing. However, I tend to accept the paper for its effectiveness.<|endoftext|>The authors presented a novel algorithm to solve PDEs by combining two mainstream PDE solving deep neural network genre, the PINN and the FNO method into PINO. The only technical difficulty of combining the two algorithm (test time PINN based optimization for FNO) that I see is the derivative calculation for neural operator space, where the authors used exact gradient calculation coded by chainrule. Fig 2c the colors are very hard to recognize on the legend. 2.Did the PINN benchmark in this work include the adaptive activation function? ("Adaptive activation functions accelerate convergence in deep and physics informed neural networks" Jagtap & Karniadakis.) What was the R(a) regularization term s underlying assumption there? I think the technical and algorithmic contribution is solid ,the emperical results are good, and (if the novelty is good) I think this is a publishable paper.<|endoftext|>This approach combines two recent and popular methods in the field of machine learning and PDEs, the physics informed neural network (PINNs) for solving PDEs and Fourier neural operator (FNO) for learning solution operators. I find that the paper is well written and introduces a method combining two known techniques for learning and solving PDEs from data. Yet, there are some weaknesses in the study that I discuss in the main review. The paper is well written and highlights the current challenges in PDE learning before introducing the approach. The authors gives a good introduction of the field with its challenges and describe their contributions clearly. The next sections describe the PINN and FNO approaches before introducing the authors novelty method called PINO. I have some concerns about the choice of the training and testing data and optimization parameters. These limitations carries over in the present paper and the method can only be applied on simple domains with homogeneous/periodic boundary conditions.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; Finally, the experimental results demonstrate that the sharpness aware optimizer outperforms baselines on simple models. **Pros**  The proposed method demonstrates large performance over the baseline L2O approaches. In a similar vein, it’s unclear how or can the proposed model generalize to larger and more realistic models. “How” as in whether the optimizer learned on the smaller model will find wide minima for a larger model and “can” as in whether there exists reasonable hardware that allows for this. I am now convinced that you can train L2O on larger model but the performance of these models are much worse than standard SGD (i.e., < 80% test accuracy) so I am not sure if the proposed method is significant for real models. In particular, it achieves 80% test accuracy on mnist. While the paper proposes an interesting idea with solid improvement over the baseline, I do not see the practicality of using a learned optimizer that seeks a wide solution when there exist much better alternatives that don t require meta learning.<|endoftext|>The authors prove a bound for the generalization error, and show empirically that adding flatness aware regularizers to existing L2O methods, can improve generalization or both the learned optimizer (across unseen tasks) and the solution (across dataset splits). Strengths:To the best of my knowledge, this is the first paper to add such regularizers to the L2O objective, and prove a bound in the generalization error. Weaknesses:  This paper doesn’t show enough evidence of the practicality of the proposed method. The experiments are done on small networks and on datasets (MNIST and CIFAR) that are more or less “solved”. We do not know whether this idea scales and generalizes to more realistic settings. The performance of the baselines (SGD and Adam) are hard to believe. On MNIST, SGD and Adam barely reach 90% accuracy. These are with 10k steps. However, I m still not convinced that the paper should be accepted given that it wasn t tested on settings that are able to achieve competitive results on the chosen datasets, and because the experiments section could be polished and more organized.<|endoftext|>The paper aims at improving the generalization of the learned optimizers. Both theoretical analysis and empirical results are presented to demonstrate that flatness aware regularizers can enhance the generalization ability of optimizees. Strengths:  The generalization issue in L2O has been a significant problem preventing the applications of learned optimizers in the real world tasks, and it is good to develop an algorithm to improve the generalization. In the abstract, the authors mentioned two types of generalization, optimizer generalization and optimizee generalization. However, in the whole paper, I think the authors mainly focused on improving the optimizee generalization using two flatness aware regularizers. I did not see any explanations why these regularizers can improve the optimizer generalization. Did the authors try this simpler and efficient method compared with Hessian spectrum? It is worth carrying out some experiments to incorporate the flatness aware regularizers in the meta testing stage for both learned optimizers and traditional analytical ones. It would be good if the performance trained with the learned optimizer (trained with flatness aware regularizers) by the vanilla classification loss can outperform the performance trained with analytical ones by the regularized training objective. The current version needs some modifications on the experimental part to include more important information like running time and variance, as well as more experiments comparing the proposed algorithm with traditional optimizers.<|endoftext|>This work proposes to apply flatness aware regularizations to learning to optimize methods. They performed a theoretical analysis of the regularized L2O method and evaluated different variants of the regularizer on simple CNN and MLP models trained on CIFAR and MNIST. Similar to the effect of flatness aware regularization on the ordinary training, the regularizer improved the learned optimizers to find minima that generalizes better. 2.The authors performed both theoretical and empirical analysis of the hessian and entropy based regularizers, and demonstrated its advantages. The improvement over the baseline (without regularizer) seems quite large. Given the sizable improvement over the other L2O method, I wonder if this method can be applied to more realistic models such as Wide ResNet or ResNet 50 for ImageNet since the learned optimizer seems to generalize over different architectures and datasets as is discussed in section 5. For example, it would help to compare the Hessian spectrum or entropy of the minima found by learned optimizer with regularizations with the ones found by the baseline learned optimizer to show that it is indeed finding a flatter minima. This paper introduces flatness aware regularizer to L2O objective to improve the learned optimizers, which is a novel contribution.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; The paper proposes a new model based offline reinforcement learning, where a pool of Pareto optimal policies is trained on a model of the environment provided by offline RL data. The authors argue that providing a Pareto optimal set of policies is superior to prior methods that relied on regularization of the two objectives into a combined metric, as having a Pareto optimal set of policies enables one too explore a range of behavior that is inherent with trade off between model return vs model uncertainty in offline model based RL. Initially, the authors initialize a set of reference vectors along the Pareto frontiers to ensure a diverse set of Pareto policies can be found. Subsequently, the authors apply "Local Pareto Extension" where first a given policy is corrected to a desired range along the objective dimensions provided by the reference vectors. The method shows general outperformance compared to other methods in lower data quality settings in the D4RL benchmark. **Weaknesses** I think the paper could be strengthened by providing more detail on the following:  Clarifying how the policies for the score in Table 1 were chosen. Given that you have a Pareto front, did you choose the one with the highest score on the given benchmark? Figure 5 has many (red dot) Pareto optimal points, so it appears that many reference vectors were chosen. Could you clarify if this is the case? Could you comment on why you think UWAC outperforms P3 in the medium expert and expert cases?<|endoftext|>4.At test time, each policy in the pool is evaluated, and the best one is selected. The paper is well motivated, and the idea to approximate the Pareto front between model estimated performance and uncertainty is novel to my knowledge. The performance of P3 on the D4RL tasks is good. UWAC is clearly better for cases where there is expert data, but for the lower quality datasets, P3 handily outperforms the baselines. The authors explain that the computational cost of running P3 is similar to the cost of tuning other methods, which I can believe, but even if you don’t tune P3’s hyperparameters, you still have to draw samples to evaluate each policy, and in the offline setting we do not want to be drawing so many additional samples. Thus, while the authors frame the development of many diverse policies as a strength of their method, I feel that it is only a strength when paired with a strategy to choose from this set of policies (i.e.off policy evaluation), which is not presented in this paper. Currently P3 has an advantage at evaluation time because you get to try many policies and take the max. I think the high level strategy pursued by the paper is reasonable, and the results are promising.<|endoftext|>This paper proposes a new algorithm for model based offline reinforcement learning in which the learner attempts to optimally balance maximizing the reward under the learned model and minimizing the uncertainty of the model. The authors’ proposed method attempts to address this problem by identifying a relevant collection of policies lying on the Pareto frontier that trades off these two criteria. The technical contributions and main ideas are interesting and appear to be new to this problem setting. Extensive experimental results support that the method performs well in relevant benchmarks compared to a number of state of the art methods. The problem identified by the authors is highly relevant to the RL and offline RL community. The theory is not particularly new, but it does validate certain parts of the algorithm, which is nice. The experiments are extensive and compare against a large pool of recently proposed methods. P3 appears to very clearly out perform the other methods when low medium quality trajectories are given in the dataset. Weaknesses:  The algorithm is purported to apply to the case of offline RL, yet there is a critical concession in practice: one must have online access to the simulator/domain to actually select the best policy from the frontier   the algorithm is unable to determine this with offline data alone. Given that some online access is permitted to do the final selection for P3, I think a key experiment that is missing is a comparison with hyperparameter sweeps of the benchmark algorithms like MOReL, MOPO, UWAC, etc. Otherwise it seems that P3 has an unfair advantage. I think if such a comparison is made, then the previous issue is not as significant. Clearly there are some points in high reward regions, but also many in seemingly arbitrary areas. Can additional discussion be provided to aid interpretation?<|endoftext|>2021.This is an interesting and relevant direction to study, with promising empirical results for tasks with low  and medium quality datasets. The approach, called P3, finds a Pareto front of policies that trade off between obtaining high return (with respect to the model trained on the offline dataset) and minimizing the model s uncertainty. The empirical evaluation shows that P3 outperforms existing approaches when the offline dataset is of low or medium quality. In terms of strengths, this paper presents a novel perspective on navigating the trade off between model based return versus uncertainty, for offline RL. If I understand correctly, the results in Table 1 for P3 show the best performance across all policies found. This is significantly more computationally expensive than prior approaches (because at least several of the Pareto policies require separate training from scratch), and also requires more runs on the real environment. So it is not entirely a fair comparison.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; This paper presents an instance specific label smoothing (self distillation) technique, Pseudo KD, formulated as a two stage optimization problem. This paper presents an online self distillation method. The paper is well written and I enjoy reading it. However, the proposed link between label smoothing and knowledge distillation, in my opinion, is still based on previous research findings (e.g., Yuan et al., 2020). However, in section 3.2, this paper presents a closed form solution to the two stage optimization problem. 1. novelty is limited compared to the recent work on self distillation and instance specific label smoothing2. missing relevant baselines in the evaluation section<|endoftext|>The proposed bilevel programming framework is implemented by an alternating two stage process. The overall framework is equivalent to self distillation without the need for extra computational cost. The authors claim that the extensive experiments on both image classification and natural language understanding tasks validate the effectiveness of the proposed method. 2.The paper proposes an adaptive label smoothing method, which is novel to some extent. The method is called Pseudo Knowledge Distillation and the authors claim the method bridges the gap between label smoothing and knowledge distillation, however it seems to have no connection to Knowledge Distillation. The improvement of Pseudo KD in practice is not significant.<|endoftext|>The authors propose a self distillation approach that is derived from a series of observation made regarding how LS and KD work, linking in the two together in the process. The paper then experimentally shows the superiority of the proposed approach against KD on CIFAR 10/100 and languge datasets. Important comparisons against closely related methods are however missing. + The performance is tested on both vision and language data showing that the approach generalizes well. Little comparison with state of the art on knowledge distillation and while surpassing the KD baseline is important in this context its unclear if applying such method in practice will suffice. How well does this scale to larger datasets, such as ImageNet?<|endoftext|>This paper proposes a new formulation of an objective function for label smoothing regularization to unify label smoothing (LS) regularization and knowledge distillation (KD). The objective function learns an instance specific label smoothing regularization while training a model towards its prediction target. When the instance specific label smoothing regularization uses the output distribution of a teacher model, the objective function learns a KD model. The paper formulates the learning of the optimal instance specific label smoothing regularization as a bi level optimization problem and derived a closed form solution for the inner level of the problem. Experimental results on both image classification and natural language understanding tasks show that the proposed objective and learning algorithm are effective and yield results comparable to the baseline LS and KD models. The results of the propose Pseudo KD model does not seem particularly attractive in these two tables. The paper proposes a new formulation of an objective function for instance specific label smoothing regularization to unify label smoothing (LS) regularization and knowledge distillation (KD), which is quite interesting but is also largely based on the CVPR 2020 work by Yuan et al.The paper further formulates the proposed objective function as a bi level optimization problem and finds a closed form solution for the inner level of the problem. Experiments confirmed the effectiveness of the proposed objective function on both image classification and natural language understanding tasks, while the improvements over the baselines are somewhat limited. Overall this is a solid study with reasonable contributions.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 6; This paper focus on the contextual multi armed bandit with communication constraints. It provides some discussions and numerical results. Most importantly, there is no complete theoretical analysis (e.g.the upper bound/lower bound on the regret of the algorithm.) 1.The algorithm should be explicitly written out with its pseudocode. 1.By the end of Section 1, it states that the communication is one way from the decision maker towards to controller. Besides, how the goal of this work is different from existing papers is confusing. 2.The problem setup is not clear: is the state i.i.d.sampled from the distribution $P_S$ or not? 3.The relation between the constraint quantities $(\rho,R)$ and the discussions in Section 3 is not clear. Usually, we denote the original KL divergence by $D_{KL}$. The detailed comments are as above.<|endoftext|>This paper studies a contextual bandit problem where the decision maker must communicate its intended actions (given observations of the contexts) to a controller through a constrained communication channel. The original part of the paper is that the “bandit algorithm” must encode its actions into a compressed version that then serves to the controller. Strength:* The paper proposes an original problem that combines the challenges of bandit modelling and information constrained communication problems. * Minor: Lower level literature review: I feel like the background is given in the related work but I am surprised you were not able to connect your work better with the existing literature on contextual bandit.<|endoftext|>This paper studies a CMAB problem where the actions for multiple agents are sent from the decision maker over a rate limited communication channel. The authors developed information theoretic performance bound for Thompson sampling based policies, which reduce the problem to transmitting conditional probability distributions over a communication channel. A practical coding method was also developed. Numerical experiments were carried out to validate the proposed design. One knock of this paper is that the results were largely adapted from known information theoretic results: the optimal bound was derived based on Kramer&Savari; TS for RC CMAB was based on the double minimization problem in Cover&Thomas as well as the well known Blahut Arimoto algorithm; the practical coding scheme was based on the well known Lloyd algorithm for clustering. In fact, the practical coding scheme is about sending the state (via a compact state representation) to the corresponding agent. This may not be necessary to begin with in practice. Also, with the aforementioned argument, the controller "decoder" function $g_t^{(N)}$ should have the joint states of all agents as an input. This would enhance the decoder design. A solid work overall, but with some questions on the novelty and assumptions.<|endoftext|>This work studies a rate constrained contextual multi arm bandit (RC CMAB) problem: the decision maker has to make action decisions for multiple parallel (independent and identical) CMAB problems (i.e.agents), but can only communicate the actions for each CMAB problem to a controller through a rate constrained communication channel, from which the controller receives and decodes the actions of the decision maker, and applies such decoded decisions to each CMAB. The paper first formulates the problem as a policy compression problem under an information theoretic framework, and then characterizes the optimal compression scheme for infinite agents. In terms of weaknesses, my main concern is that the paper lacks theoretical performance guarantees for the clustering coding scheme. would provide us with a more in depth understanding of the performances of the clustering scheme in various scenarios. If providing theoretical analyses for the dependence on the state space is infeasible, at least having some experiments that address related issues would be beneficial. The paper is well written and well motivated. The analyses for numerical results are comprehensive and thorough.
Reject; rating score: 1; rating score: 3; rating score: 3; rating score: 8; They focus on M estimators for high dimensional linear regression. The SE equations describe the mean square error reconstruction of the M estimator. 1) It is badly written and full of typos. The authors should make a important effort to make it readable. "Pedagogical parts" like "Derived SEs from AMP", "Derived SEs from CGMT", "Derived SEs from LOO" are completely useless: a specialist does not need it (and anyway it is badly written I believe), and a non specialist will get nothing out of it. Not everyone is familiar with these concepts and literature, so a priori it may be interesting to show a relation between AMP and LOO, but it is not possible to do so by completely skipping all that is known.. Again, what is really not re assuring, is that the authors did not even mention these know facts and related literature. The part on the equivalence between CGMT and the rest seems more interesting to me. It is badly written, and no real insights are provided. The relevant literature is hugely missing. And, importantly, to cite much more carefully and expmain more thoroughly what is new or not in the paper (as such, I do not believe that much is actually new).<|endoftext|>The paper establishes equivalence of state evolution for approximate message passing (AMP), convex min max Gaussian theorem (CGMT), and Leaving one out (LOO) for three different problems: 1. a robust linear model estimator called the M estimator throughout the paper; 2. Lasso; and 3. logistic regression. Weaknesses:  The main issue I have with this paper is significance and novelty of the results. As I said, it is interesting to show that the recursion can be turned into each other for all these methods, but the paper does not give any intuition what the implications of such results are? The authors just say that they hope this leads to finding a deeper structure in such high dimensional problems. I find it interesting to see that the SEs can be turned into each other, but I believe it is not surprising and unless showing this equivalence leads to more interesting results, I do not see this result to be significant enough to be accepted for ICLR. Theorem 3 is not new. Minor comments/typos:  The paper has some grammatical and typographical errors. I believe the contributions of this paper are not significant enough.<|endoftext|>This paper considers the state evolution equations obtained via different methods (AMP, LOO and CGMT), and it shows that their fixed points are the same for some high dimensional inference problems (M estimation, LASSO, and logistic regression). Most of these state evolutions were obtained in recent work (which is properly cited here). Consequently, the main novelty of this contribution is in showing that the fixed points (when unique) are the same for different methods. STRENGHTSAMP, LOO and CGMT have been widely studied in the recent literature, and it has been already pointed out that, in some cases, results concerning the performance of a certain high dimensional inference task can be obtained using different methods (with very different proof techniques). WEAKNESSESThis paper falls short from this goal and does not offer much insight into *why* the fixed points are the same. The proofs of the equivalence of the fixed point are rather simple and they do not shed much light into the deep connections between e.g.AMP and CGMT. Proposition 3.2 appears to be novel, but its proof can be derived from existing work. Thus, I would recommend the authors to perform a thorough check during the revision. As a result, the contribution of this work appears to be clearly below the acceptance bar.<|endoftext|>This paper compares the so called "State Evolution" equations from 3 different derivations for the MSE of high dimensional M estimators in the proportional asymptotic regime (number of samples and features go to infinity at a fixed rate). While the main results of the paper were believed to be true by many researchers, to my knowledge this is the first paper that lays out clearly what the equivalence is between the outcomes of these 3 techniques. However, on closer inspection, the paper does not provide any insight into why this equivalence holds, most of the explanations are cursory at best. To my knowledge, the derivation from the CGMT technique is based solely on the fixed points of an optimization problem. On the other hand, the AMP derivation is essentially carefully constructing a dynamical system that has an equivalent scalar dynamical system. There is little discussion about the LOO technique or insights related to it. This body of work can serve as a way for researchers using one method to check their predictions with another method using a parameter transformation suggested in this paper. The lack of insight provided is also a concern but perhaps can be clarified in future works.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 8; Instead of modelling the clean label transition as typically done in previous literature, the authors propose to estimate the Bayes label transition using a DNN, motivate by several advantages including theoretically guaranteed Bayes label collection and smaller feasible solution space, hence empirically easier to model. Controlled experiments show consistent improvement over other SOTA methods in noisy label transition. To the best of my knowledge, this is the first paper that introduces to model Bayes label transition rather than clean label transition. The paper is well written and easy to follow. Results are very promising compared to other SOTA methods. Given that the idea of modelling Bayes label transition is a new and interesting idea, the techniques are mostly based on existing works:  Step 1: The Bayes label collection closely follows the framework proposed by Cheng et al.(2020).Note that the extension to multiclass setting, despite being straightforward as agreed by the authors, was also presented in the Supplementary Section B in Cheng et al.(2020) already. Step 2: By replacing clean labels with Bayes labels, the authors model transition matrix with a DNN. This seems to be the main contribution (as it is included in the paper title too?) For instance,  As stated in Sec 1, one of the motivations for modelling Bayes label transition is that Bayes label has a smaller feature solution space hence easier to model because it is one hot, but it would support the paper if the authors can include a formal definition and/or theoretical justification on this claim. Other minor concerns/suggestions:  Since the paper significantly relies on the distilled example collection from Cheng et al.(2020), Cheng et al.(2020) spent quite some space discussing the known issues and mitigation strategies of their distilled example collection (e.g., the automatically collected distilled examples only is not statistically consistent, covariate shift correction, how to collect without knowledge on noise rate bound). This paper introduces an interesting idea to model Bayes label transition rather than clean label transition based on several existing literatures.<|endoftext|>Therefore, the proposed noise generating process does not satisfy the bounded noise assumption. The authors should have compared with methods that are designed for instance dependent label noise. Finally, using DNNs for estimations has nothing new. 6.The clarity of this paper is of concern. The correctness and clarity are also of concern. For example, the paper used a long paragraph in page 1 to discuss about *statistical consistency*, but such information is not directly related to this work since this work does not have consistency results. This paper simply applies the setting and the idea in Cheng et al.(2020), the bounded instance dependent label noise paper. Instead, it is quite problematic. Therefore I vote for a reject. This is not a meaningful contribution. 2.The use of *Bayes label transition matrix* seems novel, but lacks justifications. The paper claims (second paragraph in page 2, (b)) that "The feasible solution space of the Bayes label transition matrix is much smaller than that of the clean label transition matrix. ", but no justifications are given for the claimed "sparsity". Recall that "sparsity" means that entries of a matrix are mostly zeros. In Section 4.2, the authors acknowledged that "if we have a distilled example for the i th class, we can only make use of it to learn the i th row of the transition matrix. ", but the explanation for why the other rows are not random nor learnable is far from satisfactory. 3.The use of DNNs to estimate *Bayes label transition matrix* has nothing new. No insights about how to design such nets are provided.<|endoftext|>This paper proposed a new noise transition matrix based label correction method for robust deep learning against label noise. Different from conventional methods, that estimating noise transition matrix from clean class posterior to the noisy one, the proposed method try to estimation the transition matrix from Bayes class posterior to the noisy one, which is expected to reduce the feasible solution space, and lead to better performance. Strength:(1) The view of dealing with Bayes class posterior instead of clean class posterior is new, and seems provides benefits. (2) Explicitly parameterizing the transition matrix is good, and might have advantages in task generalization consideration. Weakness:(1) No theoretical guarantee that dealing with Bayes class posterior transition is better than conventional noise transition matrix estimation methods, either in robustness or generalization error. (3) Experiments were only conducted on datasets with relatively few classes, such as CIFAR 10  and SVHN with only 10 classes. The idea is new and interesting, but final solution is not satisfactory, especially for theoretical guarantee.<|endoftext|>This transformation will not affect the practical use but makes the estimation of the matrix much easier. Specifically, this paper designs a DNN to estimate the transition matrix. Extensive experiments are conducted to support the proposed method. Strengths:This paper transforms the original clean transition matrix estimation problem to the estimation of transition from Bayes optimal labels to noisy labels, which is quite interesting and novel for the noisy label learning. A DNN is used to estimate the transition matrix, which can be optimized simultaneously with the classifier in an end to end manner. This is also a major contribution in this area. Extensive experiments are conducted to demonstrate the method. And the results have shown great improvements. Minor weaknesses:The authors introduced a parametric Bayes label transition network, which is a DNN. It would be good if the author can provide an example of the learned transition matrix from Bayes optimal labels to noisy labels. Or there should be some investigation about how the Bayes transition matrix was being estimated, empirically or theoretically. The method has made certain theoretical contributions to the label noise learning community. The algorithm also clearly outperforms the current SotA. Therefore, I would like to lean on the positive side of this paper.
Reject; rating score: 5; rating score: 5; rating score: 6; Previously IMP is criticized to be time consuming, layer independent and sub optimal in performance. The paper revisits IMP, a basic yet important pruning approach. 2.Extensive studies with detailed experimental setups are provided. 3.While the paper provides empirical studies for IMP, the technical contribution can be minor, as most components are existing methodologies. Post rebuttal  Thanks for the authors  response and paper revision, which addresses some of my concerns. While the paper introduces inspiring findings on how SLR (or CLR) help IMP, most components are from existing techniques.<|endoftext|>This work focuses on highlighting the strengths of Iterative Magnitude Pruning (IMP). While the implementation and proposed work are focused on resurrecting IMP as a valid SOTA baseline, more in depth work in alternative settings (unseen in existing literature) would be one possible way to help further highlight the novelty aspect of IMP. The authors have consistently clarified the difference in setup and mentioned that the exact outcomes differ from already existing work. As I described previously the consistent references and experimental structure borrowed from existing work hinder the novelty of the work.<|endoftext|>This paper investigated several recently proposed pruning stable approaches and compared them with the basic iterative magnitude based pruning (IMP), and observe that IMP actually performs on par or even better in the experiments. This paper is motivated by a simple question: whether the basic IMP method is enough for weights pruning? Through the whole paper, the authors conducted lots of experiments to show that IMP with a good retraining is as good as many recently proposed pruning methods. In addition, many new pruning algorithms are actually built for NLP and speech tasks. If the proposed claim can be further validated, I think it will be a significant contribution to the community.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; (2) The method proposed both naturally follows from the existing treatment of the problem in the literature and presents a novel approach to address it. The generative modeling framework presented by the authors is well motivated, described with a reasonably high degree of precision, and its relationship to other approaches (e.g.GEORGE, contrastive representation learning) is analyzed in a compelling manner. Weaknesses of the paper include the following:(1) Though the presentation and analysis of the proposed method is reasonable, comparisons to other methods are at some points overstated in the opinion of this reviewer. “equal partition constraint”  > If I understand correctly, this statement deserves more discussion. It also appears (at least as far as this reviewer understands it) that the current formulation imposes a constraint that subclasses be of equal size. Or an OOD example? Or for both?<|endoftext|>This has been pointed out in relation to generalized zero shot learning https://arxiv.org/pdf/1712.00981.pdf. I recommend accept since there is clear novelty in the paper, problem is important and well motivated, empirical results are convincing I advise to replace the last paragraph in the intro, which is too wordy and is not to the point  There have been some work studying the relationship between coarse and fine grained level supervision in the zero shot classification domain, for example https://arxiv.org/pdf/1906.11892.pdf. The model uses gaussian mixture approach to link the structures in the low granularity data with the few fine grained labels.<|endoftext|>Weaknesses:The main focus of the proposed approach is learning embeddings, but the evaluations are focused on classification. This problem is formulated as a superclass subclass latent model and learned with maximum likelihood via expectation maximization. The proposed approach, super class conditional Gaussian mixture (SCGM) model, defines a hierarchical Gaussian distribution on the class hierarchy that models both the superclasses and subclasses. The paper is well motivated and addresses the important problem of few shot multi granularity adaptation.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; The paper views different variants of DRO are simply instances of a finite sum composite optimization, from which efficient optimization algorithms were proposed. It regards DRO as a composite optimization problem and proposed a a novel mini batch constraint sampling for handling heavily constrained optimization problems. The main weakness is that the convergence analysis heavily depends on the existing work (Zhang and Xiao, 2019b). The originality and novelty of the proof techniques are very limited.<|endoftext|>In this paper, the authors propose to study different distributionally robust optimization problems in the same form of a composite optimization problem. The paper is well written and the method is well motivated. The theoretical analysis seems to be incremental and is very similar to those in the literature such as Fang et al.and Zhang & Xiao. How many repetitions are conducted to obtain the results? Although the algorithm structure and the relaxation might be different, it will be good to see the empirical comparison of the proposed algorithm with other algorithms on distributionally robust optimization such as those in Table 1.<|endoftext|>They also provide empirical results that demonstrate the effectiveness of our proposed algorithm with respect to the prior art in order to learn robust models from very large datasets. The proposed variance reduced algorithm mainly targets on solving a  finite sum composite optimization. The summarization of different variants of DRO are simply instances of a finite sum composite optimization is interestingWeaknesses:1. The paper is clearly written and provide comprehensive comparison among relative works.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; There experiments are in multi limbed mujoco tasks such as Walker, HalfCheetah, and Ant, and show that their method causes the agent to receive lower reward than a random adversary. — why is the case? That being said, it could be interesting to compare to their method with and without a learned model to replace the need for interactions with the environment. What the real world instances where you d have an adversary be able to select a different agent at each timestep but not be able to simple perturb all of them? Overall I felt this paper was below the acceptance threshold primarily because I felt the experiment domains rather limited and disappointing that the only baseline was random noise (and that the method seems to do only marginally better than random noise).<|endoftext|>***Clarity***This paper is not well written, and there are various grammatical errors throughout the paper. 4) The presented results are not surprising because the instability of MARL has been well studied. 5) The proposed method has access to the policy of all agents. Please show the results of the baseline described in Soundness (4).<|endoftext|>Although the authors mentioned that the target action is not available in the continuous action space, one can actually back prop from the centralized critic toward to actor policy with an objective to minimize the team reward. First, a model is trained to predict the next state based on the current state and action. 5.One of the contributions in this work is the victim selection during the attack. What are the assumptions of the attack?<|endoftext|>While this is missing in the work. Subsequently they solved an optimization problem with the learned dynamics to produce small perturbations. ****************************************After carefully reviewing the authors  responses and comments from other reviewers, I decide to keep the score. However, the numerical results shown in the paper to me are not very convincing. 4.In the numerical results, the authors showed different results for Mujoco environment. Moreover, what about topologies?
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 5; The reviewer s major concerns here are based on the novelty and the feasibility of the proposed approach. 1.Regarding the novelty, it is not very surprising to see that bagging / ensembling helps with the prediction stability for classifiers, especially when equipped with reject option so that the most ambiguous (hard) examples can somehow evade the strict metric computation. What seems more novel here is the improvement of gradient stability, which is still expected (since taking gradient is still a linear operation inside the expectation) but has involved less study. (This comment is slightly nitpicking)  On one hand, neural net ensembles can be technically constructed as a single neural net with an aggregating final layer and multiplicative numbers of parameters. For reject (abstention) options there is usually a penalty for saying "I m not sure". Theorem 3.1 is also referred as Theorem A.1 in multiple places. The paper could use a bit more theoretical discussions regarding the inner workings of the proposed method as well as the relationship between the work and existing ensemble studies to highlight its novelty.<|endoftext|>On seven benchmark datasets, the selective ensembles have zero points with inconsistent predictions. And an abstention rate as low as 1.5%. In the selective ensemble, predictions are aborted when the constituent models disagree sufficiently. Furthermore, using seven benchmark datasets, we show that the selective ensemble has zero points with inconsistent predictions and a low abstention rate of 1.5%. This paper addresses an important problem in machine learning that is of great interest to many researchers and practitioners. Experiments on benchmark data show that the proposed method seems to work well for the problem. In conclusion, we believe that this paper is worthy of acceptance.<|endoftext|>The topic is interesting and the algorithms nice and simple. Table 1 could do with comparitive numbers from a standard ensemble. Table 4 in particular is uninterpretable as it is unclear what the two numbersin the cells actuall are. Resampling can also involve features not just samples, like the random forest (an ensemble method) has demonstrated. This is not explored in the paper. While an interesting method the lack of clarity in the empirical experiments reduces the impact. Furthermore, the method reduces disagreement by essentially avoiding the problem through abstention; it s not really demonstrated that this has a practical advantage other than through highlighting specific cases for manual intervention or inspection.<|endoftext|>This paper presents a selective ensemble method. the authors demonstrated the effectiveness of their method on improving classification accuracy by abstaining from those that are difficult to classify. Although the idea is interesting and theoretically sound, it is unclear in which aspect the proposed method is superior to existing methods. If it s classification accuracy, the method should be compared with such methods as hard voting prediction, soft voting prediction, and other basic ensemble methods that aggregate individual predictions, and also individual model s prediction and MC dropout. Deep ensemble can also be compared. how does it relate to the use of a model in classifying data? For the proposed method, the absention rate may be adjustable by using different criteria for the statistical testing. It s unclear how the proposed method is beneficial in classification problems.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; The paper introduces a method for code editing from few examples. It allows to automatically generalize code edits from few support examples via adaptive multi extent composition. The approach is evaluated on two standard code editing datasets (C# fixer and Py fixer) against the Graph2Edit baselines. The method leads to a 8 10% improvement over baselines. The paper introduces the problem of code editing from few examples and formulates the method to use multiple examples in training and inference. While the task of code editing from few examples is new, it is conceptually similar to programming by example (PBE). The paper is relatively difficult to read, and some of the terminology is not explained. Program repair is arguably not a style change, and may lead to a different code semantics (see introduction, first paragraph). On the other hand, programming style is a well established concept which is different from editing style, which needs to be made more clear in text. The paper introduces a task of code editing from few support examples, and formulates the learning and inference methods to implement the composition method on top of the Graph2Edit method. While the evaluation is sound, and shows an improvement of accuracy as a function of number of support exemplars, one practical challenge in applying this technique would be to identify groups of support examples / identifying the edit intent.<|endoftext|>This paper argues that a single pair may not be enough to unambiguously perform the edit and multiple examples can help. The authors present an approach to aggregate edit representations from multiple such pairs. The approach is evaluated on code editing datasets in C# and Python. The scores for the edit representations are computed by a series of matching steps over the tree representations of the programs. The results show that the multi extent matching does better than all other choices. The paper aggregates edit representations from multiple examples using a number of complex steps. However, the examples come from a single type of fixer and share the same intent. In particular, it might suffice to use a single, most suitable examplar. The number of examples is also small in this case. An iterative baseline that takes each of the examples is also required to demonstrate that the combination of multiple examples is indeed better than any of the examples separately. The paper is difficult to understand at times. However, there is a conceptual gap about the need for combining representations from multiple examples.<|endoftext|>This paper presents a few shot learning approach to editing source code with a few exemplars. The presentation can be enhanced if the authors provide more theorical analysis of the proposed method with respect to AST representations. Unlike previous work that learns code editing with only one exemplar, the proposed approach learns the edit representations from a few exemplars. The proposed approach models the extents of node tree representations using a λ softmax with an adaptive composition of multiple extents. Experimental results on two code editing datasets demonstrate some improvement over baseline models. Comparison with more existing work (including traditional approaches) should be performed. Code editing styles can be better learned from more editing exemplars, so the results are expected. Currently, it only compared with two deep learning based baseline models. The evaluation metrics used in the paper, Macro and Micro accuracy,  are not explained. Therefore, it is difficult to interpret the experimental results. To capture the multi extent matching, we design a λ  softmax function by scaling the importance of nodes in an abstract syntax tree.<|endoftext|>This paper tackles the task of applying an edit to code that is similar to some prior edits (think of a linter edit that replaces double quotes with single quotes in Python). Q2: What s the use case for this multi exemplar task? However, this work recognizes that generalizing from a single exemplar can push baseline models to overfit the patern modifications of the exemplar. Is there some measurable metric of fitness for a set of exemplars, e.g., based on diversity or some other similar metric? Would you expect an editor to provide this functionality? An ensemble approach combines multiple values for this parameter. On a number of datasets generated by applying stock C# and Python fixers to GitHub code, this new approach outperforms baselines. The motivation for the problem was well presented, and made sense. In equation 2, they have $gs_k$. You argue that more are better for your approach, which makes intuitive sense, but presumably, it s just as easy to have a bunch of exemplars that happen to modify a declaration as in Support #1 of Case a in your example, as it is to have one of those. Your model would be just as likely to be convinced that the rewrite only applies to declaration statements. Are you expecting a developer to write a few example edits and let the model apply them to diverse "queries"? # QuestionsQ1: How many exemplars are enough?
Reject; rating score: 5; rating score: 5; rating score: 6; The paper proposes an improvement over previously published MQ forecaster model for multi horizon forecasts in time series data. Pros:a) Significant gains in forecast accuracy on the proprietary data, and on the public retail forecasting taskb) Including forecast volatility as part of the model leads to a significant reduction in excess volatility over the MQ CNN modelCons:a) A large part of the empirical analysis is conducted on a proprietary dataset, and thus those results are not reproducible or verifiable.<|endoftext|>This paper looks into applying Transformer with encoder decoder architecture for forecasting. The work follows the previous work of multi horizon quantile forecasts that predict quantile for each horizon. The framework applies a 1D conv and a MLP network on time series feature to model positional information, and apply an attention head for each horizon.<|endoftext|>This study proposes a decoder encoder attention mechanism, a positional encoding mechanism and a decoder self attention scheme to improve the forecast accuracy and minimise the excess variations in time series forecasting applications. Given the significance of the three contributions, supported by the strong experimental setup (well established benchmarks and datasets) and empirical evidence (statistical significance), I am recommending to accept this paper. What is the reason for excluding other forecasting benchmarks, such as DeepAR, TFT, MQ RNN in Table 3 ?
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 8; The main contribution of the paper is a differential private mechanism for federated learning. Further, they provide experimental evaluation of the same using MNIST and Fashion MNIST datasets. Strengths of the paper: They study a problem that is of fundamental importance in privacy preserving machine learning. Weaknesses of the paper:Firstly from a theoretical point of view, the contribution seems quite incremental. In the abstract the authors claim "highly non trivial" analysis   which is nothing more than elementary calculus and statistical analysis. The central theoretical contribution seems to be a differentially private multi party protocol to compute sum of vectors. In a nutshell, the protocol is simply to add noise (based on "Skellam distribution") by each participant and adding them up in a distributed fashion. For example, getting an accuracy of ~80% on MNIST is unacceptable. Further, there is no "non private" baseline provided.<|endoftext|>This paper presents a mechanism based on Skellam distribution (called Distributed Skellam Mechanism (DSM)) to prevent privacy leakage for federated learning. It provides analysis of privacy guarantee in the decentralized setting. Also, DSM is applied to differentially private federated learning with distributed SGD and quantized gradients. The paper is clearly structured and the theoretical proof is very logical. 2.Some nice properties of Skellam mechanism are considered, i.e., sum of Skellam random variables is Skellam distributed. Lack of novelty. Similar to this paper, the idea of applying the Skellam mechanism to federated learning has already been explored, and extended to high dimensional settings in a recent paper. Doubt about the accuracy privacy trade offs. Due to composition theorems applicable in learning settings, the mechanism will be applied many times. However, a similar work has already been done.<|endoftext|>This paper studies federated learning under the distributed DP framework [KLS 2021] and proposes the distributed Skellam mechanism (DSM). This thus results in a small utility loss compared to the centralized model. Since the proposed algorithm does not perform random rotation on $x$ in the pre processing steps, in the worst case scenario, $\Delta_\infty$ can be as large as $\Delta_2$. In other words, there should be a modular clipping step before the aggregation (because SecAgg only works on a finite group). I believe all these issues can be fixed by simply pre processing $x_i$ with a random rotation and replacing the summation with modular sum though. 4.In the experiments, DSM is compared with DDG and cpSGD. It would be good if the authors can also include the accuracy of the (centralized) Gaussian mechanism as a baseline, which would let us know how close the DSM is to the centralized error. The contribution is solid since the result directly improves the previous DDG scheme. However, there are some minor issues that the authors should clarify or address.<|endoftext|>I therefore recommend accepting the paper. The paper considers the problem of distributed differentially private (DP) learning using black box secure multi party computation (MPC) for aggregating gradients for learning the model. However, since the existing tools (mainly Gaussian mechanism) typically assume continuous space, while MPC works with discrete values, this does not work in practice: the main problem is that the discretised noise is not infinitely divisible so the sum is not guaranteed to follow the same distribution as the individual contributions. To remedy the problem, the authors propose the distributed Skellam mechanism, which is both dicrete and infinitely divisible. The authors show that the Skellam mechanism privacy cost can be calculated using Rényi DP (RDP), and continue to show that it performs significantly better than the existing methods based on binomial noise and discrete Gaussian noise using MNIST and Fashion MNIST data for testing. For example, looking at [1], there does seem to be calculations for some tighter bounds for the binomial mechanism. Is there some reason not to consider this in the comparisons? Overall I think this is a nice paper.
Reject; rating score: 5; rating score: 5; rating score: 6; The manuscript challenges the widely believed assumption that Bayesian neural networks (BNNs) are well suit for out of distribution (OOD) detection, by showing empirical results obtained using infinite width (allowing the exact inference, because a network can be equivalently represented by a GP with the included kernel) and finite width networks (requiring the approximate inference). The manuscript includes not only finite width networks but also infinite width networks in the analyses.<|endoftext|>This paper carries out an analysis that motivates that the use of the Bayesian predictive distribution and its uncertainty is not appropriate for detecting out of distribution data. Namely, a Gaussian process. They argue that there is a trade off between good generalization and having high uncertainty on OOD. It is natural to expect that exact inference in infinite width networks under common architectural choices does not necessarily lead to desirable OOD behavior. It will reduce uncertainty, but that need not be bad for OOD detection. This will limit its impact in the community. I also the paper a bit misleading since in several figures the data presented for training is incompatible with the assumptions implied by the chosen prior. The paper lack novelty.<|endoftext|>In this work, the authors challenge the assumption that underlies many recent works that Bayesian neural networks should be well suited to out of distribution detection. In order to do this, the authors focus on a function space view by examining the properties of infinite width BNNs. They use this analysis to argue that BNNs may not necessarily be well suited to OOD detection. They further argue that there is a tradeoff between OOD generalization and uncertainty, and finally propose an alternative method of validating OOD properties of models. The paper is well written and clear, and I enjoyed reading it.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 5; Strengths:This paper tackles an important and relevant problem of RL in practice   imitation learning with access to expert data that might be different from the actual experimental/online environment, and more importantly, that lacks contextual (confounders) information. Results in this paper are not a huge advancement of our understanding of RL but I do think this direction should be explored at some point. The authors should expand the legend a bit to explain the figure in more detail. Overall, this is a technically sound paper. Both the theoretical and empirical results in this paper are expected. But it is good to have a paper to confirm the related theory.<|endoftext|>The authors considered the problem of imitation and reinforcement learning in the setting of contextual MDP with latent confounders. It provides cases that is infeasible to solve this problem from expert data. Therefore, it is quite interesting to see the impossibility results. It is unclear why the authors put it in the appendix. Therefore, the advantage of using the proposed algorithm is not justified theoretically.<|endoftext|>This paper considers a very interesting setting:  the expert data in the imitation learning is confounded by the context, and the distribution of context may vary between expert data and online data. The authors give a analysis about the limitation of imitation learning methods under the counfounded imitation learning, and proposed a solution for it even if the counfounder distribution varies. The setting considered in this paper is interesting and practical, and the authors give a theoretical analysis about the limitation of imitation learning in such a setting. Causal imitation learning with unobserved confounders. In summary, I think that the setting considered this paper is quite interesting, but it would be better to perform more experiments to evaluate the performance of the proposed method.<|endoftext|>The proposed algorithm is potentially interesting, but I have concerns about the relevance of the proposed problem setting and how it compares to some simpler baselines. The paper would benefit from more thorough analysis (both theoretical and empirical) of the proposed algorithm as well as additional discussion about the problem setting. I m not convinced this is a very interesting result, and it s likely that stronger assumptions will be needed to obtain any stronger results. While the proposed algorithm is interesting, the theoretical analysis is not very useful, since (as noted by the authors) it does not show that the expert data is actually useful for policy learning. More generally, it would also be important to reference and compare to existing work on incorporating imperfect expert data in imitation and reinforcement learning settings (for example [1]).
Reject; rating score: 1; rating score: 3; rating score: 5; In essense, the proposed method leans some lower dimensional representation of the frames pertaining to observed sequences, and then postulates an RNN type of model in the leaned latent space to model temporal dynamics. The combination of some rudimentary ideas such as learning some framewise latent space representations (ideally via some flow model) and then using some sort of RNN to capture temporal dynamics within is far from seemingly  novel, let alone interesting. The experimental results do not support publication of an methodologically boring paper.<|endoftext|>This paper proposes LatTe Flows, a novel autoregressive flow model with autoencoders to learn latent embeddings from time series.<|endoftext|>Though the idea is very interesting, and the experimental results are also very encouraging, I have below major/minor concerns on this paper:  Major 1:The idea of modeling the latent representations with normalizing flow is not new. According to Figures one and two, it seems the proposed method is also using a restrictive prior? The model does not improve in every scenario, but the overall performance does look good.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; This paper proposes a novel method for single shot domain adaptation for GANs. The writing and presentation is kind of messy to me. More details of II2S should be provided for readability.<|endoftext|>4.When compared with other methods, using the CLIP model is kind of unfair. The results in the paper are a little amazing and can be used in style transfer on human faces. This proposed method by combining StyleGAN and CLIP is elegant.<|endoftext|>Overall I think the paper is interesting, as it proposes a novel way for one shot GAN adaptation. Strengths:  The proposed method is novel based on best of my knowledge. The writing can be further refined. The achieves results are very impressive, notably better than competing methods as demonstrated in fig 4.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; The paoer us well written and intuitive. All in all the paper is a nice read and provides interesting insights. “dichotomies if”  > “dichotomies is”  I find Figure 2 hard to interpret, mainly because of the vertical axis being “capacity”.<|endoftext|>I will keep my score at 6 at this point. The main result in the paper is showing that the number of dichotomies (different binary classifications) is related to the dimension of the fixed subspace of the representations, rather than the dimension of the representation. ** Good writing: The authors did a great job writing the paper. after rebuttal:I appreciate the authors  thoughtful response.<|endoftext|>(I guess not.) This means it is easier to interpret and potentially useful. The authors also consider the case of useful but not quite equivariant operations such as pooling. Lastly, they provide a convincing empirical verification of their results.<|endoftext|>The authors first use a classical notion of the perceptron capacity to offer a quantification of the expressivity. The idea is novel and interesting. Overall, I vote for rejection.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 6; This paper introduces an entropic coding method that is able to compress a sentence into a latent space and perform text generation through a uniform sampling from the implicit latent space. 1.The writing of this paper has much room for improvement. It s hard to fully understand the method without referring to the pseudo code.<|endoftext|>The paper shows some good attributes of this method via some experiments. 1.I feel the paper is poorly organized. 2.After digesting it, I think I like to problem this paper discussed, and it s very pertinent to the representation learning. Leaving all the materials in an algorithm table without much explanations make it hard to understand.<|endoftext|>The paper proposes to use an autoregressive language model s hidden state to index multidimensional arithmetic coding representations of vocabularies and sentences. My next biggest concern is that I don t think I quite understand the problem this paper is trying to solve in terms of downstream use. Finally, I should disclose: I have reviewed this paper before (but do not know its authors or anything beyond that) and some more fundamental concerns are unchanged from my last review, though the paper has improved noticeably in my mind (thank you for these improvements, it really has gotten better).<|endoftext|>I have reviewed this paper last year, I can see the authors have reorganized the content significantly. I will give a weak acceptance this time and willing to discuss among reviewers. Strengths:  A novel approach for latent modelling  Strong results in terms of validity  WeaknessIn Table 1, the validity percentage of a Transformer with 512 latent dimensionality is only 17.2%. The algorithm is based on arithmetic coding.
Accept (Poster); rating score: 6; rating score: 5; rating score: 5; rating score: 5; The aim of the paper is to revisit topic modeling through the lens of word embeddings and using a bidirectional version optimal transport. TIn this paper each document is considered as a set of embeddings words, each topic is seen as a mixture of embedding living in the same space as words embeddingsThe aim is to minimize the distance of the topic distribution and the words distribution for each documentsStrength of the paper : the point of view is relatively original, since it provides a new framework for topic modelling. It relies on a version of bidirectional optimal transport which is quite origineal and embedd words and topics in the same space. The relevance of the model is assess by numerical experiments comparing in terms of perplexity this approach with others ones more classical as vanilla LDA, OTLDA... Nevertheless, this idea is not new and has been already proposed in other papers<|endoftext|>This paper presents a new topic modeling framework called WeTe: each document in text corpus is represented as a bag of word embeddings vectors, and each topic is modeled as an embedding vector in the shared word embedding space. WeTe minimizes the expected difference between those two sets over all documents. A bidirectional transport based method is proposed to learn the topic embeddings as well as topic proportions for documents efficiently. Extensive experiments on news and web pages show that the proposed model outperforms competitive methods for both deriving high quality latent topics and generating better document representations for clustering tasks. Given a few recent works on learning dense word vectors jointly with latent document level mixtures of topic vectors, the novelty of this paper is not significant. 2009.4.For comparison of time/space complexities, it d be great if the authors can provide a detailed complexity analysis. This paper presents extensive experiments on news and web pages to show the superiority of the proposed method, compared to other state of the art methods.<|endoftext|>Jointly learning latent topic representations with word embeddings has caught some attention in the past. The reason why researchers choose this route is to exploit the complementary advantage of both these models, for instance, to improve the performance of the model on short texts. In this paper, the authors study a new topic model where topics and words are encoded in the same embedding space. A probabilistic bidirectional transport model is developed that measures the difference between the discrete distributions in the embedding space. The authors also discuss several key advantages of the model. Experiments are then followed with topic diversity and coherence measures as evaluation metrics which are widely used in the literature. The authors also present the derivation of the model and the key equations that demonstrate how the model works. As a result, the authors have conducted some experiments to demonstrate that the model improves upon some existing methods.<|endoftext|>The manuscript proposes a combined topic modeling and word embedding method. The idea is to define word and topic embeddings in the same space, represent documents based on both of them, and use a transport based method to train these two representations to be similar to each other. An optimal transport (or conditional transport) approach such as the one proposed is a promising direction. The ability to initialize with pre trained word embeddings and fine tune the model is valuable (though this paper is not the first to do this). This is already a crowded space which includes the cited papers, such as the ETM and neural topic models, and several others as well. Transformer based language models such as BERT have become the state of the art for representation learning in natural language processing in recent times. It also needs to discuss the merits of the proposed approach versus transformer based language models. The training algorithm could have been explained more clearly, and in the paper itself instead of in the appendix. Neural embedding allocation: Distributed representations of topic models.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; To tackle the issue, it proposes a noise contrastive loss and some architectural refinements based on the variational information bottleneck. * The empirical comparison with baselines is rather comprehensive which includes different metrics. The proposed method leads to consistent better performance over output regularization methods. * The claim that the noise contrastive loss increases the separability of correct and incorrect samples is not supported by enough empirical evidence. It would be great to have a plot with the original VIBN model (i.e., Fig.4) in the main body. A discussion for the comparison between Fig.2 and Fig.4 can also highlight the effect of the noise contrastive loss. Thus, I recommend a ‘reject’ for the paper but will consider raising my score if my concerns are well addressed.<|endoftext|>This work suggests a *noise contrastive loss* for variational information bottleneck networks to resolve the poor performance at separating correct and incorrect predictions in regularization methods. **Cons**My main concern is empirical verification. The reviewer thinks that the proposed method should be tested to determine if it works well with different convolutional networks other than ResNet.<|endoftext|>The paper proposes a new model NC VIBN for the classification tasks. Please could you clarify what is what is the connection between the entropy term with the “noise contrastive”. It seems to be that the only difference between the proposed framework and conventional VAE framework is the entropy term and the additional prior on the network weights. It seems to me that the minor difference here is that the generative model now belongs to a variational inference family such as VAE rather than adversarial training (i.e., GAN) in [a] [b]. Please correct me if I misunderstood anything. However, it seems to me that the empirical evidence does not fully support this claim. I noticed that on Tiny ImageNet100 which is seemingly more complex than CIFAR10/100, the proposed method could hardly beat the other state of art method, leaving me doubt the effectiveness of the method.<|endoftext|>To do so, they combine various techniques: the information bottleneck, a variational distribution over neural net weights, a noise contrastive loss term, and an L2 normalisation step for the last part of encoder. The L2 normalisation step is an additional minor contribution. While the use of a variational distribution over neural net weights is not novel in itself, its combination with the other components is shown to be helpful. Good performance from the proposed methods, especially in terms of the negative log likelihood (NLLH). Could the authors comment on this? Readers might find it helpful if the paper comments on the similarity between these methods.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; This paper analyses a variant of generalized federated averaging ([Wang et al.](https://arxiv.org/abs/2107.06917)) with partial worker participation and asynchrony in the stateful (i.e., the worker specific data can be saved on the server) and stateless settings, which characterize cross silo and cross device federated learning ([Kairouz et al.](https://arxiv.org/abs/1912.04977?utm_source feedburner&utm_medium feed&utm_campaign Feed%253A+arxiv%252FQSXk+%2528ExcitingAds%2521+cs+updates+on+arXiv.org%2529)) respectively. Then they upper bound the convergence of their algorithms in the stateful and stateless setting when the local updates and delay are bounded. Some experiments are provided to measure the effect of the anarchic worker behavior. ***## Minor comments1) Linear speed up is an abused term in FL literature. Thus the theoretical result falls short of giving any novel insights. I was hoping your analysis doesn t require these boundedness assumptions.<|endoftext|>This paper proposes a general FL framework, called anarchic federated learning (AFL), that allows voluntary client participation of clients, with individual update steps and delay. I m willing to give the authors a chance to address these concerns, and I ll be open to adjusting the score accordingly. (3) Following up on (2), Theorem 3 is not well explained. (5) Typo: CS AFL in page 2 should be AFL CS. The other responses are largely in line with my understanding of the paper.<|endoftext|>Although the paper I mentioned lets the server sample clients asynchronously, algorithmically looks like these two works are doing the same thing. Can the work give some references on why the server is assumed to have no historical information of the works? (2) The submission assumes uniform arrival for cross device FL, and bounded delay for cross silo FL. This point should be validated empirically (by simulating systems heterogeneity, for example). * In practice, the delays may be unbounded (e.g., when each device only participates once or twice during the training); so the two assumptions can be too strong. My major concerns are the first three points in the  concerns  part of my comments, especially the differences between the submission and that related work. update I have read the authors  responses. My concerns regarding the uniform arrival and bounded delay assumptions in federated learning, the experiments, etc remain.<|endoftext|>However, I found the following issues regarding AFL concerning. I hope the authors can address them in the rebuttal. The anarchic federated learning is vulnerable to attacks and can be slower than FedAvg because of unbounded delay, local steps, and heterogeneity of clients.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; This paper presents the original idea of applying rank coding to classical LSTM networks in order to improve their performance. The paper is very clearly written and has an extensive introduction that allows to introduce the scientific problem. The method is briefly described and then applied in experimental demonstrations. One.A particularly interesting point is the trade off curves between speed and accuracy as well as the performance of the network depending on the use or not of rank coding. In the first practical example it seems that you use this coding in different time windows that correspond to different bins in a sequence and that these sequences are then processed independently.<|endoftext|>The authors introduce a new way to train RNNs using rank order coding (ROC). Furthermore, the speed accuracy trade off is tunable by varying the threshold. The authors validate their idea using LSTMs on two toy problems, and then on MNIST and on the Google Speech Command dataset. In addition, the authors seem to target the spiking neural network community (part of which uses ROC). In discrete time, the LIF can be seen as a recurrent ANN unit, and BPTT could be used as well.<|endoftext|>The authors propose a method for fast and efficient classification of sequential data. Their model reduces inference time by learning a rank code that is inspired by spiking neural networks. Reported results show improved inference times in two toy sequence classification tasks, temporal MNIST, and in Google Speech Commands classification (compared to models without optimizing timing of inference through learning a rank code). Increasing inference speed comes with a minimal decrease in accuracy, the authors, however, introduce and show the effectiveness of a regularization term that allows for tuning of this speed accuracy trade off. PROS: I think the proposed method is very practical and the ideas of this paper are organized logically. Overall, I vote for accepting.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 3; This paper investigates whether incorporating anisotropy (treating neighbors differently using latent functions and thus resulting in $\mathcal{O}(E)$ memory cost) is necessary for boosting GCN s accuracy. They argue that the proposed EGC with $\mathcal{O}(V)$ memory requirement can achieve higher accuracy than prior anisotropy based works (e.g., GAT) using adaptive filters and thus achieve (1) better accuracy than vanilla GCN; (2) and less memory cost and latency than anisotropy based methods. The hardware efficiency and memory cost are arguably two bottlenecks in the GCN inference and training and they are often correlated. * The adaptive filter is a kind of meta learning approach that allows different basis filters to explore different latent spaces and finally weighted summed. I suppose the experiments are a fair comparison with a similar number of parameters as Fig.3 shows.* A clean codebase is provided. In addition, why the inference time of GraphSAGE is also lower than GCN since we will not sample the subgraphs during inference? I am also wondering whether EGC can also be implemented in a sampling based way? * Could you elaborate more about the potential impact on the GCN hardware acceleration? I think the introduction and related works description clearly recap the background of both GCN algorithm and potential algorithm accelerator co design, and also the current dilemma in the GCN community, that is, the more accurate but less time/memory efficient anisotropy approaches VS. less accurate but more time/memory efficient vanilla GCN approaches.<|endoftext|>The authors propose a different formulation for a graph neural network, focused on achieving accuracy equal to anisotropic GNNs without anisotropic mechanisms. Strengths:  The authors  direct challenge to accepted folk knowledge about anisotropic GNNs is healthy and welcome. The intrepretations are useful and provide good intuition for how to place their proposed model in the context of other current work. It also seems to indicate that parameter normalized results do not correlate particularly well with memory footprint across models, which weakens the argument for O(V) vs. O(E). I agree that there is definitely a lot of potential for this approach to be efficient, but I wish the paper had strong support for it. The memory behavior of sampled methods is vastly different than non sampled GNNs when dealing with distributed execution or on graphs that are substantially larger than single device memory capacity. This is technically correct but misleading. The results support their claim that they can achieve competitive accuracy, but the quantitative computation and memory results are somewhat underwhelming.<|endoftext|>The authors claim that this architecture is not only better performing, and use less memory, but more efficient for GPUs through the use of sparse matrix multiplication. These are then weighted and summed by a weighting vector calculated as a function of the receiving node. Since there are no functions that take as input both sending, receiving or edge inputs, the memory will scale with the number of nodes. The paper conducts a thorough analysis on latency, which I appreciate. This is an important point and I encourage work in the GNN literature that addresses this. I do not think these results are state of the art   simply that they are better than the baselines reported. Thus, I think the impact may be limited. Some nits: It would be nice to have more detail on the OGB and Zinc results   perhaps explain why this model does not perform as well as MolHIV. You cite "Training graph neural networks with 1000 layers" which I believe scales O(V), and I think achieves better results on OGBN ARXIV. The ideas seem very related to existing work.<|endoftext|>This paper claimed they designed a new GNN architecture that achieves state of the art performance with lower memory consumption and latency. More specifically, the proposed model uses memory proportional to the number of vertices in the graph $O(V)$, in contrast to competing methods which require memory proportional to the number of edges $O(E)$. The experiments found that the proposed efficient model could achieve higher accuracy than competing approaches across six large and varied datasets against strong baselines. In terms of performance, the improvements compared with PNA are also not very convincing. Currently, section 4 is well written, but the conclusions are limited and may have some flaws. The major weakness of this paper, in my opinion, is that this paper seems to have misunderstood the real bottleneck to scale up GNNs, and the proposed architecture may not be suitable for large graphs (i.e., size over a million). To sum up, I think the architecture proposed in the paper does not solve the scalability problem on large graphs.
Accept (Poster); rating score: 6; rating score: 5; rating score: 5; rating score: 5; In this paper, a novel method of structured pruning of neural networks using tropical geometry has been proposed. The core idea is to use the tools from tropical geometry to represent a neural network as tropical polynomials and then apply k means clustering using some distance metrics that work on such representation. Experiments have been performed on linear layers of a binary classification network trained on MNIST (3/5 and 4/9) and LeNet5 and VGG trained on MNIST and Fashion MNIST. This is a novel idea that has not been explored widely for structured network compression. 2.A proper introduction to tropical geometry and the related tool has been done for non domain experts. 3.Illustrations are very helpful in understanding the method. Comparison with non tropical methods has not been performed. 3.Without pseudo code or code segment(s) for zonotope generators part, it is hard to reproduce the experiments. The paper will also be more accessible to non domain experts if included. I am leaning towards acceptance of the paper as it provides a very interesting and new idea. As I am not a domain expert in tropical geometry, I would wait for other expert reviewers to judge the theoretical part. If authors can provide pseudo code or code segment that makes this paper more accessible for me leading to better understanding, then I am willing to change my score.<|endoftext|>They present approximation bounds for tropical polynomials based on Hausdorff distance and use this to motivate the development of two neural network compression methods. This is pointed out but not explained. This suggests that the bounds of Prop. 4 and 5 aren t very useful. We should expect that if we can use more weights, we can at least avoid making the error worse, but this is not borne out by the computations in Table 2. Compression of networks such as AlexNet and VGG is of limited interest as such architectures have been out of date for years. For all experiments, the authors should clearly state the current SOTA method for pruning and include those results as well. While beating SOTA is not necessarily a requirement, it at least needs to be provided for comparison. Positive feedback:  The background on tropical geometry is well explained. Theorem 2 has a very clean statement. This paper has some interesting results (such as Theorem 2) and the techniques are interesting and seem well motivated (although this paper is not the first to study tropical geometric neural compression). Thus, I hesitate to recommend acceptance at this time. It would be great if the authors can provide clear responses to my questions above.<|endoftext|>This paper proposes a compression method using a framework based on geometrical zonotope reduction. The authors further analyze the error bounds of the proposed methods and compare its performance with modern pruning techniques. I think the main contribution of the work would be novelty of the proposed method. Pros: The proposed method seems novel and has theoretical guarantees. Cons:   The paper is a bit hard to process for general audience in machine learning, and the topic of the work does not seem to align well with ICLR. As this work is outside my domain of expertise, this is only an educated guess type of review, and I am willing to hear how other reviewers  opinions. Still, I have decided to keep my score as a) the empirical contribution is a bit limited, and as mentioned by the authors, the experiments are mainly proof of concept; and b) though the theoretical contribution is novel, it still needs a bit more work to justify its significance.<|endoftext|>The paper studies neural networks from the perspective of tropical geometry and applies its theoretical results to develop a novel algorithm for the compression of neural networks with ReLU activations. The CNNs used in the comparison in Figure 4 are quite old. It would be good to remove LeNet and have a more modern architecture, like a ResNet instead. Having a comparison between the paper’s algorithm and the one from Smyrnis & Maragos would be beneficial. Transformer networks contain only fully connected layers, so it would be really interesting to apply the algorithm to them. ThiNet is a filter pruning method, so I assume that at least some of the methods also prune the convolutional layers. 5.The paper should state in the abstract that the algorithms only apply to networks with only ReLU activations. Regarding (3): The most “modern” pruning technique the authors compare to is ThiNet from 2017 which is clearly not state of the art by looking at Figure 3 of the pruning survey [1]. Since the experiments focus on small architectures, it is unclear to me whether the proposed algorithm can be used to compress larger networks. Since theoretical work has an important value on its own, the compression algorithm is not required to be of immediate practical interest.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; Interestingly the authors use feature visualisation to show that although the weights are shared, the network still learns hierarchical features. # Weaknesses    The minimal effect of weight sharing was already shown in some cases [1,2], although this paper analyses it much more thoroughly and shows a broader picture of the effect        There are some indications that the similarity would break down with deeper networks. # Questions/suggestions    For the maze problem, it is unclear how the performance is measured. Is it pixel wise, or the whole maze should be correct for the sample to be considered correct? For the ResNet architectures, the parameters of the batch norm are not shared between the layer. Probably visualisation of the same unit in different depths would be informative. It would be nice to visualise the same neuron over “depth”. Together with the previous point, I would be curious about the gap with a more realistically sized ResNet.<|endoftext|>At the current scope, I am concerned this work runs the risk of confusing matters, while a broader look would be more clarifying for the community. To start, experiments could cover deeper nets, and then go on to look at other tasks like sequence modeling (as done by implicit nets, which also share weights). The prior existence of such networks leaves this work without technical novelty. The maze task is non standard, and therefore less informative than any other established task, although it does make some sense. As this paper could bring more attention to recurrence and depth, as earlier work seems not to have done, it could inform the community. Sharing the affine parameters would make the model even more recurrent, while still allowing for different statistics across recurrent iterations. This work could provide more in depth analysis, or more up to date analysis w.r.t.the models considered, but prior work must be credited, above all in case the earlier work can inspire more analysis. [related work] As a possible counterpoint to theory of layer specialization, you may be interested in reading https://arxiv.org/abs/1605.06431 for its experiments that question the importance of any one layer in a residual network.<|endoftext|>To me the most exciting part of the paper was the analysis in section 5.2. Overall, the authors observe that across all models and tasks, the recurrent and stacked networks achieve similar performance. They also verify this with batch normalization and dilated CNNs in some tasks. Finally, they visualize filter activations at different depths for both CIFAR 10 & ImageNet, finding comparable feature maps in both architectures. But in several cases:1. Instead, they just achieve similar accuracy to the original model The above comment is in reference to:> First, the trend in performance as a function of effective depth is not always monotonic and yet the effects of adding iterations to recurrent models closely match the trends when adding unique layers to feed forward networks. This further suggests that the types of functions approximated by the model (either as stacked non linearities or recurrent ones) is more important.<|endoftext|>The authors explore the similarity between neural networks that accrue information processing via depth and via recurrent iterations. Update after author response  I have read the authors  response and updated manuscript, after which I am increasing my score to 6. Cons:I have the following concerns about this work:1. How many steps of recurrence is implemented in these layers? 5.I feel that the paper makes quite a few very strong claims in the Discussion section that aren t fully tested by the Methods.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; This paper proposes a task adaptive semantic feature learning mechanism to incorporate semantic features for both support and query samples. Furthermore, the proposed modality combination approaches are well known methods. Even though the technical novelty of the paper is limited, as also supported by the new experiments added, the responses to my questions are satisfying.<|endoftext|>This paper proposes a few shot learning method that incorporates semantic features (i.e., class label embeddings) for both support and query samples. The experiments are unconvincing. Experiments show the effectiveness of the proposed method.<|endoftext|>This paper describes a few shot learning approach that takes into account the semantic relatedness of the ground truth labels rather than just using them as binary labels. This way both the support and query examples would have semantic features and empirically it has shown to work on many few shot benchmarks on computer vision datasets. Questions:(i) The method (TASNet) is built over Protonet (Snell et al) method.<|endoftext|>In few shot learning for image classification, visual features alone may represent multiple objects in an image. With that said, the proposed solution looks overly complicated. The authors show that the additional "semantic features" (because Glove embedding is used) help improve few shot image classification. The solution is a combination of different existing tricks.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; The paper develops a method for repairing neural networks with ReLU activationfunctions. The scalability of the method is also discussed and additionalexperiments on AlexNet are provided. I think it would be impractical to remove all buggy behaviour (outside of atest set) from image classifiers. In light of this a metric that is potentiallymore important than the ones measured in the experiments is  the robustness ofclassifiers w.r.t to input transformations such as noise perturbations.<|endoftext|>The notation in eq.(3) that (c,d) \in {(c,d)...} is weird. The main idea is to synthesize a patch network and combine it with the buggy network. Based on the contributions, I would recommend acceptance of this work. I enjoyed reading the paper, it is well written.<|endoftext|>I do think it is important to show in the main paper 1) the application of the techniques on more complex models; 2) area repairs (for robustness properties for instance) on perception networks. Overall, while I think the idea is interesting, more work needs to be done to show the benefit beyond minimal change for point wise repair and to address the scalability issues. I do have some concerns and questions.<|endoftext|>Different with existing techniques that retrain the model or change the weights, the proposed approach generates a patch in the original model, i.e., add sub models. Overall, this paper is well written and interesting. 2) This paper is well written and easy to follow. 3) The approach has some novelty. The usage of LP solving may also introduce the scalability issues.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; The paper describes a new active learning algorithm for classification problems. The authors should make this clearer from the start of the paper. ### StrengthsS1) This paper identifies and addresses a common issue in active learning, which is that selection criteria generally don t address class imbalance and decision boundary imbalance. This guarantee is only with respect to the proposed selection criteria $f(S)$, but there is no guarantee that a set $S$ which maximizes $f(S)$ is actually a good training set. This point should be made clearer. **W2) Missing discussions and comparisons against other related works**The paper is missing discussions and comparisons against about other relevant active learning techniques such as [Deep Bayesian Active Learning (Gal et al., 2017)](https://proceedings.mlr.press/v70/gal17a) and [Active Learning via Influence Functions (Xu and Kazantsev, 2019)](http://arxiv.org/abs/1905.13183). However, active learning via influence functions does not seem to fall within the uncertainty vs. diversity framework that this submission describes. They provide decent empirical evidence to suggest that their methods are at least as good as existing methods, and sometimes better. However, I hesitate to recommend this paper for acceptance as is, due to misleading claims, incomplete discussions, and a number of clarity issues.<|endoftext|>This paper presents a new method for batch active learning in deep neural networks, which aims to progressively construct compact subsets from a training dataset that maximize accuracy of learned models. The resulting optimization problem is solved using a greedy algorithm. Proofs of important properties of the method (submodularity of objective functions) are provided in the appendix. However, no ablation experiments are included to study the importance of boundary level balancing constraints in addition to class level balancing. Theoretical analysis or ablation studies for important hyperparameters of the proposed method are also missing from the experiments. Lack of comparison to state of the art active learning methods. In addition, comparisons in the paper are limited to classification datasets, but not for more label intensive tasks such as semantic segmentation, which [B, C] has studied. This makes it hard to determine the statistical significance of results when comparing two approaches. However, as detailed in the Cons section above, the importance of many elements of the proposed method are not adequately justified through theoretical or empirical results, and comparisons to prior work are lacking in number of baselines as well as range of tasks.<|endoftext|>The paper proposes a technique for incorporating class balance and boundary balance constraints to the sub modular optimization problem and performs experiments over several image data sets to show the value of class and boundary balancing. AFTER REBUTTAL:I thank the authors for their detailed rebuttal and for testing out some of the suggested ablations. However, I feel that the paper can be improved with additional ablation and analysis. Below are some suggestions. An alternative to the above point, in terms of delineating the value of parameter tuning would be to implement the class and decision boundary constraints with respect to existing active learning strategies. How well does the proposed method work in covering class and decision boundaries? These analyses can answer: how much does the current method improve over baselines in selecting class and boundary balanced data, whether there is room for more improvement, and what is more important (class or boundary balance?)<|endoftext|>Hence its greedy algorithm comes with approximation guarantees (Nemhauser et al., 1978). 2.The proposed boundary balancing constraint is interesting. The question is that the subset with optimal approximation of a dataset in some feature space doesn’t guarantee that it is the optimal for training neural networks. The performance comparison also verified it. Otherwise, it is unclear which constraint works. 3.The experiments lack the comparison to state of the art subset selection methods. Is this assumption realistic in the experimental setting (active learning)? The paper lacks deep analysis and solid experiments that explain why the new components are better. The contribution is somewhat incremental.
Reject; rating score: 3; rating score: 5; rating score: 6; The paper describes a scaling law for approximating the number of samples needed for pre training to reach a certain performance on a downstream task. Weaknesses:  The scaling law is derived by empirical observation. This is, in my opinion, an oversimplification of the scaling law.<|endoftext|>This paper proposes a scaling law for transfer learning that takes into account  the size of both the pretraining and finetuning datasets. Strenght  The paper proposes a scaling law for transfer learning that takes into account the size of the pretraining dataset. For example, could the authors discussed other choices of g(n), showed the limitation and motivate why the chosen one is the most resonable one. >After rebuttal. I agree of the tremendous importance of the sim2real transfer, but the derivations presented here are not specific to the sim2real scenario.<|endoftext|>This paper postulates a scaling law for pre training and transfer learning via fine tuning. In one part, the authors say, "Theoretical analysis was also attempted [Hutter, 2021][Bahri et al., 2021]". They find that parameters of their scaling law can be estimated to find very good fits to empirical error across all the different transfer tasks. The error rates for all tasks are in the high 80s and 90s.
Reject; rating score: 3; rating score: 5; rating score: 5; The paper studied clustering of data that has multiple dimensions of relation. It proposed a chained non negative matrix factorization (NMF) technique, which, according to the authors, is different from existing multiple view NMF. The authors provided mathematical discussion and evaluation on a small synthetic dataset.<|endoftext|>This paper proposes a linked NMF based clustering to tackle the problem of linked dataset. While the idea of incorporating star schema information is not new, this paper proposes a new approach to model the non multi view information. While this paper provides novel idea, it lacks some comparison and justification in comparison to the existing literature.<|endoftext|>The paper presents a new technique, grounded in non negative matrix factorization (NMF), for unsupervised, representation learning of linked data. Finally, the paper overstates its claim about the proposed technique being a novel paradigm in artificial intelligence. So, while it is a new technique, the paper does not present a new paradigm. The paper has some serious clarity issues that detract from the reader being able to understand the paper and contributions of the paper.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 3; This paper studies the "expressivity" of emergent language in language games. In this paper, the authors propose two factors of the underlying language game that can affect the expressivity of emergent language: context complexity and unpredicability. In referential games, the context is the distractor candidates for a sample. Complexity denotes how likely a "close" distractor will be included in the candidates. The hypotheses are clearly stated, reasonable, and well supported by the experiment results. Besides the main results, the paper also proposes a simple update on training loss, contrastive loss, which improved the model behavior. #### Weakness  The introduced concepts (context complexity and unpredictability) are finally simplified to be only dependent on the candidate set size $|C|$. Regarding the proposed training loss, to my understanding, it simply replaces the original candidate set with the batch. In this way, the contribution does not look significant. #### Questions & Suggestions  The definition of key concepts, context complexity and unpredictability, do not seem fully accurate. This paper studies "expressivity" of emergent language, which is empirically the transferring ability of the language. The paper also defines "context complexity" and "unpredictability" of the underlying language game. The study shows that these two factors both contribute to the language expressivity, but they are contradictory on the candidate set size $|C|$ so a trade off is needed.<|endoftext|>The paper studies the properties of emergent languages in DL based language games. In this setting, complexity refers to the similarity between different objects in a given context. In their definition, the notion of unpredictability can be thought of as how stable the information necessary for encoding is across different trials. Their primary contribution is to support the hypothesis that the expressivity of emergent language is determined by (and a trade off between) the complexity and unpredictability of context in language games. They argue that mutual information is not the most appropriate measure to evaluate the expressivity of languages. They propose a contrastive loss which they show helps mitigate the issue of the collapse of message types. The definitions and different notions introduced in the paper seem natural and reasonable. I see no obvious issues with the paper.<|endoftext|>This paper studies the expressivity, complexity and unpredictability of emergent languages in referential games. The authors demonstrate that the expressivity of emergent languages is a trade off between the complexity and unpredictability of the context that the languages are used in. They also introduce a contrastive loss based training method for referential games that alleviates the collapse of message types often seen when using other standard training methods. The experiments are extensive which is great to see. **Weaknesses:**I have included actions that can be taken to strengthen this paper. The key contributions of the paper is predicated on the definition and closed form expressions for unpredictability and complexity.<|endoftext|>To be honest, I find the unpredictability measure more opaque after author response. Further, the authors show that defining a softmax loss over the entire batch of possible references (dubbed the  contrastive  loss) outperforms the  referential  loss used in previous works. I brought up this point in my original review, and it is not addressed in the author response. The authors note that mutual information is a poor measure for expressivity, and I like their proposed metric. I believe that there is much scope to precisely control complexity and unpredictability using such schemes, which I believe would make a future revision of this paper much more solid. Most notably, the issue of the definition of unpredictability and how it affects the communication are still unclear, and I believe this needs to be clarified substantially in future revisions. However, I m not convinced by the unpredictability measure. One final thing is that the  contrastive  loss presented in equation 1 is contrasted with a  referential  loss which is never explicitly given in the paper. To make the exposition self contained, it would at least help to see how the  referential  loss is computed, so that the reader can make a better comparison. Defining unpredictability on a longer history simply changes the power of Equation 3 in our setting, but doesn’t influence the growth relationship between unpredictability and context size. This is only assuming a uniform bigram/trigram/n gram distribution.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; The authors make many claims that are not adequately justified. I find this explanation very confusing. It would be great if the authors explain more clearly what the advantage is with respect to using images where we know the location of the relevant object.<|endoftext|>However, several drawbacks including writing, novelty and fair comparison suggests me to reject this paper. The author should definitely discuss with that paper.<|endoftext|>&nbsp;### Other CommentsIn addition to Weaknesses, there are concerns about the following items. The authors argue that the time complexity of the proposed method is not high, then why not use the entire dataset?<|endoftext|>Overall, I vote for accepting, even with some concerns unsolved. This work demonstrates to be superior to previous CAMs on corresponding metrics. The adversarial patch is novel and contributes to this filed.
Accept (Poster); rating score: 8; rating score: 6; rating score: 3; In general the paper is innovative and solid, there are several issues as in the main review section that need to be addressed. The author showed that with reward shaping or applying relu on advantage, optimizing the reformulated policy is implicitly maximizing lower bound of mutual information across agents  policies. In the explanation text, line 19 is not mentioned. The algorithm is evaluated on a variety of multi agent decentralized learning environments with promising results.<|endoftext|>This work introduces InfoPG for learning coordination in fully decentralized multi agent games. The authors present a theoretical analysis to show that, under InfoPG, policy gradient optimizes a form of mutual information related to coordination. # Strengths  The paper is well written. Much of the paper is devoted to demonstrating the *implicit* connection between MI of agents  actions and the InfoPG policy formulation. Good performance is encouraging, but the most relevant analyses for validating the intuitions of the method are largely ignored in the main text. Consider making it a bit more clear what "InfoPG" refers to.<|endoftext|>This paper discusses a hierarchical, iterated reasoning scheme for learning policies conditioned on the other agents  policies at that stage,  maximizing both the mutual information between agent policies and agent rewards. Unfortunately my biggest issue with the paper is lack of clarity. This paper has interesting ideas but is lacking in how it anchors its contribution to related work and compares against it. I believe it should be revised substantially before it is suitable for publication at a venue such as ICLR.
Reject; rating score: 3; rating score: 5; rating score: 5; The paper proposes BIGRoC (Boosting Image Generation via a Robust Classifier), a method to refine samples from a base generative model using a robust classifier. [Empirical Evaluation]It is appreciable that the experiments have been conducted on several base models.<|endoftext|>This paper presents a post processing method to improve the GAN results. 2.In section 2.2, the robust classifier method optimizes both the input images and the network, but in section 3, it only optimizes the input images. The definition is not aligned.<|endoftext|>This paper proposes a model agnostic method for improving the quality of images produced by generative models. It is model agnostic and can be used to improve generative models without re training. The only requirement is a robust classifier trained on the same dataset as the generative models. Even better, the same classifier can help all generative models trained on the dataset. Although the technical novelty is not significant, the proposed method has advantages and could be useful in applications.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; In order to fool both the victim model $f$ and a detector $g$, SPGD selectively optimise either $f$ or $g$ depending on whether the modified input is misclassified as the target class, while OPGD further orthogonalizes the gradients. Evaluation on four previously unbroken, state of the art defence methods demonstrate the effectiveness of the proposed attacks. I find this paper pleasant to read in general: SPGD and OPGD are well motivated and clearly explained, while the experimental verification is sufficient and convincing. I only have several minor comments:1. How effective are SPGD and OPGD against an ensemble of detectors instead of just one? The proposed attacks for creating adversarial samples are simple but effective, and the evaluation is sufficient.<|endoftext|>It proposes a new optimization algorithm to simultaneously meet two different requirements. It verifies its effectiveness on several state of the art adversarial example detection methods. ### ProsBreaking the defenses of adversarial example detection is an important yet underdeveloped field. The approach proposed in this paper is straightforward and effective in practice. The experimental evaluations seem reasonable. ### ConsThe proposed optimization formulation is based on intuition. It would be better if the author can provide theoretical evidences that can explain its effectiveness. Currently, the empirical evaluations mainly shows its performance against the detection defense. It can be improved by adding more analytical results. In addition, I want to clarify that I served as a reviewer of this paper for its last submission.<|endoftext|>The authors propose an attack that could break 4 adversarial detection methods published recently. Traditionally, attacks against detection methods have attempted to maximize the loss for both classification and detection simultaneously. However, using a toy example the authors show that this is suboptimal, as it may not find the worst case adversaries. The authors also propose a variant of the attack by considering gradient steps for the classification pipeline to be orthogonal to the gradients of the detection pipeline and vice versa. Finally, the paper shows that these two proposed attacks completely circumvent four recent adversarial detection methods. The ROC curves help in better understanding. [1,2]    The clarity on the discussion related to the four defences that are broken could be improved. Could the authors clarify why an epsilon bound of 0.01 was used to draw the ROC curves?<|endoftext|>This paper considers the problem of finding adversarial examples that simultaneously defeat a detector of adversarial examples. These are evaluated against four existing detection based defence methods, with successful results. The main proposed approaches are a fairly simple update to existing attack methods, though appear to be quite effective. In section 5.4, the specification of the two ranges of features is the same. It does not seem clear in section 3.1, even if it were corrected, why a Lagrangian formulation would not work. (Note that that is an independent statement of whether gradient descent would work). However, the presentation of the paper is a bit confused in places and the motivation for the approach a bit unclearly argued. I am not clear what the authors are arguing exactly.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; The paper studies the problem of parameter estimation of one layer ReLU neural networks where the data is generated by the same network and with probability p, some of the examples are corrupted to arbitrary outliers.They give an algorithm with theoretical guarantees on the sample complexity and also, provide simulations on how the estimation error scales with the important parameters. The problem of learning ReLU Gaussian distribution over outliers that this work is looking at is not very well motivated and I am not convinced by the significance of the problem. The techniques used in this paper look incremental in addition to [1] where they worked with the same problem but without outliers. There is a pi(2 p)3 term which does not go down to 0.<|endoftext|>Given observations $x_1, \cdots, x_n$ assume that a fraction $p$ of the observations are drawn from $$ x   ReLU(Wz + b), \text{ where } z \sim N(0, I_k) $$and the rest are drawn from an arbitrary distribution $\mathcal{G}.$ The authors study the problem of estimating $W, b$ in this model. The error does not tend to 0 even as the sample size $n\rightarrow 0$. This is true even there are no outliers $(p 1)$.<|endoftext|>The authors study the problem of learning the parameters of a generative model defined by a single layer relu network in the presence of outliers from the Huber contamination model. They derive a gradient based algorithm for estimating the norms and angles between rows of the generating matrix, which suffices for estimating the matrix WW^T (which is the only estimable parameters in this setting). This seems unusual. It appears to me that the current result does not recover the Wu et al.result as the probability p of uncorruption tends to 1, which is weird (see below; please correct me if I m wrong).<|endoftext|>The paper presents and analyzes an approach to learn single layer ReLU networks (assuming Gaussian distributed input data) in the Huber contamination model where a constant fraction of the training samples may be corrupted. Strengths+ The paper is well written. + The techniques in this paper are fairly natural and intuitive (for the specific assumptions made in this paper   see below.) + The results make sense and are consistent with existing results for learning single layer NNs without outliers.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; rating score: 8; I think comparing the attention weights to the results of a baseline probe with and without KI would be necessary to convince a reader that the obtained attention weights can really be interpreted like that. 5.Unsatisfactory verification: GCS is only verified on two entity typing datasets. The GCS model is trained by maximizing the mutual information (MI) of the entity representations of GCS and the entity representaitons obtained from the PLM. In experiments, GCS is first verified on entity typing. Subsequently, GCS is used to investigate what information is learned by ERNIE and K Adapter. How are you taking into account the different relation types when doing GAT in GCS? The paper is well structured. It is still not 100% clear to me what exactly is ultimately done in the approach, but also in the experiments, where it is not clear what exactly is being done in Table 2 (appendix F specifies the GCS architecture and interpretation principle, which have already been explained in the paper, rather than exactly explaining the experimental setup of Table 2). The change of factual knowledge could be measured by measuring local KG subgraph MI with PLM representations with and without KI using Hou and Sachan s method. **before rebuttal**  While the paper focuses on an interesting question, due to the above issues, in my current understanding, I don t think it deserves publication in its current form at a venue like ICLR. Without the authors discussing these possibilities and comparing to their approach, it is not clear why the presented approach should be chosen over more obvious possibilities.<|endoftext|>This paper presents a method to analyze how knowledge integration methods perform. The proposed approach, graph convolution simulator (GCS) is to understand the KI process by simulating the change of mutual information with graph Fourier transformation as well as interpretable graph attention. And then the authors proposed to use graph convolution to simplify the computation of GFT. The findings with GCS about the factors such as the popularity of entities and the size of KI dataset are beneficial for future research in KI. Weakness:  The study is quite limited to two particular KI methods and it s not convincing that the GCS method can be applied to analyze general KI methods and lead to correct conclusions. And are there serious justification why entity representation should be the starting point of your formulation? It s a bit vague and unjustified. The key findings, although useful, are quite trivial and the analysis on the K Adapter and ERNIE may not be particularly interesting to the audience.<|endoftext|>This paper presents a probe model to analyze how well existing methods integrate knowledge into pretrianed language models. The authors then propose a method to quickly evaluate the mutual information by using a graph attention network. I really appreciate the theorical foundation that the authors take effort to provide, which is interesting and promising. Strengths: see "Summary Of The Paper". There seems to be a gap in the logic for me. However, I can still see that some experimental results are not very good (e.g., Sec.4.2.3).Considering all these, I would like to raise my score to 6. So which layer we should use for interpretation? For example, can we consider the new pipeline with GCNs as a simplied model for which MI could be easily computed (e.g., we only need to compute the MI for the attention layers)? It is not obvious to me why we cannot using model agnostic explanation methods for black box models, e.g., [1][2]? I think there are multiple types of edges, so if we set G(vi) to N_vi, we will ignore the relation type, right? For example, this sentence is difficult for me to understand: "The transformation is a simulation of the KI process, i.e., MI change, and it promise the accuracy."<|endoftext|>Finally, additional experiments show that increasing the size of the training corpora does not correlate with improved knowledge integration, motivating more careful development of future knowledge enhanced pretraining methods. Overall, the paper is well written and presents a very useful method of analyzing how structured knowledge of entities may or may not be integrated into a language model through additional pretraining. The detailed analysis characterizing what kinds of knowledge are well integrated, catastrophically remembered, or catastrophically forgotten based on relation complexity is helpful at demonstrating what kinds of probing the GCS method is capable of. Tying GCS to downstream task performance also helps to showcase the validity of the method, although this could be further strengthened by expanding on these results (see below). While the results compare each of K Adapter and ERNIE with their versions that drop certain pretraining examples, it would also be helpful to compare to BERT/RoBERTa results in the same table, as it is not immediately clear which differences between the two versions of each model are meaningful versus negligible without a baseline performance to compare to (though the random dropping results in the appendix somewhat help to clarify this). It would also be worth exploring downstream task performance in finer grained detail to further support the method   for instance, how does an entity being catastrophically remembered/forgotten affect entity typing performance on examples that involve that entity?<|endoftext|>The paper claims that the Knowledge Integration (KI) process can be simulated with Graph convolutions and designed a Graph Convolution Simulator (GCS) to analyze the performance of two popular KI models, K Adapter and ERNIE. The experiments are well conducted and the conclusions are insightful. The paper proposes to measure the amount of integrated knowledge with mutual information and shows the strong connection between the KI process and the Graph Fourier Transformation and then model it as graph convolution. The analysis process is interesting and coped with interesting insights. A few comments on the paper:1. 2.The authors uses attention weight on edges as measurement of relevance. It s okay to make the analysis using the attention weight in this paper, but it is arguable that attentions are not always explanations, i.e."Attention is not Explanation" (Jain et al.2019).Have you thought of performing you analysis using other measurement? ", how do you measure the "the numberof aligned sentences for knowledge triples in the dataset"?
Reject; rating score: 3; rating score: 6; rating score: 6; The paper characterizes the escape rate of SGD for the mean square loss via the perspective of SDE. The paper provides an intuitive introduction to the theoretical understanding of the papers and good sectional clarity in the way main concepts are presented. It seems like the application of existing techniques on a restrictive class of functions (with mean squared loss) may be somewhat interesting, but I m unclear why (it s not clearly stated in the introduction). Furthermore, the escape rate of SGD is with respect to a local minima, theta^*, so it is unclear how you define theta^* when it is not a global minima.<|endoftext|>This paper considers the rate at which the SGD iterations will escape the valley around a local minimum. Under some approximation assumptions, this paper shows that the SGD noise covariance is highly structured as it aligns with the Hessian at the local minimum in the immediate vicinity. The contribution of this paper is clearly presented. 2.I also did not see a clear statement/reference of the escape rate (16). Maybe it is trivial to the authors, but I think for the broader audience, it would be very helpful if more reference/discuss could be included.<|endoftext|>This paper studies the behavior of SGD around the minimum. The new SDE is a dynamics on the log loss landscape with a simple additive noise. Numerical experiments are conducted to justify the assumptions made in the analysis, and verify the theoretical conclusions in the case of linear regression. Numerically, it is verified for neural network models. Further, the escape rate depends on the effective non vanishing dimension of the landscape. The connection of the stability of SGD around minima with the effective dimension is novel. Will the results be different? I am willing to increase the score if strong numerical results are provided. Could the authors provide justifications (either theoretically or numerically), that the behavior is not changed by considering an isotropic noise here.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; This paper studies the optimal early stopping time of gradient flow for linear regression in two regimes: over informative features or under informative features, where the first regime means the features contain sufficient information to generate the response, while the second regime means responses rely on additional information beyond those in the features. In the two different regimes, the paper shows that the optimal early stopping time behaves quite differently. # Pros:+ The different behaviors of optimal stopping time for gradient flow in over  or under informative settings seem to be interesting and novel. + The observation from neural network experiments are also inspiring. However in this paper, if I understand it correctly, the authors aim to mean two different feature generating processes. Under parameterization paragraph: I am not sure whether or not these are "discovered" by the authors. The claims about case $ n \gg d$ is clearly incorrect. All the following claims need revision. Similar assumptions on hidden variable is also made for Thm2. In the second setting, it is more like an issue of insufficient observation, and no wonder one can stop latter as $d$ increases (where the information in features increases). The current paper is misleading and needs a significant revision.<|endoftext|>The authors show that the optimal early stopping time behaves differently in overparameterized and underparameterized regimes. Specifically, the optimal early stopping time decreases when increasing model size in the overparameterized regime, but increases with model size in the underparameterized regime. Some empirical results demonstrate the theoretical results in linear regression and deep neural networks. I think the authors should clarify what do they mean by “model size” in general (e.g.in neural networks), is it the number of parameters? However, I think the novelty of the paper is quite limited. Also, the optimal ridge parameter $\lambda\sim\frac{1}{d}$ (lemma 3 therein), and since stopping time decreases when $\lambda$ increases, the optimal stopping time is monotonic in model size, which is also the result in this paper. Would be good to show that the results hold when playing with the amount of label noise. I think Figure 13 should go more to the right (more epochs) to see the entire curve. In figures 11,12 – can you explain how $a,b$ are selected ? Therefore, I don’t recommend the paper for acceptance at this stage.<|endoftext|>This paper studies an interesting and important problem: what is the optimal early stopping time (considering the model dimension and sample size, especially under an overparameterization setting)? Considering a standard linear least squares regression problem, the gradient flow is expressed as a differential equation. Given such a result from an earlier work, the paper studies the optimal stopping time that minimizes the expected risk and presents high probability (upper and lower) bounds for the overparameterization setting. Empirical results validate the theory. 2.The results are insightful for me. The main results about the overparameterization setting agree with some empirical findings and make a step towards understanding the training procedure of machine learning algorithms. I understand that this is an early theoretical attempt and I notice that the authors also find some related work falls into the same setting. However, the approaches in this paper rely on the assumption that the estimation can be expressed as a function of early stopping time. Currently, I tend to accept it.<|endoftext|>This work looks at implicit (algorithmic) regularization of learning algorithms, with a particular focus on the impact of early stopping in the context of iterative gradient based learning algorithms. One is also left wondering if it would be straightforward to obtain high probability bounds on the risk, since bounds are with high probability for the stopping time control. Some are notation issues, others are explanation issues. On p.6, the authors say that "Theorem 2 implies that when the number of features is larger than the model dimension, optimal early stopping time increases as $n$ and $d$ increases." To the best of my understanding, the theoretical results obtained by the authors (optimal stopping time control + resulting risk bounds) are new and provide more refined insights that the existing literature with respect to the impact of sample size and dimensionality, despite the limiting Gaussian assumptions. My overall take on this paper is that there are some results of value here, but the paper itself needs some work, so I am left on the borderline with this work, tending to accept, but with the caveat that I am not very familiar with the developments in this line of work since Ali et al.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 5; Sparse neural networks, on the other hand, can reduce the cost of each weak learner significantly. This work also provides some interesting insights into the ensemble of sparse neural networks. Given that smaller networks cost less training and inference FLOPs, it feels natural to use more of them for ensembling. **Minor**  I am not sure it makes sense to introduce FreeTickets name as the individual methods are introduced later under different synonyms (DST and EDST). I think authors can introduce these methods later in the text, while focus on the main problem this paper addresses (making ensemble more efficient?). However, the writing and the overall structure of the paper feels a bit rushed: there are many typos and some important baselines/papers are missing. Given the experimental nature of this work, I think it is important to further polish the paper both in terms of writing and experiments. I recommend rephrasing them accordingly.  "<|endoftext|>The authors propose to improve the generalization of dynamic sparse neural network training methods, such as RiGL, by learning an ensemble of these models. This result will drive further research into DST methods, as well as encourage researchers to explore the practicality of DST training for making DNN research more computationally accessible. This paper is cited by the authors elsewhere but not in reference to these results. The authors expand on these results however, and I believe it would strengthen the author s results on the diversity of solutions if they cited this work in the appropriate places and compared their findings (which I believe corroborate that paper s). I can only reasonably assume they are initialized from \theta_s in the exploration phase. Or is this referring to each of the subnetwork s refinements?<|endoftext|>After RebuttalI thank the authors for their detailed response to each of the comments posted and a well done revision of their manuscript. The proposed methodology is supported strongly by the main set of results. Conceptually and practically, the improvements offered by training an ensemble of sparse networks to augment the failures of a single dense/sparse model is extremely useful. While the comparison to dense networks and their ensembles is fair from the perspective of performance, could the authors clarify if methods like static LTR ensemble and others are similarly "cheap" to  DST and EDST? Could the authors comment on the significance of these values and their changes? Fig.3 and Section 5.2 do not comprehensively cover the tSNE projections of other models tested in Tables 1,2 and 3. Section 5.4: Based on Fig.4 there is a claim that the pattern of ensemble accuracy is highly correlated with the accuracy of the individual sub network. I encourage the authors to support this with a quantifiable measure. Could the authors clarify the statement in the manuscript?<|endoftext|>The authors proposed a method to train an ensemble of sparse DNNs by means of DST such as RigL which, within the same training flop budget, is reported to generalize better than the enclosing dense net or other efficient ensembles; it is also reported to have higher inference time efficiency. From the presentation of the results, I do not understand how emsembling $M$ sparse subnets would be efficient at inference time. The paper presented hypotheses on the generalization performance of the sparse ensemble trained as such, which are interesting scientific questions, but did not provide convincing results or discussions to validate or disprove the hypotheses. For example, it is not clear if *diversity*, i.e.disagreement between the subnets, is responsible for the generalization. From a practical perspective, DST or EDST are by nature handicapped in producing diverse subnets, because the enclosing dense net architecture is still the same in order to obtain diversity, wouldn t one ensemble individual nets of completely different architecture rather than sparse subnets of the same enclosing dense net? Minor concerns:  Writing.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 6; This paper proposes an empirical study of different mechanisms often set in place to prevent overfitting in Deep RL. Hyperparameter sensitivity is (unfortunately) a recurrent, known problem in RL (even without the problem of generalization). And (unfortunately), no new insights are provided in this section. Although the empirical findings are interesting and well conducted, they don t fully cover the span of regularization methods. Additionally, it is mostly redundant with the introduction section and most citations already appear in the paper. Despite these aspects and the fact that it lacks content (see next comments), the paper is rather clear, well organized and pleasant to read. The paper itself needs an important effort of editing (it is still a very preliminary version, assembled in a rush), and a significant contribution to improve the generalization capability of RL algorithms, to have a meaningful impact for the community. As the authors point out, many recent work have concentrated on this aspect, including the three papers by Raileanu et al, which serve as a basis for this work. This categorization applies both to supervised and to reinforcement learning. And a natural question is then to assess whether the techniques evaluated in the paper actually cover a significant span of the possible regularization techniques overall. Besides the raw results, the reader does not get much insight from the reading. Therefore, I think the problem should not be to evaluate whether "policies have level specific features" but rather to assess whether these features are theme independent. In Section 5, the discussion on auxiliary tasks deserves a lot more details. Also, data augmentation was mixed up with domain randomization. I assume it is a moment of inattention coupled with a lack of proof reading.<|endoftext|>They investigate approaches that use data augmentation and domain invariant features and provide some new insights. The authors also show that simple auxiliary tasks can improve generalization policies and that other approaches. ### Strengths The paper s overarching goal is to understand why current RL algorithms (aimed at improving generalization) still fall short in certain settings. This is an important problem and answers to this question would be valuable for the community. I think these insights would be valuable for the community and those experiments seemed reasonably well designed for the conclusions to hold. ### WeaknessesHowever, the paper does not seem to provide a clear and definite answer to the question. In addition, some of the claims do not seem to be supported by the experiments. However, I am not convinced the experiments described in this paper support the claims made by the authors. Given all the above, I do not think the paper is ready for publication. However, this does not seem to be the case here, but I found the description to be quite unclear. I encourage the authors to take the feedback into account in order to improve the paper, and resubmit at a future conference. However, there are games where the differences are significant, and others where they are not.<|endoftext|>The paper investigates (1) data augmentation, (2) policy generalization via domain confusion (3) generalization via auxiliary tasks (4) effects of hyperparameter selection and (5) adaptation to new levels. **Strengths**The paper attempts to address why policies learned for video games have generalization issues, which is an important problem and helps us design better models and learning algorithms in the future. Most of the conclusions are already known. That is also already known, and several others papers have shown that. Some of these papers have been cited in the paper as well. This statement is not correct. Some of the conclusions are not aligned with the observations of other works. However, works such as [A] show better visual encoders are important for generalization to unseen environments. I am not sure what this means. First, I do not see any novel conclusion in the paper:(1) It is already shown by several papers that inverse models are helpful. (2) It is already known that data augmentation techniques are required for better transfer.<|endoftext|>The paper studies generalization of several previously published visual RL SOTA algorithms with PPO as a baseline in ProcGen environment (2d platformed style, procedurally generated maps). Moreover authors propose a simple auxiliary task "inverse model" for ppo and show that it s comparable with UCB DRAC. Finally it is mentioned that all methods need careful on per game basis hyperparameter tuning which requires further researchThe paper asks important questions about generalization theme and task wise and answers them but I am not sure (not saying that they are not, just cannot wrap my head around it) if generalization of the conclusions is justified. In abstract starting from "Contrary to..." it s not clear what you meant. The paper is not fully explicit in terms of experimental part but it s usually "like in the <link provided>" so I assume that the sources are explicit enough. Similar approach is taken to descriptions of algos and such   no elaboration and just reference (usually I feel that there is too much but here is too little perhaps). However I m not sure whether to count it as positive or negative. The whole paper is not groundbreaking but makes its point in a clear and understandable way and seems to be natural consequence of previous research.
Reject; rating score: 3; rating score: 3; rating score: 6; The paper considers the problem that personalization methods in federated learning may cause the personalized models to overfit on spurious features, thereby increasing the accuracy disparity compared to the global model.<|endoftext|>The authors have proposed a new FL training strategy to reduce the performance discrepancy between the central model and the client models. Authors have spent a decent amount of effort explaining the relationship between the spurious features and adversarial transferability. Weakness:  The presentation has space for improvement, please explicitly explain all the critical terms (accuracy disparity, bias conflicting, etc) at the first occurrence in this paper. The adversarial examples are generated using a global model, however, the way of generating adversarial examples in FL worth a lot of analysis and description of the details. There exist some papers that discuss the best way of improving adversarial robustness with adversarial training. The idea is good and novel, however, the presentation is disappointing and the experiments are weak.<|endoftext|>The approach is evaluated on artificial settings with spurious features. Strength  The work exposes a possible generalisation issues in personalised federated learning and proposes a novel approach to tackle it  The idea is well motivated, paper is generally well written and experiments are provided to substantiate the claimsWeakness  Use of some non standard hyperparams like 0.031 eps budget for MNIST and the batches of 96, 40, 30. Could the authors provide an explanation? I think this work tackles an interesting hypothesis that can limit generalization in case of personalised FL.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; I am in favour of accepting this paper. This paper does a great job at investigating an important problem in AI safety in a concrete and rigorous way. There is an informal assumption in AI safety that the reward hacking problem will tend to get worse as the agent becomes more capable.<|endoftext|>This paper targets the very important problem of reward hacking that occurs when the objectives optimized by intelligent agents are misaligned with respect to the tru objectives of the algorithm designer. None of the experiments in the paper are on sufficiently realistic settings. I request the authors to explain how this non trivial. The paper is well written and easy to follow, and has a thorough coverage of related works in AI alignment.<|endoftext|>I think the paper is a strong attempt at an important and hard problem, and will provide a good basis on which others can study reward hacking with increased formalization and rigor. This results in several interesting hacking behaviors. As researchers tend to cherry pick the most convincing results, it is good to see that the authors avoided this. The paper’s findings are clearly useful to the research community as the reward hacking problem has been a poorly understood but ubiquitous.<|endoftext|>This paper provides a systematic study of “reward hacking” in the environments with the misspecified rewards. The paper provides an interesting study of reward hacking behaviour where different parameters of the problem are varied to demonstrate the phenomenon. Unfortunately, the paper does not provide any extensive related work overview that seems to be an important missing part given that the main contribution of the paper is the systematic study of reward hacking. Despite being small, such problem formulation could be useful for sparking more research in this direction. The rewards might be assigned (and misspecified) based on the distances or positions of the objects.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; The authors propose three kinds of adversarial attacks based on feature level perturbations of the latent representation of generative models. [Strengths]The paper is well motivated to generate adversarial patches that are human interpretable and physically realizable. Does this attack mean that only a square region of the latent representation is perturbed and the other regions of the latent representation are not perturbed? This conflicts with the disguise requirement. Thus, I think the paper should be rejected.<|endoftext|>They propose feature fool attack, a method to synthesize adversarial examples that are can be placed onto any source image, uses interpretable features, and physically realizable. In doing so, they regularize adversarial features so that they appear interpretable but not similar to the attack s target class. They apply their method to copy/paste attacks. Strengths:  Their method can synthesize adversarial examples that are universal to any source image, uses interpretable features, and physically realizable, which is novel as far as I know.<|endoftext|>The authors propose an interpretable attack on DNNs that is also universal to source images and physically realizable as well. The goal is targeted adversarial attack. First, it allows a for humanly interpretable images. For example, the attach show in attack 1 can be circumvented using object detection, which would crop two separate part of the image and hence mitigate the attack. Given that the patch is visible easily, I assume this change will be recognizable to an anomaly detection method. The paper is well written and the ideas are novel.<|endoftext|>Strengths:  The result that adversarial patches can be created that look like a different class to the target (and are classified as such) is a novel, interesting and important result. The core idea of the paper is interesting and novel. I am concerned by the limited experiments, but this does not cause me to doubt the author s main claims. The paper includes plenty of visual examples to demonstrate that the attacks are interpretable in the way they claimWeaknesses:  The paper contains very few quantitative experiments.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; The paper suggests an approach to design arbitrary steerable kernels. The idea is to parametrize kernels using a steerable basis. Overall the paper is well written. However, I do have some concerns, detailed next. The authors identify that this could lead to non smoothness. is there a theoretical justification for that? Would it be possible to design a toy experiment and visualize learned filters with the current approach versus Lang & Weiler (2020)? I think that it is also important to test the method on an equivariant task. * Seems like a solid contribution to steerable network design. * The applicability of the method to 3D classification is not clear as it is compared to other alternatives.<|endoftext|>The paper generalizes the Wigner Eckart theorem (Lang & Weiler (2020)) from their own homogeneous space to arbitrary spaces. It extends upon previous paper (Lang & Weiler (2020)), and the benefits are clear: (1) it allows for parametrization with a more flexible and compact basis; (2) it is easier to implement steerable CNNs that are equivariant to a composition of group symmetries. As the paper aims for generality, I would not down score the contribution for not achieving the state of the art against every other method working on a specific type of symmetry. Nonetheless, very few recent works were compared in the paper, which makes it hard to have an understanding of the practical performance of the proposed method. In addition, many concepts are well explained in the related works such as the irreducible representation, etc.<|endoftext|>This paper introduces a general framework for constructing equivariant steerable CNNs over arbitrary subgroups of E(3). The central idea of their approach is to use the well known basis for steerable kernels over E(3) and then restrict them to a given subgroup. 4.“Band limited” is not defined (as far as I could tell) until example 2. This paper presents an elegant and general technique for constructing equivariant kernels for subgroups of E(3). The paper is generally well written and the authors intend on releasing code along with the paper to help researchers build their own kernels. Finally, while the theory is elegant and general, I don’t know that the authors did a good enough job of motivating why we need such exotic CNNs (beyond O(3), SO(3), and SO(2)). The core idea of the paper seems elegant. 3.Although I liked the examples discussed by the authors (SO(2) and C^4), I do think the paper would be better served if there was more detail in the constructions. The authors talk a lot about the Wigner Eckart theorem but they don’t describe what it is until section 2.2.<|endoftext|>I am sticking to my original rating, but the paper does have theoretically interesting ideas which may be useful to the field. The main idea is that by constructing a steerable basis for a large symmetry group such as O(3), using the ideas in the paper, we can readily construct equivariant networks which are equivariants to smaller subgroups, such as SO(2). The authors present a general algorithm for constructing steerable equivariant neural networks. The authors provide a generalization of the Wigner Eckart theorem for steerable kernels in Lang and Weiler (2020). They also provide a simpler computational procedure to build the kernels equivariant to action of $G$. I am a little unclear on the practical significance of such an idea. The authors should try to improve the motivation of the paper and if possible, present more possible application areas where they see these ideas being most useful. UPDATE AFTER AUTHOR RESPONSE:I thank the authors for a clear response to my questions as well as the explanations. Overall, I am not convinced about the motivation of this paper and its practical significance.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 8; The authors propose a new consistency loss for improving model robustness to common image corruptions. They use a student teacher training setup where only the student network uses batch normalization at training time. Improvements are shown on small scale corruption datasets (CIFAR C), a single domain generalization dataset (VLCS), and RobustPointSet. The paper is mostly well written with a clear story, and nicely motivated by a few toy examples. This should be made clear in §4.2.<|endoftext|>The authors study the effects of BatchNorm layers on model robustness. The third paragraph states:  > "Ideally, a classifier should not rely only on the dominant features topredict the class. Are the synthetic features shown really the low(est) variance features that disambiguate 2 from 3 in MNIST? I imagine there are many light / dark pixels that are also low variance & predictive. The central insight of the paper is that normalization of features leads to an over reliance on low variance features. Is there no other way to account for the different scales of learned features? *Should* the method account for them? Given how the student teacher regularized loss is defined in equation [7], it seems odd that the authors do not present an ablation study where they reduce the regularization effect tuning $\lambda * \mathcal{L}_{ct}$ as $\lambda \rightarrow 0$The authors present evidence that BatchNorm leads to a preference for parameters that minimize a data dependent norm, and demonstrate a simple student teacher training method to correct for these effects. There are a few alterations I d really like to see, though I still believe that, given the required corrections, there is enough value in this particular insight and the experiments to see it published.<|endoftext|>The proposed approach is also evaluated on a 3D point cloud dataset. It first identifies the reliance on low variance features in batch normalization, and proposes a counterbalancing teacher approach to distill from a BN model to a non BN model. Experiments verify the hypothesis of the paper and show that the proposed approach outperforms the various baselines on robustness to data corruption and domain generalization. 2.The authors should clarify why relying on low frequency features are undesirable.<|endoftext|>The paper argues that Batch Normalization and similar normalization layers can make neural networks more fragile to data distribution shifts and proposes a distillation based regularization method to ameliorate this issue while maintaining in domain accuracy. The paper proposes a training method where a teacher model is first trained without BN, then a student model with the same architecture but with BN layers is trained both on the task loss and on a regularization loss that pushes its last hidden representation towards the one of the frozen teacher model. Question for the authors: are the teacher and the student initialized with the same weights?
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 8; The paper proposes a new two step approach for learning disentangled representation. Results show that the proposed approach achieves state of the art performance on several datasets for disentangled representation learning. The experiment results are convincing. 2.The motivation of using contrastive learning to learn disentangled representation is unclear. 3.The presentation is not very clear and I find it hard to understand some parts of the algorithm. Minor comments that do not affect the rating:1. The presentation can be improved as well.<|endoftext|>It is based on the generative latent optimization scheme for learning disentangled latent codes for a given dataset, followed by training an encoder network to predict these latent codes given an arbitrary image. They validate the resulting model using standard image generation and disentanglement benchmarks across multiple datasets. The authors provide two types of learned continuous embeddings: content and residual, but from the text of the paper, I could not deduce the difference between these two variables, since they are treated equally throughout the optimization process and in the model itself. Therefore I would like the authors to clarify, what is the difference between these two variables. The comparison with [4] is presented in Table 4 has mixed results and, to the best of my knowledge, no discussion within the paper. Why does the proposed method outperform [4] on synthetic benchmarks, but not on all real world benchmarks? Also, I consider the contribution of adding contrastive losses into the training of discriminative models to be quite minor.<|endoftext|>Despite the simplicity, the authors demonstrated that this leads to a significant improvement in the disentanglement of representation. Overall, I was impressed that simple contrastive learning can improve disentanglement learning. However, the paper did not go much deeper into why and how it improves the performance, and lack some technical novelty. The presentation should be improved in many ways too. Despite the impressive results, the technical contribution of this work is still quite incremental. 5.In Table 1, comparisons to MoCO and SinCLR are unfair since they are self supervised while the proposed method is based on supervised learning (i.e., trained with class labels).<|endoftext|>Their method extends LORD, a latent optimization method for improving content embeddings, and OverLORD which performs disentanglement on embeddings, by incorporating contrastive learning. The results validate the efficacy of the proposed method on many datasets/metrics. Cons:  The algorithm has many moving components such as generators, encoders, momentum encoders, and heads interacting in ways that seem complicated. Complicated methods are more difficult to reproduce and to use for new problem domains or build on for future research. Getting 100% performance in Table 4 by just adding the residual level loss seems suspicious, can the authors explain that this result is not an anomaly? No robustness test showing how the weight coefficients affect the residual and content level loss terms. I recommend an accept since the method seems sound, well motivated and reasonable and the results seem promising in difficult datasets.
Reject; rating score: 6; rating score: 6; rating score: 6; rating score: 6; The authors propose a modification of DDPM for conditional generation, in which they explicitly model the conditional mean ("cluster center") in the forward and reverse diffusion processes. This modification is motivated by the empirical observation that for the large portion of the diffusion samples form several distinct cluster. The resulting model, ST DDPM, shows competitive results on conditional image generation and inpainting; and enjoys interpretability by means of cluster center visualisation. **Strengths*** The method is well motivated through empirical observations that are clearly set forth in the paper. * I expect the paper to be interesting to a wide audience due to the wide range of tasks it considers, and separately due to the proposed method being more (easily) interpretable than existing score based generative model. One would expect the conditional generative model would outperform the unconditional one by a large margin. The paper provides an interesting and well motivated modification of DDPMs for conditional generation, but needs stronger baselines to help assess the performance of the proposed modification.<|endoftext|>Score based generative models show great performance on image generation tasks. On the forward process, the score based model gradually corrupts the data with noise, and the reverse process restores the image by denoising. This paper measures the ratio of intra class and inter class, and they find that the class separation decreases sharply and the data distribution is totally covered by the noise. From a new finding, this paper proposes a class center on both forward and reverse processes for a controllable generation. This paper provides a new variant of a score based generative model, and it can be attractive for many machine learning researchers. 2.This paper suggests a new interesting finding, the changes of the ratio of intra class and inter class. 3.From a new finding, this paper proposes a new class center based algorithm for conditional image generation. The proposed model, ST DDPM, shows a significant performance improvement on the FID score and Inception Score. The quantitative results are reported for the CIFAR 10 dataset only. There are qualitative results for other datasets such as FFHQ and LSUN, but there are no quantitative results for that kind of dataset. I am curious about the results of negative log likelihood.<|endoftext|>Taking inspiration from this class clustering process, the paper proposes to explicitly model the class center in the forward and the reverse process, whereby during the forward process the samples are pulled towards the center, before ultimately becoming noise. The paper also shows good performance for other controllable image generation tasks such as image inpainting, attribute conditioned generation, and text to image generation by efficiently conditioning the diffusion model. Strength   1) The idea of doing controllable generation from diffusion models across different tasks by efficiently conditioning the model is interesting. 2) The paper shows strong conditional image generation results. Qualitative results for the image inpainting tasks also look good. 3) Extensive experiments across different tasks and datasets. 2) Adding on to the above point, the motivation around modeling class centers is not clear. Can the authors make it clearer as to how is the training done for it? 5) I couldn t find a comparison of the text to image generation against other works.<|endoftext|>The paper proposes a new forward conditional process. The transition kernel is controlled by a condition embedding. It also derives a reverse process. The paper conducts experiments on different conditional generation tasks. Strengths:The conditional framework proposed in this paper is general and is applicable to different tasks. The earlier starting (ES) is only slightly mentioned in Sec.6.5. Although the author claims that details can be found in Appendix B, I didn t find how it is derived and what is the principle. 4.At the beginning of Section 5, the author denotes $u_\phi(c)$ as the class center. However, $u_\phi(c)$ is actually learned, which is not rigorously the class center. 8.This paper is about conditional diffusion models, thereby it would be better if experimental comparisons with prior conditional diffusion models (e.g., [1*, 2* ,3*]) are added. The author should at least discuss these works in the related work section.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; First, the authors considered the linear NNs and showed that the duality gap of general deep NN is non zero, but for parallel NN with enough number of parallel branches, that would be zero. The paper tackles an interesting and I believe important problem in NN optimization. As a theoretical paper, I expected to see more precise statements. There are some minor typos in the equations, such as  Section 2, first equation, $A_l \Phi(W_lA_{l 1})$ should be $A_l \Phi(A_{l 1}W_l)$. First sentence section 4.2, "there is no duality gap for arbitrary deep ReLU networks", seems "parallel with large enough number of branches" should be added to make the statement accurate. works by Ergen and Pilanci. However, the writing and presentation of the paper falls short from the expectations.<|endoftext|>The topic of this paper is the duality theorem for deep neural networks with linear and ReLU activation functions. [Contributions]The analysis of the loss landscape of deep neural networks is an important topic to understand the global convergence property of optimization. Recently, the dual formulation of two layer neural networks has been studied and the strong duality has been shown by the series of [Ergen and Pilanci] works and [Zhang et al.(2019)].A natural question is to extend these results to deep models and this study answer the question. Stochastic Processes and their Applications, 2020. I could not find the proof of the first part of Proposition 4 and 6. Moreover, there are many similar typos with the wrong uppercase and lowercase letters in the paper. The statements of main theorems are interesting and important, but the quality of the paper should be much improved. See the main review for detail. Mean field theory of two layers neural networks:dimension free bounds and kernel limit.<|endoftext|>However, I am not an expert in this field of "theory of training neural networks", so I am looking forward to rebuttals from the authors, as well as comments from other reviewers and the area chair. Using duality theory, this paper studies the duality gap between prime and dual optimization problems for deep linear (or relu) networks that have sequential or parallel structure. 1.The claims are very interesting. It should be right multiplied. This happens many times. 1.4.Problem (7) has a typo. 2.There seems to be very few technical novelty. Most of the paper is about the standard step of calculating the dual problem, the rescaling technique is also quite standard and well known (e.g., in SVM), and was used in (Ergen & Pilanci, 2020b) for neural networks. Please clarify any technical novelty compared to prior works. 5.Assume Proposition 2 is correct.<|endoftext|>In this paper, the author studies the duality gap in the neural network training problem. They show that in general (deep linear networks and three layer ReLU networks), the duality gap is not zero. However, when restricting the parallel architecture, the authors show that the duality gap is indeed zero. Analyzing the duality gap in neural network is an important research question both theoretically and empirically. The author proves that the duality gap is zero when the neural network has parallel architecture, but I find there are little discussion behind this setting, i.e.why the parallel architecture can help closing the duality gap? Has it been used empirically? I found the manuscript to be clearly written and technically sound. Although it has some weakness points, I think it still worth a publication.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; The paper addresses few shot learning in a combination of episodic meta learning based and pre train finetune based approach. Just to mention a few combining pre training and episodic training: https://arxiv.org/pdf/2003.04390.pdfhttps://arxiv.org/abs/1903.03096. Results lack a clear comparison with the recent literature.<|endoftext|>This paper aims to improve the generalization power of meta learning. I understand from the abstract and introduction that the paper aims to combine two different solutions of meta learning and build a unified one that could potentially improve the task generalization ability of the meta model. More importantly, it is not new in terms of the "idea" of combining "the episodic meta learning based and pre train finetune based few shot learning".<|endoftext|>Pros:1.The idea to unify the episodic meta learning and pre train finetune based few shot learning is meaningful for the improvment of the few shot learning performance.<|endoftext|>Weakness: unclear contributionsThe paper claims to have three contributions: 1. utilize the idea of meta learning to integrate two very different streams of few shot learning, i.e., the episodic meta learning based and pre train ﬁnetune based few shot learning, and form a uniﬁed meta learning framework. As for the experiment results, i appreciate the authors do experiment on two tasks. However, after reading this paper, I felt the contribution claimed is not supported well and thus, the contribution of this paper is not very clearly stated.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; **Few sentences summary**: the paper proposes a new training loss for adversarial training in the Lp norm setting. * Very extensive experiments in the main paper and in the appendix. It gives a lot of intuitions about the problem and Lp norm robustness in general. It would require twice as more computations but would be interesting to check. The authors also propose great analytical tools to investigate their hypothesis.<|endoftext|>This paper proposes a helper based adversarial training (HAT) method to alleviate the trade off between robustness and accuracy. Weaknesses:  The modifications introduced in HAT are simple (which is good), but they depend on an assumption that ``the model should not be robust beyond the threat model``.<|endoftext|>* The paper achieves a significant boost as compared to existing methods on strong attacks like Auto Attack. Based on this hypothesis the authors propose a training method where the excessive invariance is minimized using the cross entropy loss between the prediction(made using the standard trained model) of larger epsilon adversarial image and the prediction of the adversarial image, in addition to the TRADES loss formulation. It shows a significant boost as compared to existing art and has some minor issues at present.<|endoftext|>The authors proposed a simple technique to improve clean accuracy. For example, it is incorrect to assume that all adversarial examples with perturbations $2 \epsilon$ should be labelled with its adversarial label. The model is trained to classify these helper adversarial examples as the adversarial label predicted by the model trained without adversarial training. Based on the new results for the medium perturbation $\epsilon   12$ and additional experiments with other attacks, I tend to overlook my doubts about the paper s approach.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; The paper proposes a new tabular deep learning architecture based on sharing attention matrices enable transformers and linear layers. I have serious concerns about the paper, as listed below:   The paper writing is very poor. It is unclear whether the results are statistically significant.<|endoftext|>This paper proposes a hybrid deep network, named SALT (Sharing Attention between Linear layer and Transformer). There is no motivation/justification for the Transformers and Linear layers with attention sharing.<|endoftext|>This paper proposed a hybrid deep network architecture for tabular data, dubbed SALT (Sharing Attention between Linear layer and Transformer). And sharing attention matrices are introduced to promote cooperation between these two blocks. I recommend rejecting it. The writing of the manuscript can be polished.<|endoftext|>This paper introduces a Transformer based architecture for tabular datasets. This architecture combines a Transformer with a gating MLP (gMLP) by sharing the attention matrices. **Weaknesses**  Section 3.5: “SALT improves this method to use the same embedding matrix between categorical variables and continuous variables.” I am not sure what is the intuition behind this idea. I recommend adding information about the dataset or metric used in the caption.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; To improve both, they propose to sparsify input feature maps by learning a classifier whether to include an input feature or not. Supervision for this classifier comes from Transformer decoder attention weights. The original DETR paper shows that the encoder can be scaled to 12 layers and show improved performance over 6 layers (+1AP), table 2. Would in this case more topk features be required to match performance, would be proposed method still be a useful method to improve computational efficiency? Perhaps there is a better word for it. The paper suggests a novel approach for improving computational efficiency of Transformer based object detectors. Overall I recommend accept.<|endoftext|>This paper proposes methods to further improve the efficiency of Deformable DETR. To this end, this paper proposes Sparse DETR that selectively updates only the queries expected to be referenced by the decoder. An auxiliary detection loss in the encoder also improves the performance. Experiments prove the effectiveness of the method. There are some suggestions to improve the literature: a. There is an argument that “the encoder queries referenced by the decoder account for only 45% of the total”. In DETR and Deformable DETR, they all use the word “feature maps”, because from the perspective of the entire Transformer, the query is the learnable object embeddings. I recommend the authors revise this part to make the presentation more clear. Overall, this is a good paper that has good motivations and reasonable solutions. The experiments are thorough.<|endoftext|>This paper proposes an efficient end to end object detection architecture based on Deformable DETR, called Sparse DETR. Sparse DETR can use only 10% 50% of the original encoder query while achieving comparable results. And only the tokens which have top k scores will be chosen as queries in the encoder attention computing. The scores are used to select the appropriate token as the decoder`s query. Strengths:The method is novel and technically sound. As we all know that the queries or tokens are somewhat redundant for DETR based object detection, this work brings us some new insight of how to efficiently condense queries without performance dropping. Compared to PnP and other dynamic ViTs, this work is more effective and efficient. Overall, this paper provides valuable insight into reducing queries for DETR based detectors and I prefer to accept it.<|endoftext|>This paper tries to solve the problem of expensive computation on the encoder of Deformable DETR. The hypothesis is the encoder inputs a large number of image feature queries, but only a small number of them are actually referred by the decoder. With some other components, e.g.“top k decoder queries” and “encoder auxiliary loss”, the Sparse DETR can achieve slightly better performances at fewer computations. The computational and memory complexity of Deformable DETR are indeed some problems. I am quite confused about the two additional components in Sec.3.4.I don’t get their motivations and they seem to be irrelevant to the main motivation of this paper. There are quite a few confusing/unclear points in the paper, which are difficult to understand. A very naïve baseline? The auxiliary loss was first used in [A], instead of GoogleNet. For DAM, if you already use the decoder for saliency, how can you not compute the encoder? There are no object queries as input to the encoder, so no detections for encoder.
Accept (Poster); rating score: 8; rating score: 6; rating score: 3; This paper explores the training of GNN with compressed activation maps. It provides an optimized GPU implementation and comprehensively studies the trade off among the memory saving, time overhead, and accuracy drop. Experimental results show that the proposed framework can reduce the memory footprint of activations by up to 32x with only 0.2 0.5% accuracy drop and 10 25% time overhead. The first work that explores the compressed activation in GNN training. 2.Provide comprehensive empirical evaluation and theoretical analysis of the trade off among memory consumption, accuracy loss, and time overhead3. Impressive resultsWeaknesses:There is no major weakness of this paper, but adding more comparisons against other techniques would be great. 1.Adding experiments to compare EXACT with other memory saving techniques (e.g., gradient checkpointing, swapping)2. Adding experiments to compare EXACT with sampling approaches given a fixed memory budget. Maybe you have to choose slightly different settings of different approaches due to their different memory requirements. I strongly recommend accepting this paper.<|endoftext|>This paper proposes EXACT, a framework for training GNNs with compressed activations with two methods: quantization and random projection. The primary design objective of EXACT is to trim down the memory consumption in GNN training while maintaining acceptable training speed and accuracy. EXACT achieves significant memory savings with 0.2 0.5% accuracy drop and 10 25% slowdown of training throughput across five models and five graph datasets. Limited memory capacity of GPUs impose severe constraints on the model/dataset size of GNNs that can be trained. That said, this paper tackles an important problem. On the positive side, the proposed technique is simple and based on proven ideas for tensor compression such as quantization and random projection. Evaluation demonstrates promising results, which backed by solid theoretical analysis. The paper is well written. However, I still have several questions and concerns as follows:* *Overall Memory Savings*   It s a bit misleading to report memory savings just for activations (Table 3). Also, in Section 5.3, the authors claim EXACT reduces the memory usage in training GraphSAGE on obgn products to fit in 11GB. However, a sizable portion of memory savings come from AMP, which is orthogonal to EXACT. The authors should separately report memory savings of EXACT from those of AMP (and other techniques, if any). * *Scalability*   How effective would EXACT be if node wise or layer wise sampling methods were used in a mini batch setting? Would input data may take a dominant portion of memory usage once the activation map shrinks after aggregation? What if we switch the order? Is there any convincing reason why RP followed by quantization is preferred over quantization followed by RP? Can you provide some insight into why GNNs are so robust against this somewhat extreme compression of activations? Nits:* (Section 5) "it applies the random projection *following by* a 2 bit quantization"  > followed by * (Section 5.2) "*From Table 3*, the overhead of EXACT (INT2) is roughly 12% ∼ 25%." I d ask the authors to address them in their rebuttal.<|endoftext|>This work attempts to train GNN models with reduced memory requirements. The results are good, especially when considering quantization only. The authors did a good job choosing datasets   they go up to OGB products, which is nice. I d be delighted if they added GAT as well, though : )   that s really part of the reason we want their method! **Weaknesses:**  I am not entirely sure what the benefit of this approach is compared to sampling or historical embedding approaches is. It s not really clear what the benefit of random projections are, given that they cause significant accuracy degradation. I am aware that they do improve the compression ratio, but I would argue that in most cases there s little incentive to use it. If we consider just quantization, then there is little novelty to this work I would argue: I am not sure what is novel over the Chakrabati paper at NeurIPS 2019 is. I am aware that they focused on CNNs, but there is little reason to believe that their method would not work for GNNs. or studied it too carefully. It s useful that this paper has done that though. I am unsurprised it is effective down to 1 bit integer by the way   the gradients are much "smoother" for GNNs than for CNNs, as the effective batch size is very large. There is an interesting work that was accepted to NeurIPS this year that you should compare to: "AC GC: Lossy Activation Compression with Guaranteed Convergence"; they look at CNNs and Transformers, but there s no reason why you can t apply their technique to graphs as far as I can tell. I don t think that they re unanswerable, and I consider myself to be pretty reasonable at rebuttal if you can present new evidence / answer some questions. I would make the case that novelty is not the absolute most important thing   but I am a little skeptical of the contributions to the literature presented by this work. However, the work is likely to be of good use to practitioners   it s not difficult to implement many parts of this, and the results do seem impressive. I strongly recommend that the AC rejects the paper since I do not believe the research contribution is sufficient.
Reject; rating score: 1; rating score: 3; rating score: 5; rating score: 5; Unfortunately, it seems to me that this paper has a fundamental mistake and cannot be accepted.<|endoftext|>The paper proposes changes to improve the performance of neural network training with multi party computation, that is an activation function based on cosine and a linear functionality that reduces the number of weights to be trained.<|endoftext|>Experiments are provided to demonstrate the improvement on the test accuracy of the proposed approach over other baselines that use different activation functions. Strengths  The paper demonstrates the utility of cosine activation function and Hadamard transform in the privacy preserving machine learning paradigm.<|endoftext|>Pros:1.Compare to ReLU in MPC with multiple rounds of communication, proposed cosine function in 2PC setup only involves two rounds of communication. Since communication overhead is a big deal in MPC among parties, cosine approach decreases the cost.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 5; This paper proposes a novel differentiable k means clustering layer (DKM) for deep neural network model compression. It is better to help to understand by adding a figure with more details. 4.Some details should be noticed, please check and correct them: a)	The format of reference should be unified; b)	Some format errors and spelling mistakes exist in the paper; c)	The Figure 1 and Figure 2 are not clear, I recommend to use vector graphics; 5. The comments are listed as following: 1.<|endoftext|>The paper proposes a new building block that performs a soft k means algorithm where each weight is assigned a convex combination of the cluster centers. My main concern is that the paper does not present many insights into the method itself besides good results on benchmark datasets. I would require some more intuition, proofs, or experimental evidence to be convinced that this can be expected. How confident (i.e., how close to one) are the weight assignments (i.e., attention) to different cluster centers during training? How does the accuracy change when going from the soft clustered weights (as used during training) to the hard clustered weights (as used during testing). How important is the initialization of the weights using a pre trained model? How well does the proposed method work when trained from scratch? Is there a discrepancy in the number of iterations of k means for different layers? Mostly well written paper with good experimental results on benchmark data, but hardly any insights into the method itself.<|endoftext|>The authors provide a differentiable k means clustering layer (DKM) to optimize the cluster centers and cluster assignments throughout the training without separating the problem of training and compression. The paper is well written and easy to follow. I like Figure 4 b since it shows very clearly that the weight distribution is trained to be centered around clusters as desired. Here are some of questions and concerns:  The authors claim that weight cluster assignments have not been optimized in prior work. I believe making this distinction more clear would be fairer to the prior work. If I understand it correctly, the proposed method DKM is applied to each layer separately with different weight cluster centers and attention matrices. The authors make a connection between entropy and model quality and they say "... its entropy is $b$.". I am not sure what "it" refers to here. "Universal deep neural network compression." The authors propose a simple but novel clustering method for DNN compression.<|endoftext|>The paper claims that a competitive branch of model compression method: Weight clustering, can be greatly improved by allowing a soft(or say differentiable) usage of clustering mechanism to achieve a better approximation result. If the hypothesis in this paper were right, by using this fixed attention should also give good results. I think in general the method proposed in this paper is simple, which is ok to me. I am not peculiarly familiar with quantization based method, but I believe Hawq v3 (v2 is cited in this paper) is a competitive baseline method to discuss. I d like to see authors add the experiments of this method and have some meaningful discussions. I will categorize this paper as quantization method so I think this comparison is important to gain more insights. I think simple is good, but empirical results don t quite convince me.
Reject; rating score: 1; rating score: 3; rating score: 3; rating score: 5; The idea is to split a learning task T that can be solved optimally by a learning algorithm L_b into a set of N learning tasks T_i. The tasks T_i are chosen in such a way that a  learning algorithm L_a (which can not solve T optimally) can solve each T_i optimally in the sense that the predicted model h_i minimize the out of sample error on T_i given dataset D_i. Note:Since i am unfamiliar with the notation and some of the terms used, i see a possibility for error in my review and lower my confidence as a result of this. I believe the statement is either wrong or trivial. Even if the proposed theorem is correct, L_m is only expanding the set of tasks L_a can solve optimally, but the NFL then implies that there must be a set of tasks it can t solve (and even worse than L_a due to the equal averages of NFL)Moreover, I think that the theorem itself is wrong. Unfortunately the analysis of the paper does not take into account the accumulated composition errors of each task T_i.<|endoftext|>The paper claims that there is an optimal sequence of task training, but it doesn t provide an algorithm for finding it. There are no real world or nontrivial synthetic data experiments. The paper is not well written and the mathematical notation in the expression of the theorem Is very poor and there are definitions that are missing. I also wanted to criticize the approach to the problem which seems very abstract, very theoretical, and most likely does not seem to be useful. Catastrophic forgetting is a phenomenon that is caused by the sequential learning of tasks. It would have been nice to show how this is addressed by TAFL. This is not the case. An opinion from a theory expert should be more appropriate. It is my impression that the reviewers that would be capable of giving a clear opinion are probably not at ICLR.<|endoftext|>Also, it is declared that if moreproblems are solved the difficulty of the new task will be decreased. They alsomentioned an example to use the theorem. The proof of the TAFL theorem isinspired by the universal approximation theorem. There are doubts in accepting the formulation because the proof is not clear, andthe example stated in the paper is trivial; no experiments are done toprove the formulation. Also, it would be nice if there was a piece of information about mathematicalnotations like the off training set error stated in the paper, and also the universalapproximation   theorem   which   the   proof   of   the   formulation   is   inspired   by   thistheorem but there is no discussion about that. In equation 3, it is not clear how we can reach the point that if tasks are in specificorder algorithm a can solve the b s distribution as optimal as algorithm b. I wouldexplain it more to make it more convincing.<|endoftext|>The paper addresses an important topic in the theory of learning algorithms which regards the limits of learning algorithms. I list the main questions below in the hope that each question provides an opportunity for the authors to improve the readability and notation of the paper:  Is the theorem concerned only with learning algorithms or algorithms with memory or is it also applicable to general optimization methods? If so, use the adequate symbol for the real set. If so, please, explain how and why this is necessary. How can $d_a$ be sampled from $f_b$? This connection is not clear to me even after reading the proof provided. In Eq 3, how $y_0$ can be equal to $x$? What are the assumptions about $d$? What does $n$, $m$ and $k$ mean in Eq.3?In Eq.5, what assumptions allow the composition of $f_s$ with $f_i$ and $f_j$? In other words, why can $(f_i, f_j)$ replace $x$? In Section 2 $h$ is used to represent the learned model. Is there a reason for the change? "series of tasks" or "sequence of tasks" would fit better in my opinion.
Reject; rating score: 1; rating score: 3; rating score: 3; rating score: 6; T SNE plots show that it is successful in disentangling the gesture classes. For this purpose, a VAE latent space representation is employed, which is shown to be effective in clustering and pseudo labeling tasks. I explain these weaknesses above. This paper looks like an unfinished study.<|endoftext|>However, and this is the reason for my recommendation, the evaluation of the proposal somehow only relies on the capacity of the system for modelling the input. By the way I looked for the ablation experiments in the Appendix and was not able to find them. I understand these technologies have been around for a while and the idea may be that all of them sum up for the work’s contribution.<|endoftext|>Also, event streams have very high temporal resolution and HDR. In fact, I would remove this part from the paper since it makes the objectives of the work a bit more diffuseMy recommendation is that the paper should not be published in its present form.<|endoftext|>### WeaknessesThe full extent of the benefits offered by the different components of the proposed approach was unclear from my reading of the paper. However, I have some questions regarding the benefits of the individual components of the neural network and concerns regarding the proportion of wasteful computation done by the network, as detailed under "weaknesses". 3.While it is appreciable that the proposed network reads event data at the resolution of 1 ms, how useful is it to read the data at such a high resolution?
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; The paper proposes an unsupervised exploration method for reinforcement learning, called UPSIDE, that combines learning of directed skills that enable covering distant states, and a diffusive part that explores locally and helps expand the explored region further. The main contributions are both the topology of the policy (division to tree structured skills and diffusive parts), a theory for training such policies, and a practical implementation that simplifies some of the technicalities induced by the theory. The experiments also compare to prior methods such as DIYAN and provide ablations of the importance of the different components. The submission has all components that make up a good paper: it reads well, describes and motivates the problem, positions itself with respect to the prior literature, covers the theory and implementation with necessary details, and compares the prior methods. I don’t have any critical concerns, and only mostly minor comments and suggestions. I cannot fully parse the objective in Equation 2. Especially the sentence “In words, the skill is incentivized to bring the diffusing part to a discriminable region of the state space.” seems to be incorrect. The reward does incentivize the skills to stay in a discriminable region, but only the discriminability of the end of the episode should matter (not how we got there). In fact, I would expect this objective to lead to skills that try to avoid unvisited states (as those regions are indiscriminable) and the skills should be biased in already visited direction rather than being symmetrically distributed, as is the case for example in Figure 4(a). The text clarifies that in the Half Cheetah and Walker2d environments, “DIAYN policies learn to fall on the agent’s back”. The proposed algorithm is novel, the paper provides both theoretical analysis as well as a practical version of the algorithm, and the experiments indicate substantial improvement over prior works.<|endoftext|>To increase the state space coverage with unsupervised skill discovery, the authors propose UPSIDE. Also, they discover skills by forming a tree, which enables chaining of smaller skills to form far reaching skills. Empirical results on Maze and MuJoCo environments and further analyses are presented. This work suggests a workaround to explore local regions with a decoupled policy structure (direct and diffuse), which I find a meaningful contribution. * In comparison with the baseline skill discovery methods, this method shows improved state space coverage and performance empirically (see my comment about the issue with MuJoCo experiments below, though). They show that the proposed method is effective for learning a set of skills covering the state space on the toy (Maze) environments (Table 2 and Fig.2). They also present how well their method can reach different goals on Bottleneck Maze before and after fine tuning (Fig.5). **Weaknesses*** The diffusing with the simple random walk policy (performing uniform random actions) is one of the most basic strategy for exploration and wouldn t be very effective on many environments, as it does not necessarily lead to good (local) state space coverage. This is also mentioned by the authors in Sec.6. The authors propose an interesting approach to unsupervised skill discovery tackling multiple issues with some existing prior methods. They also provide meaningful empirical results and analyses.<|endoftext|>The paper proposes a novel algorithm for learning unsupervised skills based on empowerment. Specifically, direct and diffuse policies are proposed that first optimize empowerment and then after a fixed number of episode steps optimize action entropy. This yields policies that go to a certain point in a directed manner and then move around that point at random. Further, the UPSIDE algorithm is based on tracking the policies with high discriminability (i.e.low H(z|x)). Cons:  The proposed method is overly complex and contains many ad hoc decisions. It is unclear how to adapt the method to a larger or a continuous space of skills, which is important for more realistic applications. The theoretical justification for the proposed method is weak. For instance, Lemma 1 shows that the proposed objective is a lower bound on empowerment, but the paper entirely ignores the question of whether the bound is tight (in contrast to all previous work in the area which uses tight bounds). As far as I can tell, the paper does not follow any existing evaluation protocol. It seems that a very similar algorithm can be obtained by jointly optimizing I(x;z) and N for maximum empowerment. Pros:  The paper focuses on the relevant problem of improving coverage in unsupervised skill discovery  The proposed method significantly outperforms prior work and provides important insights into what properties are important for a successful skill discovery algorithm, such as using discriminability of the final state in the trajectory and stacking skills sequentially for better exploration. Further, the paper empirically presents good performance on the challenging ant domain and contains several important insights that might be useful for the future researchers. Therefore I am leaning towards accept.<|endoftext|>This work proposes a new framework for unsupervised skill discovery termed UPSIDE which aims to learn a fix set of state space covering skills that have a directed and diffusing (noisy) component, are constrained to be sufficiently discriminable and are chained together in a tree structure to enable composition. **Strengths**: This work takes a novel approach to the skill discovery problem. Instead of directly finding maximal entropy policies that act as skills, this work splits skills into directed and diffusing (exploratory) components. The paper also includes a technique for limiting the number of skills by ensuring they satisfy some minimal discriminability threshold. Finally, the method also includes a method for generating compositions of directed skills via a tree structure and then diffusing from the leaf to explore. Overall the method is novel and is appears to be supported by experimental results. 1.For the continuous control environments, why are there no finetuning results on the downstream tasks besides Ant? Please report downstream results on the other two tasks as well. 2.Why is DADS not included as a baseline? Instead as mentioned above, please comparison against the original DIAYN codebase. Figure 10 or something like it should go in the main paper (it is very helpful in understanding the main idea of the work)Overall, I recommend rejection but I am open to updating my score if my concerns are addressed. Overall, the method proposed in this work is novel but the experiments, while showing performance improvements for the proposed method, do have several flaws which I point out in my review. If these issues can be addressed, I would be open to updating my score from rejection. UPDATE After discussion with the authors, I have raised my score as my experimental concerns have been addressed.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; rating score: 6; Therefore, I recommend to accept the paper.<|endoftext|>I agree to accept this paper.<|endoftext|>The area of NAS and attacks on NAS is an interesting area which is currently receiving quite a bit of attention.<|endoftext|>1.Interesting approach and impressive results. 2.Need more clarifications for a few points. 2.Due to the lack of knowledge on hardware attacks, I did not fully understand what assumptions are made to achieve this kind of low error rate.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; I believe it would have been better to talk about your work in this paper. The flow fluctuates. Also, you just talk about the existence of a survey paper without mentioning what to take away from it. After that, you only discuss other models that are patient specific.<|endoftext|>*Misleading statement*   Abstract: "While there have been many neural network models proposed for survival analysis, none of them are calibrated." However, I found several issues with this submission. *Minor Issues*  There are several typos, I encourage the authors to proofread carefully**Missing references**  [1] Chapfuwa et al., "Calibration and Uncertainty in Neural Time to Event Modeling", IEEE Transactions on Neural Networks and Learning Systems, 2020.<|endoftext|>Third, cross validation or some other approaches should be used to provide some measure of statistical significance for the work. Defining calibration of the learned survival function with respect to the actual Kaplan Meier (or similar) curve is reasonable. The reproducibility and value of resources provided by the authors is clearly limited. Typos, etc.<|endoftext|>This could mean that individual level predictions are no longer as good. A LIL type result for the product limit estimator. The Annals of Statistics 1997.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; The paper proposes to rethink deep learning for learning functions (with functions being gaussian mixtures GMs, with  positive and/or negative weights unlike for PDFs) as opposed to tensors traditionally used for embedding data. Applying  an activation/transfer function (e.g.ReLu) to a GM does not produce a GM and the paper proposes to approximate a GM from the output of the activation function. The paper proposes approaches for  GM fitting so that the output of the activation function (ReLu) becomes a GM. As the convolution of GMs produces a GMs with more components (Gaussians), reduction techniques are used to limit the number of components in the GMs as these pass through the network. The resulting proposed approach shows good performance for classification. Training time are said to not be competitive against state of the art. (i.e.why is activation not applied in the parameter space of the Gaussian ?) The paper is interesting and novel as it rethink neural networks  for learning functions.<|endoftext|>CNNs have made an undeniably successful/impact on computer vision tasks. This paper tries to disrupt this well established idea used so far, by proposing a Gaussian of mixtures (GMs) for tackling the curse of dimensionality, that is not avoidable with the conventional CNN architectures. The resulting architecture is termed as a deep functional network. WeaknessesAlthough I find the proposed approach interesting I would expect some more experimental evaluation to other related approaches and other datasets. Nevertheless, this does not substantially hamper the paper. See my concern in the detailed review. Next, I leave several questions for the discussion: it is known that the EM suffers from the problem of the initialization, that is, how many components (per node)  should we represent the data. Also, it is mentioned that reduction of  the number to Gaussians must be performed. Since the proposal is based on the EM algorithm, a clear mapping with this should be done.<|endoftext|>The paper proposes a new convolution method to process sparse data in a memory efficient manner. Since the number of mixture components would increase quickly from layer to layer and application of ReLU destroys the property of intermediate layers to be mixtures of Gaussians, they re fit intermediate feature maps and develop a heuristic to reduce the number of mixture components. The authors evaluate their proposed method on MNIST (2d) and ModelNet10 (3D) and show competitive performance compared to classic PointNet/++ methods. *[Update on this point after discussions] After some clarifications on the method, I am somewhat unsure about the role of potential negative weights in the convolution kernels. It appears that the method effectively does not use them (or, if so, in a very indirect way). *### What limits applicability to more complex datasets? I am a bit torn about the paper in its current form. This makes it a bit difficult to properly assess the benefits and limitations of the method compared to other approaches, in particular more recent ones than PointNet/++. ### Update after rebuttal/discussionsI am still a bit torn, since the intuitions behind e.g.the dense fitting step are still not clear and the role of negative weights.<|endoftext|>For the curse of dimensionality problem of CNNs, this paper presents the Gaussian mixture convolution network (GMCN), a method that alleviate the problem of high dimension data. It fits a Gaussian mixture to result of transfer functions, such as RELUs. The effectiveness of this architecture is verified on MNIST and ModelNet10. Compared with other methods, such as PointNet and PointNet++, the effectiveness and practicability of this method cannot be sufficiently demonstrated. It is recommended to validate GMCN on more complex datasets and to give a more adequate analysis. 2.TreeHEM is used to reduce the number of Gaussians in the mixture, and  $T 2$ works best in terms of computation time, memory consumption, and often fitting error. 3.The evaluation is quite insufficient as this paper claims that it is a general purpose deep learning The results are not very convincing on complex tasks.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; So I think this is a good paper. They designed two policy gradient algorithms based on the mirror descent method, which are named BGPO and VR BGPO. The paper discusses the momentum and STORM version of the mirror descent PG method, which is a relatively new result. The algorithms of this paper also apply the STORM technique.<|endoftext|>This work proposes a Bregman gradient policy optimization framework for RL. In general, the paper is well written with some minor grammar issues. The developed BGPO and VR BGPO are simple and easy to implement. The derived convergence results match the state of the art one. Why does the work only focus on policy gradient rather than general gradient?<|endoftext|>First of all, mirro policy gradient with Bregman divergence regularizer was already proposed in other papers. after the author s response   I thank the authors for their detailed response on the differences of the submission from closely related work. Given these comments and discussions, I am willing to raise my score to accept this paper. This paper has limited novelty in both algorithm design and theoretical analysis.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; The authors present a bold and thought provoking conjecture: neural networks trained with SGD converge to the same low loss basin, up to permutations of their hidden neurons. They go on to provide some limited but intriguing evidence in support of this conjecture. Second, they empirically show that the loss barriers between different SGD solutions are similar in magnitude to the loss barriers between different permutations of a single SGD solution, across a range of models, tasks, and hyperparameters (e.g.width, depth). The paper also includes a number of experiments investigating the effects of width, depth, task, and architecture on loss barrier size. I think the authors take a meaningful first step in showing that a simple simulated annealing algorithm is largely unsuccessful in finding winning permutations. As a second idea, since the function implemented by a neural network is invariant to permutation of its hidden neurons, if the authors  conjecture is correct then for two SGD solutions NN1 and NN2 there exists a solution NN2  (a permutation of NN2) which implements the same function as NN2 but lies in the same basin as NN1. But it is only a small step towards demonstrating that their conjecture holds. I was left wondering whether there are other measurable properties of pairs of neural networks (beyond loss barriers) which could provide further evidence for (or against) their claim. "Deep ensembles: A loss landscape perspective."<|endoftext|>This paper studies the optimization landscape of neural networks. In particular, this paper touches on the question of whether two local minima of the optimization landscape are connected by a “line” of linearly interpolated neural networks. To probe this question, this paper evaluates the loss gap (so called “barrier”) between two local minima and their linear interpolation. The finding is that this barrier is non zero. Next, the authors conjecture that if one takes into account permutation invariance of a local minimum, the barrier should be reduced to zero. To support this conjecture, the authors considered a simulated annealing algorithm to search for permutations of the weight matrices, to show that the barrier reduces after applying the simulated annealing algorithm## Strength  Understanding properties about the optimization landscape of neural networks is a central problem in deep learning.<|endoftext|>The authors empirically study the linear mode connectivity property for four model types, their scalings, and four datasets. Their main contribution is a conjecture:  all solutions can be linearly connected by permuting the hidden neurons of one of the solutions. The barrier predictions of their model using SA are convincing. How do the authors compare their conjecture to the Theorem 4.2 of Simsek et. It looks like this theorem has already proven the conjecture for two layers networks for a special activation function and in an infinite data regime. The authors study the effect of width, depth, skip connections on the barriers (via lines, i.e., LMC) after having rotating one of the solutions with a suitable permutation.<|endoftext|>I think the paper is a fairly thorough empirical evaluation and makes some valuable findings. They conduct fairly extensive experiments that are unable to refute this conjecture, but do not prove it with certainty. Concerns/Comments:   In Sec 2.2, and for most of the main paper, the authors restrict their attention to training loss barrier as the "barrier to linear interpolation". I understand that this is something that can be computed at train time, but isn t the claim of linear interpolation to do with the "true" landscape of the loss function? Does it imply that SGD somehow has different dynamics on feedforward networks when compared to ResNets? And if so, how does linear mode connectivity now interact with generalization? If the barriers were all non existent up to permutations, then shouldn t the experiments on $S $ show a lower barrier than the other experiments?
Accept (Oral); rating score: 8; rating score: 8; rating score: 8; rating score: 8; rating score: 8; To me it seems that the optimal variance could also be used for training, using for example only $M 1$. Weaknesses:  For $K<100$, FID scores of the proposed method greatly rely on a trick of clipping the variance of the step $n 2$ (can be seen in appendix G.4). The paper provides an important insight in DPMs and shows improved results for pretrained models by a simple post processing technique. This could be done for a few different instantiations of $n$ (I guess there will be more clipping for $n$ being small).<|endoftext|>The paper studies an estimate of the reverse process of a diffusion probabilistic model (DPM). To reduce the bias, bounds of the optimal reverse variance are analyzed and the estimate is clipped based on the bound. Finally, the authors present the relationship between the score function and the data covariance matrix and assess the proposed approach in the experiments. So, I am concerned that the contribution is not as much as what was introduced in the paper. The paper contributes interesting analytic results to the DPM models and the methods perform well in practice.<|endoftext|>The paper proposed a modification of the diffusion probabilistic models (DPMs) called analytic DPM that is based on an analytic estimate of the optimal reverse variance. Using this analytic estimate, the proposed method can achieve fast and performant inference through the Monte Carlo method and pre trained score based model. Overall, I think the paper solves an interesting problem in DPM. The experimental results suggest that the proposed method can potentially provide better performance more efficiently compared to alternatives.<|endoftext|>The paper proposes a theoretically grounded method for estimating *optimal* reverse process variances for DDPMs and DDIMs. The proposed method and theoretical insights also perform strongly in practice across a range of models and datasets. **Strengths*** Strong theoretical motivation and strong empirical results to support it. **Weaknesses**I am convinced by the paper in its current form. The improved understanding of these model classes could have been sufficient to recommend acceptance.<|endoftext|>The paper studies diffusion probabilistic models, and derives the optimal mean and variance (as functions of the expected data score) for the reverse process. In addition, authors combine their approach with recent work that optimizes for "knot" locations given a fixed number of knots for faster sampling. Given the recent progress and interest on DPMs, I think this work will also have reasonable significance and influence. **presentation**Authors do a reasonable job on the writing and presenting experimental results. Authors study choosing the optimal variance for the reverse process in DPMs and propose to Monte Carlo estimate it for improved inference.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; The paper proposes a new group of methods for supervised OOD detection. In particular, the authors propose to use the Fisher Rao distance between output distributions on the in distribution data and test samples to detect OOD. The authors additionally propose to use Fisher Rao distance in the hidden layer feature space, when possible (white box setting). On the other hand, I want to highlight that the authors perform a fairly exhaustive experimental evaluation in terms of the out of distribution datasets considered for each in distribution dataset, including both near  and far OOD. The empirical results are good, and the method is generally novel.<|endoftext|>The main idea is interesting, the paper is well structured in terms of methodological description and the problem/experimental setups, but the empirical results do not appear to be sufficient to validate the intended purpose of this study. It further combines confidence scores from the logit outputs and the layer wise features of a deep neural network. The proposed model is based on the assumption that the layer output follows a Gaussian distribution. (5) and (6), the authors note that taking the sum (5) instead of the minimum distance to the class conditional centroid produces better results. This paper presents a novel idea of using Fisher Rao distance for confidence scoring in OOD detection.<|endoftext|>The paper proposes to use a score based on the Fisher Rao information metric (the Riemannian metric in the space of probability distributions) for the detection of out of distribution samples input to a trained DNN. The approach models each sample as providing posterior probabilities – (a) the SoftMax probability in the label space, and (b) class conditional PDFs over the corresponding feature spaces for each DNN layer. There are some concerns regarding the validation of assumptions and the experimental evaluation. I recommend that evaluation and analysis of this assumption is added to the paper. This is very good.<|endoftext|>The paper proposes an out of distribution detection (OOD) metric based on Fisher Rao distance which can be applied for pre trained classifiers. First, the authors derive a Fisher Rao distance applied to the distribution of softmax. Furthermore, they formulate the Fisher Rao distance based framework, IGEOOD on Black(Grey) Box, where we can only get access on the logit of the network output, and White Box, where we can get access to intermediate feature layers. In the grey box setting, ODIN outperforms IGEOOD. Furthermore, in the white box setting, the method is only compared against the Mahalanobis distance. My biggest concern is that the experiment results are fairly weak given the complex procedure of obtaining the detection scores.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; This paper focused on a classical federated learning setting where data was non iid partitioned in different local servers, and came up with a method named SDA FL, which combined GAN generated data, differential privacy to both address the non iid problem and keep local data privacy. The privacy is not well maintained;1. Consider the extreme case, where each local client only keeps a single class of data. The differential private GAN is not an original idea. This idea has been broadly explored in the GAN community recently, thus proper citations are needed. This can be a minor issue, but the quality of the GAN generated images provided in the paper is kind of low, not sure whether it s because of the noisy gradients;8.<|endoftext|>This paper proposes a new framework for federated learning to resolve the non IID issue by sharing differentially private synthetic data. Each client pretrains a local GAN to generate synthetic data and upload the data to the parameter server. Strength * The proposed idea is clear and well presented: The idea to use partially private synthetic data to resolve the non IID issue in federated learning is interesting and clearly presented. The trade offs between privacy level and the quality of synthetic data are also presented. * More empirical support would be necessary: the proposed scheme highly depends on the quality of pseudo labels as well as the synthetic data. However, the overhead of training local GANs in each client can be huge, which makes this approach not very practical. The overhead should be clearly addressed to compare this work with other baselines.<|endoftext|>The paper proposes a new federated learning algorithm called SDA FL, which utilizes GANs to generate synthetic data for federated training on non IID data. Specifically, each client pretrains a GAN to generate synthetic data and send them to the server. Experiments show that SDA FL significantly outperforms the other federated learning approaches on non IID data. Non IID data is a key challenge in federated learning. 2.From the experiments, the improvement of the proposed approach is significant. Some important details about the algorithm are missing. 2.The GAN seems to be an important part of the algorithm. The dataset has already been updated by the server. However, there is no convincing explanation to support it. For the results on CIFAR 10, the differentially private version does not show superior performance compared with FedAvg. The authors should add them to understand the proposed algorithm more clearly. 8.What is the non IID data setting in the experiments of different privacy budget?<|endoftext|>A line of previous works address the non IID problem by data sharing which violates the privacy requirement. The author should make sure that same number of gradient steps in taken in each round. Moreover, this paper study the sensitivity to privacy budget and effect of synthetic data. It is a straightforward idea to share synthetic data instead of real data to avoid privacy leak. DP GAN also has theoretically guarantee of privacy preserving. In SDA FL, each participant train a GAN before training the classifier, introducing the following requirements:      Each client must have sufficient data to train GAN. For me it seems like SDA FL can only be applied to cross silo settings. This paper addresses important non IID and privacy problems in federated learning, and proposes a frameworks with some components are evaluated. However, components of interplaying labeling and ServerUpdate are not well evaluated in this paper.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; This motivates a stronger type of attack that can fool adversarially learned models. The logic and content are well organized. Although this paper proposes other techniques for improvements such as the consistent label bias, overall, the novelty is quite weak. Although the poison data looks visually similar to the original data, it is still possible to detect or remove such attacks. More discussion on this will be helpful. If I understand correctly, in most of the experiments in the paper, the poison ratio is 100%. For instance, when 20% / 40% / 60% of the data samples are poisoned, the robust accuracy only decreases by 0% / 3% / 7%. The paper proposes a data poison attack to fool the adversarial training algorithms.<|endoftext|>This work tackles the problem of data poisoning adversarial training in the (image classification) setting where the adversary can add $\ell_p$ constrained perturbations to each training example. However, the authors do not properly motivate the technique, the baselines are inadequate, and the paper can be sometimes unclear. The authors try a new attack that focusses on adding "robust" features to examples: in the most basic attack, for each (example, label) combination the authors perturb the example to maximize the probability of a different (label consistent   i.e.each image of a given label is perturbed towards the same class, one such mapping could be target_label   label + 1 % num_classes ) label. Others have previously studied this problem in the standard ERM setting by adding $\ell_p$ perturbations to examples that minimize loss on pretrained models: when the downstream learner tries to minimize risk on these perturbed examples, they can do well on the train set but not on a held out set from the original distribution (this work is all performed in CIFAR).<|endoftext|>Pros:  This paper is the first to study the problem of poisoning the performance of adversarial training. The proposed poisoning attack can successfully make adversarial training with small $\epsilon$ ineffective. In short, they all studied the threat that aims to degrade test accuracy by perturbing training data. Actually, The commonly used $\ell_{\infty}$ perturbation budget in the literature is 0.3 for MNIST and 8/255 for CIFAR 10, SVHN, and CIFAR 100. As a reminder, [6] was the first to point out that adversarial example noise can be used as poisons.<|endoftext|>This paper studies a data poisoning method for adversarial training. My concern for this paper is along the following lines: The threat model is really not clear   and to the degree of incomparable. This is already a significant change of threat model, and if labels are allowed to change, why would it be surprising that AT can be fooled? This is interesting   and as I said this paper has some interesting content. More revisions are needed   in fact I think a significant revision is needed.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; rating score: 6; The authors introduce BankGCN, a graph convolutional network that learns (chebyshev) polynomial filters over the graph. Comparison to at least some of these methods, or justification as to why these methods are not comparable is a must in an empirically driven paper like this. Could this be clarified further in the writing? Predict then propagate: Graph neural networks meet personalized pagerank. While the experiments are well designed, given that there are very few comparisons to similar works that claim to solve the problems of oversmoothing, and limited theoretical contribution I lean towards rejection.<|endoftext|>Weaknesses:  The novelty of the proposed BankGCN is limited. This paper proposes a new variant of GCN model, namely BankGCN, based on a novel graph convolutional operator. Strengths:  The paper is easy to follow with clear motivation and is well written. Most graphs in the benchmarks seem to be small and it is a known problem with those small and noisy datasets.<|endoftext|>The manuscript proposes an interesting point of view on graph convolutional operators. The empirical evaluation of the operator shows interesting results, but some points about the experimental methodology have to be clarified since the comparison with the baseline models seems not completely fair. Minors:eq (6): the last parenthesis is missingthe acronym IGFT is used without a prior definition of its meaningFor what concerns the paper organization, putting the notation in the appendix makes it a bit complex for the reader to follow the mathematical definition of the various components. Considering that these models are very similar to the BankGCN with p 1, they should be also considered in the discussion and the experimental comparison.<|endoftext|>There are many graph neural networks to deal with low pass properties of MPGCNs [1], [2]. (2) This paper is well written and structured. (3) The evaluation for the proposed methods is not sufficient. My concern is about the limited novelty and not enough experimental results.<|endoftext|>This paper proposes a graph convolutional networks where, at each layer, the graph signal is decomposed with an adaptive filter bank. Weaknesses:Integrating filter banks in graph neural networks is not new. Experiments need to be extensive. The authors need to provide a thorough analysis of the computational complexity of the proposed method. We thank the authors for addressing our concerns. However, there are several related methods in the literature.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 6; This paper proposed a method called LAMBDA for optimizing an agent s policy in constrained Markov decision processes. LAMBDA is a model based approach leveraging Bayesian world models, which deals with reward optimistically and safety pessimistically. The authors demonstrate the effectiveness of SG6 tasks in Safety Gym in terms of sample efficiency and satisfaction of safety constraint(s).<|endoftext|>Was this version of Lagrangian method used in previous work on Constrained RL? The authors present LAMBDA, a novel algorithm for learning in CMDPs. ## Strengths  The paper is very well written and easy to follow. I believe that using model based approaches to increase sample efficiency in safety sensitive applications is very relevant.<|endoftext|>This paper is about solving CMDPs in an uncertainty informed model based fashion, without going through the usual LP route. The blueprints for this paper is the following: 1. This is not a bad thing, on its own and in fact your paper made me learn about a lot of new ideas.<|endoftext|>This is a critical issue in differentiating between model free and model based RL. The approach relies on the model to simulate trajectories and therefore improve efficiency of learning and effectiveness of safety. Which aspects of the proposed approach are novel? The paper raises interesting questions, but ultimately cannot compellingly make the case for the proposed approach.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 6; This paper proposes a low precision version of SGLD which makes use of low bit arithmetic. In particular, since low precision gradient accumulators might lead to divergence of SGLD, the authors propose a new quantization function to preserve the correct variance in each update step. In the theoretical results of this paper, the authors leverage results in Dalalyan and Karagulyan (2019) to provide a (sampling) bound on the Wasserstein distance between the distribution generated by the low precision SGLD and the target distribution, but compare this with an optimization bound in Yang et al.(2019), which is the distance between the SGD estimation and the optimum. The metric used (test error) also does not seem to measure the accuracy of the sampled distribution using low precision SGLD. Some mathematical statements and notation in this paper are also unclear. Also, using $\theta_{k+1}$ to represent both the updates with and without quantization have been leading to much confusion during my review. "Non convex learning via stochastic gradient Langevin dynamics: a nonasymptotic analysis."<|endoftext|>This paper discusses using the stochastic gradient Langevin dynamics with low precision implementation. The authors present convergence results in Wasserstein distance norm for three implementation cases, which distinguish specific low precision thresholding schemes and different quantization techniques. The authors have tried their proposed algorithms in multiple classic application scenarios. Weaknesses:  The submission does not appear to be a major contribution to the field, as the main novel point is the analysis of the variance of low precision gradient estimator. The main theorems are proved with the results from Dalalyan and Karagulyan s work. In light of the specifics in the main review, the discussion of low precision SGLD in this submission is not significant enough and there are some places in the argument that are not clear.<|endoftext|>The author proposed a comprehensive study of low precision Stochastic Gradient Langevin Dynamics (SGLD). Empirical experiments based on large scale DNN examples are evaluated. Pros: the authors conducted some bias analysis for low precision SGLD based on [Dalalyan and Karagulyan 19] s result. Cons: 1.**Limited novelty in methodology**: the novelty in terms of methodology is limited and is not very interesting. Based on these evaluations, I tend to reject this paper.<|endoftext|>The paper gives the convergence of SGLD under Wasserstein distance with quantized weights and gradients. It also considers the situation when the accumulator is also in low precision, and proposes a variance correction strategy to deal with the additional quantization noise. Posterior sampling of quantized neural networks is an important problem to study. While the optimization of quantized neural networks are relatively well studied, this paper is the first to consider the sampling of such networks. The technical quality of the is nice. Post rebuttal Thanks for addressing my concerns. A novel paper with reasonable techniques.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; This paper aims to learn robust representation from the replay buffer for reinforcement learning. The key idea is to leverage the concept of mutual information and the InfoNCE tool to compute the mutual information as a regularizer (a.k.a.DRIBO loss). The authors conducted experiments on some standard benchmarks (DeepMind Visual Control Suite and ProcGen). Supplementary material shows that the authors seem to have digged into the experiment data with some depth. Weakness:  The writing is not so clean. Many other places also show that the writing is very rough. The results on DeepMind Control Suite are somewhat marginal improvement. While the results on ProcGen are impressive, the analysis and interpretation into these results are somewhat thin. Because the method involves many differences in comparison to the baselines, it is unclear whether the proposed main idea, mutual information loss, is the source of the major contributions.<|endoftext|>This paper tackles the problem of generalization amidst visual distractors for control tasks. In particular, the distractors have no dependence on the optimal policy and thus clearly form a task irrelevant component. The proposal is to use mutual information between two views as a proxy for how much task relevant information is present in the constructed representation. Post Rebuttal  The new results provided by the authors convince me fairly that the method is somewhat useful. I am unsure how much this sort of an evaluation scheme affects RAD s results, which should produce much better numbers than reported. Moreover, the performance reported for the proposed method is not good enough. For the ProcGen experiments, why don t you compare with DAAC [1] and IDAAC [1] since I believe they are the state of the art methods on ProcGen currently.<|endoftext|>This paper studies the problem of pixel based control with reinforcement learning. * The authors claim that $I(S_t^{(1)};S_t^{(2)}|S_{t 1},A_{t 1})$ can be maximized by maximizing $I(S_t^{(1)};S_t^{(2)})$. I don t think this is mathematically correct without making certain assumptions. * The author parameterizes mutual information as like $I_\theta$. * CEB is a generic information bottleneck. * The appendix shows in the main paper file. The approach is novel. The paper shows good empirical results on DM Control with natural video background and procgen. However, it s not clear to me why the procgen type of generalization and the procgen experiment are relevant to the main story and I have some concerns about the mathematical correctness.<|endoftext|>Motivated by the fact that RL agents are sensitive to unseen environments, this paper proposes a method to separate task relevant and task irrelevant information from observations based on unsupervised multi view settings. They use data augmentation methods to create multi view data for calculating contrastive Multi View Information Bottleneck (MIB) and use it as the objective to increase the generalization of model representation. The authors provide clear analyses of how to use mutual information to learn task irrelevant information based on multi view settings. \  The proposed methods show advantages in DMC and ProcGen environments. It would be great if the author can conduct experiments on other realistic tasks. If the entire image is rotated, will the state change and the action not be consistent with the state? (3)    Based on (2), I am also wondering if this method can be extended to more general settings to increase the robustness? Also, the experiment part seems a little weak since the two environment suites are very similar. I think this paper is marginally above the borderline.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; The paper proposes a way to combine neural architecture search (NAS) with federated learning (FL) and personalized federated learning (PFL). Weaknesses:  Main issue is the experiments. The result is that one cannot say anything about the abilities of these models in practice.<|endoftext|>FEDPNAS searches for an architecture with a base component (shared across clients) and a personalized component. The algorithm of learning architecture with a base component and a personalized component in FL is novel and interesting. Fig.2 is not referred to in the paper. What is the difference between  task personalization  in this paper and distribution shift across clients? Why not use these federated datasets for evaluation? However, the definition of task personalization is confusing, and the experiment used for evaluation is not sufficient to fully demonstrate the effectiveness of FEDPNAS.<|endoftext|>It would be helpful to see how the personalized architectures differ from each other to judge how well it is really helping accuracy. Experimental results on both CIFAR 10 and MNIST datasets demonstrate the promise of the proposed method. Strengths:+ The authors target the personalization problem in federated learning from a combination of task and context points of view. Weakness:  The paper does not show the variety of architectures that were created due to the added personalization.<|endoftext|>This paper presents a personalized federated learning framework, which leverage a stacked search space of base and personalized architectures along with the context aware search strategy to learn personalized neural network in the FL scenario. For example, I doubt about whether the NAS method could handle the non iid data situation or stateless setting, which are a common case in FL setting. The paper is well organized with good motivation on conducting personalized NAS for FL. Lack of experimental results on real dataset to provide the effectiveness of the proposed method.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; There are quite a few datasets and baselines used in the comparison. It could be that a slightly deeper or shallower MLP would change which method is the best. The presentation needs significant improvement. Also, see below for grammar issues. How sensitive are the results to the hyperparameters of the MLP?<|endoftext|>The paper propose the title of sphere2vec for novelty but the main improvements to the method is not with the encoding but with the framework itself. Strengths:1) The approach is demonstrated to be better than the previous method both qualitatively and quantitatively. As a paper, the results does show improvements through the framework, in the form of representation learning. Weaknesses:1) Paper was slightly harder read and some useful information such as the definition of the studied approaches were left in the appendices.<|endoftext|>Would you please discuss it or add it as a baseline? Using a simple MLP or (sin/cos) to encode these Cartesian coordinates to the network and check the performance. Why do we need to enforce that "multi scale representation of x preserves the spherical surface distance"? Would you please add some motivations? From the tasks (Figure 2) listed in this paper, I would assume the GPS coordinates are distributed sparsely.<|endoftext|>seem to be missing or relegated to the supplemental. Additionally, several  variants of the method are proposed, and it is not clear what variant  to use when.
Reject; rating score: 3; rating score: 3; rating score: 6; rating score: 6; rating score: 6; This paper proposes a two stage method for transfer learning, which firstly infers unstable features from the source task and then learns stable correlations for the target. Experimental results validate the effectiveness of the proposed method. However, there exist many weaknesses, including method, problem setting, theoretical analysis and experiments. KDD 2020This paper proposes a two stage method for OOD generalization and transfer learning problems. (2) Weakness: 		(a) Method: The proposed method infers the unstable features on the source task and then learns the stable correlation on the target leveraging the knowledge of unstable features. However, I think the method is quite ad hoc and naïve, and it lacks technical contributions. The idea of clustering data with unstable features is also quite similar to the HRM[1], which further reduces the contributions. (b)Problem Setting: This method requires both labeled data from the source task and target task, which uses much much more information than existing methods for OOD generalization. Why it can infer the unstable features? (d)Experiments: Since it is both the OOD generalization method and the transfer learning method (setting is the same), there lack many baselines. I think more transfer learning methods should be taken into consideration, as well as some domain generalization methods. (e)Definitions: What do the stable features or unstable features mean in this paper? What is the formal definition of stable and unstable? I think the authors missed many related works here.<|endoftext|>The paper considers a transfer problem when the spurious correlations of source tasks can be applied to the target task. The authors propose to identify the unstable features on source tasks first and then cluster the target data according to these features. Weakness:(1) A critical limitation of this work is the strong assumption on the transferability of the unstable feature. IMO, such an assumption is restrictive in most settings. For the intra task transfer (where the classification tasks are the same between source and target), finding the invariant part via invariance learning methods is more realistic. As for the inter task transfer (a wilder setting),  it is very rare that the unstable features and the way they correlate the outcome are exactly the same across the tasks. (2) The related work and baselines are not discussed thoroughly. Domain generalization using a mixture of multiple latent domains. This paper addresses the spurious correlation by transferring knowledge from source tasks. Although intuitions are provided and empirical effectiveness is illustrated accordingly, the method is restrictive due to the strong assumption on the transferability of unstable features.<|endoftext|>The authors introduce a method (TOFU) for learning classifiers that are robust to spurious correlations in a transfer learning setting. They argue that approaches which rely only on (input, label) pairs and use no extra information to identify spurious features could fail to learn a robust model due to insufficient data for the target task. Namely, a classifier is trained on data from environment E1, and is evaluated on data from a second environment E2. It also provides a solution in the form of a method called TOFU which has few requirements in terms of additional data annotations compared to existing methods. TOFU is an extension of an existing work, and has some theoretical motivations, but the underlying assumptions may be too strong. Extensive empirical evaluations are performed, comparing the performance of TOFU against many different baseline methods on both image and text classification tasks. The paper itself is well written, and does a good job at describing the current research landscape of robust feature learning. Limitations of the method are not considered, even though there are a few clear ones both technical and empirical. It seems like for TOFU to work, the classifiers trained in step S.1 need to be unstable such that they will make sufficient errors on E_2 in order to have sufficient negative samples for the triplet loss objective in S.3. TOFU has fewer requirements in terms of additional annotations compared to other methods, and domain specific prior knowledge is encoded into how the datasets are split into E_1 and E_2. Quantitative analysis of TOFU reveals that it is significantly superior to the many baselines considered, and that is consistent on all tasks and datasets. The novelty of the work is somewhat limited. S.1 to S.3 is nearly identical to the cited work of Bao et al., except that the objective in S.3 is changed to the triplet loss objective. This objective comes from Theorem 1 which is a novel contribution, but is based on theoretical results also from the cited work of Bao et al.It would also be recommended that more real life examples of the transfer learning scenario in question be provided, since it currently seems as if it was created to show the applicability of the method. Impressive empirical results on many tasks/datasets relevant for this application.<|endoftext|>The idea is that if a lot of source data is available, these spurious correlation features should be easy to find. To find the unstable features, authors hypothesize that they are related to mistakes that a classifier makes in different environments. Therefore, they learn a model on one environment and run it on a different environment (source data), splitting the data into correctly predicted and not. It is really complicated though (involves many steps) and i fail to make a connection between this and say Domain Invariant Representation learning. For example, if i was to solve the problem of domain generalization (the setup seems really similar to me), a reasonable baseline would be to train a source model that learns an invariantt representation between various environments (using DANN, or CMD or MMD or whatever). But this is not in the baselines. The way i read it is that environement is more mixed   it can contain multiple backgrounds. It would be nice to draw comparison between your method and DIRL or at least explain why DIRL is not applicablePros:  The paper is well written and easy to follow (albeit a lot of helpful info is also in the Appendix)  The experimental results seem convincing, there is a study of dependence of (some) hyperparameters on the end result (num of clusters)Cons:  DRO needs at least a brief introduction  Experiments are only on 2 environements (domains) in train  The method seems extremely costly: for a number n of source environements, you train n classifiers, then for n^2 pairs you do partions, then you learn fz representation, then cluster the target data all while tuning and looking for fz dimension and number of clusters. There are hyperparameters that need to be tuned: fz dimension, number of clusters. Tuning all the hyperparameters using limited target data   how does it work?<|endoftext|>This paper aims to transfer the knowledge of spurious correlations in a set of source environments to a target environment. Then they aim to transfer the spurious knowledge from source environments to learn a classifier in the target task that ignores the spurious pattern. I am leaning towards the borderline with marginal acceptance. Second, they use the error of the f1 on E2 to "separate" the E2 examples into two groups, one group for correct predictions and the other group for the wrong predictions under each class. I feel the assumption is too strong which would need some justifications. Thus they learn a f_Z that outputs an embedding to separate these two groups by triplet loss. This ignores the spurious patterns based on f_Z in the target task and learns a stable classifier. Overall I like this paper. This paper builds upon the core idea from Bao et al.2021 that uses the error of the model in different environments to capture the spurious patterns, and it further extends to the transfer learning setup, which I believe it s a new setup. My concerns are as followed:1. None of it corresponds to the unstable features. If this assumption breaks, then there is no real spurious pattern to be learned here. B) On the data side, this work assumes the source tasks need to have varying degrees of spuriousness and the target task has the same spurious patterns. How do we possibly check these assumptions are real in the real world setting? Will this method break if there are multiple spurious patterns? Maybe the authors can show if this approach is robust when the assumptions are mildly violated like there is a distribution shift among environments, or when there are multiple spurious patterns in the data, and illustrate how to select hyperparameters. + The results are demonstrated in multiple datasets and the number is convincing compared to several recent baselines.
Reject; rating score: 6; rating score: 6; rating score: 6; rating score: 6; The paper presents a novel (or an unusual type of) generative modeling approach that is kernel based and non parametric. The basic idea is using an operator that maps from the RKHS of Z to the RKHS of X, so that data generation can be done by mapping the prior distribution $p_{prior}(z)$ to the RKHS of Z, applying the operator, and project down the result in the RKHS of X to the data space. The operator is constructed as the RKHS analogy of the desired conditional distribution p(x|z) (which makes $\int p(x|z) p_{prior}(z) dz   p_{data}(x)$), so that it transforms the RHKS embedding of $p_{prior}(z)$ to that of $p_{data}(x)$. * What s the difference between the Perron Frobenius operator in Definition 3.1 and the push forward operator of a distribution by the same map $f$ (unnecessarily invertible)? Up to my knowledge I did not notice similar ideas before. * The generated data is a _linear_ transformation on training data samples. * The presentation seems misleading. In this sense, it is of the same type of kernel density estimation (KDE).<|endoftext|>The paper proposes to leverage the Perron Frobenius operator to simplify the mapping from Z  > X in latent variable generative models. Strengths:   The idea of using kernel transfer operators for generative modeling is novel and interesting, and. Weaknesses:   Notation is a bit confusing at times, as some items are not defined. “We often [approach] this” in the intro  Citation formatting is confusing (should use parentheses)  “We [demonstrate]” at the end of Section 1  “MDS based” in Section 4.1This paper proposes to use kernel transfer operators for generative models, by leveraging a kernelized embedding of the “forward operator” (generative model). The authors experiment with various kernels and obtain favorable results on high dimensional image generation tasks.<|endoftext|>The new scheme is based on a kernel transfer operator that leads to a cheap method for distribution matching. The authors rely on rigorous theory on RKHS and propose a framework for transferring a prior distribution linearly (in RKHS) to the data distribution. For these reasons, I recommend a weak rejection of the paper. On the same point, what is the inference time required by the method to generate a new sample? Minor comments: we often approaches > we often approach by via  > remove by SGD mentioned before it is defined. ”A natural extension of Klaus et al.” > perhaps you mean “A natural extension of the work of Klaus et al.” In general some of the citations should be in brackets. ”There, for” this sentence is not clear. X’ and x’ are not well defined. To summarize, the authors address an important problem and propose a new generative model with several benefits over existing methods in the low sample size regime.<|endoftext|>The paper proposes a density modelling approach that is based on applying conditional embedding operators to RKHS embeddings of a base distribution. The approach, thanks to use results from the kernel literature, is conceptually simpler and appears less computation heavy compared to VAE/flow models, while at the same time performing favorably. It would be great to make this more clear. * A simple linear operator in RKHS that maps the embedded base to the embedded data distribution, neat idea! However, the transfer operator is just the conditional embedding operator of Song et al, marginalized over the conditioning variable / applied to the mean embedding of Z, plus a pre image estimation (see alos below). Great idea, though there are some issues with presentation and experiments. As the authors mention, pre images are problematic as RKHS embeddings are not necessarily surjective. So supposedly, there is a convergence question in Fig 5.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; rating score: 8; The main goal is to obtain an encoder that learns a consistent and reusable mapping from fMRI space to representation space. ## Strengths    An interesting idea of constraining the embedding space with an existing non generalizable manifold learning solution. A mostly clear exposition and a well written paper. The experiments are performed on a single ROI time courses, which diminishes the value of the work. The multivariate nature of fMRI is not taken into account and the experiments have mostly toy example nature. Since the decoders are subject specific and training is happening on a few subjects simultaneously, it is unclear how a new subject is handled. Which $g _i$ decoder is used in this case? Two main concerns: limited technical novelty and limited value for neuroimaging. My recommendation is based on these limitations.<|endoftext|>The paper proposes a model to learn a low dimensional representation of fMRI data over multiple subjects of the same experiment. The model is built as an auto encoder, with an encoder shared across subjects, and a separate decoder per subject. ## StrengthsThe problem tackled by the paper is interesting, and the proposed model is original. The paper is clear and well structured. It is not super clear why the proposed architecture uses a shared encoder and a separate decoder per subject. This choice seems central to the proposed method and should be better explained. 2.It is also not clear which decoder was used to predict on held out subjects. The authors should explain how the hyper parameters were chosen. The landmark approach is actually below MRMD AE at 70% of train set, and the difference does not seem relatively smaller than the difference at other percentages. 5.Tables with numbers are very non intuitive, and might even be considered a ploy to hide small differences in performances between methods. 7."We will provide a link to an anonymous repository to the reviewers and area chairs", yet the link was not provided. and a "strong subject batch effect" ?<|endoftext|>This paper proposes a neural network based modeling strategy to learn a common latent space from multi subject fMRI data. The proposed framework is tested on two large fMRI datasets and an improved stimulus decoding (from the shared space) and cross subject translation accuracy is achieved over competitive baselines. Strengths: 1. 3.Thorough comparisons are employed with multiple datasets  4. Some results are not particularly exciting, for instance using the proposed MRDM AE  framework does not result in a major improvement over a standard manifold regularized AE atleast in terms of stimulus decodability. In general, how could the proposed autoencoder framework be extended to a new subject? Do the authors envision this would involve simply training a new subject specific decoder? The merits of the proposed technique are well supported by the experiments and results.<|endoftext|>I have only some minor suggestions for improvement here: [1a] As far as I can tell, the stimulus set $X$ and its elements $X_k$ is only used to say that the encoder maps the fMRI activation map to some representation of the stimulus, but $f ()$ is never defined explicitly. Addressing [2c] and to a lesser extent [2d] would also strengthen the paper, but would not be sufficient if [2a] and [2b] were not addressed. EDITED: I am adjusting my rating to a 6 based on additional information provided about the raw classification scores, but I m still concerned about the baselines chosen. This means that (very roughly) differences between the best and second best performing models smaller than the SD may not be statistically reliable. MRMD AE is rarely convincingly above the next best model by even a single SD. The paper reports untrained cross subject translation (which is a sort of out of sample reconstruction metric), and the future point placement metric (which is sort of a latent space closeness metric). Importantly, the overarching narrative present in the front matter of the paper is much weaker here, and the link made above between the contributions and the evaluations is not made explicit. To be clear: I don t think ablations studies are critical to this paper   but the specific ablation study in the submission could be stronger.<|endoftext|>Figure 2: I find the MSE values extremely small, and the confidence intervals overlapping a lot. Subject specific decoders are used to more directly recover individual signals, while the encoder is shared across every subject under the assumption that every person will share common low dimensional features for the same stimuli. Given these reasons, I recommend acceptance of this work. The paper mentions a “translation” task several times throughout the paper, but it never explains what it specifically consists of. Given this seems to be the task where the proposed architecture is the least strong (from Table 3), this is an important part to clarify. This is important to understand whether accuracy is a good enough metric for this task. Are these the same or did something change? 3.The paper mentions in the “Related Work” section that it used a similar multi objective NN approach like in SAUCIE; however, it is not clear what are the differences to this previous method which would grant novelty. 7.Footnote (1) is confusing. Although the authors didn t address all my questions (for example my comment titled "Further discussion") nor they uploaded a new pdf version to check corrections, I believe my main concerns were clarified. I don’t believe this is a metric well known by the community. In any case, why would batch effects be a problem in this case, as it is well known how they can influence fMRI timeseries and therefore could be important for a rich low dimensional manifold? 6.A big point of the paper is how PHATE seems to correct for strong subject batch effects existent in fMRI, and how this is important in creating a subject independent manifold. 6.How did the paper arrive at this set of hyperparameters?
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; The paper presents a new bound as the objective for variational inference. How does it relate to other bounds? But I expect more comprehensive (unnecessarily deep and involved) technical investigations that help readers grasp a general understanding of the developed bound.<|endoftext|>Moreover, the description of the experimental settings is limited to reproduce the results. I recommend improving writing and experimentation. The critical issue of this work is that the authors did not present the motivation for the extension. I could not find the practical benefit of the generalization of TVO to f VI. Could not follow the discussion below Eq24.<|endoftext|>How to guarantee the existence of the function \chi for any generation function f? The paper provides some theoretical analysis and the optimization methods of the suggested framework and supports the proposed $f$ TVO with numerical results. I did not find the monotonicity of $S_\beta$ in the paper.<|endoftext|>However, I still think that the experimental setup should be improved and some design choices (eg.wrt the custom divergences) better motivated and evaluated. The experiments can also be improved, showing standard (from multiple runs) to begin with, and providing more discussion on the results. Overall the paper is well written and (mostly) easy to follow. Post rebuttal update:Many thanks to the authors for addressing some of my concerns.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 8; This paper investigates the modality greediness of learning in multi modal deep neural networks (DNNs). The paper provides experimental results to validate their claims and the effectiveness of the proposed methods. * Weaknesses    The main claims related to worse generalization performance from the greedy nature is not so clearly validated with experiments. In other words, I think the experimental design and their evidences are weak to validate their claims. * Minors    Section 5.3 in page 8, obverse  > observeThe research topic is interesting and it seems that the main answer points of the research questions are appropriate. I think that the authors should reconsider whether the differences on conditional utilization rate and those of conditional learning speed really affect the generalization performances, or they should try to establish other strategies to find evidences to support main claims. From the reason, I recommend this paper as reject.<|endoftext|>This paper hypothesizes that due to the greedy nature of deep learning, these models tend to rely on just one modality while under utilizing the other modalities. The authors empirically observe this phenomenon on several dataset with a proposed metric. Researches about this issue are meaningful and the idea of controlling the training process specific to modality is an intuitively promising way. It seems the experiments verifies the greedy learner hypothesis, but no proofs or theories are provided to support this hypothesis, either detailed analysis why the training speeds are different. The discussion in related work section is also inadequate. More natural modalities, like audio and visual, should be included for better evaluation. Learning to balance the learning rates between various modalities via adaptive tracking factor. This paper focuses on an interesting phenomenon in the multi modal learning: the imbalance problem in the multi modal training. According to the provided experiment results, this method addresses the issue of greedy learning. Related works are also not well discussed and compared.<|endoftext|>This paper focuses on the multi modal interaction problem, that multi modal models tend to rely on just one modality while under utilizing the other modalities. Since conditional utilization rate cannot be computed efficiently during training, they introduce an efficient proxy based on the pace at which a DNN learns from each modality, which we refer to as conditional learning speed. So they propose a training algorithm, balanced multi modal learning, and demonstrate that it indeed addresses the issue of greedy learning. 1.The topic of modal balance is interesting and has attracted much attention. The proposed method is direct and easy to follow. What are the improvements to the proposed methods? Why not compare these methods? The reviewer believes that the experiments are very lacking in comparison with the SOTA methods.<|endoftext|>The paper talks about efficient multi modal training. Based on conditional learning speed, they propose a multi modal training algorithm that mitigates the imbalance in modality utilization. Through experiments, the authors empirically show that it is true and their training paradigm performs better than vanilla multi modal training. They also show through experiments that the proposed training procedure leads to more efficient modality utilization and shows improved accuracy. The empirical results are strong. So, I am not clearly convinced with the statement in Section 4.1 that "If the greedy learner hypothesis is true, the conditional learning speed would serve as a proxy for the conditional utilization rate. This paper is easy to read and addresses the shortcomings of vanilla multi modal training scenarios.
Reject; rating score: 1; rating score: 3; rating score: 5; rating score: 8; This paper targets to a new evaluation metric for the performance of generative models given a set of real images and a set of fake images. They demonstrate their metric is less sensitive than FID against image perturbations. In this case, please show the image examples with the maximal amount of perturbations. The experiments are not convincing. We therefore need a metric to be sensitive enough in this range to distinguish cutting edge generative model techniques. Research community cares about the quality on generated images.<|endoftext|>This paper generalizes the widely used FID metric for image generation evaluation by fitting a mixture of Gaussians instead of a single Gaussian on the extracted features. Some of my major concerns still remain (e.g.the justification of using ratios when comparing different metrics with different or unbounded scales, the fairness of the empirical evaluation still seems not convincing enough to me). From my perspective, I think there are issues with them: comparing metrics with different scales are hard and using these ratios may not make sense either. In short, although the authors found this issue during comparing them, I do not think the solution is good enough, which corresponds to the central empirical findings of this paper. The way that adversarial perturbations are constructed may lead to results that are not fair for FID.<|endoftext|>This paper propose a new method to evaluate GANs. 2.Authors express simple and convincing examples (Figures 1~2) to support the motivation. The motivation of this paper is address the shortcoming of FID, which is about Gaussian assumption. 3.For different K,  do WaM always win compared to FID about the sensitiveness? I would like to see the application of the proposed method on GANs based methods. After rebuttal Thanks for authors  response. I think authors did not address my concerns about why authors performance the evaluation of the sensitiveness, and more comparison with the variants of StyleGAN.<|endoftext|>This paper proposes a new metric (WaM) for the evaluation of images generated by a network, as an alternative to the commonly used FID metric. In particular, Figures 1 and 2 highlight the main problem behind the FID metric, the fact that the features of the Inception v3 model do not follow a Gaussian distribution. Some comparisons with previous work on the subject are still needed in my opinion, but overall this is a solid work, and I believe it should be accepted. The motivation behind the paper is also cleanly presented.
Accept (Poster); rating score: 6; rating score: 6; rating score: 5; rating score: 5; rating score: 3; The paper presents a new method for continual meta reinforcement learning called CoMPS. I would however, have liked to see some more ablations and analysis of the approaches and maybe some better motivations too. The authors show that this approach outperforms generic meta learning and transfer learning approaches across various environments for the specific settings considered in the paper. The paper presents a novel, potentially useful setting/objective for the meta RL/transfer learning communities to focus on, which in some sense bridges objectives pursued by the meta RL and transfer learning communities. 3.The paper is also really well written with clear diagrams and algorithm figures which make it very easy to read. 5.The paper also does a nice job of tuning the baseline hyperparameters to make sure the results are competitive. Maybe the trajectories used for BC update aren t good enough in these types of problems?<|endoftext|>This paper proposes a meta RL algorithm that is focused in particular on the learning of sequential tasks without revisiting previously seen tasks. This issue continues in the introduction where the contribution part focuses on what problem is being solved, but not how. What are the main scientific and technological innovation that are introduced with respect to existing approaches? Is this one of the major contribution of the paper? However, this is not sufficiently followed up in the experimental analysis. The experimental results are encouraging. However, it fails to make clear claims or clearly explain the key novel aspects of the algorithm, how and why they are sufficiently novel.<|endoftext|>The empirical results show that CoMPS can outperform the prior continual learning or meta learning methods on both stationary and non stationary task distributions. weakness:  This paper can be viewed as a combination of several existing techiniques. In particularly, this paper claims that it addressed a type of continual learning problem but did not consider one of the main characteristic of contual learning   i.e., the forgetting issue. though the paper claims that this is the first to  formulate and address the continual meta learning problem, this problem setting has already been discussed before, some work can be referred to:[1] Online Meta Learning. The issues considered and the solutions proposed in this paper sound reasonable, and the proposed method does achieve the better performance. But some details of the whole pipeline need more justification, and the optimization between sequential tasks seems to be independent.<|endoftext|>This paper proposes a method that incorporates fast adaptation into continual learning. The main challenge is that previous tasks cannot be revisited during continual learning, and the policy has to be trained on limited offline data. The authors address this challenge by utilizing importance sampling policy gradient and value estimators in the inner loop and behavior cloning in the outer loop, which enables meta training with fully offline data. One major concern is its novelty, which resembles the previous approach GMPS. 3.Backward transfer: although the authors claim that they do not evaluate backward transfer or forgetting, it is one of the core challenges in the field of continual learning. A discussion over these works should be added.<|endoftext|>The authors then propose an algorithm that combines RL on the current task with offline meta RL on the previous tasks. The main novelty of this work is the proposed continual meta RL setup and the design of the experiments. Regarding the algorithm, I find it to be reasonable application of existing ideas to this setup. To give a few examples: The ablation study indicates that the method performs better by using an existing algorithm for doing off policy learning. The baselines that were chosen are complete methods, that were not specifically designed for this new setup and are not high performing in it for that reason. To summarize, I feel that the paper will have to make stronger algorithmic contributions and more carful ablation study for it to get accepted to ICLR.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 5; A neural network representing the PDE solution is trained simultaneously with a sparse regression on the partial derivatives of this neural network. Pros:    The task of learning a PDE is interesting. Both names seem to be used in the paper. (I imagine a sparsity inducing approach should still fare better overall, but a comparison to alternative/simpler approaches should be warranted.) There are many other differentiable approaches to sparse regression that the paper does not mention.<|endoftext|>Would this not also be a soft relaxation of the hard mask constraint? What am I missing here? The paper seems to focus on (a), and does not focus on the benefits of (b) in the text.<|endoftext|>$\lambda$, the strength of the regularization constant, is a hyperparameter that is varied uniformly. * The authors need to elaborate on the training strategy they used for DeepMod. This is also consistent with the observation made by the authors in sec.4.2.<|endoftext|>The authors seem to work around this issue by imposing proper hyperpriors, and pretrain the PINN so that the joint optimization starts at an informative initial value, these fixes seem somewhat hackish: proper hyperpriors will restrict the algorithm s ability to obtain truly sparse solutions in practice, or the behavior of the algorithm may depend on the value of the hyperprior parameters $a,b,c,d$ which are difficult to determine a priori. Therefore I am keeping my score unchanged.
Reject; rating score: 5; rating score: 5; rating score: 8; This paper considers a property of MDPs where multiple sequences of states end up at the same action. Overall, I am not convinced that the method is truly useful in real settings beyond the deterministic + spatial problems that the paper focuses on. The method is designed for deterministic MDPs, in which case it is believable that the transitions are known. The authors also discuss an extension to the stochastic case, where equivalences are determined based on next state distributions.<|endoftext|>In the context of reinforcement learning, authors propose an exploration strategy based on environment specific prior knowledge of action equivalence. An example of such equivalence is rotating 180° twice in a grid world, as the agent comes back to the same original state: the action sequence forms an identity in this case. The paper is written very clearly and the contributions are easy to follow. I don t find this sufficiently significant as its own contribution. Additionally, the applicability seems to be limited to grid like MDPs, where it is easy to hand craft the action equivalences.<|endoftext|>The idea proposed in the paper is quite elegant, albeit its realization is quite complex. Overall, the paper is well written, even if its complexity is sometimes a bit difficult for the reader. Equivalent action sequences are sequences that lead to the same state (please correct me if I m wrong)  These sequences are used, from a current state $s_t$, to build a DAG, that sorts of models where the agent will end up after any sequence of actions of length $d$. This is the information that we want to extract. As such, I recommend accepting it. The algorithm is general enough to be applicable to an Atari game, with minimal engineering effort. As such, I think that the paper will be very interesting to people attending ICLR, and recommend accepting it.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; The paper presents a study of continual learning of neural networks in the neural tangent kernel (NTK) regime. This is a very well written paper. But the authors provide just enough background detail on the NTK and the base Bordelon et al.(2020) and Canatar et al.(2021) papers to have a grasp of the main tools used to derive the new results. I was not capable of reviewing the calculations presented in the appendix. Therefore I cannot stand for the correctness of the results presented in the paper. As I am not an expert in this field, it is also possible that I missed some important related work that should have been cited.<|endoftext|>The paper is a theoretical paper about continual learning, that studies 2 tasks settings in the NTK regime. The paper is well written and well structured. Each section is clear in what the authors try to explain demonstrate. Continual learning is usually about learning in a non iid setting with several tasks, where each task might lead to forgetting of the previous one. I still will grade this paper above the acceptance threshold, because I think the paper can be useful for the community.<|endoftext|>NTK is one of the powerful theoretical tools for analyzing the behavior of neural networks. Although the paper s significance is a little limited by the relationship with previous work such as [1] and the restricted scenario, the results (especially the self knowledge transfer case) are interesting. 2.The studies are restricted to continual learning with explicit task boundaries. But I suggested the authors claim the settings more explicitly at the beginning of the paper/method for clarity.<|endoftext|>3.Most experiments results are about forward transfer, I think it would be better to show backward transfer as well. The paper provides some theoretical insights on the forward and backward transfer in sequential training. But in Sec.4.3, the authors claim no forgetting appears for equal sample sizes, which is not the case for most continual learning tasks in my experience. In this sense, I think the claims of this work are more about sequential training on a large dataset rather than continual learning.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; This work (EVFM) aims to to improve predictions of n body system dynamics by combining continuous lie symmetries with permutation symmetry. Initial Recommendation: RejectReason: In my view, in current format, the weaknesses outweigh the strengths of the paper. The results on the synthetic task and the molecular conformer generation task are promisingWeaknesses:1. Please add scalability results especially against E(N) GNN which the authors compare against. "Cormorant: Covariant molecular neural networks." Overall, there are certain merits for the proposed architectures, but the paper is hard to read due to some imprecise math, lack of clarity, proofs, etc. I initially suggest rejection.<|endoftext|>This paper proposes a new model: Equivariant Vector Field Network (`EVFN`), which aims to solve the multi body system modeling problem. The key differences of `EVFN` are 1). Extending the equivariant basis from one dimension to three dimensions. 4.The number of training samples used in the multi body modeling experiment is not reported. Meanwhile, there is no analysis of the impact of the number of training samples for the different baselines. 5.From Table 1, the result of `GCN`  surpasses many sophisticated baselines in the interpolation and extrapolation tasks, such as `EGNN`.<|endoftext|>The paper proposes an rotationally equivariant neural network based on a transform of particle pairs into a rotationally invariant reference frame. This is applied to trajectory prediction of many body particle systems and molecular conformer search. Therefore, I can not recommend acceptance at this stage. This is not possible in the proposed approach:The EVFN transform is rotationally equivariant and translational invariant only with respect to global transforms.<|endoftext|>The authors introduce a model to predict the time evolution of Newton mechanical systems and small molecules. #### Strengths:  The paper is well organized and well written (besides some non standard use of English). The mathematical presentation is fairly clear. the initial centering giving translation invariance, and the scalarization block giving SO(3) invariance  Did the authors experiment with other kinds of SO(3) equivariant frames in the scalarization block? Moreover, it does not seem that the learned representations are equivariant.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; This paper studies the exploding and vanishing gradient problem (EVGP) in the sequential modelling with chaotic dynamics. Theoretical analysis of the EVGP, based  on the relationship between loss gradients during RNN training and Lyapunov spectrum of RNN generated orbits, is provided. Inspired by this analysis, an alternative to BPTT training algorithm is proposed, named sparsely forced BPTT, that forces the diverging dynamics to conform to the true trajectory, at regular time interval provided by the  Lyapunov spectrum. **Strengths**  Analyzing chaotic behaviour of RNNs (vanilla, GRU, & LSTMs) based on the connection between loss gradients and Lyapunov spectrum is novel. Although ideas similar to sparsely forced BPTT exists in the literature, its interesting to see there exists a natural notion of the interval at which the teacher should be forced onto the student. No evidence for this point has been provided. In essence, the theory only characterizes how the gradients will behave knowing the dynamics and essentially under the hood just boils down to the eigenspectrum of the jacobians, which has been explored numerous times in the literature (starting with Bengio et.al.as to why gradients explode or vanish). This work does not help any practictioner to carefully choose one architecture over other given that the time series dynamics is chaotic. Do you have any sense of what values might be a good place to initialize the search from? Did the authors try out similar training scheme in their experiments? Is there any reason behind not using ODE RNNs as a baseline, since they are designed to be operating with stable transitions? **Writing Clarity**  After Eq.1, For RNNs, you use $h_t$ to indicate the hidden states and the transition function $F_\theta$ for standard RNN, uses $h$ as the bias. The latter has been explored in the literature and the paper fails to mention these works in the literature review. Theoretical contribution is of some novelty, but it would be another way to explain EVGP, albeit with dynamics in mind.<|endoftext|>The authors provide a study of gradients of the recurrent neural networks using Lyapunov exponents. They claim to offer a simple and effective training for the chaotic data. Strengths: The authors establish a connection between Lyapunov exponents and behaviors of the dynamics of recurrent neural networks. Some derivations for particular recurrent neural networks are provided. Weaknesses: In general, Lyapunov exponents are one of the most important and well known techniques to characterize the dynamics of the chaotic systems. One of the concerns when training with time varying signals the dynamics becomes essentially time varying and the results as stated do not hold. Beside long calculations in the appendix, that are mostly straightforward I am not convinced that the approach will be numerically efficient. They also highly depend on initial conditions, and I failed to find any analysis related to this. In addition, the RNN dynamics are mostly bounded stable and show chaotic behaviors only in bounded regions and can be divergent only locally. Based on what is provided it is difficult for me to believe the claims are valid.<|endoftext|>The paper connects the gradient behavior of recurrent neural networks with its dynamics through the maximum Lyapunov exponent of a trajectory, which is generated by the respective RNN. POST REBUTTAL COMMENTS In general, I m satisfied with how the authors addressed my minor concerns, in particular I appreciate the additional experiments.\However, my main concern that the theoretical framework provided is very limited in its usefulness remains.\I raise my score to (very) marginal acceptance. Moreover, the paper shows that RNNs generating a chaotic trajectory always suffer from the exploding gradient problem. In particular, several connections between RNNs which produce specific dynamics (e.g.with stable fixed points or limit cycles) and the mitigation of the exploding gradient problem are proved. However, given that these RNNs   at least the moment the optimizer finds parameters such that the resulting RNN produces chaotic trajectories   will always run into the exploding gradient problem, the paper suggests an algorithmic fix for that, by forcing the hidden states of the RNN to be the pseudo inverse of the underlying ground truth trajectory at equidistantly distributed points in time during training. This cuts off the gradients at these points in time and forces the output states to be close to the ground truth trajectories. The paper also states in the very end of the discussion that this forcing can significantly change the dynamics. Hence, it would very interesting to see some sensitivity analysis with respect to changes in the input for popular architectures (one with stable fixed points and one with limit cycle behavior for instance) and a discussion on how this might change the provided theory. I would like the paper to acknowledge that the provided approach is in fact equivalent to windowing, which is a very standard practice for training RNNs. Moreover, I would like to see results using the same window length $\tau$, as in the suggested approach, but using a standard initial hidden state (set to zero). But I could be mistaken. * Unfortunately, the empirical investigation is very limited and I would have liked to see results on other tasks, such as NLP tasks, health care applications and others (as paper claims in the introduction: chaotic behavior can be found in many sequential data sets), to see if this approach actually increases the performance of RNNs in widely used RNN applications. * the suggested training method seems to be a minor deviation of very standard approaches* the empirical evidence is not sufficient (more diverse datasets should be considered) and the presented results lack more useful comparisons.<|endoftext|>The paper looks at the asymptotic behavior of the Jacobian of various RNN variants (standard, LSTM, GRU, PWL RNN) when realizing stable vs chaotic dynamics. In particular, the paper shows that in the chaotic setting, the gradients asymptotically explode, i.e., when learning chaotic dynamics, the gradients have to explode (asymptotically). The paper proposes to overcome this limitation by truncating the BP length and applying a teacher forcing method that periodically projects the observation onto the hidden state during training. **Weakness:**  The Lorenz and Rossler systems used in the experiments seem to be deterministic and fully observable, i.e., the initial state + dynamics function defines the entire trajectory. Thus there is no need for using an RNN with a hidden state. What are the advantages of using an RNN, which suffers from gradient issues, compared to feedforward networks? Experiments on partially observable chaotic systems would have been appreciated. What happens in the "inverting the output mapping step" of the in teacher forcing when the output dimension is much lower dimensional than the hidden state, and the output mapping is not uniquely invertible? Please elaborate on the weaknesses mentioned above.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; The paper proposes evaluates using statistical tests on random 1d projections in the latent space of a flow model for groupwise out of distribution (OOD) detection. I found it easy to understand the ideas and methods of the paper, results are also well understandable. But in any case good to see precisely how well OOD detection works in this way.<|endoftext|>The paper provides an interesting approach for OOD detection problem for flow models. The approach is based on random projections on the real line where KD statistical test is applied in order to compare two distributions. I admire the simplicity of the approach and I tent to recommend to accept this work at this stage. It can be applied both in data space and latent of the flow, but according to experimental studies application of latent space gives better results.<|endoftext|>This paper applies a goodness of fit test (KS test) to the latent space of normalizing flows for purposes of out of distribution detection. To combat high dimensionality and model misspecification, extensions such as random projections and two sample tests are also proposed. Thus, this paper provides value to practitioners, guiding them on how to make these crucial implementation choices. The paper is fairly well executed; however, I can t say that I learned much from the work as its contribution is mostly in the implementation details. And it is unclear to what extent these details generalize beyond standard image benchmarks.<|endoftext|>Experimental performance decreases  when the method is used in combination with undertrained normalizing flow or a normalizing flow with a larger capacity. The proposed approach is interesting, simple, mathematically sound and effective. I do not expect this to be competitive, but averaging may improve over the pointwise result. Include algorithmic presentation of GOD2KS in the supplement (for completeness). This paper provides interesting insights about normalizing flows and OOD detection.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; $y_d$ does not appear in the first equation? It seems that those functions are only queried once for "unlabeled training set", but doesn t it make it just a multi labeled training set? So it is easy to forget their meanings and their value ranges. ## Post rebuttalI ve read other reviews, the author s responses, and the updated paper. The presentation of the paper has been greatly improved.<|endoftext|>* The notations are a little complicated; it would be great if the authors could mention that there is a glossary in the Appendix A in the beginning. Weakness:I don t see major weaknesses; here are some suggestions and questions.<|endoftext|>Overall, the paper is well written and easy to follow. Weaknesses:I can t see any major issues with this work. in section 5.1 "... The work is novel and technically sound.<|endoftext|>I wanted to ask the authors to clarify and discuss on this topic in the main text. I also agree on other reviewer s points for the contributions of the paper and increased my score.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; The authors describe a graph neural network model to predict severity of depression from available text.Application of this method to text data itself is not unique but the clinical context is relevant and important.Many a times in clinical scenarios we do not have access to audio video data and that s where methods such as these are important utilizing transcribed text.Comparison with prior models is relevant and important and the improvement somewhat significant. Strengths:  Interesting application of GNN in a clinical field where there is a great need of such applications.Clear clinical benefit. Shows superior performance compared to prior methods with limited text data only. Unclear why the performance is better compared to multimodal data approaches.Details of the comparison might be worth investigating. This paper has a very practical clinical context and if generalizable has good opportunity to for application.It is unclear why the performance is superior to other multimodal methods which should be investigated more with details of performance variability.Overall good work.<|endoftext|>This paper proposes a schema based GNN method to measure the severity of depression. To gain a global representation of each word, the proposed method constructs word nodes and uses schema structure to capture the context level information. The main contribution of this paper is the introduction of the schema encoder. + The results indicate the effectiveness of the proposed method.<|endoftext|>This paper tries to leverage graph neural networks to improve the performance of measuring the severity of depressive symptoms. Experiments are conducted on a benchmark dataset. Strengths: The motivation of the paper is clear. And the overall framework is introduced well. The paper is easy to read. The novelty of the method is limited. The experimental analysis is insufficient. Computer Methods and Programs in Biomedicine, 2021: 106433. The comparisons with recent works are also not involved.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; The comparison on oYAGO 3 10 (117k) was not achieved due to memory issues. Overall this work represents a promising direction with some promising results. However, the proposed approach seems to be flawed and shows degraded quality in some of the situations. It is probably better to have a learning based approach for anchor selection. Minor issues:Eq 1 and Eq 2: why are they called "hashes"? these representations are just learnable embeddings right? I am not sure that it is possible to have injective functions  for R(k+m)×d → Rd. After author response: I am satisfied with the answers to all my questions.<|endoftext|>This paper presents NodePiece, a method inspired by subword embeddings in NLP that is designed for constructing compositional representations of entities in a knowledge graph using a fixed size entity and relation vocabulary. This allows for learning and storing entity representations with a number of parameters that does not scale with the size of the knowledge graph, as well as being able to construct representations for unseen entities. Overall, the paper is clearly written and the NodePiece approach is well motivated. The diverse range of experiments is a strong point of the work, as they demonstrate the usefulness of NodePiece for a range of important tasks including link prediction, relation prediction, and node classification, including in the inductive setting. The ablation analyses performed in each setting could also have been limited to a few relevant tasks, in order to be able to include results that explore other design choices such as how sampling strategies for anchor nodes and outgoing relations affect downstream task performance (some of this analysis seems to have been moved to the appendix). Though the experiments could include more baselines and further analysis of how design decisions for NodePiece affect task performance, the current set of results seems to sufficiently demonstrate the utility and versatility of the method.<|endoftext|>Conventional methods for learning knowledge graph embeddings often learn separate embeddings for each vertex in a knowledge graph. The paper repeatedly claims the uniqueness of the hashes: based on my reading of the paper none of the hashing strategies considered in the paper can guarantee that two distinct nodes will always have distinct hashes. However the paper claims that the hashes are unique in multiple paragraphs. It is useful to maintain the distinction between linear with a small constant factor and sub linear growth and I don t see compelling evidence for the latter. The paper presents a novel approach for embedding large knowledge graphs using a small number of parameters. The empirical results strongly support the usefulness of the proposed approach and I believe that even though this won t be the final word on creating compositional representations of KG vertices, but it will be appreciated by ICLR readers.<|endoftext|>This paper presents NodePiece, a method to scale up GNNs by means of removing their dependence on individual node embeddings which grow linearly in size and are inefficient in huge graphs. Apart from induced efficiency, NodePiece can be especially useful in inductive learning where an unseen node’s embedding can be created through the anchor nodes. Weakness:1) The paper has only focused on graphs with multi type relations (knowledge graphs). It would be great to see if such heuristic apply to single type datasets and whether it could help methods such as transductive unsupervised embedding learners (e.g., DeepWalk). The fact that anchor nodes are not useful in these tasks is probably an indicator of the triviality of the tasks. – Based on this, I doubt if NodePiece will do well on single type relation graphs (first weakness). 3) The paper emphasizes that the goal of NodePiece is not to improve performance of the baselines but rather highlight efficiency in memory and maintaining a reasonable accuracy. Analogously, what is special about WikiKG dataset that leads to such good result? It is mentioned that it is due to maintaining reachability and the balancing of in and out degrees.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 3; This paper proposed a variational based model VLOG for solving the oracle guiding RL problem. I did not capture major bugs in this paper. The proposed method is intuitive and effective, as an application of variational models. I think the key difference is the training observations (or oracle observations) contain a richer signal than the observations in the testing environment (or executor observation), but there could be more. Where are the new contributions? This paper tackles an intriguing oracle guiding problem.<|endoftext|>This paper presents an approach to dealing with problems where agents could benefit from learning from state features that are only available during training, but not during evaluation. In particular, I like the simplicity of the approach. Is this really how Mahjong is played/evaluated? Questions for the authors:1. 1.What is "suphx style oracle guiding"? The abstract starts with "How to make intelligent decisions is a central problem in machine learning and cognitive science". 1.In the abstract should say "RL using variational method***s***."<|endoftext|>This scenario is the relaxation of traditional POMDP by allowing additional observations to be available during training time. A variational Bayes approach is proposed under this scenario. Weakness of the paper:  In fact, the studied problem is not absolutely new. Why and when the proposed approach would indeed work? The paper just says that it could be calculated by any policy evaluation method. This is not assumed in the paper. https://arxiv.org/pdf/1912.12294.pdfTo summarize, I think the paper proposes a reasonable approach for RL with training time privileged information.<|endoftext|>The proposed method aims to derive a representative latent state via incorporating a variational inference perspective. $\textbf{Strength}$:  This paper is well written and easy to follow. Leveraging Bayesian theory to tackle the representation learning problem in reinforcement learning is an important research direction. $\textbf{Limitations}$:  The proposed method is not very general as it could only be used in the tasks where oracle observations are available. 4.The authors employ a very simple RL algorithm to deal with very simple tasks. I believe such setting is not very reasonable. But the applicability of the method is rather limited as it  poses a very strong assumption that oracle observations need to be accessed for training the policy.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; The authors propose a Hierarchical framework for multimodal learning HMVAE. They represent modality specific variations using latent variables dependent on the shared top level variable. They parameterize the posterior distribution over the shared latent variable using a mixture of experts. The modality specific latent factors are adaptive inferred with both bottom up and top down information. However, there are a few concerns about the paper, the approach proposed, and the evaluation:* On page 3, authors clarify the differences between MHAVE and HMAVE(the proposed model). However, MHVAE is also a hierarchical model, and thus, Fig.17a seems not a proper graph for MHVAE. Though section 4.4 shows comparison to SOTA methods that have modality specific latent factors, the authors use MVAE and MMVAE in section 4.2 and 4.3, both of which have only the shared latent space without the modality specific space. I find this unfair. This needs clarification. Two primary element should be resolved to provide convincing arguments about the utility of the proposed approach.<|endoftext|>This paper proposes a new type of model called a hierarchical multimodal VAE (HMVAE) that captures modality specific variations using latent variables dependent on a shared top level variable, in a manner similar to a multi layer hierarchy. The model hinges on the motivation that  modality specific variations can sometimes depend on the structure shared across modalities . 2.The results are better but the improvement could be confounded by the increased number of parameters (I believe the hierarchical model does have more params + potentially more flexibility in modeling the multimodal data). 4.Would also be good to have some human evaluation results for text to image generation (figure 6). While I believe that there is merit to their approach, there are still many possible confounding factors in my opinion, especially regarding the issue of the number of parameters. 6.I still have concerns over the novelty of the approach since the main contribution is to define a hierarchy of latent variables. It is not clear when and why this works (related to weakness point 1) so the paper can be improved if there were deeper insights in this part.<|endoftext|>The authors propose a hierarchical multimodal VAE to capture the heterogeneity through latent variables dependent on a shared top level variable. CUB and Oxford flower datasets were used for performance evaluation. It is a very interesting paper. The hierarchical multimodal VAE can capture the realistic variations and help the decoder to share features between different modalities. The two experiments used L(m)   2, will that be better to set it as a tuned parameter? 2.The evaluation metrics used in the paper (Fig 4, 5, 6) seem difficult to conclude which method performs better. Might be an application limitation for other multimodal data. But, I do have concerns about this method’s application in real situations of multimodal learning.<|endoftext|>The authors proposed a hierarchical multimodal VAE (HMVAE) that represents modality specific variations using latent variables dependent on a shared top level variable. They demonstrate that the proposed approach can represent multimodal heterogeneity and outperform existing methods in sample generation quality and quantitative measures. Weaknesses* evaluation is restricted to dense modalities such as images and text. * The paper can be improved if the author can provide further insight into why defining a hierarchy of latent variables provides a better result. It would be helpful if they demonstrate this by showing the representation of modality specific variations with quantification. * It would be helpful if the author can add a heterogeneity metric as their goal was identifying the representation of heterogeneity within modalities. The authors proposed an HMVAE where unimodal latent variables depend on a shared latent variable. They demonstrated that the model improves generative modeling performance on multimodal data but is restricted to dense modalities such as images and text.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; The authors propose Eigencurve, a new approach to learning rate scheduling that utilizes information form the eigenvalues of the Hessian. They show that this scheduler obtains the minimax optimal rate on the noisy quadratic problem. Empirically, this scheduler demonstrates faster convergence on CIFAR 10 and ImageNet, especially when the number of epochs is small.<|endoftext|>The paper studies convergence rates of SGD with different stepsize schemes, in the context of linear regression. For convergence of the last iterate of SGD, the best known result still misses a $\\log T$ factor compared to the minimax rate. This work aims to fill this gap with an improved stepsize scheme that utilizes the eigenvalue distribution of the Hessian. When the true Hessian is known, the proposed method successfully fills the gap, provably and in the sense of the worst problem instances.<|endoftext|>To this end, this work proposes a new learning rate schedule (named Eigencurve) based on the Hessian spectrum and provides an optimal last iterate convergence rate. Overall, the paper is easy to follow and well motivated. The paper proposed an interesting learning rate schedule that achieves an optimal asymptotic rate on strongly convex quadratics. Weaknesses:  In terms of optimal learning rate schedule for convex quadratics, it was previously discussed in [1] with a noisy quadratic model.<|endoftext|>It may converge to a saddle point, resulting in the negative eigenvalues. The approach, dubbed Eigencurve, is shown to achieve the minimax optimal convergence rate for stochastic gradient descent on quadratic functions under an additional condition that the eigenspectrum of the Hessian decays according to a power law. The submission concludes with an empirical investigation of Eigencurve for (non convex) optimization of several popular neural network architectures on the CIFAR 10 and ImageNet datasets. Eigencurve does attain the idea minimax complexity for SGD on convex quadratics when the "power power law" condition holds. However, it is not clear that this means Eigencurve is optimal in a rigorous sense.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; The authors propose an algorithm inspired by the perturbed reward approaches of Kveton et al (2019a,b,2020) which ensures an exploration exploitation balance by adding additional noise to the observed rewards. This approach avoids the inversion of large covariance matrices which existing approaches (NeuralUCB and NeuralTS) necessitate, and results in a computational speed up while maintaining near optimal regret guarantees. PROS:•	The paper is very nicely written. All the regret plots look linear but with differing gradients in fig 1. MINOR COMMENTS ON CONS•	The definition of $f(x,\theta)$ may be better in an equation display as the page break in the middle of it is unfortunate. I’m very positive about the idea behind this paper: to use combine perturbed rewards with neural contextual bandits and think that this work is timely and potentially very impactful.<|endoftext|>The paper studies neural contextual bandits in the realizable setting. This is a big advantage over NeuralTS and NeuralUCB. 2.The paper uses the same idea as [Kveton et al 2020] however it rigorously proves regret guarantees for the neural network function class under the NTK setting. 1.My biggest concern is that the paper fails to discus and compare with the line of work in FALCON (https://arxiv.org/pdf/2003.12699.pdf). Both the papers lie in the realizable setting. Besides the paper should also compare with this algorithm in the experiments as it is easy to implement. 5.In the synthetic experiments in Figure 1 it does seem that all algorithms kind of have a linear regret scaling.<|endoftext|>This paper investigates neural contextual bandit problem. A new, computationally tractable and efficient algorithm (Neural bandit with perturbed reward, NPR) is proposed and proved to suffer a nearly optimal minimax regret upper bound. Weaknesses* NPR is technically sound but hard to be applied to practical applications due to that 1) During each time step, the algorithm has to use all of the previous rewards. If I read the paper correctly, it seems the key lemma (Lemma 4.1) is from Jacot et al., 2018. * The experiment setup seems to be different from the one discussed in this paper. However the proposed algorithm is still hard to be applied to practical applications. After Lemma 4.3: ... the the neural network ...  the extra the should be removedLemma 4.4: it might be better if the authors could define $E_{t, 3}$ in the beginning of Lemma 4.4.<|endoftext|>This paper addresses the reduction of computational cost of the neural contextual bandit problem which uses the deep neural networks to model the reward function. However, using the reward perturbation has been exploited in previous bandit models as in generalized linear bandits. This is also mentioned by the authors in the related works. In an over parameterization regime, the neural network behaves as the linear model. So I do not see technical challenges here because the main challenges of the neural bandits have been solved in both NeuralUCB and NeuralTS. The used techniques of this paper is an adaptation of  NeuralUCB, NeuralTS to the situation with perturbed rewards. This is a good paper. However, I think that the novelty is not enough.
Reject; rating score: 1; rating score: 3; rating score: 3; rating score: 5; rating score: 6; This paper claims to contribute a novel generalization bound for deep neural networks based on treating the network as a polynomial (for polynomial activation function) with degree bounded by O(depth). This bound is experimentally compared against the norm based bound from the prior work [NBS18]. The statements and proofs brush many details under the rug and hence are very difficult to verify.<|endoftext|>The paper uses tools from geometric function analysis to derive bounds for the covering number of neural networks, and derive generalization bounds using the covering numbers. The idea of using geometric function analysis for deep learning theory seems interesting and I encourage the authors to pursue this more, but the paper falls short of the bar.<|endoftext|>This paper aims at proving generalization bound via geometric functional analysis. Therefore, I tend to give a "reject" and I hope the authors can provide more explanations for the theorems and carefully polish the writing in the next version. And each subsection in section3 is too short. More explanations on the theorem and the proof procedure may help reshape the paper.<|endoftext|>The authors study the generalization bound of neural networks. Under this assumption, they prove a novel generalization bound using geometric functional analysis. This paper is clearly written and well organized. I find it easy to follow. Hope that the authors make more discussions on this issue.<|endoftext|>The paper presents a new generalization bound for neural networks, which is shown to be tighter than the bound in [Neyshabur 2018]. * More background literature on the generalization bounds of neural nets can be included.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; They use their result to motivate an initialization scheme for DEQs that matches the solution of a trained kGLM. (+) The authors provide a useful and simple framework to study and bridge the connection between iterative root finding models, such as the DEQ, and solving optimization problems in the forward pass. (+) The initialization scheme, which is motivated by the result, indeed seems to work well. If the DDN and DEQ are equivalent (as claimed), then we may expect to see similar performance here (and if not, there should be some further analysis/emphasis of the limitations of the theorem). The paper is well written, and provides a useful result that gives insight into a rich class of models.<|endoftext|>The paper starts with a specific set of DDNs with a kernelized generalize linear model and shows that some DEMs are equivalent to the DDNs. The paper studies the very interesting problem of connecting DEMs with existing models, namely DDNs in this paper. I have seen another discussion on existance of solution in (El Ghaoui, 2019) for DEMs (refered as well posedness for implicit model in the paper) where the condition can be further relaxed to something not necessarily contractive. I think this is worth mentioning but should not undermine the contribution of this paper.<|endoftext|>The discussions are thorough and establish a clear optimization based interpretation of the DEQ models. Weaknesses (I ll expand on this part a bit so that the authors can address the issues):1. This is good, but generally impractical for DEQ models outside the scope of this paper. As mentioned in the main review, I think this paper provides a good and solid theoretic perspective to understanding the deep equilibrium networks, which is a form of implicit network that has drawn growing attention these days. While the model is simple and constrained, the idea that the plausibility of DEQ models may be provided by some underlying optimization problems is an interesting and profound property. Despite the limited scope and practicality, I believe this paper is still a good theoretical addition to the current set of works on implicit modeling (and how to make sense of it). [1] https://arxiv.org/pdf/2008.02217.pdf<|endoftext|>This paper describes a particular DEQ formulation that is motivated by a kGLM based deep declarative network (DDN). Cons:    While satisfying nice properties, it s not clear how expressive the kGLM based DEQ is. Empirically, how many iterations of training would be equivalent to the time used for solving for this initialization? Is it assuming that all data points are the same value? Perhaps writing out the expression for the non data dependent W s and V s may be helpful here. (i) Does this restricted architecture negatively impact performance? The paper mentions that it does not contain "SOTA" experimental results because DEQ has shown good performance already, but there seems to be a disconnect between the DEQ architecture that shows good performance and a kGLM based DEQ. From what I understood, this section is defining two sets of data (clean vs noisy, train vs test?), a network F_gamma, and the loss function.
Reject; rating score: 6; rating score: 6; rating score: 8; rating score: 8; This paper studies differentially private algorithms in federated learning, and proposes to take advantage of the randomness in Relative Entropy Coding (REC) to achieve good privacy utility trade offs while significantly saving the communication costs. IIUC, secure aggregation is future work in this draft. It is not very convincing. That being said, The idea seems to be interesting, and I will raise the score from borderline reject to borderline accept. And it seems that aggressive compression will cause more accuracy drop. Could the authors provide the clip norm and noise for both DP FedAvg and DP REC with some explanation? Appendix C.2 provides some, but I would appreciate an “apple to apple” comparison and some explanation on why from a first glance, the noise in DP REC can be much smaller.<|endoftext|>This paper proposes a compression scheme for federated learning built upon previous relative entropy encoding works. It then proves that with some small modification (clipping the model updates), the algorithm is inherently differentially private. Empirical evaluation shows that the proposed method can achieve much more communication reduction at the cost of accuracy degradation. It describes the relations with previous works and the experimental setups reasonably well. The goal of DDGauss is not compression (though it results in quantized model updates). It would be more clear to use different notations for random variables and samples. That will demonstrate how good the (biased) compression itself can be. * What does  tensor (generally understood as multi dimensional arrays) mean in the context of this work? I think the most interesting part of this paper is that it leverages the randomness in the sampling process to achieve differential privacy.<|endoftext|>This paper introduces a compression and privatization technique to federated learning based on Relative Entropy Coding (REC). However, instead of directly adding Gaussian noise, the client first picks $K$ random vectors from a prior distribution (which is independent of $w_s$) with shared randomness and then performs importance sampling according to the law of $w_s + N(0, \sigma I)$. By accounting the privacy loss over $T$ rounds, the authors characterize the overall privacy guarantees, which have a clean form and connect to the communication budgets nicely. However, I am not very sure about this statement. Similarly, I don t see how $\pi_k$ in Algorithm 3 can be decomposed into a product form. Since the main theorem is based on equation (4) and all experiments are carried out with per tensor compression, I think it is important to give a formal statement and proof to show that the results also hold for per parameter compression. The paper is well written and the proposed idea is novel. However, my major concern is the correctness of the per parameter compression.<|endoftext|>The paper proposes a differentially private and communication efficient method to aggregate the client updates in federated learning. The authors further modify this technique to satisfy differential privacy guarantees and perform various experiments to back their claims. 2) The paper introduces plenty of new ideas to the federated learning community, both in the main algorithm and its analysis. 3) Experiments validate that the proposed algorithm provides very high compression in the high privacy regime ($\varepsilon < 1$). **Weaknesses**1) Computational complexity: The algorithm in its general form requires $b$ samples to be drawn from a high dimensional distribution. 2) Given that Chen 2020 is known to be optimal under joint privacy and communication constraints, a *theoretical* comparison with results in Chen would have to theory of this paper. This could have been done for both *good* and *bad* prior. Overall, I like this paper and recommend it be accepted.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; This paper presents to simulate physics via a constraint based approach instead of direct prediction. This paper is overall well written. 2.Although the novelty of combining GNN with constraint projection is weak (see the weaknesses below), it is valuable to check if this method can outperform those typical forward approaches (such as the work by Sanchez Gonzalez et al., 2020). The experimental evaluations generally serve this purpose. 1.2 In related work, the authors state that the neural projection by Yang et al.(2020) is the first work that uses learned constraints.<|endoftext|>I find the extend to graph neural networks exciting, but in the meantime, incremental. I appreciate the idea of using graph neural networks with a constraint based learning paradigm. Is there a reason why it is not a part of the standard model given it has an advantage? The authors extend it using graph neural networks.<|endoftext|>Therefore, I feel it is a bit too much to claim that the proposed approach is doing constraint based simulation and to claim that the proposed network is a constraint solver. The proposed simulator uses a graph neural network to encode a constraint solver and shows results in a number of simulation environments. The main contribution of the paper is its idea of using graph networks to encode constraint based simulation. This does not seem to be very ideal for a simulator.<|endoftext|>The authors built the framework on top of graph neural networks (GNNs) to capture the compositionality of the underlying system and enforce the constraint using an implicit constraint function optimized via gradient descent. As a result, I don t think the current experiments in the paper are enough to demonstrate the benefit of the constrained optimization process. Without further details, it is hard for me to imagine how they are defined and implemented.
Reject; rating score: 5; rating score: 5; rating score: 6; In this paper, the authors proposed a method to train Spiking Neural Networks (SNN) with spike based implicit differentiation on the equilibrium state. There have been several works on training SNNs with spike based learning rules based on the back propagation or the equilibrium propagation, but most of them failed to provide successful experimental results. The writing can be improved for easier understanding. It is hard to understand basic concepts without reading the reference, so It would be better to explain more details about the background such as the IDE. Even though I have not fully understood the mathematical derivations in the paper, it was not easy to follow mathematical derivations in the paper without referring to other papers. Comments after rebuttal  I apologize for the late response and thank the authors for adding a detailed explanation of the proposed idea. Can the proposed method also be applied to other spiking neuron models? Averaging out spikes with rate coding with IF neuron is functionally identical to quantized non spiking neural networks, in which the activation function has quantized output. However, I doubt that the theoretical improvements of this paper is significant enough especially with my poor understanding on the mathematical derivations in the paper.<|endoftext|>The previous IDE used firing rates rather than spikes for computation, although reference Xiao et al in NeurIPS 2021 had already addressed implementations in spiking neural networks. The authors analyze the approximation error that results from solving implicit differentiation by spikes and report a solution based on ternary spiking neurons, that can be implemented with pairs of standard spiking neurons. It has been demonstrated, e.g.for Intel s Loihi chip in (Davies et al., 2021) that one hardly gets and energy advantage in spike based hardware if one works in a rate based coding regime. A solution to the learning dilemma for recurrent networks of spiking neurons. The authors should summarize and discuss more benefits of the proposed method. It is important for enhancing the impact of this work and drawing more audiences in the neuromorphic community. In section 4.1, please explain why negative information is necessary for implicit differentiation calculation. The paper presents a nice step in an interesting direction. But it does not manage to clarify what exactly its innovations and possible applications are.<|endoftext|>* My comments after the rebuttal: The authors made a very attentive rebuttal and emphasized their contribution as a purely spike based training approach. At the same time, the math details cannot tell the upper bound and the potential of the proposed method, which is my main concern for the series of approaches. 1.I cannot fully track every step in the updating formula to make sure that it is purely spiked based. Yet my two major concerns still remain here. As the authors mentioned, the energy cost is related to both the latency and spiking rate. Based on these two points, I tend to keep my rating at 5 but am open to marginally accepting this paper if the other reviewers strongly think it should be accepted. Overall, this paper is clearly written and easy to follow, but the methods may not apply to the more complex scene with less latency. It would be also great if the authors can additionally add the results on the spiking dataset like CIFAR gesture and CIFAR10 DVS as well. Can the author explain the reason? Deep residual learning in spiking neural networks.
Reject; rating score: 6; rating score: 6; rating score: 6; The proposal to use zero order optimization methods is novel and interesting. The paper is well written. 3.I think the stability of a NAS method can be claimed only when the method is able to produce similar searc results stably. Can the authors prove that multiple implementations of the proposed search method would lead to the same architecture or similar architectures with close performances? 4.The search results are not significantly better than some baselines. as reported in [1].<|endoftext|>Then the work proposes three zero order optimization methods to solve the issue. The instability of DARTS has always been an topic in the NAS area, and has drawn a lot of attention. More experiments may strengthen the results. In ZARTS GLD, it seems that the network weight $w^{*}$ is estimated by the current weight $w$ and the method searches within a small area around current architecture parameter $\alpha$. The paper is well organized, with sufficient analysis and sound technical contribution. I think it is a good work.<|endoftext|>The search costs of ZARTS are not appealing comparing to other differentiable NAS methods like P DARTS or FairDARTS (Table 4). Overall, it is a good paper that opens a new research direction of differentiable NAS family. ++ The paper is the first to apply zero order optimization methods to solve NAS, which could open a new research direction of NAS. The quality in terms of visualization, theoretical proof and empirical evaluation are also good.
Accept (Oral); rating score: 8; rating score: 8; rating score: 8; rating score: 8; In this paper, the authors study the training of two layer ReLU networks with weight decay. Questions for authors:One of the technical themes of the paper is the idea of minimal / nearly minimal networks. Several of the theorems are stated explicitly in terms of nearly minimal networks. A previous paper (Pilanci and Ergen 2020) introduced a convex optimization problem that corresponds to this non convex case.<|endoftext|>"Convex geometry of two layer ReLU networks", 2021. It introduces a number of new notions such as (nearly) minimal neural networks, developing a set of interesting tools, and draws connections between the minimal neural networks and the convex optimization landscape. The paper is mostly well written and organized, and technically precise.<|endoftext|>This paper establishes some very interesting connections between the global optimizer of the two layer ReLU neural networks with the solution of a convex optimization program with cone constraints. This paper is strong enough and I do not see much weaknesses. I recommend accepting the paper.<|endoftext|>The paper explores the landscape of the objective function in training a single hidden layer neural network with ReLU activation and L2 regularization. Impressively, the paper has the following contributions:1. 3.It defines a subclass of single hidden layer neural networks which it terms "nearly minimal".
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 8; In particular, their focus is on how  effective robustness  changes during fine tuning and on the characteristics of these models. To change my mind on this point would require additional discussion by the authors to connect this work to general principles of machine learning and establish the novelty of the insights reached from these numerical experiments. That said, my many years in research have taught me that sometimes results that seem to me to be  remarkably obvious  are actually not so for the general audience, and that  simple  examples demonstrating such principles can actually have a large impact and generate huge citation indices. I mean this genuinely; not trying to be cynical here. So for that reason I would respect the decision of other reviewers and the AEs if this paper was in fact recommended for the conference series.<|endoftext|>This is not well justified in the current version of the paper. Also, the reasons for the peaks in ER during training are not justified, why are they intriguing? The random initializations don’t fluctuate that much, why not investigate these observations in detail? This paper relies heavily on Taori et al.(2020), which seem to have a number of unresolved concerns, most important of all is that the paper is a bit short on novelty, however, the empirical study in itself is interesting. Overall, the paper has breadth in the number of experiments and the directions that it explores without enough depth and justifications to a majority of findings. I would give it a score of 4.<|endoftext|>Also, they don t provide analysis for the observations and the observations may not be helpful in understanding the problem. Besides, they discuss several potential solutions to mitigate the problem of vanishing of effective robustness during fine tuning, but find that none of them are able to maintain high effective robustness at high in distribution accuracy. I still think that the models studied in this paper are not enough to represent all models that exhibit ER. Also, when the fine tuning converges, the models have high accuracy on the in distribution dataset but don t have effective robustness. So such a discussion may not be useful. It is unclear whether such a phenomenon is general or it just exists on some datasets. Also, it seems these results could not demonstrate that the models with effective robustness have prediction diversity. For example, the observation that the effective robustness increases with the larger size and more diversity of the dataset seems obvious. I think this paper doesn t make enough contributions and the claims are not well supported by results.<|endoftext|>The authors also present a negative result showing that several reasonable approaches to maintaining high ER while fine tuning fail. Weaknesses:One weakness of this paper is that the authors do not properly define fine tuning. It would help if the authors give some examples of when such a training procedure would be useful. Usually fine tuning is carried out on the distribution that the model is going to be evaluated on. An analysis of the relation between the fine tuning dataset and the OOD test set would be useful. There are many experiments in the main paper as well as in the appendix that validate their claim.
Accept (Poster); rating score: 6; rating score: 6; rating score: 5; rating score: 5; The paper proposed a provable multi stage algorithm to match the lower bound established by Woodworth for intermediate heterogeneity levels in federated optimization. In this case, the two stage algorithm is reduced to the last stage which is the accelerated SGD. In particular, they first run a local method up to reach certain error floor and then switch to a centralized method for the rest steps. The theoretical result is new in FL. In the proof of Theorem 1, the $n$ in the expression of $\Phi$ and $\eta$ of the first stage should be $N$. This is equivalent to let the first stage be a pure local training rather than local SGD. 3.The paper only analyzes the strongly convex case and omit the generally convex case. Considering the analysis method used in the paper is just to combine convergence rates of outputs from two (super)stages, I think it is not hard to extend the result to the generally convex case. I have read the authors’ responses as well as other reviewers’ feedback. At the moment, I think the proposed two stage method is more like a trick rather than a universal method that can be implemented to other scenarios.<|endoftext|>The paper proposes "multi stage FL algorithms" to bridge the gap between local step methods (that are theoretically known to be optimal for a no heterogeneity regime where $\zeta 0$) and SGD type methods that are known to be good for high heterogeneity regimes. Minor comments:  the authors mention that FedAvg s lower bound can only result in lower communication complexity than AC SA when $\zeta_*^2\leq \mu \epsilon$. After reading other reviews and the authors  responses    I agree with other reviewers that the novelty of the work is limited, especially given the fact that it applies to limited scenarios and does not even consider the general convex setting (which in my opinion would be more informative as the local updates can converge to different solutions of the same problem). The writing of the paper is misleading about the methods. This limits the importance of the achieved results (basically just changing the initialization for the centralized approach based on a local method could result in an optimal rate). I do not think that this is translatable to more complicated cases (other than strongly convex cases), but the authors have not empirically or theoretically studied the limitations of such an approach. Based on the above comments, I believe the paper lacks in the areas of contribution, applicability, significance, practicality, experiments, and writing. I believe this makes the algorithm impractical. The only reasonable justification for that is assuming $K \infty$ or very large, which is not necessarily the case and limits the significance of the results in practice.<|endoftext|>However, I am looking forward to reading the authors  rebuttal. On the other hand, when the level of heterogeneity is very low, then FedAvg/LocalSGD outperforms the former in terms of communication complexity and needs only a few communication rounds given enough local computation. At the end of page 4, it is mentioned that $\zeta_*^2$ is scaled by exponentially decaying term in the lower bound (6), while $\zeta^2$ is scaled by $\frac{1}{R^2}$ in the rate of FedAvg (6). **Correctness and optimality of the rates. Let me provide some details for the proof of FedAvg$\to$ASG. **Technical novelty**Most of the analyses heavily use the results of previous works. **Experiments**Experiments do not seem to be aligned with the given theory. In the convex experiment in Figure 2, the best performing method is SCAFFOLD$\to$SGD, which (i) has an inferior rate than, say SCAFFOLD$\to$ASG, and (ii) as far as I get, the presented theory for SCAFFOLD$\to$SGD is the same as FedAvg$\to$SGD, which is not the case for SCAFFOLD versus FedAvg. It is mentioned that the good performance of SCAFFOLD$\to$SGD can be attributed to small $K$ in the experiment. Multi staging does improve over the two stage method, and in the second plot, FedAvg almost reaches to FedAvg$\to$SGD. The paper provides a clear intuition on why multistage algorithms are useful in practice.<|endoftext|>This paper introduces the multistage optimization technique for federated learning applications. Because centralized methods are optimal when data are heterogeneous and local methods are optimal when data are purely homogeneous, using a multistage optimization technique can incorporate the benefits from both sides. The theoretical part is relatively easy. I think that adding the analysis (just using different $\eta_g$ and $\eta_l$ in different stages, but not choosing $\eta_l   0$ in the second stage) for the nonconvex case will make the results more interesting. 3.This paper only analyzes the strongly convex case. The empirical part includes two experiments: logistic regression and neural network, which belongs to strongly convex case and nonconvex case respectively. For each experiment, the authors compare different minibatch algorithms like SGD, AGD, different local methods like FedAvg, SCAFFOLD, and some multistage procedures that combine local methods with minibatch methods. For the convex setting, multistage procedures perform the best, and for the nonconvex setting, multistage algorithms also perform generally the best. In summary, I think this paper needs to be revised because of the following reasons:1. The algorithm and the analysis seem a little incremental. The algorithm seems like a direct combination of different algorithms and the proof is also very straightforward: directly applying and combining the convergence analysis from the algorithm algorithms. The results are not discussed in Table 1. However, in my understanding, the algorithm also needs to know the heterogeneity parameter $\xi^2$ in advance in order to choose the appropriate local steps $K$ in the first stage. 4.For the experiment hyperparameter selection, I think that the comparison is not very fair. Based on the previous concerns, I have the following suggestions and questions:1.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; In particular:(a) Are there (combinatorial) problems where the tree would need to be significantly large to ensure high quality points are actually found? The idea is to learn a proxy of the distance of samples to the Pareto frontier, and leverage such information to split the search space via a tree structure, Samples can then be extracted from promising nodes of the tree from other multiobjective algorithms. However, here authors use the dominance metric to partition the primal space, which leads to interesting properties and a tree encoding. It seems that it is highly optimized for the numerical experiments that were performed in the paperA well written paper that provides an interesting and relevant contribution to multiobjective optimization.<|endoftext|>This paper proposes to use MCTS to enhance existing algorithms for Multi Objective Optimization problems. Experimental results on a wide range of benchmark problems show the improving performance of the proposed method, LaMOO. The proposed method uses an interesting combination of algorithms. The results are compelling. The paper is well written. I think the related work section should mention these. The idea is simple, well explained, and the results are promising. This paper should be valuable for many applications.<|endoftext|>This paper presents a learning space partitions based multi objective optimization framework by using Monte Carlo tree search and an innovatively proposed metric, i.e., dominance number. This paper seems to be the first work by extending the space partition approach for SOO to MOO. The metric of dominance number is reasonable and easy to follow. Sufficient theoretical analysis and experimental demonstration are provided. However, this paper still suffers from the issue of insufficient presentation. Some revisions are need to further improve the quality of the paper, listed as:1. But it is unclear how to set it in the experiments. More interpretations are needed.<|endoftext|>**Strengths:**+ This work is well organized and generally well written. Pareto rank learning in multi objective evolutionary algorithms. **2.Theoretical Analysis:**A large part of this work is on the theoretical understanding for space partition and LaMCTS. **3.Why LaMOO Works:** Further discussions are needed to clearly clarify the properties of LaMOO. This work tackles an important research problem that could be useful for different partitioners in the community (e.g., multi objective optimization, Bayesian optimization, and NAS). The proposed method is a reasonable generalization from the LaMCTS method. **4.Time Complexity:**What is the time complexity of the proposed algorithm?
Reject; rating score: 3; rating score: 3; rating score: 5; The paper proposes a method to sample from conditional distributions using a pre trained flow based method for applications in problems that have missing features. The proposed method utilizes a lower bound on the log probability of the conditional distribution (conditioned on observed features) using the Schur complement. The paper is relatively well written although I believe that parts of writing in the paper can be made more precise that will help the overall quality of the paper. The problem formulation is clear and the proposed solution is interesting as well as explored in detail. Precisely, since the experimental results obtained for methods like PL MCMC, MFlow use a different flow architecture than for the results for VISCOS flows, I believe the experimental comparisons are not fair and sufficient enough to draw any meaningful conclusions.<|endoftext|>This paper proposes a technique for performing data imputation using pre trained normalizing flows. They do so by fitting a variational distribution over a subset of the  base  variables of the normalizing flow. Along with the observations, a sample from this variational distribution is sufficient to fully specify a sample from the normalizing flow model. The authors present techniques for constructing samples in this way, and computing the derivatives required to optimise the variational distribution.<|endoftext|>1.This paper provides an interesting utilization of the Schur complement to model the dimensions of observed and unobserved features. This flow model is applied to the in painting task, which estimates the unobserved feature variables given the observed features. I wonder whether these metrics could be used or not. This paper presents a variant of flow, VISCOS flow, utilizing the Schur complement.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; I recommend rejecting this paper because of its unclear method description, poor results, and insufficient evaluations. The authors use a Transformer with a copy mechanism and an offset vector to produce a bag of vectors.<|endoftext|>This paper extends previous work, Emb2Emb, by replacing the LSTM autoencoder with a Transformer autoencoder. The authors propose some techniques to handle the difficulty of this replacement. Here are some suggestions and questions:  I think the authors should describe more about the autoencoder. If it s the latter case, it is weird to view the embedded as a bag of vectors since the order indeed matters a lot.<|endoftext|>The paper extends the unsupervised conditional text generation framework of Emb2Emb (Mai et al., 2020) from a single encoding to variable length encodings. The new model is called a bag of vectors autoencoders (BoV AE).<|endoftext|>This paper presents an autoencoding model which is trained for conditional text generation tasks in an unsupervised manner. This bag of vectors can then be transformed (in embedding space) in order to generate a modified sequence of text based on the original input sequence. The empirical results on style transfer and summarization appear to be strong (although as mentioned below, Figures 3 and 5 are unclear).
Reject; rating score: 3; rating score: 6; rating score: 6; rating score: 8; This paper investigates the problem of fairness learning on graphs. Would they involve any discrimination in the articles? However, here the category of articles is not an inherent attribute but is summarized by humans. Several main concerns should be well addressed by the authors. It seems that the authors find that "augmentation has not been employed for fairness learning on graph, so we do". 8.In experiments, most of the fairness learning approaches are not employed as the baselines for comparison, such as the related studies in section 2, as well as [1,2]. Furthermore, the authors do not provide the drawbacks of these related studies compared to the proposed model. However, this equation is a bit confusing. So why this kind of contrastive learning, though with the proposed augmentations, can address the fairness issue? Sensitive attributes are the inherent attributes of instances, and can involve some kind of discrimination.<|endoftext|>This paper proposes a set of data augmentation approach to improve the fairness of GNNs. Then it proposes three data augmentation tricks to respectively reduce three terms of the upper bound. The proposed method is evaluated on several benchmark datasets to show the effectiveness. Strengths:(1) This paper studies an interesting and important problem of GNN fairness. The proposed method still seems somewhat ad hoc but overall this work would be an interesting addition to the GNN fairness literature. However, the presentation of analysis and motivation is not convincing and can be largely improved. For example, based on the definition of $\gamma_1$, it seems very easy to reduce it to 0 by rewiring the graph. What are the potential trade offs for not fully optimizing those terms? Could you include an ablation study testing methods that directly optimize for these terms, as mentioned in (2).<|endoftext|>This paper studies the bias issue in the representation learned by GNN based models. Some theoretical analysis is presented, and based on this analysis they propose several data augmentation techniques, such as feature masking, edge addition/deletion and node sampling. Also, the sensitivity of the proposed methods w.r.t the hyperparameters should be investigated. How to specify the right amount of augmentation for each type is not well studied. 3.Some case study on the real world dataset or even toy dataset can be helpful for readers to better understand these data augmentation techniques in a more intuitive manner. Overall the proposed technique following the analysis on the theoretical upper bound, is simple and effective in improving fairness metrics.<|endoftext|>Concretely, the paper derives an upper bound on the correlation between sensitive attributes and representations and uses this bound to propose various graph augmentations that increase different group fairness metrics (e.g., statistical parity). **Strengths**  `The problem setting is relevant.` Fair representation learning for tabular data has been studied extensively in the literature. However, fair representation learning has not been considered yet for graphs, which is the problem that this paper investigates. As the paper empirically demonstrates, prior work on graph representation learning typically incurs group fairness violations, which need to be addressed. `The method is novel and well motivated.` The paper theoretically analyses the correlation between sensitive attributes and the graph representation and uses the analysis to propose different graph augmentation methods to reduce (intrinsic) bias of the representations. `The paper is clearly written.` Even as an outsider of the field of graph neural networks I was able to follow the paper as the ideas are well motivated and clearly presented. **Weaknesses**  `The ablation study yields inconclusive results.` Depending on the task, different ablated versions of the proposed method (e.g., without edge deletion) significantly outperform the full method, which is not ideal. The paper conducts an ablation study to compare the performance of different augmentation strategies, but it would be interesting to see if any theoretical conclusions can be made. Do you have any insight on this?
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; The authors create a language game in which one neural agent describes an item to another. The results show an impact of this parameter on the speed at which communication converges and the degree of generalisation of the languageI don t understand the purpose of the paper. It seems to fit into the emergence of language field but I cannot work out what the authors are saying. Or are they finding evidence that rejects this? The abstract and introduction introduces the general field and makes clear what the experiments involve. This is not a peer reviewed source   we have no way of knowing whether claims in these papers meet normal professional criteria. It is sometimes a little hard to follow. I didn t follow section 3 very well   I think more detail is needed on Chaabouni et al s measure of compositionality, as it is key to the approach and the paper cannot be fully understood without it. The conclusions do not conclude anything.<|endoftext|>This paper proposes to use a well studied compositionality metric as an additional auxiliary loss function in a reconstruction task used to study language emergence. In certain cases, this loss function allows for improved generalization. The paper additionally finds that there is no simple correlation in increased weighting of this loss and increased generalization. **Strengths:**  The paper uses the topological similarity metric [1, 3, 4] commonly used in the emergent communication literature to evaluate compositionality of emergent languages. The key contribution in this paper being the fact that it is used as an additional loss function to explicity tune the pressure for compositionality. **Overall:**      Paper has numerous typos and punctuation mistakes that make the paper a little difficult to follow in some places. The paper also includes some evidence that explicitly optimizing this compositionality metric can improve generalization. Compositionality and generalization in emergent languages. **Language Emergence Game and Setup:**      The experimental setup is not clear. What are some examples from this dataset? Comparisons to the reconstruction task in [2] is made but this does not seem like the same task. As such, I would recommend that the paper is rejected. **Action:** Run experiments with more learning rates and more seeds. **Figure 2:** I don t this that these plot adds much towards the paper.<|endoftext|>Senders are said to sample from somemessage space a certain representation about an object. On the other hand the paper cannot be read and understood standingalone as a self contained piece of work. The entire discussion isheavily dependent on assumedknowledge of the topic, the broader area, and the usual practices ofsuch simulation experiments. It does not try to explain what is thepurpose of training, what is supposed to happen in testing, and whatcomposition and generalization mean in this context. The pairing between the object space and the message space is said tobe done with some matrices but it is not clear how such matrices are constructed. One other issue with the paper is that there are no examples at all throughout. The entire discussion about setup, experiments, results is abstract. The fact that the entire work uses only synthetic objects with noconnection to the real world is also a weakness.<|endoftext|>"However there is more than enough evidence to accept advantage of the compositional pressure on generalization"  > I feel this is overly strong. The paper investigates the use of topological similarity as a loss function to improve the generalization ability of agents, in the context of emergent communication. Thus, I feel it is important to motivate clearly what the goal of the additional loss function is. The paper does give an additional data point showing that the link between compositionality, as measured by topological similarity, and generalization is tenuous. ## NotesI wrote the following notes whilst reading through the paper### Abstractinteresting idea to incorporate metric of compositionality into learning  doesnt this go against the idea of letting languages emerge naturally, and then evaluating them? However, given the clear knowledge of prior work expressed in the introduction, I m unclear why the observation that there is no simple correlation between generalization and compositionality is novel? Measuring the same metric as our output seems somewhat circular, but measuring generalization ability seems a plausibly reasonable thing to do, to me. How do you know that the term is not functioning simply as a regularization term? This would also act as a regularizer, by increasing entropy. Don t your results at M(50,2) align with a hypothesis that the compositionality loss could be acting uniquely as a regularizer, rather than by improving something concrete that correlates with generalization?
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; This paper proposes a method to utilize partially labeled data for the CTC loss. Dynamic time warping (or DP matching) with unconstrained endpoints itself is not a new idea and a classical topic for speech recognition (e.g.word spotting). The contribution of the paper is the formal introduction of the approach to CTC and to give experimental results to confirm the effectiveness. The experiments are fair and the effectiveness of the approach is confirmed using simulated data. It is great that the results are given for three different domains.<|endoftext|>**Weak points**  mostly the idea is based on SPRING (dynamic time warping, DTW) which is mentioned in the paper and applied for CTC instead of DTW. W CTC is empirically proved to significantly improve performance over CTC even if up to 40 70% label sequence is missed (overall performance similar to the complete label case) across different tasks, like speech, optical character, and continuous sign language recognition. The paper is very well written with clear explanation of proposed method, well covered experimental results across domains and tasks.<|endoftext|>However, the contribution of how to use dynamic programming to compute the CTC loss over all sub segments is marginal. The authors propose to minimize the loss over all possible sub segments of the input to automatically align the one that matches the available transcript. The paper is well written overall, easy to understand with clear definitions and examples. The ablation study on the weighting of the losses for different sub segments is interesting. Attention based methods are not addressed at all by the paper anyway. It is not completely clear either whether the wild cards are used at inference as well or not. The problem is interesting and the paper is clear and easy to understand.<|endoftext|>This paper proposes an extension of CTC by considering the wild card to adjust the label missing issues during training, which tends to happen in the onset/offset edges of the utterance. I m expecting that this method may work even if missing labels happen in the middle if we use the self attention based network. In this case, the segmentation time stamp is not accurate and the missing labels or missing audios in the edge often happen. The experimental effectiveness is valid. Also, it was shown in three different tasks. This paper would have a broad impact on the machine learning community to tackle noisy data training based on CTC.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; rating score: 6; SCARF generates an augmentation of a data point by selecting a random subset of features and replace them with their marginal distributions. The paper could be more complete if there was a discussion between two self supervised learning approaches for tabular data: contrastive learning and pre text learning models such as VIME (NeurIPS 2020) or TabNET (AAAI 2021). On 69 datasets from OpenML CC18 benchmarks, SCARF is compared with dozens of baselines and showed its efficacy.<|endoftext|>This paper proposes a data augmentation method (SCARF) used for self supervised learning for tabular data. SCARF generates different views by corrupting a random subset of features (via sampling from the marginal distribution). The authors also conduct ablation studies and compare with various methods. They experimented with 69 datasets and repeated 30 times for each run.<|endoftext|>This work proposes a good method to use SSL methods on tabular datasets. They achieve this by randomly corrupting a subset of the features. It would be nice to have some more analysis on this front.<|endoftext|>Strengths:  Exploring self supervision on novel domains (e.g., tabular data) is valuable  The method for generating ‘augmentations’ for contrastive learning via random replacement from the marginal feature distribution makes sense and is novel to the best of my knowledge  The idea is simple but seems quite effective  The paper is well written, and the method is represented well  Experiments show relative improvements over several baselinesWeaknesses:  All the experiments only show relative comparisons. An obvious baseline on tabular data would be a tree ensemble model (like XGBoost). The paper introduces a simple adaptation of contrastive learning to tabular data by introducing a novel ‘data augmentation’ on tabular data.
Reject; rating score: 5; rating score: 6; rating score: 8; This paper is concerned with understanding when RNNs are useful in POMDPs. They should that, for a class of POMDPS (e.g., meta RL, robust RL), standard RNNs can be competitive with solutions that are tailored to the given POMDP structure. The authors discuss four design considerations that they claim are essential for performance: (1) decoupling actor and critic networks, (2) using off policy instead of on policy algorithms, (3) context length of RNN, and (4) using rewards as historical input. They should on 4 mujoco baselines that their results are competitive with sota methods whenever these design choices are taken into account. This work is quite straightforward and simple   which is good. The authors state that an end to end approach can be competitive with sota methods on tailored problems, if only specific considerations are taken into account. The main drawback of this work is that the results are inconclusive. It is hard for me to assess the correctness of their claims solely from their experiments on 4 mujoco tasks. For the claims of this paper, I would at least want to see extensive experiments on another large domain (e.g., 4 other environments in atari, and perhaps at least 1 hard task in another domain). I assume the authors can find environments and/or algorithms for which their design choices fail. Good direction, results are inconclusive.<|endoftext|>This paper revisits recurrent neural network based model free RL methods with carefully tuning on various aspects of the learning details. Moreover, the perspective viewing meta RL, robust RL, and generalization or transferability in RL as POMDP problem is also very interesting and reasonable. In the experiments, could the authors provide details on the network structures used for different tasks? This would provide a most straightforward view that how much benefit RNN brings. For all these tasks, will stacking RNN layers be better than a single layer? Overall, although this paper focuses on RNN implementations, instead of proposing new algorithms and methods, I think the contexts discussed in the paper can contribute to the practical RL domain. The paper provides many useful implementation details for RNN based RL methods.<|endoftext|>This paper primarily provides an empirical analysis of recurrent model free RL on several classes of POMDP, showing that if parameters are well chosen, the basic approach of just applying a recurrent layer is not only competitive with but often outperforms specially designed methods for those problem classes. It sets out a clear picture of the current POMDP landscape, making clear the distinctions between different classes of problem (such as stationary vs nonstationary, across episode, policy inputs) and the concerns that each raise. This is not because any new methods are proposed, but rather because so little previous work has really considered the question of actually getting recurrent algorithms to work well. In general, these appear to be recent, SOTA, and well chosen, but between that and the use of bar graphs instead of learning curves, it s hard to really sanity check. For the comparisons to other methods, the authors use an off policy algorithm (SAC) with access to information that is not necessarily common in POMDP literature (rewards and done signals aren t often counted in the stored history). 4) I preferred the learning curves found in the appendix to the bar chart comparisons in the main body. I think it would be more informative to compare the methods with some error bars and a better idea of the process. Also, it seemed like some environments (like Cheetah Vel) didn t learn? I also appreciate the clarity with which this paper addresses the classes of POMDP and the considerations required for each. That said, I have some concerns with the thoroughness of the experiments and would like to see more baselines and fairer comparisons.
Reject; rating score: 1; rating score: 3; rating score: 3; rating score: 5; I have read the other reviews and agree that this paper lacks novelty, and that main concepts and methods are already known in prior works. This paper first analyzes the convergence of SGD under assumptions of PL condition on loss function and growth condition on stochastic gradients. The proof of the growth condition of the shallow network is incorrect. >PL conditions are shown on neural networks in Liu et al (2021).<|endoftext|>This paper focuses on improving the condition of overparameterization for stochastic gradient descent on overparameterized shallow neural networks. While the results of this paper look correct and rigorous, I have a major concern about the significance and novelty of the paper.<|endoftext|>Thus, the technical novelty of this paper might be a bit limited. This paper provided a better width requirement bound for linear convergence in two layer neural network training by generalizing techniques from previous results and taking batch size into consideration.<|endoftext|>The main result of this paper is proving the subquadratic width scaling for SGD convergence in two layer neural networks assuming the batch size grows with the number of training samples. The proof relies on assuming that the initial Jacobian matrix is non singular and shows that it stays non singular since SGD iterates remain close to the initialization. Cons:1.I am concerned about the novelty of the proof.
Accept (Spotlight); rating score: 8; rating score: 6; rating score: 6; rating score: 5; The manuscript addresses the question of using deep generative models in lossless compression. First, PCs are faster for compression, achieving results from 5 to 20 times faster than competitor neural networks. A strength of this paper is its clever use of PCs in compression. First, it highlights the importance of tractability in lossless compression, which is an unexplored perspective in related works. The manuscript provides a class of tractable models for lossless data compression. These models are well defined and algorithms are shown.<|endoftext|>The paper showcases an application of Probabilistic Circuits to lossless compression, and achieves competitive compression performance to state of the art method. This has several advantages, one being that bits back coding is not needed, enabling single sample compression. Comments regarding writingPage 4:   Not clear what happens when 2 different input units have the same variable. This is exactly the case of Figure 1, where there are 2 input nodes for each of X1, X2, and X3. Overall the work is interesting and timely and I am leaning towards an accept if the authors can convincingly address the points above.<|endoftext|>Probabilistic circuits are a formalism, developed relatively recently, for describing multivariate probability distributions. Overall, I felt that whilst this was a reasonably well written paper highlighting the interesting concept of PCs, that I was not previously aware of, the methodological contribution wasn t clear enough and that the experiments were presented in a way which was potentially misleading. To someone, such as myself, who is an expert on neural lossless compression, these are both obvious points. I understand where this came from, since tractibility is a key property in the PCs literature, but I still felt this could be misleading, since neural compression and decompression with existing methods, whilst they may have different performance characteristics to PCs, certainly _are_ tractible. I had two main issues with the experiments. The first relates to the presentation of the timings of the method. In section 4.1 I think it s best to consistently use the word  latent  rather than  hidden , since this is the standard terminology in the deep generative modelling community.<|endoftext|>This paper proposes a new probabilistic model named PC for efficient encoding and decoding. The idea of PC for lossless compression is new and interesting. But I am not able to figure out how to incorporate neural networks with PC in O(log D |p|). Moreover, for auto regressive models, it seems that PC cannot reduce the complexity to O(log D |p|) but remains O(log D |p|).
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; This paper proposes a method that models the visual difference between images as a topological transformation for images. Using this model and computing the topological distance by minimizing the distortions between imagers, one can find neighbors that are conceptually similar to the input image. Evaluation results show that this simple method can achieve strong performance when only very few samples are available and no pretraining is allowed. 2.The empirical results look promising. With very limited data and no pretraining, the proposed method archives significant results on MNIST, EMNIST and Omniglot. Even the most challenging Omniglot dataset has a clean background and somewhat simple visual appearance. In this sense, I am not convinced whether the method can still work well when the image becomes more complex. When more complicated distortion exists (e.g.), is the model still able to capture it through the transformation? I would expect at least some more complicated benchmark datasets to be evaluated, for example, CIFAR10/100. 3.I am not very convinced about the setting. It has been studied that good pretraining can benefit the downstream tasks even in a few shot setting [R1]. Can authors give me some examples in real world about training a classifier with very few training samples while no additional pretraining data is available? This paper is interesting and has some promising results. But I have concern about its application in the real world and its robustness in more complex scenarios, which I hope the authors can address.<|endoftext|>**Summary**The authors propose a white box model that uses a similarity score in a space that is constructed based on topological transformation. Can you describe the process in more detail?‌ Do you cut the kernel at some point? To achieve this, they first compute the topological distance with training data points while minimizing the distortions (this part was a little bit unclear to me.I would appreciate it if the authors provide more context or examples of this). They use an interesting idea of using gradient descent to find a sequence of transformation which is the main reason that contributes to having a white box model. For example, it is true that meta learning needs data for a pre "meta learn" step, but there is a nuanced difference. So they propose using a chain of lattices to be able to apply their technique in higher abstractions. As a result, I would try to expand this section. I liked how they connected ideas from how the human brain works to train their model. Well structured writing. In International Conference on Learning Representations 2018As a result, even though I liked the ideas in the paper a lot, I am going to vote for rejection unless I can see the experiments added to the paper. I also liked making the model work as a white box by using their method. **Weak points**: Biggest drawback is lacking experiments on more challenging datasets, and unfortunately it is very important for me to see the results compared at least on very well known datasets for few shot learning. The main reason for my review is that there are other methods that leverage other data in the domain like (model agnostic meta learning[1]) and tested on a few samples on the classes that are never seen during the meta learning phase. It seems to me that this method has replaced the need for data with a good amount of domain knowledge (We know that characters could convert to each other by transformations, but what about cats and dogs?Another example is identity recognition from faces. If we have two images of two different people. However, the experiments did not convince me that this is always true. One interesting experiment that could be added is to apply this method to CelebA[2] face recognition dataset. Results on these could convince the reader about the effectiveness of the method.<|endoftext|>The paper on hand tackles the problem of learning from very few samples, which is of high relevance for many machine learning problems. To this end, an approach to model transformation based topological similarity is introduced, allowing for covering many kinds of invariants in image data. The approach is demonstrated for well known benchmark datasets for different classification models, demonstrating that in this way just using a small number of samples competitive classification results can be obtained. The paper on hand tackles the problem of data variety in a different way. As  the typical approach nowadays would be to use some kind of data augmentation, here vice versa some kind of transformation are proposed to provide meaningful similarities. The overall idea is not new, however, the specific way how this is realized is. From this point of view the paper would be a valuable scientific contribution. Moreover, it is unclear if the proposed approach would also work for more realistic datasets: MNIST and EMNIST might be to simple. On the other hand, the Omniglot challenge seems to be a too complex tasks, that no existing approach can solve not even rudimentary. So using this dataset was probably not a good choice? (3) To make the experimental setup more clear, all methods that are used for training should be mentioned explicitly. (6) For better understanding, it might be meaningful also to show examples for MNIST and EMNIST. (7) To allow for a fair evaluation and to give better insights, it might be meaningful to use the same learning approaches for all datasets. (8) The ocker background in Fig.6 does not look nice and removes the focus from the main content of the figure. In this way, it would be better to describe the other experiments more detailed and to shift this part to a separate publication. (11) The discussion on related work is rather short, which can also be seen from the very short bibliography. (12) The bibliography needs to be seriously checked for consistency and correctness. However, it seems to be questionable if this would also work for more realistic datasets. Moreover, the experimental section would need to be overworked. In this way, the paper would be an interesting contribution, but there are still a few open points hampering a publication as it is.<|endoftext|>This paper proposes a novel white box model for one or few shot learning, which tries to simulate the human recognition ability for “distort” objects. Extensive experiments on standard character recognition benchmarks demonstrate the proposed method outperforms the classical machine learning methods with very limited training data, such as less than 20. This approach is a white box model and human interpretable. 2.This paper is well written and easy to understand. The proposed approach is based on topological similarity. It seems that it only is suitable for images with simple topological structure, such as the character images. Maybe it is hardly used to classifier complex nature images since we need more information for natural image classification, not only use topological structure. 2.The authors did not provide the experimental comparison with enough training data, such as the whole training set in MNIST. The reviewer wonders about the upper performance of this approach with enough data. The main idea is novel and interesting, and the experimental results on limited training data demonstrate the superior performance of the proposed method. However, it may be not easy to apply this approach to other more complex tasks.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; rating score: 5; This paper proposes methods to improve the efficiency of large pre trained models (millions or billions of parameters) by combining multiple notions of _sparsity_ and/or _dimensionality reduction_ on the model weights. The authors validate their proposed method on several model architectures and datasets. Weaknesses:  In general, the motivation and exact contributions of this work are hard to follow. It s hard to judge the practical benefit of the proposed approaches simply by looking at the number of trainable parameters and the number of flops. Overall, I think that the topic the paper tackles is important and timely. That said, unfortunately, in its current version the paper is not so clear to follow (in motivation and implementation). In terms of technical novelty, it seems that the added components over LoRA might need a bit more work for their performance gains to be more convincing.<|endoftext|>The paper is presented in an easy to read manner. 2.I believe it s an important research topic to work on which can have a broad impact. I feel this then make the experiments not able to justify the proposed method. The organization can be improved. Overall, I think the paper present an easily understandable method to an important and interesting question. And consequently I think the proposed method is a bit ad hoc. I think to make me increase the score, the introduction should introduce more rationale of previous methods and why the proposed method can improve. 2.I believe the authors overlook an important work in the line of efficient fine tuning (https://arxiv.org/abs/2101.00190). As the low rank method in general is easy to train and the proposed method seems to have many steps. 5.This leads to my concern that each functionality of the method is not well discussed.<|endoftext|>* Technical novelty is somewhat limited, or at least can be better justified. This paper addresses an important problem and shows some promising results to improve the efficiency of NLP models. However, the presentation of the paper can be improved and there should be more justification of the motivation and novelty of the method. In particular, it seems that the main difference between this method and LoRA is the sparsity of pertained models, which is achieved mostly with existing techniques, yet it seems that LoRA (and/or similar method) can also be applied on top of sparse pre trained models.<|endoftext|>This paper proposes a principled way for achieving both parameter efficient finetuning and resource efficient inference. The first step aimed to achieve the parameter efficient finetuning but w/ the sparse residual components based on sparsity from the pretrained model to preserve the performance. The second step can reduce the memory of parameter storage. Below is my comments on this. Strength:1) Clearly defines the problem, goal and most part of the method2) Although not fully novel to scratch, the present an interesting idea of achieving both parameter and resource efficiency. 3) Multiple solid experimental results with a diverse set of models, benchmarks, and metrics showing the generalization of their approach. Weakness:1) My major concern is the evaluation. It is difficult to combine the experiments to support the claims. The gains in performance are marginal in compare to LORA, where LORA is a simpler method.<|endoftext|>Combining weight pruning and parameter efficient tuning is a valuable attempt to further improve resource efficiency especially at inference time2. Experiments demonstrate the effectiveness of the proposed method. This detail seems not to be mentioned in the paper – it is common practice to reports results from several random runs since the GLUE results are known to be sensitive to initialization 2. The necessity of adding a sparse matrix (i.e.S2 in the paper) into LoRA is not well supported. I know that this is not a fair comparison because of different models sizes, however, I suggest the authors double the size of UV + S2 for a fair comparison instead of downsizing UV – on the scale of 589.8K or 0.39M parameters, the number of parameters is not really a practical concern anymore. ACL 2021Some core method design is not well justified and the technical contributions are limited.
Reject; rating score: 5; rating score: 5; rating score: 8; rating score: 8; To achieve this goal, the paper presents a residue block that is consists of dilated convolution component for time dependency and Correlation component for cross asset dependency. 2.The paper presents a reasonable solution to deal with permutation invariance across assets, which is a relevant problem in portfolio optimization with deep learning. Presentation: The description of the architecture is not clear. 3.Experiments: While the proposed method is validated on various datasets competing with various existing methods, it is not clear to me whether the compared methods are indeed state of the art as claimed. It is also not clear to me whether the experiments indeed demonstrate that the proposed method can capture cross asset information. It is unclear to me whether the proposed method is original and significant enough.<|endoftext|>This paper proposed a portfolio policy network that has the permutation invariance (equivariance) property when treating multiple assets’ information. The proposed permutation invariant (equivariance) architecture seems effective. Please see the comments below:	      It is unclear how to obtain the random vector of asset returns. Is it estimated from the historical data? In addition, the hyper parameters used in the RL algorithms should be provided. What’s the difference between Cai et al.’s PI property and that defined in Definition 3.1?<|endoftext|>In this paper, the authors propose a new portfolio policy network architecture for DRl to exploit cross asset dependency information, which is shown to be able to achieve better performance than existing ones. Such a scheme mainly introduces a permutation invariance property, and it is very interesting. 2.The proposed design is well grounded and has potential applications. The portfolio management task is a good example but should be treated as an application, not a goal. Such a permutation invariant policy network may have more applications. Interesting design with potential applications.<|endoftext|>The paper proposes a new type of the permutation invariant (PI) principle for a class of functions (e.g., neural network blocks), and it demonstrates that \texttt{WaveCorr} structure satisfies the PI principle. Is it possible to include some simple test in the appendix to test the performance in this setting? The idea of adding the corr layer and the result that **WaveCorr** portfolio policy network architecture satisfies the PI property are interesting. Cons:  The paper discusses in Remark 3.1 about the distinction of definition of PI between this paper and the previous literature, but it seems to me that the motivation for this part is still insufficient.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; This paper shows a connection between the reward maximization (RM) approach and the distribution matching (DM) approach for fine tuning language models. The authors further suggest borrowing the baseline idea from reinforcement learning and applying it to DM to reduce variance. In Sec 5.5 Gradient Variance, $\pi$ is missing in "$G_\theta(x) A(x)\nabla_\theta \log_{\theta}(x)$"I think this paper is below the acceptance threshold because it is just a simple addition to the method of Khalifa et al.<|endoftext|>The authors explore the connections between reward maximimization (RM) with REINFORCE and distribution matching (DM) using distributional policy gradients (DPG), which uses importance sampling from a proposal distribution $q$ to minimize  $L   E_p[log\pi]$, where $p(x)   P(x)/Z$ is a distribution (energy based model) that is difficult to sample from (but incorporates generation constraints, such as gender percentage, etc.). Weaknesses:  The connection between importance sampling based distillation and REINFORCE was introduced in in the DPG paper, and from this perspective, adding a baseline is a straightforward exercise, and lower in novelty. Based on Algorithm 1, it seems that the DPG importance weights are not normalized, which is the most common way to reduce variance (at the expense of bias) in IS. Is this the case? Baselines for DPG methods are introduced, which improve optimization performance.<|endoftext|>This paper untangles the connections between the RM and DM paradigm, and exploit these connections to propose a baseline for the DPG method. * Strength    * This paper proposes an interesting idea and interpretation to connect RM and DM paradigm    * This paper proposes a variance reduction method for DPG which demonstrates its improvement on performance, stability and sample efficiency* Weakness    * From my understanding, the baseline mostly comes from the observation in 3.3, which has limited technical novelty.<|endoftext|>The paper builds upon previous work on fine tuning pre train language models to conform to pre specified "distributional constraints" (such as: "the model should generate the same proportion of sentences with female or male subjects", etc...). Specifically, the authors draw parallel between a previous approach (distribution matching, DM) and RL based approaches a la REINFORCE. Based on this comparison, they propose to adapt variance reduction techniques from RL (in particular the use of a baseline) to DM in order to facilitate training. Experimental results show that the resulting improved approach boasts better sample efficiency, improved stability and overall competitive performance. That being said is not quite clear to me that there isn t a simpler, more straightforward way to reduce variance in DM (see my full review for details).
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 10; This paper provides a very strong result for the implicit bias of SGD after it achieves zero training loss, i.e., capturing its behavior once it is in the manifold of the global minimizers. On the other hand, it seems that this analysis heavily relies on the label noise. Is it possible to extend this result to the mini batch SGD? This paper has very good theory and is well structured.<|endoftext|>You need to assume that the SDE in equation (10) has a strong solution. In particular, they show a sample complexity gap between label noise SGD and GD in the kernel regime for an overparametrized linear model, justifying the generalization benefit of SGD. Strength.The results are novel and the analysis is non trivial.<|endoftext|>This paper provides a new mathematical framework for analyzing the implicit regularization of SGD after attaining zero training loss. The framework is novel, and it leads to a simpler analysis of sample complexity under noisy label regularization. I am in favor of accepting this paper.<|endoftext|>Hence I see this result as a limiting one: surely in practice, the dynamics is not exactly the one depicted: a first phase where the loss goes to zero and a second phase where the dynamics diffuses in $\Gamma$. Here, it is explained that the nature of SGD noise for machine learning problem is very degenerate.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; Minor comment: table 1 has a misnomer. This paper introduces an evolutionary algorithm to automate the design of auxiliary loss functions, based on prior approaches like forward dynamics, inverse dynamics, contrastive state representation learning, etc. The evolution solves a bilevel optimization problem in which the inner loop is regular RL training while the outer loop evolves the loss function. The contribution is greatly undermined by the fact that simple RL with image augmentation can outperform the complicated bilevel optimization and auxiliary loss in AARL. Experiments are conducted on the Deepmind Control Suite for both pixel and state based observations. The paper ignores at least 2 simple but important prior works, and do not outperform the results in those baselines. * To my knowledge, evolving the optimal loss function for auxiliary RL tasks seems to be a novel approach. **Weaknesses**My major concern is the weak experimental results. To elaborate: For pixel based Deepmind Control Suite, the strongest baseline that the authors compare to is CURL (Laskin et al., 2020). However, this is quite an outdated baseline. Also known as "RAD". Both papers (neither cited in the paper) are published on top ML conferences before June 2021, so it is fair to request comparison with these prior SoTAs per the ICLR review guideline. In fact, if we compare table 1 of the "DrQ" paper (Yarats et al.) *This indicates that even **simple RL with image augmentation can outperform the complicated bilevel optimization and auxiliary loss in AARL**, which greatly undermines this paper s contribution. For state based DMControl experiments, the paper claims in section 3.2 that "there is no data augmentation in the state  based setting."<|endoftext|>Also using their method the author analyse different auxiliary losses to identify common patterns. This is clearly not enough to support the broader claims made by the authors, in the abstract, introduction and conclusion. Another issue of this approach is that the search space is done on the loss input (I) and the loss operator (f), but the encoder is kept constant. For instance, would the conclusion hold if the CURL encoder is substituted with a Transformer? The authors have an ablation on Encoder Architectures in section 3.4, but it’s limited to the state based case, which is less interesting than the pixel based one. I believe this is an important limitation as it could drastically change the conclusion of the paper. Indeed, it is well know that this kind off loss works well in fully observable 2D environments, but it could have severe issues in partially observable 3D environments due to the increase state aliasing problem (e.g.Badia et al., 2020). The authors combine different AutoML techniques to automatically derive the best auxiliary loss function for RL.<|endoftext|>This paper introduces a method for searching for the best auxiliary loss function from a huge search space automatically, while the best auxiliary loss function is defined as the one that encourages the agent to get a higher return. In this work, the size of the search space is around $4.6 \times 10^{19}$. The space is pruned firstly by random sampling from the similarity measure space and choosing the one showing the highest averaged performance, then the evolutionary algorithm is applied to select top candidates for loss inputs, which is a method in the literature. The chosen auxiliary losses are empirically shown to be helpful with increasing the learning efficiency and have generalization ability to new tasks. It avoids the hassle of manually designing and testing auxiliary loss functions, as well as provides a more efficient search method than grid search when dealing with a huge search space of the auxiliary functions. (+) The experiments are done with either 5 or 15 seeds, providing reasonably reliable results, although more seeds could improve the accuracy of evaluation even more. I agree that this method is more computationally efficient than grid search. Thus, it introduces the risk that the best candidate is never selected and evaluated. It might be better to give an analysis on how large this risk is or the probability this situation happens. The empirical results suggest that the average performance of applying the chosen auxiliary task is higher than baselines.<|endoftext|>In this paper, the authors intend to automatically search for the optimal auxiliary loss. The paper conducts extensive experiments on the DeepMind Control Suite and shows that the searched auxiliary losses have significantly improved RL performance in both pixel based and state based settings. My major concern is about the efficiency of the proposed method AARL. The overall loss function is a combination of reinforcement learning loss and evolutionary loss, both of which are difficult to optimize. The combination of RL and evolution would be even more difficult to converge. 2.The combination of RL and evolution could be improved. Equation (1) optimizes the two algorithms by a simple weighted sum of the two loss functions. A more practical solution is to alternatively train the two losses. There are two reasons for doing so: 1). The scalabilities of the RL loss, as well as the auxiliary loss, are different, and it is difficult to confirm the weight \lambda; 2). The major technical contribution is to design a search space and an evolution strategy to derive an optimal auxiliary loss. This paper has good motivation, a good solution for the problem, and extensive and supportive experiments. The major concerns are low efficiency, convergence difficulty, and non significant contributions.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 5; Since the model is singular, the Laplacian approximation of the normalized evidence needs a new method. This is the first time I hear about the theory. First, I feel that the theoretical improvements over previous methods are not significant. Do you actually check whether the model is singular or not? I hope to see experiments based on realistic datasets. Can we also include other variational inference methods as the baseline when estimating the normalized evidence? Third, I don t know how significant it is to use normalizing flow to learn the resolution map? Is the analysis still valid for the complex flow model? In practice, the method has competitors from other variational inference methods. How practice is this method in estimating the normalized evidence. Fourth, the writing of the paper can be greatly improved. A lot of my understanding depends on previous work by [Bhattacharya et al.2020].Here is a list of detailed issues that block my understanding. 1.I think the paper should make clear the Laplacian approximation from the beginning and also the significance of estimating lambda and m. 2. Equation 6, "in the asymptotic expansion of ..." what expansion? The paper studies an interesting topic.<|endoftext|>Inspired by asymptotic results from singular learning theory, the authors of the paper propose using a generalised gamma mean field distribution with a normalising flow (that targets the "desingularization map") to perform variational inference. 6) Although the authors use a Gamma source distribution, in practice, they approximate this with a Gaussian distribution. 7) *The flow that is used within the paper is very shallow—it only has two coupling layers. It is well known that many layers of affine couplings to have expressive distributions. This claim is not well supported; the experimental results use particularly non expressive flows, and as such, this claim only stands in this context. Thank you for including this. 8) The authors write "for large n, the posterior is not Gaussian, but a mixture of standard forms". The discussion of what it means for "n to be large" is extremely limited. I have a number of concerns about the paper. Further, it is entirely unclear to me whether practical Bayesian Neural Networks are in a "large n" regime. This issue, at the very least, should be discussed within the paper. Clarifications:1) As far as I can tell, the theory assumes that the space of parameters is a compact set. The paper proposes using a normalising flow approximate posterior with a Gamma source. 2) The asymptotic expansion of the normalised model evidence holds only for large values of n, right? Intuition could be added on why neural networks are singular models. 4) Singular learning theory provides this resolution map, which in practice is learnt using a normalising flow. I understand that simple networks are required to apply singular learning theory,  but the proposed method could be applied to much larger networks. The experiments within the related work, which is almost exclusively work within the Bayesian Deep Learning community, typically uses experiments with a much larger scale.<|endoftext|>The paper builds on the recent theoretic results of [Bhattacharya et al.(2020)],which shows that a mean field variational approximation with carefully chosen approximation familyleads to an ELBO $\Psi$ which is sharp up to a constant C(d) which only depends on the dimensionality d of the parameter space. For the proof, [Bhattacharya et al.(2020)] assumes that the approximation family is a Gamma distribution truncated to [0,1]. The authors are clear about the limitations of their approach. Weakness:  Theorem 5.1. does not seem to tell more than Theorem 3.1 in [Bhattacharya et al.(2020)].Am I missing here something? What is the relationship between the mean field approximation and the normalizing flow? Since the authors use a Gaussian approximation for the Gamma distribution, it would be interesting if the authors could show whether and when the approximation is justified in their setting. They should be summarized in the main paper. The idea of trying to use recent singular model theory to improve variational inference is interesting. However, this work, in its current form, leaves too many open questions about the relation between theory and their experiments. Furthermore, it is unclear what the take home message is.<|endoftext|>Working with Bayesian neural networks, this paper proposed a variational algorithm to approximate posterior distribution of the network weights. To overcome model singularities, the authors used the idea of normalizing flow by transforming the weights through an affine coupling network, and subsequently worked on the desingularized parameter space. In addition, they derived an asymptotic expression for the ELBO, and compared the Gaussian and generalized gamma approximating families in the experiments. Equation (8) is misleading because it is not exact equality but should be $\asymp$, i.e., $\sup_{q\in\mathcal{Q}_{(0,1]}}\Psi_K(q,g)$ is bounded above and below by the right hand side up to constant multiples (see (15)). Therefore the lower bound of (8) is similar to the lower bound obtained by Bhattacharya et al.(2020) in (7). I don t think recovering the leading order term of $\log{\bar{Z}_K(n)}$ guarantees that the mean field generalized gamma family is the optimal approximating family (as suggested in Pages 20 21 in Bhattacharya et al., 2020 and implicitly in this paper). There are many examples in the literature where dependent approximating families give clear improvement with respect to mean field (see "Variational Inference with Normalizing Flows" for example). Why do you work with $\Psi_K(q,g)$ and $\bar{Z}_K(n)$ in (4) and (5)? The results would be more significant if their stochastic counterparts $\Psi(q,g)$ and $\bar{Z}(n)$, which are the quantities of main interest, are considered instead.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 6; This paper introduces ShiftAddNAS that performs neural architecture search on a hybrid design space that contains conv operations, attention operations, and hardware efficient shift/add operations. I think it is quite interesting to integrate hardware efficient shift/add operations into the search space of NAS, which can potentially improve the efficiency of neural networks on hardware. The idea of heterogeneous weight sharing is not new. Compared to the previous strategy, this work has additional regularization terms (Gaussian for Conv ops and Laplacian for Add ops). I would expect to see comparisons with this approach as well. However, I find the technical contributions and empirical results presented in this version are not strong enough.<|endoftext|>This paper designs a hybrid search space that includes multiplication based operators and multiplication free operations to find good trading points between accuracy efficiency. Further, this work defines the problem when training weight sharing supernet on the hybrid search space and proposes the heterogenous weight sharing algorithm to address the problem. The proposed method is validated on both NLP and CV tasks, outperforming several competitive baseline methods in terms of accuracy and saving efficiency on latency or energy. **Strengths**  The paper is well motivated, well written, good quality. To my knowledge, it is the first paper to design a hardware inspired hybrid search space between multiplication based operators and multiplication free operators for NAS. **Weaknesses**  Search algorithm is not new. The reviewer thinks the cost to train supernet and search cost should be reported. Generality issue on multiple hardware devices and energy & latency information are missing in Table 4.<|endoftext|>This paper proposes a NAS method for both multiplication based and adder based networks. The contributions mainly lie on hybrid search space and new weight sharing strategy. The main contribution of the paper is a NAS framework for multiplication based and multiplication free operators. I recommend accptance for it.<|endoftext|>This paper propose a hybrid search space including both multiplication based and multiplication free operators. The authors also give a novel weight sharing strategy to enable effective weight sharing among different operators. 2.The results are the state of the arts on both NLP and CV tasks. The idea of this paper is novel and the results are satisfying, but there are more details should be provided.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 5; 3.The experiment studies an important aspect of IL, which is the relationship between the amount of expert data and the IL s performance. It is unclear whether similar patterns would be observed if the expert matches the level of the performance observed in previous works (e.g.https://github.com/berkeleydeeprlcourse/homework/tree/master/hw1/experts). In my experiences, these details have quite significant impacts on IL s performance and I hope the authors could expand on the experiments to further validate their theoretical results. Comparison between Wang et al.2019 vs the 1 min L_2 distance reward. E.g.How much does it matter in the practical algorithm? Is it even possible to differentiate between stochastic vs deterministic one from limited training data? The paper shows that, for deterministic experts, imitation learning (IL) could be reduced to RL with a stationary reward. The paper thus provides theoretical grounding for several existing works. On the other hand, the empirical results are consistent with those from existing works, but are limited in its scope. Despite the flaws, I think the work is still interesting to the community. It would be nice if the paper could include more experiments to further support the theoretical analysis.<|endoftext|>In this paper, the authors show that, for deterministic experts, imitation learning can be done by reductionto reinforcement learning with a stationary reward. theoretical analysis bothcertifies the recovery of expert reward and bounds the total variation distancebetween the expert and the imitation learner, showing a link to adversarial imitationlearning. Experiments are given to confirm that the reduction works well inpractice for continuous control tasks. The paper includes potentially interesting results. 1) In this paper, average rewards are considered. Is there any reason to consider the average reward MDP instead of discounted MDP, which is more widely used? 3) It would be better to discuss what kinds of RLs can be used in Proposition 1. It is not clear in the current paper. 5) It is not clear how the imitation learner can learn the expert policy from the intrinsic reward in section 3. 7) Overall, the organization of the paper can be improved further to more clearly deliver the ideas by adding more detailed backgrounds.<|endoftext|>This paper considers imitation learning problem which aims at obtaining a policy that imitate expert behavior. Some empirical experiments are performed on continuous control tasks. Although the better intrinsic reward leads to better extrinsic reward, it is still troublesome to get a good policy by any reinforcement learning algorithm with this sparse reward function. This makes the result less useful than just applying other comparison methods. Could you give more explanation on this? It is much clearer if you can provide comparison of computation time of different algorithms in the experiment section. The result is promising and interesting from a theoretical point of view. This paper provides rigorous analysis for using reinforcement learning in imitation learning. The result is novel and shows the potential to accomplish imitation learning in a straightforward way.<|endoftext|>The authors propose an algorithm for imitation learning which rewards the agent for observing state action pairs that are part of the demonstration set. The main contribution of the paper is the theoretical analysis which shows that the algorithm, given sufficient expert data, matches the expert’s occupancy distribution and expected reward. The authors also provide an empirical evaluation in standard control benchmarks. The main issue lies in the assumption on the amount of data that is required which is linear (with a factor > 1) in the squared size of the state space (note that the bound remains large even if \delta is set to 1). It would be good if the authors could add motivation and discussion for this form of the reward. The results are very positive, but it is unclear whether they are due to the theoretical properties of the algorithm or due to the particular nature of the mujoco walkers.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; rating score: 5; This paper starts from the random Fourier model, generalizes the recent results on weighting the coefficients to adopt weighted least squares loss. Under different settings of noise levels and over /under  parameterized regimes, error analysis has been provided. strengths: solid analysismy concerns:1. I hope that the author(s) could provide some motivating application scenarios. Prior information in these two spaces could be different.<|endoftext|>The authors study generalization errors in Fourier regression scenarios with over and under parameterized models. The technical contribution is solid and generally of interest to the machine learning community. Some parts could be underpinned with more information, e.g., I am wondering what implication follows from the statement that "in the case of α   0, when β ̸  0, our result in (14) is slightly different from its equivalence in Xie et al.(2020)"?Strong technical part, perhaps somewhat off topic for ICLR22 but nevertheless still a very good paper.<|endoftext|>The paper considers a novel generalized weighted least squares optimization method for the random Fourier feature model and conducts its generalization error analysis in both under  and over parameterized regimes. Overall, the paper is well written and technically sound. The results should be of interest to the community.<|endoftext|>In particular, this paper follows the random Fourier model setting in Xie et al.(2020) and analyzed a generalized weighted least square optimization method that allows the weighting in both the parametrization and data space. The authors derived the generalization error of such weighted least square framework for the over parametrized and under parameterized regimes and compare them in these two cases. The paper also studies both noise free and noise cases. The paper is a solid work that provides some substantial extension of the work (Xie et al.(2020)) for a more general weighted scheme and the noise case study. This solid theoretical work is a substantial extension of Xie et al.(2020) by considering weighting in both parameter space and data space.<|endoftext|>This paper studies the weighted least squares, random features model under noise one dimensional data setting in under /over parameterized regime. Regarding to Eq.(3), this is actually an assumption. It requires that the second order moment (matrix) of the parameter theta is diagonal and decay fast. I do not find any justification for the diagonality assumption on the parameter. If both LHS and RHS are random, a probability error bound is needed. Besides, it would be best to compare Theorem 5.1 with the following refs that focuses on RFF under noisy data in the interpolation regime.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; rating score: 5; This paper presents a new language model that is enhanced with knowledge from Wikidata. This is expected and the paper shows it when compared to vanilla pre trained Transformer models. The authors use two masked language modeling strategies by training with language switched relation triples and Wikidata graph cycles (unrolled as sequences of triples).<|endoftext|>     This paper proposed a new pretraining framework of incorporating multilingual knowledge into language models. And encoding logical patterns into LMs is an interesting problem and this work sampled simple “circles” from the KGs. Why are two kinds of “circles” important for logical reasoning? Seems the authors did not report all the performance of KMLM large without logical reasoning. This work proposed to solve interesting problems of multilingual knowledge pretraining. More importantly, the author should think more about logical reasoning.<|endoftext|>### Strengths of the paper:  This addresses an important area in large language models, of better encoding knowledge in multilingual models. Given a better defined hypothesis, it may be easier to evaluate how much of the problem is solved by the techniques in this paper.<|endoftext|>The language model not only memorizes the factual knowledge but also learns useful logical patterns. The author also proposes a new task to test the model s ability of logical reasoning. This should be further clarified in the paper contribution. The paper is well written and well presented.<|endoftext|>The paper argues that existing knowledge based language models are all trained on monolingual knowledge graph data, hence limiting their application to more languages. The authors provide good and clear motivation and introduction to their research work. In general, the paper is well structured. The discussion of related work is adequate with a number of recent citations. This is however not clear, a more clear explanation would be helpful.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; The authors provide a nice background of pruning literature and sparse representations in neural networks. However, the background citations are for a slightly different, but related topic; pruning techniques are not exactly what this paper is about, but it does share some background with spares representations. The experimental results look very preliminary. The last section of encoding messages in weights seems quite unrelated to the rest of the paper. This paper has some interesting concepts but falls short on uncovering novel insight.<|endoftext|>Unfortunately, there are serious issues with this submission:The introduction and motivation seem to be written for a paper that is on pruning not quantization, and they are out of context compared to the title, abstract, and rest of the paper. This is most likely a LaTeX error. Can the author clarify what they mean by He distribution, I believe they mean He conditions on variance. Can the authors evaluate their work on more contemporary networks such as ResNet on ImageNet and similar tasks? Message encoding in the neural network s weight using steganography in training is interesting. However, why does it matter?<|endoftext|>This paper proposes a neural network training technique such that individual weight bits can be optimized separately. In detail, each weight is represented as a weighted sum of its bits weighted by powers of 2. (Sec 6.)Here are the strengths and weaknesses of the paper. [1] Zhang, Dongqing, et al."Lq nets: Learned quantization for highly accurate and compact deep neural networks." 2018.The perspective of the paper is interesting, but some claims might need correction.<|endoftext|>Strengths:  This paper experiments with an interesting an intuitive idea. There are many interesting applications, e.g., more efficient networks and embedding hidden messages in network weights. The method matches standard training on ImageNet but faces accuracy degradation on CIFAR. A concern is that this accuracy degradation would be even more substantial for problems such as ImageNet. Since the paper is not 9 pages, there is definitely room for this.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; This paper tackles the challenge of generating adversarial perturbation for a target model   with no access to the model, or the model s training data (i.e.target domain). However, the writing is error ridden, and the proposed method is only marginally novel w.r.t.existing works. ### Strengths:1) The problem setting (no access to target data) is of importance   in practice, access to data is as hard, if not harder, than access to model. 2) The experiments are extensive, and clearly show a significant improvement in black box attack capability. 2) The novelty of the method is limited. This is an important question that the manuscript only deals with in the appendix.<|endoftext|>This paper focuses on the transferability of black box domains. Therefore, Beyond ImageNet Attack (BIA) is proposed to investigate the transferability towards black box domains (unknown classification tasks) with the only knowledge of the ImageNet domain. 2.In competitors, diverse inputs method (DIM) is not new. However, the effects of DA and RN seem to depend on different models. 2.Does the comparison method differ greatly in training costs? I tend to accept this paper because it focuses on more realistic black box attack settings and proposes two modules to improve performance.<|endoftext|>This work first identifies a more practical threat model for black box transfer adversarial attack, where the target model s domain remains unknown, and the attacker s surrogate model may be trained in another domain. Experimental results demonstrate that BIA is more effective than existing methods. 2.The specific methodology of BIA seems not new. Considering more practical threat model is certainly helpful and important for the transfer attack research.
Reject; rating score: 3; rating score: 5; rating score: 8; This paper proposes a defense against backdoor attack on pre trained large language models. Strengths. W3: The paper needs major improvement in writing. Comparing the gradient and Q can be interesting. This paper proposes a simple defense against backdoor attacks on pre trained language models. Thus, it is not possible to confirm the benefit of the proposed approach.<|endoftext|>This paper proposes a method to defend against NLP backdoor attacks. By doing so, rare words can be updated to a "normal state" and are expected to be no trigger of attacks anymore. The authors also empirically show the effectiveness of the proposed method. The proposed method is well motivated and novel to me. 3.The authors conduct experiments and show that the approach can help defend against backdoor attacks, with only a negligible generalization drop. \Plus, some technical details are not clear to me.<|endoftext|>This paper identifies an emerging threat for the prevailing pre trained models   the inheritance of backdoor attack, and proposes a simple yet effective defense approach: gradient broadcast adaptation (GBA). Instead of the traditional “erasing triggers”, GBA utilizes the “prompt tuning” as a tool to guide the “perturbed weights” back to the normal state, which helps avoid the degradation of generalization ability. Meanwhile, the authors perform an empirical evaluation of the proposed method against four state of the art backdoor attacks. The paper is well organized, and the motivation is clearly written.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; This paper describes novel loss functions for learning to predict an implicit 3D scene representation from a single image. They argue that when working with real scan data of scenes (rather than single objects) it is difficult to generate accurate occupancy or signed distance function (SDF) ground truth as would be required for supervised learning. Instead, they propose to only use occupancy or SDF supervision near the surfaces of objects; elsewhere, they rely on constraints on the gradient of the occupancy or SDF adapted from Gropp et al.2020.They perform a thorough evaluation on several benchmark datasets and compare against state of the art competing methods. Their proposed loss functions are novel as far as I know. They also derive the closed form gradients of their loss function and show the importance of using them over numerical derivatives. The paper is also well written and nicely presented.<|endoftext|>In this paper, the authors propose a new method for single view 3D reconstruction. A conditional (image feature prior) implicit representation framework is proposed to reconstruct 3D scene from a single view. Although the approach is new, I still have some concerns listed below. # Good resultThe authors have shown good results in a variety of datasets, quantitatively and qualitatively. Could authors shed some light on the difference? The paper is well written and presented overall. The essential experiments are performed and the results are well presented.<|endoftext|>This paper presents a new method to learn implicit 3D scene reconstructions from single image input. The main improvement is a closed form Differentiable Gradient Sampling. By taking spatial gradient into  consideration, the proposed method can apply back propagation of the  loss on spatial gradients to feature maps and allow the training for the case of without dense 3D supervision. Overall, I think the proposed method is novel and with reasonable performance. I am in favor for acceptance if the authors can provide some discussion about the cons listed above.<|endoftext|>This paper presents a method for 3D scene reconstruction from a single image using implicit surface representations such as occupancy or SDF. Would these methods yield a boost of performance? How close is a surface/surfel prediction to the ground truth is considered a "positive precision"? Two main components are described in the paper:  1. The authors argue that conditioning the spatial gradients on pixels is novel. First, I don t see why the authors emphasize that the formulation is closed form, as the spatial gradient expression is clearly taken numerically and its derivative can be computed via automatic differentiation.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; Overall the paper is a good submission, and I would recommend accepting it. The paper presents derivations transforming this convex constrained optimization to a practical, unconstrained objective. The proposed algorithm is novel (although heavily derives from existing work). The empirical results & baselines are comprehensive.<|endoftext|>The paper considers an offline imitation learning (IL) problem with an addition of supplementary imperfect demonstrations. ICLR, 2021. I overall like the idea and execution in the paper. The derivation of the method is also grounded. It is indeed  possible to find a non parametric $\pi$ from equation (21).<|endoftext|>I raised my rating for the rebuttal and the paper has a nontrivial contribution to the field. The theoretical analysis and empirical experiments demonstrate the improvement of the method over BC and ValueDICE. 2.The authors mentioned that the supplementary demonstrations are imperfect, containing expert or near expert demonstrations. It is also worse than ValueDICE in Walker2d. I hope the author could incorporate the suggestions by the reviewers to improve the overall quality of the paper.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 8; This work presents WFN, an ensemble of techniques to reduce data movement costs by reducing the number of uniqueweights in a network. This paper falls under the same category as pruning or quantization, but provides a very fresh perspective. Thus, I have given a rating of 5 for this paper. The paper is well written (with some minor grammatical errors at some places) and the results shown by the author are comprehensive. One major weakness I found in the paper is that the analysis is not exactly correct. They substantially reduces the number of unique parameters in a network when compared to existing SOTA quantisation approaches. But reduction in parameters does not necessarily mean energy efficiency. , your data access costs may vary. Can the authors comment on impliction of WFN and how introducing more unique weights can impact adversarial robustness?<|endoftext|>A technique is developed to reduce the number of unique weights used in a network. Results demonstrate that good compression ratios are possible, comparable to SoA quantization techniques, but with far fewer numbers of unique parameters. The work is interesting and appears to achieve its goals. The argument is made that the approach presented would reduce data movement. For this reason I am currently recommended that the paper is not accepted.<|endoftext|>This paper focuses on reducing the data movement costs by reducing the number of unique weights in a network, i.e., resuing weights. By doing so, the empirical studies show that the proposed WFN is efficient in terms of reducing parameter counts. Empirical studies support that this method is able to reduce the parameter counts significantly. weakness:  The empirical studies seem to be a little bit strange because parameter counts are also a type of "weak proxy" for the energy consumption. These might be more intereting and important to see than the parameter counts. The idea of this paper is novel and may inspire broadly future works. The empirical design can be improved.<|endoftext|>The paper proposes a method called Weight Fixing Networks (WFN), which is designed to minimize the data movement in deep learning inference, thus minimizing the energy cost. The most expensive energy costs lie in memory reads. Thus, data read and movement operations, not arithmetic operations, dominate the energy costs of deep learning inference. This work focuses on reducing data movement costs by reducing the number of unique weights in a neural network. The clustering stage carefully choses the closest fixed centers for some of the weights. The paper introduces WFN, weight fixing networks, an algorithm that transforms a neural network into a representation with very few unique weights, the ones most frequently used being powers of two, all while maintaining good inference performance.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; Many existing graph generation models are surveyed. The proposed measures are calculated on many real world datasets. It seems that the goal of this paper is only to reveal and explain preferential attachment. What is the intuition of the proposed measure in terms of analyzing structure evolution? This paper studies an interesting problem and proposes two novel measures.<|endoftext|>The authors tested Gaussian model, Preferential Placement (PP) model, and Directional Preferential Placement (DPP) model for node generations. (Main Contributions)1) Propose reasonable processes that can explain the evolution of knowledge embeddings in terms of preferential attachment and attractive repulsive force. As both models and new metrics are proposed, there must be a natural question about the stability and sensitivity of this metric. (Strengths)This paper proposes interesting approaches to review popular static word embeddings.<|endoftext|>In this paper,  the authors try to study if we can learn about the human s process of generating new ideas or concepts from embeddings. One concern I have is the embedding the authors use. The authors finally compare the embeddings from generative models with the embeddings from real world data driven methods. But there are some limitations of this study, e.g., not looking at contextual embeddings.<|endoftext|>Specifically, the authors propose two metrics to characterize embeddings trained on different datasets. Finally, they conclude that the real world data can be well simulated by a certain generative model. This paper studies data generation processes with the help of embedding spaces. The title is a bit confusing. The authors ask a question "why do embedding spaces look as they do" in the title. However, as noted by the ICLR committee, reviewers are not required to read appendices. I agree that this work gives a good start for an interesting problem. This paper studies an important and interesting problem.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; rating score: 5; The paper presents a new approach to self supervised learning using VAEs which uses augmentations and a non contrastive self supervised objective to perform unsupervised representation learning on the data. The paper motivates VAE and the related works for a significant portion of the paper and does not talk about the proposed idea itself. which is not true. By removing the KL loss, is the existent method still a VAE?<|endoftext|>This paper proposes a VAE based approach for the task of self supervised learning. Non contrastive approaches can be trained with relatively smaller batch sizes, compared to contrastive approaches. However, one paragraph after, they suggest removing the KL and replacing it with a domain specific loss term for better performance. I don t agree with the discussion around approximating data level stats and the requirement of large batch sizes and how the proposed method handles that. I don t agree with the argument in Section 4.1; autoencoding is not a viable alternative to non contrastive methods, while it can be another approach with similar performance to some of pretext tasks designed for SSL.<|endoftext|>The paper presents a third family for self supervised learning that relies on generative modeling, in particular variational autoencoders (VAE). Learned representations are evaluated on image classification downstream tasks and compared with recent contrastive and non contrastive self supervised methods. Also, the method is not variational anymore. While there is a significant improvement compared to AE based methods, there is a huge performance gap between the proposed method and most of the SOTA considered in the experiments. The proposed methods share the idea of training VAEs with a denoising criterion, albeit removing the KL term in the proposed method.<|endoftext|>This paper presents a modification to the VAE training scheme. Finally, the authors empirically evaluate the proposed model on standard datasets such as CIFAR 10, STL 10, and ImageNet. I find the current name and presentation somehow misleading since by removing the KL divergence, your method is not really optimizing a variational objective anymore, is it? By comparing with the AAAE baseline, you tested the importance of your augmentations on the usefulness of the representations. Did you also test what happens if you do not use augmentations only add noise to the latent representations? Regarding your experiments: these results would be more convincing if you trained longer such that the accuracy does not change anymore. Furthermore, these changes should be feasible to implement during the revision period.<|endoftext|>This paper introduces a self supervised learning method augmentation augmented variational autoencoders (AAVAE) by removing the KL divergence in the traditional VAE. And the experiments on image classification show that the learned features by AAVAE have better properties than the existing alternatives. Thus, the paper lacks novelty for me. The proposed method is simple to follow.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; The paper proposes measures of how two types of explanation methods   rule based and exemplar based   generalize and extrapolate to unseen data regions. Some of the ideas introduced by the paper to think about certain inductive biases is interesting. For example, feature level bias being which features are easier or harder to learn. I found some of the exposition unnecessarily confusing. For example, Section 4.2 starts off by saying that one NN, one GLM, and one GP has been trained and then make a jump to Figure 4a which shows decision boundaries that are not surprising; I am not sure how it validates the protocol proposed to measure EVR without confounds. The paper is ambitious but falls short in clearly explaining the framework proposed, and substantiating the idea with strong experiments.<|endoftext|>Motivated by the studies of exemplar vs. rule based generalization in cognitive psychology, the authors present a protocol directly probing this trade off in machine learning systems. The authors present empirical results across a range of models in both expository and real world image and language domains and demonstrate that using the trad off provides a more complete picture of extrapolation behaviour than existing methods. CC condition is unclear and I do not know feature level bias. Similarly, “Exemplar vs rule bias is measured by the difference between performance in the PE and ZS conditions—…” When reading PE and ZS, they are not defined either. There are measures obtained from models trained by some specific methods. Also, I do not know what spurious correlation means in this paper. What rules can we generate based on the two examples? The paper is unclear based on the current presentation.<|endoftext|>The inductive biases of the learner determine such extrapolation. Inspired by these experimental approaches, we have proposed a protocol to investigate this trade off in learning systems directly. We demonstrate that controlling for feature level bias while measuring the trade off between exemplars and rules provides a complete picture of extrapolative behavior than existing formalisms. Generalization in the domain of extrapolation, which the authors address in this paper, is one of the critical issues in machine learning. Therefore, they propose a protocol to investigate the problem of inductive bias in extrapolation. Specifically, inspired by psychological research, they propose a protocol to investigate the inductive bias of the learning system towards different features (FLB) and the inductive bias of the learning system towards different ways of using features, either by rule based or exemplar based generalization (EVR). However, we are not convinced that it is a practically valid method, as we have only shown experiments on two datasets. The authors  treatment of inductive bias in extrapolation is interesting and will interest many researchers. However, we are not convinced that the proposed protocol is practical.<|endoftext|>This paper proposes two measures for evaluating the feature level bias and the exemplar vs rule bias in learning systems. My only concern is the practical utility of the proposed measures. The inductive bias of a given learning system is measured by its extrapolation performance difference when trained on different training conditions. Strength:+ Probing the behavior of the learning system by comparing its generalization performance when trained on different datasets is interesting and novel. + The paper is very well written. The main contributions of the paper are the two proposed measures. + For the exemplar vs rule propensity (EVR), my understanding is that it captures the extent to which the distractor features are utilized in the learning system. + You argued in the paper that the measure is capturing something more than the spurious correlation. I think it will be more precise to state it as **linear** correlation in Figure 3.
Reject; rating score: 3; rating score: 3; rating score: 6; rating score: 8; The paper is an empirical study of combining multiple feature priors along with some pre processing to solve a variety of computer vision tasks. Concerns  Very meagre contribution in terms of technical novelty and framework. Experimental evaluation and comparisons seem dated, not state of the art. The work is very much below the expected standards of ICLR. The paper is clearly below par, can be rejected.<|endoftext|>The authors consider feature priors as distinct perspectives on the data. The results show that models trained with diverse sets of various feature priors have less overlapping modes and are more efficiently combined. The paper is not very clearly written. Later in the text, the co training is mentioned, and it seems that using different priors   co training using different views. As far as I understand, there are two "priors" only explored in the paper: shape and texture. The current contribution is an exploratory work, combining several state of the art methods (for instance, self training and co training are used in the experiments). There is a lack of technical novelty.<|endoftext|>The feature priors concern shape and texture. Some of the co training experimental results are strong. One concern I have is that the ensemble results presented in section 3.2 are generated using very primitive ensembling techniques. I would prefer that the ensemble section be redone with more sophisticated ensembling and/or removed and I would prefer that the absence of spurious correlation in unlabelled data assumption be presented more cautiously. Related work is normally presented earlier in a paper. It also would be nice to show the method on another domain aside from image classification, although I realize space constraints might make this difficult. The authors achieve some positive results from cotraining of groups image classification models designed to focus on shape but not texture or vice versa.<|endoftext|>The paper proposes a formalized framework for imposing priors on the feature extraction in deep visual processing models. The core contribution of this paper is the systematic formulation and investigation of how different, distinct feature priors leads to complementary feature representations that can be combined to provide more robust data representations   in other words, creating synthesized multi view data representations. The paper ties back to early (1998) work on co training (which essentially is multi modal bootstrapping) and ties this to the more recent body of work on self supervision and self training. Experiments are performed with classical shape  and texture biased models, and show that the hypothesis   that diverse feature priors are able to robustly create a set of complementary data views   holds. This paper has a number of strengths, that combined makes me recommend the paper for acceptance:+ The topic of this paper, creating and combining robust, generalizable and diverse feature representations, is of high relevance to a large portion of the ICLR audience.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; This paper attempts to solve the problem of image inversion by introducing three augmentation based techniques. They validated all the augmentations using Vision transformer and MLP based vision models and compared them against the exiting method Deep Inversion(Yin et al., 2020). The paper is well written but lack quantitative evaluation which is a very significant shortcoming. The paper mentioned several drawbacks in the Introduction section of the existing methods   the authors should put any of such examples like the generated images being highly sensitive to the weights assigned to the regularized terms, etc, and compare that with the images generated by PII to show how PII is working better. One of the main issues in this paper is the applicability of the solution of the problem is not discussed in the paper.<|endoftext|>This paper proposes Plug In Inversion (PII), which can invert a trained image classifier so that it generates a class conditional image. # Strengths* The authors aim for a model agnostic and hyperparameter robust method. This is an important direction. I agree with one of the goals of model inversion is so, but the paper lacks a discussion of how PII helps us to understand vision models, even though the authors conducted experiments on various image recognition models. I recommend the authors to use CIFAR 10 as Yin+20. # Comments* References are not well organized and written in a consistent format. * Section 3.3 says $e 8$ results in images of acceptable quality, while in the main experiment, $e$ is set to 32. The model aims for a model agnostic and hyperparameter robust method to investigate vision models by model inversion. I highly appreciate this goal.<|endoftext|>This method is evaluated on ImageNet trained models and is compared with other techniques used for the same goal. This paper proposes a technique called Plug In Inversion, which is to be used as is, irrespective of the underlying model which is to be inverted. Strengths:  The motivation behind the paper is clearly presented   the goal is to reduce the need for extensive hyperparameter optimization of an inversion system. The authors demonstrate the effects of each different augmentation used in their inversion system, both in the main paper and the appendix. As such, to fully support the argument of the lack of need of extensive hyperparameter tuning, one would also need to show that using the same regularizer (for example, TV, which can be applied to any model) with the same hyperparameter leads to more extensive degradation, when applied to different models. I believe that a small example to demonstrate this motivation would greatly improve the paper. The above point is made even more unclear by the fact that one of the models considered by the authors does use a TV regularizer (which leads to drastic improvement in image quality). **Update after rebuttal**: See response to the authors  comments below.<|endoftext|>In this paper, the authors introduce a novel method for the class inversion problem: given a trained classifier and a specific class/label, generate an example image of that class. The proposed method is robust across architectures with little to no hyperparameter tuningWeak points of the paper:  The specific method proposed is not clearly presented at a mathematical level (it is described only in words)  The only baseline comparison I saw was relative to one method (DeepInversion). The paper does not comment on methods for adversarial examples, such as adversarial patches, either as related work or as baselines. In the rebuttal, please justify why these comparisons are not present, or comment about them. Replace these with the citations. The reason to accept is: They provide a class inversion method that produces interpretable visualizations for recent architectures (Vision Transformers + MLPs) that existing methods do not work for. The authors should compare their visualization method on Transformers/MLPs to the best existing approach that exists before this work.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 6; Coming to the technical side of the paper, please consider the following comments:1) Although the abstract starts with a very ambitious note that the distributed computing framework can encapsulate "any" black box optimizer, I do not see many provisions for simpler and very often better performing algorithms like the Differential Evolution (DE). The tweakings introduced in the paper seem minor and hence, the major novel algorithmic contributions seem limited. Considering the lack of originality, inadequate match to the theme of this conference and lack of rigorousness in experimental procedures, I will have to recommend a rejection for this paper.<|endoftext|>This paper suggests Distributing Black Box Optimization (DiBB) framework that enables the running of black box optimization techniques in a distributed manner. The proposed method is not techincally novel enought and the experiment result does not fully support the effectiveness of the proposed method. This, I think, highly violates the assumption that the authors made.<|endoftext|>This paper proposes Distributed Black Box optimization (DiBB), which involves using disjoint distributed pipelines to perform CMA/Hessian based updates, over functions with assumed separability in terms of parameters. I would suggest to the authors to propose a mechanism to automate this process, as it seems like it may be a huge issue. Can you provide any form of justifications for this claim?<|endoftext|>This paper addresses the distributed black box optimization problem. 3) The line of existing works are well discussed. Also, there are some clarity issues that may need to addressed. This is a major limitation of DiBB. This poses questions on the practical usefulness of the proposed method.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; This paper proposes a false classification detection method. The generative model is then exploited for quantifying the uncertainty of the model. Pros,This paper proposes a novel approach that classifies corrupted data. The experiments provide comparisons only to MC dropout (2016) and EDL (2018). There are more recent algorithms.<|endoftext|>This paper presents a method that simultaneously classifies corrupted data and quantifies uncertainty, despite the model being fitted only on uncorrupted data. The idea is to fit a semi supervised autoencoder. Please compare the performance of the proposed approach to such selective classification methods, and use more complex data sets than MNIST.<|endoftext|>This paper proposes a framework that tries to classify corrupted data while using models trained merely on clean data. Additionally, this framework also quantifies the classification uncertainty by using the Mahalanobis distance. Some references on data augmentation: [1] [2]2) For the loss of semi supervised autoencoder (formula 1), do the two losses have the same magnitude? 3) Since the experiments in this paper focus on simple datasets (e.g., MNIST and Fashion MNIST), so it is not clear whether the proposed method will be computationally efficient for a large scale dataset such as ImageNet.<|endoftext|>This paper proposed a novel approach to classify heavily corrupted data with parametric classifiers trained on uncorrupted data. 3.An ablation study is missing. 1.Add more complex datasets, such as CIFAR10, CIFAR100. The proposed method can quantify both classification and model uncertainty, allowing for reliable detection of false classifications.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; The authors propose a derived network via using the structure regularized pruning, namely SRPN L and SRPN. Likewise for Mult addsThis paper is well written, containing extensive experiments and impressive results. For instance, the method works by selecting filters with the same indices which are connected by the same residual. This is important for minimizing performance degradation.<|endoftext|>This paper tackles this issue thus potentially can have a big impact. 6.The authors provide demo code to reproduce the results in the paper, which makes the paper more reliable and convincing. The authors are highly suggested to explain more. 1, the listed “pruning ratio” seems not aligned with the parameter reduction. E.g., for pruning ratio 0.5, the compression ratio should be 2.<|endoftext|>This paper proposes a Structure Regularized Pruning (SRP) for the super resolution task. I think the comparison with the existing Purning method is somewhat lacking. The proposed method is simple and effective.<|endoftext|>Weakness:1 In general, this paper lacks novelty. The difference might be the pruning criterion. 3 The authors claim a general idea to prune SR networks, especially for large networks. This paper is clear and well written. Finally, some experiments are missed.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; This work proposes a new architecture that adds attention to actor critic RL methods. This new architecture is evaluated on several Roboschool tasks. This work would severely benefit from adding high level motivation / justification behind the design choices of the proposed architecture. The rationale behind the design choices for the attention are also unclear. Why is it reasonable to attend over the actor features using the Q values as the key? It s worth noting that the paper does try to provide some high level motivation and states:> In the human decision making process, people often modify their concepts based on the feedback and results to obtain a better decision. Inspired by this decision making process, we use meta attention to adapt the features generated by the policy network based on the evaluation of value network. However, it s still not entirely clear to me how this justifies the proposed attention mechanism (feedback and results seem more closely related to reward signal than a critic), and the concrete design decisions remain unjustified. Additionally, the plots are cutoff while many of the curves appear to still be learning, so it s difficult to gauge if the proposed approach indeed achieves a higher final performance. I do not believe this work is ready for publication in its current form due to significant clarity issues.<|endoftext|>As such, they are not sufficient to conclude that the proposed method represents a significant improvement for actor critic methods. In the first stage, the algorithm is a standard actor critic that produces an action probability distribution and a prediction of the action value function. In the second state, the algorithm attends over the policy s and value function s hidden features. This paper proposes an attention based architecture for actor critic algorithms. The authors evaluate their proposed algorithm under DDPG and SAC on standard continuous control tasks and demonstrate that it can improve performance. The basic idea of this paper is quite interesting; it introduces an attention mechanism that tries to exploit differences in representations between the policy and the value function to improve the policy. The attention mechanism is also learned in an interesting fashion, as it is formulated as a meta learning problem: the goal of the attention layer is to be such that it improves learning of other components of the agent. Empirically, the authors demonstrate that the algorithm does yield performance gains. With that said, this paper is not ready for publication. As an example, the authors motivate their work in the abstract by the phrase "our meta attention method introduces attention in the actor and the critic of the typical Actor Critic framework rather than in multiple pixels of an image or multiple information sources". While that is valid, given that the attention mechanism is a well established concept, I would expect such a departure from the norm to be well motivated, but the authors only present it as is.<|endoftext|>This paper proposes to modify the off policy actor critic framework by introducing an attention mechanism in the actor and critic. The attention mechanism is used to adjust the actor features (i.e.intermediate features generated by the actor neural network) for better action selection in the continuous control tasks. When the critic estimation is problematic or not accurate enough, will the proposed attention mechanism even harm the performance of the standard off policy actor critic? The output from the attention network is the similarity of each feature dimension. The advantage of the proposed method seems to rely on an accurate critic network and this advantage may become a disadvantage when the critic is not good enough. Experiments demonstrated that the proposed attention mechanism between actor and critic network can improve actor critic algorithms, DDPG, TD3, and SAC. Strengths:The paper is generally well written. It is relatively novel to consider the idea of applying attention mechanism between actor and critic instead of only for image feature extraction. The proposed method can be easily used in any off policy actor critic network and the experiments show the advantage of the proposed method on continuous control tasks. Weaknesses:It seems that this work positions itself in the literature of meta learning. In this work, the attention weight is calculated for each feature dimension. Then why should some dimensions be more important than the other dimensions? What information is represented in each dimension? It is said: "meta attention network to calculate similarity of each feature dimension". Is there any intuition behind it?<|endoftext|>The paper introduces attention mechanism into actor critics method, and formulates RL as a bi level optimization to learn the (meta) attention parameters. However, the model design is not well motivated, and the optimization objective does not match the practical algorithm. Empirically, the proposed model shows improved performance over baseline methods. The paper brings an interesting perspective to RL. The attention mechanism combines the information from the actor and the critic to further refine the actor representation. This is also supported by better empirical performance from the proposed method. In addition, argmin from Eq.7 is approximated by a single gradient step. Instead, the algorithm requires \phi_old and \phi_new as input for meta training. b.The use of tanh in the meta learning loss is not a standard choice and deserves more justification. Could the authors further elaborate on this choice? 3.The presentation for the empirical results is too dense (figures only), and without any numerical comparison. It is difficult to assess how much better than proposed method is compared to the baselines. It s also difficult to assess the stability of different methods, when the figures appear to show significant standard deviation of performance for the meta trained methods. Could the authors provide numerical comparison of the experiments? Could the authors elaborate on the comparison between the two variants? Could we compare with the attention action without the extra selection step? It would be good to compare the computational resources needed, as well as the sample complexity of different methods.
Reject; rating score: 6; rating score: 6; rating score: 6; Outliers are systematic with predictable recurring patterns;3. The formulation is very clear. The performance of outlier removal is satisfactory on simple datasets. Could the author gave some insight? The idea and formulation is a little incremental and similar to RVAE [Eduardo et.al.] As mentioned above, I wish the author could answer the two following questions:1.<|endoftext|>However, the main motivations and assumptions are not well comprehensible, theoretical and practical implementation aspects are mixed, and the presentation is lacking clarity in some places. This paper presents a neural network model based on Variational Autoencoders (VAE) that learn an implicit representation separating outliers with systematic "errors" from inliers using a small labelled subset of the training data set (trusted set).<|endoftext|>Weaknesses:  While I found Section. An issue of the paper, from my perspective, is that the experiments are simulated and not real. This maybe a future work direction for the authors to consider. 2 (2020): 314 333. "Learning to find good correspondences."
Reject; rating score: 3; rating score: 3; rating score: 5; The method continually updates the shared model with data from all observed tasks and the task specific model with data from the current task. [1] Nagabandi et al.Deep online learning via meta learning: Continual adaptation for model based RL. However, the authors simply train this shared model in a multi task fashion with data from all tasks. It would be useful to include videos of the learned behaviors to assess whether the transfer results are in any way significant from a behavioral perspective, or if they re simply minor reward increases across poor behaviors. Unfortunately, I recommend the rejection of this work. While I agree with the premise of the submission that model based lifelong RL is a relevant area of research, with potential implications on real world applications of lifelong RL, the submission as it stands appears to not be ready for publication. This is not novel insight.<|endoftext|>The authors work with a Bayesian framework, assuming a hierarchical distribution of the two levels, and learn the two levels separately. Finally, experiments are provided supporting the utility of the approach. The main theoretical contribution suggested in the paper is a PAC MDP of Theorem 1 for a single task. Also, their use of model predictive control is common in current ML applications, but, again, this is not mentioned or discussed (e.g., the length of the future horizon and how is it selected). It would be nice to acknowledge these roots. However, I do not find that the level on innovation in this combination of approaches suffices for publication at ICLR, nor did I find the theoretical or experimental results of sufficient  interest (see comments above).<|endoftext|>The authors proposes a Hierarchical Bayesian approach for lifelong RL. The current version makes understanding hard. These are not just a few, but observed broadly across the entire paper, perhaps Section 3 requires the most significant improvement. It s like applying Variational Continual Learning to the world model learning in the RL setting. It can also be understood as Bayesian meta learning but with sequential task exposure. The experiment is also quite simple. So, I m somewhat doubtful if this method can be an important milestone toward more realistic and complex settings. It s currently a major drawback hindering the understanding of the proposed model (I understood at the later part of the paper but it was hard until reaching there).
Reject; rating score: 1; rating score: 1; rating score: 3; rating score: 3; rating score: 3; The authors propose a new approach to activation functions in DNNs. More precisely, the authors present the trainable matrix activation function (TMAF), which is an activation realized by a matrix vector multiplication whose entries are trainable. In [1], we find a learnable piecewise function which leads to the activation function PWLU. The authors do not mention [1] as related work and of course, the novelty of the contribution is significantly constrained. Finally, it seems like the paper only does one run for each of the experiments which might be problematic to estimate the actual performance of the activation function. Although that space might be better used for a more comprehensive empirical evaluation. (2021).Learning specialized activation functions with the Piecewise Linear Unit.<|endoftext|>The authors introduce trainable matrix activation functions, consisting of trainable matrices at the activation layers to generalize ReLU, in order to increase the approximation performance of deep neural networks. I have several concerns with this paper, which I did not find well written and significant enough for the following reasons. Moreover, I think that the contributions are not significant enough and not well supported by experiments for ICLR. Hence, I recommend rejection of the paper. The section 2, introducing the trainable matrix activation function is not written clearly. This is basically a sort of disconnected splines so I do not see the main novelty with respect to the existing literature on this topic. There should be a discussion about this in this section. In sections 3.1 3.2, the authors aim to approximation smooth functions and compare with ReLU.<|endoftext|>This paper proposes a new type of activation function, called Trainable Matrix Activation Functions (TMAF), to replace the existing activation functions in neural networks, such as ReLU. Strength:The idea seems to be new. However, the authors did not compare TMAF with most of them, making the effectiveness of TMAF questionable. So using them in the experiments is not convincing. 5.The CIFAR 100 result is missing in Table 3, making the paper look incomplete and probably written in a hurry. All in all, the paper needs a lot more thorough experiments to justify the usefulness of the proposed activation functions.<|endoftext|>The paper introduces a trainable matrix of activation functions. The authors propose to replace activation with a custom learnable piecewise linear function. The empirical results are also not well described. I don t understand the motivation in the beginning of Section 2:   application of activation $\sigma$ returns a vector, while D returns a matrix. Some other comments:  Unfortunate re use of the same variable that can be easily avoided: $f$ as a function and $f_n$ as the output. As is, the paper is very far from begin ready even for a proper reviewer s evaluation.<|endoftext|>The paper describes an approach to train activation functions. The paper also proposes a generalization where values at different neurons can be combined. The paper has several major concerns that are listed below. NOVELTY: the paper does not provide a review of related works and fails to mention connections to existing trainable (or untrainable) AFs. The idea here is to combine Swish with a piecewise constant s(x), but the way it is described in the paper is convoluted (where the original AF is replaced with D(s(x))x, where D is a diagonal matrix). Concrete results on the overhead should be given to justify this sentence.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; The approach also extends to time resolved simulation. The architecture of Pfaff et al.is generally similar to the proposed approach, they demonstrate generalization to unseen domains, and the experimental results go beyond that what is shown in the proposed method (including 3D, adaptive remeshing, and dynamic simulation domains). The authors should cite this work and explain if or how their contributions differ. In Proc.ICLR 2021. **Misc.**  The authors note that there is a periodic boundary condition for the Navier Stokes equations they solve. While the paper seems generally well written and the method appears sound and thoroughly evaluated, I m not sure what the novelty of the paper is given that the contributions overlap significantly with Pfaff et al.(2021).Moreover, the authors do not cite this paper or explain how their work differs from it.<|endoftext|>However, based on my understanding, there are a few weaknesses (and questions that could help reviewers understand the paper better). Could you be more specific with more details on the novelty? 3.3 What is the baseline that the proposed method is comparing with? How about other networks that was mentioned in the related works? Lacking these two important reference makes it hard to measure the performance of the proposed method. 3.4 While the result visualization is nicely presented, without detailed numerical presentation, I am not sure how well the performance is and how efficient the training is? I strongly recommend authors to present motivations and design of networks with more details and analysis, and the experiment requires improvement.<|endoftext|>Without these it is difficult to make sense of the number in the submission. The proposed GNN architecture makes a lot of sense. The method is well laid out and written with enough clarity that the setup is understandable and intuitive. Using a GNN on the mesh is intuitive and the particular features used are also sensible. There are also no ablations for the reader to understand the impact of each design choice the authors make. I was wondering, what are the limits on this assumption? Does the solution space have to be analytic/smooth? Experiments: It is not obvious to me what is going on in your experimental setup. — Which optimiser do you use?<|endoftext|>I would love to hear some limitations of the proposed method. The proposed method is a nice idea and has been well verified. It further shows that the trained solver can find a temporal sequence of PDE solutions in different domains. Comparisons with other methods need to be added.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 3; 2020.The proposed molecule graph generation method is flexible and achieves good experimentals results. Strengths:This paper proposes a flexible framework that can use motifs to generate molecules, as well as support atom by atom generation. This design strategy makes the proposed method can be trained using all generation steps in the same batch. Particularly, [3] has also investigated different strategies to determine the generation order, which is closely related to this work. Could the authors include the closely related work [3, 4] for references?<|endoftext|>I find the work overall convincing. This is a fundamental issue that cannot be easily addressed, and the authors assess its impacts quite thoroughly. Because more than one path can connect an initial and a final product, it is important to train over multiple pathsAmong the strengths , i identify  that the model is practical and pragmatic.<|endoftext|>To strengthen this point, I would welcome if the authors could elaborate a bit more on the quantitative comparison in in Figure 2. The paper is throughout very well written and clear. The presented method is very similar to work from Jin et al.(2018, 2020). I was also curious about the phrase "After each choice, if the currently selected atom is part of a motif, we add the entire motif into the partial graph at once."<|endoftext|>Strengths of the paper: 1. 2.The proposed method could directly start with a scaffold, which is usually used in practical drug molecule discovery. But there are still concerns about results. 5.The experiment also show that generation order and vocab size is important to the quality. A bit confusing).
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; The paper describes the design philosophy and structure of the Flashlight deep learning framework. Flashlight is modular, small, and narrowly oriented toward systems researchers. However, it has not demonstrated its capabilities in practice. This would be a strong paper if it contained a case study showing how its design contributed to systems research. Flashlight is well designed and shows promise as a research tool for systems research in machine learning.<|endoftext|>The paper proposed a minimal design API (or mostly API?) I have to admit that this paper gives me mixed feelings. It is unfortunately that such vague arguments are bad for a scientific paper. The paper highlighted the importance of them, but the contribution of Flashlight in these aspects seems minimal (just described the minimal APIs and connected to downstream implementations). Recent, MLIR made a big noise in machine learning framework community, but still, the popularity of the tool is way more important that the publication of it. I think it will be helpful to show case how Flashlight facilities other researchers by the "agile and minimal" design.<|endoftext|>**Strength of the work. However, the system is still able to outperformexisting large systems like TensorFlow. Claims need more justification with data and comparison with other frameworks. It would be desirable to quantifiably understand the reasonbehind the claim. Compared with TensorFlow where a lot of research have been done on for distributed training,why the proposed system is more suitable? ** The paper is not hard to read, but might need more data to justify many of it claims. However, in terms of paper writing, it would be more desirable for the readers and reviewers to understand the novelty and quantifiably justify the claims in the paper.<|endoftext|>This paper proposes a neural network training and inference framework aimed at framework researchers with a focus on modularity, simplicity of design, and extensability by researchers. The paper is well written, with clear background sections on the current state of framework design, and the need for a more research oriented extensible framework. While I commend the authors for what is no small undertaking, I have a few concerns:1. So the question I m left asking myself is whether Flashlight provides any significant advantages over the PyTorch C++ library for the majority of use cases. and similar for Tensorflow? It seems the Flashlight framework could be beneficial for researchers looking for a boilerplate template framework on which to build their very highly customized solutions. However, for anything other than an solution requiring rewriting large portions of a neural network framework, the researchers using this library will give up optimizations and other benefits of using a more mature library such as PyTorch C++ (for which custom ops and some other features mentioned in this work are possible without rewriting internals).
Reject; rating score: 3; rating score: 5; rating score: 5; The authors first propose to extend the definition of PI to be a union of disjoint intervals, allowing for more fidelity in evaluation. They then propose a method to explicitly generate multimodal PIs. As a result, I do not think the experiments validate the main claims of the paper. A simple procedure to do this would be to place the centre of these PIs at the top K modes of the predictive distribution. * As a result of the above issue, the authors evaluate their method using a disjoint set of PIs while evaluating baselines using a single PI. However, there is no evidence that the proposed PI generation method is any better than competing approaches, which could potentially also produce multimodal distributions. 2.The motivation for the proposed PI generation method is unclear to me. * The authors propose to use a mixture density network combined with an auxiliary NN that outputs PI lower and upper bounds from the GMM parameters. This paper proposes a higher fidelity method for the evaluation of predictive intervals under multimodal predictions. Unfortunately, the author’s evaluation setup assumes that their proposed method is the only one to produce multimodal predictions (which is not the case).<|endoftext|>The paper proposes a method to report the prediction interval as the union of disjoint intervals, in contrast with the previous methods which report a unified continuous interval. The motivation is that if the conditional density function has multiple modes, a single prediction interval may not be well descriptive of the uncertainty of the predictive model. The paper seems to be written in a rushed way and consists of many sentences which have missing words or are grammatically incorrect. Strengths:  Studies an interesting and well motivated problem. Overall I think the paper is not ready for publication and needs improvements both in technical content and presentation. Why are the experiments for the Protein dataset repeated 5 times and the other datasets repeated 20 times? I do not find the exact mathematical definition of F_i which is an important part of your approach.<|endoftext|>This paper provides the algorithm for the construction of prediction intervals composed of disjoint intervals. The algorithms are also well accommodated with the statistical or learning based prediction intervals, which contribute to the assessment of the uncertainty of prediction in general. The strength of this paper is to construct better prediction intervals addressing the multimodality of predictive distribution and show better performance compared to the previous. However, I have a doubt that the title ‘Distribution driven disjoint uncertainty estimation for deep learning” is adequate. The mixture of density networks is an architecture, and there is no solution or explanation when we use the other deep architectures for other tasks. Also, I have some issues with the proposed algorithms. 2) The $K$ in the mixture density network is a hyperparameter, and the empirical evidence for using $K 5$ is validated. Additionally, I cannot find the more significant use of disjoint PIs in various tasks. Presentation is not better.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; The paper showcases the vulnerability of existing adversarial detection algorithms. It seems like the paper is based on some pre assumptions that the detector can not work. Most of the assumptions are based on one work only: Rebuffi et al.2021.I feel the authors need to revisit the paper and literature thoroughly not only a few papers which showcase that existing defenses do not work or depend on the robsutness paper. The claims made in the paper are not empirically tested, they are mainly based on the assumptions of the authors or one selected paper.<|endoftext|>However, the theorem is only correct without considering the computational complexity and poses another question about the relation between robust classifier training and computational complexity. From the theoretical perspective, the authors show that: one can always (ideally) construct a robust classifier from a robust detector which has equivalent robustness, and vice versa. Also, the experiment part should include approximate results of the reduction steps to verify the feasibility of the theorems in practice. Overall the paper considers an interesting problem and tries to unify the detection and classifier defenses.<|endoftext|>This submission connects the areas of adversarial training and adversarial robust detection. Generally speaking, I think the conclusions in the submitted paper can provide beneficial insights for the community, and avoid overclaims in future adversarial detection research. Weakness：The construction of equivalent classifiers and detectors in Theorem 4 and 5 are interesting. However, I have concerns about theorem 4 that epsilon robust detection implies inefficient epsilon/2 robust classification. For one thing, it lacks a practical and efficient solution to find a perturbed input that is classified differently; for another, the claim is not necessary in practice, for example, if the input is rejected by an adversarial detection, practitioners can simply add random noise on the input until the new input is not rejected. The title of this submission seems inappropriate. By contrast, the detection task just has to tell the difference between adversarial examples and natural examples. However, it lacks practical solutions, which implies limited contributions. Though I cannot recommend acceptance at this stage, I will increase my score if my concerns are solved properly.<|endoftext|>The paper proves a natural theoretical result that is at the heart of robust learning. I think one should be very cautious to not overly interpret the implications of this paper, but I think the mere theoretical observation that testing and decoding in the context of adversarial learning are equivalent has a merit, that at least puts this paper on the border for ICLR. On the positive side: the connection between detection and classification is a very natural question that deserves attention. On the down side, the fact that the theorem of the paper is proved using information theoretic (rather than computationally efficient) reductions, limits the ways one can benefit from such a connection. This further limits the applicability of the results.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; This paper develops an approximate second order policy optimization method to overcome the slow convergence of policy gradient methods. The biggest issue is with this paper is that it does not offer clear support that this Hessian approximation is close when the policy is not trivially close to the optimal policy. The first is that the comparison between methods is unfair. The paper also makes it seem like it is addressing policy optimization issues for typical RL settings, but the setting of this paper is not typical in practice. First, it does not acknowledge that these results do not pertain to cases with function approximation, which is the more common case of policy optimization. Does the approximate Hessian provide a helpful update direction and step length when the policy is not nearly optimal? Why would one use this Hessian approximation instead of the diagonal of the Gauss Newton method? The algorithm does not recover the natural gradient algorithm but uses the diagonal of the Fisher information matrix.<|endoftext|>This paper proposes a quasi Newton method for policy gradient algorithm with entropy regularization, which is popular in solving the reinforcement learning problem. With various entropy functions, this paper establishes quadratic convergence rate for the proposed algorithm. The presentation is clear, but the writing can be improved, as explained in the minor comments. To complete the story, it is better to provide a bound on the difference between $\pi^*$ of the modified problem and the true optimal policy? As a follow up question, I believe there is a trade off between the asymptotic error and the convergence rate in choosing the regularization coefficient. Is it possible to make the regularization coefficient time varying so that we have asymptotic convergence to the true optimal policy, and also have improved convergence rate? What is the major difficulty there? It would be nice to have a paragraph discussing about the possibility of extending the results in this paper to the RL setting.<|endoftext|>In this paper, the authors propose a quasi Newton method for policy gradient algorithms in reinforcement learning while being entropy regularised. Although, I think this is an interesting paper I have some questions that I would be grateful if answered: 1. I found the exposition of the paper to be a bit confusing and not very clear. In other words, how is this analysis novel compared to standard analysis of quasi newton methods beyond its application to entropy regularised reinforcement learning? 3.Concerning the assumptions, is this analysis assuming convexity? Of course, in discrete cases, this can be met but it is not general. 4.Could the authors report the running time of their algorithm rather than just demonstrating iterations?<|endoftext|>The paper is concerned with infinite horizon discounted (finite state and finite action space) MDP problem with known reward and transition matrices. As said in the summary section, this paper proposes a quasi Newton method that uses the diagonal of the Hessian matrix as an approximate to accelerate the convergence of the policy gradient algorithm. It shows that under certain conditions along with close to the optimal initial policy, the quasi Newton policy gradient algorithm converges quadratically. Also, for the compared algorithms, do you also use the same initial policy? How does this paper s result differ from it? The quadratic convergence of the proposed quasi newton algorithm is interesting, but the conditions under which it holds is pretty unclear. I will wait for the rebuttal to address my concerns.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 8; Specifically, it uses two examples (one in fully connected NN and one in CNN) to show the splines visualization with different pruning rates. Inspired by the theory, it then proposes a policy that is based on calculating the cosine similarity between slope and biases to determine the importance. The experiment results show the proposed metric could achieve a similar or better accuracy with good energy efficiency in multiple datasets with both structured pruning and unstructured pruning. Pros:1.The proposed idea to explain network pruning is quite interesting and could have a significant impact. 2.The experiment results do show the proposed method could achieve similar or better accuracy with good energy efficiency. It is only through two simple examples to illustrate the relation. There lacks a systematic way how to derive which spline function could be affect the final decision boundary. Also, the early bird detection is also through two simple trajectories which are not convincing to me either. 3.To use the global spline pruning, it has to first use PCA to shrink the space. However, the dimension reduction method is crucial to calculating the cosine similarity score. Although the paper has proposed a very interesting perspective on network pruning, it lacks supporting evidence on some of their main claims.<|endoftext|>(4) Though starting with max affine spline DNs in Eq.(1), the paper only considers ReLU activations. In particular, the final decision boundary is only determined by a few splines defined by a few filters of the network. Another observation is the early bird tickets: the paper argues that the important splines do not change too much after a few early epochs since the binary activation patterns of data converge rapidly, so prunning can be applied after only a few epochs. and node/structured pruning have been widely studied and well known in recent years, this paper provides an interesting and novel perspective to relate them, i.e., explaining why a high pruning ratio can still work, early stopping for pruning, and pruning by removing redundant units. However, the main results are a little bit disappointing to me due to the lack of in depth discussion of the phenomenons. Both the early bird metric and the pruning strategy lack a strong or insightful connection to the spline partition and decision boundary discussed in the first part of the paper. Update The authors address most of my major concerns in their new reported experiments and updated draft. I expected to find some in depth analysis of why the trained decision boundary only depends on a small subset of splines and why the applied pruning method here can preserve them, which could be an important novelty of this paper, but only the phenomenon is described. (2) Section 3.2 claims that a novel metric based on those subdivision lines is developed to study the early bird ticket phenomenon but the metric is simply the change of ReLU activation patterns between two consecutive epochs. The change of partition is only reflected via a toy example in Fig.4 without quantitative analysis and in depth discussion, e.g., do all the splines or just the important ones (forming the boundary) not change since very early epochs, what happens to the partitioned regions that do not contain any training data, etc. It proposes an intuitive pruning strategy, i.e., removing the redundant units that are too similar to each other in terms of cosine similarity of parameters and difference between bias terms. In each round, its complexity is quadratic in the number of units since it needs to compute all pairwise similarities, which is also costly. In the current regime, "better pruning strategies" can not be well distinguished from "worse pruning strategies" and thus cannot provide strong evidence to show the advantage of the proposed method. They might be as efficient as or even more efficient than the early bird metric used in this paper.<|endoftext|>This paper mainly proposes a new angle to understand the deep neural network pruning based on Spline theory and propose a new pruning algorithm approach. Weakness: 1.The author claim that ‘The observation consistently shows that only parts of subdivision splines are useful for decision boundary; and the goal of pruning is to remove those (redundant) subdivision splines and find winning tickets.’, however, in theoretical part, the author didn’t provide how the proposed algorithm in detail to remove the subdivision splines. 2.When the author introduces the proposed algorithm, the author didn’t analysis if such method has the same convergence guarantee as Lottery Ticket Hypothesis. 3.In the experiment, the author didn’t consider Vision Transformer, which is an important SOTA model in image classification. The idea and the observation is interesting, and most of the experiment results are promising. However, it is unclear how the author finds the subdivision spline to remove in experiment implementation.<|endoftext|>This paper presents a novel methodology for deep network pruning from the perspective of the max affine spline. The key idea of the paper is to remove the redundant subdivision splines and find winning tickets. This is an interesting and very nicely written paper that bridges the max affine spline formulation and empirical pruning techniques. The idea of pruning the deep network by removing the redundant subdivision splines is a very novel idea. What s more, the authors interpret this idea with many vivid figures which is very readable. The authors first bridge the connection between deep network spline theory and deep network pruning. The authors provide new insights into how pruning deep network nodes affect the decision boundary. The authors propose a new robust and efficient algorithm for deep network training by effective network pruning techniques, which outperforms state of the art competitors. There remains a lack of explicit understanding of network pruning impact on a deep network’s decision boundary. The authors illustrate the effectiveness of their method from many aspects. I don t understand why the authors choose the pairwise redundancy measure in (2). Is there any other measure that may be better? Some simple analysis may help us recognize it intuitively. Is there any reason to measure the similarity between different layers by PCA or FA? However, some points listed above need to be clarified.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; They show the numerical results for these step sizes using a toy example and classification example for CIFAR dataset. They provide the convergence guarantees for SGD and SGD M using some choices of step sizes including the step decay and other cyclical step sizes. Given the fact that the step sizes scheme is a "perturbed" version of the diminishing learning rate, I see that this perturbation is the main reason behind this additional assumption, and therefore I think it can not be removed easily. The stage lengths are also chosen using some particular choices, which further restricts the setting of this paper. For these reasons, and since the bandwidth step size framework is introduced before, the results of this paper are not quite new. Hence it is not surprising that these schemes achieve the same rate of convergence as the $\mathcal{O}(1/\sqrt{T})$ learning rate in the previous work. And since the theoretical results does not address the generalization performance of our algorithms, it might be more reasonable to report the training loss in the classification examples. The authors make some improvements over the existing literature for SGD and SGD M algorithms, using a class of bandwidth based step sizes that is specified by stage lengths. The main weakness of this paper is the bounded assumption on the expected output of the problem. Since this assumption is quite strong, it is not quite comparable with the existing works for SGD without this condition.<|endoftext|>The paper proposes a unified framework to analyze convergence conclusion of SGD/SGDM with bandwidth step size including popular "constant and then drop" step size,  cosine step size and the triangular step size. Experiments justify the advantage of the proposed bandwidth based decaying stepsize over standard decaying stepsize. The theoretical analysis, particularly, the analysis for momentum SGD seems to be novel. I have not looked into the literature but non monotone learning rate seems to be not well understood previously. While it is related to many cyclic stepsizes, this assumption appears to be too general,  and is not exactly equivalent to the cyclic schedule such as (Loschilov and Hutter 2017, Smith 2017). In the toy example in 5.2, do you need to consider cyclic stepsize alternate between "large" and "small"? In the experiments, what is m and M? Since the paper discuss cyclic stepsize, can you also compare with those standard cyclic stepsize in the literature, not the stagewise stepsize? It occurs to me that the main contribution is some new convergence analysis of SGD(M) with non monotone stepsize in a multistage setting. I could be wrong, however, whether the perturbation is cosine, triangular or any cyclic rule does not really matter, which makes this work less precise in characterizing the real effectiveness of each specific cyclic rule. Although I agree with the author on the technical novelty in handling the non monotoniticity, it was somewhat misleading given the title only mention Bandwidth stepsize.<|endoftext|>They provide near optimal convergence guarantees for smooth, non convex functions when the boundary function is $1/\sqrt{t}$, where $t$ can be thought of as the epoch counter. The analysis works by dividing the total number of iterations into stages, where the stage length can either be constant or increasing exponentially. By appropriately selecting the number of stages and the stage length, they obtained optimal rate for the stage wise step decay scheme. Furthermore, near optimal and optimal rates are also provided for SGD with heavy ball momentum under the bandwidth step size assumption that covers the stage wise step decay setting. Lastly, they proposed non monotonic step size schedules (step decay with linear/cosine perturbation) and compared their empirical performance on several deep learning benchmarks. In particular, the SGD with momentum result with stage wise step decay is very close to what practitioners use, modulo the fact that the result only holds for a sampled iterate rather than the last iterate, and the use of unbiased gradients which do not apply to the epoch wise random reshuffled training. Although the bandwidth step size framework has already been introduced in Wang et al., 2021, the results presented in this paper are either new or improving upon existing rates. Overall, the paper is well organized and easy to follow. It would be great if the authors can clarify whether I m missing something here about the randomness dependence. Toy example as evidence for "bandwidth schedule helps to avoid bad local minima": I m not sure if this toy setup is strong enough to demonstrate what it s meant to show. This would help demonstrate how close the training loss approximates the test loss. The significance of the newly introduced schedules are mainly a combination of decreasing step size and existing cosine/linear rules, and it s hard to tell whether they are significantly better.<|endoftext|>For step decay SGD, the paper proves the non asymptotic convergence and an optimal rate is derived. The problem and assumption setup is proper, the convergence derivation is clear and covers multiple variants of bandwidth step size application. I have a few concerns:regarding the theory, 1. the experiments shows that the multistage gradient methods helps avoid the local minima, can you provide any theoretical inspiration for this? 3.In the toy example, it is difficult for small constant step size to jump out of bad local minima, and large constant step size achieves comparable performance with step decay methods. Therefore, in the numerical examples for CIFAR, why do you choose the lower bound step size $m\delta(t)$ as baseline instead of  $M\delta(t)$. The overall paper is well orgainzed and easy to follow. The theoretical results improves the current convergence rate than previous work. The experiments show the performance of bandwidth SGD.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; The paper proposes a novel principle for representation learning using an autoencoder, named interventional consistency. It extends the notion of consistency in a meaningful way. The paper is very difficult to follow, and I believe the difficulty is attributed to the unpolished exposition. What are the examples? Providing a concrete, real world example of interventional consistency would be helpful. The other parts of the paper also have clarity issues. I vote to reject the paper, due to the lack of clarity.<|endoftext|>This work proposes the notion of "interventional consistency" as a beneficial property learned representations should have and introduces a regularization term to enforce it in autoencoders. The paper is motivated by the goal of "causal representation learning" and explicitly mentions it does not address the important question of identifiability. I found the paper hard to read even though the ideas proposed are rather simple. Important contributions should be presented in the main text. Is it all components of $Z $ that are not $i$? Is it $Z \_{j<i}$ for some ordering of the nodes? This point matters because (4) goes on specifying what is meant for the "ICM principle to be preserved by the response map" based on this above equation. My main concerns are with (i) the lack of motivation for both the "interventional consistency" and the "explicit causal latent block", as well as (ii) the (sometimes significant) lack of clarity.<|endoftext|>The authors argue that the interventional consistency is a desired property for the representation of auto encoders and proposed a new metric on it. Some assumptions seem too strong to me and I feel the main objective of the paper is not conveyed well. 3) Assumptions:There is an assumption that the statistical dependencies in the prior are preserved by the response map. This assumption looks to me very strong. I feel the draft is not in a status for publication yet. ########Update after the authors  rebuttal###########I would like to thank the authors  explanations on the notations as well as the assumption concerns. Overall I feel this draft has potential to be a good one. As stated in the authors feedback, in its current status more work is needed to make it easy to read.<|endoftext|>The idea is well motivated by the ICM principle and theoretically justified. The paper suggests to use interventional consistency for both training and evaluation of the representation learnt by VAEs. Results show that the proposed idea can give more modular and interpretable representation. Strengths  The idea is well motivated and seems to be theoretically justified. The empirical results are promising and support the main claim of the paper. There is a broken citation in the 3rd line of the 3rd paragraph of Section 3    The first two figures in Section 5 is not numbered. The order of all methods in the two figures is weird/inconsistent. The results also support the main claim of the paper.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 5; This paper proposes a pairwise adversarial training approach for class imbalanced domain adaptation. Specifically, the adversarial samples are generated from the interpolated line of the aligned pairwise source domain samples and target domain samples. Since the target domain is unlabeled, the interpolation between a source sample and a target sample of the same class is not reliable, which may face the same error accumulation issue as the pseudo labeling methods. If the pseudo label is not correct, the augmented data may not only cause the error accumulation but also damage the domain alignment. It seems that the generation process that generates data merely resorts to the guidance of the class probability of the source domain data. The interpolation method looks quite general, will this method also work well on the UDA problem? In general, the proposed method is somewhat novel. However, the method is not very well justified and some parts of the method are unclear.<|endoftext|>The challenge lies in how to handle the difficulties introduced by imbalanced classes. To this end, this work proposes a new data augmentation strategy, that is taking the interpolation of two samples from the same class but from different domains as the augmented samples. The interpolation based data augmentation method is interesting. 2.The proposed method does not outperform the baseline Sentry. 3.Some statements are based on intuition but not well supported.<|endoftext|>This paper proposes a new method called Pairwise Adversarial Training (PAT) that augments training data for class imbalanced domain adaptation (CDA). Different from vanilla unsupervised domain adaptation, the label distributions of different distributions are quite different in CDA. The proposed PAT approach mainly consists of two part, centroid alignment (CA) and interpolated adversarial samples (IAS). Experiments on several benchmarks verify the effectiveness of PAT for the CDA problem. "Do we really need to access the source data? And the results in Table 4 only show 3 out of 6 tasks on the OfficeHome dataset, and the contribution of IAS seems not significant (I think IAS is main contribution of this paper). The strategy of interpolated adversarial samples (IAS) is new and interesting, however, the overall novelty of the proposed method sounds not much high for a top tier conference.<|endoftext|>This paper proposes an adversarial data augmented method to solve the class imbalanced problem in domain adaptation. This paper solve the class imbalanced problem from two aspects:1. pairwise adversarial training and generate the samples on the interpolated line from a source sample to a target sample of the same class. And samples from the minority class will have larger chance. 2.Align the conditional feature distributions of source and target domains by explicitly matching the centroids of two domains. However, my major concerns are as follows:1. 2.The improvements of the experimental results are not obvious. The novelty is limited and some statements should be explained further so that the advantage of the proposed method could be more obvious.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; The method uses pseudo labels, derived from a surrogate preference model. The semi supervised learning approach uses the preference classifier for deriving high confidence pseudo labels, that can be used as additional, supervised input data. Especially, in sparse reward domains, this is not a given. Advantages of the proposed method are clearly visible. The proposed method is sound and interesting, but of limited novelty. The paper is clearly written and can be easily understood.<|endoftext|>The methodology introduced by this paper is inspired by related studies in supervised learning. I think it is quite important to understand why and how these techniques benefit RL algorithms. Why do we need to train our reward model on these sample pairs with extremely high confidence? The proposed method is motivated by the success of data augmentation in the supervised learning area.<|endoftext|>Strengths:s1) The paper proposes a semi supervised reward learning pipeline aiming to reduce reward engineering efforts, which is an important topic. If multiple seeds are used, do these reward functions generate similar results on the samples? This work introduces a semi supervised reward learning approach to reduce the efforts of reward engineering. However, the technical novelty is limited. Some assumptions made by this work are too strong and may not hold.
Reject; rating score: 3; rating score: 5; rating score: 8; rating score: 8; Experiments are performed on artificially generated nonlinear sequence mixtures, RF sensing data and an EEG EOG dataset. This should be supported later in the experiments (which the authors did not), by discussing computation costs of the proposed algorithm, with respect to e.g., ICA. While it is successfully shown that sVCD can perform this, it is not a meaningful experiment to demonstrate. It is also not clear how did the authors implement ICA (as a comparison baseline) in their experiments? The paper is clearly written and organized.<|endoftext|>The paper poses this as a source extraction from a nonlinear mixture. Generally the notation follows MATLAB/Octave but it is not clear to the reader. The authors have performed additional comparisons that help set the context. I agree the concerns with non causality may not be valid in all settings, and the additional testing on performance at shorter time segments is important. 1.Much of the time series aspects are ignored.<|endoftext|>I think the paper is good for acceptance, but I think it would be interesting if the authors could also relate their work with Sparse Coding and not just non linear ICA. The paper introduces a novel application of Seq2Seq and variational inference and the results show that the method s performance is superior to other state of the art methods for a restricted version of Blind Source Separation where only one source is extracted from a non linear mixture. My recommendation is to accept this paper for the conference. Although the techniques themselves are not novel, the application of these techniques to solve the non linear Blind Source Separation problem is somewhat novel.<|endoftext|>Additional contribution:* lower bound of variation for the new architecture is provedStrengths1] Far above average clarity and overall writing quality and completeness2] In section 5.4, the Ablation Study explores limits of the Gaussian assumptionSuggestions1] "source" <  "sour" at the top of page 22] Try comparing against more advanced technique (whther another deep learning model, or an algorithm that has been tuned to the specific application)The authors make a strong theoretical justification of the model approach to source extraction from time series.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 3; I think it should be clarified that the interventions are not independent (i.e.different interventions w_i^k could be operating on some common set of edges), and that the earlier layers are ancestors of the later layers. The \tilde{y} notation is a little confusing.<|endoftext|>The authors need to explain the process more rigorously. What are $w_0^{11}$ and $w_0^{21}$?<|endoftext|>For example, it contains the sentence "The subscribe number indicates that this is the input layer l_0". All Figures in the manuscript are very hard to read as they use extremely small font sizes. I will try to summarize this in detail below, but before doing so I do want to encourage the authors to invest more time into this work as it is underlying idea is quite interesting.<|endoftext|>3.In Proposition 2, the authors name CS_i as the causal effect. In weighing the pros and cons of the paper, I believe that the current version is not ready for formal publication. The problem raised by the paper is well motivated.
Reject; rating score: 3; rating score: 3; rating score: 6; This paper proposes an approach based on MuZero to learn strategies to enhance the search query. The approach is tested on NQ dataset. + The idea to built a structured query following a grammar is interesting. The grammar corresponds to those used in Lucene (and other search engines). The complexity of the method is very high, making it difficult to use in practice. It is unclear if the grammar defined in this way is reasonable. So it is quite general. This second part is not taken into account in this paper, or only partially taken into account. The retrieval using BM25 is a strong limitation to the approach.<|endoftext|>Does this mean that the proposed NDCEM does not really benefit the retrieval performance? The action space involves adding keywords to the query or form search terms with three operators that constrain the search space. The main motivation of transparent and interpretable agent based query reformulation could be a significant contribution but would be better highlighted with experiments on additional datasets, further discussion on the complexity of the method and qualitative results that show the enhanced interpretability over prior "pure RL" works. **Strengths:**  This work is generally well written with good coverage of the literature.<|endoftext|>This paper investigates a vital direction: How can RL agents learn to use search engines to find information. The method performs comparably to neural retrievers on OpenQA NQ but operates in a more interpretable way. The paper is well written and easy to followWeaknesses  The results of the paper might be hard to reproduce though the authors have provided enough details.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 5; This paper introduces a new RL benchmark, Crafter, which is a simplified version of Minecraft. With Crafter, the authors aim to provide a benchmark that isn t as memorization heavy (even though they still claim that memorization is one of the main research challenges of Crafter in Sec.3.4).### Quality / ClarityOverall, the paper was very well written and clear, and was a pleasure to read! One of the main selling points of Crafter is the relative sample efficiency of training relative to other benchmarks. I understand that this is to take into account sample efficiency – but that is already being forced by capping the training to 1M timesteps. It would be useful to have some additional clarity about why this is. The low amount of compute required for training (while preserving environment and potential behavior complexity) is worth emphasizing, as it could speed up the rate of progress of further research in this space.<|endoftext|>The work introduces a gridworld style benchmark inspired by Minecraft. Strengths:    The paper is clearly written, and the evaluation appears sound. The computational demands of MineRL can be challenging especially for researchers with limited compute resource. It s significantly less complex in terms of both visuals and game mechanics than MineRL, so I expect many researchers to continue to prefer MineRL. I am concerned that RL algorithms are being trained to maximize the reward but being evaluated on a correlated but distinct metric (aggregate score). This applies to the many subsequent uses of "it" to refer to the player as well.<|endoftext|>This paper introduces a new environment for development of agent capabilities, called Crafter. So overall I m lukewarm on Crafter   I could see it possibly appealing to a subset of the RL community and perhaps helping to drive research in particular subfields of RL, but this is just my guess. Would these new agents be generalizable beyond Crafter to other domains and applications? As a community, would we be impressed by agents that could complete the full set of achievements on this domain? There interesting aspects of the environment that are novel such as the need to find food/water/shelter/sleep. I d love to hear from someone who works on RL in Minecraft/MineRL as to whether this environment would be welcome and useful in the space.<|endoftext|>This paper presents a new RL environment called Crafter that is somewhat inspired by the RL environments based around the Minecraft game. # Strengths  New RL environments such as this are always welcome. The authors have provided code. The paper presents a number of benchmarks against with known RL algorithms. The procedural generated environment every episode to support learning of general policies is very nice. # Weaknesses  It is not clear what the actual novel contribution of the work is. It would be nice to see how the learning changes as the action space increases. The paper is well written and presents a new Minecraft inspired environment for RL research which has some novel features.
Reject; rating score: 3; rating score: 3; rating score: 8; rating score: 8; As the authors mentioned, there are a lot of previous results on distributed solvers for SVMs. It seems to me that this paper proposes a distributed solving method for a similar problem to SVM(ODM). The authors should clarify the differences between the proposed method and previous results on distributed SVMs, so that the technical contribution is clear.<|endoftext|>However, with non linear kernels, the proposed methods improves over some baselines. Currently, it seems that the paper is presenting a distributed algorithm, which is not the case. Overall, the methodology seems to have limited novelty and limited potential for improvement over baselines.<|endoftext|>Strengths:Overall, the paper proposes a solid contribution to a problem with broad applications, e.g., classification.<|endoftext|>Pros1.The paper proposes a novel partition strategy for training kernel method in distributed scenario, leading to nearly ten times speedup. Generally, this is a solid work for accelerating ODM. The paper also presents the theoretical analysis inspiring the partition strategy, and extensive experimental results verifying the superiority of the proposed algorithm. Considering all these, I vote for acceptance.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; The paper proposes such a technique and do experiments for three text classification tasks and various adversarial attacks. Strengths  The problem being addressed is cutting edge (we are barely understanding prompt tuning, and this paper already jumps ahead to adversarial defenses!) Clear logic motivating the problem and what constraints need to be accounted for while solving it. Clear research question. The other thing is that it is hard to contextualize how good the numbers in your framework are. Other comments  I think the paper may benefit from spending just a little more time describing how susceptible prefix tuning is to adversarial attacks compared with regular finetuning. It will be good to see if the proposed method not only improves performance against adversarial attacks, but also if it improves performance against different paraphrases of expressing the context and question, for example. The paper proposes a method that maintains the advantages of prefix tuning against finetuning, a formidible problem. The characterization of the method could also be further improved by providing finetuning (with and without defenses) as baselines.<|endoftext|>This paper introduces a tweak to Prefix Tuning to make it more resilient to adversarial perturbations of the input. The idea is to add a batch level prefix at inference to the original one which enhances robustness. Same goes for comparable approaches such as P tuning. There are still some concerns over the threat model but some of my questions on the impact of test batch size have been answered. The motivation, experimental settings and results look good overall. One major caveat however is that it is not clear how flexible the inference setup is. I have highlighted experiments that would provide more realistic results. This is my **key concern with this paper** as it brings up quite a few issues that should be mentioned. Reject is too harsh for a paper that is otherwise promising. How does performance vary with inference batch size (one or two settings should be enough)    Does the method work when only x of N samples in the test batch are adversarial?<|endoftext|>The paper investigates the robustness of prefix tuning methods and proposes a simple yet effective method to improve the robustness. The experiments show that the proposed method can largely improve the performance in adversarial settings and slightly improve the performance in clean settings. The authors study an important and novel problem. The proposed method is shown to be effective even combining with adversarial methods. The authors argue that robustness is important for lightweight tuning methods. But I still think it is better to provide a comparison between lightweight tuning methods will full tuning methods. The proposed method is simple and clear. The experiments justify the effectiveness of the proposed method.<|endoftext|>This paper focuses on improving the adversarial robustness of prefix tuning (Li et al.2021), which is a recent parameter efficient tuning method. Adversarial robustness is an important problem and has not been explored much for relatively new prefix/prompt tuning approaches. Thus the topic of this paper can be of interest to a general audience. Also, this paper is timely given the recent attention on prompts. 3.The experimental results are strong. What is the batch size at test time tuning? This is an important point to assess whether the experiments are in an online setting where test data arrives in stream or not. As said above, it is ok to do it this way in the appendix, but using over one page of the main content for this is not convincing to me. For LM, I think that the diagonals in the attention figure should be zero, or am I missing something? Also, the presentation could be improved here, in the figure caption you can explain what the rows and columns mean in the visualization to make it easier to read.
Reject; rating score: 1; rating score: 3; rating score: 3; This paper investigates the properties of adversarial examples from frequency and spatial perspectives and claims that standard models are vulnerable to high frequency perturbations and adversarially robust models are vulnerable to locally consistent perturbations. However, similar analyses have already been shown in (Yin et al., 2019, Wang et al., 2020a, Tsuzuku and Sato 2019). .The paper s assumptions are not well defined and its claims and limitations are not clear.<|endoftext|>Strength:+ This paper studies an interesting problem: the connection between adversarial perturbations and local properties of adversarial examples (how perturbations associate to image shape). The explanation of the relationship between frequency and spatial domain is not quite clear to me. This makes the paper a bit vague to understand. [1] Yin, Dong, et al."A fourier perspective on model robustness in computer vision." arXiv preprint arXiv:1906.08988 (2019).<|endoftext|>This paper provides a set of empirical studies of the spectral and spatial properties of adversarial examples of deep neural nets (DNNs) classifiers. The studies illustrate that standard DNNs are much more sensitive to high frequency components of adversarial examples compared to adversarially trained DNNs, and also that the adversarial examples corresponding to the latter exhibit more local consistency in the spatial domain. **Concerns and Questions:**1  A major concern with the paper is that it does not read well, contains many confusing statements, and as such I think it needs a major editorial fix. Over how many samples?
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; Empirical analysis is not very insightful, and the contributions are incremental. In this paper, linear chain Conditional Random Field (CRF) model is considered with a number of hidden states which is relatively small w.r.t.number of words, with each hidden state representing a set of words. Just like word indices are discrete, one can think of the indices of the states of CRF as discrete even though the hidden state vectors are high dimensional continuous vectors, hence the authors project this approach as of  discrete latent structures. Even empirically, I don’t see any interesting insights from the analysis. Further, since CRF models are trained using dynamic programming which is efficient in itself but it has quadratic cost in the number of states. This approach is just a heuristic, with many possible flaws. Sampling strategy is not explained well, and I believe challenging to obtain for NLP like high dimensional problems. It is also worth noting that the proposed sampling approach is not novel but explored in related problems. Theoretical analysis on zero bias is not enough, and there isn’t sufficient analysis for ensuring low variance. Probably, this is a preliminary work, and it could become a good paper with lot more research efforts put into it. I don’t see much justification in mapping 32k contextual embeddings to 2k latent embeddings to build a so called latent representation.<|endoftext|>This paper proposes a new way to greatly reduce the memory requirements of inference on discrete latent variable models (LVMs). "we extract CRFs on the fly from different LVM training stages." This technique is generally applicable to many dynamic programming algorithms and enables scaling LVMs to much more states by reducing memory requirements. 2.Table 1 PPL, I assume you meant exp  ELBO for your models since true PPL is intractable under the generative model? This paper is well organized, has a novel contribution and I believe it will be of interest to various subgroups in the ICLR community. I d recommend the authors to split this paper into two papers. For example, the first paper can be an in depth analysis of the proposed randomized dynamic programming algorithm with applications in different domains and different sampling strategies, and the second paper can be about structure discovery in pretrained models with a correct setup. In its current form, I am leaning towards rejecting this paper due to the issues mentioned before. Based on existing results in the paper, I cannot tell if it s because the approximation is bad, or because LVMs with more states do not get better performance, but either case is evidence against this work (in the former case, the proposed approximation is bad even though it s better than top k; in the latter case, why would we want to scale LVMs then?). 2.The paper only focuses on a single application of the proposed randomized dynamic programming technique. 4.The paraphrasing experiment is not very clear. It would be nice if you can compare to existing unsupervised clustering works such as Brown clustering which has a similar probabilistic formulation (https://github.com/percyliang/brown cluster). 2.While the algorithm is inspired by randomized automatic differentiation, a method that provides an unbiased approximation of gradients, this method uses biased gradients of an unbiased approximation of the partition function?<|endoftext|>By constructing an unbiased estimator for the partition function with subsampled DP paths, the authors are able to reduce the memory complexity of forward backward computation on Linear CRFs by 2 orders of magnitude. State word connections in the representation space provide linguistic information that encapsulates syntactic and semantic roles and state state connections correspond to the construction of phrases. Generalizations to different graph structures are discussed in the Appendix, I believe the proposed approach could be extended to tree graphs straightforwardly  As expected, memory and runtime for RDP from Appendix section E.4 are low (equivalent to TopK which sets $K_1   K, K_2 0$)  The proposed approach provides insights into linguistic roles and compositions without supervision unlike many preceding works which utilize supervised probing Weaknesses:  To the best of my knowledge, the RDP mechanism detailed in this paper combines two well studied approaches (i) importance sampling (i)TopK budget [Sun et al.2019].As a result, the methodological novelty of the sampling scheme is somewhat limited. As discussed in the paper, most linguistic information seems to be captured by 500 states. It would be helpful to compare experimental results for state word and state state relations among the Full, TopK, and RDP approaches which should all be feasible for 500 states. In particular, as seen in Figure 2, the benefits of RDP seem especially beneficial in Dense distributions. Marginal performance benefits in terms of Test PPL on MSCOCO [Table 1] and on Paraphrase generation on MSCOCO [Table 2] when compared to TopK and Full (for small # of states). Interesting approach for inferring large scale structured latent variable models in contextualized representation space. Empirical results are insightful but methodological contributions seem a bit limited in the context of prior literature.<|endoftext|>It combines the randomized summation, an idea used in [1], to the problem of Forward Algorithm, by viewing Forward Algorithm as a sum product algorithm applied over the consecutive time steps. Such randomization is proved by the authors to be an unbiased estimator for the true sums. Although the authors frame it in Linear Chain CRF problems with pre trained language model for contextualization, I think it could be conceptually expected to a broader spectrum of sum product algorithms (the author briefly covered that in the Appendix), although this may be beyond the scope of a single paper. # Weak Points:There are a few missing pieces in this work which should be better addressed. The first is the difference between the true proposal $\~{a}$ that is proved by the authors to lead to an unbiased estimator but cannot achieve improvements in computational efficiency, and the practically used proposal as in Eq.10.Although Fig 2. shows some distributions by using the practically used prospal that show no bias, it remains unclear how much it differs from the true proposal. However there is no such experiment showing what would be happen with larger $N$. Finally, the experiments visualizing the latent space are for the proposed method only. Are they chosen in some particular way? Maybe “the more $q_i$ correlates with $a_i$, the less variance $\hat{S}_2$ has“ can be explained in more detail ? In Figure 3, subfigure B1 and B2, states and words are shown in the same space. Still, I would like to keep my recommendation "6: marginally above the acceptance threshold", since (1) the contribution on RDP and that in linguistic application are entangled and do not provide a clear message, and (2) as issues in my reviews and other reviews remain. I think this is an interesting paper and is above the threshold of acceptance since the presented method is somewhat novel and inspiring.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; This paper proposes a dimension reduction technique for private machine learning called random freeze. The basic idea is to randomly select a subset of model parameters and zero out corresponding gradients during the training process. Weakness:  The experiments do not sufficiently demonstrate the benefits of the proposed method. The paper argues that, because of the use of momentum in gradient updates, the noise added in previous iterations has residual effects in later iterations. Thus, while the concern raised by the paper is a valid one, I am not sure the proposed method is the right solution. Discussion on related work is missing. Therefore, it does not meet the standard for publication at a top conference like ICLR in my opinion.<|endoftext|>In order to improve the accuracy and reduce the communication cost, this paper proposes to randomly freeze a progressively increasing subset of parameters, which results in sparse gradient updates. Empirical results also show that the new algorithm can largely reduce the communication cost, while maintaining the performance. This paper proposes a new algorithm of differentially privately training neural networks, which randomly freezes a progressively increasing subset of parameters. First, the idea of the random freezing is not complex. Besides, this idea is not fully novel, at least it has appeared in other papers. Consider the following example, in each iteration, each coordinate is 1 with prob 0.5; else 0. 3.Some related work is missing, at least the paper I just mentioned, and this paper (https://scholar.google.com/citations?view_op view_citation&hl en&user m8NUgw0AAAAJ&sortby pubdate&citation_for_view m8NUgw0AAAAJ:u_35RYKgDlwC).<|endoftext|>This work introduces the idea of randomly pruning gradients in order to reduce the dimensionality, which, in turn, results in greater efficiency for DP SGD. The work provides an empirical analysis to support the method. Without a theoretical or first principles explanation, this paper relies mostly on an empirical analysis. Furthermore, the connection between Theorem 1 and the proposed method of random freezing does not seem that solid. For the systems analysis, the authors do not report two key quantities: (1) the memory consumption (in bytes) of the method and (2) the runtime of the method. For (2), it seems that although a sparse representation can reduce memory, I am not sure how the runtime of the sparse gradient computations might also be affected here in the DP SGD. It is important to also benchmark and report this, or at least mention that the runtime is not significantly affected if that is the case.<|endoftext|>This paper proposes to use sparse gradients in combination with differential privacy techniques in order to mitigate performance drop that comes from applying DP to a large number of parameters. Weaknesses:  The appendix proves a result for the MSE, but there is no insight or discussion into how this result would look for the proposed method. Relatedly, the conclusion mentions "... random freeze can reduce the computational cost and memory footprint of the power method in GEP" but this is not explored in much detail in the results. In the results in Table 1 the proposed method does not improve on the accuracy and a significant accuracy improvement is also only shown for one model. Interesting idea but numerical experiments are not yet fully convincing and a lack of theory and discussion
Accept (Spotlight); rating score: 8; rating score: 6; rating score: 6; rating score: 5; In this paper, they introduce relational multi task learning in which they construct a graph with the data points and task and labels as edges. Then they solve the link label prediction problem between each node and task by using GNN and heterogeneous message passing method to predict the label of test data point on a new task at inference time. Strengths:  The paper is very well organized and well written. I really liked the idea of creating a knowledge graph and then solve the link label prediction problem. Specifically how relational ML could take advantage of more related tasks. Notes:  In figure 1, in the right panel, I would put 1 s and 0 s in a right place, e.g., 1 s for the solid lines or 0 s with the dashed line?! I know that I have seen figure 2 somewhere before, but do not remember the paper, if you have seen this figure from another paper, that will be a good idea to put the reference.<|endoftext|>The paper exploits the multi task modeling using a heterogenous Graph Neural Network. The key contribution of the work is having both data (sample) and task nodes in the same graph, focusing on data task relation, accommodating for sparse task labels by design. Experimental results show relevant improvements with the proposed relational model on biomedical datasets and Imagenet splits for few shot. Is there some consistent explanation for this hyperparameter? Such an example: in c) and d) why are data nodes (the ones from the triangle structures) useful if they are already connected via known label edges? How could the authors adapt the proposed MetaLink to work with dense predictions rather than 1 D tasks? What would be the implications? The work also offers an approach for the common labeling limitation in multi task problems (by working with sparse labels in a dynamic graph architecture ).<|endoftext|>The paper introduces a novel multi task learning framework called MetaLink that takes the opportunity to utilize the auxiliary task labels by constructing a knowledge base with the tasks and data(instances) as nodes and auxiliary labels as the edge labels between them. This method shows significant empirical success Strengths:1. 2.I find the idea of creating the knowledge graph for harnessing the relational information about the data and tasks very intreging. Weakness: 1. The writing could have been more precise in some parts of the paper. in the introduction itself. and explained why their work is different from the others. 3.The tasks are very specific to multilabel classification.<|endoftext|>According to the setting introduced in Section 2, it seems that the proposed setting is just a meta learning setting but not a multi task learning setting. The name of relational multi task learning seems a bit misleading. The knowledge graph stated in this paper is just a graph. This is different from the actual ‘knowledge graph’ and this seems misleading. The heterogeneous GNN used in Section 3.2 seems no difference with the conventional GNN. In experiments, only multi label datasets are used. More standard benckmark datasets should be used.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; rating score: 6; It is also a strength that the paper presents experimental results on a variety of benchmarks with different types of data, though it should be noted that all benchmarks are small scale (which should be sufficient for the purposes of this paper). The paper is well written and the presented architecture is intuitively appealing and supported by reasonable theoretical results.<|endoftext|>The paper tries to propose a new recurrent architecture that could address the well known issues (of recurrent models) like vanishing gradient and exploding gradient. The architecture is a kind of a realization of numerical discretization of ODEs using an implicit explicit time stepping scheme.<|endoftext|>The key novelty is on the multi scale aspect of this system. What is meant by "LEM is gradient based" and "gradient based architecture"? I would refer to the model in the way I did in the summary of the paper   you re giving a circuit for the model architecture which is defined by discrete time updates of a system of ODEs.<|endoftext|>The authors propose a new RNN architecture, long expressive memory (LEM), motivated by a system of ODEs with multiple time constants. What exactly is being ablated? I would also note that there exist approaches to RNN analysis and modification entirely outside the  vanishing gradient" framework, e.g.those based on statistical definitions of long range dependence.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; I think this practical mind is great. + It also shows good generalization of the finding. E.g.it can be transferred to more recent models (PiT), other tasks (object detection, video). This suggests solid improvement. It has some good findings, it shows good numbers, and it also teaches us something. However, the method (for search) is based on existing work, the application to other devices is quite straightforward (though surely requires a lot of hard work), and to me, it is more like a technical report about something is well executed, rather than an interesting "finding" that has a good deal of scientific merit. I think this paper is more on the "technical report" side, with quite limited novelty. It definitely teaches me something and presents some interesting data points, but to me the scientific merit is not enough to warrant a publication.<|endoftext|>It is clearly shown that optimization for a specific hardware results in a better corresponding model. Improvements are shown over the baseline Deit models and are up to 1.9% better. Will be interesting to look at the updated figure with recent models. Existing calling methods for ViT. in section 3.2 authors first talk about CNN scaling strategies and conclude that those applied to ViTs can lead to ambiguity and sub optimal performance. The method of the search is not well explained (it seems to be the previously proposed method).<|endoftext|>A more detailed comparison with [1] is required. Compared with the existing ViT variants, the proposed method achieves better performance. Strength: This paper analyzes different scaling strategies of vision transformers and proposes a hard ware scaling strategy for vision transformers.<|endoftext|>This work presents a study for exploring hardware aware ViT scaling and demonstrates that a simply scaled vanilla ViT model can achieve a comparable accuracy efficiency trade off as compared to dedicatedly designed ViT variants. Furthermore, they discuss the transferability of the scaling strategies on different hardware devices, ViT variants, and computer vision tasks. Therefore, it is recommended to accept this paper.
Reject; rating score: 3; rating score: 6; rating score: 6; rating score: 6; The paper introduces Finite Volume neural network for modelling fluid dynamics inspired by the finite volume method. The work demonstrates more precise fluid simulation than the related physics inspired models. **Weak points:**The paper requires more explanation on the structure of the model. Based on figure 1, the right hand side of the equation is a combination of four neural network outputs. It would be helpful to emphasise the differences between FINN, PINN and PhyDNet and other models in the methods section. Some of this explanation is provided in section 4.1, but it is worth emphasising and explaining more how the models differ, what exactly is modelled by a neural network and the motivation why FINN performs better than other models. **Questions:**Page 7: “PINN requires complete knowledge of the modelled system in form of the equation”. Can you provide some insight why FINN performs better than PINN in this case? Is it due to the fact that PINN also models the function u(x, t) or because of the bigger network? As the paper requires a significant re write, I suggest a reject.<|endoftext|>In this work, the authors propose the finite volume neural network to solve advection diffusion partial differential equations. This is a very concrete paper with solid experiments. To better assess this work, I would like to ask several questions:1. Therefore, I think it could be valuable if the authors can add some numerical experiments comparing against the numerical solver. 4.Standard advection diffusion equations are relatively easy to solve. I wonder if the idea proposed in this paper can be generalized and transferred to other PDEs? I think the paper is above the threshold. If the authors can provide evidence that the proposed method outperforms (or has some relative advantage) compared to standard FVM solvers, I will be happy to raise the score.<|endoftext|>This paper presents a compositional physics aware neural network (FINN) for learning spatiotemporal advection diffusion processes. It claims that the FINN outperforms pure machine learning and other state of the art physics aware models in all cases—often even by multiple orders of magnitude. However, the design of the network depends too much on the form of the equation, which leads to a very narrow application of the method. StrengthsThis method can not only deal with smooth solutions of general PDE but also can deal with weak solutions of hyperbolic problems, which is a good point; It is also compared with many recent SOTA methods and is significantly ahead of recently published methods WeaknessesThe method limits the form of the equation, which will greatly increase the training complexity when it is extended to higher dimensional problemsThe problems studied by this method are relatively simple and are the fitting of some linear problems or low order nonlinear problems. I am wondering if polynomial fitting will get better results. Such as equations containing an exponential function, the fitted coefficient containing singularity, or can this method be used for fluid equations, etc.<|endoftext|>The authors propose to model advection diffusion partial differential equations as a composition of multiple neural networks. Extensive experimental results are presented to support the author’s claims that their framework performs better than state of the art. The authors propose very detailed experimental results, with both synthetic and real data2. 3.The training process is not completely clear, the authors describe the forward process, one would assume backpropagation of error for updating the neural network, but this is not explicit in the text. At the end of section 3, the authors claim to use NODE in place of Euler for the reason of numerical stability, however, NODE also used Euler and does not mention anything about adaptive time stepping, more details on this part would help to clear things upThe paper has very thorough experimental results and good descriptions of the methods. However, the claims made in the paper may need a second look/ need some rewriting.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 8; Moreover, when pixels are missing, SLAHS+PC has much better performance than DeepProbLog. It leads to interesting experimental results. is not clear: which predictions were removed and how were they chosen? This should be clarified. Definition 3 states that the probability of a set of queries is given by the product of the probabilities of the individual queries: this is true only if no pair of queries depends on the same NPP. The theoretical treatment of learning is confusing: the authors state that they want to maximize the log likelihood of a set of queries but then they state that they learn from a set of pairs (Query,hat p) where they want to match the probability assigned by the model with the one specified in the pair.<|endoftext|>The ability of SLASH of handling missing data in a principled way is indeed a welcome feature. I found the paper too hasty on the latter aspects and I think that some examples could definitely help. If that s not the case, it would be useful to mention why.<|endoftext|>It includes :  a NPP: which is a standard NN decription language Reading the paper, it looks like it can describe any NN, but  you also use  an ASP program, that computes probabilities by ASP eval. thing is trained using grad ascent   the initial probabilities were obtained from? Small questions: gradient for the solution does not have to be positive: zero? I think this is a nice paper, suggesting the logic program provides strong background data for the network and that the gradient based search can  provide good prob  estimates. The one flaw in this paper is that you only compare with DeepProblog, You drop NeurASP after the first table In general. it is hard for me to fully accept your claims :(<|endoftext|>This paper presents a new deep probabilistic programming language based on ASP. Using generative probabilistic circuits allow to cope with missing data. I think the paper can be safely accepted. In S3.1, I believe that the generative nature of SPNs is crucial and this point should be better stressed.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 8; This paper claims that existing BNNs embed 32bit features, which can improve the accuracy of BNNs but must lead to overheads during inference. In the proposed network, full precision batchnorm layers are replaced with  shifted sign  layers, full precision scaling factors are eliminated, and multi slice binary convolutions and 1b shortcuts are used. This paper refers to a previous study (Fromm et al., 2020), but in that paper, only squeezenet was evaluated. To strengthen the motivation of this paper, this paper also includes simulation results on energy consumption, but it is a little hard for this reviewer to accept the simulated results because 1) a few networks are compared with the proposed work and 2) simulation results strongly depend on assumptions about h/w implementations. Since current BNNs show better accuracy than the proposed work, this paper should strengthen and prove the motivation, why we need to eliminate the 32 bit features from BNNs completely, with deeper analysis. This paper claims that existing BNNs use various 32 bit features and proposes various techniques to make neural networks that consist of only binary operations. However, some ideas seem to be similar to previous works. In addition, for some ideas, there seems to be a lack of comparisons and citations. The proposed shifted sign idea looks similar to ‘Batchnorm activation as Threshold’ (Umuroglu et al., 2017). Multi slice binary convolution layers seem to make a significant impact on improved accuracy. But this idea looks similar to ABC net (Lin, 2017) and GroupNet (Zhuang, 2019). While ABC net is compared with some descriptions, the GroupNet (it may be improved than ABC net) is not compared and it is simply mentioned just one time. Figure 4 uses ReActNet(Bi Real based) for comparison. Why don’t you use ReActNet A that shows better accuracy and lower FLOPS? Indeed, the GitHub repo of ReActNet also includes ReActNet A. Minor    Page 4 : with an running → with a runningUmuroglu, Yaman, et al."Finn: A framework for fast, scalable binarized neural network inference." 2019.As this reviewer mentioned, a more concrete analysis for the motivation should be added and comparisons with previous BNN architectures are required. The current manuscript seems to be lack novelty in this reviewer’s opinion. But, this reviewer is ready to listen to other reviewers’ opinions.<|endoftext|>The paper proposes two novel BNNs (BaseNet and BooLNet) where most of the parameters are represented in the binary formats which is the major difference compared to pervious works where activations in layers are represented in 32 bit floats. Moreover, the proposed networks removes the BN and most of ReLu activations and replaces with sign function. To increase the knowledge capacity, the authors uses Multi slice Binary Convolution and Local Adaptive Shifting approaches. The inference performance of BaseNet and BooLNet are evaluated and compared with the inference performance of ResNet 34 using 32 bit floating point format. Strength: 1  Reducing number of FP32 operations in BNN that reduces the energy consumption. 2  The author evaluated the effect of various approaches such as Multi slice Binary Convolution for BNN on accuracy, memory footprint and number of operations ( the ablation study on ImageNet and ResNet 34 is interesting)  Weakness:1  The novelty of paper is not obvious. It seems the author uses pervious approaches and combines it with BNN to improve accuracy. 2  The motivation behind using sign function for BN is not explained in the paper. Please elaborate this. 3  In the table on the Figure 4, to have a fair comparison, is distillation and long training epoch considered in pervious works? 4  The author did not compare BooLNet with reference [1] approach which also uses Multi slice Binary Convolution approach. "Least squares binary quantization of neural networks." 2020.5  The BooLNet is only evaluated on ImageNet and ResNet 34 and it is not obvious that this proposed work can be generalized for others datasets and networks. As this reviewer mentioned, the motivation to use only binary network is good research direction, However, the novelty of this paper is not obvious and the BoolNet needs to be evaluated on other benchmarks. I recommend this paper to be rejected.<|endoftext|>This paper proposes methods to further reduce energy consumption of binary neural networks by removing or replacing 32 bit components (e.g., skip connections) in SOTA BNNs. More specifically, the proposed architecture (1) reduces the precision of skip connections and activation functions, (2) transforms the BatchNorm layer into a simple sign function, and (3) employs a multi slice strategy to alleviate the loss of representational capacity incurred by binarizing the feature maps and shortcut connections. The results shows that the new model achieves 4.7x energy reduction with some accuracy degradation compared to SOTA architectures. This paper is an attempt to reduce the precision of 32 bit components in BNNs. To this end, a new network architecture is introduced that integrates batchnorm into sign function, modifies downsample block and skip connections to reduce their required precision, and exploit a multi slice convolution to compensate for the loss incurred by the precision reduction techniques. The proposed network architecture is novel and is an attempt to further reduce the complexity of neural networks for their efficient deployment. Weaknesses:  My first concern is the accuracy performance of the proposed architecture w.r.t.the SOTA architecture (i.e., ReActNet). In the ReActNet paper, there is a network architecture called ReActNet A which achieves the accuracy of 69.4% while requiring 0.87 OPs. If we compare performance of this architecture given the accuracy and the number of OPs with the results reported in Figure 4(a), we can see ReActNet A significantly outperforms BoolNets and BaseNets. In industry, we don t usually design a custom hardware for a specific network architecture. On the other hand, we consider a general custom hardware that can support a wide range of computing units and layer types. Of course, in this scenario, each network architecture will show a different implementation performance on the hardware accelerator. Therefore, I believe the hardware performance of BoolNets and ReActNets should be measured on a specific hardware accelerator for a fair comparison, and the choice of hardware accelerator should be limited to well know ones that are being used commercially. Otherwise, the design of custom hardware for a specific network can be biased to favor that network and does not constitute a fair comparison in my opinion. After carefully reading the paper, I believe while this paper made novel contributions in reducing the precision of 32 bit component of BNNs, it couldn t show its advantages over existing works such as ReActNet. More specifically, ReActNet A achieves a better accuracy and also requires a lower number of OPs, making the advantage of BoolNet limited to the implementation results. However, the implementation results on a custom hardware designed for a specific network doesn t establish a fair comparison. My suggestion is to report hardware performance of the networks on a commercial accelerator instead for a fair comparison and more reliable results.<|endoftext|>The paper is the first to build fully 1bit neural networks. The proposed BoolNet achieves state of the art performance on the trade off between accuracy and energy consumption. Strengths :+ The first work for fully 1bit neural networks, which is very important for practical deployment on hardware. + The effects of 32 bit layers in commen BNNs are analyzed and removed by specific design. + A Multi slice strategy and other tricks are proposed to alleviate the accuracy loss. Will it increase the memory usage and inference time? MS BConv increases the number of channels, which is related to [1]. This related work should be included. [1] Searching for accurate binary neural architectures. ICCVW 2019. This paper is the first work to build fully 1bit neural networks. I like this work and recommend acceptance for it.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; The paper discusses a method to learning a 3D shape descriptor. This embedding is used in the contrastive learning procedure. Together with the "real" images, they use adversarially perturbed images. At test time they use many images of the object to get object embedding. According to the tables the description can bump the accuracy by a couple of percent. The paper proposes an unsupervised descriptor which seems to be efficient. **1.The presentation is lacking clarity and details. Then $f_{\theta}$ is introduced prior to eq 2 but not used before eq 3, differential renderer is introduced twice before eq 1. All these issues undermine the quality of the paper. Then, in the first sentence of section 4, they say they evaluate on 3 dataset, while in fact they do so on two. Table 1 misses references to baselines, what is VoxNet?. Furthermore, important implementation details are missing. It s not clear how they integrate into GVCNN and MVCNN, I guess there can be many ways. I believe, this single concern is sufficient to recommend rejection and encourage resubmission. **2.Unsupervisedness of the approach. ** I cannot really name the current approach unsupervised for object classification at least. **3.Ablation of adversarial views.<|endoftext|>Metrics are indeed in favour of the proposed method compared to a supervised training, with no augmentation or adversarial samples. “Connoations of all feces”  > sorry what? …  The supplementary has broken references to the main paper (“Section ??”)The method seems to be working well, but is not very novel and lacks ablation studies to disentangle which components contribute to the performance improvements and how. Similarly, Table 5 in the supplementary is an attempt at quantifying the effect of augmentations, but seems to be carried on already trained networks. The caption for “adversarial view generation” introduces the augmented 3D shape $t(x)$, while the adversarial view is not related to this augmentation. In Fig.2, it would be interesting to show how L is increasing during hard sample mining. What is the size of a descriptor? Many typos, and quite verbose language that does not help having a smooth read:  The abstract+intro should state more clearly what the contributions are. 2.2 “required to generatING adversarial…”  3.1 “To this end, the direct adaptation of the adversarial sample generation method prevents us dynamically generating adversarial views due to field viewpoints” is unclear.<|endoftext|>This paper focuses on the problem of learning a descriptor for 3D shape. This is done with a combination from techniques inspired by adversarial learning and contrastive learning. The authors compare with a number of methods on the ModelNet40 benchmark. I particularly like the idea of identifying and rendering challenging viewpoints on the fly, compared to the more static setting of using a pre rendered and pre specified set of views adopted by many other works. In general, I think, that there are more choices that could be ablated in the main ablation study, instead of just the number of views (e.g., potentially a different hard negative mining method?). I like the presentation of the paper overall (e.g., the paper is clearly written, Figure 2 gives intuition about the type of hard examples that the method discovers), but there are quite a few typos (I list some of these below), so I would encourage the authors to do a more thorough proofread to eliminate those. pg 5 : some wrong pointers leading to ?? All in all, I think this is an interesting paper, and the method could be applied in other settings as well.<|endoftext|>The authors introduce adversarial views that enable to explicitly train the network on challenging views of a shape. further improving the method by generating novel adversarial views. The idea of using adversarial views is particularly interesting: in addition to being efficient, adversarial views are conveniently interpretable contrary to imperceptible noise values in more common adversarial methods. They could be useful for other applications in 3D vision due to their interpretability. The paper provides comprehensive experiments on multiple datasets while comparing to relevant baselines. Up to the results section, the paper is well written and clear. In table 1, the results in bold are the ones from the presented method (“ours”) (3D2SeqViews/Retrieval is not in bold in table 1). Some sentences are unclear and there are many typos. They are clearly supported by the performance shown in the extensive experiments and ablation studies. I strongly suggest the authors to work on the writing, especially in the results section.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; This paper introduces a simple technique for target side data augmentation. Results show that the proposed data augmentation method is very helpful for the dialog generation task, while also being somewhat helpful with neural machine translation for abstractive summarization. + The paper is nicely structured and very well written. + I have some minor doubts about the real world usefulness of the method. + Continuing on the previous point, it will also be interesting to see if the proposed method is also stackable with back translation when extra monolingual data is provided, especially since the authors have already considered multi round enhancement. It would be nice to elaborate.<|endoftext|>This paper experiments with a target side sequence level data augmentation scheme for sequence to sequence generation tasks. The primary contribution of this work is an algorithm that leverages model outputs to construct pseudo target side tokens (and consequently pseudo sequences) for augmentation. In its current form, and based on the reasons stated above, I would recommend a weak reject for this paper. Is the number of training passes equal for all the methods in Table 3. It seems like the "Ours" model is trained through more passes of the dataset (+ augmentation)  Table 4 and 5: Are the results statistically significant?<|endoftext|>This paper presents an approach of data augmentation for the input of the decoder during training. Specifically, the authors still feed the ground truth to the decoder, and then obtain its output vocabulary distributions that are multiplied with word embedding weights to get "pseudo tokens". The contribution of this work comes from narrowing the gap of decoding procedure between training and testing. Its method is straightforward and well illustrated. Furthermore, the experimental results demonstrate certain improvements in these tasks. ### Weaknesses:My main concern with this paper is that since the augmented target side input is produced by the decoder conditioning on the ground truth, this method seems helpful but limited. Are there any mistakes? Overall, this work tries to tackle the important problem of language generation and proposes a data augmentation method to give soft pseudo decoder input.<|endoftext|>This paper focuses on sequence generation tasks and proposes to perform data augmentation on the target side. 3.The consistency loss is unintuitive and more analyses are required. The author(s) state that "for a fair comparison with previous works, we compute the BLEU score by the Moses script ", but the scores obtained by this script depend on the tokenizer, which can be different in different papers and SacreBLEU (https://github.com/mjpost/sacrebleu) is recommended for a truly fair comparison. While I think their idea of augmenting the dataset is interesting and it does not introduce any inference burden, based on the weaknesses I mentioned above, I m leaning towards a rejection of the paper.
Reject; rating score: 1; rating score: 3; rating score: 5; rating score: 5; The paper proposes an approach to predict the confidence of missing edges in a knowledge graph in the purely inductive setting, in which the test knowledge graph does not share any entities with the training graph. W2 (neither well argued for and nor convincing). There is not clear, convincing argument for the approach.<|endoftext|>This paper focuses on the problem of inductive relational prediction for KG completion. Based on that, the authors propose a GCN based method for learning rules. strengths: * The problem of learning rules for inductive relation prediction is important. "Differentiable learning of logical rules for knowledge base reasoning." 2019.The paper proposed a circle based method for rule learning, which is an interesting view. However, some claims (e.g., shorter rules are better) are not convincing, and the motivation of such a design is not clear.<|endoftext|>The paper proposes a GNN framework to solve the inductive relation prediction problem, namely CBGNN. Overall the paper is well written and easy to follow. I m concerned about this approach, as the direction and the type of edge are critical to the KG. Besides the aforementioned issues, my main concern with this work is that the proposed framework is unnecessarily complicated. I m also concerned about the method scalability.<|endoftext|>This paper addresses the problem of inductive relation prediction in knowledge graphs. Treating rule learning as a cycle learning problem is very interesting. The experiments on popular knowledge graph completion datasets verify the effectiveness of the proposed method. strengths:  The paper is well written and easy to follow. Is this due to random seed? The paper proposes to treat rule learning as cycle learning in knowledge graphs.
Accept (Oral); rating score: 8; rating score: 8; rating score: 8; rating score: 6; More references in the mathematical and probability literature would be helpful due to the coverage of mixed random variables and associated theory in this literature. **Reasons to accept:**  The paper is well written and appears relevant to recent developments. This is solid well written work, my review remains unchanged. This is a paper with mathematical rigor and connections to emergent communication, which is a current topic of interest for specific machine learning communities.<|endoftext|>2.The idea of this paper makes sense, and this paper is well written and easy to read. This paper is very solid. It seems that this will be useful for the future research on this topic. I believe the theoretical foundations built in this paper will be useful in the future research on mixed random variables.<|endoftext|>This paper proposes mixed distributions over convex polytopes such as the probability simplex. This paper was a pleasure to read. While there is room for improving the experiments, I believe the presented experiments and theoretical developments   which can easily enable future work   should be accepted.<|endoftext|>The authors wish to introduce a new kind of random variables that are consisted of both continuous and categorical data. The authors are encouraged for their good work. Also, what is the algorithmic complexity with respect to the number of observations, the number/rate of continuous and discrete outcomes and the number of variables (in the case of multivariate mixed variable?Finally, Table 1 is quite unclear to me.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; rating score: 6; This paper proposes a gray box optimization method for hyperparameter optimization of deep neural network models. In order to deal with the different budgets available for training NNs in the framework of multi fidelity optimization, the proposed method uses a multi task Gaussian process modeling that simultaneously measures the similarity between not only the inputs x but also the outputs y trained with different budgets. In particular, the multi task Gaussian process model is constructed using a deep kernel with a feature extractor instead of the existing kernel function. The performance of the proposed method is evaluated by experiments on three types of neural nets: MLP, RNN, and CNN. There are several parts of this paper that are not clearly related to similar studies. 2015.2.There is a lack of explanation about the architecture of the deep kernel (why this architecture, what makes this architecture effective for multi fidelity optimization, etc.). It looks to be just a presentation of a kernel architecture that happens to work well.<|endoftext|>This paper is concerned with multi fidelity HPO (the authors call this "grey box", which is a non standard term). Experiments are presented, where the method is compared to a range of synchronous HPO baselines. A potential strength of this paper is the proposal of a novel surrogate model for learning curve data which, despite involving a neural network, seems to be operational on just the data observed during a single HPO experiment. Having said that, I could not find any mentioning of this point, and I am really curious about the authors explaining how their "deep GP" model can be trained just on the very limited data observed during a single HPO experiment. Now, even if these correlations are poor, it is not clear to me why DyHPO could do anything about that. Learning curve prediction is just hard, because by far most of the data is from early evaluations, but you are interested in late performance, so you need to extrapolate. The only way to really know about certain anti correlations that can be exploited, is to either fit models to data from past HPO experiments (which DyHPO does not do), or to built the knowledge into the model (which they don t do either). How long do you re train? Apart from most relevant baselines missing (DyHPO is asynchronous, all competitors are synchronous), the curves are also not very meaningful, because the x axis is number of epochs instead of wall clock time. Finally, there are quite some works on using complex surrogates to model learning curves in the context of HPO, for example [5]. The proposed method uses asynchronous scheduling (much like Freeze Thaw), but is compared against synchronous scheduling baselines, which have a major disadvantage. For example, the paper claims that it is a new idea that DyHPO "never discards a configuration".<|endoftext|>A gray box hyperparameter optimization framework has been developed based on a multi fidelity acquisition function and a surrogate model incorporated with learning curve dynamics. The proposed method was built on top of deep kernel learning [Wilson et al., 2016] and multi fidelity Bayesian optimization [Kandasamy et al., 2017]. Pros:  Incorporating learning curve dynamics into the surrogate model is well motivated and supported by the ablation study on the NAS Bench 201 dataset. Extensive experimental results have been provided in terms of tabular datasets, NLP tasks, and NAS. Cons:  Predicting learning curves is not new for HPO as it has been well explored by previous works like [1]. While the proposed method tries to involve the budget information for modeling curve dynamics, the technical novelty of this work is still somewhat limited since it seems like a direct combination between [Wilson et al., 2016] and [Kandasamy et al., 2017]. Overall, the paper is easy to follow and well motivated. While some ablated models and baselines are missing, the experimental results are comprehensive and seem to be solid.<|endoftext|>The paper presents an extension to the commonly used Gaussian Process based HPO by incorporating the learning curve dynamics to decide the next HP configuration to be tried out. ### Post rebuttalGiven the detailed rebuttals by the authors, the updated baselines, I m confident about increasing my rating for this paper. The experimental evidence of the efficacy of the method is strong. However, the paper in its current form misses some important details, and ablations. If the authors can address these points adequately in their rebuttal, I’d be quite happy to raise my score. The paper is also well placed in the context of previous methods. The experiments section is quite strong in the experiments and baselines covered. The method proposed is simple, and is an intuitive extension to the methods in the literature. How are the hyperparameters of this training chosen? In the absence of these, it is hard to judge the merits of the proposed \phi, as I do not understand how these specifics were arrived at. 3.Evaluations: The use of Epochs and Steps to describe the x axis in various plots is a little confusing. Also, it is ideal that the authors include the true performance (say test accuracy on CIFAR/ImageNet exps), and the true wall clock times somewhere in the paper, in addition to the regret plots presented. Also the proposed method’s rank fluctuates quite a bit in the initial steps in Fig 3 and 5; can the authors comment on this? This might have interesting implications, if it can be.<|endoftext|>This paper present a new Bayesian Optimization algorithm that integrates a Deep Kernel over both the hyperparameters x and the fidelity budget j (typically number of epochs). It also present a slightly modified version of the expected improvement acquisition function to for the fidelity budget. The baselines are good, with Hyperband, BOHB and DEHB being serious multi fidelity contenders. The empirical results clearly show that this is not the case, however the explanations do not make clear why it would not be the case. There are no clear experiments with learning curves that are best later on to synthetically show that DyHPO performs well in this case. This would be extremely valuable to support strongly that the reason why DyHPO works so well is that it indeed let the best trials train even though they do not perform well at the beginning. The paper lacks important details in my opinion with respect to the optimization of the expected improvement and how it ensures a good fraction of the trials continue training.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; The paper considers an imitation learning (IL) problem with both expert and suboptimal demonstrations. To solve this problem, the paper proposes TRAIL, which pre trains an action encoder decoder and a latent transition model using sub optimal data, and performs behavioral cloning (BC) with expert data to learn a policy in the latent action space. The method is theoretically justified and empirically well supported. ## Strengths  The paper proposes a theoretically justified method to learn from expert and sub optimal data. ## Weakness   The main issue is on the clarity of the paper. ** Update after authors  rebuttal **The rebuttal and the revision address my concerns on the paper.<|endoftext|>The paper proposes a method to accelerate behavioral cloning (BC) (especially in the low data regime) by utilizing a (much larger) auxiliary dataset of suboptimal behaviors. The authors claim that learning a good latent action representation (in this case, by learning a transition based action representation from the auxiliary dataset) should make the imitation learning problem easier. In all of the experiments, the action embedding ($> 64$) is bigger than the original action space ($ 8$ for the ant). (7) To empirically prove the theoretical claims, the authors could run another experiment in which they artificially blow up the action dimensionality and check if they can learn a much smaller embedding. ### Decision due to rebuttalThe authors addressed all of my concerns sufficiently well.<|endoftext|>The paper proposes an imitation learning algorithm, TRAIL, that can benefit from a large amount of suboptimal demonstrations besides a small amount of high quality demonstrations. This is achieved through learning a factored transition model with action reparameterization from the suboptimal or even random demonstrations before doing behavior cloning. The authors analyze the error bound of the algorithm and show improved sample complexity with action reparameterization. This method of learning action reparameterization is applicable beyond imitation learning. I am excited about the experimental results. Some form of visualizing the latent action space learned may be interesting.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 5; This work proposes GATv2, the improved variant of the graph attention network (GAT). However, I recommend accepting this paper since the implications from its theoretical and empirical results can be significant to the graph neural network community. Many recent papers are designing variants of attention and applying them for various domains, but no paper clearly answers why the attention looks like what they proposed and why it works. I believe that future research of designing attentions can leverage the principle and practices in this work. How does GATv2 perform on the PubMed dataset with the public split? The proposed GATv2 model is simple but more expressive than the original GAT. The authors’ claim is justified both theoretically and empirically.<|endoftext|>In this paper, the author mainly redesign the Graph Attention Network, and they show that the new model can capture the dynamic attention instead of static attention in the original GAT. Strengths:1. a new type of GAT model, that can have very borader impact. 2.The design is super simple as shown in Eq.(6) and Eq(7). The impacts are broad. The differences of the dynamic attention and static attention can be further discussed.<|endoftext|>This paper analyzes the limitation of GAT by pointing out that GAT computes a limited kind of attention: static attention. This paper then introduces a simple fix by modifying the order of operations and proposes GATv2: a dynamic attention variant. The example in Figure 1 and Section 4.1 is a **complete** bipartite graph. In this special case, queries have the same set of keys, and the theory works,but the special example is far from the real world datasets. The empirical improvement cannot be properly established on static and dynamic attention<|endoftext|>The paper proposes a method named dynamic graph attention by modifying the order of operations. The explanations and justifications of the proposed methods can be improved. Why is it a problem that, for any query node, the attention function is monotonic with respect to the neighbor (key) scores? The simple fix captured by Equation 7 was claimed to be able to compute dynamic attention as defined in the paper. Some results show that GATv2 seems to outperform GAT on a set of benchmark datasets, but are mixed in other datasets. Given it is able to compute dynamic attention, shouldn’t more attention heads be better? Gaining deeper insights to these questions will be really helpful.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 8; Another path forward, since this paper is heavily based on the approach of [F21a], improves its results, and that paper appears to be still unpublished, would be to consider whether the authors of {F21a] would be amenable to merging the papers. While such a combination may require substantial novel insights, the exposition does not currently make it clear what those are. I will discuss several particular relationships which do not appear to be sufficiently clearly explained, and for some of them this appears to expose weaknesses in the paper. Why not adopt these techniques rather than the ones that lead to O(T^{ 0.75})? I don’t a priori see why they should be incompatible with the framework from Section 3 and if they can be achieved it makes the headline result of O(T^{ 0.75}) less impressive unless there are other reasons to prefer this approach. So it would be helpful to call out which specific results require new insights to apply it to a stable predictive version (which have been explored in several previous works, although perhaps mostly for normal form games). This paper describes it simply as “very recent follow up work” to Celli et al.2020, while [F21a] describes itself as an extended version of that paper in a note on the first page. One possibility is that it does but the technical exposition needs to be improved to more clearly highlight the contribution.<|endoftext|>This paper proves a faster no regret learning dynamics for extensive form correlated equilibrium (EFCE) in multiplayer general sum imperfect information extensive form games. ## StrengthsThe problem that the authors considered is an important problem for no regret learning dynamics in games. ## WeaknessFirst I think the summary of related works on no regret learning dynamics for normal form games are missing some most recent results. Given the many other existing rates for normal games, it would be good to add more details on this point. The paper is well written. Post rebuttal update: I have read the authors  response and will keep the original score.<|endoftext|>This paper presents an uncoupled no regret learning dynamic provably converging to the extensive form correlated equilibrium in general sum n player extensive form games. The text is extremely technical and notation heavy, and I believe many concepts should be more adequately explained for a reader less familiar with no regret learning to understand this work. The authors  result is interesting, and I am convinced that the presented experimental results provide enough evidence to support their claims. I remain unsure if a better exposition is possible given the ICLR s strict page limit and if this work would not be better suited for journals or conferences that allow longer narratives. I have a few concerns as well, though. Perhaps the main one relates to the presentation of this work. This may make some readers confused. This makes it difficult to follow the authors  thought processes, and the reader is forced to rely on the explanations in the appendix, which contradicts the idea of the main text being self contained.<|endoftext|>The authors consider the problem of finding an approximate extensive form correlated equilibrium (EFCE) of a finite general sum multiplayer extensive form game under the perfect recall assumption. They prove that when all agents play T repetitions of the game according to the proposed algorithm the correlated distribution of play is an O(T ^−3/4 ) approximate EFCE where O hides quantities polynomial in the size of the game. The main technical point to obtain this result is to characterize the stability of certain fixed point strategies through a refined perturbation analysis of a structured Markov chain. They also provide preliminary experiments on simple two or three players games. The proofs seem correct (I did not check the appendices in detail). Providing an accelerated convergence rate for multiplayer extensive form games (and in particular proving the counterpart of what is known for normal form game) is a valuable contribution. My main concern is that in the current form the paper is difficult to read. Furthermore, some parts should be better introduced, e.g.the \Phi regret minimization framework (see specific comments). Main comments:  It is hard to find what is the final algorithm mentioned in Theorem 1.1 by reading the main text and even by looking at Appendix B. Could you state clearly the complexity of the proposed algorithm (it seems that it scales at least quadratically with the size of the game?) and polynomial dependence on the size of the game hidden in Theorem 1.1. This is important since usually the size of the game is very large. P13, top of the page: \cJ is in the introduction the set of information set.
Reject; rating score: 3; rating score: 6; rating score: 6; rating score: 6; This paper presents a pruning before training algorithm for improving the communication efficiency of federated learning. In each iteration, all local devices with the same initial weights perform the pruning on their own local datasets. The edge scores produced by local devices are aggregated by the server. The order of the edge scores will be used in the next iteration of score initialization process. After the pruning convergence, the edges are pruned correspondingly and the federated learning model is trained on the pruned network. The edge ranking communication improves the efficiency of the federated supermask learning itself. The original paper is only able to prove that switching edges according to iterated edge scores can reduce the loss on randomly initialized networks. However, pruning after training and pruning during training techniques often achieve the better approximation. The authors should discuss recent advances in federated learning. 4.The paper lacks of enough novelties, since two main components of the paper, EdgePop and voting, are from existing techniques. Overall, the well designed structure makes the workﬂow clear and easy to follow, but the further analysis and discussion are expected to clarify the contributions in the techniques as well as in the evaluation section.<|endoftext|>This paper exploit the "supermask" technique to improve both robustness and communication efficiency in federated learning. The authors also provides theoretical and empirical study on how the proposed method can improve robustness and communication efficiency. Strengths:1)	This paper combines “supermask” techniques with FL and improve the communication efficiency and robustness of FL at the same time. There is no doubt that there is information loss when the devices transmit only the ranking of scores. In this paper, the user uses Dirichlet distributions to construct non IID data for MNIST and CIFAR10. And I guess the robustness results have the same problem since the authors use a voting mechanism to update the global ranking. 3)	The idea of utilizing “supermask” seems novel, but this paper seems just simply combining “supermask” with FL. It is okey to do “A plus B” things, but you need to provide some scientific contributions like providing a theoretical analysis about why “supermask plus FL” works, and what challenges that you solved make it deserve an acceptance by a top avenue like ICLR. In addition, the experimental settings are not hard enough to judge the usability of the algorithm proposed by this paper. Based on the experimental settings and the lack of scientific contributions, I would give a negative score. If the authors can solve these two major concerns, I am delighted to increase my score.<|endoftext|>The paper proposes to collaboratively learn a supermask within a randomly initialized neural networks, instead of learning the model parameters. # Strengths* The idea of introducing supermask learning (on a randomly initialized network) for federated learning is interesting. * The algorithm leverages the ideas like edge ranking order and majority voting to achieve the reduced communication cost, robustness to malicious clients. Some numerical experiments are performed on these aspects to justify the performance gain. # Weaknesses* The authors may ignore some prior works that attempt to introduce the idea of masking to federated learning, like [1]. * A more recent communication efficient techniques developed for distributed learning need to be considered, e.g., at least the error feedback framework [2, 3, 4] should be integrated with these compression techniques for FL [10, 11]. Authors are also encouraged to comment the integrability of these FL methods on the proposed FSL method. * In the appendix, same hyper parameters are used by all methods. It would be great if the authors can at least provide one Table/Figure to justify that tuning hyper parameters for all methods will not change the performance gain of the proposed method. In general this paper is interesting; but before the acceptance of the paper, the authors need to provide additional numerical comparison over other strong baselines in the rebuttal (as pointed out in the main review section).<|endoftext|>The authors show how to apply the method of Ramanujan et al.(2020) and Wortsman et al.(2020) to the federated setting. Final disclaimer: I am not an expert in defences and attacks in the FL setup and will defer to other reviewers in their evaluationIn Summary, the authors apply an existing algorithm to the FL setup, including a novel voting algorithm that applies to this algorithm. The execution of the paper has some flaws that I would like to see corrected before the paper is ready for publication. Regardless, several things remain unclear. High level: Throughout the text the authors refer to robustness and communication as  the  important issues in FL. I would argue that they are two important issues, but there are also many other important issues, such as differential privacy, efficiency, data/hardware/network heterogeneity, questions of fairness and representation among many others. Using the provided numbers in Table 3 for the Cifar10 model, I compute sum([np.ceil(np.sum(np.log2(np.arange(2,l+1))))/(8 * 1024 * 1024) for l in layers])   11.698538184165955mb for a single message, instead of the 13.1MB that the authors claim. Additionally, it might also be a good idea to average evaluation accuracies across the last few epochs to get a more robust estimate. I landed on the Lehmer code (https://en.wikipedia.org/wiki/Lehmer_code#Encoding_and_decoding) for computing the index, which seems to scale quadratically in $n_l$. Please plot learning curves where on the x axis we have accumulated communication budget and on the y axis we see validation accuracy. I would argue that a stronger attack might be considered when considering potential effects across all layers of the network. Stochastic quantization is important here. Further compression can be achieved by performing vector quantisation. Some minor aspects:  In Algorithm 3 you index with $mu$, as well as only $u$ and only $m$. Why is this form of collusion the strongest form of attack? What is the meaning of these individual or joint indices?
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; It appears that you can have significantly more ground atoms than are necessary to describe the two scene? This is not discussed clearly in the paper and there are no significant experimental results. What are the losses used for training? It is also unclear how this work build on previous work. From the introduction it is not clear exactly what the contributions of the paper are and what the related work is?<|endoftext|>A technically solid paper that lacks a bit of novelty and falls shorts in evalaution. Pros:1.A whole system pipeline is developed for neural symbolic reasoning. If the authors can open source their code, this will be a good contribution to the community. But I think works such as Amizadeh et al., 2020 and neural symbolic concept learner have both of these modules?<|endoftext|>However, I do have serious misgivings about the paper. Not only does this directly encode class 1 of the dataset, it also, by construction (ie: by “cheating”), solves the confounding issue that the CLEVR Hans dataset was designed to bring out. But in the test set, it is sometimes another colour. This isn t necessarily a problem, although it precludes fine tuning the pre trained encoder for a given task. However, I have major concerns about the extent to which the architecture relies on hand designed features and rules. But I am open to having my mind changed by the rebuttal. It seems to me that this builds in half the solution to the problem from the outset.<|endoftext|>Compared to earlier works in this topic, the paper clearly defines the notions and related concepts. Therefore, I m confused on the specific novelty of this work. How does it compare with other deep learning models that can be trained without them? I have read the reviews and the authors  response. The authors acknowledge some general issues and point out  differences with earlier works, though I do not think is significant enough.
Reject; rating score: 3; rating score: 6; rating score: 6; This work proposes an approach for modeling sparse and multivariate sequential data which are commonly found in the healthcare domain. The paper presents the value aware transformer, a transformer decoder only architecture for the 1.5d data representation. The contributions of the paper are not clear. The paper is also missing important related works and comparisons with recent approaches in this domain. The authors are missing several important related works and comparisons with recent and SOTA methods for sparse and multivariate sequential data [1, 2, 3]. Recurrent neuralnetworks for multivariate time series with missing values.<|endoftext|>Authors proposed the method that could represent the sparse sequential highly multivariate data, which could be trivially represented as neither 1D nor dense multivariate series data. Authors proposed the 1.5D representation which is composed of token value pair and proposed the value aware transformer that is able to use the representation. I think this baseline is required as the evidence for authors  insist that the sparse sequential highly multivariate data could be better represented in 1.5D compared to multivariate representation.<|endoftext|>This could strengthen the contributions. The value  aware transformer performs marginally better than a value unaware transformer with respect to $\mathcal{L_{\text{\[TOK\]}}}$. The paper can also be improved to make it better for the reader. Adding why the value aware representations would be beneficial for downstream tasks in the introduction would help to situate the need for this work. 3.Che, Zhengping, et al."Recurrent neural networks for multivariate time series with missing values." In general, the paper suffers from clarity as well as comparison with existing baseline other than transformer based approaches and feed forward networks.
Reject; rating score: 1; rating score: 3; rating score: 3; rating score: 3; rating score: 6; I have provided the details above with feedback on how to address each point). Therefore, I cannot recommend the current version for the conference. However, this should not stop the authors from improving on their current work and hopefully submit to a future conference when it is ready. So, the statement should be corrected. The authors should include those in their comparisons.<|endoftext|>And there is no clear and solid explanations for the design choice. The authors do not explain why their model can do this. And it should be  dark channel prior .<|endoftext|>The authors employ 3 types of attention methods in parallel: pixel attention, channel attention, and spatial attention. However, the proposed method does not lead to task generalization. As the attention module operates on feature space, such an interpretation is hard to build. I believe this is a jump of logic without sufficient grounds.<|endoftext|>2.This paper tends to improve the generalization ability of a single restoration model for different degradation types. Although the motivation to improve the model generalization ability is good, this paper does not clearly explain why the proposed architecture could actually do this. 7.Overall, the experimental results are not impressive and even not good. Or it is not convincing to demonstrate the effectiveness of the proposed method. Therefore, I recommend this paper with rejection.<|endoftext|>While on it’s own those metrics are valid and representative, the problem arises from the fact that PSNR and SSIM are also directly used in a loss function for the model (PS loss term, section 3.4). 2.Paper does not include any mention of important performance details, such as number of parameters, memory usage or training stability. The paper also includes insightful observations and thorough prior work overview.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; The experiments are explained in detail and include ablation procedures. It is not clear to me how a simple correlation can enhance a nonlinear Gaussian kernel. Obtained results elucidate an interesting strategy for seizure analysis on a well known public database. The experiments are exhaustive and convincing.<|endoftext|>Both of these are well demonstrated and the results demonstrate how one structure is superior to another for a a particular type of rare seizure. It also includes an occlusion map based seizure localization method. The accuracy of occlusion based seizure localization was quantified and visualized in a very comprehensible way.<|endoftext|>The paper presents a method for seizure detection and classification. In particular, the method is self supervised, based on graph neural network and use EEG signals. The details of the self supervised training must be added in more detail for clarity and reproducibility.
Accept (Poster); rating score: 6; rating score: 6; rating score: 5; rating score: 5; In addition to data and model uncertainties, this paper brought attention to the role and importance of modeling optimizer uncertainty. Here are some comments / questions:1. However the structure of Rastrigin seems to be the simplest among all three test functions. TYPOS:1.The first line in Section 3.1, am  > an2.<|endoftext|>However, there are some unclear points that make me not sure of the acceptance of the paper. Are the samples drawn at the initial stage of the optimization or drawn at each iteration of the optimization? How sensitive is the algorithm to the number of samples? How does the algorithm scale with the number of samples being used? 3)  The "space of algorithms" is a slight over statementAlthough I find the notion of the "space of the optimization algorithms" quite appealing, the actual implementation given in the paper seems rather disappointing; as the authors already stated in the paper, there are many elements to be considered for a single optimization run.<|endoftext|>There are some clear errors in the paper, and I am not convinced that the empirical performance justifies this methodology to be accepted at ICLR. Do the authors agree with this?<|endoftext|>I found the main idea of the paper novel and interesting. Is this supposed to be $p(\mathbf{x}^* |\mathbf{z}_T)$ as given in Eq 5? What is $p(g|\mathbf{z}_t)$ that is  $p(g|\mathcal{D})$ representing?
Reject; rating score: 1; rating score: 3; rating score: 3; The particular system is the Uruguayan system. It may be currently in use as the authors are saying yet that should qualify their approach as a good option for whoever is interested in using it for the Uruguayan system but not as a scientific argument, in my opinion. As for weaknesses, the paper seem to be a concatenation of well known and simple techniques with some specifics that are not justified.<|endoftext|>* The high level approach and assumptions are clearly described. * Additional details need to be provided in the experiments, in order to fully understand the efficacy of the proposed method.<|endoftext|>4.Although the authors seem not a researcher in the ML community, the fundamental knowledge of ML described in the paper is almost correct and the logic is very clear. The comparisons with the related works in the same field is important.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; The paper proposes a hierarchical approach for generating sequences with global structures (e.g., music and poems with repeating elements). The overall framework is well motivated for certain generation tasks such as music and poetry generation, and learning relational constraints from data is an interesting idea. However, the paper sweeps under the rug some key foundations of the proposed approach. This seems like the crucial premise that the proposed method could work, and it should be clearly stated in the main paper. Relations with these papers should be discussed: e.g., they all condition sequence modeling on a set of global constraints; constraints of these papers are human specified while those of this submission are drawn from a learned distribution (though its learning depends on human specified relational features).<|endoftext|>The paper presents a neurosymbolic generative model for sequence data. Experiments are conducted on two datasets, one for music generation and the other for poem generation. 2. effectiveness of the approach on music generation and poem generation. 3.Soundness: the authors pay a lot of effort to formalizing the constraints and describing the learning details of the constraints. However, important details of how to build the generative model are missing in the main part. If it is hard to find other metrics, at least human studies can also be done in music generation. Some descriptions about the related work are placed in appendix, and more seriously, some important details, such as the details of Section 4.2 are also placed in appendix.<|endoftext|>Empirical evaluations on music and poetry generation tasks are presented. The idea of formulating the constraint synthesizing as a constrained optimization problem is novel to me. The other concern I have is on the efficiency of the synthesizing relational constraint process. Learning neurosymbolic generative models via program synthesis. 2019.The generative model for sequential data with relational constraints proposed in this work is interesting to me. Still, I have several concerns as mentioned in the main review.<|endoftext|>This paper proposes a generative model for sequential data guided by relational constraint. The paper experiments the proposed model on music generation and poem generation tasks, showing some substantial improvements given the proposed metrics. I think the constrained/guided generation problem is an important one. And the method proposed in this work is interesting. But I don t see this problem in the baseline experiment results though. This is less exciting since the constrained sampling could end up pretty slow and bad. On one hand, the GCN discriminator metric is subject to artifacts in training data and thus potentially can be broken. I think the major contribution of this paper is the framework.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 8; The paper analyzes average convergence rates of random quadratic optimization problems. The authors attempt to characterize the rates via the expected spectral distribution (e.s.d) of the random matrix (objective s Hessian) and show that the proposed algorithms work well asyptotically. The paper is a follow up work of the two articles by Paquette et al., 2020 and Pedregosa & Scieur, 2020. In particular, I wonder why the authors claim to have "established that the asymptotic convergence of first order methods on quadratic problems in the convex regime..." Is there a clear correspondence between conditions like Assumption 1 and the convex regimes?<|endoftext|>However, a nice recent average case analysis by Pedrogosa and Scieur gives the following result: if the spectral density of the Hessian converges to a "nice" probability measure (such as the Manchenko Pastur law), then first order methods that are tailored to this density may converge faster. The contributions of the paper are probably correct. In spite of this, they are able to derive bounds of the same order as in the beta case. There are three main weaknesses to the paper. Major issues  (1) It is nice that the authors give a full analysis of their "generalized Chebyshev method" under their e.s.d. (2) It is never true that a discrete spectral distribution equals a Beta, a Laguerre law, or satisfies Assumption 1. It seems to me that you need all of these to work asymptotically, but this is not mentioned in the paper. Perhaps there s no need to give an example at that point.<|endoftext|>This paper is a theoretical work concerning the convergence rate of random quadratic optimization problems. The paper seems to be a direct following paper of the two papers by Pedregosa and Scieur and the main technical difficulty is in the calculus part in the appendix.<|endoftext|>The paper considers the problem of average convergence rate of first order methods on a given ensemble of quadratic problems. is beta distribution. They also show that so long as we know the behavior of e.s.d. Weaknesses:  The main concern I have is that the setting considered is rather limited. Minor Comments/Typos:  Even though it is clearly stated in the Contributions Section, when I read the paper it was not clear to me at first that Algorithm 1 is a contribution of the paper. s and knowledge of the e.s.d. only around the edges of the support.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 5; This paper used autoencoders to approximate different phases of a PDE solver. The method proposed in this paper is novel and validated. Since this work is about accelerating the PDE algorithms, I would expect to read more discussions on how or why this method can be faster than traditional solvers by the ratio given by the authors (e.g., 40 50x faster). Besides, it would be more solid if the author compare between using different sizes of layers (number of parameters) in the autoencoders. How is the balance between performance, accuracy, as well as generalization ability, regarding to the size of the autoencoders? How to choose the layer size regarding to different problems or some performance requirement? I would like to see the acceptance of this paper if the authors can provide more comparisons and discussion on the performance / accuracy regarding to the size of layers.<|endoftext|>This paper proposes a new ML approach called CoAE MLSim that is a faster alternative to PDE solvers. "Figure 30 shows comparisons of CoAE MLSim with Ansys Fluent for 4 unseen objects in addition to the example shown in the main paper." Some of the claims are not carefully backed up. The unsupervised and iterative inferencing aspects are only positives if they have the claimed benefits, as compared to other ML methods (more accurate and better generalization). However, the results are not yet strongly backing up the claims, especially that it is more accurate and generalizes better than the previous ML methods for this task.<|endoftext|>The paper propsed a composable AE based PDE solver that combines traditional PDE solver with ML techniques. 2.The clarity of method description needs some significant improvement. I believe the Figure 1 is not mentioned in the main text? CoAE MLSim is the proposed algorithm right? Update on Nov. 20th during rebuttal:I think during the rebuttal period, I have gain much more understanding of the paper and willing to raise the score. The paper raised an interesting idea, using changing the simulation to the latent space by compression of autoencoders. However as the overall goal is still to simulate new designs, the generalizability of it due to autoencoders might still bottleneck its practicle impact.<|endoftext|>The paper proposed CoAE MLSim to learn with relatively fewer samples of PDE solutions and solve PDEs. The main issue is that their numerical experiments are not convincing enough to support their claims on small data, accuracy, and extrapolation. During the inference stage, because the method needs to couple all the solutions in subdomains together by using an iterative algorithm. The inference time is significantly slower than other ML methods, which only requires one forward pass of the network. Also, it is not fair to compare the speed with Ansys solver, because Ansys is much more accuracy than the proposed method. The authors claims that the method has a good generalization, but there is no evidence to support this. The method is not unsupervised.
Accept (Poster); rating score: 8; rating score: 8; rating score: 5; rating score: 5; This paper proposes to use fixed semantic anchors for each category, which are embedding vectors from a language model (or randomly generated vectors) for each category. Importantly, when new data arrives, new embeddings are added while previous ones do not change. Also the "traditional detection" losses need to be explained because this is a continual learning setup and not a "traditional" detection setup. Are boxes that RPN defines as "unknown" a separate class in the RoI classifier? Or are only the currently known categories used? Some form of justification would be good. #### Related work  How is the concept of "semantic anchors" different to approaches in zero shot learning/detection that also encode the category names with a language model and use the embeddings as the classification/regression targets? Zhou et al.arXiv 21Overall, I think the paper presents a solid contribution to the task of open world object detection.<|endoftext|>It seems to be implied that way, but it is not made explicit in the paper. It would be good to have more discussion on the nature and choice of unknown anchor in the semantic topology. [A] Towards Open World Object Detection, Joseph et al., CVPR 2021The proposed approach is significant and relevant in open world object detection, proposing an end to end approach that learns incrementally and can detect unknown objects. However, I have concern regarding the main results reported (see weakness 1), and some other questions. Important related work on zero shot learning was not cited in the preliminary version, which the authors included later on. The idea to use semantic topology to enforce discriminative features that are consistent across incremental learning is an interesting direction for open set detection. The paper is well written and the ideas and experiments are presented with clarity. * It was brought to notice by a reviewer the discrepancy between the problem definition in Section 3.1 and the evaluation protocol. The proposed approach seems to show significant improvement in detecting unknown objects. Could this be explained based on the data?<|endoftext|>This paper extends the recently introduced object ORE object detector [A]. In case my interpretation is wrong I would like to see a very thorough discussion in relation to the aforementioned (that should already have been in the original manuscript), with a focus on what exactly the difference is. Those were based on my initial miss conception on how semantic anchors were used. However, I still think that the evaluation protocol studies only incremental learning and provides no evidence whether the proposed method can be actually applied to the (significantly more challenging!) As a reviewer, I cannot recommend accepting a paper that (in my view) has a mismatch between paper claims and premise (open world detection) and the actual delivery (incremental learning). In summary, there is no evidence that any of the "new" classes in the new task sets are actually being detected before the network update. However, this is not the case here; this paper entirely ignores the field of zero shot learning, which is a basis for this work. In terms of zero shot recognition, a good source would be [D].<|endoftext|>This paper proposes a semantic topology embedding for Open World Object Detection (OWOD) where an object detector identifies objects of unknown classes and incrementally learns to classify them assuming that their annotations are progressively given by humans. While the semantic topology scheme is somewhat new, this work is heavily built on the work of Joseph et al., CVPR’21. Many parts of this paper are recaps of Joseph et al.s work.Furthermore, leveraging the embeddings of the pretrained language model is popular in the zero shot learning literature, so I don t think the core idea is very original while it s interesting to see the effect of the simple method. In this aspect, the performance comparison to the previous work can be seen as not fair. It needs more careful investigation on this issue. I suggest changing the title and making it more informative, at least, including the term  open world detection .
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; This paper obtains rather impressive results about the problem of estimating a quantum ground state energy using a variational method which could be implemented on a quantum computer. Here a theorem is formulated and proven that there is a phase transition of this type, with good estimates only above a threshold in the number of parameters, and with an explicit expression for the threshold. In section 5 a numerical confirmation of the results is done on a 1d Hubbard Hamiltonian. I found the main question well motivated and clearly asked, and an impressive variety of techniques were brought to bear on it. I was surprised by the basic claim that this class of models is sufficiently random to justify dropping all of the higher moments in computing the ground state energy. I suspect that there is a much simpler argument waiting to be found, perhaps building on the comments in Appendix A.3. A sharp formulation and sound theoretical analysis of a well motivated and important claim, that quantum variational states which do not depend on specifics of the Hamiltonian under study, require exponentially many parameters to produce accurate ground state energies.<|endoftext|>These predictions are compared with small numerical experiments, as well as previous works on this topic. ### Strengths* The paper gives a rigorous analysis of a general result connected to an important open question in quantum machine learning, namely the ease or difficulty of finding high quality solutions via gradient based learning of parameterized quantum circuits. The paper s (mostly negative) result provides a useful starting point for researchers to build better quantum generative models, by avoiding the same assumptions used to derive results here. ### Weaknesses* Despite the quality of the writing, the paper s results are still quite technical, and likely not accessible to most readers (even those working in quantum machine learning). * On the above note, I would encourage the authors to work a bit harder on including a more self contained and accessible summary of their results for readers who can t or won t take the time to follow the entirety of the paper s technical development. However, it would be very useful to have a one paragraph description in a more prominent place (end of introduction?), simply stating the expected quality of local minima as a function of (a) Hilbert space dimension (or number of qubits) and (b) quantum circuit parameter count.<|endoftext|>Is it a quantum Bayesian network? When the authors provide some numerical experiments, overall the validation is not very strong and could be improved. 3.Many important related papers are not cited, e.g, for the expressiveness of QNN, which has to be credited from“The effect of data encoding on the expressive power of variational quantum machine learning models,” Physical Review A, 2021   APS***### Post Author Response I think most of my comments have been carefully covered by the authors. With the current version, some of the writing, reproducible discussion, and presentation have been largely improved. I therefore increase my score to 6; one remaining place that would still improve is the current VQA experiments is limited to a single empirical case. ### ProsThe work leverages the concepts of random matrix theory to analyze the optimizationperformance of quantum generative models. Several English grammar and word mistakes preventthe readers from clearly understanding the main idea in the paper. 3.The proposed methods are not very technically sound because the barren plateau associatedwith the optimization of QNN mainly arises from the quantum noise of NISQ devices. 2.The connections between local minima to the random matrix theory are still not very clear. In particular, the authors do not take into account how to make use of their theory to improve optimization performance.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; This paper proposed to use randomly selected experts for Mixture of Experts models instead of gating function based selection methods. The paper is easy to follow and the proposed methods are simple and novel. The results look promising and the method should be easy to be adopted. 3.In page 5, the paper states `During inference, we can also select a pair of experts to activate at each layer for each input, similar to that in training.`. Also, if two experts are selected, how the final predictions are selected? I am wondering if the authors can provide the real inference time for transformer base model and the THOR model (with same FLOPS) in both batch inference mode and real time inference mode. The paper presented a simple and novel MoE method. I vote for acceptance.<|endoftext|>The paper proposes THoR, an approach towards training MoE like models (called SAMs in the paper) that have multiple internal experts which are chosen in a discrete fashion. This paper reports that THoR outperforms other methods, including the recent Switch Transformer MoE style model, on several classes of multilingual translation tasks. However, MoEs are complex models, for which it is very easy to subtly apply an incorrect comparison, or to evaluate improperly. Fundamentally, the main request of this rebuttal is that the authors address this inconsistency between their MoE/Switch results and those reported by others. This should make the network at eval deterministic. [1] *Switch Transformer* https://arxiv.org/abs/2101.03961[2] *Hash layers* https://arxiv.org/abs/2106.04426[3] *GShard* https://arxiv.org/abs/2006.16668[4] https://arxiv.org/abs/2109.10465[5] *BASE Layers* https://arxiv.org/abs/2103.16716The paper is well written and the proposed ideal seems novel. In the current form, I feel this paper is borderline due to these unanswered questions.<|endoftext|>In this paper, the authors propose to equip transformer with stochastic experts ( i.e., a number of FFN layers in parallel)  to boost model capacity without increasing much computation. Moreover, they propose a consistency regularization between a pair of experts in the training, which can alleviate performance deterioration of random expert selection in the inference. Strength1 The paper is well written with good structure2 The method is concise and novel to some degree. 2 I am wondering if this mechanism can be used in the vision transformers? The novelty and experiments are good to show effectiveness.<|endoftext|>The paper proposes a new routing mechanism for sparse models in the context of language tasks. A consistency loss is used to force experts to provide similar predictions. A number of experiments are provided suggesting the algorithm outperforms previous works. Also, I was wondering if there s concrete evidence to support this claim in page 5: "Although these experts are learned to make consistent predictions, they converge to different (local) optima given the randomness introduced in training [...]". This is a field that is gaining lots of attention recently, as models have grown large, expensive and inference time has increased accordingly. **I have some fundamental concerns with the experimental section. If my understanding of the experiments is correct, the comparison between THOR and the Switch Transformer [2] is not fair. Another couple of comments regarding experiments. I ve raised my score given the authors rebuttal and updated results (3  > 5).
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; The authors propose a new data augmentation technique, called AdversarialAugment, to increase robustness of image classification models. The proposed method optimizes the parameters of image to image models to generate adversarially corrupted images, where they also show sufficient conditions for the consistency in the simple setting. They empirically show that AdversarialAugment improves common corruption robustness on CIFAR 10 C as well as worst case performance against lp norm bounded perturbations on CIFAR 10 and ImageNet. Strength:      Connecting common image corruption robustness and adversarial robustness is a good direction. This is an important improvement over DeepAugment+AugMix, because these previous approaches didn t show robustness against adversarial perturbations. Previously, adversarially trained networks were shown to be robust to some corruption types, but they fail at corruption types like Fog. Future work might be able to fix this gap by leveraging AdversarialAugment. Weaknesses:      The current formulation seems to be much more computationally demanding than their counterparts such as AugMix, DeepAugment etc. It would be more fair to discuss the limitation on this aspect and/or compare the baselines controlled for similar computational resources. This paper contributes to the body of work that tries to tackle both types of robustness problems (common corruption and adversarial corruption) under a unifying view, which is an important step. While their method seems to be computationally demanding, it demonstrates good empirical performance along with theoretical justification.<|endoftext|>AdA generates augmented versions of an input by passing the input through an corruption network (such as a pretrained image to image model) while adding a worst case perturbation to the _weights_ of this pretrained network. The paper thus aims at using worst case perturbations for increasing average case out of distribution generalization such as common corruption robustness. The paper provides theoretical considerations that state assumptions under which AdA is well behaved (converges) and how it is related to prior work such as DeepAugment. The paper presents an extensive evaluation on common corruption benchmarks (CIFAR10 C and ImageNet C), domain shift (ImageNet R), resampled test sets (CIFAR 10.1 and ImageNet v2), and worst case robustness against $\ell_p$ perturbations. Strength:* the proposed Adversarial Augment (AdA) is a novel contribution that comes with the benefit of being a general purpose approach. Moreover, strong results are also shown for ImageNet C* paper is well written and experiments are well structured and presented (Table 1 and Table 2)Weaknesses: * Related work on sharpness aware minimization (SAM; Foret et al."Sharpness aware Minimization for Efficiently Improving Generalization", ICLR 2021) and adversarial weight perturbation (AWP, Wu et al., "Adversarial Weight Perturbation Helps Robust Generalization", NeurIPS 2020) should be discussed. Since the setting of SAM/AWP is more generic (no requirement of pretrained vorruption network), it would be important to clearly show an advantage of AdA over these works. * since the general method is not restricted to image classifiers or even image based tasks, including results on other types of data would strengthen the paper. Moreover, the additional corruption network needs to fit in memory, increasing the required memory. This should be part of a discussion of pros and cons of the methods. * AdA is by design a deterministic procedure that should generate the same augmented version of the input for a given model when applied repeatedly. A desirable property of data augmentation is diversity, that is: the same input is augmented differently in every application of the augmentation operator. Could the authors comment on why AdA is defined as a deterministic procedure? * Having error bars in Figure 3 that go beyond 1 does not make sense since SSIM cannot be larger than 1. This indicates that a different way of plotting the empirical distribution of SSIM scores would be better suited for the data. However, the strong empirical results come at the cost of highly increased computation at train time and it remains unclear if appropriately tuned SAM couldn t provide similar benefits. Finally, the theoretical considerations are based on an impractical assumption. Showing the potential of AdA on other types of data _and_ comparing to SAM/AWP could bring it above the threshold. ### Update after rebuttal ###The authors have convincingly addressed my concerns regarding a comparison to SAM/AWP, discussing the computational and memory overhead more transparently, and several further minor points. The authors describe in the discussion how the theoretical considerations could be revised to be grounded on more realistic assumptions. In summary, I lean towards acceptance because the paper is strong on the empirical side and provides a compelling approach   but this is in spite of (not because of) the theoretical considerations.<|endoftext|>This paper provides a data augmentation method that augments samples by perturbing parameters of a generative model. This is a very important drawback, which is  hard to justify, that only by combinations with other techniques, the proposed method becomes effective. Although one of the aspects of this work is building adversarial robustness into models, no thorough comparisons against SOTA adversarial training methods (such as [1,2,3]) are provided. The proposed approach seems to be significantly more computationally demanding than its counterparts AugMix and DeepAugment. and also on the linear interpolation, some statistics on what values of lambda have been used would be insightful, to demonstrate that actually the augmented samples have been largely influential, and not ignored by the guard or interpolation. In Appx C, it is mentioned that for ImageNet and CIFAR10 standard data augmentation has been used. Is this the basic setting? E.g, all methods used and evaluated in this paper, use the "standard" augmentations, in addition to the mentioned augmentations (e.g, DeemAugment, AugMix, Ada)? I recommend to rephrase or strengthen the claim by additional results. Adversarial Mixing [4] is an augmentation method that similar to Ada uses a generative model, as well as an adversarial loss, to create augmented samples. I expect to see how it compares to Ada, at least on some of the experiments. arXiv preprint arXiv:2010.03593 (2020). [3] Kireev, Klim, Maksym Andriushchenko, and Nicolas Flammarion. "On the effectiveness of adversarial training against common corruptions." arXiv preprint arXiv:2103.02325 (2021). [4] Gowal, Sven, et al."Achieving robustness in the wild via adversarial mixing with disentangled representations." Some relevant approaches have been missed in comparisons, as discussed above. ## after rebuttal:During the rebuttal, the authors responded to all my concerns, and strengthened their empirical evaluations with additional results. Also the claims of the paper are now aligned with the reported results, and the limitations of the proposed approach is clarified and sufficiently discussed.<|endoftext|>This paper makes use of image to image translation networks to generate adversarial data augmentation for input images. When combined with existing approaches, it achieves state of the art robustness against unseen corruptions and worst case perturbations. *Strength*In contrast to attacking the classifier, this approach makes a novel use of image to image translations network to generate corrupted samples for the classifier. *Weakness*I think that the proposed approach is significantly more computationally expensive than baselines (except DeepAugment) This is because the optimization process is solved over image to image translation networks. Unlike classification networks, these models largely operate on full resolution images throughout the network, thus have much higher latency. In particular, clean accuracy itself degrades significantly when using them as a source of adversarial dataset augmentation. Is it because of their inability to reconstruct the input image in absence of perturbation or hard to achieve high fidelity in presence of perturbations. Can authors shed some light on this phenomenon? I agree that the image to image translation network is expressive enough to model most image corruptions techniques, when specifically optimized for them (figure 3). However, we operate in an untargeted setting, i.e., not aware of the test time corruption mechanism. In this untargeted setting, does the translation network using corruptions similar to the existing set of corruptions (such as blurring, weather changes)? 3 appears above 1 (which is not feasible since SSIM is bounded between [ 1, 1].Is it just a plotting issue with errorbars? "The surprising effectiveness of linear unsupervised image to image translation." IEEE, 2021. This paper provides a general framework to defend against unseen image corruptions and adversarial perturbation.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; The paper is very good in this respect. Pros: State of the art performer for point cloud completion on various datasets. I am impressed by the comprehensiveness of the paper. Novel approach.<|endoftext|>This paper proposed a novel way to utilize DDPM s for point cloud completion. Their main contribution is is in the design of the conditional feature extraction and denoising subnet, which essentially form dual path connected U Net type structures with internal modules based on an improved PointNet++ design. I thoroughly enjoyed this paper.<|endoftext|>The relationship of CGNet, RFNet, CFENet, and Denoise Net are not well represented. I m leaning to reject this paper (Borderline Reject) *at this time* due to the following reasons:1. 2.The authors propose a novel pipeline for point cloud completion.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 5; The novelty of this work is incremental. The proposed model introduces numerous operations over adjacency matrix and combines them as final aggregation result. Experiments are conducted to show that the proposed method outperforms some baseline methods. This paper proposes an equivariant heterogeneous graph neural network model for heterogeneous graph representation learning.<|endoftext|>The focus of this paper is in the design of GNNs that are amenable to operate on heterogeneous graphs   that is, networks where relationships can be of different types. Experiments do not show a statistically significant improvement in the node classification task over H GCN.<|endoftext|>Most Graph Neural Networks (GNNs) are often proposed for homogeneous graphs or convert heterogeneous graphs into homogeneous graph and utilize them. This paper proposes a graph neural network which maintains its expressivity while retaining the equivariance property. 2.It goes on to show that all linear maps that satisfy the invariance property can be made from the given set of operations. The theoretically analysis is fairly clean and the model, as a whole, is very cleanly motivated.<|endoftext|>This paper proposes a heterogeneous graph neural network model that ispermutation equivalent. Weakness:1.The novelty of the proposed method is marginal. However, the novelty of this paper is marginal, the results arehighly varying, and some important baselines are missing. It achieved permutation equivalence by defining a setof linear mapping operations (contraction and expansion) that are permutationequivariant for of type specific adjacency matrix.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; I find that this work provides a thorough study of the efficiency accuracy trade off offered by ensembles of shallow models w.r.t.single deeper models. I may be overlooking something, but the simple accuracy efficiency plots (like Fig.1 or Fig.2) are already sufficient to draw the same conclusions and are simpler to interpret. These points are discussed in the paper, but either much later or in different contexts.<|endoftext|>(Though in the context of the overall paper, seems a relatively minor point, since a validation set was indeed used for the cascade). Overall, the measurements provided here demonstrating the effectiveness of this simple and sometimes overlooked approach, are welcome evaluations.<|endoftext|>Springer Verlag, 2001.   is also rather simple. ## Weak pointsGiven that efficiency is of core concern it would be good to at least discuss the concept and define what is referred to in the paper as efficient.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; The paper proposes a novel coordinate based network architecture which proposes to process each of the input coordinates independently in the first layer instead of together in a fully connected layer. A novel formulation of volume rendering which is compatible with the architecture change is proposed, and evaluated on one scene, which provides promising results that this can speed up the training of NeRF like models in neural rendering, but this section of the appendix could be expanded upon significantly to create a significantly stronger paper. Thus, I have updated my score, and think that the paper in its current state should be accepted. The degree of splitting versus number of layers comparison is insightful and shows that the method proposed is in fact more powerful than simply using smaller MLPs (i.e.the individual processing FC blocks for each coordinate are highly useful). 2.The contribution of the paper is quite clear. I think that the simple idea of splitting coordinate processing, ablating it well and showing that it doesn’t degrade quality but speeds up the training and evaluation is quite clear. While fitting a single image, video, or 3D signal with a coordinate based network is intellectually interesting as a toy problem, the main use of coordinate based networks is in neural rendering. The notation is a bit confusing in equation (2).<|endoftext|>Summary: This paper proposes a modification to INR models on multidimensional coordinate grids where a subset of the earlier layers operate on the decomposed coordinate grid. As it turns out, this absolutely works as advertised, is extremely easy to implement, and provides the advertised compute speedups, albeit at a slight cost in parametric efficiency. Detailed notes:  The authors mention that they require more parameters to reach a given performance level a few times, but I think they should be more upfront in specifying that this is a drawback of the method. * This also retains the specific design of the weight sharing layers as a visible contribution. This paper presents a technique of general interest to the subfield of implicit neural representations, has solid empirical results, and is in my opinion quite surprising. The paper is clearly in accept territory.<|endoftext|>This paper proposes a new architecture for implicit neural representations, called CoordX, which splits each dimension of the input signal into separate branches (e.g.the x and y coordinates of pixel locations in an image) and processes each of these separately before fusing them. This was one of the first thoughts I had when initially reading the paper and I think it would be interesting to expand more on this, e.g.in the context of [4]. The fused layers are processed by a few more MLP layers to output the predicted features. Post rebuttal update  I appreciate the large efforts the authors have put in to address the issues put forward in this review and by the other reviewers. The extended discussion and experiments around NeRF scenes are also helpful and appreciated. For example, on the image experiments, the training time can be halved but at the cost of a 3dB drop in PSNR. The authors refer to this as “a slight drop in quality” but 3dB is a very significant drop (PSNR is measured on a log scale). However, comparisons to e.g.[1] or other methods aiming to accelerate implicit reps are missing. Methods such as these would very likely both be faster and have better reconstruction accuracy than the proposed method. The related work section disregards some very closely related work in accelerating training + inference of implicit representations [1]. While [1] can arguably be considered concurrent work, I believe there should be a more fair discussion of this.<|endoftext|>The paper proposes an interesting tweak to the network architecture to accelerate CoortMLP. The idea is to split the input coordinates along the dimensions and then share weights before fusion. The authors analyze the theoretical upper bound (as far as the MAC ops are concerned) and show about 2X speedup on actual machines. I have no major issue with the paper. The algorithmic novelty, however, is rather limited. It s incremental, but delivers good results.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 3; They show that including an Independent Component layer (using batch normalization and dropout) before every fully connected layer, followed by a ReLU nonlinearity, and adding skip connections, help pretrained MLPs to generalize well on different datasets. The primary strength of the paper is the careful experimental study the authors conducted to show the utility of the Independent Component layer and skip connections in building deep feedforward neural networks. There are a few suggestions in which the authors may try to build from the current version. Examples of such phenomenons can be smoothening the loss landscape or some underlying implicit bias of skip connections. Another question is whether we have faster optimization with the skip connections and the IC layer.<|endoftext|>This paper does not bring new knowledge to the community. MLPs have been widely generalized with dropout,BN and skip connections. The paper is casually written, the  whitening  is not a real whitening. This kind of basic analysis is of little interest to the community. The authors may investigate some fast whitening methods that are still not widely used in transformers to make the work a little more useful. The paper is below the ICLR standard.<|endoftext|>The paper proposes a network architecture for MLP networks. Further they use skip connections (He et al., 2016) in their architecture. Based on these, the authors suggest a new neural network architecture for MLPs. At the end of the paper, the authors provide an analysis on MC Dropout with their architecture, which seems interesting, but which would also require more detailed comparisons to the behavior of MC Dropout with other architectures and a more detailed quantitative analysis w.r.t.data that is infrequent in the training sets. Furthermore, it would be interesting, whether the architecture would also work in an end to end way well (e.g., at the top of a CNN), and it would be interesting how well the architecture especially works for tabular datasets as e.g., the Tox21 dataset with circular fingerprints (e.g., ECFP features) or e.g., datasets from the UCI machine learning repository. Since the experimental evaluation of the proposed network architecture was not convincing for us, and since there are no new theoretical analyses in the publication, we have doubts on the relevance of this paper for ICLR and therefore recommend rejecting the paper.<|endoftext|>This paper proposes a general architecture for MLPs. The main ingredients are skip connections and whitening. The authors show that MLPs do benefit from them as well. The whitening (BND) is implemented by a batch normalization (BN) layer followed by a dropout (D) layer before the fully connected layer (FC). The authors claim that no one had proposed the actual block for MLP before. So, potentially, the proposed block architecture could be useful to many practitioners. Unfortunately, the empirical results do not convince one that their particular block is superior. Also, the ablation does not include previously published similar blocks (FC BN R D by Martinez et.al.2017, and FC BN D FC BN D R by Siravenha et.al.2019). Therefore it is not made clear that the particular order of layer in the proposed block is providing any benefits empirically.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 8; "Positive unlabeled reward learning." Strengths:* The work tackles the important problem of overfitting in inverse reinforcement learning. Drawing insights from invariant risk minimization the work proposes a well motivated and theoretically supported objective. * Figure 2 is convincing in showing that larger regularization strength results in better rewards with the IRM compared to the ERM objective. [2] Zolna, Konrad, et al."Task relevant adversarial imitation learning." * The paper was well written and had a good background work section. The paper has several major issues from the large assumption of interventions in the expert dataset, to insufficient comparisons to baselines, and insufficient experimental evidence. This is not a realistic setting as the demonstrations need to be generated with special care for different state interventions and the learner has ground truth knowledge of these different settings. This comparison is especially important because the proposed method is a form of regularization. The L2 normalization baseline also is missing from all experiments except the grid world experiment. More recent methods than AIRL, such as f IRL [3], have been proposed which perform better than AIRL. While the authors mention that [2] do not study IRL, it seems like the techniques from [1,2] can easily be extended to IRL? The main experiments only show results on a small handful of tasks. I think experiments on more environments are necessary (such as a larger set of the Gym MuJoCo tasks).<|endoftext|>This paper investigates a specific technique for Inverse reinforcement learning to avoid the detrimental effect of "spurious correlations in the data by the learning model". The objective is to avoid "behavioural overfitting to the expert data set". The paper uses an invariant risk minimization principle as a regularization approach for the maximum entropy inverse RL problem. As the paper is mainly empirical (no theoretical justification for many choices), the experimental parts should provide more insights or relevant experiments to be convincing Overall the claim that "the regularization objective for inverse reinforcement learning recover robust reward functions which avoid to learn spurious correlations present in demonstration data sets" is not clearly established from the paper. Some other elements from the paper are unclear. These correlations coincide with the binary label information encoding the optimality of the expert." Is the source code provided? Additional remarks:  Section 3: the state space and action space are not clearly defined. Are these discrete or continuous spaces?<|endoftext|>The authors  propose a regularization loss for inverse reinforcement learning (IRL) based on invariant risk minimisation (IRM). They close by evaluating the reward function learnt by this regularized adversarial IRL in terms of its utility in training a new policy from scratch. The intersection of IRL and IRM is quite novel as far as I know, and at a high level it seems like a promising approach. But this paper is let down by a glaring lack of details and some paper structuring issue, with more fundamental issues lurking beneath them. The biggest oversight is the complete lack of an appendix. Converting the gradient of the log likelihood loss into the difference of feature expectations is exactly the sort of prior work that needs to be understood, rather than merely cited. It is also unclear where these features come from ("output of a neural network" is far too vague), which ties into my larger complaint about the lack of relevant detail. There is a reason GAIL and related approaches are typically framed as  imitation learning  rather than IRL: asymptotically their reward function should converge to being constant (i.e.when the policy is so good than one can t distinguish it s state occupancy from the experts ). 2.2) I agree the IRL approaches tend to overfit when expert data is limited, but this is separable from the issue of generalizing to novel MDPs (i.e.different state transition dynamics). This paper conflates these two issues throughout; seemingly motivated by the first, the experiments only address the second. 3.3) Figure 4 is very confusing. If the experts are optimal for completely different reward functions, what should be expect imitating their aggregate policy to yield?<|endoftext|>The paper studies the Inverse Reinforcement Learning (IRL) problem, addressing the issue of avoiding overfitting when having access to a finite set of expert demonstrations. The paper proposes an approach based on Invariant Risk Minimization (IRM) as a regularization approach for maximum entropy IRL. After having presented the method, an experimental evaluation on both finite and continuous environments, showing the advantages of the presented approach. The experimental evaluation, especially the gridworld experiment, effectively shows that the proposed approach is able to generalize very well, recovering a very smooth reward function. Moreover, the recovered rewards, in the LunarLander experiment, display better transferability properties compared to plain maximum entropy IRL. Can the authors clarify? Minor issues  The ticks on the plot axis are very small  Pag. Overall, although the paper is not free of weaknesses, I think it addresses a very important problem of IRL making a first step towards the understanding of the generalization of reward functions recovered by IRL.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 3; This paper presents a simple classification based approach to a newly defined problem: determining ethno nationality of writers of English text data. The empirical validations are mostly applications of standard (and even obsolete) classification models on the newly collected data for the introduced problem. 2) Suitability to ICLR: I don t see how this paper aligns with ICLR at all. This is in its essence a straightforward application of simple ML based techniques to a socio linguistic problem. There s nothing in the paper advancing representation learning methodology or even advancing our computational tools to study linguistics   such work is tangentially relevant also for conferences such as EMNLP, not to mention more general ML conferences.<|endoftext|>The work aims to automatically determine the country of origin of authors given their English texts. There is a line of works by Shuly Wintner that concerns authorship and style modeling in non native languages, and there may be more related works that are missed here. The documents are represented as TF IDF weighted vectors of words, and a classifier is applied (it is not very obvious which classifier exactly, perhaps I missed this detail?). The paper is technically below ICLR threshold.<|endoftext|>They evaluate standard algorithms such as decision trees and support vector machines. I think the motivation for studying ethnonationality is a bit weak. Both motivations are, in my view, ethically questionable, but the study of ethnonationality also has direct applications for *improving* the fairness of NLP systems. In the context of forensic linguists, Australians are not gonna talk about surfing or kangaroos, especially not if they want to hide where they’re from. b) The description of state of the art is out dated, e.g., see [0 2].<|endoftext|>This paper aims to determine the country of origin of English language writers. The main premise of this work is that previous work in this space used controlled settings such as English learners texts or standardized examples, while this work examines more naturalistic settings. This is a reasonable motivation. However, besides that, the contributions of this work are very limited: it uses standard algorithms and features, there isn t much analysis of the results, and so the reader does not learn much new from this study. It also doesn t seem like a very good fit for ICLR. This paper is an empirical study of standard methods on the problem of classifying a writer s country of origin.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; The paper is relevant since it introduces new and useful ideas in the space of privacy and it has practical applications. A way to fix the previous is to do “shuffling”, pool LDP responses, apply a permutation and pass that to the analyst. This paper introduces d_sigma privacy which provides a “tuning knob” between unshuffled LDP and shuffled DP, which rigorously allows trading off privacy and data learnability.<|endoftext|>The paper proposes a neighbouring information based DP framework to bridge between local DP and shuffling based global DP. The utility of this framework is validated using experimental evaluations on three datasets. The definition is intuitive and reasonable. It also yields interesting resilience results against Bayesian and decision theoretic adversaries. shuffling DP, which is the central problem of the paper. The heuristics used in the proposed privacy preserving mechanism is not well analysed and well justified. The results derived for the decision theoretic and Bayesian attackers are useful and provides insights.<|endoftext|>The paper proposes a privacy model in between the guarantees of local DP and shuffle DP. As a result, it achieves better utility than LDP. But then the approach proposed by the paper is to reveal that the user is part of some group. The privacy definition also feel a bit like k anonymity which is is know to be susceptible to attacks based on belonging to a group. That is, can one say you achieve shuffle DP for a group of users?
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; The method proposes a two branch discriminator, one for content discrimination and the other for layout discrimination. Experiments show the method generates diverse and high quality scene images. The proposed method generates good quality results. However, I think the technical novelty is limited and the task of learning from a single video is much simpler than learning from a single image. 1.The method proposes to learn new scene generation from a short video. Although a short video takes similar access effort as an image, it literally contains at least tens or hundreds images. 2.The paper didn t use the temporal information in the video, so what s the difference from a video and hundreds of frames? In current setting, if z1 and z2 is very close, the loss also encourages the generated images G(z1), G(z2) to be different. In summary, the statement of learning from a single video is basically the same as learning from hundreds of images, and is thus much easier than learning from a single image. The proposed contributions, i.e., the two branch discriminator and diversity loss, lack technical novelty (see above). So I think the paper is not good enough for acceptance.<|endoftext|>This paper proposes an approach to train an image generator using only a short video clip or a single image. The overall framework follows the design of sinGAN that has an unconditional generator that generates new images from random noises, and the generated images and the input real images/frames are fed to discriminators to enforce similar distributions. The key contribution is to disentangle the image into content and layout features, and use different discriminators, which is claimed to reduce the memorization issue. This paper improve the sinGAN on image generation using very few training images. For the technical details, the paper is written well and easy to understand. In feature augmentation, what exactly does "For the single image setting this is done by mixing the features of two different augmentations of the original image"? 2.In feature augmentation again, I am a bit concerned about the proposed augmentation. I am either not sure about the content augmentation. A discussion could help understand the intuition. I am okay with acceptance, but I am a bit concerned that the contribution might not be significant enough for acceptance, since it is essentially the sinGAN setting plus content/layout separation.<|endoftext|>The paper presents a new GAN model that generates new scene compositions from a single training image or a single video clip. The paper overall is well written and easy to follow. [Motivation] The authors worried about training GANs with limited data will lead to memorization of training images (claimed in the abstract). Otherwise, learning from a single video is actually learning from multiple frames. I think the novelty can be limited. 5.[Approach] Since there is only one real sample during training, I don’t think this two branch discriminator is able to prevent memorization as claimed on Page 5. In my view, the regularization technique is actually a maximization of the feature distance of different samples. The results of ”no content branch” also show a worse layout, which is hard to verify the effectiveness of the layout branch and content branch respectively. 8.[Experiments] I suggest ablation studies (by visual comparisons) on the feature augmentation and the diversity regularization, to verify the effectiveness of the proposed model. Overall, I vote for rejecting.<|endoftext|>This paper proposes a single image/single vide GAN model that obtains both high quality and high diversity in the generated images, as opposed to previous models that only achieve one of the two. To achieve this, the discriminator consists of two branches that evaluate global layout and quality independently and a novel regularization approach for the generator is introduced. Comparisons to SinGAN, ConSinGAN, and FastGAN show improved performance. Based on the observation that previous models either fail to generate realistic layouts (SinGAN, ConSinGAN) or collapse to the training data without diversity (FastGAN) the paper proposes a new discriminator architecture to address this. One branch is trained to judge the global image layout while the second branch is trained to evaluate individual object quality. At the generator side, a diversity regularization is added which forces to generator to generate different images for different latent samples. While the approach is intuitive and does seem to lead to clear improvements I wonder how well it translates to applications such as image harmonization/editing/animation/retargeting/etc. Since at least the single image GANs seem to be able to perform those tasks without or only minor changes in the training pipeline it should be relatively simple to evaluate this approach on some of these tasks, too. This may affect tasks such as animation or latent space interpolation. The proposed novleties make sense and seem to lead to clear improvements. However, the evaluation is only based on unconditional image synthesis and ignores many more practical applications for single image models such as animation, harmonization, retargeting, etc.
Reject; rating score: 3; rating score: 5; rating score: 5; This paper presents a model based RL algorithm that provides decomposed dynamics models when the state and action spaces are defined as a multi dimensional Cartesian space. This paper proposed to build multiple world models, one per action space partition, and average it to improve the model error. This idea is generic to other MBRL approaches, and the paper implements the method in "Dreamer" and "MBPO" and shows improvements in the model error. #### Strengths* The idea is simple, and the experiment results showed improvements in model error or value in some domains#### Weakness* If I understood correctly, this action space partitioning can only be applicable when there is a clear decomposable structure in the action space. #### Questions* Have you experimented with this dynamics decomposition approach to other domains like ATARI? * When combining the latent states, the paper proposed to use averaging the outcomes. This paper shows MBRL methods that utilized the decomposition of the action space and showed experiment results on the control domains. In some domains, we see improvements in the model error or higher values.<|endoftext|>The authors propose a clustering method to decompose world models for model based RL into sub models reflecting the dynamics of the task. I have some issues with the technical approach, raising doubts over how this method will generalize. Then they average the output of all submodels to get the new state, also a strong assumption that the state can be additively decomposed this way. Experiments:  The Dreamer and MBPO papers you compare against have 20 and 6 benchmarks respectively, it is unfortunate that you only present a subset of 4 from each. I recommend another pass. Your state space definition is seemingly much more complex than theirs, but as I understand it you use the same benchmarks. The problem of learning decomposed world models is interesting and the authors do show a small improvement on most of the presented benchmarks. The empirical results are not very strong to begin with, and were only presented for a smaller subset of the benchmarks in the papers they compare against.<|endoftext|>The authors propose an environment dynamics decomposition (ED2) framework to decompose the environmental dynamics into multiple sub dynamics. Empirical results show that ED2 improves the performance of several model based reinforcement learning (MBRL) algorithms. The sub dynamics model is therefore not a dynamics model. Q2.As the underlying idea of ED2 is very simple, I believe it is possible to theoretically analyze the advantage of ED2, which can improve the paper’s contribution significantly. Q3.Doya et al.[1] also leverage the idea of dynamics decomposition to model the environmental dynamics but decompose the dynamics into multiple domains in state space and time. Although their method is different from ED2, I suspect that it is important to discuss the difference between the two studies because both are based on the idea of dynamics decomposition. Q4.The introduction section mentions many MBRL algorithms, such as ME TRPO [1] and SimPLe [2], but the authors empirically compare ED2 with only two MBRL methods in the experiment section. ICLR.2019.Although the paper is well written, the current contribution is not enough for me to recommend acceptance.
Reject; rating score: 3; rating score: 3; rating score: 8; Additionally, the ablations and design choices for transformation & loss functions should be done on more than one dataset to be able to conclusively make the claims made in the paper. The authors claim "theoretical explanations" as a contribution of the paper.<|endoftext|>The paper proposes a distillation approach for the learning to rank setting. Weaknesses:My major concern is with the novelty. I also have a concerns about section 4.3, ReLU is an odd transformation to use in the learning to rank setting as it squashes all negative scores to 0 making that part of the ranking indistinguishable.<|endoftext|>The authors clearly state two important design choices: 1) a listwise Softmax loss function and 2) a teacher score transformation. 2.Extensive experiment studies are conducted to demonstrate the effectiveness of the proposed method. There are several places where I am not certain, or maybe can make the paper better:1. 2.In (4) there is a hyperparameter \alpha, and in (7) there is a and b for the affine transformation. I believe this paper is well written with clear statement of their research questions, extensive experiments, and rigorous arguments.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; Even for the simple problem setting, the paper doesn t have performance guarantees for the proposed policy in the non asymptotic region. The analysis is limited that the provided performance results are only in the asymptotic region when the discount factor converges to one.<|endoftext|>This paper tackles a two armed bandit problem (of means $Ber(1/2+\mu)$ and $Ber(1/2 \mu)$, respectively) when memory is limited. In this case, the authors present a policy that plays the same action sequence cyclically ( necklaces ). The main novelty, in my opinion, is the presentation of the Gray ordered necklaces, which I find to be very nice and quite original (while the CCP policy is well studied).<|endoftext|>The authors compare two different memory architectures for the two hypothesis testing problem. As is, the work is simply too narrow in scope and experiments to be a significant contribution to modern techniques and understanding.<|endoftext|>The comparison is limited to a single problem. The paper seems to be missing that literature.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; This paper addresses the very important problem of reducing the memory footprint of neural networks training. The proposed scheme of optimizer state quantization has three components (I) block wise quantization which osolat4ed outliers impact on the error (ii) dynamic quantization which quantize both small and large values with high precision and (iii) stable embedding layer which improves the stability of the optimizer during training. There are several broken references that should be fixedThe proposed paper proposed 8 bit quantized optimizers counterparts for saving memory footprints during training.<|endoftext|>The paper shows a working implementation of 8bit states for momentum and second momentum. This is achieved by using block wise dynamic quantization for efficient compression and fast implementation. They show drastic improvement over f32 diagonal optimizers (mainly Adam) and improvement over a subset of previously proposed sub linear memory optimizer (AdaFactor f32 variant). It would have been interesting to see the method is applicable for large batch training.<|endoftext|>This paper proposes a non linear block wise quantization method to reduce the memory overhead of stateful optimizers, without sacrificing performance going from 32bits to 8 bit. This paper shows very impressive empirical results and has an open sourced codebase which makes it reproducible. These empirical results themselves are very valuable to the community given there are not many open sourced codebases, to begin with.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; rating score: 6; This paper proposes a white box attack to generate rectangular patches of pixel color shifts as a means to generate adversarial examples for DNNs. The method proposed in this paper is very similar to Square Attack proposed in [1], except that it is done under the white box setting rather than the original black box. The related works section should mention and differentiate this paper from them. 2.The evaluation is lacking in terms of compared attacks. The results are missing important comparisons here. They can simply be reduced to sequences of patches, or even sets of patches if order independent. 2.SyGuS is not rigorously introduced in the paper, even the section introducing APSyn does not mention SyGuS and how APSyn is related. It is unclear why the authors chose to emphasize the patches as programs. https://arxiv.org/abs/1706.06083[5]: Croce et al., Reliable evaluation of adversarial robustness with an ensemble of diverse parameter free attacks.<|endoftext|>This paper proposes APSyn to generate a sequence of imperceptible adversarial patches for a physical and imperceptible attack against image classification models. Also, imperceptible patches are studied in prior works [4,5]. These papers should be a more reasonable baseline to compare with. The experiments on small subsets of the MNIST, Fashion MNIST, CIFAR 10, and ImageNet demonstrate that the proposed approach can achieve smaller L_0 distortion and larger L_2 distortion compared to the C&W attack. (II) It is interesting to see the authors formulating the attack as a program synthesis problem (though I have concerns about the necessity of such a formulation; see weakness II below). As a result, there are existing solutions (e.g., functional adversarial attacks [1]) to this problem, which undermines the contributions of this paper. I suggest the author at least report the trade off between attack success rate and attack time (balanced by the number of patches/candidates). Since the paper does not evaluate the attack in the physical world setting, I would suggest the authors rephrase the problem as finding a balance between L_2 and L_0 attacks (instead of claiming the attack is physically realizable). In “adversarial programs”, there is an inconsistency between $I_{\alpha,\beta,p}$ and $I_{\alpha,\beta,m}$. [4] Liu et al., “Perceptualsensitive GAN for generating adversarial patches”, AAAI 2019. 1.The problem is not well motivated and justified (see weakness I), and it might be trivial to succeed in this attack (see weakness III.2). 2.The problem of imperceptible patches is not new [4,5]. 3.The loss for the attack is from existing work (C&W attack). 3.The evaluation is incomprehensive, and the performance comparison chooses an unreasonable baseline (see weakness III).<|endoftext|>This paper studies multi patch adversarial attacks, where each perturbation is generated via a program. This work frames the adversarial attack as a program synthesis problem, which is interesting. However, I don t think this problem setup is very well motivated. Specifically, there might be simpler adaptions of existing adversarial attack algorithms that can achieve similar or better performance, while they do not require a computationally expensive program search process. 1.I find that the confidences of the generated adversarial examples are all pretty low. What is the attack success rate of generating such strong adversarial examples, and what is the execution time? If the goal is to generate multi patch adversarial examples, one simpler approach is to randomly generate several masks, and optimize the perturbations in the regions covered in the mask. Have you run this baseline? 3.Is there any reason to build upon C&W, instead of some other adversarial attack algorithms, e.g., PGD? Have you tried existing L0 adversarial attacks, and does your algorithm perform better than these existing attacks? 4.The authors demonstrate some generalization results, i.e., using the same program to generate perturbations for multiple images. While it is interesting to see some transferability, the current generalization rate is still pretty low, and is not competitive compared to prior works focusing on universal adversarial perturbations.<|endoftext|>The paper proposed a new adversarial perturbation by stacking imperceptible adversarial patches. By applying an adversarial patch into the input sample (e.g., image), the values of input inside the mask will be scaled and shifted. This paper regards the generation of adversarial examples as a programming problem and designs an adversarial patch based programming language to handle this issue. Experiments demonstrate the effectiveness of APSYN. 3.Considering that each adversarial patch only scales or shifts a specific part of the input, the generated adversarial examples are more explainable compared to other white/black box gradients based attack algorithms. This paper aims to generate a kind of physical imperceptible patches. These patches are valid and imperceptible in the constraint of $\ell_0$ or $\ell_2$ norm but that doesn’t mean they are still valid in the physical world. So it seems like the APSYN is a black box attack algorithm. 4.Missing some related physical world patch attack methods in the related work [1,2]. European Conference on Computer Vision. [2] Xu, Kaidi, et al."Adversarial t shirt! Generally, this paper proposed a novel programming based adversarial perturbation and it is effective and competitive in various benchmarks.<|endoftext|>This paper focuses on crafting imperceptible patches to fool image classifiers. The experiments on various image datasets demonstrate the effectiveness of the method. The idea of the paper is well motivated, and it is a natural direction to explore the possibility of imperceptible patches. The formulation of adversarial programs is neat and quite clever to get around the discrete nature of gradient. I do notice a couple of places where the reality is less optimal than the assumptions such as eta_c vs eta_d, these empirical values are sometimes physical constraints that researchers have to deal with. In general, I like the evaluations shown in figure 2 figure 5, these results indicate the proposed method is effective. The title of the paper boasts “physical”, and yet figure 6 indicates the experiments were mainly carried out digitally. I.e.how effective is the proposed method when deployed in the real physical world? 2.This work mainly used l2 distance to measure distortion, but l2 distance sometimes could be flawed itself, how about measure the distance between 2 distributions? Would that change the results?
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; The paper s core contribution is to extend randomized smoothing to a setting where the smoothing depends on the input data. That is, the variance of the smoothing distribution is not constant anymore but depends on the input. However, I am some concerns about the proposed approach:1) The core idea of the approach is to optimize the sigma according to Eq.(2).However, this optimization principle assumes the constant sigma set up. (Side note: I find it a bit surprising to call the solution then the "radius of certification" even though it does not lead to a valid certification radius.) This, however, leads to multiple further problems as described in the following. Therefore, the original problem setting is actually never solved by the approach. Based on my understanding the memory based principle could theoretically be used with any real number used as a radius. Finally, since the certificate is given for this "changing classifier", I highly question the (practical) use of the certificate at all. It is even worse than a KNN classifier since one has to consider all test samples (and not only the training samples). That is, the predictions might be different when a different order is used. I might be open to increase it in case the authors clearly state that they are actually not certifying the smooth classifier.<|endoftext|>It also proposes a simple memory based approach to certifying the resultant smooth classifier due to the lack of guarantee on the overlapping of the perturbation regions. I found it a big surprising that this has not been done before. So it seems a bit over claiming in the abstract that the method is generic. In addition to this, I believe there are bounds for the other norm certification, and I wonder if the proposed method can be applied to those cases? Another comment is about the setting of the parameter n in Algorithm 1. It seems the authors only consider setting n   1, which I think is highly inaccurate with high variance. The idea of this paper is interesting and the paper itself is a solid work to improve model robustness by optimizing the certified bounds w.r.t.the noise variance.<|endoftext|>The paper focuses on training robust deep neural networks. The author(s) developed a data dependent randomized smoothing method to certify the DNN classifier. The paper is not in my area but I find it easy to read and follow. I agree that the making the variance of the Gaussian noise to be data dependent has the potential to largely improve the performance of the randomized smoothing classifier. However, it is not clear to me that why maximizing the radius $R$ would be beneficial to the resulting smooth classifier.<|endoftext|>Specifically, in the original randomized smoothing approach, we use a fixed variance parameter $\sigma$ for the Gaussian smoothing. The paper also provides a simple procedure for the certification of the data dependent smooth classifier. The main contribution of the paper is to adapt the smoothing parameter $\sigma$ to every input data point by maximizing the certification radius $R$. This is a natural way to improve the randomized smoothing technique. So will this approach apply to models that have low classification accuracy? If we apply this data dependent randomized smoothing to perform adversarial training, it seems that we need to generate a specialized gaussian noise for every single sample at each time. In the SmoothAdv algorithm, there they can reuse the gaussian noises to corrupt a batch of samples (because $\sigma$ is fixed). Can the authors comment on the scalability of your approach? See the comments.
Accept (Poster); rating score: 6; rating score: 5; rating score: 5; This paper analyzes the bias introduced when regions of images are modified in order to measure the effect of a model with and without those features present in the image, for model debugging purposes. Overall the paper is well written and the authors provide clear explanations at each stage whilst analyzing the effect of various missingness methods across convolutional neural networks and the Vision Transformer architecture. The authors build on the compelling case that when using debugging methods such as LIME, that iteratively block out features (or regions of the input image in the vision case) to analyze the behavior of convolutional neural networks with and without those features, the bias introduced by the blocked out regions (via semi arbitrary manipulation such as blackout, blurring, etc) can dramatically impact the behavior of the model. For VIT architectures, dropping target regions is a clever solution that was demonstrated to produce superior explainability and debugging results when compared with a CNN and blackout or blurring. While overall I found the paper to provide meaningful insight and analysis, the paper could be improved through further analysis of adversarial robustness of CNNs vs VITs. While some analysis was conducted when analyzing the retraining of models with missingness augmentation, given that retraining proved to significantly improve the results from the CNN, I would encourage the authors to more thoroughly analyze whether the superior capability of the proposed VIT token drop method is a result of this behavior rather than blackout and blurring being inferior/problematic methods. Additional analysis by the authors into feature robustness and retraining with CNNs vs VIT to determine whether the superior explainability is due to the regions not being considered in the token drop method as argued by the paper, or whether the propensity for more robust features in VITs is the reason.<|endoftext|>The paper discusses that it is common in Computer Vision debugging and explainability techniques to remove image regions to attribute different regions of the image to the decision of a classification model. In fact, even after removing some image regions randomly, the output distribution gets highly skewed towards a few classes e.g., crossword, jigsaw puzzle etc. Some such experiments are listed in detail in the weaknesses section above. Moreover, this bias can be overcome if the model is trained with suitable augmentations that remove regions while training. However, I would like to see the authors to discuss some possible solutions to this problem so that the work does not look like an illustration of an already known problem in Computer Vision only. This work definitely will help progress the research in this. Hence, these issues seen in CNNs due to missingness bias is not prevalent in ViTs. ii) The experiments are well thought and executed. iv) The paper is clearly written. Weaknesses:i) The authors have shown all their results in section 3 by only blackenning rather than any other baseline value. 2 and 3].iii) It would have been good if authors could show the results on more models like EfficientNet or even VGG instead of only ResNet. So, it is quite natural that all these so called ViT variations will give exactly the same features with same importance for all these baseline methods. After analyzing an already existing problem, it would have been great if the authors would have talked about any method to overcome it. Shifting to transformer based architecture may not be a solution for explaining and debugging CNNs.<|endoftext|>The authors focus on the problem of model debugging (for image recognition). Then, they demonstrate that the proposed method can simply  skip  those patches that are masked and it does not result in a skewed result. The motivation of this paper is interesting, the paper is well written and it uses various visualizations to demonstrate the issue on CNNs/ResNets. However, my concerns are the following:    The paper claims that a ‘hole’ cannot be left in the image in convolutional networks. This is not clear why it is the case, since a convolutional network uses a sliding window to convolve each pixel with the kernel of the layer. The previous point of skipping some parts of the image with the sliding window, makes me wonder what the results on ResNet would be in this case; currently, the paper showcases poor results on ResNet, but it seems plausible that it was not optimized for this task. I am not convinced about the **utility of the proposed method**; ViT is a state of the art method only when it comes to extremely large scale image recognition; in fact, the authors of ViT recognize that if only trained on datasets like ImageNet it can perform similarly to ResNet. For instance, **can the proposed tool be used for debugging classifiers on medical imaging or similar constrained size datasets** that are not related to ImageNet? Another limitation of the method is that practically only a single task and mainly ImageNet (?) The contribution is based on the empirical validation across a board of tasks and across datasets, which at the moment is not the case. Currently, only ResNet is used as a baseline, while previous work [Sturmfels, 2020] used Inception v4 instead.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; The paper extends the online meta learning (OML) model with the ER during the meta training. Also, the paper proposes a replacement buffer policy for samples replacement from the reservoir. 1: Integrating the experience replay (ER) during the meta training phase is the key contribution. The paper mentioned that using ER during meta test only but not meta train creates inconsistency.<|endoftext|>The paper studies the meta continual learning problem. Representations replay instead of samples replay are used to improve the performance of meta continual learning. Experiments demonstrate the effectiveness of the proposed methods. However, the contribution of this paper is minor. It is a combination of several incremental improvements.<|endoftext|>This paper proposes to  integrate ER into meta training of online aware meta learning (OML) Javed & White (2019). The authors propose to store the samples’ representations, not the samples themselves into a replay buffer. Weakness:* The technical novelty of the proposed method is limited and is incremental compared to online aware meta learning (OML). The difference is that the proposed method  integrates ER  into the meta training of OML * The clarity of presentation could be improved. It is unclear to see the benefit and necessity of using ER during meta training only from the derived gradients. Rahaf Aljundi, et al.NeurIPS 2019The technical contributions of this paper is limited compared to OML.<|endoftext|>The paper looks at the problem of catastrophic forgetting in continual learning. They propose a method that extends OML to use ER both during training and testing. They use meta learning to select samples that are useful for avoiding catastrophic forgetting to be in the ER instead of reservoir sampling. The paper is written very well. Unclear about the significance of the contribution.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 3; The paper focuses on the noise edges in face clustering fields and proposes Adaptive Neighbour discovEry in the strucTure Space (Ada NETS). **WeakPoints**First, note that the reviewer is positive about this work, and this section is much like questions rather than criticism. It would be nice to refer to the webpage here: https://github.com/deepinsight/insightface/tree/master/model_zoo. Unless some critical issues during the reviewing process occur (such as missing very important s o t a methods), and if detailed comments of the authors are provided, the reviewer thinks the paper is worth publishing in this stage. **Post Rebuttal**Thank you for the rebuttal. The reviewer is satisfied with the feature extraction process and experiments on features with various recognition performances.<|endoftext|>The paper provides a new perspective to boost the performance of GCN based face clustering methods. In summary, the novelty is good and experimental result seems promising.<|endoftext|>In order to address the problem that face clustering is often degraded by noise edges in similarity graph, this paper proposes Ada NETS by constructing clean graphs for GCNs. The novelty of the proposed method is limited. The paper is easy to follow;2. Novelty is limited and some motivations are not clear.<|endoftext|>Unfortunately, I still believe that the paper s focus (face clustering) is too narrow for ICLR and that the novelty is limited as a number of existing techniques are put together to solve a very domain specific problem. The authors put a lot of existing techniques together to build their algorithm but overall the proposed combination as a whole does not constitute a significant and widely applicable contribution to the field that might be of interest to the ICLR community.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; The authors propose a new approach for representation learning on hierarchical data. How scalable is your approach, especially in comparison to the established hyperbolic embedding approaches you compare against? This can be mitigated by regularizations. It would also be good to include a comparison with embeddings into products of constant curvature manifolds (e.g., Gu et al., ICRL ‘19). The paper proposes an interesting approach for embedding hierarchical data into complex hyperbolic space. I have some conceptual concerns and also think that a more comprehensive experimental comparison with existing approaches would improve the paper (see comments above).<|endoftext|>The paper introduces an extension of real hyperbolic embeddings to the complex hyperbolic space [A]. The resulting manifold is of nonconstant negative curvature, which the authors expect to be favorable for embedding various hierarchical structures. One major weakness of the paper is that the motivation of using the complex hyperbolic space instead of more recent approaches such as mixed curvature representations [E] or ultrahyperbolic embeddings [F] is not clear. Moreover, even after reading the appendix and the experimental section, I do not understand the type of relationship that the distance function in Eq.(9) is supposed to describe. For instance, hyperbolic embeddings have been extended to hyperbolic (graph) neural networks, just like ref [E] was extended to graph neural networks [G].<|endoftext|>In most real world hierarchically structured data such as taxonomies and multitree networks have varying local structures and theyare not trees, thus they do not ubiquitously match the constant curvature property of the hyperbolic space. Through experiments on synthetic and real world data, the author s work shows promising empirical results. The motivation is not clear to me. How do complex hyperbolic spaces solve the varying local structures? For example, there are both tree local structures and ring local structures in a network. **2.What kind of graphs are suitable to be embedded in the complex hyperbolic space is not clear**(More questions could be raised in the following discussion.)<|endoftext|>Despite the slight concerns mentioned above, I still think the idea and the methodology of the paper is good enough for ICLR. The authors argue that the complex hyperbolic space can have variable negative curvature, which can better capture the varying local structures in real world data. Hence, I lean to recommend accept of this paper. The authors propose to learn complex hyperbolic embedding, which outperforms the real hyperbolic embedding in graph reconstruction and link prediction tasks. The proposed complex hyperbolic spaces can potentially inspire more advanced method in the field. I hope there will be a figure to illustrate at least one such example. However, I have some slight concerns about the paper. Please either provide more experiments or elaborate on why such an experiment is not done. Classical methods such as perceptrons and SVM [4,5] should not be ignored, as they come with convergence guarantees.
Reject; rating score: 1; rating score: 3; rating score: 3; rating score: 5; The paper presents a gradient based hyperparameter optimization method, wherein a derivable parameterization is proposed for various popular CNN hyperparameters including kernel size, number of channels and hidden layer size.<|endoftext|>The authors proposed a differentiable method to optimize neural networks and hyperparameters simultaneously like DARTS. Forward and reverse gradient based hyperparameter optimization. As mentioned in Main Review, the contribution of the paper is not enough. 3.The paper didn t explain their method well. Moreover, there are many writing issues and typos.<|endoftext|>This paper proposes to tune hyperparameters in a differentiable way by using a modified version of softmax function. It covers the tuning for three kinds of hyper parameters as examples: channel size, kernel size and hidden layer size. The proposed method itself is reasonable, however, this paper misses many previous works on this topic, which makes it hard to appreciate its novelty. Using a modified version of softmax to select channel size and kernel size seems novel which is not achieved by DARTS. Considering that most papers in neural architecture search report performance on  ImageNet, it is important that the authors also report the performance there.<|endoftext|>The proposed method is applied to hyper parameter tunning for two types of networks, namely CNN and FNN on two datasets MNIST and SVHN. Overall, this paper is a contribution in the right direction and has potential to cut down the hyperparameter optimization time greatly. The paper provides an elegant way to determine the optimum hyperparameters via a differentiable scheme. Having said that, I have several concerns about the experiments. Overall, the idea is interesting but the experiments are not convincing.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; This is not at all clear, but it sounds like it s saying that this is a hand crafted rule by the designed. This seems very unscalable and putting a heavy thumb on the scale. The method is interesting, but it seems very contrived for a very small and specific type of problem.<|endoftext|>Also, it is not suprising that Vanilla RL (as said by) doesn t have generalization ability. The proposed method is evaluated on the text games, with results showing its ability of interpretation, generalization, and robustness. The authors aim at achieving the interpretability, generalization ability, and robustness of their methods. However, due to the concerns I raised in the weakness part, I prefer to weak reject (marginally below the acceptance threshold) the paper.<|endoftext|>Specifically, the authors propose to split the decision making process in two: 1) an action pruner, and 2) an action selector. Also, I believe this paper could be strengthened by evaluating their approach on other types of tasks (i.e., not related to cooking) from the TextWorld suite. For those reasons, I can t recommend it for acceptance. What happens if the edges related to the goal are dropped (e.g, the  needs  edges from Figure 3)? ### Typos  p.3: The sentence "Therefore various ... Just stating $K << |A|$ seems enough to me.<|endoftext|>is defined (function that groups actions)   the definition of this function seems to be quite manual and domain dependent, which restricts its use of the proposal. The Action Selector defines the primitive action based on the GNN Abstract Supporting Edge set (ASE), also defined from the training data provided by the teacher.
Accept (Spotlight); rating score: 8; rating score: 6; rating score: 6; rating score: 5; Local memory transformers have a memory module that can only access the recent past. This is a huge advantage in the language modeling space. I don’t think it would, and that’s totally fine, but I think readers would benefit from a discussion of this property of the model. “Cross Batch Memory for Embedding Learning” This paper introduces a new method for extending the transformer model with a simple datastore. I believe this paper should be accepted.<|endoftext|>This paper presents a memory based Transformer where a memory is used to leverage the external inputs for scenarios that have long inputs like documents or codes. The key contribution is that it describes how external contexts are integrated into the representations of the current inputs. By introducing such external contexts with the memory mechanism, perplexity of LM can be improved on every dataset studied in the paper. This work is highly related to those scenarios, such as document level QA/retrieval and code completion, where the models should consider and leverage long inputs.<|endoftext|>The paper plugs a kNN memory module into a Transformer for long distance knowledge. There is no ablation study in terms of modeling. I agree that language modeling is a meaningful task, but it is more like a testbed for modeling techniques that improve generalization. Even the code would be released, the standalone paper should be informative enough. The model is only evaluated on the language modeling task.<|endoftext|>This work proposes to deal with long range dependencies in sequences using a transformer language model augmented with memory. The memory is non differentiable, but it is still possible to be updated, and used an "input" to a layer in the network. The paper studies effects of memory vs no memory,  different sizes of memory, and fine tuning by training on small memory first followed by an increase in memory size. The idea seems interesting and somewhat novel and evaluated empirically. Relevant work to this community. I ve updated my score  recommendation is to accept the paper given that the additional experiments and standard deviations are finalized and included in the paper.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; The paper has two separate contributions. Therefore, the stability and convergence of MPA is somewhat concerning. I do not think the provided numeric tests are sufficient to spot the defects, because the defected regions can be small. I.II.Backward pass: viewing matrix square root gradient as a continuous Lyapunov equation and using the matrix sign function to solveThere is some novelty in this section but I think the novelty is not much.<|endoftext|>This paper aims to solve an important problem: how to efficiently compute matrix square root in both forward and backward propagations. Experimental results on the decorrelated batch normalization and second order vision transformer show the proposed methods are faster than existing methods, including SVD based ones and NS based ones. This paper starts the point of matrix multiplication times in computation of matrix square root, and proposes some strategies to reduce number of matrix multiplications both forward and backward propagations, aiming to improve the efficiency. 3.The experiments are conducted on two real world applications, including decorrelated batch normalization and second order vision transformer. The paper is well written and easy to read. The authors would better make some discussions about this issue. The authors would better make some discussions about it. The authors would better provide more detailed computation and analysis.<|endoftext|>The work proposes a fast method to solve the matrix square root. Weaknesses and questions:  The novelty of the proposed method is limited and the theory behind it is well known.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; The paper proposed PANN, which uses tricks such as unsigned arithmetic in CNN and implement multiplications via additions to achieve a multiplier free and power aware neural network. However, the reviewer believes there are fundamental flaws that need to be thoroughly addressed before this paper is ready for ICLR. * The proposed method in Section 5 to replace multipliers with repeated additions by accumulation is equal or inferior to multiplication in terms of bit flips. ### Weaknesses* Although bit flips can be considered an important source of power consumption, the proposed approach of counting bit flips and minimizing such counts per image inference is fundamentally flawed.<|endoftext|>The paper proposes to remove signed arithmetics and removing multiplication. The power model is limited to dynamic power, and just the MAC power. Weakness:* The authors rely on a 2000 publication to support the argument that dynamic power dominates. * The authors didn t consider the memory energy, which usually dominates the total energy, as opposed to MAC energy. The idea is of intellectual merit, but the evaluation ignores too much important aspect of real energy/power modeling. The results are far from convincing in that regard.<|endoftext|>The paper argues that power consumption is a major obstacle in deploying DNNs to end devices and that current quantization approaches do not take power consumption directly into account and therefore are not optimal in reducing it. Using an approximate power model based on the average number of bit flips, they make two observation that are frequently overlooked by existing quantization approaches: 1) A significant portion of the power consumption of the MAC operation is due to the usage of signed integers and using unsigned integers instead can significantly reduce the power consumption. On the other side, in the paper we trade off multiplications with additions. This allows PANN to efficiently reduces the power consumption. It is important to validate that the power model is valid under the expected distributions as most claims of the paper are based on observations from this power model. * While the proposed approach shows good accuracy improvement for a power budget, it also has significant limitations.<|endoftext|>This paper proposed a method for reducing the power consumption of deep neural networks called a power aware neural network (PANN). The idea that quantized the model using the power budget is very important to real world applications. 2.If each layer has a different quantization bit width, the hardware implementation cost could be increased when compared to the fixed bit width based hardware. How can it be handled when using the layernorm? There is a typo on page 2, "Our approach can work in combination"  > "Our approach can work in combination"The idea of finding the optimal quantized model in terms of the power budget is an important issue for real world applications. Although some part of the paper is ambiguous to me, I think this paper could be a bridge to reduce the gap between the model researchers and hardware engineers.
Reject; rating score: 1; rating score: 1; rating score: 3; rating score: 5; This paper suffers from poor quality of writing, an unclear contribution to the already vast literature on the detection of adversarial examples and limited engagement with well known recommendations from the research community on the evaluation of defenses/detection methods for adversarial examples. This paper proposes using the difference between the highest and second highest softmax outputs from a model to detect adversarial examples.<|endoftext|>Based on the manifold assumption in the adversarial machine learning setting, this paper proposes a detection strategy using a set of different image transformations and the classifier’s output probability divergence. Some experiments were performed to show the detection performance against vanilla implementations of well known attack methods. First, I cannot understand the reason why the Section is needed at all. Manifold based defense against adversarial example is not a new approach and reviewers know well about the manifold assumption in the adversarial machine learning setting. Each paragraph is saying some topic, however, the connections between the paragraphs are not very clear, making Section 3 more confusing. Second, the average quality of paper writing is extremely poor to understand the paper properly.<|endoftext|>This paper proposes a manifold distance based detection based against adversarial samples. Otherwise, it is hardly to find the contributions of the paper;3. It is difficult to find the relation between the proposed detection method with the preprocessing methods;5. There are some typos and grammar errors, such as “Table 1 and Table 2 show”, etc.<|endoftext|>This paper studies the manifold distance detection between adversarial examples and clean data. The proposed method is based on the famous manifold hypothesis, that is the real world high dimensional data generally lie in the lower dimensional manifold. 1: I think the discussion in Section 3 is trivial and well known in the deep learning community, hence it is unnecessary to spend many contexts to discuss this. Overall, I do not think the paper provides a significant contribution to the community.
Reject; rating score: 3; rating score: 6; rating score: 6; rating score: 8; The paper tries to argue that the Bellman error cannot be trusted as a good metric or objective for off policy evaluation. This paper may bring potential attention to the community to rethink of how to view Bellman error into the design of the algorithm. Counterexample Construction: The paper construct several simple MDP as counterexample that the Bellman error is not with aligned with the value error. Reader can save their time to try BRM on TD3 (which is extensively studied in this paper and is failed)## Weak PointsNovelty: The direct use of BRM fails to solve the off policy evaluation has been widely known in the community. However, I believe there are a lot of off policy evaluation papers, especially those with high confidence interval estimation papers are trying to take the finite sample into account. I would vote for rejection at this moment and encourage the authors to further improve on the idea and submit to another venue.<|endoftext|>This paper studies the Bellman equation commonly used in reinforcement learning (RL) algorithms. The typical motivation behind using the Bellman equation to design RL objectives is that uniformly driving the Bellman error to 0 implies that the true value function has been learned, i.e., the value error is 0. Furthermore, when evaluated using off policy data, Bellman error may not even be well correlated with value error for different runs of the same algorithm. Strengths The paper is of relatively high quality. I am happy to engage in discussion with the authors and other reviewers in order to reach a more confident final recommendation.<|endoftext|>This paper studies the relation between the Bellman equation and the accuracy of the value functions. This paper shows that Bellman error is not a good measure for comparing value functions and thus is an ineffective objective function for off policy evaluation. Despite the weakness mentioned above, I still think this paper is good. The message is important and the results are novel and interesting.<|endoftext|>The Bellman error measures how “closely” the estimated value function $Q$ satisfies the Bellman equation, while the value error measures how close the estimated value function $Q$ is to the true value function $Q^\pi$. The paper provides both theoretical analysis, and empirical experiments using two algorithms related to Bellman error (BRM and FQE). + The paper includes theoretical analyses that are simple and to the point, and makes good use of illustrative examples. The authors provided a good review of (i) in Sec 5, which is more on the learning aspect. I would be strongly in favor of acceptance if the authors can aptly discuss how this contribution relates to the past work provided above.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; The paper proposes a new task named as Inference Time Personalized Federated Learning (IT PFL). Specifically, given a new client with unlabeled data joining in the federated learning system after the training process has finished, IT PFL aims to deploy personalized FL model to it. The strategy is based on recent works on hypernetwork. While the authors claimed that IT PFL is a novel problem, the solution that the authors gave is similar to [1], which utilizes hypernetwork in personalized federated learning task. At least to me, the difference between [1] and this paper is marginal, which makes me question the novelty of the paper. This paper does give some results on generalization bound and differential privacy, but I believe they are just results of simple application of existing theorems. First, I think it will be interesting to compare hypernetworks and meta learning. Personalized federated learning using hypernetworks. Given the major novelty concern, I do not think the paper is appropriate for acceptance in its current shape.<|endoftext|>This paper proposes to use hypernetwork to personalize the federated model by encoding new user data and using this embedding as a parametrization argument. The results demonstrate significant average improvement over the new clients. Furthermore, the authors evaluate the possible use of DP to encode the user embedding vectors. Weaknesses: it s clear how the proposed paper is different from [1] but I would dedicate more space for the comparison, specifically Section 5.3 of [1] suggests using the nearest client (as you describe in Table 1). And also the personalization of LM models is an important problem. Is it a single input of the user? Lastly, other personalization works that use local adaptation [2], such as finetuning global model on the local data, should be considered as baselines and I wonder what is their performance w.r.t.the proposed method. "Ditto: Fair and robust federated learning through personalization."<|endoftext|>This paper studies the problem of in personalized federated learning, the current paradigm does not allow for new clients to join during the inference time. Federated learning does not generalize well to new data distribution that is very different from training. A hyper network maps the descriptors on to a space. Cons:1.Some of the experiment results are not explained. Are these models realistic for real world use cases? 4.I was wondering how much difference it will make in comparison with the new client participate in training. same for many other quotesOverall, the problem being studied in this paper is interesting and well motivated. The novelty perspective of this paper can be highlighted better.<|endoftext|>The authors propose a new personalized federated learning paradigm composed of a hypernetwork module and an encoder module in server and an extra novel client with unlabeled data. The encoder module is enhanced thanks to the unlabeled data. This paper is clearly written. However, the reviewer still has several concerns: 1. The proposed generalization bound is established for the novel client. How does this generalization bound reflect the efficacy of the proposed personalized federated learning framework? 2.The compared baselines are not sufficient to demonstrate the efficacy of the proposed algorithm. It seems that is the novel client merely has limited data.
Reject; rating score: 6; rating score: 6; rating score: 6; rating score: 6; The authors formulate a way to transform finite sum optimization problems in a proxy strongly convex problem, and prove that it converges to a global minimum in a number of gradient steps that scales inverse quadratically with the tolerance. Strengths: the paper is fairly clear, and the proposed transformation is certainly a novel way to solve a common class of problems in the machine learning community. I respect that there s value in providing a new perspective on an old problem (with correspondingly different asymptotic bounds on its performance), but unless this proposed method is demonstrably useful at solving a concrete problem, I m not sure if it s significant enough for ICLR. At the same time, I don t want to discourage this kind of work, so I would honestly be satisfied seeing this proposed algorithm applied to _any_ problem, but as it stands, I don t think I can accept. The authors propose an interesting and novel algorithm for machine learning, but it s unclear how significant it will be.<|endoftext|>The submitted paper considers a composite formulation of machine learning loss functions, where both the inner and outer functions are required to be smooth. The submitted work presents some interesting result, such as using the composite structure for machine learning loss functions, and approximations approach to determine the descent direction. However, I have some concerns,1. This is a quite well studied formulation in the optimization community. So the proposed approach is not novel, and the authors should discuss these existing work. $$ Given the good properties of the problem. 3.It would be great if the authors can provide numerical experiments, to demonstrate the performance of the proposed work. The paper provides certain interesting result, however the novelty of the paper and advantage of the proposed algorithm need further justification.<|endoftext|>I suggest \eta H vThe assumptions in Theorem 2 don t have to be repeated. One can simply state "under the same assumption as in Theorem 1 plus additional assumptions ..." It computes the direction by means of solving a quadratic MSE problem. They provide a convergent analysis of the algorithm (there is a version where the quadratic problem is solved explicitly through a closed form expression and a version where gradient descent is applied to solve the problem approximately). The paper provides a new gradient based algorithm. I am unconvinced about the novelty of the analyses. The authors should provide more convincing arguments that the entire material is unrelated to derivate free second order algorithms and for example, BFGS. It works well for quadratic problems. They are over selling the work by using  novel  too often (in particular in connection with the formulation). The role of \beta in Theorem 1 makes me worried about the significance of the result. In short, this is nice work but not impressive. Addressing some of the weakness would definitely improve the paper.<|endoftext|>This paper presents a new optimization method for finding global minimaof nonconvex finite sum problems. In particular, the summands arefunctions of the form $\phi_{i}\circ h$ where $\phi_{i}$ is convexand Lipschitz smooth, while $h$ is nonconvex. Each iteration of themethod consists of solving an auxiliary regularized least squares(RLS) problem, followed by a gradient step. *Weaknesses*: Unfortunately, I will have to recommend rejectingthis paper due to a critical mathematical error, namely, **theconstant $V$ used in the proof of Theorems 1 2 does not have theproper dependence on the tolerance $\varepsilon$**. To elaborate,notice that the stepsize $\eta^{(t)}$ in Theorems 1 2 is $\Theta(\sqrt{\varepsilon})$and that Assumption 3 (essentially) states that $V$ is a scalar where$$\|v^{(t)}\|\leq V,\quad\|\eta^{(t)}Q^{(t)}v^{(t)} b^{(t)}\| {\cal O}(\varepsilon),\quad t\geq1,$$for matrices $Q^{(t)}$ and vectors $\{v^{(t)}\}$and $\{b^{(t)}\}$. Even under the generous assumption that$\{Q^{(t)}\}$ and $\{b^{(t)}\}$ are bounded, theassumption that $\eta^{(t)} \Theta(\sqrt{\varepsilon})$ must implythat, at the very least, $V \Omega(1/\sqrt{\varepsilon})$. [p. 5, 6, 14] The use of  "apparently" in several importantstatements in this paper implies that these statements have some degreeof ambiguity to them. [p. 14] ... this result can be **generalized** for larger ...EDIT: The authors have uploaded a revised version of their manuscript that has addressed my main concerns. The only issues that prevent me from assigning a higher score to the paper are: (i) a lack of numerical experiments and (ii) the use of non standard assumptions (in particular Assumption 3) that are difficult to verify in practice. However, if this errorcan be resolved in a revision, then I would be open to increasingmy final review score.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; rating score: 6; As such, we could view this as a voice conversion setup, but with the additional task of rendering translated voice in a different language. Aside from the results, the contributions seem to be incremental. The paper is generally very well written and results (also, samples) look good. Do they convert to text and then to speech? Not enough novelty.<|endoftext|>Translatotron2, a speech to speech neural based translation system is proposed. a)  Output from the auxiliary target phoneme decoder is used as an input to the spectrogram synthesizer b) Conformer Encoder with SpecAugment is used, which is known to improve the speech to text performancec) Duration based  Spectrogram synthesizer is used. The paper is well written.<|endoftext|>This paper advances the previously proposed direct speech to speech translation (S2ST) model Translatotron. The major contributions of this work can be summarized as follows. Compared to the previous Translatotron model,  1) Translatotron 2  uses  the output from the auxiliary target phoneme decoder as an input to the spectrogram synthesizer;  2) the spectrogram synthesizer is duration based, while still keeping the benefits of the attention mechanism.<|endoftext|>The paper proposed a speech to speech translation (S2ST) model that is and improvement to a previous work. The model is trained end to end from speech to speech, along with an auxiliary speech to phoneme task. Experiments firmly supports multiple claimed improvement to the previous model.<|endoftext|>This paper describes an updated version of Translatron which purports to be a speech to speech translation system trained end to end. The paper describes an incremental improvement over a previous paper on the same topic. Read speech is quite different from spontaneous conversational speech.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; This paper introduces a lightweight and fast neural network architecture processing 3D point cloud. It s interesting to see that these small modifications (especially the second) make dramatic changes in the results.<|endoftext|>The architecture of PointMLP is simple for implementation. Furthermore, as we see in the experiments, PointMLP does not outperform the previous SOTA model with significant margin (on ModelNet40 and ShapeNetPart). Therefore, stronger reasons are needed to support the claim that "detailed local geometrical information probably is not the key to point cloud analysis".<|endoftext|>This paper proposed an alternative point cloud feature extractor, named PointMLP. Empirically for point cloud processing I also found a lot of  sophisticated  networks can be easily outbeat by much simple networks with tuned #layers and #channels. I did not see any speed comparison.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 3; The authors propose a framework for learning to synthesize a proxy models for usually non learned components of an RL loop: namely transition dynamics and reward as defined in the interacting environment. 1.The writing is extremely clear & detailed, and makes the paper a pleasure to read. So, overall, I am extremely happy to argue for acceptance as it is. It very much feels like the authors could have a feasibly split the paper into two, one on ES and a follow up on RN, providing more space to experimentally analyse ES and RN separately and make for a cleaner and easier review process. In short, I wonder if we d see similar improvements for tasks / environments where task difficulty is extremely affected by the emerging complexity of the environment, or whether we can actually show that this is a property of SEs / RNs when trained in this manner (which would be quite outstanding!). My personal opinion is that it should indeed be often the case, but that generally it is not true (e.g.imagine an MDP with a small transition matrix but an extremely stochastic reward function). This is a good paper. The research problem and proposed methods are interesting and well placed in the literature, and the experimental section is exhaustive.<|endoftext|>This paper introduces Synthetic Environments and Synthetic Rewards as a bi level optimization problem. It would be great to know what the source of this instability is. Conceptual:  There are repeated claims in the paper that the synthetic environment "focuses the learning" on the relevant states or learns "representations" of the target environment ("an informed representation of the target environment bymodifying the state distributions to bias agents towards relevant states"). The only condition is that _optimal policies_ for the two environments match. For example, you can imagine a state transition dynamic that is entirely random but provides a large reward for the optimal action in each of the given states. Other comments:  In Figure 2, it seems like the legend is switched? A very interesting paper with early results in a potentially interesting direction. Update: Based on the discussion phase and the rebuttal I have updated the score to a 6. I believe this paper has enough merit to be published at ICLR, if there is space.<|endoftext|>This paper proposes to learn neural environment models they term Synthetic Environments, such that planning in the model with model free RL results in good performance in the real environment. The authors study learning full models as well as reward only models they call Reward Networks, investigating the feasibility of learning these models as well as their robustness to different inner loop algorithms and hyperparameters. Clipping at a somewhat arbitrary solution threshold seems like it would result in suboptimal RNs (this appears to have occurred in the half cheetah experiment in the appendix). The exposition of the method and description of experiments are quite clear and complete. It might be interesting to algorithmically address this use case by changing the fitness function to account for the relative performance of different algorithms. It may also be possible to motivate the work from a more traditional model based RL point of view, which might be more closely aligned to the problem statement given in the paper. NB I don’t fault the authors for not citing this, I believe it is only available as preprint. They argue that models learned only to further optimisation of a policy (as in this work s problem statement) may perform better than traditional models when capacity is limited. I understand the authors may have limited computational resources, but it is difficult to know how highly to weight an empirical feasibility study carried out on such small scale tasks. In the RN case optimising for the speed of training makes sense, but I still find the chosen objective strange.<|endoftext|>This paper aims to learn proxy environments (synthetic environments or SEs) and reward functions (reward networks or RNs), parameterized as neural networks, such that these proxy models provide beneficial transitions to make it more sample efficient to learn a policy for a fixed target environment (referred to as the real environment). The proposed method formulates this problem as a bi level optimization, where the inner loop consists of standard RL under the proxy model, and the outer loop consists of NES with the aim of optimizing either the performance on the true, target environment (SEs) or a the number of training steps needed to reach a certain return threshold on the real environment. Given the additional compute required by these methods, it seems that various forms of policy distillation or even training from scratch seem more efficient. While this work proposes an interesting generalization of prior work on meta learning MDP dynamics, the case made for these methods is weak, and the experimental results—especially for SEs—are uncompelling. Given these points, I recommend this paper in its current form for rejection. This formulation is more general than prior works. While there seem to be some marginal gains in sample efficiency, it would seem these gains are largely made irrelevant due to the additional training needed to train the environment in the first place. RNs are effectively an ablation of SEs, where the state is not learned. In contrast the SE objective optimizes for performance. This would directly separate out the performance gains due to training on the  SE. Zheng et al, 2020 in particular is quite similar to this work in terms of analyzing the effect of learning an RN and also includes more comprehensive experimental results in a more complex setting requiring recurrent RNs.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; rating score: 6; The authors present a method to distill StyleGAN2, an unconditional Generative adversarial network. The authors run all their experiment on StyleGAN2. Pros:A  The paper tackles an interesting problemB  The authors are referring and comparing to key previous work (CAGAN). Cons:C  My biggest concern with the current draft is the experiment section and claims. While the authors have done a good job in comparing to previous work (CAGAN), the comparison is not convincing nor rigourous. It is not supported by any evidence. D  Finally, the technical depth of the paper and contributions are also limited. “0.31”  > 0.41 in Section 4.2Given C and D comments, I can not accept the paper “as is”. I hope that the authors will be able to address my comments.<|endoftext|>This paper tackles the problem of image unconditional generative model compression by leveraging the knowledge distillation framework. Experiments are conducted using a StyleGAN2 backbone on the FFHQ dataset. **Post rebuttal update:** The reviewer thanks the authors for the feedback which addressed some of their concerns. There might be value in the analysis and lessons learnt wrt compressing StyleGANv2, but considering the limited results and their unknown generalizability to other models and more challenging datasets, the reviewer considers that the paper is not ready for acceptance. It would be worth exploring other state of the art generative models. * "Specifically, we select two students with one inheriting teacher’s style module and the other being randomly initialized for the style module.<|endoftext|>The paper identifies a key factor that affects the knowledge distillation on unconditional GANs, in particular the StyleGAN2 model: the output discrepancy between teacherNet and studentNet. It proposes that the initialization of the StyleGAN2’ style module causes the above problem to be more serious. **Questions during rebuttal period:**1. The results in Q4 should be added in the paper. However, as other reviewers said, the contribution is limited. In section 3.3, the counter evidence provided by the authors is simple and not convincing. And, for two different sizes of style modules, the conclusion is the same as above. The proposed compression method is only verified in one model, StyleGANv2. 4.Previous work [3] proposed the initialization of studentNet for compressing GAN.<|endoftext|>Typical knowledge distillation loss/pipeline failed for unconditional GAN distillation, due to the output discrepancy between teacher and student model even if same inputs are fed. The framework mainly contains two parts: 1). Solving the output discrepancy issue for distilling unconditional GAN is an interesting topic. Otherwise, if a same architecture can achieve the same quality by training from scratch, there is no need to do distilling any more. It also needs to be added to Table 4. 3.The paper only shows the results on face images (the aligned FFHQ face images). The semantic information of these types of images are not that well disentangled as face images. Overall, the paper tries to address an interesting problem of tackling the output discrepancy issues when distilling unconditional GAN models. I agree with other authors about the limitations of the proposed method, especially about how to extend to other unconditional GANs and other datasets.<|endoftext|>Overall the proposed method is well motivated and interesting. Strengths:  This could be a good study paper for the unconditional GAN. The authors conducted several investigations and show the effect of the style module. Both the proposed student initialization and the latent direction based distillation are new and interesting for the unconditional GAN distillation. Although the topic is a bit narrow to unconditional GAN, the experiments are thorough. After Rebuttal  Thank you for the feedback. It is good to see more experiments such as the new results on LSUN church dataset. However, I agree with other reviewers that the contribution is a bit narrow (or limited).
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; The paper proposes a novel method for the Attentive Neural Processes paradigm by adding stochasticity to the weights of the cross attention module between the context and target representations. This seems to indicate that the parameter lambda is determined by an MLP whose inputs are x_i, X_c and Y_c. However, the current manuscript is far from being ready for its publication.<|endoftext|>This paper proposes a neural process enhanced with stochastic attention to focus more on the context dataset. For the experiments in Movielens 100K dataset, isn t it possible to integrate the graph structure in the NP approach for a fair and higher performant model? Moreover, the paper offers an interpretation of the method from an information theory perspective, proving that the NP with stochastic attention can be seen as a regularization of the latent space such that it pays more attention to the context dataset.<|endoftext|>I think the main idea is novel and worth publishing. ### Strengths  The application of Bayesian attention to neural processes seems novel.<|endoftext|>In addition, although I see new results in the rebuttal, my concern is that the accepted version of the paper might not be a substantially improved version of the paper. Overall, I would like to hold my original score for this paper and still think this paper is not ready for the ICLR conference. Post rebuttal After carefully reading the authors  response, I am still holding the question regarding the novelty of this paper. This paper incorporates the stochastic attention in NPs to capture the context information.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; In the standard Variational Autoencoder framework the statistics of the decoder output are assumed to be pixel independent Gaussian which can lead to problems when sampling from the model when covariances are missing. To overcome these limitations, the authors propose to use the network architecture proposed by Monteiro at al. 2020 in the decoder of a variational autoencoder. 2020, the output distribution is modeled as low rank multivariate normal. Qualitative results on the CelebA dataset demonstrate that samples from the proposed modified variational autoencoder capture covariances between different parts of the image. Overall the only contrubution of this paper is to transfer the method of Monteiro et al.into the variational autoencder framework. Please provide experiments on synthetic data, for example sampled from a multivariate distribution with known mean and variance to demonstrate that the framework actually models the uncertainty of the data distribution.<|endoftext|>The authors aim at improving the canonical VAE model by replacing the standard iid Gaussian likelihood with a multivariate Gaussian with (low rank + diagonal) covariance. The authors then evaluate the expressiveness of the representations learned in the observation space by visually evaluating interpolations in the observation space, and images obtained by rescaling the principal components of the observational covariance matrix. The problem the authors tackle in this paper and the proposed solution are interesting. If not, the authors should highlight methodological differences. However, more work is needed to demonstrate improvement upon state of the art methods (specially, Dorta et al, 2020, and potentially Hou et al, 2017).<|endoftext|>The paper proposes to include some structure in the observation model of the standard VAE, typically implemented with a normal distribution with diagonal covariance matrix. The authors follow the work in [Monteiro 2020] which include low rank structure structure in the covariance matrix for the observation model in another generative framework. In section 4.1 these values are reported for (2) datasets. Adding additional analysis/explanation for these variables would benefit the paper and help persuade any reader about the robustness of  the results presented. The paper provides an extension of the work in [Monteiro et al.2020] to the case of VAEs.<|endoftext|>This paper proposes to use a low rank plus diagonal covariance matrix, rather than the usual diagonal ones, in the decoder of Gaussian VAEs. This paper is well written, easy to follow, and well motivated. Firstly, as the authors correctly point out, they are not the first to use a structured covariance matrix in the decoder of a Gaussian VAE [1], although this previous work uses sparse covariance matrices rather than low rank plus diagonal ones.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; The new method is motivated by poor conditional score estimates observed in low dimensional examples for the standard, Bayes theorem based, conditional data generation procedure of score based models. I believe all of the experiments presented in Section 5 are useful, clear, and well laid out. Although empirical evidence suggests the classifier is at fault, the paper only uses references for this motivation. There is no explanation as to how one might stratify these classifier score inaccuracies. Besides one line of research that could be included in a future paper, weaknesses are minor and/or aesthetic.<|endoftext|>The paper points up a previously underappreciated problem in training classifiers in the context of conditional generations of diffusion based generative models. The authors propose a novel objective for training classifiers to tackle the problems. If the same model was used, the unconditional generators should have been the same for the baselines and the proposed method. If it wasn t, the description of the experiments needed to be updated. In the paper, the authors refer to this phenomenon as *a score mismatch issue. will help interpret the results. Recently, the interest in diffusion based generative models has increased rapidly. I hope that the aforementioned weak points are well addressed.<|endoftext|>They introduce a new loss, which is essentially a score matching loss for the conditional distribution p(y| x). The authors compare different models, trained under different conditions, and conlude that the problem is in the loss. The models are also different. Maybe this is the main reason? 5) Why do the authors train the classifier p(y | x, \theta) and the score model (x, \phi) independently, not end to end? Why don t the authors do the same trick with \tau as with \sigma (that is, they don t train several different models with different values of \tau)? In general, the paper is well written, it has a clear and logical structure, it is easy to read. In the introduction, only clearly highlighted contributions are missing. I thank the authors for their responses.<|endoftext|>This paper presents a conditional diffusion(score matching) model to tackle the score mismatch issue in the conditional generation scenario. This paper tests multiple alternatives in creating a conditional distribution through diffusion models, and this paper introduces a classifier assisted structure for the conditioning. It will be great if authors can explain how to main the equality, after the introduction of $Z$. Given that it is good to see that the authors performed the posterior score matching by separating the score matching by conditions. Niche paper to introduce a conditional generative model by the diffusion process
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; This paper researched on the recently proposed long tailed regression problem with self supervised learning method. Authors report the results on three datasets to demonstrate the superiority of our proposed methods, including IMDN WIKI DIR, AgeDB DIR and NYUD2 DIR. Pros:[1] The imbalanced regression problem studied by this paper is of great significance not only to the academic research but also to the real world applications. Cons:[1] The performance improvements to current state of the art method is quite marginal. My suggestion is that the author can try to provide statistical analysis on the overall datasets, and quantitatively compare the benefits of noise generation method against vanilla random noise method. [5] There are several typos in the paper, for example, "Update ri by by taking" in the algorithm workflow "Algorithm 1: Self Supervised Imbalanced Regression (SSIR)" should be "Update ri by taking".<|endoftext|>This paper proposes a novel algorithm SSIR to address the imbalanced problem in regression tasks, which seamlessly combines self supervised learning and imbalanced regression by giving the formal definition of similarity in the regression task. Strengths: This paper seamlessly combines self supervised learning and imbalanced regression by giving the formal definition of similarity in the regression task, and proves that self supervised learning really relieve the long tailed regression problem. The Experiments are substantial. Weaknesses: 1)  In the penultimate paragraph of Section 4: “On the other hand, considering that a single back propagation optimization may not be an accurate approximation, we therefore performed a two step optimization of r.” Why a two step optimization can be helpful for obtaining an accurate approximation? How would the proposed method perform if the test data is unbalanced? 5)  The sentence: “the left image suggest a greater change with augmented data.” “suggest” should be “suggests”.<|endoftext|>This paper proposes to leverage self supervision to improve imbalanced regression. I recommend a borderline acceptance. Although the self supervision signal has been discussed under the context of classification, leveraging it in the imbalance regression task is novel. Solving the problem is a necessary step to introduce more self supervised methods into the imbalanced regression domain. In another word, the illustration does not fully convince me why the use of noise should be different on classification and regression and why the "migration" is not feasible. Moreover, the empirical evidence for the argument is missing in the paper. In the motivation section, self supervision is used as a pre train and steady improvement is observed in Figure 2. 4.There are some minor issues with the parameter study. The paper concluded that the proposed method achieves the best performance with $\epsilon   1e 3, \lambda 1.0$.<|endoftext|>This paper studies long tailed regression and explores self supervised learning to alleviate class imbalance. This may inspire future research when using CSL for imbalanced regression. Extensive results verify the effectiveness of the proposed method. Based on the current method, it seems that self supervised regression helps to train a more noise invariant model. I am okay if the authors want to leave this as future work, but I still expect the authors to discuss it. In the abstract, the noise is not defined and may confuse readers. The writing can be further improved. ArXiv, 2021.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; This paper addresses point cloud registration from a meta learning perspective to quickly adapt with limited training data. I am not sure about the meaning of optimal in the following sentence: "The meta learner is responsible for providing the optimal initialization of a 3D registration learner". I believe that concise explanations and providing examples could help in communicating ideas in this paper.<|endoftext|>Lastly, I am a little confused whether this is a scene flow or a point cloud registration paper  it would appear to be the former though the title and intro seem to indicate the former. Scene flow differs from 3d point cloud registration in that each point is assigned a different transformation in SE(3), typically in a 1 to 1 manner as a 3D analogue of optical flow.<|endoftext|>This paper tried to leverage the meta learning strategy to improve the generalization ability. The objective of the work: This paper proposes a learning method to improve the generalization ability for the point cloud registration tasks. Weak points: (1). The paper claimed to improve the generalization ability, while the experiments are extremely lacked to support this claim. I think this work is ongoing work and need to do more investigation.<|endoftext|>This paper presents a new architecture for point cloud registration. Weakness:Though this work is about 3D point cloud registration, many important works about 3D point cloud registration are missing. The proposed method does not show its superior performance on generalization in the experiments. The results are promising but are still not solid to support the claim (well addressing the generalization problem).
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; rating score: 6; The authors present theoretical and empirical findings on how ensembles can improve certifiable robustness of classifiers. The authors show that increased robustness requires diverse gradients among the ensemble members (as well as large confidence margins). ### Strengths * The empirical results are strong, i.e., the proposed model outperforms baselines consistently. An interesting study would have been to see how increasing $N$ changes the certified robustness. * An interesting aspect of the theoretical contribution is that Theorem 1 can also prove when a sample is *not* robust. The paper is well written and easy to follow, and the theoretical and empirical contributions are interesting and appear meaningful.<|endoftext|>This paper proposes using an ensemble model to improve certified robustness. Based on the developed theory, it proposes Diversity Regularized Training (DRT), a lightweight regularization based ensemble training approach which composes of two simple yet effective and general regularizers to promote the diversified gradients and large confidence margins respectively. Convincing experimental results. **Weaknesses of the paper**  The findings of the paper are not really new. Furthermore, randomized smoothing has been proposed before. The current analysis is done with a fixed $r$.<|endoftext|>The paper analyzed the certified robustness for ensemble ML models. It is proven that diversified gradient and large confidence margin are sufficient and necessary conditions for certifiably robust ensemble models under the model smoothness assumption. The proposed Diversity Regularized Training (DRT) method performs better than current provable methods. In definition 1, To be consistent with randomized smoothing, should ${\left\|\delta\right\|}_2 \leqslant r$ be ${\left\|\delta\right\|}_2 < r$? Is it driven by the property of Theorem II or is it due to empirical observation? I will consider raising my score based on the other reviewers  comments and the authors  responses.<|endoftext|>The paper proposes to train diverse classifiers to improve certified robustness of ensemble classifiers. Although I feel a slight logical gap in Theorem 1 → EBS, the paper presents a novel and theoretical justification to motivate the new training method, and it is further supported with a strong empirical results. I agree that exploring on the ensemble of smoothed classifiers is an important yet under explored topic. Does EBS guarantee that all the base classifiers before ensemble are $\beta$ smooth?<|endoftext|>This paper proposes a new analysis for randomized smoothing based certified robustness of ensemble models. The analysis shows that the robustness radius depends on the l2 norm of the weighted sum of the gradients of the confidence margin of base models. The experiment results are sufficient to support the claim of this paper. One question I have is how the weights of ensembles could affect the results.
Reject; rating score: 3; rating score: 6; rating score: 6; rating score: 8; This paper attempts to understand how a RNN goes about solving the Variable Copy Delay Memory task in fine detail. Their model has been trained perfectly on this task and so they are able to focus on how it goes about changing its cell values in the case. The authors present different metrics to track resetting and memorization behavior by the GRU s neurons. In terms of strengths, I think the authors definitely show that for this specific task and architecture, the GRU is learning to use working memory in a more or less explainable way. In terms of weaknesses, I feel that this paper lacks novelty. Their work is not technically novel because they are using a GRU and using a super well studied task. I don t think I took away any sort of new information from their analysis either. If the authors wanted to improve the novelty of their work, they can maybe focus on state of the art architectures and more complex tasks. This work lacks novelty, is hard to follow, and shouldn t be included in ICLR.<|endoftext|>The authors study the mechanisms that a GRU network uses to store and retrieve memory in a variable delay copy task. Studying the mechanism by which an RNN model stores and retrieves memory is an important question that can give insights into how these networks work. This paper s approach of trying to reverse engineer the mechanism of a trained GRU to understand this is interesting. Moreover the finding that slow manifolds are used for the purpose of storing memory of a sequence is interesting. But the paper does not consider or test alternate hypotheses other than slow manifolds. For example, no attempt is made to empirically show that the GRU is not using pseudo line attractors. It is also not clear why the authors specifically consider the GRU architecture, especially since it seems like using an LSTM would make it easier to reason about when information enters and exits the cell. The writing of the paper is also very confused and hard to parse in many places. The sequence of arguments made for slow manifolds is sometimes hard to follow. It might help to explain formally what a slow manifold is. The perturbation based experiment is hard to understand, as is the PCA analysis. Overall, while the goals of the paper are well founded, and their analysis seems potentially interesting, a combination of lack of clarity and some gaps in their overall argument weakens the paper significantly.<|endoftext|>The paper analyzes GRU s underlying mechanisms that store and retrieve information in the delay copy task of a sequence of K symbols. Finally, the paper provides a synthetic solution to the delay copy task for the case K 2. Overall, the authors  hypotheses are well validated through empirical evidence and informative visualization of the synthetic task. On the other hand, the writing is hard to follow. Questions and comments:  The term "slow manifold" is mentioned frequently without any explanation. Please describe it in more detail in the main manuscript. Or maybe add a simplified version of Algorithm 1. How does the GRU perform at this step? It is important to understand the underlying mechanism of models like GRU. However, I am unsure if the presented result is useful enough for designing a better mechanism for RNN/GRUs.<|endoftext|>The paper studies GRU cell dynamics on the variable length memory problem in depth. Using the discovered mechanisms the paper proposes a synthetic solution to the variable length memory problem. The paper is a good fit for ICLR. #### Weaknesses:  Related Work: Previous work by Karpathy et al.studies the behaviour of various recurrent cell gates across recurrent architectures for natural language processing tasks. (2016).The description of the experiments was often hard to read. Speaking of  neurons  instead of reset gate neurons,  update gate neurons or hidden state neurons, for example, made it harder at first. Readers can infer the exact setting from context but should not have to do that. Figure 2: I personally found it tricky to understand what is going on here. I could follow the description of the argmax,    but what is the difference between the "Projection to hyperplanes from hidden state" and the white points? If the white points are the entries of $\mathbf{y}_0, \dots , \mathbf{y}_n $$ aren t these points the projection? Why is the projection along the value axis? I think neuron is always shorthand for hidden neuron? If that is true, the authors may want to consider explicitly stating this somewhere? I firmly believe that we need more work trying to uncover the inner workings of neural networks. I am therefore recommending acceptance in the proceedings, assuming that the differences to the analysis from Karpathy et al.(citation in main review) will be discussed briefly.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 3; Specifically, the authors propose an approach that allows fast queries at inference directly on the compressed model without decompressing the model back. The idea of enabling inference directly on the compressed model without decompression is quite interesting. I especially recommend making Section 3 more clear since it is the most important section of the paper and some concepts need to be better explained by providing some background information. I believe the related work section misses many references, especially in the model encoding part, which is the most related direction to this paper. arXiv preprint arXiv:1510.00149 (2015). However, there are more recent methods such as [1], [2], [3], and [4] that are directly comparable.<|endoftext|>The paper claims that DNN inference suffers from a storage bottleneck and proposes a lossless compression scheme using wavelet trees to reduce the size of DNN models. The comparison with existing work is weak. Writing needs to be improved. There have been significant missing discussions related to model compression. Furthermore, the evaluation was also done on AlexNet and VGG, both of which have been heavily studied. To be more convincing, the paper should compare at least with some lossy compression techniques such as integer quantization and one more advanced model architecture such as ResNet and Transformers. Typos:1.Missing references, e.g.Huffman Coding (?), in the first paragraph of Section 6.<|endoftext|>It is presented an approach to store a compressed deep neural network in a form that allows fast inference. It is relatively easy to follow it, but the clarity of the paper needs improvement. The results are promising compared to the Huffman encoding. For instance, distillation approaches, as well as factorization and pruning methods. This experiment would add a significant value to the work. Related work on lossless compression: It is important to extensively discuss the related work on lossless compression and memory efficient deployment. Prior work comparisons: There is only one comparison with the Huffman encoding. It would be helpful to include more approaches (see above). It would be helpful to include a ResNet like architecture since it is the standard to be used nowadays. Improvements:  The claims in the introduction could be supported by references.<|endoftext|>The paper explores the compression of weights in a DNN. The compression technique presented permits the uncompressed values to be easily recovered without first decompressing the data. The paper discusses an interesting and important problem and proposes what appears to be a novel approach. There are two main issues for me at the moment 1) it is difficult to understand the precise scheme that has been proposed, and 2) the evaluation seems very limited and somewhat unclear. e.g.if applied to a small model that fitted in the cache? How would the results differ on embedded cores for instance? I feel that the description of the work needs to be improved.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; rating score: 5; Using different samplers, these are two distinct (!!) (ii) The paper shows that the the squared Euclidean norm of the data score (for an unspecified data distribution) empirically diverges as $t \to 0$. The paper then presents a lemma that states that no neural network can learn an unbounded function as long as it is conditioned on time $t$ or variance $\sigma$. models and values of NLL and FID/IS should not be put in the same row as is done in Table 3. This is misleading. In my opinion, the weaknesses of the paper clearly outperform its strengths. The authors then claim that importance sampling is simple for the RVESDE. So to the best of my knowledge, this parameterization takes care of the "problem". References:[1] Song et al.Denoising Diffusion Implicit Models. Is Figure 13 (a) based on the toy example from Figure 6? ICLR 2021. I think the authors should have done more experiments, even if only on toy datasets.<|endoftext|>This paper studies the behaviour of training SDE based score matching generative models for time t close to 0. To the best of my knowledge, this exact problem   issues related to t close to 0   has not been studied before in the exact form as is presented in the paper. The paper studies an interesting problem and attempts to present corresponding solutions. The presentation could be improved. Is this a toy example where you know the ground truth data generation model and its density? A locally Lipschitz function could be unbounded on a non compact subset. With this, the authors argue that the L2 distance between a generated sample at small t (denoted x_t  here to distinguish between x_t) and data x_0 can t be close. How different quantities in these plots are computed are also left undescribed.<|endoftext|>However, learning a good diffusion model can be challenging since the data score can go to infinity when $t$ goes to zero. To address this issue, this paper proposes an alternative parameterization for unbounded data scores and provides a practical trick (ST trick) to handle the variation of the scales of score values. 3.In figure 6, it seems that the proposed method is still not able to capture the data score well on the toy dataset (although it is slightly better than NCSN++). The proposed method also has better sample qualities than current SDE models. This is a very interesting work that studies the effect of how the sampling steps at different $t$ affect the performance of score based diffusion models. However, the experimental setup is not very clear and I also have some concerns about the problem formulation (see Main Review).<|endoftext|>Below are the strengths of the paper  The three issues identified in this paper are, to the best of my knowledge, quite relevant in training diffusion models. Overall the direction is quite relevant, but the presentation of the work is subpar (I’ve pointed at specific sections/paragraphs for future improvement in detailed comments), which makes me doubt it would benefit the ICLR audience *in its current form*. Perhaps the authors can provide some more explanation in the rebuttal. The optimal score function $s^*$ is a function of $x_t, t$, why is that equal to a random variable $z$? The performance gain from the new parameterization is in terms of what? Also, won’t $\eta$ go to infinity as $t$ goes to 0? I notice there’s a short note after the score matching loss equality “up to a constant”, but it’s not clear if this applies to eq (1), which can be confusing. Also, why is the assumption on an unbounded vector field a reasonable assumption here? Then what is the advantage of ST compared to importance sampling proposed in Song et al 2021 and Huang et al 2021? Or is it simply a real data point but perturbed with different levels of noise? This is regarding the paragraph *Optimal Precision* in the ablation.<|endoftext|>I think this is an interesting paper. I would encourage the authors to address the writing to make it more accessible. The fact that the score function is unbounded and causes problems for optimization is not particularly surprising. For example, one might suppose that the epsilon parameterization introduced in the DDPM paper by Ho et al.would not suffer from the same problem. I think this paper would be much stronger if they can claim that other existing parameterizations like that used by Ho et al all suffer from the problem addressed in this paper. This issue should be discussed at length. Rather, I think the issue is with the impact of the problems raised on the generative model. The issue of different parameterizations of the loss in diffusion models is also relevant to the experimental work.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 5; This paper empirically studies the lottery ticket hypothesis in deep reinforcement learning. I therefore recommend that it be accepted to ICLR. I also appreciated that the effect of convolutional layers was discussed in the main paper, and that late rewinding was also evaluated in the appendix. The paper dives quite quickly into its findings, but leaves some gaps in the motivation of its investigation.<|endoftext|>**Summary:**The paper investigates the lottery ticket effect in detail in reinforcement learning tasks. I have some small suggestions for improvements, but am currently in favor of accepting the paper. **Improvements**1) As stated earlier, my main criticism is that the “distributional shift” hypothesis is never verified and no alternative explanations are provided as to why pruning rates in RL seem to be generally lower. This observation could be an interesting approach to producing sparse input representations in RL. regularizing effect   the corresponding weights remain at low magnitudes and can be safely pruned. The paper presents results for a large number of tasks (with continuous and discrete observation  and action spaces), and loads of relevant controls, baselines and ablations.<|endoftext|>This paper investigates the Lottery Ticket hypothesis in the context of deep RL for identification of sparse task representation in low dimensional control tasks. The contributions listed in the introduction does not align well with the arrangement of the sections and respective discussions. However the current version has some major concerns that the authors need to address.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; The paper studies the communication needed in order for a group of distributed players to collectively solve a variational inequality problem. Main drawbacks and technical issues with the paper I think this is an interesting modification of the prior known work but I do not believe that this is a significant research contribution and is an incremental adaptation of the extragradient/extrastep method. Or at the bare minimum have some kind of restriction assumptions on the algorithm and then prove lower bounds that could explain why this adaptation of the extragradient method is near optimal or non trivial. Section 4 is really abrupt and it would help to ease the reader into the distributed solution. In particular they provide two algorithms MASHA1, MASHA2 to do this that solve both the deterministic and stochastic cases. Both the algorithms basically extend the extragradient/extrastep method of solving variational inequalities to the distributed setting.<|endoftext|>The authors have considered general distributed variational inequalities problems and proposed two methods using unbiased and contractive compressors.They have evaluated their methods on saddle point problems including large scale adversarial training of transformers. The authors consider a general notion of unbiased compression schemes in (1). However, this parameter can be very large in overparameterized settings, which motivate this paper as stated in the abstract. I think the discussion after Corollary 1 is not accurate. It is more likely that the standard uncompressed extragradient method outperforms Algorithm 1 in terms of communication complexity. It is also important to show that Assumptions 1 and 2 hold for such problems. It is unclear how all nodes have access to $F(w^{k+1})$ at the beginning of the iteration $k+1$ while only some nodes with $b_k 1$ compute $F_m(w^{k+1})$? A similar notion based on the expected number of required bits has been proposed in the literature e.g., QSGD, NUQSGD, signSGD ... The paper is not well written. The paper has some interesting aspects for example large scale adversarial training of transformers.<|endoftext|>The paper develops a decentralized algorithm for solving variational inequalities with a certain structure motivated by machine learning applications. The authors study a well motivated problem. A critical question that remains unanswered   from the viewpoint of theory   is whether there exist compressors that indeed lead to improved communication complexity over the uncompressed methods. However, for the randK compressor where $\beta   (M+1)/Mq$   assuming that the same compressor everywhere    this relation is not satisfied. The experimental section appears somewhat weak, I would have liked to see more.. In effect, the experiments do not appear to be a sufficiently thorough validation of the theory. The presentation can be improved by presenting and explaining the idea behind the basic centralized solution in the appendix, or before presenting the solution with the decentralized, compressed version. From the communication complextity reported below Corollary 1, it is surprising that the compression factor of the devices influences the total number of bits much more than the compression factor at the  server, essentially the communication complexity appears to be 2^{(1+q1)*log(q2)} bits, where q1,q2 respectively represent the variances of the quantizers of the devices and the servers respectively. However, some concerns exist in terms of both theoretical justification need to be addressed.<|endoftext|>Both unbiased (i.e., MASHA1) and contractive (i.e., MASHA2) compression methods have been proposed. Theoretical analysis is provided to show that the proposed method can converge. However, I also have the following concerns. 1.For practical problems such as GAN or adversarial training, the objective of min max optimization is nonconvex nonconcave, which results in a non monotonic operator in variational inequalities. The authors are encouraged to extend the analysis to the more practical nonconvex nonconcave regime. It’s easier to compare different methods by checking their corresponding average scores. Since the extra step method is specific for solving saddle point problems, it’s counter intuitive when it diverges. 5.It looks like there is no conclusion section at the end of this paper.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 8; rating score: 8; This paper can be viewed as a proposal of a new image anomaly detection system that combines existing methods. The proposed "adaptive" dictionary based approach is not new. The use of a deep encoder for image analysis is, of course, not new. The term "energy based" does not mean much because any probability distribution can be thought of as energy based. So the question is whether the combination is innovative or not. Although I appreciate the authors  efforts to develop an industry application ready image processing system, I do not think the technical novelty meets the standard that ICLR papers are supposed to have. Eq.(3) y is not defined in the generative model in Eq.(1).p.3 The problem setting should be clearly defined. What are the input and output? The anomaly score must be clearly defined in the main text as part of the problem setting. Good industrial use case paper  Limited technical novelty   less innovative combination of known methods.<|endoftext|>As a form of anomaly detection, the claims made are for a principled modeling approach and an adaptive algorithm quick to learn from just a few samples. Based on "energy based models" for modeling data densities, the paper claims improvements to avoid the need for retraining for new tasks, such as, instead of synthesizing negative samples rom noise, using a more targeted method of “learning frominpainting” operation to learn anomalies. The paper never avails itself of any discussion of  what it means to be "anomalous" in terms of probability. Since this is not what the paper proposes, the authors may want to clarify this at the outset. Honestly I find it hard to tell if the evaluation presented in the paper makes the case for improvement over other deep learning methods, or more conventional image analysis methods. How does this method stack up with previous methods when given comparably large numbers of training samples? This literature, using deep learning methods that have no connection to the existing literature that is more than a decade old, calls into question whether the proposed methods are competitive with previous methods that do not originate by applying deep learning techniques. My apologies for a shallow review due to a lack of familiarity with this branch of the field, however there are some basic questions that need to be raised about the lack of connection of the field with the vast work that precedes the current trend in deep learning to really understand the significance of the work.<|endoftext|>The author proposes a fast adaptive anomaly detection method using an adaptive sparse coding layer in the few shot setting. Strength:+ The experimental result on two benchmark datasets outperforms most of the baselines. + An ablation study is presented to demonstrate the robustness of the proposed method. However, there are also some places where this paper needs further clarifications:  It is not clearly shown in the experiment as to how the technique in Section 3.1 improves the robustness of the network on different types of objects. It seems from Table 2 that the proposed method without leveraging any temporal information can achieve even better performance in the CUHK Avenue and the SH Tech dataset. More discussions are encouraged on the benefit of incorporating such temporal information versus image frames randomly sampled from the target scenes. To summarize, this paper proposed a fast adaptive anomaly detection framework for anomaly detection in images. However, the novelty is limited on the adaptive space coding layer with receptive field, while the rest of the network structure seems to be a big melting pot incorporating the Energy Bassed Model and episodic training in the context of meta learning based few shot learning.<|endoftext|>The paper proposes a framework for anomaly detection and localization that allows fast adaptation to new tasks. Specifically, the authors propose an energy based model (EBM) with an adaptive sparse coding layer directly trained with normal features of a target task. A meta learning process is followed to extract common knowledge across tasks enabling few shots adaptation. Weaknesses:   Experimental analysis is limited to image and video datasets;  The mIoU metric is never defined in the paper;  Fairly limited scope of competitor methods (which could depend on the limited suitability of other methods in the adopted setting). The paper proposes an interesting anomaly detection approach and an experimental evaluation that also involves fully supervised models. Results are competitive, and some of them appear close to the upper bound performances provided by supervised alternatives. I think the main merit of the proposed method is the ability to achieve satisfactory anomaly detection performance without large availability of normal samples while adapting to new tasks.<|endoftext|>The paper presents an anomaly detection algorithm that uses an EBM (Energy Based Model) to distinguish between  normal  and  anomaly . This model generates pseudo anomaly instances on the fly for each  normal  instance and then learns to assign low energy to normal instances and high energy to pseudo anomaly instances. The model is further designed to be able to adapt quickly (few  normal  labeled examples) to new tasks. Good ablation studiesCons:1. Assumes all training data is  normal 2. Lacks experiments with contaminated training dataMain Comments1. 3.Section 3.1: "The sparsity regularization to ... formulated as the mean squared error (MSE) between the original"   This is a nice explanation of the design choice.
Reject; rating score: 1; rating score: 1; rating score: 3; rating score: 3; The presentation of the paper needs to be improved. For example, it is not clear Fig 2 and 3 are generated.<|endoftext|>I found the paper difficult to understand, classification results not convincing, and interpretability claims greatly overstated. Unfortunately I believe this is a clear reject. Disclaimer: I have heard about attention and capsules but do not know how they work.<|endoftext|>The additional interpretation as claimed by the authors is not sufficiently elaborated. * One of the main claims of the paper is that capsules encode similar information for cells of the same celltype.<|endoftext|>An important claim of the paper is that the suggested model offers interpretability. However, I do not believe that an LSTM is the right tool for that, given that they do not constitute a sequence. is not clear to me.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 8; This paper proposes a cross layer attention module for the image restoration tasks. In order to prevent expensive computational costs, the authors propose adaptive cross layer attention modules. The proposed method is applied to various image restoration tasks to show the generality power of the method. It would be better to mention differences from LAM more specifically. It is interpreted that the effect of the proposed method is not large. Why is the order of HAN and RDN in x4 different from x2 and x3? This paper is technically sound and quite new. Even technically, I think that difference with the existing methods is not significant.<|endoftext|>The paper proposes a Cross Layer Attention(CLA) module in order to capture the correlations. Besides, an Adaptive Cross Layer Attention (ACLA) is proposed to reduce the computational cost of the Cross Layer Attention(CLA) module. Lastly,a neural architecture search method is used to find the insert positions of ACLA modules to further improve performance. There are various layer attention module was proposed in the past three years. More importantly, the findings of the paper has been general knowledge in computer vision community. The authors may need to focus on more important and valuable problems in image restoration field. The authors may also need to compare the proposed to LAM.<|endoftext|>Furthermore, an adaptive cross layer attention (ACLA) is also formulated to dynamically select keys from different CNN layers by using a NAS method. After that, the authors embedded the presented CLA or ACLA modules into EDSR to formulate a deep model for image restoration. It tends to degrade the effectiveness of the NAS of ACLA.<|endoftext|>To reduce computational complexity deformable convolution is used to reduce the number of sampled keys. However, the approach taken in this paper appears to be novel through its use of deformable convolution, gating, and neural architecture search to reduce the computational complexity. The paper is clearly described. It would be helpful if the paper commented on the availability of source code in order for others to replicate the results. Strengths: •	While cross layer attention isn’t a new idea, the approach taken in this paper has novelty and provides a fairly generic approach that could be used with different backbones. •	The primary disadvantage is the small incremental benefit for some of the steps of the method. But the idea of the image restoration is to restore the degradation.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; rating score: 10; Different from attributing the trajectory prediction power with the success of modeling agent agent interactions, this paper presents a new perspective that the past trajectory of the target is the only feature used for predicting its future for state of the art trajectory prediction methods on standard benchmark datasets. Post Rebuttal： Thanks for the authors  feedback. Some casual inference based trajectory prediction methods [1] also found the interactions are biased in the current trajectory prediction systems and datasets. The experimental results on SportVU are also an important discovery to show the trajectory prediction models have the capability to learn the interactions. This setting may be more appropriate for self driving. Overall, I think it is a good paper with some minor problems that presents a new perspective that the past trajectory of the target is the only feature used for predicting its future for state of the art trajectory prediction methods on standard benchmark datasets.<|endoftext|>They apply this measure to the state of the art trajectory prediction methods and found that many standard benchmarks don’t require reasonings of agent interactions. The authors need to compare with prior work and show why this proposed measure is novel rather than applying it to trajectory prediction.<|endoftext|>This paper addresses feature attribution for trajectory prediction to gain insights about the actual cues contemporary methods use to make predictions. The authors designed a variant of Shapley values that is applicable to a large set of trajectory prediction models. The paper is generally well written and easy to follow. Also, is there any limitation/caveat when using Shapley value as the metric for this purpose? In particular, it would be better to conduct some experiments using the model proposed in [1], which makes the statement even more convincing.<|endoftext|>The authors propose a feature attribution method for trajectory prediction based on Shapley values. In Section 3.3, the authors note that random agents should not contribute to the future of the agent of interest, but again, they still play a role in the movement of the agent of interest due to collision avoidance. Although I believe that this manuscript is well written, tackles an interesting problem, and has an evaluation that is consistent with its claims, I do not think there is enough evidence to back the strong claim that SOTA HTP models are unable to learn almost any interactions for well studied datasets that have cases of high interaction, but are able to learn interactions for a dataset with more interaction.<|endoftext|>This work discusses the features that attribute to the prediction results in trajectory prediction models. The work defines a measurement metric called the social interaction score, in which it measures how well the interaction between agents attributes to the prediction of the other agent trajectory. It seems from the Shapley values that the random agents doesn t contribute much to the results because of the lack of interaction as shown in the work and this is a good finding. The only thing I’m skeptical about is the definition of the social interaction score. Training from scratch the trajectory prediction models with no interactions (aka single agents only, no neighbours) should produce similar (or better) results to the ones with the neighbours. The new experiments on Trajectron++ proved the theoretical analysis.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; First, they show how to solve a sequence of convex constrained optimization problems in order to solve the larger non convex problem. I found this paper to be interesting and well motivated (finding the best classifier subject to fairness constraints is a real problem), but the results were not as compelling as I hoped. What happens if they are non convex? There were also issues with writing quality, which is under the bar for ICLR. Are there settings in which even the best loss for a certain group still means they re disadvantaged?<|endoftext|>They specifically consider equalized loss fairness constraint. There is a well known and closely related work in the literature, which is (arXiv:1802.08626). (arXiv:1802.08626) studies empirical risk minimization under fairness constraints as in this paper. Both theorems seem to be using the same assumption. Only one real world dataset is considered, while there are several real world datasets publicly available and there is no comparison with the state of the art. The contribution of the paper is not good enough given the existing literature on fair machine learning.<|endoftext|>    This paper studies the problem of fair supervised learning under the Equalized Loss (EL) fairness notion, which is formulated as a non convex constrained optimization problem. Empirically, the algorithms perform well. There is no discussion on the technical contributions. This idea seems also appeared in a prior paper [L. Elisa Celis, Lingxiao Huang, Vijay Keswani, Nisheeth K. Vishnoi: Classification with Fairness Constraints: A Meta Algorithm with Provable Guarantees. The authors should provide a detailed comparison with this work. The paper considers an important fairness notion EL and provides algorithms with provable guarantees.<|endoftext|>The authors consider minimization of convex losses constrained by either bounded loss on each group, or bounded difference of losses over two groups. What happens in the case of classification losses? This might be misleading a bit   as by disadvantaged the authors simply mean the non optimized group, but since this is a "fairness" paper, it can also mean demographics groups which are at a disadvantage (minorities). Nonconvex optimization for regression with fairness constraints. Implicit Rate Constrained Optimization of Non decomposable Objectives http://proceedings.mlr.press/v139/kumar21b/kumar21b.pdf 3. Optimization with Non Differentiable Constraints with Applications to Fairness, Recall, Churn, and Other Goals https://jmlr.csail.mit.edu/papers/volume20/18 616/18 616.pdf 4.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; This paper presents a new method to do inference in simulation based systems. This paper has the potential for broad impact. The number of applications for simulation based inference in science and engineering are far broader than recognized in the ML community. The authors show in the paper that the GAN equilibrium distribution is the one that performs exact Bayesian inference through the simulator. Weaknesses:  The VAE comparison doesn t seem as useful to the reader as more details on how to do actually get training to work. Questions:  The paper assumes we do not have access to the likelihood or gradients of the simulator output.<|endoftext|>Technically, the novelties are limited, as both the original GAN and the reverse KL GAN are well known in GAN research fields. I think the novelties of this work are highly related to the importance of the considered simulation based inference (SBI). The key is that for a black box simulator, one can query a lot of data pairs $(x,\theta)$. Empirically, the performance of the proposed GATSBI is only comparable to the baselines.<|endoftext|>This algorithm paves a promising avenue for applying SBI in high dimensional parameter spaces or with implicit priors   a setup currently out of reach for simulation based inference algorithms. Experiments deliver a convincing proof of concept on a 784 dimensional parameter space, which none of the current SBI algorithms can solve. Strengths:  This work contributes a new algorithm for SBI for the setup of high dimensional parameter spaces or implicit priors. The paper is generally very well documented. The Noisy Camera model experiment is the most convincing, as it shows its real potential in comparison to other methods. The experimental validation is limited to two common benchmark problems and two high dimensional problems. Experiments are carried out properly, although the experimental validation could have been more thorough. The technical novelty is limited. I am willing to recommend it for acceptance.<|endoftext|>Authors connect the method to prior work in adversarial (variational) inference, and also consider a *sequential* variant of the method, where the posterior approximation is refined iteratively for a particular datapoint. The problem is introduced well, the method is clearly motivated and presented in great detail. The highlighted connections to existing methods are nice. I commend the authors for discussing the connection to LFVI, but this does highlight the fact that the differences between the proposed method and LFVI are arguably minor. Apart from the additional term in the loss (the role of which is unclear), the difference in the used divergence is the main one. It s not surprising that we don t win much on the low dimensional problems, but I wish authors explored more high dimensional inverse problems, perhaps also more realistic ones. References:  Adler & Öctem, Deep Bayesian Inversion, 2018. https://arxiv.org/abs/1811.05910Overall, while the paper is written excellently, my doubts about novelty and the extent of evaluation are significant enough to put the paper slightly below the bar for acceptance.
Reject; rating score: 1; rating score: 3; rating score: 3; rating score: 3; This paper presents a very preliminary first step into designing a foreground/background CNN that is robust to adversarial attacks. * Emergent properties of foveated perceptual systems. While I find the idea interesting, and I like the direction the authors are going   this work is still quite preliminary and needs more work. It seems by figure 1 that the fusion network is not end to end differentiable.<|endoftext|>This paper studies the problem of adversarial training and tries to leverage a fusion based method against adversarial attacks. However, I have concerns about the technical quality, the novelty of the manuscript, and the violation in page limitation. Therefore, I believe that the main focus of this paper is very relevant to the ICLR community.<|endoftext|>The paper tackles the adversarial example problem. Specifically, the approach combines two pre train models, that are excepted to focus on foreground and background, respectively. There is no proof that the one trained on ImageNet can be used as a foreground objects detector. 2.The novelty is limited. The method can be seen as an ensemble of different models.<|endoftext|>Overall, the main concerns of this work are the novelty and unclear experimental setups. The idea for enhancing the adversarial robustness via foreground and background is not novel and has been studied in [A]. This work may be a rush to the deadline.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; "Distributional deep reinforcement learning with a mixture of gaussians." This discretization output a fixed number of actions for each input state. The novelty and significance of this paper can be shown clearer if convincing analysis and comparison to previous Gaussian mixture models works are provided. This paper explores the connection between the continuous actions space and discrete actions space, proposes an automatic discretization. The most relevant discussion in the paper is practical comparison between proposed approach and SAC, a continuous action space based baseline. (4) As the supplementary (A) section shows, the proposed architecture looks very similar to the Gaussian mixture models. "Reinforcement learning with a Gaussian mixture model."<|endoftext|>The paper proposes learning a state dependent discretization of a continuous action space using demonstrations. The motivation is to enable the use of discrete action deep RL methods instead of relying on policies with continuous action space parametrization. The problem setting is interesting, timely, and could have a good amount of impact should the technique be adopted. I am not fully convinced that the implementation is competitive to the original. PMLR.The paper presents an interesting framework for learning the discretization of action spaces. E.g.[1] The authors also claim that this is the most related approach. The paper makes a number of somewhat handwavy claims why discrete action spaces are superior but while they may sound intuitive they are not substantiated by theory or experiments. Continuous discrete reinforcement learning for hybrid control in robotics.<|endoftext|>This paper proposes a novel method: Action Quantization from Demonstrations (AQuaDem) to learn a discretization of continuous action spaces by leveraging the priors of demonstrations. As such, one can apply any discrete action deep RL algorithm to continuous problems. In three scenarios: RL with demonstration, RL with play data and imitation learning, the effectiveness is validated. * This paper proposed an action space discretization method, it claims to not suffer from the curse of dimensionality and not needing any specific assumptions about the task. However, I might not agree that discrete control problems are harder than continuous control problems. I think the argument in the introduction that since continuous control problem is much harder than discrete ones, so we should turn a continuous problem into a discrete problem is debatable. The key novelty is Equation 1. where Figure 2 explains the key idea very well. However, I think the novelty of this paper is limited.<|endoftext|>This paper proposes a method for discrete action space learning from originally continuous action spaces using demonstrations. In such general cases, learning a discrete subspace of the continuous action space is quite useful. Regarding the latter, this method can be seen as learning a hard bias on exploration (removing a subset of actions from the original downstream action space) and only using those used more frequently in demonstrations. *" **:** This has been discussed in Ref. 3) Choices of methods to combine the proposed approach with and the environments to test in are reasonable. Is there a reason for this? 4) The breadth of discussing the related work is reasonable. **Bang bang controller:** BB controller implies a very specific type of discretization scheme, where only the extrema of the action space are used in the control problem. However, in this paper, you use this to refer to the generic uniform discretization scheme.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; The paper makes two contributions: 1) presenting conditions under which the process can be identified (up to a permutation of the latent variables and their componentwise invertible transformations), and 2) developing a training framework that enforces the assumed conditions. The first contribution is theoretical and the second methodological. How do you know these assumptions are satisfied in your real world examples, and what happens if some of the assumptions are violated? Is this a typo? An interesting paper with relevant results on the identifiability of latent causal processes. I hope the authors could clarify my above concerns in their rebuttal.<|endoftext|>The authors proposed a method to recover time delayed latent causal variables and identify their relations from measured temporal data called Latent tEmporally cAusal Processes estimation (LEAP). The results on public datasets including KiTTiMask, Mass Spring System, and CMU MoCap demonstrate that temporally causal latent processes were reliably identiﬁed from observed variables under different dependency structures. Please list both the strengths and weaknesses of the paper. When discussing weaknesses, please provide concrete, actionable feedback on the paper. Although technical contributions are derived from mixing the previous techniques, the problem setting seems to be completely new (but I am not an expert in this field), and experiments were sufficiently performed.<|endoftext|>This papers proposes new conditions for identification of latent temporal causal processes, for both non parametric and parametric processes. I really encourage the authors to invest effort in making this architure more understandable, and especially highlight how each of the assumptions from theorems 1 and 2 are embodied. The baselines should also be run on the real world datasets for comparison with the proposed approach when the data generating process does not exactly matches the assumptions made in this work.<|endoftext|>This paper propose two new conditions for nonlinear identifiability of temporal processes. The proof and the code seem good. I have some concerns:1. The proof of the theorems are not clear. The "time delayed" is not clear in this structure. How does the "C" part connect to the proposed two conditions? The mask is for sparse latent process, while it is also stated in the paper the method does not rely on sparsity in latent processes.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; These theoretical results enable them to draw some conclusions on when we can expect Offline RL to have an edge on Behavioural cloning. To name a few:  When the dataset provided is not optimal, offline RL can scale more favourable with the horizon especially for environments with horizon independent returns or with a low volume of critical states,i.e.states for which there is no significant advantage to select one action over another. The assumptions are clearly stated and transparent to the reader. The discussion around the theoretical results is well conducted and the insights are informative to the reader. The empirical evaluation on a tabular gridworld domain is appreciated as it removes the influence of function approximators on the results. I also appreciate the fact that the authors didn t shy away from highlighting potential discrepancies in their results due to the various tricks introduced in the literature to tune offline RL algorithms. As a reader, it is hard to digest and understand whether the results fully align with the theory. Overall, I consider this paper to be a great contribution to the research community. The question investigated is important for the research in offline RL and practitioners.<|endoftext|>Offline RL approaches are of quite great interest because of potentially easier real world applications. This formalization leads to a better characterization for the conditions for offline RL methods to outperform behavior cloning (and the importance of planning horizon and critical states). **Weaknesses**It s unclear how much bearing the theory has on the actual empirical experiments as the authors themselves emphasize the importance of tuning offline RL. Tuning techniques seem domain dependent and exactly what features of the domain lead to particular tuning choices remains unexplored. Overall, some way to separate the [state] representation learning problem from the best action problem is likely important to really figure out the problem which the theory in the paper largely ignores. The bigger issue is whether the "noisy expert" data is a good proxy for real world offline RL applications. While the paper cites a few examples of suboptimal data collection in Sec.4.3 for most real world applications, the data is actually seldom noisy in the same way. Moreover, there are often many safety constraints that simply would not allow running certain kind of behavior policies. Authors might find [1] an interesting read.<|endoftext|>The paper studies the sufficient conditions that VI LCB would have a better worst case sub optimality gap than the gap of a BC algorithm, under expert data and noisy data. The paper studies an interesting problem: when an offline RL algorithm has advantages over a simple BC algorithm. I think the problem is in the interests of many RL practitioners. However, this makes the results less clear and less general. 4.The paper studies a particular type of offline RL algorithm based on VI with LCB style penalty. 5.I am not sure what is the novelty and significance of the theoretical results. It seems the theoretical results are mostly borrowed from existing literature with some modifications. I want to make sure I am not missing something important there. Can you explain more about the novelty or significance for the theoretical results? 2.VI LCB with Bernstein style penalty is also used in [1], which is not cited in this paper.<|endoftext|>The paper considers a setting where we are given access to a dataset of expert or noisy expert data collected from some MDP and need to decide whether to use either behavior cloning (BC) or offline RL. Specifically, I am skeptical of the usefulness of the central question, worried that the theory is assuming away the interesting part of the problem, worried about novelty, and had a few more minor concerns about the experimental setup. It conducts a theoretical analysis in a tabular setting showing that offline RL will recover a better policy than BC when the data is sufficiently suboptimal and has sufficient coverage. Importantly the results depend on errors due to the function approximation. The comparison of the results across algorithms may be novel, but the results themselves do not seem to be a major contribution. Could the authors comment on how this algorithm was implemented?
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; This paper provides a viewpoint of spectrum domain to show self attention amounts to a low pass filter and will cause feature maps to only preserve direct current components as depth increases. Experiments are conducted on ImageNet. Strengths: 	1. The paper provides theoretical analysis of ViT from spectral domain, which explains the empirically findings of patch feature collapse of ViTs. 2.Reweight techniques of low pass and high pass components are proposed, which is sound according to the theory part. 1.The gain is limited, 0.2 ~ 0.3 improvements for 24 layer models. post rebuttal With the new provided experiments results, I raised my rating towards accept. The therotical part is nice, while the experiments does not show adequate evidence that the proposed method would actually help scale depths of ViTs (e.g.beyond 24 layers) as title indicates.<|endoftext|>However, some problems remain:1.The article proves that the self attention is a low pass filter, thus neglecting the high frequency information. The conclusion comes to the stage that pure attention leads to rank collapse in the network. The current clarification in the paper are not very convincing to verify why the ViTs cannot go deeper, even though with FFN, skip connections and mutli head. 2.The visualization of the modified attention is about the learnable parameters. However, the spectrum of the modified attention seems more convincing to show the change on high frequency components. And it is more persuasive to show the situation when the depth is 24. The overall presentation is good. The main concerns are in above. Can the authors provide more experiments on big models to further verify the effectiveness of the proposed method?<|endoftext|>The challenge is that the mechanism of self attention is applying a low pass filter. The experimental results demonstrate the improvement on accuracy for DeiT and CaiT when adopting the proposed method. Overall, this paper proposes a novel idea of thinking self attention weight matrix as a low pass filter. However, the experiments are not sufficient, the improvement is relatively minor, and it doesn t show whether the proposed approach can help deeper transformers converging. It would be very interesting to see it is helpful for deep models. Interesting and novel idea and good mathematical analysis. May need more experiments to demonstrate the claims in the paper.
Reject; rating score: 3; rating score: 5; rating score: 6; This paper proposes DeepTLF, a new framework for prediction tasks using tabular data. In essence, the authors rely on GBDT s capability to handle heterogeneous tabular data with potentially many missing values to derive a binary representation of the given sample, and feed that representation to a downstream neural network. Weaknesses:  The technical novelty is quite limited, since all the authors do is train GBDT first on the entire training set, extracts the internal node values as a feature representation, use those feature representations as new training samples to train a downstream neural network. In Table 3, it is not clear whether the training time measure a single minibatch update, or a single epoch, or the entire training process. Is there any reason D7 was not included in Table 2? The proposed method, DeepTLF, is a very straightforward combination of GDBT and DNN, which seems to outperform all modern tabular neural nets, which I find very interesting. However, the technical novelty is quite limited, and considerable amount of experiment details are missing, therefore making it difficult to accurately evaluate the paper.<|endoftext|>This paper proposes an encoding method (to encode structural information in GBDT) for DNN with tabular data, to handle the heterogeneous features. The method is simple, and the results look very promising. strength:1. the paper is clearly written, and easy to follow. weakness & question:1. some details in experiments are not clear. 2.Can you also provide the AUC metric for the binary classification tasks? 4.Do you think it is possible to get rid of GBDT (as a feature extractor), and independently design a feature extractor, and perform a similar performance? I will boost my score if the experimental details are provided and reasonable.<|endoftext|>For the problem of learning (supervised classification and regression) on tabular data, the authors propose to used the decision functions of tree based ensemble methods as input features for a deep neural network (DNN). Tabular data is typically heterogenous, that is the data come from different modalities (continuous, categorical, missing values). At least I did not came across any literature where this approach had been followed. The paper is well written and easy to follow. The major claims made in the paper are supported  *DeepTLF can preserve most of the information that is contained in the original data. The architecture of DNN is not further described. At least this questions should be addressed in the discussion. The experiments show that this method works, but are not 100% conclusive if it is superior in all aspects, in particular comparing to DNN (see my concerns above).
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 3; This paper explores the continual learning performance when combining different PLMs and common continual learning methods with 3 challenging NLP classification tasks. To benchmark these combinations the methods are evaluated in task incremental and class incremental learning settings over various NLP end tasks, which covers common learning settings in continual learning and NLP. I thus feel confident to recommend this paper for acceptance. Overall the paper shows that forms of replay outperform other methods like regularization. The authors also provide new research questions that arise from the analysis and point to interesting unsolved research challenges. The chosen data sets are not well behaved, i.e.experience imbalances, which makes the results more realistic (less artificial). Overall, this study provides a necessary step towards exploring future continual learning methodology and explores many important factors on eight pages. Tab 1 could underline the best non joint performance — makes it easier to glance  sec 4. Here intermediate layers are shown to forget as well, so this is a nice new finding, that can be contrasted.<|endoftext|>The authors perform a comprehensive study of how pretrained language models work in the continual learning setting. In addition to a thorough everything by everything evaluation, the authors hone in on the details of how the different models and CL approaches are reflected in the transformer layers. This study s most successful contribution in my opinion is an exploration of the qualitative differences among PLMs in the continual learning setting. The main reason for this is that the "veins of CL methods" has a different number over the course of the paper, which makes it hard to identify when a given list of N things is a list of the "veins of CL methods". Despite some weak points in the analysis of the quantitative results and inconsistent organization/language around the CL approaches, the thoroughness of the study, in particular the analysis at a layer by layer level, is likely of interest to the broader community.<|endoftext|>This paper conducts an empirical study on the catastrophic forgetting of pretrained language models. On two continual learning settings (class incremental and task incremental), the paper evaluates multiple pre trained models on different data sets, to see how severe the catastrophic forgetting issue is for these pre trained models. Then the paper also tests the effectiveness of multiple continual learning methods on such pre trained models and draws some conclusions. Although the authors have conducted quite a lot of experiments, the phenomena shown in experiment results is hardly surprising to me. It is also not surprising that rehearsal based methods perform the best for pre trained models. However, compared with other pre trained models, I don’t see that BERT is significantly better than others given the figures and tables. I feel from the figures and tables, BERT and other models look similar. The authors didn’t give a comprehensive explanation on how they read such information or a concrete quantitative comparison to support this claim. A thorough empirical analysis with unsurprising conclusions<|endoftext|>As the title suggests, the paper is a comparison of recent continual learning methods that prevent catastrophic forgetting and their effectiveness in some text classification tasks using popular pretrained language models such as BERT, RoBERTa, etc. The paper divides continual learning methods into three categories: (1) rehearsal based, (2) regularization based, and (3) dynamic architecture. The experimental results show that rehearsal based methods are superior to the other two, and also that BERT is generally better than other candidates. First, it should present a novel view on the problem, and second, it should draw a novel conclusion out of the experiments. I think it is a common belief that rehearsal based is more robust against catastrophic forgetting, while regularization method is more space efficient in that it doesn t have to store examples. The fact that the last layer suffers from catastrophic forgetting is also not a surprising result, given that the lower layers are known to encode linguistic features and the upper layers encode task specific features.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 3; The paper provides an efficient method to generalize to all groups in the presence of sub population shifts and domain adaptation. The paper conducts extensive simulations to derive insights and also numerical experiments on the benchmark dataset to demonstrate the performance. The proposed method is intuitive, easily implemented, and has good performance. The writing is clear and the intuition behind the method is clearly conveyed in the simulation study. The intuition of inter group interactions seems reasonable to me. 1.Close to convergence, the statement that the gradient $\nabla ||\ell_i( \theta_t)||$ can be much smaller than the loss value $\ell_i(\theta_t)$ is unclear to me. It should be $\ell(\theta_t) << ||\nabla \ell(\theta_t)||$ when close to the convergence. In Appendix A, why ||nabla \ell(\theta_t)|| could be as small as $\sqrt{\underline{\sigma} \ell_i(\theta)}$. 2.When $<g_i,g_j>   0$, it does not exactly match the group DRO update steps mentioned at the bottom of Page 2, since there is regularization between consecutive time steps. For the datasets, what are population types "Label * Group" or Group? Why does the worst ratio refect loosely the strength of the spurious correlation? This paper provides a practical algorithm to the broad field of distributional shifts, domain adaptation, and fairness.<|endoftext|>This paper proposes a novel ERM based method for classification task with group annotated training data. The goal is to be group distributionally robust while enhancing the minority performance. The authors make an improvement to an existing method named Group DRO by modifying the focus on the group with the highest regularized loss to focus on the group that leads to the largest decrease in average training loss. PROS:  Building an ERM based method that performs well on the whole dataset without sacrificing the prediction accuracy on the group level worse case really makes sense to me, especially when different groups have different amounts of label noise. The theoretical analyses and empirical understanding provide additional insight into this new algorithm. *** Post Rebuttal: After reading the authors  response and the updated components of the manuscript, I thank the authors for addressing nearly all of my concerns. The inclusion of a clearer motivation, more discussion w.r.t.CGD and datasets, all enhance my understanding of the contributions of the paper beyond my original review.<|endoftext|>This paper proposes to focus instead on the group which leads to maximum decrease in the loss while training instead of the group which has the maximum loss. The paper present several synthetic toy cases where their approach could be useful and concludes with experiments on a variety of benchmarks for this setup and shows improved results. Pros   I feel the idea of training on the group which leads to largest overall decrease in loss is natural and interesting. The synthetic examples presented in the paper are interesting and clearly bring out the use cases of the method proposed and comparison with group DRO. The empirical results presented lead to improved results on a variety of benchmark tasks. Cons  I understand the synthetic examples presented but it is still hard for me to understand why this method also works better for the benchmark datasets. Since, the gradient method is not minimizing any fixed loss function, it is hard to understand what is going on. For example, I am guessing that the groups are formed randomly in each of the synthetic examples by fixing their sizes. The main concern is that there could have been more discussion  on why this method works for these real world datasets by connecting them to the synthetic setups presented in the paper.<|endoftext|>The paper proposes a new method for robust ML under distribution shifts. Past work has looked at formulations that minimize the worst group error. This paper adds a new twist on it and instead argues for focusing on the group that leads to the greatest decrease in average training error for all the groups. The results are shown on several synthetic datasets as well as on the WILDS Robust ML benchmark that show the superior performance of the proposed algorithm over several baselines. The paper proposes a new approach for robust ML under distribution shifts that performs gradient descent not on the group with worst error but on the group which decreases the average error of all other groups. 2).Results are shown on synthetic and real world datasets which show the superior performance of the proposed method in achieving group robustness. The idea that we shouldn t minimize the worst group error but rather minimize the error on the group that decreases all other groups  errors is intriguing! This is also corroborated by the fact that the proposed algorithm doesn t perform gradient descent on a particular loss function. However, the standard benchmarks shown in Table 3 are also altered. For example, the CivilComments dataset is shown as a 2 group when it is originally a 8 group task (the groups being the demographics of the users) as shown in the WILDS dataset paper.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; This is a well written paper, with a novel contribution that is likely to leave an impact in the emerging field of generative models for molecular structures. The paper is fluent, clear, and easy to follow.<|endoftext|>The paper focuses on a generative model for 3D molecules that generates full 3D molecular gemoetry, instead of generating a molecular graph and then generating the 3D molecular geometry conditional on the graph. The method using an autoregressive flow model, which has been applied to molecular graph generation but not full geometry. "These conditional generation methods assume that we are given the target molecules in the form of molecular graphs, but we do not know their 3D geometries.<|endoftext|>The paper proposes a new way to directly generate 3D molecular geometries from scratch. They output the atoms one by one, including the type of the atom and the related position from one chosen focal atom. They have shown in 2 different tasks that they are better than previous methods.<|endoftext|>The paper addresses the problem of generating 3D molecular geometries. The generation is done in a step by step fashion, with each step generating a (distance, angle, torsion) triplet, conditioning on features extracted by an attention mechanism. In my opinion, regarding the representation of the relative atom position, the author should devise a way to validate the necessity and the benefits of this contribution.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; rating score: 8; This article proposes **label learning flows (LLF)**, a general framework for weakly supervised learning tasks. The modelling framework is primarily driven by 1) conditional normalizing flows (Trippe & Turner, 2018), which is used as a flexible conditional likelihood model, with an affine coupling flow layer, and 2) constrained optimization formulation of maximum likelihood, where weak signals/error rate bounds enter as constraints (Arachie & Huang, 2021). How does it perform then? ### Additional Experiments:See ConsConceptually, constrained flow formulation of LLF is interesting and novel. I also do not think that unpaired point cloud completion should be an example of weakly supervised learning. However, its numerical results are not too convincing and there are some questions related to the practicality of this method. Dequantization is typically considered to simplify the fitting of discrete observations, whereas in this paper we do not have any observations from the true generating process. For regression, the paper assumes the existence of M rule based weak signals (Equation 9), which again enter the optimization problem as constraints (Equation 10). Lastly, the formulation for unpaired point cloud completion is presented, which is set up differently from the classification and regression cases. This work makes use of the same weak supervision setting as ALL (Arachie & Huang, 2021), but the training is simpler as it does not require min max optimization. In this paper, it is not clear to me how the error rate bounds are computed — in order to keep things fair, error rate bounds should be determined solely based on the weak signals. Is it possible to clarify this? Inference requires sampling and summarizing which can be slow and limits the use of LLF as a final predictor. As it is right now, if we compare their inference speeds, DP seems to be better. Can we also use LLF in a two stage manner like in DP?<|endoftext|>Is it that you’re simply replacing the sampling assumption in Equation 2 with a constraint? This work focuses on the problem of learning predictive models where no ground truth labels are provided. Later, through context, I was able to understand it as the number of samples of z ~ p_Z, but it should be made clearer. The authors utilize a conditional normalizing flow model as a generative process for labels and constrain it s output by imposing regularization terms on the training objective that represent different constraints imposed by the weak supervision. In their evaluation, the authors compare LLF to a number of weakly supervised baselines and show superior performance on classification and regression tasks, while largely being outperformed by one method in point cloud completion. I am not suggesting that the work has to extensively review the methods LLF is based on, but the paper could be greatly improved simply by fully elaborating on the construction of the method and taking the time to formally define important concepts. 1.Why is it a practical assumption that the error rate for a weak labeler to be known a priori in the weak classification case? However, the paper suffers from a number of major issues. 2.There should be some discussion on the kinds of problems the weakly supervised setting proposed in this work applies to. 3) Some of the proposed used cases lack strong practical motivation. B.It is stated that for two different true labels their constrained spaces are “non overlapped”:        I. Other technical questions that need to be addressed:1. Is it rather that sampling within the constraint set is impossible or that it’s simply computationally infeasible in practice? This is helpful, but elaborating on what dequantization is and why this relationship is important would be better. 4.It’s not clear to me how Equations 2 and 4 result in the constrained optimization in Equation 5.<|endoftext|>This paper proposes a new method for weakly supervised learning where the true label of a data sample is unknown but there are known constraints on the label. The proposed method is based on conditional normalising flows, where the inverse flow is used to model the conditional label distribution and optimised with the label constraints. The proposed method is used in three applications including classification, regression, and point cloud completion. The proposed method is shown to have better performance in these applications in the comparison with several baseline methods. It seems to be novel in this specific setting. 2.The paper has a comprehensive set of experiments in a variety of applications. 3.Overall, the paper is well written and easy to follow. I feel that the context around Eq (4) needs more explanation. Therefore, it s better to have a more comprehensive comparison with the latest related methods, such as PGMV and its variants in [1] and AMCL and its variants in [2]. The idea of the paper is interesting and seems to be effective.<|endoftext|>Different from many existing weakly supervised learning methods, which learn a deterministic function that estimates labels given the input data and weak signals,  this paper proposes label learning flows (LLF), a general framework for weakly supervised learning problems, and it is a generative model based on normalizing flows. The main idea of LLF is to optimize the conditional likelihoods of all possible labeling of the data within a constrained space defined by weak signals. This paper has some positive aspects as follows:1. The proposed method is novel. 3.The paper is well written, the arguments are clear, and the methodology is well presented. The authors need to deal with some issues as follows:4. In Table 1,  for OBS Network, the performance of LLF is poor than other methods, while on other datasets can obtain the obvious improvement. the authors should provide an explanation for this result. 5.For the results in Table 4, it is better, if the authors can give a comprehensive evaluation protocol, which can mix MMD, TMD and UHD together, thus it can obviously highlight the superiority of LLF in unpaired point cloud completion. More experimental analysis can further improve the quality of this paper.<|endoftext|>Overall, I like the paper and vote to accept. That the generative problem is additionally performed within the constraints of point clouds/sets is very impressive and really helps to illustrate the versatility of the idea. That said, I had to read section 4 several times before I understood how the constraints were constructed for a particular problem and am not sure I could construct them on a different problem set. The paper would be considerably more accessible with a simple toy problem with figures/cartoons. This could also be used to help better motivate "weak signals." The use of a GAN as part of the constraints alongside the flow model appears to be a bit of a contradiction but I think helps to illustrate the variability that is allowed within the framework. The paper proposes a clever twist on conditional normalizing flows and shows the versatility of the idea on several different weakly supervised problems. An illustration would help to make the paper more accessible which would increase it s impact.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; rating score: 6; After refuting their claims, the authors introduce vector decomposition for analyzing the collapse based on the gradient analysis of l2 normalized vector, yielding a unified perspective on how negative samples and SimSiam predictor alleviate collapse. This paper is generally well written and well structured. It is important and inspiring to explore the reason why SimSiam can avoid collapse. The authors provide a convincing explanation and reach a unified conclusion for the recent progress in SSL, which is very insightful. The theoretical analysis and experimental results are solid. Based on the abovementioned strength of this paper and the interesting conclusion, I recommend to accept this paper.<|endoftext|>As the title suggests, the paper does a detailed investigation of how SimSiam avoids collapse without negative training examples. The key idea is to decompose the original vector into a center vector component and a residual vector component. The experimental designs are quite solid and insightful. This is a great step toward better understanding of existing methods.<|endoftext|>The paper analyzes how the self supervised learning (SSL) approach SimSiam avoids collapsed representations without explicit formulation of repulsive sample relations. To this end, first flaws in the original reasoning of the SimSiam paper are revealed. Next, based on center residual vector decomposition, the role of the prediction head for preventing representation collapse in SimSiam is analyzed. The conceptual explanation of de correlation in Info NCE is hard is fuzzy and does not sound convincing. While the addressed phenomenon of preventing collapsing representations without the usage of negative samples is interesting, the presented analyis provides insufficient novel insights about how collapse is prevented. Thus, I vote to reject this submission in its present form.<|endoftext|>This paper proposes a framework to understand why SimSiam avoid collapse without negative samples? Where are the "results"? And the authors claim that  the center vector gradient has the de centering effect and the residual gradient vector has the de correlation effect. + The hidden flaw in AO of SimSiam seems to be correct, which is interesting.<|endoftext|>The paper proposes another explanation for why SimSiam can avoid collapse without negative samples. Specifically, the paper decomposes the gradient of learned representation as center vector and residual vector and finds that the center vector gradient has the de centering effect and the residual gradient vector has the de correlation effect. Such an explanation can also be applied to Info NCE, which unifies the theory of self supervised learning with and without negative samples. 2.The results of SimSiam++ which shows a simple bias as a predictor can also avoid collapse without negative samples are interesting. The writing quality of the paper is not very good.
Accept (Oral); rating score: 8; rating score: 8; rating score: 8; rating score: 6; The paper undertakes extensive experimentation in the adaptability of latent space modifications from a base model, and a fine tuned model for a secondary dataset. They demonstrate which areas of the models change most, and which information remains trained in the parameters, despite re training. However, this paper is more of a study of this technique, applied to the particular domain of image GANs, and how their editability changes during the process. Strengths	The primary contribution of the paper is the extensive experimentation, around transfer learning in GANs, and they perform this very well. The paper presents some novel analysis in a popular field of research, with implications that can help drive the field further, with minimal changes.<|endoftext|>This paper provides an in depth study of the properties and applications of the semantic alignment between the original parent StyleGAN model and its finetuned child model on another dataset. Specifically, the paper empirically demonstrates the semantical alignment of the two models. The application involves the image translation, image morphing and zero shot vision tasks. The zero shot vision tasks involve utilizing the label of the parent domain to edit/classify the child domain, which gives some novel ideas but alone are not very thorough to me. (which is also discussed in `[R3]`) The paper discusses that the latent semantics are hidden rather than forgotten during finetune. It will be valuable to also discuss the potential application of this property.<|endoftext|> The paper provides interesting analysis and leveraging of GAN’s model alignment (i.e., transfer learning). Without custom architectures and losses, it demonstrates impressive performance in a diverse set of tasks (image translation and image morphing). These techniques probably inspire others to analyze their GAN models. 2.Many details of the results are missing. I like the analysis of aligned GANs’ models. The paper also demonstrates impressive experimental results by leveraging the properties of aligned models for image translation, image morphing, and zero shot classification. My major concern is about the clarity of the paper and some missing details. Hopefully, the authors can address my concern in the rebuttal period.<|endoftext|>While the model and fine tuning technology lack novelty, the shared semantic information in the generation network are interesting. Finally, the authors applied the proposed aligned model to multiple tasks, including image to image translation, cross domain image morphing, and zero shot classification and regression. Pros:1.The shared latent space for "child" and "parent" networks is interesting, and matches the learning representation goal. The StyleGAN based architecture is a powerful structure, which consists of a mapping network, an affine translation, and a generator. 3.The paper provides many interesting visual results, such as in Figures 1, 2, 3, and 4, to show the effectiveness of shared information in different networks. As can be inferred from the balance in strengths/weaknesses, my preliminary rating for this submission is borderline accept. While the interesting results are shown in the paper and supplementary material, the key contribution for models, techniques or theoretical insights is unclear.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 5; Strengths: The motivation of this paper is relatively new. Weaknesses: The analysis of the relationship between graph contrastive learning effective conditions and data augmentation is not theoretical enough. Finally, I think the contribution of the paper is not enough. The motivation of the article is good, but I have some problems in terms of contribution and theoretical analysis. Experiment and analysis are okay.<|endoftext|>This paper studies the success conditions of graph contractive learning (GCL) with generic graph augmentation (GGA) and claims the success depends on the graph edit distance between classes. The problems investigated in the paper is interesting and is significant to the unsupervised graph representation learning community. However, their claims lack rigorous mathematical and empirical demonstrations. The claims of the paper mainly build  upon the theoretical results of a recent paper [1], whose details are omitted in the paper. 3.Additionally, the claim, that the success of GCL with GGA depends on the graph edit distance between classes, lacks rigorous mathematical and empirical evidence. 4.The experiments were conducted for only three methods (i.e., GCL, GAE, AAGAE) and the analysis of paper is restricted to GCL with GGA. This paper studies an important open problem,  but is lack of rigorous mathematical demonstrations and has limited empirical studies.<|endoftext|>Many works draw lessons from the computer vision community and design a bunch of methods to generate the augmented views for contrastive learning graph representations. However, it s still not clear why and to what extent the data augmentation methods have positive benefits for directing the graph neural models to the right way. The studied problem in this work is very important for the research community. I just wonder the generalizability of the claim raised by this work. Suppose that the GED has positive impact, the writing is difficult to follow. There re many claims which are copied from the previous work proposed by HaoChen et al..<|endoftext|>The paper works on an interesting and valuable problem. One of the claimed contributions is the “theoretical analysis of when contrastive learning is expected to work well”. However, in section 3.2, there is no link of the “fixed error” of the contrastive learning performance to the graph properties. However, the analysis results are not strong enough to support the claimed contributions.
Reject; rating score: 3; rating score: 5; rating score: 8; The paper proposes to do one shot domain adaption by fine tuning a state of the art network StyleGAN. The paper conveys the main ideas of the work to the reader and I believe I got a good understanding from what the authors did by reading the paper. In a future version of the paper it would be good to see more of these interesting results. The metrics are not especially revealing if the results are good. It s good to have them, but I feel the authors still need quite a bit of work to make the system function as desired. There are multiple concerns here. I am very positive about the potential of this work, but it needs a substantial improvement. The layer is too early in the network and seems to manipulate z space rather than w space (or some of the other available latent spaces later in the network). If the z vector is split, this could be a bit better, but also carries more risks. As it is, we can only observe a limited form of domain transfer. This is not a result that should be considered domain adaptation. While this uses the same software system, it is essentially a different task that has been addressed better by many other papers. The challenging domain transfer results, e.g.Figure 5, are indeed what I was hoping to see more of.<|endoftext|>(2021).AgileGAN: stylizing portraits by inversion consistent transfer learning. Pros:1.The one shot image generation is a novel and interesting task. My preliminary rating for this submission is marginal reject. Cons:While the authors aim to address a challenging and novel task, I believe some parts need more clarification (even after considering the supplementary material):1. In addition, interesting experimental results are provided, but they are not well analyzed in the representation domain. The biggest weakness is that the proposed method has limited novelty. What are the differences between the proposed adaptor and these prior works? Why the proposed adaptor would like to perform better? In addition, if I understand correctly, the attribute classifier only judges the output is real or fake, instead of predicting attribute labels, because some examples in Figures 2 and 3 should not have corresponding labels. If this classifier just outputs real or fake labels, why not just fine tuning the final layer of the original discriminator? The authors did not provide a new direction to sell this strategy.<|endoftext|>Strengths:* This paper tackles a challenging domain adaptation problem which is very interesting. The conclusion in the paper is that it is often sufficient to obtain a good decision boundary if you have one positive example and many negative examples. The reviewer would suggest incorporating such discussions in the final version of the paper. * The limitation of the paper is not clearly stated. It seems to the reviewer that the proposed method could fail at “car  > truck” application when the inter subclass variations are huge. Please comment on this in the rebuttal. In this sense, this paper should also include comparisons to the style transfer methods. Overall, this is a very interesting paper with convincing results. I recommend accepting this paper but would kindly ask the authors to incorporate the suggested discussions in the final version.
Accept (Poster); rating score: 8; rating score: 5; rating score: 5; rating score: 5; This paper studies why optimization based adversarial attacks have close to zero target transferability in attack real world ASR pipelines. This paper studies an important problem and provides valuable insights into the mysterious fact of the very low target transferability between ASR models. The paper is also well written and nicely structured. Below are my comments that can further improve this paper:1. The experiments on the input type do not seem very interesting to me. 3.This paper only uses a single model, DeepSpeech, which is a character based model. A good paper that studies an important problem with carefully executed control experiments.<|endoftext|>It is known that targeted transferability is ineffective against typical ASRsystems, but it is not fully understand why. In this work the authors show theimpact of targeted transferability against a simplified ASR system. First, asimplified ASR system is required, as it has yet to be shown that complexreal world ASR systems have 0% targeted transferability. The writing is clear, and the design is simple. The model is not fully specified within the text, only mentioned in a reference. The model to me appears to only have 9 labels which are synonymous with vocabulary size. Also, since there are no details of the model within the paper, how is the alignment issue handled? 4.Why wasn t there an ablation study on more complex RNN cells (LSTM, GRU). d. No reference for Deepspeech. However, I think a little further workcould have been done to improve the results as well as the explanations.<|endoftext|>The paper conducts a systematic study on the phenomenon that attacks targeting ASR systems often have low transferability. To do that, the authors take a representative ASR pipeline and perform ablation studies by modifying or removing the components that may have an effect on the attack transferability. Results show that many existing designs for improving the robustness of ASR can also prevent transfer attacks. Ablation evaluation methodologies are systematic. # Weaknesses  Lack of in depth root causes analysis on the findings. The paper provides some discussion in each ablation study. However, the current discussion does not go beyond describing the observations of the experimental results. This is not an easy task, where additional small scale experiments may also be required as the authors generate hypotheses. Such analysis also helps to understand the unique challenges in the ASR and perhaps improve the robustness of image task models. The paper studies an important problem and presents many interesting findings. However, it still lacks root cause analysis on the findings to indeed “demystify” the low attack transferability in ASR.<|endoftext|>It first lists 11 known factors, e.g., smoothness of gradients, and then proposes four potential factors that limit the transferability. The second factor, MFCC, should be generalized. I am not an expert in this field, but I suppose there are more algorithms than just MFCC that do this operation. It generates 500 adversarial audio samples for each of the five ASRs and tries to transfer samples to others. I don’t understand the RNN factor, which is not clear. The current evaluation is not extensive. Strengths:+ The paper is well written and easy to follow. The paper can benefit from giving a deeper evaluation of these identified factors. + The paper does lots of work for the system implementation and testing. + The paper gives many details in implementation and evaluation. And what is the impact of using RNN with different complexity? Although this paper looks simple, it has actually identified an interesting, novel problem. I appreciate that the authors have done research on such an important topic. This is the main limitation of the paper. I would suggest accepting this paper as a short paper or a poster if possible. Isn’t the total number of Output Labels factor the same as Output Type factor? So please define what is a factor.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; From there, a latent variable for intermediate waypoints is introduced, which leads to the formulation of an ELBO for the goal reaching objective of C Learning. ICLR, 2020. Goal conditioned reinforcement learning with imagined subgoals. I would recommend acceptance. I think further clarity about the exact conditional inputs fed into the C learning classifier are necessary, preferably in the main body of the paper. Given that the classifier (critic) introduced in C Learning is a proxy for the assumed distribution over future reachable states, I think this is important.<|endoftext|>This is valuable progress on the field of learning goal conditioned policies. Strength: Robust, solid and strong theoretical development of an EM algorithm to learn long horizon goal conditional policies that learn in a curriculum manner.<|endoftext|>Why do you need those two? The experiment was conducted on various goal conditioned RL tasks on 2D mazes and the robotics manipulation tasks. The paper is clearly written and easy to follow. Also, it is counter intuitive in that the pdf of negative binomial distribution is bell shaped. Is there a reason that C planning cannot perform test time waypoint sampling?<|endoftext|>2.I am a little confused by the performance of SoRB. I tend to accept the paper. I agree that the Obstacle Drawer Close task is pretty challenging. Such an approach benefits from the fruitful goal conditioned value function learning and can make progress in sparse reward settings.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; rating score: 6; This paper proposes a deep learning based approach for extracting theorems from existing human proofs. I don t think it is a useful task to extract reusable patterns from existing proofs as stand alone theorems and I can not see how a tool that can solve this problem could help the AI/TP community. So maybe the main story of this work should be improving theorem proving by learning to compress the human proofs. The problem is formulated by classifying the nodes of the proof trees in/out of the extracted theorem.<|endoftext|>The goal is to extract useful lemmas from existing proof trees. This is then cast as the problem of predicting whether each node should be extracted as a new theorem (or lemma). The main idea of the paper is simple, clear and interesting. The assumption that the loss function decomposes as (3) is in line with the authors  explanation "Our approach inspects one proof at a time and this intuition comes from the fact that human mathematicians do not need to look at multiple proofs and can instead determine whether a proof segment is broadly applicable just from the current proof." On the contrary it feels like merely attempting to compress the library of proof trees by identifying repeated sub trees (across proofs) could also work and violates the assumption above. This is an interesting and simple (in a good way, i.e.clear) idea for turning the problem of lemma extraction into one of supervised learning on the nodes of a graph. Moreover there are more direct ways of achieving the same result, but these older methods are not compared to.<|endoftext|>The result is that they are used frequently in many proofs, lead to shorter proofs, and can improve the baseline theorem prover s performance. The paper is well written and presents an interesting methods for mining library of proofs for re usable new lemmas. * no qualitative comparison with the author s work is given with other machine learning for theorem proving, the related works are only listed. * how is this work related to other formal systems that extract lemmas, so example Isaplanner? * A lot of comparisons and results are given in absolute numbers without percentage comparison, so it is difficult to judge if the improvement is significant, for example:   * is 19.6% a good result? In comparison to what? Sure synthetic libraries could be created, but the authors do not explore this avenue.<|endoftext|>Without these baselines, I fear that the impact of the work will be hard to estimate. Figure 1: while having a motivating example is great for improving readability, the labels of the nodes in the graph are harder to understand without additional background. Is there a way to rewrite this figure with either/both (a) a simpler example, (b) introduction/descriptions of what the abbreviations mean, and what how affect the overall proof. This experiment is missing from the work, and calls into question the peak performance of REFACTOR. Using a neural network on existing proofs, the authors show that sub theorems can be extracted an reused to compress a proof library. However, aspects of the approach lack baselines or natural comparison points, making the work feel too isolated from current automated theorem proving literature and putting into question its value to the research community.<|endoftext|>The paper describes a neural method for identifying useful lemmas in large proof graphs. The method is evaluated on the MetaMath library and gives a good improvement. I have not checked if a similar approach was used in related graph learning tasks (code graphs, etc.). The paper is written quite clearly and wrt the ML community, it is on a relatively new and interesting topic. The novelty is inusing GNNs for training some of the graph classification tasks insteadof using methods such as PageRank. Despite using possibly "more advanced technology", the improvementmeasured in this work seems quite comparable to the previous simplermethods. The data reported in previous work are 5 20%, while here itis 15%. Generating relevant synthetic theorems is a nice and nontrivialresearch direction. The performance of the systems is good.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 8; This paper gives a high probability excess population risk bound for differentially private optimization algorithm under $G$ Lipschitz, $L$ smooth and PL condition with gradient perturbation. Pros:The authors discussed each assumption in detail, which gives the result under various assumptions. The analysis hints that the generalized Bernstein inequality could provide a new tool for improving population risk bound and other differentially private algorithms. Will it be open source? The paper shows that normalization helps with improving population risk bound, though the convergence rate of the algorithm is yet to be discussed. If so, to what extent should we expect normalization to affect the rate? Technical questions:For Theorem 3, in the step where you get R_n(\hat{\theta}_t)   R_n(\theta^\ast_n), the authors refer to Lemma 7, which gives us the bound on R_n(\theta_t)   R_n(\theta^\ast_n). In the proof of Lemma 3, why do we need to assume $i n$? This condition is not used in (4) and the remaining proofs. • For (17), is \psi is defined prior to this equation? There s also some reservation on the significance of the techniques presented in the proofs.<|endoftext|>The paper achieves high probability excess risk bound with rate O(1/n) w.r.t n for DP models via uniform stability by using Generalized Bernstein condition under G Lipschitz, L smooth, and PL condition. But the result is worse than before, so in order to get a better result, they propose m NGP algorithm to achieve O(1/n) high probability bound w.r.t n under α Ho ̈lder smoothness, Polyak Łojasiewicz condition, and generalized Bernstein condition. I think the problem statement of the paper is interesting and important. It would be good to bring the experiments on real datasets to verify theoretical results. But there are some limitations in the paper. Secondly, the authors only give the upper bound of excess risk bound and there is no lower bound, so it is unknown that their bound is optimal. It is not sure that there is the function satisfying assumption 4 and assumption 5,so the problem setting may be not reasonable. And the authors do not give the loss function task in detail in experimental part. There is no lower bound so we do not know if this upper bound is optimal. And the authors should give more experiments about different parameter.<|endoftext|>This paper extends the results in Klochkov & Zhivotovskiy 2021 to the DP case, proving the first $O(1/n)$ high probability excess population risk bound for differentially private algorithms, assuming Lipschitzness, smoothness, Generalized Bernstein condition and Polyak Łojasiewicz condition. Then the authors relax the assumptions of Lipschitzness+smoothness to $\alpha$ Holder smoothness, and propose a new normalized gradient pertubation algorithm that also achieves the $O(1/n)$ bound. The results of this paper are significant, improving an $O(1/\sqrt{n})$ term over previous utility bounds, with the help of introducing the Generalized Bernstein condition and the Polyak Łojasiewicz condition. Weakness:The proof of Theorem 1 mainly follows Klochkov & Zhivotovskiy 2021, extending their result to the DP case using the same method. Theorem 2&3 have more novelty, but are less important than Theorem 1 in my opinion. The technical difficulties should be discussed in more details, to convince the readers that 1: the two new conditions are not too strong, 2: extending the results in Klochkov & Zhivotovskiy 2021 to the DP setting is non trivial. The presentation of results is clear. Extending existing results to the DP case and getting the first O(1/n) utility bound is significant.<|endoftext|>This paper analyzes the utility bounds of the gradient perturbation based DP algorithm. They first provide DP by previous result (it is not the key point in this paper). So they propose an algorithm (called m NGP) to improve it under the assumption Holder smooth, and it is claimed that the utility bound can be improved to $O(p^{0.5}/(n\epsilon))$. This paper proposes the first $O(p^{0.5}/(n\epsilon))$ high probability excess population risk bound for DP algorithms under the assumptions Holder smooth, and PL condition (or Lipschitz, smooth, and PL condition). Comparing with previous results ($O(p^{0.5}/(n^{0.5}\epsilon))$), the improvement is significant, of the order $O(n^{0.5})$. 2.For the provided utility bounds, convexity of the loss function is replaced with the PL condition, so the results can be applied to many non convex settings. 3.The paper also generalizes the $O(p^{0.5}/(n\epsilon))$ result to the non smooth settings, by assuming the loss function to be Holder smooth and introducing normalization to traditional gradient perturbation method. In this paper, the algorithmic stability is applied to bound the gap between the generalization error and its expectation, and via the Generalized Bernstein condition, the expectation one is solved. 5.Experiments on real datasets are performed to evalute their proposed algorithm m NGP, the results show that the accuracy is better than traditional gradient perturbation DP method. For example, in Theorem 1, the authors say that  the gradient perturbation based DP algorithm is $\mathcal{O}(T\eta/n)$ uniformly stable w.t.t n with high probability , but what are the connections between the uniformly stability and the given result? While the algorithmic stability is indeed closely related to the generalization error, the descriptions should be more detailed. The authors should check them carefully.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; This paper introduces an extension of conformal prediction with a different formulation. It is well supported by theoretical arguments and experimental results. The presented idea is in fact quite similar to this ICML paper: https://arxiv.org/pdf/1802.07167.pdf. It would be interesting to compare empirically with that method. The experiments for multi output regression and multi class classification are a nice add on to illustrate the broad applicability of the presented method, but they are not essential for the paper. However, I don t see a problem with the main message of the paper. Some minor issues w.r.t.comparison with existing work.<|endoftext|>This paper generalizes the standard conformal prediction calibration setup to a constrained empirical risk minimization problem. In general, the paper is well written and the idea is appealing. It also explains practical ways of learning this problem via differentiable surrogate losses and Lagrangians. It also empirically shows that the proposed method can improve over baselines that are not directly optimized for efficiency. It is useful to formalize what practitioners stand to gain/lose by directly optimizing conformal predictors for efficiency. The framework itself is fairly simple (which is a nice thing), and empirically improves over a good CQR baseline. There are, however, some weaknesses, and I do have a few questions/concerns. The bounds given in Props. 3, it should be $L_{\mathrm{eff}}$ instead of $L_{\mathrm{coverage}}$.<|endoftext|>This paper considers to improve efficiency of conformal prediction (measured in the "size" of prediction sets). To this end, this paper uses multiple learnable parameters by generalizing single parameter conformal prediction. By doing so, the paper demonstrate that the proposed approach can improve the efficiency of conformal predictors while satisfying valid coverage. The paper considers an important and interesting issue (i.e., maximizing efficiency of conformal prediction). 2.The proposed approach is evaluated over various setups and datasets (i.e., 9 regression datasets, 7 multi out regression datasets, and 2 classification datasets). However, the proposed approach mixes up the two isolated procedures and making it complex for getting better conformal predictors. 5."3.2 Theory" is not well connected to the final algorithm; basically the proposed algorithm is (1) fine tune the score function with a proper efficiency loss and (2) run a known conformal prediction for coverage guarantee. *[R1]: https://arxiv.org/abs/2001.00106This paper considers interesting and important problem, and the proposed approach is broadly evaluated; but as mentioned in the main review, I lean to reject though I m willing to adjust my understanding and score. To my understanding, the key message of this paper is that finetune a base predictor with respect to a desired length metric (along with a coverage constraint) if we want to improve the efficiency (in the same length metric) of the final conformal predictor given fixed sample size; this is likely to be true and is more convincing given the additional experiments. As one minor note, [R1] is applicable to both classification and regression as demonstrated in their experiments.<|endoftext|>This may well be a matter of emphasis or branding. Thus, one main contribution of the paper is to propose a differentiable proxy that allows an efficient search over the parameter space. (Section B.2 from after Eq.(11)) $\hat L_{\text{coverage}}$   > $\hat L_{\text{eff}}$ and $L_{\text{coverage}}$   > $L_{\text{eff}}$ throughout  (Section B.3) $R_n(\mathcal{C})$ is used, but $R^{\text{eff}}_{n{\text{cal}}}(\mathcal{C})$ was given on p. 6. ## StrengthsThe paper is a useful sequel to the earlier work of Yang & Kuchibhotla (2021). (p. 21) In **Methods for learning prediction sets**, $i$ denotes the epoch count. I tentatively recommend it for acceptance with some reservations. I also do not understand what the authors mean by "1r decay." This has created an odd imbalance in the manuscript. 3.*Originality*: As of writing, I am still trying to decide whether the contributions of this paper are substantial enough in light of the contributions of earlier works.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; Their key idea is to use an existing vision language joint embedding model (teacher), and distill its knowledge into a typical object detection model (student). The proposed method achieves impressive results on novel object detection, outperforming not only the best open vocabulary detection method, but also fully supervised detectors. 4.Unlike prior work that relies on specialized pretraining, the proposed method decouples pretraining and training by using two separate models. the authors distill an existing classification model on an existing detection model via an existing distillation method. * The authors do not mention how many base and novel classes they have in the LVIS experiments. How do the authors address this problem?<|endoftext|>This detector is then evaluated on a set of novel categories. Compared to prior work, this paper tries to distill knowledge from strong pre trained vision & language models (like CLIP) into an open vocabulary detector. This may be unclear for readers not familiar with the details of object detection evaluation, and the numbers may then seem surprisingly low for a metric that measures recall.<|endoftext|>The paper shows that the proposed approach can achieve better performance than supervised methods on rare classes in the LVIS dataset. However, there are still several problems with the paper which need to resolved before it can be considered ready for acceptance. Why is this? I think the authors should just call it an open vocabulary setting which can be weaker than a real zero shot framework. 7.For table 4, what is the performance of supervised methods for COCO novel classes? Further, the authors should clarify that the CLIP model in ViLD image and the ViLD text models have seen instances of $C_N$ during the large scale pre training which makes it a weaker zero shot setting than prior works.
Reject; rating score: 3; rating score: 5; rating score: 6; The comparison of results is not appropriate and accurate. Strength: It seems that this paper provided new convergence results for the decentralized setting. So I recommend a reject. However, the related works are not well compared or missing.<|endoftext|>In this paper, the authors develop a distributed zeroth order algorithm over a time varying communication graph, as well as its multi stage variant with time varying step size. 0.This paper is well organized and clearly written. The results seem to be reasonable. More extensive numerical experiments are necessary. Overall, this paper has disadvantages in novelty, depth of analysis, and numerical experiments.<|endoftext|>This paper is well written and the theoretical results are solid. I have a few concerns as follows,  The novelty of this paper: the idea of both the MOZAPA and multi stage MOZAPA algorithms are not novel.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 8; To resolve this issue, the author proposes FedSS, by which each FL client samples a set of classes and defines the loss function only with the sampled classes. Since unsampled classes are not involved in the client s training process, the client sends only the parameters corresponding to the sampled classes. The authors empirically show the proposed approach can reduce the communication cost without sacrificing performance. The proposed method can reduce the communication costCons,The most significant weak point is that the proposed algorithm is not free from the privacy issue. Since every sampled set of each client has to include the classes that the client has, the central server can infer the classes the client has. It would be much better to provide how much the algorithm can save the communication cost. Experiments should include more results considering other data sets and neural net architectures. Therefore, ICLR papers are expected to have comprehensive experimental results. This paper should be very careful to define a class subsampling for federated learning applications since it can leak clients  private information.<|endoftext|>In this paper, the authors proposed federated sampled softmax (FedSS) for resource efficient federated image representation learning. FedSS allows subsampling the weights of negative classes to reduce data transfer and leads to similar accuracy compared to the full softmax. 2.FedSS provides a potential solution to reduce the communication cost of the classification layer in federated learning settings. However, the datasets used in this paper mostly have a limited number of classes (SOP and Landmarks). Therefore, it is questionable if the proposed algorithm has real life benefits. 3.The proposed method degrades the accuracy (even in Table 1, the results do not really match). Since the communication savings on SOP and Landmarks are small, the smaller model baseline should also have roughly the same performance. The paper is generally well written. However, the experimental results and technical novelty are less convincing. I would lean towards objection for now.<|endoftext|>This paper works on "supervised" representation learning in a federated learning setting. The main goal is to save the communication cost: by preventing sending the entire fully connected layer between the server and the clients if the clients only have data from parts of the classes. The authors proposed federated sampled softmax, which is to compute the softmax only over the "positive" classes of which a client has data and a small portion of the other negative classes. I have also listed several potential ways to strengthen the paper (please see the main review). Overall, I enjoy reading the paper as it pointed out an interesting fact and problem. The paper points out an interesting fact and problem: most of the parameters in a neural net are in the last fully connected (FC) layer, while in a large number of class and non IID setting, each client may only have data from a small portion of classes. 2.The authors conduct experiments on three large scale datasets. Specifically, in [c], the proposed method did include the positive classes in a minibatch, together with a set of subsampled classes, to compute the softmax. If we do not consider the computational cost at the client end, will metric learning (without learning the FC layer) outperform the proposed method? 3.The experimental setup can be improved. I also have concerns about using the pre trained features for 4.2 and 4.3, which makes 4.2 and 4.3 like downstream tasks rather than representation learning. [d] Li et al., Federated optimization in heterogeneous networks. I would suggest that the authors add "supervised" into their title, to contrast to many recent works on "unsupervised" representation learning.<|endoftext|>The authors propose to alleviate the resulting computation and communication burden by using sub networks for each client, with a shared feature extractor but with only a smaller number of classes, re using the sampled softmax proposed by Bengio & Senécal 2008. The resulting algorithm, Federated Sampled Softmax (fedSS), is benchmarked on image classification and image retrieval tasks along with baselines and variants. Experimental results demonstrate the validity of the approach. I have a few questions and suggestions:  in table 2, the proposed method reaches a better performance than fullSoftmax. The paper mainly focuses on the choice of the classes to include in the sampled softmax, and bring a very good experimental support for the method. It would also have been interesting to study:    different NN architectures (only mobilenet v3 is used)    what is the effect of the proposed method when the number of local epochs increases? This number remains fixed to 1, leading to a very large number of rounds (2k or 5k);    What is the effect of label class heterogeneity on the method? e.g.for the SOP dataset one could study splits with varying heterogeneity. The experiments are well conducted and support the claims of the paper. I think this paper should be accepted.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; Strengths:  The findings in the paper are useful for researchers in this domain from two folds: Firstly, by open sourcing the checkpoints, others can take advantage of the trained models to foster research in various NLP applications. Secondly, the heuristics provided can potentially guide the design of better architectures for downstream tasks.<|endoftext|>Therefore, the novelty of this strategy is not significant. [1] Goodfellow et al.Deep Learning. Some of the findings provided in this paper is kind of intuitive and the DeepNarrow strategy has been shown in previous literature, which hurt its novelty. Weakness* The discrepancy between upstream perplexity and downstream performance is somewhat related to model selection with dev set in natural language generation task.<|endoftext|>One main concern of mine is the randomness of the paper since this paper performs comprehensive experiments and concludes insights based on the results. The authors perform comprehensive experiments and provide insights from different perspectives. 3.T5 is a sequen to sequence model. Furthermore, the experiment results may be effected by the randomness and I don t find the error bar in the paper, which is the most concern of mine.<|endoftext|>Incremental experiments investigating the downstream performance are provided. The model checkpoints and codes will be publicly released to the community. **Weaknesses*** The novelty of this paper is limited.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; This paper conducted a comprehensive study of high probability generalization bounds for minimax problems in terms of various forms of measuring generalization such as strong/weak PD generalization and primal generalization error. 2.While the results are new and interesting, the proof techniques are directly modified from Klochkov and Zhivotovskiy (2021). The main proof techniques are adapted from Klochkov and Zhivotovskiy (2021).<|endoftext|>In comparison with the results of the previous work, which either provide expectation bounds or high probability generalization bounds of $O(1/\sqrt{n})$, this paper gives improved high generalization bounds of $O(1/n)$. Overall, I think this is a timely solid work for the minimax learning problems which covers many interesting topics. 4.In Remark 1, authors repeatedly claims that what if argument stability and the strong PD empirical risk are of the fast order $O(1/n)$.<|endoftext|>High probability generalization bounds for minimax problems are proposed. I took a look at the results of Farnia & Ozdaglar, and it appears that their paper compares the expected generalization risk E(R(Ax(S) R_S(Ax(S))) which corresponds to eta >0.<|endoftext|>This paper consider a stochastic minimax problem of the form $\min_x \max_y F(x,y)$ where $F(x,y)   \mathbb{E}[f(x,y,z)]$. The paper provides new results in high probability that are valuable confirmation that standard algorithms will work well.
Reject; rating score: 3; rating score: 3; rating score: 3; This paper proposes using homoglyphs to attack commercial NLP models for sentiment classification. Evaluating transferability of the attacks between different services would be useful. The contributions over "Bad‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌ Characters: Imperceptible NLP Attacks" paper is not clear at all, the authors cite this work but have not evaluated as a baseline or discussed the differences of their work.<|endoftext|>The technical novelty of the proposed method is limited. The empirical evaluations are not solid enough to demonstrate significant contributions.<|endoftext|>3.The authors do not evaluate any defense methodologies against these attacks. The authors need to better position their work w.r.t prior art to highlight their contributions. 5.What are some other use cases of the purported attack?
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 3; This paper proposes a different look at why neural networks generalize despite optimizing to zero training error, over parameterization, etc. The contribution is mostly experimental in the sense of computing various statistics of a model during training and correlating those statistics with generalization performance. The core idea is to define "image functions" which are determined by the training image and the current training iteration. Correlation statistics on these functions for different and same training images show patterns in the training dynamics, which may indicate generalization for example as training continues towards zero error the statistics still vary. In particular the results are specialized for ReLU networks training with SGD. However, there are some weaknesses with the paper that point to more work and clarity before publication. The main application shown is in Figure 7 where a candidate statistics namely ratio between early and mid cross path is correlated with generalization performance. For example both VGG 19 and ResNet 110 perform about the same for CIFAR10, but have significantly different statistics. For the other data sets such as CIFAR100 there are only two points to establish the correlation. Overall, the paper tackles an important problem, namely explaining generalization via an experimental study and using a novel idea of "image functions". Also the description of the details is somewhat unclear, and more importantly there isn t a good discussion of how this approach relates to previous work. It is claimed that the evidence shows algorithmic stability but this isn t justified directly, and the reader also isn t clear whether the results show variance reduction or some other idea that leads to generalization.<|endoftext|>I have a few questions and concerns. 1.The major claim of the paper is that neural networks generalize since they learn different functions for the same image during training (page 4). If this is the case, the paper requires some formal mathematics to bridge the gap between the dynamics of image functions and generalization error. The only thing related to images is CIFAR10 and ImageNet. This statement seems to be expected as the network parameter is keep updating via SGD. In particular, the function represented by neural networks is expected to change over time, it would be great if the authors can provide more clarification about this. 5.As the major claim is about generalization, it is better to theoretically or empirically provide some evidence related to generalization error. Although figure 7 provide some hints, but it is still elusive why this improves the generalization. However, it seems that the claim is not explicitly examined in the experiment section. For instance, how to concretely encourage the model to learn the same image in different ways? Overall, the paper addresses an important problem.<|endoftext|>The authors present empirical results about the correlation between the activations in a neural network across time for a fixed pair of images. The authors claim that this provides insight into why neural networks generalize, by arguing that the network uses different features at different epochs during training. This is an entirely empirical work. The authors look into the activations within a neural network (as either a path or a set of real valued activations) and examine how such activations change over time during training for the same pair of images. The authors  arguments about how such findings help understand generalization are heuristic. The authors claim that because the activations change over time, the network learns to use different features for the same image, and that improves generalization! Unfortunately, such statements are the main contributions of the paper and they are neither justified nor precisely stated. The paper also needs a lot of improvement in its presentation. The related works section lists papers related to deep learning but I don t see the connection to the present work. In addition, there are lots of research papers that study activations/representations in the neural network that are not cited, which would be more relevant to the present paper.<|endoftext|>This paper analyses how the representations of neural networks evolve during training, in particular which ReLU nodes are activated. The authors attempt to show how this leads to a diverse set of functions being learnt. Weaknesses :  Introduction is a bit disorganised, feels a bit too much like a list of independent concepts, and it is hard to extract the relation with the contribution of this paper  The paper is poorly written, poorly structured, and the results are extremely difficult to parse, in great part due to imprecise language. A few examples : “indicating images intersect each other on different features at different points in training” ; “the ReLU gate and SGD optimization encourages xi to use a different set of previous features from other images xj” ; “supporting the case for considering each image update as its own function, and again indicating that there is a kind of function diversity present in ReLU network training” ; “the same image xi is passing through different nodes at different times t”  Due to this lack of rigour, I struggled to understand the main message of the paper. This is not clear either from the experimental section : the fact that activations / paths become more and more correlated from one epoch to the next through training is not surprising,Comments :   “The key question behind generalization is why optimizing over E_U results in low loss over E_D” : I disagree with this. In typical studies of generalisation, the train and test data come from the same distribution (when they do not, the problem under study is out of distribution generalisation).
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; Strengths:  	This paper systematically revisits the underrated baseline of sparse training – random pruning. As the networks grow wider and deeper, the performance of training a randomly pruned sparse network will quickly grow to matching that of its dense equivalent, even at high sparsity ratios. Another meaningful discovery is that, echoing prior arts, the layer wise sparsity ratio or “effective width” seems to be a critical factor for high performant sparse training. Questions or Suggestions: 	One weakness I feel about this paper is its interpretation part in Section 4.3. A minor limitation of this work is that random sparsity will only work so well when the network is large/wide enough, as the authors also pointed out. (Optional) With SNIP/Grasp being evaluated, it is recommended that the authors can compare with SynFlow and some more recent sparse training baselines too. The paper could yet be strengthened further by providing more interpretations of its observations.<|endoftext|>Random Pruning randomly zeros out the weights in a network with a target budget and is often used at a layer level based on the chosen sparsity ratios. Overall, I think I don t see as much novelty in this paper (which is fine) however even the observations made are not completely novel. The observations seem to point to the fact that as the model size increases the pruning method becomes obsolete and everything converges to the same solution. They also show that sparsity ratios matter more than anything else for pruning techniques. I still think this paper is a good asset and will be a great one if the same scale of experiments is done on ImageNet instead of CIFAR and will prove to be a solid exploratory work in that case. The evaluation of this simple baseline should give perspective to new readers about what components affect the final deliverables most. I want to see these updates and results in the main paper and not the appendix and you can do this as revision in the next few days. 9) The experiments trying to understand random pruning via gradient flow is interesting and give some perspective as to why certain pruning ratios might be doing better. That is the same observation being made here.<|endoftext|>The comparison is not an apple to apple comparison (not scientifically valid), no matter how surprising the result may look. This paper has substantial overlap with a previous paper. Weaknesses:This paper appears to present surprising results (as indicated by the title   The “Unreasonable” Effectiveness). What this paper really discussed and compared is pruning at initialization (PaI). Otherwise, if you want to claim random pruning can be on par with other pruning methods (let’s take the most simple magnitude pruning as example) under the traditional pruning case, please try to add this result: Prune a pretrained ResNet50 with 90% sparsity on ImageNet, compare random pruning with magnitude pruning (extensive previous methods actually have shown the latter is better). Even similarly, in [*1], after discovering randomly shuffling masks does not hurt accuracy, they thus proposed “In other words, the useful information these techniques extract is not which individual weights to remove, but rather the layerwise proportions by which to prune the network”. Then look at the 2nd contribution claimed in this paper: “We further identify that appropriate layer wise sparsity ratios can be an important booster for the performance of random pruning”. Although it presents more empirical evidence to show random pruning can be useful in some cases (out of distribution detection, uncertainty estimation, and adversarial robustness), yet the major claims and points are already established in [*1].<|endoftext|>Results show that random pruning can be quite effective for sparse training especially on larger or wider models. The paper draws attention to the commonly used baseline method random pruning and finds that it can be very effective for training a sparse network. Adopting these layer wise sparsity ratios as standard sparse initialization for future work seems necessary. 4.The findings of the gradient norm seem consistent and correct, making good connections with previous works. The effectiveness of random pruning has been investigated in previous works before, although not as detailed as in this paper. 2.There is not much novelty in the method itself. Mainly using multiple existing pruning methods to support their claim. In the dynamic sparse training prune and grow scheme, if you randomly prune and grow the weights, can it be considered random pruning? Although the paper does not provide new pruning techniques,  I believe the conclusions from the paper can contribute to the sparse training domain.
Reject; rating score: 3; rating score: 6; rating score: 6; rating score: 6; This paper presents a method for segmentation using wavelets and Multi Resolution Analysis. 5) I suggest the authors to draw a diagram of their network. 6) The improvement seems marginal compared with U net7) There are many methods for segmentation that shall be cited and compared. 2) The motivation of the paper is not clear.<|endoftext|>In this paper, the authors propose a novel deep learning framework for the fast prediction of boundaries. The boundary is modeled as smooth closed curves with wavelets and multi resolution analysis. It is the first work to construct pixel independent representations of curves instead of pixel based output, which provides new insight into using wavelet analysis on segmentation tasks. It d be better to demonstrate with figures. 3.The experiment results are not convincing enough. More experiments on other datasets are needed. It lacks explanations on the reason for using these (j1,j2,j3) settings in the experiments. 4.It lacks explanations on the reason and motivation that it conducts wavelet analysis on the boundary instead of the object itself. The paper provides novel insight into medical image segmentation. The technical introduction is sufficient and makes sense.<|endoftext|>The paper proposes a method for boundary extraction from images using a U net type network architecture. Strengths:1) In contrast to the related literature, where pixel based segmentations are provided, the output of the proposed method is a closed curve. Weaknesses1) The extensive presentation of multi resolution analysis could be avoided as the theory is well known. Although an important part of the paper presents well known theory, I like the idea of estimating closed curves and I suggest accepting the paper.<|endoftext|>In this paper the authors propose a mixed approach to image segmentation using CNNs and wavelets. The results demonstrate that it could be a powerful approximation: fast, accurate and easy to train. The mathematical introduction is quite dense. Besides, there are some functions that are not defined (such as L(Z) and l(Z)), which makes it difficult to understand the article for those who do not know how Wavelets work. However, the main weaknesses of the paper are:1. Although there are other similar approaches (and even with sub pixel level segmentation) using wavelets the authors compare with a generic segmentation approach based on a classical encoder decoder structure. This dilutes the "power" of the results obtained.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 6; They provide nice rates for approximating and learning functions with mixed or anisotropic smoothness. This is arguable and the authors might give more details. The setting with an infinite dimensional input space is a very interesting topic and is closely related to distribution regression. The rates of approximation and estimation by deep ReLU neural networks given in the paper are nice.<|endoftext|>The paper studies non parametric regression for functions defined on infinite dimensional input data (such as signals in $\ell^2$), using fully connected networks or dilated convolutional networks (in the CNN case, convolutional layers are followed by a fully connected network). The authors consider certain smoothness classes similar to mixed or anisotropic smoothness but extended to infinite dimensional input data, which requires per coordinate smoothness orders ($a_i$ in Definition 2) that are non decreasing or increasing with some rate. I am thus in favor of acceptance.<|endoftext|>The paper proves novel dimension independent bounds for both approximation and estimation by convolutional neural networks when the input is infinite dimensional and the target function has mixed and anisotropic smoothness. Moreover, the authors show the advantages of dilated convolution when the smoothness of the target function has a sparse structure. The paper investigated the approximation error and estimation error of convolutional neural networks in the function spaces with mixed smoothness and anisotropic smoothness. It was shown that the convergence rates are determined by the smoothness of the target functions and independent of input dimension under mild conditions, which theoretically supports the practical success of convolutional neural networks and provides insights on how to avoid the curse of dimensionality by deep learning methods.<|endoftext|>This paper studies the approximation and estimation errors of using neural networks (NNs) to fit functions on infinite dimensional inputs that admit smoothness constraints.
Reject; rating score: 5; rating score: 5; rating score: 5; In this paper, the authors apply adversarial attack techniques, such as FGSM and PGD, to “fool” the SOTA DNN and GNN methods for anomaly and intrusion detection in time series. They show that small perturbations to the input time series can lead to significant deterioration in the performance of the SOTA methods. The paper is well written. Cons:1.This paper is not the first paper to transfer adversarial attacks to anomaly detection models for time series.<|endoftext|>This paper shows adversarial attacks to time series anomaly and IDS systems, specifically focused on CAN (Controlled Area Network) dataset. The authors devise some criteria to select datasets and algorithms, and apply PGD/FGSM based attacks and show the brittleness of the detection systems. I feel the major issue is with the limited contributions and originality of the work, and the unclear focus on CAN only attacks, when the proposed methodology does not seem to tackle this domain specifically. Moreover, you claim to be the first tackling the problem of adversarial attacks within the context of intrusion detection. This may unwittingly be a problem of overclaiming: there has been several works in the past (e.g., [1 3]) which studied the problem of adversarial attacks in the context of network intrusion detection systems, which are a super set of your considered CAN based attacks. Hence, related to the prior point, it is unclear how much significant are the findings of this work. **Problem space**. "Evaluating the effectiveness of adversarial attacks against botnet detectors."<|endoftext|>According to the authors, the attacks targeted systems devoted to “anomaly and intrusion detection methods”. The references are not appropriate. Specifically, the claim that the paper is the first to investigate adversarial attacks against time series based ML systems in the context of “anomaly and intrusion detection” is valid. I also appreciated that the attacks were carried out on multiple state of the art systems trained on distinct datasets. After reading the paper I was not able to figure out if this is also the case for the considered attacks.
Reject; rating score: 1; rating score: 1; rating score: 3; rating score: 3; The main result of the paper, Theorem 1, is dependent on Lemma 2. If you are omitting the sigma over j, I suggest you not do that because it is just confusing. Is the sum i is over 1 to n and the sum j is also over 1 to n.   Is this correct? (this just a sanity check.)<|endoftext|>Moreover, the paper has sever issues regarding clarity and deviates from the classical literature in its clarity. The ferromagnetic model is not used consistently throughout the literature. That being said, the novelty of the current paper is limited and the findings only apply to a restricted setting (more precisely, Theorem 1 is analogous to Lemma 3.3 in [Koehler].The proofs are also analogous.<|endoftext|>Clarity:  The paper contains a number of typos, but they do not affect understanding. I found the presentation of the results to be counterintuitive and frustrating, with a number of definitions presented out of order.<|endoftext|>One weakness of this paper is that the latter half of the paper is mainly proofs based and it is a little bit hard to follow the intuitions of results. The authors study the Ising model on graphs with motifs.
Reject; rating score: 3; rating score: 3; rating score: 6; rating score: 6; rating score: 6; This paper introduces implicit quantile networks (IQN) into value decomposition multi agent reinforcement learning methods (QPLEX or weighted QMIX). The proposed method uses IQN for two components of QPLEX, for the joint action value function and the transformed action value function, respectively. To be specific, the reviewer is not convinced that this method can disentangle the environment  and agent wise risk. a) Why does $w_{env}$ reflect environment wise risk? Conditioning the joint action value function on $w_{env}$ is not enough. ***Evaluation***a) The reviewer can not tell whether the results are cherry picked.<|endoftext|>This paper describes a few different loss functions and heuristics to usedistributional RL in Dec POMDPs in order to obtain risk sensitive policies. The paper describes a few different loss functions that try to combinedistributional RL with decentralised reinforcement learning inPOMDPs. As such, thepaper is purely experimental. Theauthors regard them as risk seeking, while this is not explicit in thecited papers. The authors mentionthe sign of the TD error as a way to obtain environment risk, but theydo not show that this is a reasonable idea. So, viewing everything as purely heuristic, let us look at theexperimental results.<|endoftext|>The authors introduce DRIMA   a distributional CTDE multi agent RL approach that separately learns to model return stochasticity arising from other agents vs the environment. As is, I found the results interesting and convincing enough to be just over the bar for acceptance. For example, the authors find in Fig 6 that DRIMA seems to perform best when risk seeking in terms of both agent wise and environment wise risk.<|endoftext|>This paper proposes a novel multi agent reinforcement learning algorithm that disentangles randomness/risk sources coming from (i) unobservable actions of cooperative agents and (ii) unobservable actions of enemy agents (environment stochasticity). They significantly clarified many of my concerns for the theoretical justifications of the agent wise and environment wise "risk" parameters. The proposed method shows improved performance compared to other state of the art MARL algorithms in difficult StarCraft benchmark tasks. It is not clear what “objects” are, here.<|endoftext|>This work introduces DRIMA, a Multi Agent Reinforcement Learning algorithm that attempts to learn risk specific behaviors, where risk is separately considered by its source: from other agents or from the environment. One insight is that, in the hardest setting, risk seeking preferences are useful, both in terms of agent based and environment based risk. # Weaknesses  The methodology sections of the paper are very hard to follow. There is lots of room for improvement here. A general audience will not be able to understand the methodology. Is the agent wise utility simply the estimate of the joint Q value given the observations available to the agent?
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; The paper proposes a method for controlling unfairness of a model when the test distribution may differ in the marginal distribution of a feature such as gender, race. It derives a statistical test for checking unfairness on the unknown test distribution via importance weighting combined with user input on extent of the shift. ## Related workThere have been multiple studies on fairness guarantees under shifts which have been omitted. The work provides a conceptually clear and flexible framework for learning fair classifiers under demographic shifts. In total, the work is a technically strong contribution to the nascent literature on the problem. One strategy is similar to the proposed fairness test procedure to note that loss conditional on T remains the same and somehow importance weighting or bounding the loss.<|endoftext|>The paper focuses on algorithmic fairness in machine learning (ML) and in particular on the problem of demographic shift. To account for demographic shift, the model further includes a “demographic attribute” T; the distribution of T can change over time, but the conditional distribution must remain the same (by assumption). Given this setup, the paper introduces Shifty, an ML algorithm that provides high confidence guarantees that a fairness property will hold even when a demographic shift has occurred. Then, it partitions the dataset to two subsets, finds a model based on the first dataset. That said, I think the conceptual contribution of the paper is still quite new and useful.<|endoftext|>The paper presents a class of algorithms for ensuring fairness guarantees when the deployment data is susceptible to a demographic shift, termed as a marginal shift in demographic attributes such as gender or race, in comparison to the training data. Results are provided for two settings, 1) when the exact demographic shift in the deployment setting is known and 2) when the demographic shift in the deployment environment is unknown. The empirical analysis is performed on the university dataset to predict GPA using scores, with gender as the fairness attribute and student s race as the demographic attribute, the marginal distribution of which changes across the training and deployment settings. The contributions of the paper are clear.<|endoftext|>The authors propose an approach called SHIFTY that provides high confidence fairness guarantees when the distribution of training and deployment is different. Their method of identifying the distribution shift in both known and unknown shifts relies on user provided information. The paper tackles an interesting and challenging problem. Indeed, it is an assumption to the problem which may hold untrue.
Reject; rating score: 3; rating score: 3; rating score: 6; rating score: 6; The authors claim that the usefulness of the learning components (e.g., contrastive objective) proposed in previous representation learning for RL, is highly dependent on the specifics of the task category, and thus show that they don t provide robustness across different and diverse task categories. They also claim that the reward and transition prediction are the most vital (and minimal) components providing robustness. Based on this observation, they propose to evaluate the representation learning methods by considering the specific properties of task categories instead of evaluating on a whole benchmark level such as DMC and Atari. Strengths   The paper performed various experiments analyzing the contribution of each component used in representation learning for RL. The results suggest, which I agree, that we need to analyze based on the categories of the tasks rather than relying on the average or overall performance of the benchmark. Weaknesses  One of the main claim of the paper is that reward and temporal prediction are vital components providing the most robust performance. I’m not convinced if this can be taken as a general finding about representation learning in RL. Specifically, the authors use 8 categories. The proposed baseline seems to work for these categories. However, the result is highly dependent on how the 8 categories are picked because there exist a much larger number of categories in the space of tasks and we don’t know whether the baseline would be robust generally and globally in this space. For example, the reward prediction may not work well for sparse reward tasks (which is currently considered only in one category out of 8). While I found some experiment results are interesting (e.g., cropping ), many of the findings from the paper were actually not surprising rather somewhat already known or expected. Also, the fact that pixel reconstruction does not work well on image observations with distractors has been shown in many previous works (e.g., TPC [1] and TIA and the related works of these papers). For Value Aware Learning, the paper does not discuss much about the result. It would be more interesting if the work also embraces broader classes (e.g., other categories, unsupervised RL, etc.) Also, many findings of the paper are quite what can be expected and not much surprising.<|endoftext|>The authors further consider many components proposed in previous contributions on pixel based control (such as contrastive learning, data augmentation, image reconstruction, etc) and conduct ablation studies on top of the proposed baseline approach. Strengths* The authors presented large scale empirical evaluations and ablation studies to analyze various components for pixel based control with distractors. Overall, there are good values in the large scale studies presented in this paper. However, I m not convinced by conclusions as the authors try to generalize behavior of specific implementation to a family of methods. If I were to implement a new agent, I don t feel like I can believe these conclusions so that makes me question what knowledge this paper can add to the community. Furthermore, many details are either missing or not made clear, and the main story isn t very strong. Therefore, I don t think this paper is ready for publication in the current status. [b] presents an even more complicated implementation of contrastive learning. It s not clear to me whether it s with distractors or not. * What does "non contrastive loss" even mean?<|endoftext|>This paper presents an approach for learning representations from pixel data that are amenable for control tasks. The proposed approach is a simple baseline that does not require data augmentation, world models, contrastive losses etc. but only contains two simple sub tasks that are supposed to contribute heavily towards an effective representation: reward prediction and state transition prediction. Along with evaluating this proposed baseline, the paper also compares it to several prior works on representation learning: i.e., several approaches such as data augmentation, distance metric losses, contrastive losses, relevant reconstruction etc. It is shown that the proposed simple baseline either outperforms several of these methods or at least is very close in performance. Finally, the paper presents an interesting discussion about how evaluating an algorithm is not just about the dataset and the chosen benchmark task, but requires a more nuanced point of view of several factors such as reward sparsity, action continuity/discreteness, relevance and irrelevance of features to the task, and so on. The findings of the paper are not just about the effectiveness of the proposed method, but a more overarching view of which types of representation learning methods work in what conditions.<|endoftext|>As is often the case for these types of works, some prescriptive claims made are not fully supported by experimental evidence (e.g Section 4.2). This work investigates representations for pixel based reinforcement learning. Some of the results show evidence for aspects of representation learning in RL that could be of general interest (though they are not all novel in themselves, only summarized together). I am also of the opinion that not including other baselines using reward prediction does not inspire confidence that this approach to its incorporation is any better than the others that exist. I am willing to reconsider my current evaluation. It is nice to see that for the most part the results confirm existing "lore" e.g.world model reconstruction methods work well without distractors but perform significantly worse with distractors, getting closer to the proposed baseline (difference between C4 and C8)* Data augmentation is used to learn invariances with respect to non informative (for the task) transformations of the input. This is well known in the representation learning literature and I appreciate the investigation of these properties in the context of RL (discussion in Section 5). * I agree in principle about the potential utility of predicting rewards to improve the quality of transition model representations, and am happy to see SAC AE with reward transitions performing on par with more involved baselines. [Weaknesses]* It unclear if the main contribution of this work is to introduce an improved variant of the SAC AE baseline (with reward and transition prediction) to improve robustness against distractors or instead investigate the performance of different approaches to RL in tasks with different characteristics. * To elaborate on above: the paper suffers from tension between attempting to highlight how claiming superiority of a method across all environments is misleading, while at the same time spending much of its time arguing in favor of a specific method that is supposed to "learn meaningful representations" (which is never shown). In principle, I understand the choice of CDPs as the focus is on distractors. Are there other motivations behind this choice? The Section could benefit from providing a summary of experiments done in support of each claim. * The paper is missing a description of what types of distractors are considered and how they have been implemented. Are distractors kept the same (once chosen) across a given training dataset? If not, i.e you sample distractors, why are you treating them any different than data augmentation (that is guaranteed to retain task information)?
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; The authors formulated the Fenchel dual problems to maximum likelihood and score matching training applied to energy based models defined by shallow neural networks. The authors then proposed practical algorithms based on the Hahn decomposition and compared this to various predecessor algorithms. An interesting part is that the learning dynamics can interpolate between maximum likelihood and score matching. There are also rich results in the Appendix. This is a theoretically rich paper. I personally find this paper hard to follow, with subtle unclear statements and jumps at various places. If these are stated in the abstract, they need to be clearly specified in the main. Statements like this make readers second guess what the authors want to convey. [The ref of this highly related paper indicates that the paper is not yet published, but it is actually accepted in May and published in July 2021, well before the current submission deadline.] Is this related to the TV norm? In particular, no experiments try to verify the Proposition 3, one of the more interesting theoretical results in my opinion.<|endoftext|>The author claims that this new training method has a quicker convergence rate than the primary problem. But the key question I find myself asking is what the practical benefit of the proposed method is compared with scoring matching, and what difficulty it bypasses in the original MLE problem. The author does not explain too much on why this dual problem is necessarily better than the primary problem other than saying the latter can be metastable. Yet even it is much better than the primary problem, why is it better than the score matching (4), in which there is no need to compute  $Z_{\beta}$ at all? The structure of the paper is fine. The proposed method is novel and promising for some useful applications. The paper is tightly presented although some reorganization can make the main contribution clearer. However, the theory justification, comparisons to the benchmark, and empirical evaluations are weak.<|endoftext|>The “sentiment” I have is that the paper in its current form is not finished, because there is a disconnect between the theory and the empirical findings. What guarantees do we have that the alpha>>1 would be better also in other settings? Minor: 1) It is not clear why the discussion about the two different F_1 and F_2 spaces is important for the reader. Section 5 explores the implementation of Algorithm 1 on a synthetic dataset. Comparison of the primal and dual (alpha<<1, alpha>>1) shows that the dual performs much better than the primal. The connection on a continuum of maximum likelihood and score based training is very interesting from a theoretical point of view, and I think this is a novel and valuable contribution. My biggest concerns about the work are the following: 1) In the introduction it is stated that score matching is a weaker loss than maximum likelihood or, in other words, that score matching falls short in statistical power. However, the empirical evaluation in the paper shows that alpha>>1 (Score Matching) is superior to alpha<<1 (Maximum Likelihood). How do the authors explain this phenomenon?<|endoftext|>This paper derives a Fenchel duality formulation of the maximum likelihood loss of F1 EBMs, which turns the optimization into a min max problem on probability measures over the sample space. Using the dual aogrithm they also draw a connection between maximum likelihood training and score matching. The author introduces the motivation and derivation in detail. * Numerical experiments seem to show that it is a promising alternative to train EBMs. Weaknesses:* I would say that the notation and formulation are a bit heavy and not familiar to the ML community, which makes the paper really hard to follow. * The paper empirically shows that the dual algorithm trains the EBM faster. However, I would like to see more theoretical analysis in terms of the convergence speed. I would like to see at least one example of applying the proposed algorithm to a realistic dataset (e.g., MNIST). Overall speaking, this is a decent and novel paper analyzing the training algorithm of EBMs from the perspective of probability measure. The paper could be strengthened if more theoretical analysis of convergence speed and more empirical results are included.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 8; The authors propose an approach for goal conditioned reinforcement learning via self imitation. The paper first outlines a meta algorithm which defines conditions under which an instantiation of the algorithm constitutes a valid policy improvement operator. Based on this alone, the paper is not fit for publication in its current state. I find the experimental evaluation to be unconvincing: first, HER can be significantly improved by using a more capable reinforcement learning algorithm and the sample efficiency gains shown in the plots may be due to this alone. Similarly, it is unclear whether the method is still improving or converging on FetchSlide; final performance on this task should be significantly higher. The paper is suffering from a large number of grammatical as well as spelling mistakes that make it very difficult to follow.<|endoftext|>This paper presents a self imitation learning method for goal conditioned continuous control tasks. Finally, the parameters are updated with behavior cloning to the relabeled data. The SELECT requires resetting the environment to a given previous state, which can only be achieved in simulator but not real world. But Go Explore is not compared. 4.It is unclear to me why it is called "meta algorithm". 5.The experiments are weak. The role of $\delta$ distilled policy is unclear. 6.It is unclear how the algorithm improves the quality of "the timesteps taken to achieve the goal". However, it heavily relies on the ability of reseting the environment to a previous state, which is unfair to the baselines.<|endoftext|>By ensuring that the target policy reaches goals more optimally than the behavior policy, the paper shows that under certain conditions, performance improvement can be guaranteed. Strengths:  I appreciate the theoretical analysis of the method, as well as the step by step walk through of the algorithm in section 3.1. The paper claims that SPLID does parameter space exploration, but they simply add noise to the actions of the previous policy, which is not parameter space exploration. There is no discussion on the assumptions (deterministic, resettable environment) that SPLID makes in relation to the baselines. While this paper seems to get strong performance in the selected experiments, I find the experiments to be a bit suspicious and under explored and I am unconvinced that the performance gained with this method are worth the strong assumptions (env is resettable to a selected state).<|endoftext|>The authors demonstrate that their policy distillation step improves the state of the art on 3 RL baseline settings, and also give theoretical backing to their approach and conditions for convergence. Authors give convincing theoretical backing to their approach, as well as conditions under which their key results hold. The authors state that this is due to the difficulty of the domains   perhaps then, the authors should include some experimental domains where GCSL performs at least better than trivially poor, so that it does not give the feeling that these set of experimental domains were somehow biased toward the proposed method. I recommend that the authors move some of the experiments from appendix E to the main text, since it is more convincing that domains were balanced for the competitor as well as the proposed method. I hope the authors can either explain this in greater detail or include a domain in the final version where the state of the art is at least able to learn something.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; rating score: 10; This paper proposes a bucketing scheme for robust distributed learning with heterogeneous data. Gradients are more homogeneous after bucketing, which will increase the robustness of existing algorithms. The author provides theoretical analysis as well as empirical results. ## StrengthsThe bucketing scheme proposed in this paper is interesting. It is easy to adopt and does not bring much extra computational cost, but can improve the performance of existing Byzantine resilient algorithms, as shown in the empirical results. Thus, the number of Byzantine workers that can be tolerated will decrease to $1/s$ when adopting bucketing. (Post rebuttal)My major concerns have been properly addressed by the authors and the quality of this paper has been improved after revision.<|endoftext|>Providing a simple randomized bucketing scheme. The authors also theoretically study optimization methods that make use of aggregation methods (via stochastic gradient descent). In the regime where these terms are non zero, convergence is not guaranteed, but via information theoretic methods, the authors also provide a matching lower bound If heterogeneity bounds are more refined (whereby gradient variation is bounded by the order global gradient of the entire dataset), then the authors demonstrate convergence of robust aggregation methods on SGD. The Rademacher distribution is a good starting point to justify the analysis, but seeing the performance of existing aggregators on more complicated data sets when \delta 0 would be interesting. This seems like a fitting extension of existing work, and the authors have provided a corresponding framework for quantifying performance loss in the presence of byzantine agents and heterogeneous data for federated learning.<|endoftext|>In this paper, the authors consider Byzantine robustness of a distributed learning system in the setting of non iid data distribution. These two techniques can be combined with various robust aggregation rules. The authors prove the convergence of the combined methods, and show that they reach the lower bound. 2.In Page 2, the authors claim that “none of these (non iid Byzantine robust) methods are applicable to the standard federated learning.” Please justify this claim. 4.Also about the numerical experiments, the MNIST dataset is too simple. The authors should indicate its contribution to the performance improvement. But centered clipping has certain robustness in the non iid setting. Please check whether this condition can be satisfied in the numerical experiments.<|endoftext|>## Questions and comments about the proofs1. 2.**Page 2, "However, none of these methods are applicable to the standard federated learning. This is crucial for the correctness of the lower bound. 3.**Page 20, robustness of Krum. ** The authors prove new complexity bounds for Byzantine robust optimization under bounded heterogeneity in the non convex case. 3.**Bucketing as a tool to make Krum, Geometric Median, and Coordinate wise Median robust. This work fixes this drawback of the mentioned aggregation rules (Theorem I) via a simple tool called bucketing. ** In view of my previous comment, this requirement may imply that $\delta$ is tiny. I think the discussion of this requirement should be added to the paper. 2.**Comparison with related work. The second formula on page 24 is also inaccurate: the RHS should have $\zeta^2(1   (1 \alpha)^t)$.
Reject; rating score: 1; rating score: 3; rating score: 5; Writing       The authors did not introduce many concepts that are essential to their theoretical analysis. However, that argument was not made in the paper.<|endoftext|>I am however not sure if the proposed algorithm would be successful in learning invariant representations. The toy example described above shows that the proposed algorithm should not be able to learn invariant representations. I’m willing to increase my score should the authors address the above mentioned issue.<|endoftext|>The paper is well written. Given that the empirical results are not very strong, I cannot recommend an acceptance.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; The paper propose the Federated Neural Architecture Search (FedNAS) method to search for both global model and personalized model architectures collaboratively among edge devices and show its performance in a variety of federated learning  settings. 2.It also investigate the role of NAS to address the challenge of data heterogeneity in federated learning and show superiority in results as well as its efficacy. It was not clear the purpose of Figure 4 (1) given that the graph shows uniform local training dataset size. In related works, authors lists our some work in NAS, such as Kairouz et al,, 2019, Zhu & Jin 2020, etc.. The paper would be stronger to also benchmark against related methods in NAS. Overall technical seems significant and somewhat new, but as mentioned above in the weakness, the paper is missing some comparison against stronger baselines, especially for previous methods in NAS.<|endoftext|>The authors employ an existing neural architecture search method in the federated learning setting. Specifically, the authors propose FeNAS and extend an existing NAS method MiLeNAS into federated learning to address the data heterogeneity problem and conduct personalization. The experiments show that the proposed method is able to achieve improvement compared to some other federated learning methods. The personalized architecture shows good improvements. Originality: This work is a simple extension of MiLeNAS on federated learning. The NAS on clients for personalization seems different from other NAS works. But I did not see any challenge highlighted by authors and they directly adopt NAS methods for common scenarios. 2.Quality: The aggregation on the central server follows the similar way of FedAvg. The most difference with other existing works is to introduce NAS into federated learning. Even though MiLeNAS is a recent effective method, other NAS methods should be introduced into federated learning to justify the authors  choice. 3.Soundness: One of the most important parts of federated learning especially in new settings is convergence guarantee. The architectures on clients keep changing and personalization is needed, resulting in more heterogeneous model weights and architectures. How to guarantee the convergence of the proposed framework? The authors extend an existing NAS method MiLeNAS on federated personalization. However, the novelty of the work is somewhat limited. It is lacking clear support evidence and insights to support why MiLeNAS was chosen. To support the idea of NAS for federated learning, more other NAS methods are preferred to provide more insights and depict the challenges of this problem.<|endoftext|>This paper proposes a method for automatic model design in the context of federated learning and introduces an auto ml system under the framework of federated learning. The motivation is not clear. and the introduction says "We aim to address data heterogeneity in FL via ... NAS."For me, it is not clear whether the paper aims to automate model design in FL or solve the non IID problem. 2.NAS method and FL are naively combined, which undermines the novelty of this paper. Model searched by NAS are generally smaller than handcrafted ones while enjoying higher accuracy. In comparison, I think a DARTS model should be used as a base for previous FL methods.<|endoftext|>This paper combines gradient based NAS (DARTS like algorithm: MileNAS) with Federated Learning (FL) setup, to improve both global and personalization performance with learned neural architecture. Since both NAS and FL learning are based on gradient, the extension to FL setup becomes intuitive and effective. Empirically FedNAS shows improved performance comparing to existing FL methods. The global model search with FedNAS is better is not surprising, since there is no difference between training on FL and centralized settings, on only gradients. What becomes really interesting, is the personalization performance. Contributions:The paper s work is a natural extension of MileNAS. Since MileNAS, like DARTS, is gradient based algorithm, then make it available to Federated Learning is natural. In this sense, the innovation is not extraordinary. How to extend NAS to NLP tasks or other paradigms, could be an interesting next step. (3) Interpretation of the learned architecture. The author confirms that the personalized performance does improve, but the interpretation is missing. It could be interesting, to see what if some clients learns less/more complex models. Overall, the proposed method to use gradient based NAS for FL to improve personalization, shows good empirical performance. There are very few modification on original method, other than the settings are different. In all, I give this paper score 6, and I am willing to discuss with other reviewers and the authors in later rebuttal session.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; It shows that these factors can indeed be used to learn a latent space with disentangled factors of variation which can be used via an MPC based planning method for solving downstream tasks in a OpenAI Gym ant environment. ## Strengths  The related work section provides a comprehensive overview of relevant prior works. The provided planning visualizations clearly show the differences between the learned skill spaces and how they influence downstream learning. Thus, I believe the problem formulation of the paper is fundamentally flawed. Currently it only vaguely mentions that a lack of disentanglement and interpretability of skills "limits their applicability".<|endoftext|>The experiments also lack a comparison to plain Soft Actor Critic. The skill policy is trained to match the trajectories of the predictive model, and the predictive model is then used for MPC. The writing would benefit from an overhaul wrt grammar. In my view, the experiments in the paper fail to justify the complexity of the method that was introduced.<|endoftext|>The paper proposes a method that learns disentangled skill representations, and shows qualitative & quantitative results on Mujoco Ant navigation. Strengths:* Interesting visualizations of learned interpretable skills on the Ant navigation task. 1a) How does the dataset size & quality affect the quality of the learned disentanglement, and performance of the learned policy? (2) The idea of using weakly supervised disentangled representations in order to learn an “interpretable” policy behavior, and comparing against unsupervised representation learning methods such as VAE, has also already been studied in a prior work [1], which was not discussed/cited in the paper. [1] “Weakly Supervised RL for Controllable Behavior”, NeurIPS 2020 https://arxiv.org/abs/2004.02860(3) In general, I felt that the writing & presentation could be improved.<|endoftext|>Then, a skill based policy (i.e.a policy conditioned on the learned sequence representation) is trained to imitate the learned model via soft actor critic. The authors show empirically both the disentanglement of WET VAE and the advantage of their hierarchical approach. My main concerns with the work are its lack of clarity and how many decisions related to the design of the algorithm are not well justified. As a non expert, I found the paper hard to digest. Then, one could use this encoder trained with supervision to train a generative model conditioned on these. This connects to the following points related to the MPC approach. An ablation study would be convincing.
Reject; rating score: 1; rating score: 3; rating score: 3; rating score: 3; rating score: 6; The authors attempt to view PPO as a Chinese Restaurant Process, and although this link has little relevance to their empirical study, various differences with and without the use of shaped rewards are presented from experience in a single, two dimensional navigation domain. However, I think the current manuscript has too many fundamental issues that prevent it from making progress on this problem. 2.It is unclear whether the presented data contains a positive result that would make a significant contribution to research community. 3.The manuscript does not contain enough details to reproduce its results. * What purpose does the Chinese Restaurant Process serve in this work? * How are the experiments supportive of the general claims about reward shaping and prior information that you wish to make? But given the major issues it currently has, I will not reccomend for this paper for acceptance.<|endoftext|>The work uses a new simple navigation task for the study. And the action and state space for both are different. The analysis and the results obtained with the agent’s entropy and behavior differences can be said for RL problems in general and not just to emergent language learning. I am not familiar if such analysis has been performed on RL in general. The related work section also doesn t include literature on analyzing reward shaping [1] . The validity of CRP as a way to explain the behaviors is well motivated and supported in the paper. Since the experiments are performed in a simple environment, experiments on a complex task should make that paper stronger.<|endoftext|>This paper investigates emergent language research, whose goal is to study language as it emerges from the inherent properties of the environment, language, and agents. This is typically in the context of reinforcement learning such that the phenomena arises from the pressure to maximize reward. StrengthsThe key contribution of the paper is the findings on the impacts of shaped rewards to the the emergent language:1. shaped rewards can explicitly bias the semantics of the learned language;2. shaped rewards change the entropy of the learned language;3. shaped rewards masks the potential effects of other environmental variables of interest. We offer one possible explanation for this effect in Section 5.4. The current results can be summarized in a workshop paper.<|endoftext|>Does this mean the teacher does not use the other units to communicate, or do the other units not have interpretable effects on the receiver s trajectory? Or rather, are all 6 sea star plots responses to different units in a single language? The authors show that, for a simple multi agent navigation environment, adding shaped rewards (i.e.auxiliary signals) to the environment bias the emergent language. The relation between section 5.4 and the rest of the paper could also be made more clear—this investigation seems completely tangential to the question of how shaped rewards bias the environment, and the experiment is not mentioned in abstract. In any experiment I could likely find some metric for the resulting language that changes wildly if I modify training hyperparam or environment reward. It s not clear to me how the confounding factors in this environment are teaching us something new about how to approach emergent communication studies across all environments.<|endoftext|>The paper showed that shaping the rewards can lead to higher entropy than training with just the base reward. We care about the entropy because this relates to how expressive the language is. There are a bunch of typos /repeated words, such as  allow allow , but I felt the structure and the paper was good, the pacing worked well for me, the explanations were clear. The work seemed rather complete to me. maybe a way of phrasing trivial in the other way around. ( non trivial ?:) )Like the paper  use of PPO in emergent communication is new  empirical exploration of effect of reward shaping on the entropy of the emergent language is new  theoretical analysis of why the reward shaping affects the entropy of the emergent language is new
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 8; This paper proposes to solve the continuous Optimal Mass Transport problem as a convex mapping function estimation problem based on Brenier s theorem using input convex neural networks. The proposed method is compared with other DNN based methods in the ubiquitous density estimation and generative modeling tasks in statistics, and good results are obtained. (If there is no evidence, it would be better to specify the possibility that it is not optimized?That would be positively evaluated as a scientific paper, as it would present the limits of verification). The proposed method in this paper has the potential to be a very useful method for solving the continuous optimal transport problem using DNNs.<|endoftext|>This refers to the Brenier s theorem concerning that a unique optimal transport map exists which is given by the gradient of a convex function. This consequently leads to the Monge Ampere equation that help formulate the loss function in the proposed work. The paper is in general well written and the idea is clearly presented.<|endoftext|>Strength: + The probabilistic universal approximation framework can be much more efficient than the finite element or difference methods which is known to have exponential scaling with respect to dimensions. The main high level idea is that solving optimal transport maps under some conditions can be posed as a PDE. However, many technical details regarding the algorithm are missing   (i) what is the overall procedure? It seems like some of these questions regarding how and what PDE is exactly being solved can be answered by reading various other papers cited in the experiments, but it is not clear from the submission. However, the main task density estimation which the paper mentions quite in detail about in the beginning of the paper is only minimally experimented.<|endoftext|>The paper combines a range of interesting ideas to solve density estimation problems. In addition, the authors use input convex neural networks (ICNNs) to  incorporate the Brenier potential into the PINN. I not aware of another paper that uses PINNs for density estimation. * _Significance:_ This paper is of interest for two communities within ML. First, it is of interest for the PINN community and the paper has the potential to motivate future research in the intersection of PINNs and density estimation. Does Brenier s Theorem apply to other cost functions? Can you provide theory to back this claim up, or is this only an educated guess?
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; The authors also extend SRG to combine the benefits of both importance sampling based preconditioning and variance reduction. The convergence rate is studied and the authors show that the proposed method can achieve a better asymptotic error than SGD. There is a rich literature in the domain for SGD based on importance sampling and conditional variances approximation. I have some concerns as follows. However in the proof, the impact of $q_k$ is not involved, and $p_k$ seems to be treated as a constant. I just have one question on the technical soundness of the convergence results.<|endoftext|>It would be more practical to analyze the method for non convex settings to see the effectiveness of the method in practice for training NNs. It would be useful to apply this method to NN training. 3  For the experiments, it is not clear how many times each experiment has been run and if the result is the average of those. The major comment is that the analysis is for very limiting settings.<|endoftext|>This paper uses importance sampling in SGD to improve performance. The importance sampling procedure is new, however there have been many similar works in the past and this paper s results maybe very incremental.<|endoftext|>SRG improves on the of stochastic gradient descent error for the strongly convex and smooth objective functions. The authors extended  also SRG to SRG+ to combine the importance sampling based preconditioning and variance reduction. I would like to see more applications and experiments than those in the paper. Should I try to use this? What is the sensitivity of your methods to theta?
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 6; This paper doesn t make clear claims. This study trains several supervised and unsupervised, and one reinforcement, tasks with the same data and ResNet architecture. The paper speculates that this might be due to a one image per neuron representation emerging even in early layers. I enjoyed a lot about this paper. Examples:  As I said, much of the attention is on RL, but there s only one RL objective and several supervised and unsupervised objectives.<|endoftext|>The paper includes a comparison of the representations learned according to a variety of training objectives, including reinforcement, supervised and unsupervised objectives, in a common architecture and with similar training data. It s not clear what question this characterization helps to answer or why this analysis is most appropriate. Either way, I recommend rejection.<|endoftext|>They are all trained on the same images from the RL task, and the supervised tasks are related to the RL task (e.g., what task is this image from). Minor points/typos:Figure 2A is not referred to in the text. This paper does a nice job of comparing representations learned by an identical network architecture, given different training objectives. The results are interesting, but not completely unexpected. This is one result that perhaps could be analyzed more closely as to why it happens.<|endoftext|>The paper explores how similar the representations learned by supervised, unsupervised, and RL techniques are in an egocentric virtual rodent environment. pros:* comparing representations of networks trained with different objectives is a very timely question, and I am frankly really glad someone is working on this direction. Because of that, supervised models are not trained in the more common object classification paradigm, but instead to e.g.predict reward and classify tasks.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; I think this approach could potentially be useful, but I think the current set of experiments don t adequately demonstrate that it poses much of an advantage or will be scalable in a practical manner, for a number of reasons:  The central assumption of the proposed approach is that we have access to a groundtruth algorithm that governs the interaction of entities in the target domain. Given that one has access to such a groundtruth algorithm, wouldn t it be more straightforward to simply use supervised learning to learn representations of the naturalistic inputs of the form that the algorithm expects, and then directly use the algorithm, rather than a learned proxy for it? The paper also suggests that this approach may be impractical because of imperfections in the groundtruth algorithm (or the information available to it). The primary contributions emphasized in the conclusion are that the proposed approach protects against bottlenecks, and that it is more data efficient, but as far as I can tell there is no systematic evaluation of either of these claims. This is an important comparison, but the authors should also clarify how these results compare to the current state of the art for the metrics that are evaluated. One exception is the comparison with embeddings from Agent57, but, as the authors note, it s unsurprising that these embeddings don t work very well given that they were learned for the purpose of performing a different task, so that particularly baseline seems a bit contrived. Other considerations:  It s not very easy to resolve what s going on in Figure 3. Can Figure 4 be made quantitative rather than qualitative, e.g.by representing the performance of RMR as a percentage of the baseline performance? The qualitative clustering analyses are kind of cool, but what do we learn about the proposed approach from these analyses? There are also some additional experiments that need to be performed to evaluate some of the paper s primary claims.<|endoftext|>This paper presents a learning paradigm referred to as "Reasoning Modulated Representation" (RMR). The main argument is that it first learns a latent space processor for abstract simplification of a problem, then use the processor for any tasks that can be modeled with algorithmic reasoning. In addition, it is possible that the latent space processor can be re used to tasks that do not even directly align with the abstract environment. **Strengths**:\The paper is well presented, and explores an important and exciting territory. (For example, the representation of sentences, speech, or images for object detection)\The key difficulty of representation learning is to learn internal representations for natural inputs (images here). However, the authors pre define the abstract inputs based on their knowledge. And $P$ is learned easily. 2.I also found tasks being evaluated are still toy tasks. I’d suggest test on more realistic tasks, such as visual question answering. 3.There isn’t significant improvement compared to an end end model (The difference might not be even distinguishable in the reconstruction rollout..); while RMR involves much more complicated modules. 4.Not weakness, but a question for style/typo: In Section 5 **natural pipeline**, is there a typo in the function? The right hand side of equation should be $\bf{y}$, not $\tilde{f}(\bf y)$? And I do not recommend acceptance of the paper in its current form.<|endoftext|>This paper proposes a training method based on ideas on neural algorithmic reasoning. This work finds that inserting the pre trained $P$ into a model that processes raw input data leads to improved performance compared to end to end training of a model on raw input data. One take home from your results is that training $P$ is best done with lower dimensional abstract representations than the high dimensional raw input. This paper is a somewhat unusual submission. The methodological contribution is minimal (the approach is easily described in words). Instead, the main contribution of this work is to spell out a philosophy on how to combine deep learning and classical algorithms. It is for this reason that I wouldn’t be surprised if the paper receives some negative reviews. The key requirement in order to use this method, that there are problems where a cheap abstract simulator is available, is fairly reasonable. The approach is simple and general, and could be replicated for other related problems. Good empirical performance compared to C SWM. Proper treatment of statistical significance. **Questions and suggestions:**  One of the central ideas in this work (also present in a couple of prior works) is that of a dimensionality bottleneck if one were to fix the reasoning module $P$ to have the same low dimensionality as the abstract inputs (size, position etc.). While this is an intuitive point, and somewhat observed already, it is not yet a widely known thing. I fully expect RMR to do better, but I would be interested in seeing the delta so I can understand how much of the work is being done by the avoidance of a dimensionality bottleneck.<|endoftext|>This paper contributes an implementation of the neural algorithmic reasoning blueprint outlined in Velickovic & Blundell (2021). The setting that is considered is one where, for a given complex reasoning task that one is interested in solving, there is access to information about the underlying system that should be incorporated to simplify learning. * When training the processor for the contrastive learning task, how is it ensured that the transformation of interest is performed by the processor and not the encoder or decoder (which are discarded afterward)? This paper contributes an implementation of an interesting approach to incorporating prior knowledge about a problem in neural networks that was outlined previously. I have suggested a number of concrete improvements that can be made, which would have me reconsider the current score. The implementation considered here (Reasoning Modulated Representations), is evaluated on two environments: the bouncing balls environment, and various Atari games. On bouncing balls it is shown how RMR can transfer learned representations from trajectories to videos. On Atari, RMR is trained on transitions of RAM states to improve predicting latent space transitions from pixels. Compared to C SWM (Kipf et al., 2019), it is shown how RMR leads to better representations in several games, and it is shown how processors may transfer across games. This paper is concerned with the important problem of incorporating prior knowledge about relevant knowledge for solving a task in neural networks. At the same time, the implementation of this idea is by no means trivial and overall the technical contribution is sufficient in my view. The idea of training a neural processor in the way outlined in this paper is appealing, and the possibility of using it for pixel level prediction (eg.for bouncing balls) as well as for representation learning (eg.atari) is appealing. A stronger baseline that also incorporates assumptions about the underlying state of the world is desirable. In particular, a comparison to a pre trained IN as a processor would reveal how much knowledge about the underlying task is distilled.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; To the best of my knowledge, the claims made in the paper are supported by the empirical results shown. **Update after authors  response:** After reading the other reviews, the updated manuscript and the authors response I think the authors have made several small improvements based on the reviewers  suggestions. **Summary**: The paper proposes a novel method for lifelong compositional RL. The paper proposes to use a modular neural architecture (consisting of layers of modules) to capture the compositional nature of the tasks to learn. Experiments are shown on a set of 2D grid world tasks, and a number of simulated robot arms with different sensory setups, and the method performs well against a number of baseline methods. The paper aims at addressing a very timely and important, and notoriously difficult problem: lifelong RL with task families that have compositional structure. The main innovation is a training procedure to train such an architecture. I am not saying this is trivial, and I would consider it a significant addition to the paper. The paper is well written, and clear, results are good, and I have no complaints regarding the correctness of results and claims.<|endoftext|>The paper introduces an algorithm for lifelong reinforcement learning using functional neural composition. This algorithm is then evaluated on two domains, one multi task lifelong gridworld domain and a multi task lifelong robotics domain. I very much agree with the authors  goals: lifelong reinforcement learning is an extremely interesting and underexplored area. I also agree that compositional modularity is an extremely promising direction that merits further study by itself. One example is that the number of permutations is aligned with the number of modules. Strengths:  good research direction  paper is extremely well written  the core of the paper, the introduced algorithm, is a well engineered solution to the problemWeaknesses:  all details relevant to contextualize both the approach and the results are in the appendix  the paper jointly introduces a domain and an approach, opening it to the critic that the experiments are hand crafted towards the approach  the experiments seem too simple  I disagree with some assumptions behind compositionality<|endoftext|>Indeed, the authors  compositional gridworld task is reminiscent of the compositional task in this paper, which is in turn based off of the compositional "grid sailing" task in Fermin et al, 2010 (https://www.tandfonline.com/doi/full/10.1080/00222895.2010.526467). Overall, I would recommend this paper for acceptance. Overall, I found this to be an interesting paper. The idea of functional compositionality is rather intriguing, and its application to RL and continual learning is original. Moreover, as the authors have pointed out, the complexity of finding the right combination of modules can grow combinatorially with the number of possible modules, making it difficult to scale this to a larger number of modules. A final limitation is that while the framework shows good performance on compositional gridworld tasks, the empirical performance advantage seem weaker on higher dimensional robotic manipulation tasks. Again, I had initially understood the discreteSearch() to be discovering the graph in addition to the modules that should go in the graph node.<|endoftext|>This paper describes a method that is sensible and seems to work well. The tasks it is trained on? This seems like a very blunt instrument for addressing the problem with PPO. It s not a major contribution, but it is well described and the experiments seem good, and most importantly, I think it is heading in a very good direction. Lifelong learning and compositional generalization are critical aspects of moving forward to bigger and more interesting problems and so I think it is important to encourage work in this area.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; In this publication, the authors present an interesting framework for approximation the mapping between sequences by feedforward spiking neural networks (SNN), providing insights on the computational capabilities of feedforward SNNs for the approximation of the mapping of input to output spike trains. The performance of the approach is demonstrated by several spatiotemporal classification tasks, on datasets like DVS Gesture, N Caltech 101 and sequential MNIST. My main criticism is that the good theoretical results are not sufficiently well supported by numerical experiments, which could further consolidate the good theory results. I hope to provide a possible motivation for supplementing the results in my review.<|endoftext|>This paper presents some theoretical understanding of sequence approximation using feedforward SNNs. This paper takes a step in this direction. The motivation is interesting and significant. 3.Can the authors provide a formal description and proof for their theoretical results? I would suggest that the authors use rigorous proof to re write clearly what the conclusions of the theory are?<|endoftext|>The authors introduce a theory on how to learn arbitrary input spike train   output spike train mappings, using a feedforward SNN with heterogeneous neurons and skip connections. Isn t it equivalent to having a set of weights that is shared between neurons (like in a convolutional layer)? Pros:* very good accuracyCons:* many doubts need to be cleared about the theory* the connection between the theory and the experiments is unclearThe theory is unclear:* Does it only apply to sequences with constant inter spike intervals (ISI), as Fig 1a suggests?
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 8; This paper studied congested bandits, a special class of multi armed bandit problems where each arm s reward depends on the number of times it was played in the past few steps. The route recommendation problem may be understood from different perspectives. A UCRL type algorithm was then proposed and the regret bound of the algorithm was analyzed. The authors showed the reduction of the congested bandit problem to an MDP problem. It is shown that the congested bandit problem can be reduced to an MDP problem. However, the proposed model has almost no connection with the classic routing problem. Near optimal Regret Bounds for Reinforcement Learning. Theorem 2 then provides the regret bound of the proposed algorithm in routing problems. Although it could be understood as another type of "congestion", I see no reason it could be applied to the traffic routing problem. The congested bandit model is also extended the contextual version. My feeling in general is that it is not that easy to analyze the regret bounds, nevertheless, as contextual bandits and contextual MDP are all well studied topics, the technical novelty is still not significant. This setting cannot address the traffic routing problem which motivated the study. 2.Compared with existing bandit models which take history into consideration, the major difference in this paper is that it considers a short term reset, which is not a significant modification. 3.The technical contribution is not substantial, as the overall structure of the algorithm and the corresponding regret analysis follows directly from other papers with limited new insights.<|endoftext|>In this setting, the reward of an arm depends on the number of times this arm has been pulled during the past $\Delta$ time. The authors also extended the congested bandits to graph based congested bandits and congested linear contextual bandits. For congested linear contextual bandits, the regret upper bound is $\tilde{O}(\sqrt{dT})$. Strengths: This paper raises a new interesting problem, and proposed solid algorithms and derived theoretical analysis on the upper bounds. The results seem to be optimal from my point of view. 1.The problems of congested bandits in the paper seem trivial for me. For instance, in the congested bandits, there are actually $K\Delta$ numbers or expected rewards we need to learn from the environment. Thus, I do not see enough theoretical novelty from this paper. The other cases have the similar issue. 2.It is better to have results on the lower bounds, without which, it is hard to say the results of this paper are good enough. Thus, I tend to vote a reject.<|endoftext|>This paper proposes a new structured bandit model, called congested bandits, where an arm’s reward is a decaying function of how many times it has been played during a recent time window. The proposed bandit model aims to address recommendation problems such as route recommendation, in which recommended routes tend to get congested (hence yield lower rewards). Different from prior work on bandits with changing arm reward distributions, the current paper proposes a model in which the effect of congestion resets over Delta time steps. By viewing the problem as structured MDP, the paper develops a variant of UCRL2 that learns to recommend the optimal arm for each congestion state. It is shown that the proposed algorithm achieves a policy regret of tilde{O}(sqrt{KDeltaT}), which significantly improves upon the exponential dependence on the state space of UCRL2. The authors also propose a variant of their algorithm tailored for the linear stochastic contextual bandit setup and analyze the policy regret for this case as well. Weaknesses:A niche problem formulation. The current formulation does not truly capture congestion models for traffic routing platforms or other practical routing problems. No lower bounds on regret. This paper proposes a new theoretical bandit model called congested bandits.<|endoftext|>This paper proposes a novel multi armed bandit framework to formulate the scenario of route recommendation based on dynamic traffic situations. The formulated model fits in both stochastic bandits and linear contextual bandits. The paper first introduces the stochastic bandit setting with time varying expected reward for each arm, which depends on the pulling history within a fixed sized sliding window. The authors updated the model and propose an algorithm CARCB with regret analysis indicating the regret upper bound scales as $\tilde O(\sqrt{dT+\Delta})$ with high probability. Strengths: The idea in this paper is novel and practical. I believe that the described traffic recommendation is popular in industry, also the theoretical formulation is non trivial. Two bandit settings are well discussed including almost every possible raised problem. (iii) in the related work section, I would like to know more about differences between existing works and the paper. Current version is not clear to me, like what is instantaneous regret and how it differs from the proposed regret? Is their model formulation the same as the proposed? Still there could be some improvable weaknesses, see above.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; Weaknesses  The novelty of the paper (though admitted by the authors) could be more significant. While it is understandable that the main focus of the paper is not on doing significantly better than these methods since they share a similar concept, the paper may be more novel if it can more clearly differentiate their contribution. What is the main novel contribution of this paper that differentiates it from other works in this space? Unfortunately, I currently recommend weak rejection since I don t believe that there are quite enough significant novel contributions. While the experiments and ablations shed some further light into how to effectively use these methods and provides some better understanding, this space and the concepts have been thoroughly explored in previous works.<|endoftext|>The authors do this by either setting them to zero at a certain iteration or regularizing their sizes. The project details are well documented in the appendixThe paper is well writtenWeaknesses:I can t find any comparisons with other, similar, sparsity methods. I would like to see confidence intervals on the different model performances (if you have the resources of course). The code is not available (as far as I can see). The problem is well motivated and the implementation well described. The experiments are extensive. I would be able to change my mind if the authors provide such results, or argue why they don t provide it. E.g.why is the threshold important.<|endoftext|>As mentioned above the experimental results provided at the end of the paper (one of the most extensive set of experiments I have seen in a paper), would be a strong contribution alone. Finally, the relationship between correlation and exploration is not clear. This contribution alone could have a significant impact on the field. "by training sparse models delineated so far" Not clear what this refers to. So far when? I recommend authors to do extended training comparisons (like 2x or 5x) for the Resnet50 classification experiments. I am not sure what new insights are provided in Section 2 compared to [3].<|endoftext|>One example is that sparse models usually perform well in inference, but do not perform well in training. Based on this point of view, this paper proposes a series of steps to augment search spaces of sparse models during training to approximate the behavior of larger models. More results and explanations are required. 3.The authors propose several techniques to encourage the exploration and exploitation when training deep models. More details should be provided. It is an interesting paper and the experiments are strong for me.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; The main methodological contribution of the paper is an information theoretic selection criterion Predictive Information Content (PIC) to choose training points. While the proposed is conceptually interesting, I have major concerns with the practicality of the proposed method. If the hold out set is large then why would one not use that for training the model in the first place? Some ablation experiments showing the differences from equation 4 to equation 6 can be insightful. Overall, while the proposed method is novel, there is some serious concern with the practicality of the RHOLS method because it assumes an already trained model on a hold out set to train another model on training set.<|endoftext|>This paper introduces a new technique to select a sequence of training points for faster model training. The newly derived method is tractable, can improve efficiency through selections of examples that are neither too hard nor too easy, and is robust to noise. In general, the paper is well written, with the main ideas outlined clearly. The approach is well motivated, backed by information theoretical reasonings, and seems to be built upon established literature. 4.The derivation of the PIC (and to some extent, the problem considered in this work) is motivated by developments in active learning but is adapted in a novel way.<|endoftext|>The paper presents a method for selecting the most appropriate data points and using them for training to realize the speedup in training. The problem addressed in the paper is important given the long training demand by data centric applications. Therefore, it will be of interest to see the performance of the proposed method on bigger datasets. There may be chances that more decisive data samples are there in Dt  therefore this approximation may not hold true. More discussion is required about Figure 2.<|endoftext|>The paper proposes a technique to speed up training by another trained model as a proxy for how informative examples are. However, it defeats the original purpose of the work, which is to present a method for accelerating training in the very large data regime. In the experiments, the authors use small datasets only (of CIFAR10/100 size). In addition, there is the issue of reserving a labeled holdout set. Since another model is trained and a subset of the labeled data is reserved, I don t find the speedup significant enough for this approach to be useful. Post Rebuttal: I have increased my score based on the new materials added to the paper that addressed some of my concerns.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 8; The paper studies the Brownian bridge formulation for diffusion based generative models, derives theory for it, and shows connections to earlier works. There are no large scale or exhaustive experiments. This is confusing* Eq 4 is confusing. The three main major issues with the paper nevertheless still stand: the presentation is overly complex; the motivation is not convincing enough; and experiments are anecdotal. While the theoretical derivations and the idea of Brownian bridge are interesting, and potentially a breakthrough, I feel that the theoretical results alone are not sufficient for publication. Does this relate to the rest of the paper or the proposed method? The paper shows that Song’21 method can be seen as special cases of covariant Brownian and OU processes. * Eq 25: why would matching s≈X result in s≈E[Y]? Perhaps the authors run out of time.<|endoftext|>In this paper the authors introduce a new model for diffusion based generativemodelling. I would like to see more theoretical andexperimental comparisons with existing works to assess the efficiency of theproposed method. Instead of relying on time reversed processes as in existing works,they propose an approach based on the mixing of diffusion bridges. The mixture of diffusion bridges is clearlymotivated and the theoretical results seem to be correct. The fact that the resulting drift can be approximatedusing score matching techniques is also interesting. Also, the authors should compared their methods with other works which also pindown the initial and terminal distributions such as [2,3] There is no theoretical justification why diffusion bridge based generativemodelling might be better than the classical time reversed one. COMMENTS: The authors claim that every approach for score based generative modeling is based on time reversal.<|endoftext|>This paper proposes a diffusion bridge approach — based on conditioning on diffusions on both ends. In many places, the motivations and constructions are not clear and not written precise enough for the ICLR readers. This result is deferred to several other theorems but it is hard to check whether assumptions made in these works to prove these theorems hold in this setting. 2) I wonder why authors move from Eq.(7) to mixtures, as it is not made clear. Why is it not enough to use Eq.(7) for the generative modelling? 4) I found Section 4 quite confusing. Also, these results need to be proved clearly instead of referring to the Oksendal’s book as done in the paper. I found the paper hard to read   but found the ideas interesting.<|endoftext|>The paper introduces a methodological framework for generative modeling through diffusion processes without time reversal arguments. By utilizing diffusion bridges the authors consider processes that start and end in pre determined points $x_0$ and $a_{\tau}$. Then by extending this to mixtures of diffusion bridges they show how to transport a distribution $\Pi_0   P_D$ to $\Pi_{\tau}   P_Z$ without time reversal of the diffusion process. Finally, methods for moving beyond fully factorial noise models are presented by extending the process and the noise term from functions of time to be functions of both time and space. This allows for incorporating "priors" with better spatial dependency models more suitable for specific datasets. The flow of the writing is clear, supporting arguments are properly presented, and representative results are shown appropriately. Writing is clear, problem is significant, and proposed methods are novel.
Reject; rating score: 3; rating score: 5; rating score: 6; This paper proposes a metric for measuring the difficulty of instances. This paper shows that hard adversarial instances lead to degraded generalization performance in adversarial training based on the measurement of instance difficulty. I am concerned about the meaning of  model agnostic  which is one of the property of the metric for instance difficulty. 2.In Sec.4.2, this paper claims that adversarial overfitting arises from the model’s attempt to fit the hard adversarial instances. The reweight strategy gives lower weight for the instance whose predicted probability is lower. However, I am not convinced of the novelty and the correctness of the proposed method.<|endoftext|>The difficulty level of samples has been considered in the adversarial training, especially the strategy that considers the weight of examples (like MART and SAT). The authors first said they provide a model agnostic metric, which can measure the difﬁculty of an instance. In my opinion, section 6 does not seem to have much to do with the previous observation. 3.I carefully compare and analyze the difference between the algorithm in this paper and SAT, and I think it is mainly on the final adaptive target. Through experiments and theoretical analysis, this paper concludes that hard adversarial samples will lead to overfitting. Even in the experiments, the corresponding experiments are also missing.<|endoftext|>This paper proposes a metric to measure the difficulty of training examples and find that hard training examples influence the generalization of adversarial training and cause overfitting in adversarial training. By definition, the difficulties of training examples are defined with losses of corresponding models, which means that it is not model agnostic. The method to calculate the metric is model agnostic, but the metric is not model agnostic. In Section 3, the authors define the difficulty metric for training samples. If the evaluation results also show improvements on standard adversarial training, the paper will be more compelling.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 3; How is it different from a standard sequence to sequence learning problem, in which the target sequence takes values in the label space? This paper proposes a complex solution for what is claimed to be a novel problem, but the problem setting as stated seems well known and the solution is unclear both in its motivation and its details. My conclusion is that the paper is not ready for publication at this time. There is also a question of computational complexity. There is no discussion of how long this takes relative to the baselines, but it is reasonable to suspect that the proposed method may be significantly slower.<|endoftext|>This work proposes to continuously classify time series at every timestep to provide early classification capability and address real world needs. I believe it s a combination of fragmented sentences within paragraphs, the usage of notations, and the lack of justification for certain modeling choices or evaluation metrics. The paper proposed a reinforcement learning method for early classification of time series. Although it seems to have some technical novelty, the presentation and evaluation will need to be improved.<|endoftext|>Thus, it is only feasible in some specific applications where the accumulated data is small and not time demanding for inference. positives:  This paper studies a critical problem in real world applications. The application scope of the proposed method is limited.<|endoftext|>CCTS is a problem setup that aims to forecast the target variable every time. Any real world time series forecasting application handles this problem, and there are many services and implementations to process time series streams. There are also lots of works on long term, or long range forecasting. #### <Presentation>It is not easy to read through the paper to understand how the ACCTS model works. This paper proposes an adaptive solution for sustainable time series forecasting.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 8; This paper proposed a lightweight convolutional neural network based on the idea of quaternion neural networks. The comparison is not enough to show the effectiveness of the proposed method. The authors defined the parameterization of hypercomplex convolutional layers so that convolution neural networks can utilize the quaternion algebra to improve the parameter efficiency. Without the comparison to other commonly used DNN compression approaches, it s hard to understand the significance of the proposed method.<|endoftext|>* The paper is clearly written. It would be nice to see, for example, some results with a ResNet50 on ImageNet. * It isn t immediately clear to me that parameter count is what needs to be cared about the most. In AlgebraNets (https://arxiv.org/abs/2006.07360) the authors use BatchNorm, arguing the FLOP expense of whitening is not worth it. Is this something that you considered? Specifically, I would have liked to see a measure of FLOPs required to reach given accuracies.<|endoftext|>The main advantage of the PHC Layer is that they enable choosing an arbitrary $n$ to reduce the number of parameters to $1/n$, whereas this was limited to $4$, $8$, and $16$ with quaternions. Hyperparameter $n$ also drives the number of convolutional filters. This method significantly improves the time and computational power required for the training of large neural networks while achieving good performance. Although the research has many real world applications, it is lacking novelty. 2.Many details required to reimplement these models to achieve similar results are missing in the paper. This is a major concern for me.<|endoftext|>I would encourage the authors to move this to the experimental part instead, or rewrite the two last paragraphs as they currently sound as "look, we tried, it works, but we don t show you the results" even if you give then in Appendix. It is not appropriate to present it in this way and at this stage of the paper. My only concern, that prevents me from assigning a high rank on the current submission relates to the quality of the writing. In practice, the contribution is minimal, but very valuable as the code is given.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 8; The paper uses CORnet S (a network that has been proposed to resemble primate ventral stream), and BrainScore (a benchmark of how closely related deep network responses are to visual responses in primate ventral stream). A combination of methods leads to 80% of full brain predictivity with 0.5% of the standard number of weight updates. This seems to be the motivation for the work, but those authors don t raise the issues that are addressed in this paper. The Marcus book predates deep learning. Further discussion of how this relates to and departs from biology would be helpful. I think the approach and the results are new and interesting, but I have reservations about the motivation for the work and its connection to biology.<|endoftext|>This paper is well written and addresses an important question possibly helping bridge neuroscience and machine learning. Second, the results obtained are very subjective to the authors as the result of the improvement of the score is minor. ### Strength:First of all, the question addressed in the paper is essential for both the machine learning and neuroscience community and the author aim at addressing in a well written piece of work that also clearly contextualizes and motivates the issue at hand. The authors also propose a good analysis and clearly lay out their results, although their interpretation is one of the weaknesses of the paper.<|endoftext|>The paper is concerned with closing the gap between the amount of training in deep networks and in developing brains, as the current deep learning models use an unrealistically large number of synaptic updates. The authors address that with three strategies: less training, weight clustering and training in a subset of layers. I therefore favor acceptance. ## StrengthsThe paper addresses an important problem in theoretical neuroscience: while trained deep networks appear to be good models of the visual stream, the amount of training they need to get there is incompatible with biology. Overall, the paper presents a novel set of results for an important problem, and supports its claims with a significant number of experiments. 1.Clustering should be better explained in the main text. ## RecommendationAs the paper addresses an important problem with a novel approach and extensive experiments, I recommend to accept the paper. > Abstract> First, we find that only 2% of supervised updates (epochs and images) are needed to achieve ∼80% of the match to adult ventral stream.<|endoftext|>While deep networks are known to be biologically implausible, the strong similarities between the feature maps in such networks and the response characteristics (tuning curves) in different areas of the mammalian visual cortex (in particular the ventral stream) has prompted much work into trying to reconcile the known differences. Overall, we find the authors  presented work to be of great interest to those interested in applying ML models to theoretical questions in neuroscience and to have a thorough set of analyses that both illustrates progress in this area while suggesting future avenues of work. Such findings are highly relevant to allowing for such networks to be more plausibly accepted by the neuroscience community as valid models of cortical networks. The striking finding that well chosen random weights (Weight Compression) can predict neural responses with more than 50% accuracy is  especially interesting, as it suggests that indeed there may be less training required in mammalian brains compared to randomly initialized neuronal networks.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; In this paper, authors studied the problem of achieving both the overall differential privacy and certified robustness simultaneously for pre trained models. Authors analyzed DP guarantee provided by these perturbations and empirically evaluate their methods on MNIST and CIFAR 10, and shown that TransDenoiser is effective against FGSM and PGD attacks with guaranteed DP. Strengths:* The idea is well described. This paper mainly builds off the work by (Salman et al, 2020) [1], which proposed to train a denoiser on input perturbations and leveraged randomized smoothing to achieve certified robustness. * Throughout the paper, it is not made clear why achieving DP is important, and what is the difference between "partial" and "overall" DP. A good chunk of your introduction could be moved to the related work section as well. * In terms of clarity, the overall writing could be greatly improved. * I’m not sure what s going on in Figure 3    * there is no figure reference in main text. * Under "Empirical defense", the authors are mainly describe figures in the Appendix.<|endoftext|>This paper studies the problem of integrating differential privacy and robustness to adversarial examples for pre trained machine learning models. In Thereom 3, the authors present a result on privacy preservation for Algorithm 1 based on a previous result in [5]. While this technique is known to provide state of the art "certified accuracy" against adversarial examples, its privacy guarantees remained to be studied. The authors claim three main contributions: 1. In fact, I am not sure that the claims of the paper are technically correct, especially with respect to Lemmas 1 and 2. Finally, looking at the proof, I have some additional concerns, among which a) the reason for the jump from (9) to (10) is not clear to me, and b) the transition from (10) to the equality between the gradient of $z_{(i)}$ and the perturbed gradient of $x_{(i)}$ is also not clear to me. This claim does not seem sufficiently conclusive to me. I think the authors should compare with this previous work. I have two concerns with this: a) the authors are comparing methods that do not preserve privacy on the same dataset, which makes the comparison unfair compared to previous methods, and b) I checked the pre trained classifiers and it appears that they use the same datasets as the authors  trained auto encoder (since these models are not trained with privacy, I think this represents a clear privacy breach).<|endoftext|>This improves upon past work by reducing the differential privacy budget needed by showing that some differential privacy is obtained from the input perturbation. In addition to new analysis techniques for providing the privacy bounds, experiments show gains over prior methods, e.g.better differential privacy under adversarial attack. Strengths:It may be desirable to defend the same classifier against adversarial input perturbations and privacy attacks, so approaches which simultaneously guarantee certified robustness and differential privacy are interesting and useful. Prior work has noted that certified robustness techniques provide differential privacy, current work provides a quantification of this claim. How are the perturbation scale thresholds xi_low and xi_up set in the experiments? Experiments compare with other approaches that simultaneously try certified robustness and DP, but what is the gap from approaches that optimize for exactly one or the other? It appears to not be the case in Algorithm 1? A clarification would be useful if there is abuse of notation, or if the little oh choice is just coincidental. The paper considers an interesting and relevant question but the technical novelty and significance of the provided results does not seem to be sufficient. It is not clear how to set the hyperparameters of the proposed algorithm.<|endoftext|>This paper focuses on providing both differential privacy and certified adversarial robustness to machine learning models. The authors propose an algorithm called TransDenoiser to achieve such a goal. TransDenoiser consists a denoiser through both input andgradient perturbation for achieving DP and certified robustness, and following by a pre trained classifier for classification. strengths: 1). This paper considers transforming input perturbation into gradient perturbation, then the noise introduced by random smoothing can be quantified with the explicit gradient perturbation for the privacy guarantee. weaknesses:1), Multivariate Gaussian Mechanism is not new, and many previous works are also investigated multivariate Gaussian differential privacy to achieve DP. "Mvg mechanism: Differential privacy under matrix valued query." In Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security, pp.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 8; rating score: 8; The paper constructs a Gaussian mixture model with uniform prior and one Gaussian centered at each key and shows that the posterior distribution ie. the probability that a given query is generated by a given key matches the attention scores in the transformer architecture. The paper proposes that this formulation can learn more diverse attention heads and reduce the computation by replacing some heads with the Gaussian mixture heads which are easier to compute. Moreover, the authors show that it is straightforward to transfer the aforementioned formulation to linear transformers. Finally, the authors perform experiments on the LRA benchmark and language modelling experiments on Wikitext 103. For instance, is the performance improvement coming from using the distance instead of the dot product? It is quite hard to compare the performance with different number of heads without having M for MGK and MLK. It is quite interesting in tables 1 and 2 that reducing the number of heads has minimal impact in performance. The rank argument does not show diversity between attention heads given that the attention matrices can all be the same but have high rank. Moreover, when thinking about how diversity can be achieved with this formulation, it can also be measured and enforced with some type of loss or regularizer. Suggestions   It would be much more informative to exchange FLOPS with wall clock time and although number of parameters is important for large transformer models, so is the memory footprint in the GPU during training.<|endoftext|>This paper proposes a transformer with a mixture of Gaussian keys (Transformer MGK) in order to avoid training redundant attention heads in the transformer modules. The authors formulate the attention mechanism as a posterior distribution $p(t_j   1 | q_i)$ (i.e., given the query $q_i$, how likely $q_i$ matches the key $k_j$), and extend this as the mixture of Gaussian keys where each key $k_j$ is modeled as a mixture of $M$ Gaussians with several assumptions. The authors also propose a liner variant, Transformer MLK, which can be more computationally efficient to long sequences. Results are shown on the long range arena benchmark and language model on WikiText 103 corpus. Especially, in section 2, the formula explanations have a neat flow so that it is easy to follow. The proposed model shows comparable results to general softmax based transformers with fewer parameters and FLOPs. In the experiment analysis, the visualization of the attention matrix and rank distribution supports that the paper s hypothesis that Transformer GMK would learn diverse attention patterns. Weak points  The main concern about this paper is the experimental improvements are marginal. Moreover, although the method shows comparable to original transformers with fewer parameters and FLOPs, I suggest the authors to discuss whether there is any difficulty in the learning process using EM based algorithms for practical usability; for example, how many learning epochs are required, whether it is sensitive to initial values, and an increase of computation required to calculate the E step during the training process. Beyond the attention matrix analysis, in order to demonstrate the keys are well modeled as Gaussian mixtures, the learned $\pi_{jr}$,  $k_{jr}$, or $W^k_r$ which is the weight matrix projecting to $k_{jr}$ seems to be required to be analyzed. I m not convinced with the experimental results are reported at their convergence since the learning curves in Figures 1 and 4 end before converging. Questions  How does the proposed model result with the increasing number of mixtures? Typo in Figure 3.<|endoftext|>This paper proposes Transformer with a Mixture of Gaussian Keys (Transformer MGK), which replaces the multi head self attention module with a Gaussian mixture model. Compared with vanilla Transformer, Transformer MGK has fewer parameters and accelerates training and inference with comparable performance. Even though there are some assumptions: (i) the query and the key are normalized; (ii) all the standard derivations are identity; (iii) the prior distribution is uniform, I think these assumptions are easy to follow. Based on the connection, a natural extension is the mixture Gaussian model and they apply the EM algorithm to learn the posterior distribution. In Table 1 and Table 2, the authors discover the influence of the number of heads. However, [1] claims that one head is indeed sufficient, which is not consistent with your results. 2.In Table 1, Transformer sMGK is better than Transformer MGK in some cases but not in all cases. Can you provide some explanation about the reason? 3.In Table 1 and Table 2, the authors start from 8 heads. Can you provide the results from the vanilla Transformer (12 heads) for better illustration? The experiment results are sufficient to illustrate the improvement of their models.<|endoftext|>This paper proposes a mixture model perspective of multihead attention. Each key within each attention head is treated as a Gaussian component; with some assumptions, the softmax based attention weights can be recovered as the posterior probability of the Gaussian mixture, conditioning on the query. The framework leads to a new parameterization of multihead attention, which is explored in transformers. It can be applied to several linear transformer variants, too. Experiments with text classification, language modeling show that the proposed method achieves same or better accuracy controlling the number of attention heads, and reduces computational overhead. For example, in causal attention, the number of components of the mixture changes over time steps. This could be avoided by, e.g., treating only the multihead part as a mixture, not the keys over timesteps. Second, it can be confusing to weave into the narrative something that is actually not used at all, e.g., the norm constraints around Eq 5, and the M step around Eq 13. This seems  much more expensive than standard multihead attention. If this is true, it is worth clarifying. Does it have 4 x seq_len number of Gaussian components for the keys? Additionally, it would be nice to include a time overhead comparison. Lastly about baselines and further experiments: the WikiText103 baseline seems pretty far behind, e.g., Baeski and Auli (2018). Could the authors explore MGK on top of a stronger baseline? Besides, [1] seems pretty related and worth comparing to. Given the weak LM baseline, I suggest the authors including other sequence modeling tasks such as machine translation. Start of Section 3, “full the number of heads” doesn’t parse for me. References:[1] https://arxiv.org/abs/1911.02150[2] https://arxiv.org/abs/2103.02143Strengths:  An interesting mixture model perspective of multihead attention  It is nice to reduce compute cost with the same or better accuracyWeaknesses:  It can be confusing to introduce concepts that are never used in the model  Several key details of the implementation need to be clarified.<|endoftext|>The paper addresses the efficiency of Transformer models. The paper provides a probabilistic view of self attention in transformers and introduces a novel transformer model, Transformer MGK, and its extensions. In Transformer MGK, each key of self attention is modeled as a mixture of Gaussians whose mixture weights are estimated by the E step of EM algorithm. In the experiments, they show Transformer MGK is comparable or better than Transformers on LRA benchmark and a language modeling benchmark. Strong points 1. The proposed method is empirically efficient in terms of the number of parameters and computational cost in terms of FLOPS. 4.The paper also introduces the linear attention variant of the proposed method. Although the authors argue that Transformer MGK is introduced to improve the explanation power of each key, increase the representation of each attention head, and reduce the chance of learning redundant heads, this paper provides only empirical results and no theoretical explanation for these improvements. (2) Although I can somehow expect that the mixture of Gaussian increases the representation power of keys, I cannot imagine why the proposed method reduces redundant heads and promote the diversity in attention pattern. I like the probabilistic interpretation of self attention and the empirical results are strong. My major concern is about the clarity of the paper (please see weak point #1 below).
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; This paper introduces an agent architecture with two independent policy networks (called ‘go’ and ‘stop’) and an assignment mechanism consisting of a network or a rule that activates one of the two every step. Each policy network is trained on replayed data from the subset of states that has been assigned to it by the assignment mechanism. The agent presented here just has to choose between two policies. One question that the two policy situation raises is why to stop at two. Depending on the task, it might be that a different number is optimal. It would help place the paper more clearly in the overall RL literature if this similarity and difference was discussed more explicitly. Could the authors comment on the use of SAC as a starting point / base training method? Both the motivation from inhibition in the neuroscience literature, and the effective implementation of a two policy agent are fairly applicable to many reinforcement learning techniques, not just SAC. Including experiments on a wider range of environments would address that concern. Including other baselines than SAC would also help to clarify the value of the contribution. The proposed architecture is potentially interesting, even if it seems to go against Sutton’s “Bitter Lesson” by specializing an architecture for a specific case, rather than relying on learning. It does look brittle due to the hardwired nature of the choices that are being made; in particular the number of policy modules in the agent and the division of states across the policies. I am open to being convinced these aspects don’t make the agent brittle, and would raise my score in that case, but that would require more extensive experimentation and baselines.<|endoftext|>The authors present SAC I, which is a modified version of SAC designed for retraining of some existing agent on an updated environment or task. Inspired by the concept of inhibitory control from neuroscience, they learn a separate Q functions and optionally a policy for inhibition. **Strengths*** Considering that RL is about sequential decision making and its importance in developing human like agents, approaching the problem with a neuroscientific view might be a helpful and meaningful direction. **Weaknesses*** The concept of retraining should be more formal or specific. In Sec.2.4, the authors state that "we are primarily focused on transferring learned value and policy functions among identical aspects of a task, while learning new skills (value functions) and retraining the previous learned policy within the similar environment", but its difference from transfer learning in RL is still not clear, as usual transfer learning in RL also involves transferring learned components. I think "identical aspects of a task" doesn t clarify the dissimilarity. I suggest the authors to discuss which components of the original MDP can change and why and how retraining is different from common practices of transfer learning in RL. * My biggest concern is that this work seems to require or rely on human engineering of reward functions too much (regardless of learning an inhibitory policy or not). of Sec.4.1 for the LunarLanderContinuous experiments, the authors state that the penalty, $r_{bomb.proxy}$, which is defined using the agent s distance to the bomb, is combined with the original reward and given to the agent for the retraining. I m aware that the authors suggest it as part of the problem setup, but assuming the specific problem, I think the contribution of this work is not very significant. * Little more details about the inhibitory policy would be nice.<|endoftext|>The paper proposes inhibitory networks for reinforcement learning. Inhibitory networks choose a behavior among a set of low level behaviors to be executed at the current state. The method is implemented on top of soft actor critic and evaluated in two OpenAI Gym environments, LunarLander and BipedalWalkerHardcore with some non default modifications, and compared to a baseline standard SAC implementation. The clarity of the paper is not sufficient for assessing its contributions and how it positions itself with respect to the prior literature. However, I have several concerns regarding clarity of presentation, novelty of the contribution, and the experiments. Many of the details still remain unclear to me (as I will explain below), and it is very possible that I have missed important ones. In particular, I would suggest including a concrete example of the realization of the method, with the connection to hierarchical policies right in the introduction; and improving the Methods section by including more details and being more careful with the notation and claims. Currently there are many aspects that are not accurate or not addressed at all. * Are there separate policies for the Go and the Stop behaviors, or just a single one as in Figure 2? Regarding contributions, it is not clear how the approach is different from hierarchical RL with discrete low level behaviors. I think the work would substantially benefit from reformulating the approach as an HRL method, carefully pinpointing the improvements and contributions over existing HRL literature, and evaluating their relative importance. The connection to inhibition in the brain is interesting, but as presented at the moment, hides the connection to HRL. Regarding the experiments, there is some mismatch between the figures and the text. For example, Figure 5 shows that training without the inhibitory network is more efficient. Comparing to SAC I* that learns the inhibition rule (if I got this right) is well justified, but no details are given regarding how it is learned. Minor comments:* The sentence: “ For states with high reward, a low entropy policy is permitted while, for states with low reward, high entropy policies are preferred, leading to greater exploration” is inaccurate.<|endoftext|>Various agents are presented on two Box2D environments from OpenAI gym. The paper appears to be technically correct, employing a modified version of SAC to investigate transfer learning tasks. The authors examine ablations of their proposed method to isolate the contributions from each aspect. For instance, the main baseline used in the paper is pre trained SAC. While I agree that these are interesting tasks, it may be helpful to also evaluate in settings used by previous works in continual RL, enabling a proper comparison. Overall, the presentation of the method comes across as somewhat ad hoc. For instance, in some cases, an inhibition rule is defined, whereas in other cases, this is learned by a separate inhibition policy. In both environments, the authors use various redefinitions of the reward functions for shaping and learning the task. The Q value is fed into the policy network, which is an odd design choice that seems poorly motivated. There is a considerable amount of work in related areas of multi task RL, non stationary environments, online/continual learning, etc. SAC outperforms SAC I in Figure 5, although the accompanying text states otherwise. The authors motivate the proposed method by describing connections to inhibitory control, from neuroscience. While I’m generally supportive of neuroscience inspiration in machine learning, it’s not clear what these sections/paragraphs provide for the paper. That is, the details of SAC I are so disconnected from the specifics of the neuroscience research that this motivation effectively amounts to ‘animals can switch between multiple tasks.’ This doesn’t strike me as any more helpful than the motivation from existing works in hierarchical RL, e.g., options. I would suggest that the authors either a) largely remove these sections or b) show more specifically how the inhibitory control literature can inform RL beyond what currently exists. Section 3.1: By conditioning the policy on $Q$, this effectively becomes a mixture of policies, which is equivalent to a hierarchical policy. Ideally, connections and differences from previous works in hierarchical RL should be discussed. It seems like the point is that SAC I is less negatively affected by the shaping reward. Currently, the paper is lacking an adequate discussion of related works, as well as appropriate baselines beyond SAC. As the authors are proposing a new method for transfer learning, a comparison with existing works is necessary for proper empirical evaluation.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; + The proposed approach is very simple and can be adapted to any generative model. Overall, I think that this is an interesting paper, showing that generative models contain a lot of information about the structure of images.<|endoftext|>This paper proposed to mine useful semantic concepts  from deep generative models which is an interesting and unexplored topic. Although the core idea of this work is to find plausible directions for latent space manupilation which is similar to [1]. Then train a segmentation model and evaluate it on Celeba HQ. 5.The latent manipulation may not be perfect. The experimental evaluation is not very convicing.<|endoftext|>I think that the authors demonstrate that their method can be applied to various architectures and that overall it has potential on identifying the foreground background information. The motivation of the paper and the overall method is very interesting. This will give a clear indication on how the generative methods compare against other methods. I think that this has a lot of applications and is an interesting area of research.<|endoftext|>The paper proposes an unsupervised approach for instance segmentation that leverages pretrained GANs. Quantitative results are computed on a variety of tasks, not just instance segmentation. The main contribution claimed by the authors is that their method can be applied to off the shelf pretrained GANs, whereas competing approaches require architectural variations and re training.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 8; The method employs a mixture distribution formulation that can construct an optimal flow model with n layers of transformations from the transformation set. In the experiment, the authors proved the effectiveness of the optimization method which via approximate upper bound. In this paper, the authors propose an automated normalizing flow architecture search method which can find the best distribution for each layer from a set of given distribution sequences. 3.The authors optimized the model via approximate upper bound instead of using KL divergence between the target distribution and the mixed flow, so that the model can go out of the local minimum. 3.Eqn.(8) should use the symbol of expectation instead of directly using capital E.4. It seems that both cases suffer the same issue. Thus, the impact of lambda should be investigated in the experiment section. ICLR 2019. It is unclear the essential technical contributions compared to existing NAS methods.<|endoftext|>In this paper, the authors proposed to adapt a differentiable architecture search formulation (Liu et al., 2019) based on learned weighting of an ensemble of modules to automated search for Normalizing Flow architectures. Furthermore, the authors proposed to optimize the full network using an approximated upper bound of the KL divergence, instead of directly optimization. The authors proposed two methods to decompose the optimization problem: grow method, which is more straightforward and greedy, and block method, that alternatively adjusts each block. The authors experimentally compared their proposed method with manually specified architectures across various datasets, including POWER, GAS, HERMASS, MINIBOONE and BSDS300. The results seem mixed, as the searched model outperforms the manual model in some contexts but not others. On the novelty side, though the authors made problem specific adaptations for the differentiable architecture search algorithm for normalizing flows, the main idea is very much adapted from Liu et al.2019 and somewhat marginally novel.<|endoftext|>This paper proposes a DARTS like method for searching automated normalization flow models. Instead of directly using the output ensembles, which leads to infeasible flow models, this work proposed distribution mixture to guarantee that the supernet is always a valid flow model. Experiments on small to medium scale datasets valid the effectiveness of the proposed method. Although the proposed method is based on DARTs, it requires some efforts to make it work on flow models, such as distribution mixture. The paper is a bit difficult to follow if the reader is not very familiar with flow model. b) The proof of upper bound optimization should be further clarified. At least from the experiments, it seems that binarization  is still a necessary step. 5.Table 1 need improvements. For now it is difficult to compare results since they have different costs.<|endoftext|>This work provides the novel approach for searching flow architectures. In order to enforce invertible properties of the model the authors suggest to use the mixing distribution approach instead of mixing the base transformations. Some experiments are also performed to show the quality of the approach with respect to the baseline that is expert based selection. StrengthsThe problem considered in this work is novel and important for the community that works with flows. The problem is significantly more challenging than standard architecture search. The idea of using mixing the probabilities and the application of upper bound instead of direct optimisation is non trivial. The selection manual flow baseline is a bit tricky to be. How far is the manual approach from the optimal combination of transformations on validation set?
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 8; A rigorous proof is given, and the convergence to a global minimum of the cost functional is given. The authors may also need to discuss this point in the mean field regime. However, I have several concerns about this paper. al., A mean field analysis of deep ResNet and beyond: Towards provably optimization via overparameterization from depth. [2] Ding et. 2.It is also not very clear what is the advantage of this paper compared with prior works on mean field or NTK. 3.My next concern is regarding the comparison to the NTK results. Then one question is what can we gain by studying the convergence of ResNet in the mean field regime? al., Algorithm dependent generalization bounds for overparameterized deep residual networks.<|endoftext|>This work analyzes infinitely deep and infinitely wide residual networks. The infinite width limit is the mean field limit. Since this paper is meant to be a standalone paper distinct from [2], I would like to get some clarification as to what are the specific contributions not present in [2]. I would like to hear from the authors about the novelty of the proof technique compared to that presented and used in [3]. [2] Zhiyan Ding, Shi Chen, Qin Li, Stephen Wright, Overparameterization of deep ResNet: zero loss and mean field analysis, arXiv, 2021.<|endoftext|>This paper studies the gradient flow training dynamics for deep and wide ResNet with the mean field scaling. The authors also justify the well posedness of the resulting distributional dynamics, which seems only heuristically defined in prior work. * In Assumption 4.1, the activation f is required to be C^2. * In Theorem 5.1, the final bound seems not uniform for 0< s<S, which is a much weaker nonasymptotic result than that in (Mei et al., 2018) for two layer NN. Overall, I think this paper has useful ideas for understanding the training dynamics of ResNet in the nonlinear regime, and it complements the existing research in the literature.<|endoftext|>Many thanks to the authors for addressing my concerns and correcting the mistake. As said in my review, there are reasons to believe why the contribution of this paper is a clearly meaningful one on top of prior works in similar resnet mean field setups, namely the removal of the full support assumption on $\rho_{t \infty}$. Global convergence of three layer neural networks in the mean field regime, ICLR 2021. This is a technical paper that removes a strong assumption from the analysis of Lu et al 2020, though the techniques are not surprising. In other words, this analysis can be carried out (with some technical confidence, admittedly) for the right assumptions.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 3; This paper provides a framework called Multi Trigger Key(MTK) to achieve privacy preserving inference in multi task setting. This paper is quite confusing in the way it describes its motivation and methods. In empirical study the authors present two different triggers: square and cross. The motivations and methodology of this paper is not adequately explained in this paper.<|endoftext|>Multi task classification (MTC) is a kind of multi task learning, which performs multiple multi class classification tasks at the same time. I will read other reviewers’ comments and authors’ feedback to make my final decision. The trigger keys in the work seems to be intentionally and manually assigned by people.<|endoftext|>This paper proposes a new framework, multi trigger key (MTK), which aims to protect sensitive information during inference for multi task classification. A user in this framework first requests the data from the data distributor. In summary, the paper offers a novel approach for information privacy, relying on the lack of secret data augmentation to poison the model. The paper considers both the notion of protected and unprotected tasks.<|endoftext|>This paper focuses on access control to multi task image classification service. We would suggest authors consider carefully the necessity of such a design proposed in the submission in practices.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; rating score: 5; This paper proposes to use the graph based method for similarity search in hyperbolic space. Extensive analysis shows that graph based method achieves sublinear time complexity for hyperbolic space and its performance is even better than for the Euclidean space under some mild conditions. Empirical experiments are also conducted to validate the theoretical analysis. It is interesting to know that the graph based method, which works very well for the Euclidean space, also works for the hyperbolic space. The analysis is solid and the experiments justify the theoretical findings. I have read the reviews of other reviewers and the author response. However, I agree the experiments should be improved to include more datasets/graph based similarity search algorithms. I think this is a solid and interesting paper.<|endoftext|>This paper looks into graph based algorithms for nearest neighbor search (NNS) problem in the hyperbolic space. Theoretically, the authors derived the time and space complexity of graph based NNS under some assumptions (data uniformed distributed in the ball, low dimensional dense), and in some cases, graph based NNS has lower time complexity in the hyperbolic space. Theoretically the authors do show that under some assumptions and in some cases, the graph based NNS in hyperbolic space can be more efficient than that in the Euclidean space, though clearly these assumptions probably not hold in realistic dataset. However, there are some weakness:1. the assumptions are strong, it looks like to be common in this area to assume that data elements are distributed uniformly within a sphere in the space? This paper is an interesting work to analyze best first search graph based NNS algorithms in the hyperbolic space.<|endoftext|>This paper addresses the important problem of nearest neighbour search for data that lives in hyperbolic space and provides a novel result on the relative time complexity of NNS in hyperbolic and Euclidean spaces under some assumptions. The main result of the paper is that under certain assumptions, NNS in hyperbolic space can be faster than in Euclidean space. My main concerns are around the validity of the assumptions and the empirical evaluation. Why did the authors choose to do the theoretical analysis for the dense case and then empirically analyse the sparse case? Could the authors perform this experiment? Interesting and potentially impactful work, but I would like to see more comment on the validity of the assumptions and improved empirical evaluation before raising my score.<|endoftext|>The paper approaches the problem of nearest neighbor search in hyperbolic spaces with focus on graph methods. The authors claim to obtain an acceleration in hyperbolic spaces over Euclidean ones using graph based NNS method (e.g.Prokhorenkova 2020). The paper is clearly written and has the main ingredients of theoretical analysis accompanied with experiments and also experiments that attempt to validate the theoretical results. 2.If the authors need to address the intrinsic dimension, I recommend a separate parameter d’3. 4.Some more motivation on the advantage of hyperbolic spaces for real life settings can enhance the view on the impact of the method. Overall the insights of the paper are important but their validity in the practical domain should be better explained in the paper.<|endoftext|>The paper considers the setting of nearest neighbor search (NNS) and devises a graph based algorithm for this task in Hyperbolic space. What was done in the paper? It is shown that under some assumptions on $d$ and $R$, graph based NNS has lower time complexity in hyperbolic space when compared with its Euclidean counterpart. Given that the changes were fairly considerable, I hesitate to recommend acceptance until they have been carefully evaluated; moreover, I believe more can be done to address the concerns reviewers have, specifically surrounding empirical results. 8.I would like to see more than a toy experiment (given in Section 5.3) to ascertain the results for the dense setting. Although the paper extends the result of Prokhorenkova & Shekhovtsov (2020) for dense graph based NNS in Euclidean space to hyperbolic space, it does not attempt to provide theory for the more interesting and practical sparse setting. 2.A large portion of the paper lacks novelty. "alternative approach" would be more appropriate. Although the problem is relevant, the paper as is lacks considerably, both in terms of theoretical and empirical results. These assumptions were critical for a proper theoretical analysis, and should in some way be handled when the theory is applied.
Accept (Poster); rating score: 8; rating score: 5; rating score: 5; This paper presents a differentiable probabilistic model (DP DAG) over DAGs that retains accuracy over comparable differentiable DAG models. The computational gains come from the factorization of the DAG distribution into the product of orderings and edges, which results in intractable scoring as all valid permutations must be marginalized over in order to score a graph. However, this is not an issue: the pathwise derivative is used during training, requiring only a differentiable sampling procedure, and evaluation does not require scoring. * The experimental evaluation is thorough. The results are convincing, with the proposed method VI DP DAG either achieving the best performance or comparable for structure learning. * The paper is clearly written. In other words, what is the total training time for each method? _Feedback_* Can you add a table with DP DAG and second best method with and without pre/post processing to the appendix? I recommend this paper for acceptance.<|endoftext|>Following the work of NOTEARS, this paper focuses on the problem of DAG structure learning using gradient based methods. As mentioned by the authors, “the augmented Lagrangian optimization is computationally expensive”. However, my major concern is that the proposed method of this paper is only a combination of well developed techniques. Thus, I think it is not appropriate to state it as a theorem. Moreover, Theorem 1 has been applied to sampling DAGs already. Therefore, in my opinion, the idea of separately sampling an upper triangular matrix and a permutation matrix in order to sample a DAG is not novel. 1  > Th.1Overall, this paper is well written. Thus, I think the contributions are only marginally significant or novel.<|endoftext|>The authors proposed an algorithm for sampling DAGs that is suited for continuous optimization. The sampling algorithm has two main steps: In the first step, a causal order over the variables is selected. In the second step, edges are sampled based on the selected order. Moreover, based on this algorithm, they proposed a method in order to learn the causal structure from the observational data. The causal structure learning algorithm is guaranteed to output a DAG at any time and it is not required any pre  or post processing unlike previous work.
Accept (Poster); rating score: 6; rating score: 5; rating score: 5; This topical paper claims to raise awareness of climate change by projecting flooding images of popular places. PositivesThe creation of a simulated flood datasetThe use of segmentation "Masker" conditioned on the depth of an image is something I haven t seen beforeThe qualitative results appear improved over other methods, and there is a positive human evaluation also Negativesthe system is a collection of black boxes, depth estimation, segmentation estimation etcLimited to a single depth of flood value (although this is discussed in future work)This paper is different  and topical, with still has a contribution to the research community and hopefully will provoke discussions<|endoftext|>In this paper, the authors introduce ClimateGAN, a framework for the generation of images of flooded scenarios in order to raise climate change awareness and prompt action. Please clarify it in the text. However, from a technical viewpoint, I believe that the contribution is not novel enough for acceptance at ICLR as ClimateGAN is given by combining two already existing models (DADA and GauGAN) with minor modifications. I would also include more failure cases in the paper (like the two in figure 18 in the appendix), where the Painter also fails at producing plausible flooding. It would be interesting to see what happens if the Masker and the Painter are trained jointly and compare this to the results in the paper. If this is the case, I believe that the results would not represent an accurate estimate of the performance comparison between the ClimateGAn and the other models.<|endoftext|>This paper proposes to generate images of floods, that is, simulate photo realistic floods, with a model named ClimateGAN, which consists of a Masker to generate masks for floods and a Painter to draw floods on images. The outputs of Masker and Painter are evaluated separately compared to several previous image to image translation methods, also the ablation study is conducted to validate the effectiveness of each component in ClimateGAN. Strengths:   This is a good application by leveraging the cutting edge GAN related techniques on an interesting area of generating flooded images. The two stage design with a Masker and a Painter is useful for synthesizing floods on images. Second, generating flooded images is not well related to the climate change. The paper can be regarded as a particular application for synthesizing flooded images with street level cityscape images, however, since this work is not well motivated, yet the contribution and comparison are not strong enough, overall, I vote for rejecting.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; This paper proposed to personalize only part of the model parameters instead of the full model, and studied two algorithms FedSim and FedAlt. This is a well written paper and easy to follow. However, I have the following concerns. If I understand correctly, the proposed FedAlt algorithm is very similar to the paper, except that FedAlt requires a local state v_i. Li et al.2021 presented a convergence rate of FedSim, could the authors compare? FedAlt algorithm, which is a client state variant of Singhal et al.2021, and achieves marginal improvement over FedSim [Liang et al.2019, Arivazhagan et al.2019, Collins et al.2021, Li et al.2021] (always < 0.5%.) I found this to be interesting, and I would encourage the authors to provide more discussion and connections to empirical results. Savings of memory footprint  It would be good to show either analytically or empirically how much memory the partial model training can reduce compared to full model training as this is the main motivation. Authors mentioned “All methods, including FedSim, FedAlt and the baselines are initialized with a global model trained with FedAvg. For table 3, does the full model in baselines include adapter parameters?<|endoftext|>While the idea of partial personalization and the objective of (3) have been widely studied in previous literatures, the main contributions of the paper are the convergence analysis of two proposed algorithms FedSim and FedAlt in nonconvex case. The authors also did extensive experiments to compare partial personalized models with fully personalized models by using various model structures and on different datasets. The idea of partial personalization has been proposed in previous literature. In order to derive Theorem 2, the authors claim that they proposed a new technique called virtual full participation; however, this technique is actually not new and has been used widely in convergence analysis for FedAvg/Local SGD. This way, the whole contribution of the paper relies on the analysis of FedAlt in nonconvex case based on well known techniques, which seems quite weak contribution to me. I would recommend the authors to give more detailed explanation on what the key theoretical challenge in deriving Theorem 2 is. But given the idea of partial personalization is proposed elsewhere, the gain from the experiments is limited. I can easily understand the paper. In a word, given the ideas used in the paper are now new, I think this paper is maybe better written in a pure theoretical one. The key would be to stress the theoretical challenge and novelty. Personalized Federated Learning: A Unified Framework and Universal Optimization Techniques.<|endoftext|>The main idea is to split the trainable parameters into  shared and personalized parameters where, unlike existing personalization schema, only the shared model is exchanged with the server in communication rounds to be aggregated  thus partial personalization. In regression, this corresponds to learning the residual error of shared model via personalized model and in the classification setting corresponds to output averaging (unlike interpolation based personalization methods that do parameter mixing). The authors propose two algorithms FedSim (simultaneous updating of shared and personalized models locally) and FedAlt (alternative updating of shared and personalized models locally), to learn in this setting, theoretically analyze the convergence rates in non convex settings and conduct empirical studies on image classification and next word prediction to evaluate the proposed methods. This paper studies an interesting question and proposed idea looks interesting with corroborating empirical results, but there are few issues that prevents me from giving it a high score:  The key motivation for the paper is to utilize a small footprint in clients but the proposal is somehow mis leading. Also, I was left wondering how large the coupling between personalized and shared parameters captured by $\chi$ would be in presence of heterogeneity and significant difference in model sizes. For example, the gradient diversity assumption (Assumption 3) is only defined over the shared model.<|endoftext|>This paper proposed a personalized FL framework with partial model personalization. It separates the model parameters into two parts, shared model and personalized model, and optimize them in an interleaving manner. The authors proposed two optimization algorithm, named as FedSim and FedAlt, with partial clients participation. The experiments conduct on NLP or vision tasks also demonstrate that the proposed algorithm outperforms other personalization method. Pros:1.The authors proposed a unified framework for personalized federated learning. To my best knowledge, they give the first analysis of this framework and algorithm on smooth nonconvex function. Cons:1.The personalized layer idea is not new, as authors mentioned, it has been proposed in prior works [Arivazhagan et al.(2019) and Collins et al.(2021)].2.I think the established theory does not perfectly fit with experiment model, since they assume smoothness in theory, but in practice the motivated example is usually non smooth ReLU network. Hence I think it will be great if they can perform analysis on ReLU network, perhaps very simple one layer setting. However, the novelty for me is somewhat limited. If they can come up with theory that can explain their empirical observations (ReLU network), I believe it will be a strong work.
Accept (Spotlight); rating score: 8; rating score: 6; rating score: 6; &nbsp;###   Post rebuttal Comments The authors addressed most of my concerns in the feedback. They also propose to evaluate incremental learning based on the zero shot learning setting. Extensive experiment results are provided to show the effectiveness of the proposed method. ### Strengths&nbsp;  This paper is well organized and easy to follow. I recommend acceptance. They also provided the additional experimental results I asked for. &nbsp;### Weaknesses&nbsp;  ***Replaying synthesised samples for old classes is not a novel idea. So I tend to accept this paper.<|endoftext|>This paper proposes to generate pseudo data using the gradient based method for class  and task incremental learning. Experimental results show the effectiveness of the proposed method. Discussion on memory replay methods is incomplete or misleading. I found that this paper did not cite any zero shot learning works; I recommend [Xian et al.] In ECCV, 2018.\[Javed et al.] Overcoming Catastrophic Forgetting With Unlabeled Data in the Wild.<|endoftext|>This paper proposes a zero shot incremental learning approach that doesn t store past examples or metadata for experience reply. The proposed method distills information from the past networks to form a synthetic set of past concepts. The paper is well written and the structure is clear. Missing ablation study on the hyperparameters. This paper provides a very interesting and novel idea for continual learning. Post rebuttal Comments The authors addressed most of my concerns in the feedback.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; But based on the overall development, the edge device is the only compute limited part and the cloud is an over provisioned server. I think the authors are thinking about a very interesting problem, but the work is at an early stage and not ready for an ICLR publication. Strengths:The problem they consider is highly significant for practical application of DNNs on edge devices. I suggest using clock frequency instead of latency. It is not clear to me what ‘base limited setting’ and ‘global limited setting’ mean in the last sentence of the introduction. How much does it degrade the performance compared with end to end training of components for each desired level? I’m curious about the task of learning a router model.<|endoftext|>2.The idea of introducing a routing model to route more challenging inputs to the cloud is quite novel. The goal is to improve the inference accuracy by predicting the more challenging inputs on the high latency cloud. 3.Table 7 is the only experiment that indicates the latency of the hybrid inference system proposed, but no detailed experiment setup is given, including the network bandwidth and the computation resources on the edge and cloud. FLOPs are typically 2x MACs. It would be better to use the same term in the paper. [1] Huang, Yangsibo, et al."Instahide: Instance hiding schemes for private distributed learning." The experiments should be redesigned to make the proposed method more convincing.<|endoftext|>This paper proposed a hybird framework to process inference efficiently: the framework contains a global network to deal with hard query, a base model for easy query and a router protocol to deliver queries. Even though the end to end training diagram is easy to come up with. 2.What contributes to this framework should be: 1) How to train the router, 2) How to determine the big/small model architectures. 3.For the determination of model architecture, author proposed to use OFA, which is a seperated work. I don t think using OFA can be described as "propose" in the writting. Noted that this is not compulsory.<|endoftext|>This work attempts to improve the accuracy of prediction tasks on edge devices by employing a hybrid approach where a low capacity model is deployed on the edge device and a counterpart high capacity model is deployed on the cloud. All the three models are learnt end to end from training data for the desired trade off. Negatives:Only one dataset is used in the empirical study. The effectiveness of the proposed approach is not only contingent on the predictive accuracy of the routing model (it needs to do a good job of identifying challenging queries (on which the low capacity model is likely to fail) but also on most queries being relatively less challenging (so that the low capacity model can make accurate predictions on them).
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; The authors of the paper start from a great initial question that connects the complexity of the encoder and generator of a VAE. It is reasonable to suspect that the complexity of one mapping would have an effect on the complexity of the other mapping. This whole paper doesn’t read like a paper about VAEs, or about DL, but rather about applying a conjecture by Katz & Lindell (2020), having to do with Boolean circuits, to the situation of VAEs. The whole edifice of deep learning is predicated upon assumption that patterns in the experimental data will play nicely with the inductive bias of neural networks. In fact, the whole paper rests upon the aforementioned conjecture coming from the world of cryptography, for which not much background is given. That would make this very paper completely void. After having said all that, I’ll admit to my lack of familiarity with some of the techniques used by the authors despite being knowledgeable about VAEs.<|endoftext|>The proofs in the paper appear correct, and at first glance, they appear relevant to generative models research community. The paper reads as a string of theorems and remarks, where the reader has to jump around to understand how the theorems relate to the overall points of the paper. VAE basics, for example, are given considerable amount of space, but there is no ending summary or Conclusion to draw things together, which adds to the impression that the ultimate significance of the work is not clear. There is no empirical part nor clear indication of empirical usefulness of the results. Being familiar with the cited (Arjovsky, 2017), I believe I understand what the authors mean, but I don t see how their results really lend the said support to that idea. Could the authors explain this, or point to where is this explained?<|endoftext|>The paper answers the question how complex inference models need to be to accurately estimate posterior distributions. However, I still have some concerns and questions about it. It s difficult to achieve that for a purely theoretical paper. 2.The proofs are reasonable. Sorry I only checked the main theorem and some lemmas and didn t find the time to delve into the proofs in the appendix. This heavily limits the usage of the conclusion in usual VAEs. 2.In remark 3, the authors mention that the case for layerwise invertible models is very simple. This paper (https://arxiv.org/abs/2106.13746) gives some explanation why deep generations models can deal with some manifolds easily but fail on others. Can we use the method in your paper to analyze the problem as well?
Reject; rating score: 5; rating score: 6; rating score: 8; Weaknesses  There have been quite a few works that focus on how self training / self learning improves distribution shift and how self training and pre training stack together: https://arxiv.org/abs/2012.04550, http://proceedings.mlr.press/v139/cai21b.html (in the experimental section), https://arxiv.org/abs/2002.11361, https://arxiv.org/abs/2106.04732, https://arxiv.org/abs/2006.10032, https://arxiv.org/abs/2012.11460, https://arxiv.org/abs/2006.06882 (this one is just about stacking them, not distribution shift). In some sense, the main premise of the paper (for example the title) is somewhat well established. However, the premise that self learning improves robustness is already somewhat well established   the main contribution here is a systematic application to different methods and datasets.<|endoftext|>The main contribution of this paper is to perform a systematic and large study of self training as a method to deal with distribution shifts. The technical contributions of the paper are only marginal and more of incremental nature, however the study itself is valuable and can be beneficial for the community.<|endoftext|>The authors stress that, although simple, these techniques consistently improve the robustness to distribution shifts regardless of model architecture or pre training techniques used. Weaknesses/Suggestions: The insights from this paper are mainly empirical, and there is littlealgorithmic/theoretical novelty. The self training algorithms evaluated hereare well established, even in a deep learning setting (see below). Self training using deep learning as presented here has gained popularity recently, and the paper is missing some related references (see below to name a few). Proceedings of the European conference on computer vision (ECCV).
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; This allows activations to be communicated in parallel (rather than sequentially) across GPUs, which improves throughput. The staleness is in activations/gradients, not weights. The paper should acknowledge this limitation, rather than claiming superiority in all scenarios to sampling based methods. I take that as a promising sign that the method will continue to perform well with distributed network machines. While the idea of stale activations/gradients is not new (as the authors have acknowledged in the related work), the convergence result seems new to the best of my limited knowledge. Overall, it is an interesting if slightly flawed paper, and I lean towards acceptance.<|endoftext|>The authors propose PipeGCN   a method for efficiently distributed full graph GCN training. With that in mind, I am concerned about the scalability of the system. 2.The paper provides novel theoretical proof of the convergence of GCN training with stale feature and feature gradients, which is useful for future work. The paper proposes an interesting framework to speed up the GCN training with the theoretical proof for the complexity.<|endoftext|>This paper proposes PipeGCN, which pipelines communication and computation in distributed GCN training to improve training throughput. I think the paper is a well executed work: the discussions about related works are extensive, the idea behind PipeGCN is clearly explained, the convergence speed of PipeGCN is analyzed theoretically and the experiments are comprehensive. I have some concerns on the novelty of the paper. PipeGCN seems to combine the two ideas by using both stale features and stale gradients. I think the paper can be improved by fixing the following problems. The paper is complete but lacks comparison with an important related work and a justification of the technical challenges.<|endoftext|>This paper proposes a distributed full graph GCN training method to speed up GCN training for large scale graphs. Convergence proof is also provided. This paper proposes a distributed GCN training on large graphs named PipeGCN. Several limitations of this paper are listed as follows:1. 2.Some statements in the paper are not clear enough. The paper can be accepted if the authors improve the convergence proof and modify the uncleared statements.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; Additionally, this paper also proves that the training dynamics of the regularized neural network can approximate the corresponding kernel ridge regression. Numerical results are provided to support those theoretical results. This paper is well written and shows some novel results. The following are some detailed comments. This paper is a good paper with sufficient contribution. Some minor places could be stated more clearly.<|endoftext|>In this paper, the authors analyze the convergence rate of both the unregularized and the L2 regularized gradient descent for a regression problem. Under a positivity assumption of NTK,  this paper shows that without early stopping, the vanilla GD may fail. Overall I think this paper is well organized and well written. The proof of this paper seems solid. 3.Typo: Theorem 3.6 this should be $O((\frac{\lambda_{0}\delta}{n})^{2/3})$Overall I think this paper is well written, and the proof seems solid. Given Hu et al.[2021], the results of this paper are not that surprising.<|endoftext|>The paper made technical contribution along the recent line of theoretical work on DNN: It proves that when data is generated from a nonparametric regression model with i.i.d.Gaussian noise, training (over parametrized, i.e., wide) ReLU DNN with a regularization term with GD approximately converges, and furthermore that the generalization error decays in order faster than O(1/n^{1/2}), where n is sample size. Experiments are not very convincing: in the middle plot of Figure 1, it seems that the best un regularized generalization prediction risk is lower than (or at least on par with) the best regularized one, and that the best result of un regularized generalization error is achieved with very few epochs.<|endoftext|>The paper proves a bound on the excess risk in a nonparametric regression setting, where the estimator is a deep ReLU network trained by GD with L2 regularization, and the regression function belongs to the RKHS induced by the Neural Tangent Kernel (NTK) related to a deep network. The paper gives a positive answer to this question and moreover shows that this can be done with an optimal rate. The proof is heavily based on the NTK machinery, and in particular on the "coupling" between predictions of the network and a ridge regressor given the NTK kernel matrix. Over parametrized deep neural networks do not generalize well.arXivpreprint arXiv:1912.03925, 2019.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; This paper proposes to apply meta learning framework presented by Finn et al.to class imbalanced problems. The proposed method is rather straightforward extension of MAML, but it is indeed reasonable extension. Given these prior works, the technical novelty of the presented paper is limited. 1.The methods compared with the proposed method are not sufficient as baseline methods. The empirical novelty is not enough, too.<|endoftext|>The paper proposed to use meta learning to improve class imbalanced learning. The empirical results demonstrated the improvements over several previous methods for imbalanced data. # Strengths+ The method is simple and straightforward. __Limited novelty w.r.t.the literature__  The novelty is limited. Further, to me it seems the whole MetaBalance method is a direct adaptation of MAML, with importance weighting added. Otherwise, the improvements are unjustified. Otherwise, the claim of performance gains is not well justified.<|endoftext|>While this paper studies a practical and important problem, the technical parts are not well motivated and experiments are not very sound. However, the proposed method is not well motivated. The authors have claimed that the method is proposed “to simultaneously combine the benefits of training on imbalanced data (prevention of over fitting) with the benefits of training on balanced data (minority class don’t get ignored).” This is not enough to illustrate the rationale behind adopting the meta learning process.<|endoftext|>The paper proposes a meta learning based method to learn under class imbalance. In the data section, the paper only mentions train and test splits but not the validation splits. By doing so, it is expected to learn a model from different subsamples (support set) of the imbalanced datasets while performing well on a balanced dataset. The proposed method is compared with several undersampling/oversampling/re weighting based methods to handle class imbalance.
Reject; rating score: 3; rating score: 3; rating score: 3; The paper proposes the OBM Net model architecture to address this problem and identify all the distinct objects observed over each episode. The key idea of OBM Net to have fixed set of slots which allow tracking multiple hypothesis over time and use an attention mechanism to update the slots with observations over time. "In particular, we train a system to construct a memory of the objects in the environment, without prior models of the robot’s sensing, the types of objects to be encountered, or the patterns in which they might move in the environment. " Since the model is being trained on the agent s input. There are other datasets like CATER https://rohitgirdhar.github.io/CATER/ which setup versions of the problem. Why setup a different problem rather than use CATER or some of the long term tracking benchmarks https://github.com/wangdongdut/Long term Visual Tracking which involve object re association? I am not convinced that the proposed method is a better alternative for associating/clustering objects compared to using pre trained representations on data from the corresponding domains.<|endoftext|>The paper proposes an end to end system for the data association and filtering (DAF) problem. The resulting system is evaluated on several different tasks using synthetic data. As such, the idea proposed in the paper is sensible and has merit. Despite this, there are several issues with the paper in its current state. One of them is the apparent disconnect between the data association based description and motivation in the beginning, and the clustering focused experimental validation. The experiments are very disconnected from the paper s motivation, and their link to the initially described task is unclear. The last experiment, which seems to be more in the direction of the initially described problem does not appear to have an actual DAF method included in the baselines. Given that the proposed method seemed to be replicating a specific DAF method I would have expected to see at least that method as a baseline.<|endoftext|>The evaluation is also not convincing in terms of how the method can handle real world challenges and lacks evaluation of real world data. They propose a neural network architecture that uses self attention as a mechanism for data association to construct a memory of the objects in the environment and demonstrate its effectiveness in a set of illustrative problems. In general, the paper is easy to follow, the evaluation proves some potential of the proposed method. I therefore vote for a rejection. This paper addressed the problem “entity monitoring”. I would expect some highlights on this regard in the introduction and related works. B.All experiments are performed with synthetic dataset. Moreover, another critical point from my view is the lack of evaluation on real world dataset. 2021.3)	He, Shuting, et al."Transreid: Transformer based object re identification." 4)	Zhang, Yifu, et al."Fairmot: On the fairness of detection and re identification in multiple object tracking."
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 5; The problem is to learn by few shots (FSL) when the final domain is highly distant from the source base (i.e.natural, medical and satellite), dubbed single source cross domain few shot learning. In short, the approach consists of three steps: 1) train a feature extracting backbone with the contrastive loss on a (single source) base category; 2) train a masking module to select relevant features for the target domain; 3) fine tuned along with the backbone to give features similar to the relevant ones. In the experimental results, some approaches have been considered. This to justify the meta train meta test cycle, and to make clear that FSL is not just meta learning. Why do we need to apply unsupervised training on the single source domain, instead of a supervised one? Instead of this nested general algorithm, the present one is more a serial 3 step architecture, where each step is done just one time. What is strange to me is the selection of so many hyper parameters, 8. Experiments seem convincing.<|endoftext|>Specifically, it firstly trains a feature extracting backbone with the contrastive loss on the base category data for learning better features. Experiments are conducted on several cross domain few shot learning benchmarks. Paper strengths:  The cross domain few shot learning problem studied in this paper is a fundamental task that deserves further study. The paper is well written and easy to follow. Could it be simplified in the training process? In experiments, most results of the proposed method are satisfactory.<|endoftext|>There are three steps: 1. The paper is well written and the easy to read. The authors have clear motivation on each step of the proposed framework. The feature selection module is also designed with a clear motivation for cross domain few shot learning. 2.No standard deviations are reported in the tables which is important for the evaluation of the few shot learning due to the randomness. How about using negative features for fine tuning? 2.Another ablation is to pre train supervisedly to show the advantages of self supervised pretraining. In this paper, the authors propose a framework for cross domain few shot learning.<|endoftext|>This paper proposes a framework for cross domain few shot learning. The framework consists of three steps: 1) pre training backbone; 2) meta learn the feature masking network; 3) fine tuning on target domain. Experiments on the CDFSL dataset show this framework outperforms SOTA methods. The positive and negative features separation is novel, which serves as a regularization for fine tuning on the target domain. 2.The algorithm 1 is clean and easy to understand. 3.The performance on CDFSL outperforms several SOTA methods. c) The intuition on the losses in eq.(6) and (8) are not very straightforward.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; rating score: 6; The paper presents a generative model for unsupervised disentangling of shape and identity of images. Overall, this is achieved with a encoder decoder like architecture, where the latent code is divided into a shape and a identity part. Why not use color augmentations for the pose image of the generated pose identity pairs? The text mentions that StarGAN v2 and CLUIT have the best performance but CLUIT is not in Tab. 1 and both DATGAN and SNI are reported to have better FID scores than StarGAN v2. In fact, the numbers reported in Tab.<|endoftext|>This paper proposes a disentangled method for the image translation task, which is meaningful for the computer vision community. The method sounds works but the experiments are insufficient, more experiments should be conducted on high resolution images. [strength]The main idea of this paper is that automatically learn the arrangement mask, and use the vector quantization with spatially adaptive normalization.<|endoftext|>This paper presents PIVQGAN, a pipeline that tackles the image to image translation task. It is not clear why such a module can learn the pose because there is no direct evidence that the model learns a disentangled representation. Does the number 14 represent the same concept in both cases? ICLR 2018**Writing:** The presentation of the paper needs some revision. Overall, the text is not self contained and the concepts used are briefly described.<|endoftext|>This paper presents a neural network module for achieving posture (shape) and identity (appearance) disentangling for image synthesis. 3.The learned (unsupervised) masks are very interesting but their quality is not good enough to be useful as intuitive control signals for interactive image editing. Cons:1.The image contents used in the experiments seem somewhat limited, e.g.aligned faces or animal heads.<|endoftext|>The experiments show the model is able to separate pose and identity and performs better than existing baselines. [2]Richardson E, Alaluf Y, Patashnik O, et al.Encoding in style: a stylegan encoder for image to image translation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 3.Implementation details missed. Which generative architecture does the authors use for E, G, D? Overall, the paper is novel in method while experiments are somewhat weak to support the claims.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; The paper proposes a benchmark of formal Olympiad level math problems focusing on algebra, number theory and inequalities with cross system support on Metamath, Lean and Isabelle (in development). The paper evaluates performance of $\textit{GPT f}$ on Metamath and Lean, and a custom baseline $\textit{tidy}$ on Lean as well. $\textbf{strengths}$  The paper formalizes a decent amount of cross system Olympiad level benchmark of 488 problems. Olympiad level problems are also interesting to both researchers and general public. The inclusion of formalization of a subset of MATH benchmark also enables comparing provers in formal and informal format. The paper is well written with good literature review on theorem proving benchmarks. Will this benchmark in some way skew the research direction of the community to only focus on developing algorithms particularly suitable for solving these types of problems that may or may not generalize well to other types of problems such as geometry problems?<|endoftext|>This paper presents miniF2F, a test suite of Olympiad level problems of theorem proving that is implemented in Metamath, Lean and Isabelle. MiniF2F contains 488 individual theorem statements that are formalized from Olympiad math contests. Strengths: (1) Since previous benchmarks of ATP mainly focus on basic math theorems, miniF2F fills the vacancy of the contest level test suite for verifying the performance of theorem provers. (2) The cross system design of miniF2F provides a good way to compare different formalizations and proving systems. Overall, I think miniF2F is an important benchmark that could help the community advance the research of theorem proving. I recommend accepting this paper.<|endoftext|>The authors present miniF2F, a dataset of formalized mathematical problems drawn from diverse sources including IMO, AIME, AMC, undergraduate, and high school problems. The formalization is done in Metamath, Lean, with efforts for Isabelle ongoing. The methods the authors apply on the dataset are fairly state of the art and serve as a good baseline for someone wanting to make further progress. I do however think that some more analysis would be worthwhile. In particular, I think the authors should add the following(1) Breakdown of the performance on the problems sourced from the MATH dataset by level of difficulty. (2) A qualitative analysis of what kinds of problems the baseline models fail on and whether they fail on similar problems. However detailed analysis of the baselines on the dataset is lacking<|endoftext|>This paper presents a new formal mathematics benchmark consisting of 488 statements expressed in three prominent theorem proving/verification systems. Baseline ATP systems, notably GPT f/PACT in Lean, are evaluated on this benchmark. Strengths:  The advantages of this benchmark are that it is cross system and that it covers a variety of mathematical topics at the Olympiad level. This is the first effort to unify various Olympiad topics in one dataset, and the problems cover a wide scope of tactics and difficulties. Weaknesses / questions:  The benchmark is not really as cross system as claimed in the abstract. Only 12% of the training statements are available in Isabelle. How were the Olympiad and Custom problems chosen?
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; The paper presents a new deep neural network architecture for 3D point cloud representation learning, based on wavelet decomposition. It s ok, but the paper will be more convincing if the authors can analyze the features learned by their wavelet transform, and show how the learned features are better or different from prior works that do not use wavelet transform. In addition, why doesn t Table 2 also show the result for  AWT Net + V ? It s unclear to me what s the network architecture after ablating all the modules. Is there any other way to demonstrate the effeteness of using wavelet decomposition?<|endoftext|>This paper presents a novel framework for 3D shape representation learning, which is based on multi scale wavelet decomposition. The strengths: 1) The paper is well organized, the experiments are sufficient demonstrating the effectiveness of the proposed framework design. 2) Relying on wavelet decomposition for 3D shape learning is theoretically sound and interesting. 3) The authors made many efforts on the framework design, like the proposed new adaptive lifting scheme, the transformer etc. It seems the performance is just comparable with existing methods. This brings a doubt to me: is it really necessary to use wavenet, a relatively new framework?<|endoftext|>This paper proposes a novel 3D point cloud representation learning framework. In summary, this paper proposes a method for processing point cloud using a lifting scheme. Proposed to use the lifting scheme in point cloud processing, using graph convolution networks and transformers as backbone. Strengths:  The authors propose a novel framework for point cloud processing, which is inspired by the lifting scheme in wavelet transformation. Extensive experiments on the performance of the model, especially w.r.t number of scales, point sparsity and rotations. Though using the lifting scheme is novel, it is less motivated during the narrative of the paper. I think it s introducing more and more redundant representation, but not necessarily redundant information. Is it because the more scales you have, the more over parametrized you are and therefore more prune to overfitting?<|endoftext|>I think that the claims of the paper would be even stronger if the authors added an additional evaluation on the more challenging S3DIS dataset. Subsequently, two transformer models are used to refine the coarse and approximate geometry of the 3D shape. However it is not thoroughly discussed in the Related Work section. What optimizer did the authors use? What was the learning rate? However, the authors should provide them in order for the results of the paper to be reproducible. So how under which configuration was the performance of Table 1 achieved?
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 5; The paper proposes and addresses the problem of graph condensation. In a nutshell, provided a large graph G, the scope of the paper is to propose a solution able to generate a smaller synthetic graph G’, which effectively allows to train Graph Neural Networks (GNNs) able to achieve similar performance as if they were trained on G. In order to do that, the authors propose to match the gradients produced by a specific GNN on mini batches extracted from G and G’. 8) In Table 4, surprisingly a graph condensed with GCN is not the best one for training GCN (the graph condensed with SGC allows to achieve 10% more in terms of accuracy with GCN). The authors evaluate the quality of their approach on the Cora, Citeseer, OGBN Arxiv and Reddit datasets, condensing and evaluating the new graphs with a variety of different architectures. Overall, I personally found the idea of the problem presented in the paper intriguing. This said, I’m not sure the solution proposed in the paper fully addresses the problem. Updated score to 6 after author s rebuttal.<|endoftext|>It first constructs a much smaller synthetic graph, and then train GNN models on this small graph. Empirical results show that, graph condensation can reduce the size of the graph by 99% but still achieves comparable accuracy. The idea of using data condensation/distillation on GNNs is new and interesting. It is impressive that the compression ratio can be more than 100, and the testing accuracies remain competitive in many cases. This generalizability is a useful property of this technique. 4.The paper is well written. Although applying condensation techniques on GNNs is new and some technical details need to be taken care of, the main ideas still follow previous work, e.g.the gradient matching loss. However, to construct the smaller graph, one still needs to train a GNN model on the original graph, which still requires sampling methods for large graphs. I think the authors should provide more discussions on the computation costs of the condensation process. 3.On more complicated datasets such as OGB arxiv, the accuracy gap is still quite noticeable. On the other hand, given previous work on data condensation, the novelty of the current method is not significant.<|endoftext|>This paper studies the graph condensation problem for GNN training, in which a large graph is condensed into a small synthetic graph, and the GNN trained on the small graph is expected to perform similar to the GNN trained on the large graph. Strength:  The paper is easy to follow and overall well organized. The innovation is limited, but the setting is new for applying this method. The authors claim that it is important to parameterize A  as a function of X . However, experiments are not conducted to support this very important claim. What s the performance of this approach, in terms of both the optimization efficiency and the test performance? Experiments use GNNs with 2 layers. What if more layers are used? Con: The method is similar to the existing gradient matching method, but the graph setting is new. Con: Experiments can be improved:  Ablation study is needed to verify the effectiveness of the proposed parameterization. post rebuttalThe new experimental results in the revised paper have addressed my concerns so I choose to raise the score to 6.<|endoftext|>This paper introduces GCond, a graph condensation framework designed to compress graph datasets and reduce storage and time requirements while training GNNs on large scale graphs. This paper is generally well written and easy to follow. As a result, I believe this work lacks innovation in terms of technical contribution. Dataset condensation with gradient matching. This observation leads me to wonder if the proposed method is better suited for *dataset* condensation rather than *graph* condensation. 3.The paper makes no attempt to analyze the algorithm s complexity or to report its runtime. 2.On page 4, line 1, it appears that the phrases  [...] and unrolling the entire training trajectory of the inner problem  have been omitted or repeated. The paper s idea is interesting, as evidenced by the experimental results. However, the theoretical depth and novelty may not be enough to meet ICLR s standards.
Reject; rating score: 3; rating score: 6; rating score: 6; The paper shows reduction in bias on few datasets and a small degradation to the GLUE benchmark. To me, this paper is no different from prior work in this aspect. Also given most of the differences are quite small, I am not sure whether the paper is making meaningful progress. The proposed approach is quite limited in the setup. Also, as described in section 5, it is not scalable for words that need not be debiased. [1] https://aclanthology.org/2021.acl long.81/[2] https://arxiv.org/abs/1903.03862Overall, while there are merits in proposing attention based debiasing, the evaluation methodology is weak.<|endoftext|>This paper proposes a new debiasing method for contextualized word embeddings, specifically for attention based text encoders. More specifically, it attaches a sequence of words from different social groups to the original training sentence, which makes it very controllable to adjust the attention scores assigned to specific words. The experimental results, especially in Table 4, are not very convincing. The proposed method seems only work well on reducing race bias. Gender bias seems to be the hardest one to mitigate across all methods and Kaneko is doing better on reducing religion bias. 3.I am not fully convinced about the motivation (intuition) in Sec.3.1.Figure1 definitely presents some good examples of bias in encoders but it is hard to conclude that bias is mostly from the encoder. E.g."man" comes first or "woman" comes first. I would recommend to accept this paper.<|endoftext|>This paper addresses potential biases introduced by attention modelsby re weighing the attention weights. For evaluation, the paper shows the performance of severaltransformer based models. The modelobtains competitive performance for the GLUE tasks, while reducingbias compared to original models and a couple of related works. I would have liked tounderstand how much the performance of the model (perhaps on the GLUEtasks) is changed if such a teacher model is not used. As recent studiesshowed, intrinsic measures of bias do not necessarily correlate withbias measures on a concrete, down stream task that can be used in anapplication. I would have liked the evaluation of the model toinclude a task for which group annotations exist and the bias for theoriginal and modified model are presented. You may want to take a look at this paper that lists some of the problems with benchmarks such as StereoSet:https://www.microsoft.com/en us/research/uploads/prod/2021/06/The_Salmon_paper.pdfI am not sure if I understood the need to separate the original inputfrom the artificially constructed one.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; This paper presents a graph neural network architecture for analyzing dynamic brain imaging data. The proposed method is tested on a multi task fMRI dataset and demonstrated better performance in both ablation studies and comparison to state of the art GNN methods. Also, how are the graphs constructed for the baseline model   are the same connections used or did the authors vary connections per sample like in their approach? And are the connections based on structural or functional data? The authors compare to GNN only baselines, but not TCN only baseline. I think it would make sense to compare to TCN models given (over) half of their network is based on TCN, and also given the large gap in performance using GNN only models; this may point to the feature learning in the TCN layers as being very important compared to the GNN component for classification learning.<|endoftext|>This paper proposes a method of graph neural network to jointly models dynamic functional signals and structural connectivities, to learn a good deep representation of brain dynamics. These improvements have also been verified in experiments to learn better temporal and spatial importance sanity. 2.The improvement of several methods on graph neural network in this paper is mainly based on some previous work. But the main contribution is to make some methodological adjustments for brain imaging, which greatly reduces the innovation point of this article. This article improves the network method of graph neural network to learn a better representation for brain dynamics in the application of brain images.<|endoftext|>This paper proposes a deep learning method for temporal data on graph nodes, specifically designed for brain imaging data. Strength:   Clearly written and easy to follow. It is not clear why having individual adjacency matrix implies a latent graph cannot be learnt. How was this problem dealt? The paper is mostly clear, but I do have some concerns as mentioned in the Main Review above which can be very critical.<|endoftext|>This paper proposes a Graph Neural Network model to estimate latent dynamics in the human brain using functional Magnetic Resonance Imaging (fMRI) and Diffusion Weighted Imaging (DWI) while performing a classification task. 2) The model is not straightforward to understand in one go.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; The paper presents a new approach to perform transfer learning between heterogeneous architectures and tasks by introducing a new algorithm based on adversarial multi armed bandit, which automatically learns a mapping between source and target representations, as well as the way to combine such representations. The algorithm is relatively easy to implement and results should be reproducible. Code is provided. If this is the case, the narrative of the paper should reflect the finding (from the beginning of the manuscript). This observation affects the consistency of the paper and triggers multiple questions that need to be addressed during the rebuttal period. The “full” configuration, in which the multi arm bandits’ algorithm has to select these operators comes in second place. As a follow up to the previous question, could the authors provide additional details on the “fixed configuration”? The paper presents a nice narrative about the adversarial multi armed bandit approach, highlighting the benefits of the routing of representations in combination with the dynamic selection of operators to combine and transform features. However, experiments seem to hint at a different reality, where fixing these operators is indeed more beneficial. Based on this, I consider that authors need to present stronger evidence that supports the value of the approach, as it is currently presented in the paper, or they need to update the narrative of the paper to account for the empirical findings.<|endoftext|>To transfer knowledge between heterogeneous source and target networks and tasks, this paper proposes a novel adversarial multi armed bandit approach (AMAB) which automatically learns to route source representations to appropriate target representations. It combines feature representations received from the source network with the target network generated feature representations via various aggregation operations. The work is interesting and makes sense to some extent. This paper only compares with methods used in L2T ww. However, L2T ww is a paper proposed in 2019, which cannot represent the best results now. Some more work, such as SNOW[1], has been proposed for knowledge transfer. 3.Since the paper is similar to L2T ww, a method proposed in 2019, it is better to compare all results showed in L2T ww. And more new benchmarks are needed. Overall, I recommend rejecting the current version of the paper.<|endoftext|>The authors propose to use an adversarial  multi armed bandit to decide where, what and how to combine outputs from different layers of the two networks, improving on the recent line of work that achieves transfer by enforcing closeness between the representations obtained by the two networks. For each layer, the proposed method introduces some additional parameters in the form of intermediate representations and parametrized ways to combine such intermediate representations with the source network s output. The multi armed bandit is trained to choose which intermediate representation and which aggregating function to use. The authors compare the proposed approach with some previous work, showing improvements, and perform a qualitative analysis of the results. Strengths:* I like the principled approach to letting the model figure out what information to transfer. And more recent baselines? How do different parts of the proposed method impact the overall effectiveness? How does the choice of number of representations influence the performance? Are these learned intermediate representations meaningful? * The authors mention the ability to decide whether to overwrite the target s network own information as an advantage of their method. It seems like giving the option to let the network choose the aggregation operation resulted in always the same choice, Minor:* Some experimental details are missing, for example how many random initializations where tried to compute the variance over the reported accuracy? While the authors address an important problem with an interesting technique, a more principled evaluation would be necessary for this work to be complete.<|endoftext|>The idea is to wire in a certain way the intermediate representations of the source model into those of the "target" one. On one hand, it can help to save lots of energy and compute by training once and benefitting many times from it. In some sense, by paying in terms of inference cost and storage, we can t forget the original representations. So far, the golden standard is to finetune the model on the new dataset (maybe after distilling to the target architecture if needed, and possibly by applying some additional regularization to avoid overfitting or diverging too much from the original model). This paper proposes an alternative. There are many ways to use the source predictions and representations in a potentially useful downstream manner. Regardless of the cost and complexity of finetuning, the proposed approach requires that during inference we ll need to apply both networks (source and target). At the end of the day, these considerations probably matter a lot in real practical systems. The former is a discrete optimization problem, the latter is a continuous one. It would be extremely informative to see this comparison as I think it is the cleanest one. Moreover, the direct finetuning should be much faster; can we also see a time matched comparison (i.e.train for the same amount of time the RN X to RN X finetune transfer and the Auto Transfer approach)? Can we use a simple UCB?
Accept (Oral); rating score: 8; rating score: 8; rating score: 10; This work extends CoPhy, in which the author(s) address the problem of predicting counterfactual outcomes of physics based tasks from pixel space (CoPhy used ground truth object positions). The author(s) also propose a new benchmark (built upon CoPhy s benchmark) for counterfactual prediction (after intervening the initial set of objects in the scene) which satisfies the Identifiability and the Counterfactuality constraints of causality. Strengths:1) Filtered CoPhy seems to be the next intuitive step from CoPhy from a modeling perspective. This would give at least an upper bound for the proposed benchmark. Overall I feel that the paper is a strong contribution towards visual counterfactual reasoning (making the framework unsupervised as opposed to CoPhy) and would strongly urge the author(s) to add the experiments mentioned in the Weaknesses section. Based on the novelty of the paper, I am voting for a weak acceptance of the paper. Post rebuttal decision:After reading the comments of the authors as well as reviews of the other reviewers, I m very much satisfied with the additional experiments added in the paper and I believe this paper would be a good contribution towards unsupervised visual counterfactual prediction.<|endoftext|>The new form of representation allows the model to be trained in an unsupervised manner only with the supervision of RGB images, as opposed to the training procedure with the supervision of object positions in CoPhyNet. the newly designed representation (i.e, high dimensional features + 2D keypoints + coefficients) is shown to facilitate unsupervised learning from pixel space. The paper provides good empirical results and extensive ablation studies for the effectiveness of each model component. The proposed method is only compared with two existing approaches, including PhyDNet and a modified version of V CDN. All experiments are conducted in a simulated environment based on CoPhy, which is somewhat insufficient compared with previous video prediction literature (though I understand the paper studies a new problem). This paper explores an interesting problem of learning counterfactual physics from pixels. One of my concerns is that it does not provide sufficient empirical comparisons with existing video prediction models other than PhyDNet and the modified V CDN. Furthermore, I would increase my score if the effectiveness of the method can be validated in a real world dataset.<|endoftext|>It uses sensible benchmarks including previous methods and common sense baselines where either the original or counterfactual input is used as a prediction, and does relatively well on this challenging problem, although the video on the website shows that there is still plenty of room for improvement. The paper introduces a testing approach, dataset, and method for counterfactual video predictions, using 3d physics simulation videos. I would be happy to list them if this would be beneficial to the authors; I assume this was due to time limitations. This paper introduces an ambitious new baseline for counterfactual 3d physics predictions in pixel space.
Reject; rating score: 1; rating score: 5; rating score: 5; rating score: 8; This paper proposed a new method to deal with batch size invariance for policy optimization. So, what is the contribution of this paper? I doubt the value of this work. By comparing Figure 1(b) and Figure 1(c), we see that the vanilla PPO algorithm performs better than their proposed algorithm!<|endoftext|>This paper develops two new algorithm variants that provide a way to achieve batch size invariance. Empirical results show that the methods are somewhat effective at providing batch size invariance. The results do indicate that there is some batch size invariance present. We leave further analysis of this to future work." I could not find it. There are ablations trying to show what components of the proposed algorithms are necessary for batch size invariance. "This demonstrates that the decoupling the proximal policy from the behavior policy is safe and useful."<|endoftext|>This paper studies the method to achieve the batch size invariant for policy gradient algorithms (PPO, PPG). The paper achieves this by decoupling the proximal policy from the behavior policy. The writing is clear, and the method is easy to understand. Can you give a formal analysis? However, this method seems not to be theoretical justified. This paper presented some interesting ideas of batch size invariance of policy gradient.<|endoftext|>The paper proposes decoupling the behavior and proximal policies used in policy optimization algorithms such as PPO and PPG. However, it fails to motivate a single objective or problem which is addressed by the proposed method. Typically, the behavior policy itself is used as the proximal policy, i.e.the policy to which updates need to be close to (maintain trust region). I particularly like that the authors do not oversell any of the results. On the other hand, I do not understand the main takeaway from the paper. There are two clear contributions here, one of the decoupling objective and the other of achieving batch size invariance.
Reject; rating score: 3; rating score: 3; rating score: 5; This paper presents an approach that trains local Hebbian learning rules to allow a neural network to perform reasonably well in two types of problems: sequence memorization and prediction. Two approaches to train these models are compared, which are based on meta gradients and evolutionary strategies. The introduction describes the approach as a HyperNetwork, in which a trained function is responsible for the weight updates (also mentioning related work such as HyperNetworks/CPPN), but then only later in the paper is the actual Hebbian approach described.<|endoftext|>This paper aims at using evolution strategies instead of back propagation for evolving the parameters of weight and activation updates for an online learning sequence model, specifically a next character prediction model. With such experiments, the empirical significance could be improved drastically. 19.In the abstract I would suggest to write “recurrent neural network” instead of just “recurrent network” for clarity. 20. n^l and n_l are the same because of a typo, I assume? The paper is very hard to read and suffers from missing clarity. The experimental evaluation is insufficient. Otherwise how would learning to process the current data help in the future data?<|endoftext|>I liked the use of multi dimensional activations that are then locally combined, as well as the use of different activation channels in the weight update. The authors design and apply recurrent neural network models with "fast weights" to sequence compression problems. The paper focuses on a long sequence working memory problem and additionally presents some results on language modeling. Approching sequence learning problems with fast weight architectures is interesting, but the experimental results are not yet convincing enough and the clarity of presentation should be improved. Moreover, training details are missing. My second concern is related to how the paper itself is presented.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; Firstly, I d like to thank the authors for an exceptionally well written paper   it was a joy to read! In particular, I found the discussion on the shortcomings of prior modification enlightening and convincing. I am also not entirely convinced that the transformation should be seen as being shared between the encoder and decoder. I understand why the authors would like to push this view, since the introduced inductive bias, by definition, should improve the learnt representations, which may then be used for downstream tasks. I would very much like to see this discussed further somewhere, though it may be outside the scope of the paper. I didn t find any technical or mathematical issues in the paper, but there aren t many equations either. The paper is exceptionally well written and presented, and while it would be a stretch to call it a groundbreaking paper, it is a solid "accept".<|endoftext|>This work introduces a new method to embed used designed inductive biases into VAE architectures. the basic idea is straightforward: a fixed user defined deterministic transformation is used to map the latent z into a "structured latent" y. Technically, this is identical to using the transformation as the first layer of the decoder architecture. The approach is shown to offer an interpretable latent space and attractive performance in a large class of density estimation tasks. In my opinion the main contribution comes from these layers rather than the general technique. There is a major need of methods for embedding human knowledge into deep learning architectures and this paper is on a good direction.<|endoftext|>However, I am still not fully convinced how to properly choose the transformation $g$ and what is the recipe for that. The paper proposes to incorporate some inductive biases into VAEs by modifying decoders. Strengths:  The idea of modifying decoders is interesting. Weaknesses:  I do not agree with the statement that the prior in the VAE framework is about regularizing. Therefore, I find the paragraphs on the first page not fully convincing. It breaks the flow of reading the paper. 4.1 and 4.2 cannot be applied to any decoder. I fully understand that it is interesting to see how the inductive biases could be used and that the resulting distributions are of a specific form. I think the paper presents some interesting ideas, however, it is lost by not necessarily a good way of presenting it. UPDATE  I would like to thank the authors for their rebuttal. Indeed, some of my concerns were fully addressed and the paper looks better now.<|endoftext|>The technique is reasonable, and the experiments are with a decent comparison with baseline methods. My concern is on the novelty of the abstract proposed framework, InteL VAE. The authors emphasize that $g_\psi$ is "shared" by both decoder and encoder, and I do understand the intention of such a stance (i.e., the output of $g_\psi$ can be regarded as a transformed latent variable). Designing a decoder based on some prior knowledge of data is a common practice, and thus the abstract framework of the proposed method can hardly claim significant technical novelty. So, I would suggest pivoting the main claim of the paper on these particular instances of $g_\psi$, rather than on the abstract framework of InteL VAE. For example, it would be great if the multi modality $g_\psi$ is enough discussed in the main text, rather than being deferred to the appendix. While I think this is solid work, the current presentation is not necessarily kind for readers to understand the technical contribution specific to this paper.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; This paper presents 3 different solutions to allow MLB based deep NN to cope with variable input. The proposed solutions are based on either a modification of the gating mechanism or replace the gating mechanism with FFT. Therefore, even if the experiments seem rigorous,  the lack of rigor in the writing casts a doubt on the results. The reference to previous or related work is scarce and the results of a standard ASR methods that can deal with variable input length should be reported for comparison. The quality of the paper is not sufficient for publication in ICLR. * `C MLP achieves competitive performance with Transformer based model with only 57.1 % of its parameters`, but after computing the value, it seems to be `57.4 %`  * Should be `Table 2` not `Table 1` in section 4.4  * C MLP  is the best model based on table 1 but is not presented in  table 2  * In table 5, RTF is not defined  * In the claims (abstract), it is said that the proposed architecture reduces WER by 1.9/3.4 % but this result seems to be mainly due to the tiny attention module, which is not clearly stated.<|endoftext|>This paper proposes three architectural modifications on recently introduced MLP based neural networks, such as MLP Mixer or gMLP. The proposed three modifications are: C MLP (w/ depth wise convolution), TS MLP (w/ shift operator), and F MLP (w/ Fourier transform and w/o gating). The authors evaluated their model on two ASR datasets, LibriSpeech 100h and TedLium 2, and achieved better WER compared to Transformer based models with a similar number of parameters. (1)	The novelty of the paper is somewhat weak. C MLP, which exploits a depth wise convolution for time axis feature aggregation, is a commonly used approach in ASR.<|endoftext|>This paper presents an approach for Automatic Speech Recognition based on MLP architectures. Those architectures were recently proposed for image classification and yielded promising results. In this paper, three new MLP based architectures able to handle variable length sequences are proposed. The approaches are then evaluated on the Librispeech and Tedlium corpora and are compared to baselines from the literature. The proposed approaches are shown to yield better peformance than the baseline while keeping a low complexity. Weaknesses:  The clarity of the paper is quite bad. The author should improve the writing, as these sections are critical to understand the paper. SGU is used in Section 2.1 but only explained in Section 2.2. It s confusing.<|endoftext|>The paper proposes three mlp based structures to deal with sequences of variable size. Randomly?you may want to elaborate more on the term "tiny attention"  have you run the conventional CTC model? Would be good to see that result as baseline too. While it seems the paper presents technically interesting topics, it requires major revision in terms of writing. It includes many grammatical errors, wrong references and punctuation are not respected.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; Present a method to measure the expected robustness of a neural network model, by determining  the probability that a random input perturbation might cause misclassification, providing formal guarantees regarding the expected frequency of errors that a trained model will encounter after deployment. The method can be applied black box. Applied the approach to compare the robustness of different models, and measure how a model’s robustness is affected by the magnitude of input perturbation. How exactly are the perturbations generated, what attack, what attack parameters? This should be clarified in the text.<|endoftext|>3.The method can be implemented in a model agnostic fashion. This paper has several drawbacks, which are detailed as follows. Although the authors emphasize that the proposed method uses random sampling to evaluate robustness and obtain a statistical robustness score. The resulting bound is also similar to the proposed local sampling technique. 2.The experimental results are not convincing. Second, the authors did not compare to existing methods, which makes it difficult to assess the quality of the proposed method.<|endoftext|>Summary:The paper introduces a statistical method to measure the robustness of deep neural networks. In the abstract, "adversarial inputs" is defined as: "small input perturbations that cause the model to produce erroneous outputs." This happens several times, making the definition of adversarial inputs very confusing. Code is provided for reproducing the experimental results. "CONFERENCE SUBMISSION" should be removed from the title. I like the idea of measuring the probability of random inputs being adversarial instead of finding the extreme case. 2.The proposed method section is not clear. If the authors can address my major concerns in the rebuttal period, I m ok to increase my score and accept the paper.<|endoftext|>The paper presents a statistical method   Robustness Measurement and Assessment (RoMA) to measure the expected robustness of a neural network model. The robustness is defined as the probability that a random input perturbation causes an incorrect prediction. The reviewer is not convinced that "categorical robustness" is a new notion. In this paper how was λ selected? Could authors clarify what is meant by uniformly distributed?
Reject; rating score: 3; rating score: 3; rating score: 6; rating score: 6; The setting is robust learning to perturbations at test time (adversarial examples), where instead of minimizing the average loss, the goal is to minimize the loss of the class with the lowest accuracy. That is, instead of a min max problem, there is another maximization component over the classes. The authors suggest a method from online learning   exponential weight in the bandit setting (that is   Exp3). I m not sure what do you mean by saying that the adversary is "choosing the class". It doesn t choose a specific class (although some might be more vulnerable than others) The error across classes is not perfectly uniform in the adversarial training process, but this is the case (sometimes) in the standard setting. In the suggested method, you can perform exponential weights with full information, I don t see the reason for using bandit feedback (and it is not explained). What about the following baseline   you have the empirical error, so perform an ERM with that "penalize" more some classes. I mentioned my concerns. It is possible that I did not assess the contributions correctly, I am willing to read the other reviews and reconsider my score if they convince me otherwise.<|endoftext|>In addition, the paper proves a convergence guarantee for the worst class loss and provides empirical evaluations of their method on benchmark datasets. While the proposed method is intuitively reasonable for the main purpose of “protecting the weak”, I have several concerns regarding the motivation of the paper and the empirical results. Perhaps, if we assume attacking difference class incurs a different benefit or cost for an attacker, there may exist a particular user case for protecting the weakest class. Both the average clean and robust accuracies of CFOL are less than those of FOL by 0.04. The proposed method combines an online learning algorithm and an adversarial training approach, which is intuitive and new. In addition, the baseline results presented in the paper are far below the state of the art and there does not seem to be a clear advantage of the proposed method over existing approach. Therefore, I suggest a reject for this paper given the abovementioned concerns.<|endoftext|>In this work, the authors present a method for maximizing minimum class wise accuracy in the $\ell_p$ adversarial setting. The proposed method uses an online learning algorithm (exponential weights) to choose classes adversarially at train time to sample examples from (in place of randomly choosing examples from the given data distribution). The authors find their algorithm can effectively increase the class wise worst case accuracy on a number of standard datasets (CIFAR 10/100, STL10) in the $\ell_\inf, \epsilon 8/255$ threat model. Canonically ERM also refers to the standard training regime. The standard adv training regime should therefore not be referred to as ERM (this a problem throughout the text). Table 2/3: $\mathcal{A}$ canonically refers to an algorithm, so $\mathcal{A}_{\text{rob}}$ is confusing notation (one would assume it refers to a robust training algorithm); this should be renamed to another symbol and perhaps labels could be added to the table for further clarity. In Table 2/3 it would be good to cite the methods/name them as they are not standard acronyms people use. What do the bounds in Table 2/3 represent exactly? However, the paper has relatively minor issues with clarity and incomplete related work.<|endoftext|>This paper points out that there could be applications where the worst class accuracy could be critical   for example, the class with the worst class accuracy can be the main target for the attacks and that the difference between the worst robust test accuracy of a class and the average robust test accuracy can be large (also pointed out independently in Tian et al.(2021)).Motivated by this, the paper proposes a solution to use the Exponential weight algorithm (Auer et al., 2002), where, as the empirical distribution is taken the adversarial distribution over classes, resulting in a method called Class Focused Online Learning (CFOL). The authors mention that a single model suffices, but if I am correct there are no results with an ensemble. The abstract leaves it somewhat unclear what is the online learning aspect of the method (included in the title), maybe adding a sentence would be nice. It would be interesting to see if this observation from Fig.1 is (not) a result of AT. This raises the question if the proposed method works well only in this setup with TE, and for completeness, it would be nice to point out this to the readers. Could the authors provide results without TE on CIFAR 10? Nonetheless, I found the paper very interesting and I think it is relevant for the ICLR audience.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 6; The author proposes a method for forecasting in Partial Differential Equations by coupling Finite Element Method on an arbitrary grid with the learning of the dynamics from data. For this purpose a variant of message passing based graph networks is used. It is show in the paper that it s possible to incorporate priors on the structure of the PDE that results in an interpretable solution. The method can clearly have practical value, and the discussion of the method is clear and quite detailed. This is one of the strongest gaps in the table. Why or why not? The paper give valuable contribution, the method expected to be practical, robust, and in some cases interpretable.<|endoftext|>This paper proposes a new model for learning partial differential equations from data. Strengths:  The paper is well written and well presented. The model disentangles the dynamics into a convection term and the remainder. Is the mean temperature the mean temperature of the entire sea or of a region, say $1m^2$? Rigorous tests on the proposed model are carried out against robust baselines. Therefore I recommend acceptance, with a few clarifications to be made. Saying what the model is good at and bad at.<|endoftext|>The paper proposes a graph / simplicial neural network based on the finite element method for learning dynamics from data when only a finite number of samples exist and the true dynamics are not known or only partially known. The introduction does a good job motivating the work and pinpointing the main challenges of learning dynamics from data. The connection that the paper makes between the finite element method and message passing neural networks is interesting and, to the best of my knowledge, original. The authors show how inductive biases can be added to the model by using a certain prior over the structure of the function $F$. Therefore, I recommend the paper for acceptance.<|endoftext|>This manuscript proposes a new graph neural net (GNN) method to learn the dynamics of a spatio temporal PDE driven dynamical system directly from data. The authors propose to do that using the finite element method (FEM). The proposed method builds on using: basis function approximation for the (unknown) field u, Galerkin method with the assumption that discrepancy between the dynamics F and basis function approximation is orthogonal to (finite) basis functions, method of lines, and message passing GNN as a proxy for the dynamics. Authors also propose a method to incorporate inductive bias into model learning for models that are assumed to contain a convection component. Overall the proposed method is well motivated, and for the most part the description is clear. Is $u$ defined based on the dynamically evolving system state or by data?
Reject; rating score: 6; rating score: 6; rating score: 6; This paper as the name suggests, tries to figure out whether and through which ways bpe can affect Transformer’s memorization capacity. It evaluates Transformer’s memorization under these 4 settings: memorizing random synthetic data, memorizing random labeled natural language data, recognizing training data with lower output entropy, and training data recovery QA. Experiments show that if we have more merge times for bpe (larger vocabulary size), the model’s performance on all the four settings would improve on 3 architectures, which shows that bpe indeed can affect the model’s memorization capacity. By excluding two other hypotheses, the paper concludes that more merges in bpe results in shorter input sequence and that makes memorizing easier for transformers. Or, are there ways to directly prove that it is the sequence length that makes a difference? Thorough empirical study on whether BPE affects memorization in transformers. While the final conclusion, shorter input sequences lead to better memorization, is not well supported by experiments.<|endoftext|>This paper studies the memorization properties of an NLP models conditioned on how large is the vocabulary size. They concentrate on widely used BPE algorithm in order to split original data into subword units. Experiments empirically validate the change of memorization properties as the vocabulary size is changing where it exhibits better memorization with larger vocab size. Further authors make multiple hypotheses of what might be the underlying explanation hidden behind the improved memorization. After checking these out they conjecture that the reduced sequence length is likely the major contributor explaining the underlying observation. This work is placed on the border for me as it is investigating well crafted question and does some empirical validation of multiple hypotheses. Authors provide very high level paths such as  Our findings can provide guidance for tuning the learned data structures , but I would be very curious to read more details. which is a bit confusing.<|endoftext|>This paper investigates the impact of subword vocabulary size on the memorization ability of Transformer models. The authors designed three types of tasks to evaluate the changes of model memorization, namely learning mappings with random labels, membership inference and training data recovery. Experimental results show that the memorization ability of Transformer model is stronger with the increase of subword vocabulary size, which the authors attribute to the reduction in the sequences  length. 2.The paper explores the relationship between the memorization and generalization capabilities of the model, and shows that generalization is not directly at odds with memorization, which can inspire future model design. Other factors that are closely related to the BPE vocabulary may be also important, such as the subword frequency. The research questions explored in the work are very interesting and important. However, the experiments are relatively weak, and it is not clear how to apply their findings in practical tasks.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; They show that this is almost as efficient as using ‘old data’, and much more efficient than using ‘new data’ (the normal distillation setting). To balance the different losses they propose a reinforcement learning algorithm that judges the quality of placebos. They obtain good results on several standard datasets. The paper has the following weaknesses:a) Related work: the usage of external data has been studied before in continual learning. Class incremental learning via deep model consolidation. Overcoming catastrophic forgetting with unlabeled data in the wild. This drastically reduces the novelty of the paper. This work also proposes a method to select data from the data stream which could be compared to the proposed RL algorithm.<|endoftext|>The paper uses data chosen from a free image stream (without labels) to supply knowledge (by the "placebo" samples) for class incremental learning. The empirical results shows that the proposed method exhibits good accuracy in large and high resolution image benchmark datasets such as ImageNet 1k with small memory budget. The memory budget of the compared methods is not clearly stated. The rationale behind this claim is that the CIL is for the realistic recognition scenario that the data is given in a stream and the model update should be on the fly, adaptively to the new data.<|endoftext|>It conducts extensive experiments and achieves consistent improvements on CIFAR 100, ImageNet 100, and ImageNet 1k datasets. Pros  The idea of introducing unlabeled placebo data for alleviating catastrophic forgetting seems interesting and novel for class incremental learning. Such extra training data does not require additional data memory. This paper is well written and easy to follow. The proposed method achieves strong performances across multiple benchmarks, in particular for high resolution image classification tasks, with higher memory efficiency for old data. How many samples are taken from the free image stream (|\mathcal{U}|) as candidates? The proposed CIL strategy, which exploits distillation on unlabeled placebo data, seems novel and effective.<|endoftext|>This paper proposes to sample placebo data from a free image stream and use them for knowledge distillation during class incremental learning. The idea of sampling data from a free image stream for class incremental learning is not new, but the RL based sampling algorithm looks new. Experimental results show that the proposed method improves state of the art methods. The first contribution, leveraging "unlabeled placebo data from a free image stream to improve the KD effect in CIL models" has already been studied in a prior work, [Lee et al.].
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 8; Using the synthesized metamers, the authors show that the human ability to discriminate between natural images and their synthetic metamers drops in a similar fashion for metamers generated by inverting adversarially trained CNNs and for metamers generated by an inversion of a well studied model of peripheral vision (i.e., "Texforms") as display eccentricity increases. This may indicate that adversarially trained models bear some resemblance to human visual processing at the retinal periphery. Thoughts:* Overall, the metameric manipulation seems to make a simple prediction: if a model correctly captures human vision for a given eccentricity, then the metamers should not be discriminable by humans when presented at that eccentricity. I agree that this is a reasonable interpretation. * Some of the cited ArXiv citations are outdated and should be replaced with citations of the corresponding conference proceedings.<|endoftext|>This paper presents a psychophysical comparison between a peripheral summary statistics model ("texforms"), adversarially robust and non robust neural networks. Images are generated from each model / network using gradient ascent, then presented to humans at different retinal eccentricities. The story of the paper could also be more clearly constructed. EDIT after discussion: Given report of new experimental evidence that the texform and robust models generate perceptually similar images, I am raising my score from 5 to 8. The paper ends with the interesting conjecture that "peripheral computation may implicitly act as a natural visual regularizer". The key motivating question posed on page 2 ("Could it be that adversarially trained networks are robust because they encode object representations similar to human peripheral computation?") Presumably if they are performing "the same computations" they should? Missing experimental details.<|endoftext|>I thought the topic of the paper was interesting and relevant for ICLR since it probes at a link between the representations learned by adversarially trained image classifiers and the robustness of human peripheral vision, and suggests some possible paths forward (e.g.spatially adaptive computation of continuous representations)2. I think something along these lines should be stated in the abstract to improve readability. I find the assertion that "robust representations capture peripheral computation similar to current state of the art texture peripheral vision models" a little strong. EDIT: following the rebuttal from the authors and taking into consideration their response to other reviewer comments, I am upgrading my score. Is one model more invariant to the other’s stimuli? 3.There are various details missing from the experiments section which I think are important.<|endoftext|>While the idea of connecting summary statistics with the robustness to adversarial attacks is interesting, I feel the paper has a number of points that should be clarified by the authors before it can be accepted for publication at ICLR. 1.Two representations with the same null space are not necessarily the same. They do so by comparing \tilde{x} (synthesized images from networks which are robust to adversarial examples) and \hat{x} (synthesized images from models of human peripheral vision). They argue that given the fact that these classes of images (\tilde{x} and \hat{x}) are equally discriminable by humans from the original images, x, the image representation of robust networks is similar to the (periphery) image representation in humans.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; However, due to the insufficient experiments, especially evaluation on the defended models and target attacks and the sensitivity of the selected layers, this paper is not ready for publish. The techniques include image augmentation, reverse adversarial update on the clean example and reference attack update through interpolation with an automatic parameter selection scheme. The authors proposed / used three techniques to enhance the transferability of ILA on undefended models. The authors only evaluated their method on undefended models. However, no such experiment was provided. 3.The method is very sensitive to the selection of layers in the source model. Once non optimal layers are selected, the attack performance would drop very significantly. 1) VMI SI TI DI FDSM was examined on a black box setting when their method was evaluated on defended model. 2) Why don’t compare the proposed method with it on the defended models? In setup, the authors mention that they use 10 iterations for I FGSM and 50 iterations for Aug  ILA. 4.No experiment shows that the proposed method can work with other methods to enhance overall transferability.<|endoftext|>The data augmentations introduced include 3 kinds: simple transformations (cropping), reverse adversarial update, and attack interpolation. Can Aug ILA be considered a generalisation of any of the previous methods? The generality of the proposed method is not sufficiently demonstrated (only one attack, only one dataset). VNI FGSM performs better than VNI FGSM. However, unfortunately no comparison is provided with other SOTA methods beyond ILA based approaches, which limits the paper s contributions. More specifically, my comments regarding evaluation on new datasets, single step attacks, and defended models are not fully addressed due to the fact that the provided results are only comparisons to ILA based approaches (ILA, ILA++), and baselines such as VMI CT FGSM and I FGSM + LinBP + SGM + ILA, which were among the baseline presented in Table 1 have been missed. additional comments:   in  $L_{inf}$  attacks, it is common to state perturbation size by $\epsilon  x/255$.<|endoftext|>The paper proposed the augmented Intermediate Level Attack (ILA) algorithm to strengthen the transferability of adversarial examples. Also, it claimed that increasing the diversity of input references could improve the generalization of adversarial examples when attacking different models. Specifically, this paper performs augmentation operations, including common image data augmentations and transformations exploiting adversarial perturbation (e.g., reverse adversarial updates and attack interpolation on the reference attack), beforemaximizing the projection of intermediate feature map discrepancies. This paper is well organized. Experiments on various datasets show that it outperforms both the original ILA algorithms and other state of the art methods with a large margin. 2.In Section 3.1, this paper claims that the reverse adversarial update is used to boost the confidence of clean image $x$. It is better to show some visualization results to support this view. The experiments on various benchmarks are strong. However, this paper lacks the support of theory and some views might not be well supported.<|endoftext|>This paper presents a transfer based attack method based on Intermediate Level Attack(ILA). The overall structure of this paper is clear, and  extensive experiment results are provided to support their ideas. Based on the observation from the  previous work, diversity transformation and perturbation interpolation are used to improve the transferability. The experimental results show that the augementation is simple and effective. Especially random cropping with large size causes great damage to the image, making the generated perturbation is extremely incosistent with the original image in pixel. The author expect the reverse adversarial update input can highly activate the intermediate layers and provide a better attack direction guidence. However, the reduction of classification loss in the Appendix A seems obvious. Although the three basic methods proposed are effective and intuitive, there is a problem to be solved: how to choose the index of the attack layer, which seems to have no prior guidance other than referring to existing papers. In summary, I find the experiments satisfying, although the additional experiment show can make the claims more convincing.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; The proposed method is designed to solve the nested loop optimization problem (with an annotator that generates a label and a student network that predicts a label) based on a pre trained GAN that generates images. In particular, the authors solve this problem through a gradient matching approach, and they claim it is much more efficient than the existing end to end gradient based meta learning approach. The problem covered by the paper is important, and a good direction is proposed based on clear motivation. This paper proposes an intuitive method based on clear motivation. In addition, sufficient experiments have been conducted to prove it, and the paper is written in an easy to understand manner. **[Comments After Author Response]** I appreciate the efforts of the authors for the detailed rebuttal. I strongly recommend that the authors will reflect those contents in the final version.<|endoftext|>In this paper, the authors mainly propose a gradient matching based method for part segmentation to reduce the annotation cost. Based on the DatasetGAN, the proposed model also used the Style GAN family to generate high quality images and remove the human annotations on a handful of synthesized images. Compared with semi supervised learning methods, their proposed method achieves higher performance on three datasets. Reducing the annotation cost is a very important thing. In this work, the authors provided a potential and promising way based on DatasetGAN.<|endoftext|>This work deals with the problem of training a part segmentation network by automatically synthesizing pairs of images and annotations. In addition, this work does not require manual labeling of generated images, but requires manually labeled real data. It is well compared in the supplementary. This is on the borderline. Also, large scale experiments and diverse balance setups could have been regarded. What is the relationship of the proposed approach with disentanglement? <Minor comments>  The notation convention: In the paper, $\mathbf{\hat y}$ denotes manual annotation, while $\mathbf{y}$ denotes the predicted output. $\mathcal{D}_l$ is abused for both a real labeled dataset and a labeled dataset of generated images, which may lead to confusion.<|endoftext|>Please clarify at which stage(s) the synthetically generated images are being used in the semi supervised experiments (Figure 2). This paper aims to address this problem by learning the annotation model over annotated real images instead of annotated synthetic images. To this end, the paper proposes a meta learning approach. This meta learning idea is implemented as an approximation by using the Gradient Matching Loss. The motivations and the proposed solution is clearly explained in the paper. DatasetGAN uses the annotated synthetic samples. It would have been very valuable to see an evaluation that shows the detailed analysis on the annotation cost vs final segmentation accuracy (in mIoU), when real images are labelled & used with the proposed approach, in comparison to labelling synthetic images to be used in DatasetGAN.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; rating score: 6; This paper tries to understand the over smoothing phenomenon of Transformer based models such as BERT. The analysis is from the perspective of viewing BERT and Transformer as graph neural networks. Specifically, if the standard deviation of layer normalization is sufficiently large, the output of Transformer stacks will converge to a specific low rank subspaceand results in over smoothing. Then to alleviate the over smoothing problem, authorsconsider hierarchical fusion strategies, which combine the representations fromdifferent layers adaptively to make the output more diverse. An accept has been recommended for this paper for the interestingness of the topic and the novel perspective of theoretical study of BERT by graph neural networks. 2.The concat and max fusion has appeared in the graph paper that author cited.<|endoftext|>Empirically, the authors showed that the token over smooth problem is increasingly severe in deeper layers of a Bert model. The authors revealed that layer normalization in the transformer blocks contributed to the over smoothing issue both theoretically and empirically. Moreover, the authors proposed to improve the over smoothing issue by hierarchical fusion strategies, that is utilize token representation from earlier layers. I feel convinced that layer normalization could lead to the over smoothing issue. •	The second question is about the contribution of self attention block and feedforward block to token over smoothing. Overall, I vote for accepting this paper. I like the idea of establishing an analogy between self attention and the adjacency matrix of a graph model, and I like the theory that establishes the relationship between layer normalization and dimensionality reduction of token representation.<|endoftext|>This paper discusses the oversmoothing in transformer models such as BERT. It provides a theoretical analysis on the existence of oversmoothing in transformers and proposes a hierarchical fusion method as a solution to oversmoothing. Can authors provide further experiments with different datasets or applications to demonstrate the effectiveness of the proposed method? Is it possible to theoretically demonstrate that the fusion method overcome oversmoothing? The analysis of oversmoothing in transformer is a good contribution. The limited empirical evidence and lack strong improvement over oversmoothing by the proposed method are the main limitations of this paper.<|endoftext|>This paper explored the over smoothing problem in BERT from the perspective of graph. Then, the authors theoretically prove the observation through the comparison between self attention and graph. The authors claim that "layer normalization plays a key role in the over smoothing issue, namely, if the standard deviation of layer normalization is sufficiently large, the output of Transformer stacks will converge to a specific low rank subspace and results in over smoothing". Is it possible to theoretically prove that layer fusion can alleviate the over smoothing problem like Section 5? In fact, I notice that Fig.6 has proved cosine similarity between BERT and BERT(self gate) on three datasets. Overall, this paper addresses the over smoothing problem in BERT.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; The paper studies an important problem of tackling with restricted classes. The paper is not very clearly written, with concepts repeated several times and not clear description on some others that are mentioned below. 2.While the problem is interesting indeed, the motivation for the proposed solution is not clearly presented. One such example, a model trained to predict which treatment would be beneficial for the patient would need to be altered if the treatment cannot be offered in the future due to ethical or resource constraints. I would suggest to change this.<|endoftext|>The solution to this challenge is firstly detecting the most related model parameters that significantly affect model performance on restricted classes and then tuning on a small number of examples with the losses of desired classification capability. 1.The motivation of the new setting is not strong. In particular, in what situation there would be only a few training examples available when considering removing information of restricted class from model concerned with privacy? Following the work of data deletion, I feel there also exists an important problem which is ignored. Thus, a careful clarification about this point is required in this paper, especially from the view of class privacy. However, many data privacy related data are also tabular. If this component is quite related to data format, any workaround for this issue? The paper has a weak motivation for the new setting.<|endoftext|>An ablation study is performed on the class relevant parameters and the number of classes that are excluded. However, from the description in the paper, it is not clear why this is a real world problem. It would be beneficial to rate the importance of this application if the authors had provided sources for such cases or a more detailed description of a specific scenario. The description of the identification of the relevant parameters for the restricted classes is missing some details. The presented method of re training a model to forget a specific class is very interesting.<|endoftext|>This paper proposes a novel and practical problem called RCRMR LD, aiming to removel restricted categories from model representations with limited data. I suggest that the authors make more discussion and comparison of the various transformations. 3.Identifying those parameters that are relevant to the restricted classes through ERwP is still heuristic. I admit that ERwP seems to make sense, but some verifications about this claim need to be included. 4.Except for "Related work", I do not find any references in this paper. 2."$N_e$ and $N_r$ refer to the number of excluded classes, respectively." The setting proposed by this paper is novel and practical.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; This paper target on continual learning for the language vision ground task with an additional small network. The network is attached on a CLIP model and it contains 2 linear layers to predict a difference vector of the original text embeddings and scaling value. The network are optimized by support vector regression (SVR). Weakness: This paper tries to use a small network to achieve continued learning. The main idea to achieve it is to predict a difference vector between exit word and pseudo word. This idea is somewhat similar to the idea of GLoVe, which also has a similar difference vector between multiple word pairs. Usually, in a real case, a more complex text is more common than well defined constructed phrases or words. I think more experiments for the different few shot methods on the test dataset will be more convenient. I think the idea of the difference vector is similar to some previous work. However, It does not show that if the method can also be applied to the more complex case, which may be more common in the real life. And the comparison is not so sufficient. The author may compare with more few shot methods to support their contributions. In general, I think the paper still needs to be improved and I recommend rejecting it this time.<|endoftext|>This paper presents a way to take general purpose joint text+image representations, e.g., from CLIP, and transform them for use on a new task, while still preserving performance on the old task. The gist of the idea is this: given a new task, a classifier is trained to decide whether or not a given input belongs to the new task. If it does, then that input s representation is transformed according to an affine transformation. sec 6: "stored training examples have a very small footprint": If I understand correctly, these training examples are not stored in memory during prediction, so is it very important that their footprint be small? The first piece is an affine transformation $A$ that is used to transform an original CLIP embedding $t$ into a new representation $A(t)$ which is better suited for the new task. The second piece is a scaling function $S(t):\mathbb{R}^{512}\rightarrow [0,1]$ that predicts how likely it is that the input $t$ belongs to the new task. However, is there reason to believe that catastrophic forgetting will occur for the settings considered in this paper (nonce words, tan gram descriptions)? In both experiments, they find that their system performs well on the new task, while still retaining the performance on other examples. Straightforward description of the main technical contribution   Weaknesses     As presented, I m not sure if the presented approach has signicant technical novelty. Finally, I m not sure if the proposed solution can properly be called "continual learning" since it s unclear how it should scale with the addition of new tasks. It s common practice for the model to include a fully connected feed forward network for transformation of these input embedding. But it could also be shown explicitly by creating a held out test set of unseen color+shape combinations. I agree that this direction is worth exploring and I think it would yield results that would better fit the description of "continual learning". In a similar vein, it s unclear how this approach could be scaled to multiple tasks, which is a vital part of any continual learning system (see comments below)# Comments and clarifying questions  sec.4: Regarding the function $A(t)   \beta t + m$, could the authors give a brief explain for their choice of a affine transformation function here? Overall, the paper is well written and outlines good directions for future work. However, as is, I do not think it represents a significant enough contribution. sec 5.1: What is the "few shot classifier"?<|endoftext|>This paper proposes a simple new method for continuous learning of language image embeddings. The proposed method, CoLLIE, employs a Transformation module designed to work only on the samples undergoing few shot learning. The first score (3) has been upgraded a little to (5), but the reviewer still leans to reject the paper. The reasons are described in the last post of this thread. This paper is unique in its problem setting because it combines continuous learning with language and image embedded representations. Although the proposed method is simple, the experimental results show its ability to deal with new categories while maintaining the original performance. Although CLIP is a modern and excellent representation learning model, the authors should have conducted similar experiments with other representation learning to show that the superiority of the proposed method is a general contribution to existing representation learning methods. Otherwise, the advantage of the proposed method may exist only when applied to CLIP. In addition, the proposed method is in the form of a product of an adjustment function that performs a linear transformation and a scaling function that outputs [0,1], which is added to the original representation. The authors state that the proposed method is simple, but they do not mention that they propose the same as GLU. In linear support vector regression, the output of [0,1] as defined by the authors as S(t) cannot be satisfied, and negative values and values larger than one may be output. Similar effects may have been obtained when the module is trained so that the vector representation of the text in the negative sample is output as it is and that the vector representation of the text in the positive sample matches the vector representation of the image. It may have worked in this case, but what would happen if these nouns were also included in the Few Shot learning text? The proposed method s behavior would have been better understood if the output of the scaling function alone had been reported, along with Figure 5, for the new pseudo words and the existing words. For example, we could train the few shot training with only red giraffes and then evaluate whether the blue giraffes can be retrieved in zero shot. On the other hand, the novelty of the method is much less than the authors may think, and a similar method is not cited. The reviewer recommends that the authors expand the experiments and resubmit the revised paper to another international conference. ### Final RatingThe reviewer has read the other reviews, the responses from the authors, and the revised manuscript.<|endoftext|>The paper augments a pre trained multi modal model, which encode language and images into a shared latent space. The paper adjusts this by learning a linear transformation of the embedding space. The method is simple but effective for the setting it was evaluated in. If the “Related Work” section is accurate, it is introducing a novel and interesting problem setting. The problem itself is well motivated. The authors propose to mitigate it by keeping more negative examples, however this is not desirable for a continual learning setting. C2) While the authors make an argument that this is a continual method, there are only 2 tasks at any given point   the source task that the original model was pre trained on, and the new task, consisting of all the classes that need to be classified. As a result, I’m hesitant to support this being called a continual learning solution. Overall, it appears that this paper presents a simple method in an interesting direction, and can server as a useful baseline for further research.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; rating score: 6; This manuscript proposes a problem defined as weakly supervised graph clustering. Some variations can be made depending on selecting the targets whose labels are supposed to be predicted. Even though this manuscript reads well and tries to define a new problem and a solution for the proposed problem, I have some major concerns described below. Since there is substantial literature for anomaly detection, the proposed method should have more general applications other than anomaly detection. Unfortunately, it is hard for me to think of realistic scenarios or interesting applications of the proposed method. Overall, the proposed method seems to make a simple problem more complicated. Also, the authors should show whether each of the components plays a critical role in predicting the node labels and analyze how each component affects the results. I reject this paper because (1) it is hard to find practical applications of the proposed method; one can solve the problem more easily in many practical applications, (2) it is not clear how each of the inputs affects the predictions, (3) the experiments do not include diverse real world datasets.<|endoftext|>Authors provide the framework that combines Graph Attention Networks (GAT) and Gaussian Mixture Models (GMM) to tackle the weakly supervised graph clustering model. In particular, the consensus loss, a new loss function designed by the authors, is used for the entire framework. Strengths  The proposed methods outperform baseline models on the given datasets and synthetic datasets. Weaknesses  Despite the complicated process and a lot of notations, definitions, relationships, and explanations for notations are not sufficient or there exist some typos so that the manuscript is not easy to follow. Not knowing the number of clusters is a common problem, but the guidance to tackle that problem is not suggested. Without the clear evidence, a weakly supervised method would not be verified over an unsupervised method. Details  Eq (4) is not well defined. Second, the output of w_c seems to be the vector, while Eq (4) does not necessarily indicate the full Gaussian model.<|endoftext|>This paper investigates the graph clustering problem with the aid of graph labels. A new loss is also designed. Extensive experiments are conducted to show the effectiveness of the proposed method. The problem is well motivated. 2.The proposed method is reasonable. 3.Extensive experiments are given. The comparison methods are not convincing. It is better to employ some recent techniques. 2.The technique is not as novel as the authors claim. 1.There are some closely related works in the literature.<|endoftext|>Most previous studies for graph clustering primarily consider node/edge information and their connections, but ignore graph level side information, which might be easily accessible in real scenarios. Inspired by this observation, this paper proposed to incorporate graph level labels into graph clustering, and formulate the new problem as weakly supervised graph clustering. Based on the new problem, the authors further proposed a simple yet effective framework based on Gaussian mixture model (GMM) and graph convolutional network (GCN) named Gaussian Mixture Graph Convolutional Network (GMGCN) for learning node representations. Experiments on both synthetic and real world datasets demonstrate the effectiveness of the proposed GMGCN approach. Pros:1.The paper is well organized, and generally easy to read. Cons: 1.The current title of the submission seems overclaimed. To me, it will be much better to highlight incorporating graph level side information, which is the key contribution of this paper. 2.Although the graph labels have been shown to bring significant performance gains in their experiments, it is not clear to me what kind of graph labels are effective for graph clustering? But I still have some concerns regarding the title, the choice of graph labels, and the performance analysis.<|endoftext|>The authors propose a new Gaussian Mixture Graph Convolutional Network approach to perform graph clustering with the assistance of graph labels. Experimental results suggest that the addition of graph labels boosts performance over traditional clustering methods, and over multiple instance learning. Strength1) The paper is well written and structured. Four cases were presented, and comprehensive experimental results on synthetic data are presented for each of the four cases. When C_node is unknown, how do we determine it? Given both real world datasets are single graph case, I can t think of an example that puts case 4 in use. 4) It is not clear to me why we need two datasets of GC4NC2 for experiments and the major difference between them. 5) Is there a minimum number of graphs that the authors recommend for the proposed algorithm? The problem is novel and the authors have demonstrated that having coarse grain labels on the whole graph can assist with clustering on a node level. The comments for improvement are mainly on clarifying some details of applying the proposed algorithm.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; The paper also proposes a homogeneous mask learning. Comment:1: The proposed approach is parameter efficient and requires very few parameters for each novel task (assuming that basis masks are learned) 2: The paper follows the idea of SupSup, and the difference is how to combine the previously learned mask for the novel task. Also, the task prediction idea is the same as SupSup. Another contribution is learning homogeneous masks, which may be helpful in many scenarios, but the basic idea is the same. Overall, it seems that the paper has limited contribution compared to SupSup. As compared to the SupSup, the performance of the proposed model seems much lower. 6: The performance of the proposed model highly depends on the number of learned base tasks. Also, we can observe that the homogeneous setting model does not perform well. 7: The results are only compared with the SupSup paper.<|endoftext|>In the current paper, the authors propose a novel way to adapt an existing continual learning algorithm, leveraging principles from transfer learning. More concretely, they learn an initial set of masks (or impressions) from a small number of basis tasks, and they use afterwards linear combinations of these masks in the learning process of new tasks. Therefore, their approach is able  to generalize to new tasks, allowing for scalable and parameter efficient continual learning (with much lower parameter overhead than existing methods). Positive aspects:  The proposed approach is scalable to a large number of tasks with a small parameter overhead than existing methods. It is an incremental work, relying heavily on [Wortsman et al., 2020]  The experimental validation is limited and not convincing. The paper is not compared with any other state of the art methods. Please find below some of my concerns:1. First of all, please state clearly in the introduction in which aspects your paper is different from [Wortsman et al., 2020]2. 4.Regarding the experimental settings:  State clearly, for each dataset, how do you define the tasks. How comes that for PermutedMNIST you have 250 learned tasks and the maximum could be 784 (Table 1)? What is the relationship between number of masks and number of basis tasks: you have one mask per one basis task?<|endoftext|>The paper studies continual learning and that context the important and prominent problem of catastrophic forgetting. Similar to related work the paper achieves this through learning task dependent masks on the neural network representation (effectively generating different sub networks for different tasks). The main contribution w.r.t.related work is that the masks are constrained to a sub space which is spanned by a "basis of masks". The experiments show some reduction in the number of required parameters over a single baseline method. The paper is in general well written and structured. As mentioned by the authors this has been explored in prior work such as PackNet and Piggyback, SupSup. Related work like [1,2] are not discussed. In the experimental evaluation, the paper only compares to a single baseline, SupSup, and ignores the results of all other standard baseline methods mentioned in the related work, including replay and regularization based methods, which achieve significantly higher performance on the evaluated benchmarks (e.g.FROMP [3] achieves >90% on P MNIST). Continual learning with node importance based adaptive group sparse regularization, 2020. Overall, the contribution over related work seems limited to me, but I am open to feedback from the authors if my initial perception is not correct.<|endoftext|>The paper describes an approach to continual learning by randomly initialized deep networks. The main idea is to extend the SupSup (Wortzman 2020) and apply linear combination of binary masks for efficiency. The paper also considers learning from multiple instances of the same task. The evaluation results indicate reasonable performance for the reduced parameters in common benchmarks. Together with the new homogeneous impression approach, the paper seems to describe unique and novel technical ideas to improve on SupSup (Wortzman 2020). ## WeaknessAlthough the approach seems to novel, there are several weaknesses in the current paper. Although Sec 5 discusses the limitation when there are small number of tasks, Fig 4 seems to indicate a serious performance disadvantage compared to SupSup, which does not look reasonable. Another missing comparison is the Hopfield network in SupSup, as discussed in Sec 2. In overall, I do not think the paper reaches the acceptance threshold.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 8; This paper presents AuditAI that can do certified training, verification under semantic specifications based on a generative model. The authors extended IBP to latent space variations and evaluate on a wide range of datasets. Strength:  The paper is written clearly. I think modeling perturbation sets for images with perturbations (like adversarial corruptions) explicitly can capture the specification better. For the perturbation in the semantic space of a generative model, it is not clear to me whether this is a semantic preserving perturbation. Also it is not clear whether the latent for a particular GAN model can capture all the variations. Usually, the diversity of the GAN is quite limited. I don’t find the unit test arguments to be very novel as it is a standard verification protocol. Minor: for few references, the use of citep, citet is mixed. Minor: theorem 1 “(“ should be removed.[1] https://arxiv.org/abs/1912.03192[2] https://openreview.net/pdf?id MIDckA56aDOverall, the novelty of the paper is a bit limited and I am not sure whether the perturbation on the latent space of generative models indeed captures all the possible semantic specifications.<|endoftext|>The system uses a generative model to construct embeddings for inputs and then tests whether these embedding vectors belong to a particular set. Membership in the pre specified set is reduced to satisfying the test. The key technical insight in this work is applying the interval bound propagation technique to embedding/latent codes in order to verify semantic behaviors of trained models. This paper tackles an important problem and provides a comprehensive discussion and evaluation of the approach. The breadth/scope that this work tackles is impressive. The paper shows how to verify a particular behavior like brightness, etc, and also how to train in a certified manner. In each case, demonstrating convincing performance of the modified IBP. There is a simple experiment that the authors can run to provide a sanity check that their variation of IBP  works . Even further, the assumption is that each dimension of the latent code only encodes for a single concept. The study is interesting, but I am not sure what to take away from this part of the work. First, I don t think Turkers are quite equipped to judge Chest XRays. It seems like what IBP does (not an expert on this technique) is to use a bound on the logits to bound the output of the network. I lean towards an acceptance for this work since it provides a convincing demonstration of IBP for latent embeddings, which means more interesting properties beyond adversarial robustness on the input space can be verified.<|endoftext|>This paper presents a framework for auditing deep learning models in regards toa specification. The goal is to increase the confidence in a trained model. An evaluation was performedagainst a pixel based training approach and under consideration of 4 differentdatasets. * Page 8; Section 4.5: Authors are talking about "certified images"; so far the   certification was about the training not individual images, the intend of   this section is not clear; also is not a robustness against adversarial   attacks also a property which can be seen as a variation which is the target   of the framework. However I am unsure whether   this is a good metric for evaluation since realistic images for the human eye   are known to not be equivalent to useful images for machine learning datasets   (c.f.g.adversarial attacks). The authors motivate the paper and clearly state thecontributions as well as the followed approach. While some rework of the formalfoundation (see `Points Against: 3.) However the usage of agenerative model to create the inputs to check boundaries does limit theconfidence in the framework since the reliability of the generative network isnow at question. The evaluation of this component, thegenerative network, was done by performing a human study about how realistic theimages are for the human eye and was also performed by participants of unknownexpertise. The specification of the encoding function $e()$ is not clear asidefrom the used example of the change in degree and should be discussed further,as this is also an important aspect in the framework. Without clearspecification of $e()$ and assurance that this in fact represents the variationwhich needs to be audited the result of audit is questionable. * Page 8 and 9: "AuditAI" is not in monospace font* the authors at times claim to certify, however this is in my opinion to  strong, most of the time they speak of auditing which is the better  description## Points in Favor+ Relevant problem and in scope of the conference. + Presentation of a framework for auditing deep learning models in respect to   given specification. + Formally sound foundation of the framework and its mathematical workings. + Experiments show the improvement with the framework. The framework is dependent on the encoding function e() and it is not clear   whether this is always specifiable.<|endoftext|>This paper proposes a new and useful direction: AI model audit. The experiments show the effectiveness of the proposed method. I think this is very useful direction, in particular, for industry. Rejecting making a decision (e.g.reject making decision for an image of bad quliaty and ask the user to re submit a better image) is much better than making a wrong decision. Cons1.The classification model and the generative model share the same encoder. You cannot directly apply the proposed method to any exsiting network. For example, the classification backbone is ResNet 50. 2.The key theoritical barries are addressed by an off the shelf method, IBP. 3.The method cannot be applied to other tasks, e.g.object detection, segmentation, etc. At least, no supportive experiments for other tasks. Can you translate the face rotation angels are within 30 degree and maintain 95% accuracy to the representation with  F(x,y) <  0 and S_{i, in}. 2.In practice, many people use the uncertainty/quality/confidence score to do AI model audit. I would suggest to compare with them in Related Work section. Despite my concerns, I think this is a VERY valuable work for the society. The paper is well written and organized.
Reject; rating score: 1; rating score: 3; rating score: 3; rating score: 6; They show that transformers trained on language to language translation tasks can improve the efficiency final accuracy of learning symbolic mathematics in some settings. There were many sentences I didn t understand (to a very unusual extent). For a number of experiments, they also focus exclusively on English to Romanian; while they assess other languages as well later on, they find that the specific pair of languages can make a big difference, so I m not sure how much to trust the results with just English and Romanian. Presumably they could have easily used more standard pretrained language models (e.g.GPT 2 or T5) but don t do so for some reason. The paper also makes a number of claims that seem unsupported or at least strange and confusing. For example, they say "Therefore, our mBART transformer model is lazy and during the fine tuning method neural weights along the breadth are almost frozen and only the weights along criteria satisfied path are updated. This method is also called stunting during its training" I don t know what this means.<|endoftext|>This paper improves studies if pretraining transformers on natural language improves their accuracy on symbolic mathematics tasks. I do not see that this paper adds a significant insight. The pretrained models require a significant number of training steps in “fine tuning” on datasets for symbolic mathematics to achieve a quality that is comparable to training them only on symbolic mathematics data. In particular the section called “Theory” is not ready to be published. For example:  second line of section “Theory”: Roman  > Romanian? Our model tries to predict the sequence of the shortest length by the principle of Occam’s razor.”  The entire section is one long paragraph and I could not follow what it is saying. Also in the introduction there are passages that need to be fixed:  “and thus our model predict with better when the length …”  > “and thus our model’s predictions improve when the length …”  “Section? ?”  > “Section 3”I am not convinced of the novelty of this work and the quality of the write up is not up to ICLR standards.<|endoftext|>This work investigate the problem of whether pretraining on language task such as machine translation could help with solving symbolic mathematics problems. The authors argued that finetuning a pretrained transformer model could get similar or better accuracy with fewer epochs. The paper claims that "We achieve comparable accuracy on the integration task with our pretrained model while using around 1.5 orders of magnitude less number of training samples than the state of theart model in Lample & Charton (2019)", but it is unclear to me which result it is referring to. 3.The result of "LC" in this paper seems much lower than the results presented in [Lample & Charleston, 2019], for example, Section 4.4 in their paper shows accuracy over 95% in FWD, BWD and IBP. It might be helpful to use a concrete example or some equations to help explain the theory that the authors want to convey. Furthermore the current draft contains several flaws: the title and some claims are not well supported; some of the writing, for example, Section 3, is confusing.<|endoftext|>The authors use transformers pre trained on machine translation tasks on the integration and differential equation datasets proposed by  Lample and Charton (Deep Learning for Symbolic Mathematics, ICLR 2020). This would not change your conclusion, but result in better supported claims. This paper demonstrates that transformer pre trained on NLP tasks can be used on non linguistic, symbolic, problems. For the BWD trained set, the conclusions go in the other way. In particular, the models compared have very different sizes, which makes the effect of pre training difficult to measure. It might be interesting to experiment with such different pre training tasks, to see whether "more topical" pre training data (i.e.computer science and maths, instead of common text) helps. However, the results could be made more convincing by making mBart and LC more comparable. 2  page 2 : the claim "Thus, our method is more robust than Lample & Charton (2019) as it is invariant to the data set generation method" seems incorrect: table 4 shows that whereas your model fares a little better than L&C when tested on a different generator than the one used at training, it is by no means invariant to the generation method (see also point 8).
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; rating score: 3; Today, most NLP applications use language models. These language models can be split into two different categories. The first is the GPT 3 style models, that are only trained on the unsupervised language modeling objective. What is a ‘standard transformer LM’? I think it could be helpful in the paper if you would give more details, such as: position embedding, FF dimension, number of heads, head dimension, and number of layers. This paper asks two questions about models in the second category. The second question is whether training on more prompts improves performance. Is it the performance for a given prompt? 4.The authors shine a light on the importance of creative and diverse prompts, some of which may seem too verbose or even silly at first. I recommend that this paper be accepted, and feel that it will be very valuable to the NLP community. I feel like your main baseline is the T5+LM model (T5 finetuned on the causal LMing task). Could you explain and/or cite relevant source “GPT 3 is behaviorally capable of zero shot generalization to new tasks” (section 2) in the next draft?.<|endoftext|>This paper trains a sequence to sequence model in a large multi task setting and tests the model s ability to generalize to unseen tasks (zero shot). The crux of the contribution lies in the design of prompt setups, extensive experiment, and the evaluation. The conclusive of this paper supports a growing trend/consensus in the community that multi task learning can be a good way for generalizability on unseen tasks. StrengthsThe tasks considered are extensive, covering many areas of NLP tasks such as QA, NLI, sentiment classification, summarization etc. The zero shot evaluation tasks are of different categories than in the training set, which helps support the zero shot generalizability claim. What would also be interesting is to see/what s missing in the paper, is to carefully study the difference in domains of the training tasks, versus zero shot evaluation tasks. Repeated experiments with different subsets of training/eval tasks can help shed some light as well.<|endoftext|>The authors demonstrate that massive multi task learning with prompting can improve generalizability of large language models for zero shot inference on unseen tasks. In this work, the authors aim to use labeled training data from a large number of tasks to demonstrate the improvement of model performance on unseen tasks. One of the primary contributions of the work is in aggregating resources across 12 tasks, 54 datasets and a large number of prompts for converting these tasks to the unified format. Finally, some analysis on transferability of the tasks, to study the impact of different source tasks on target tasks, will be an interesting contribution for the multi task setting. The claim of "true" zero shot generalization needs to be further supported by two more experiments as highlighted in the main review, namely, (i) analyzing the overlap in text from the MTL setup with that of the unseen tasks, and (ii) ablating paraphrase identification tasks to find the impact on NLI tasks.<|endoftext|>This paper proposed a new method for zero shot generalization of NLP models. Since zero shot generalization of the unsupervised pretrained language models are understood as an ability which is captured **implicitly** during unsupervised training on large natural text, this paper suggested a supervised learning method to understand zero shot generalization through **explicit multi task learning** in NLP models. it underpreforms Gpt3 (6.7B) on all tasks except WiC! The paper also provided a collection of crowdsourced prompt formats for each dataset, called P3. 13  the contribution is not clear. if using more training data sometimes reduce performance, e.g.Known Unknown and Logical Deduction tasks, perhaps due to model scale constraint, it would be helpful to evaluate with a larger T0. However, the training set of T0, T0+ and T0++ are different. This will helps to understand the benefits of encoder decoder training for zero shot learning. This way, the T0 can be evaluated on more BigBench tasks which has not in vocabulary T5 tokens.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; This method shows improved performance on several MiniGrid and Sokoban tasks, outperforming other baseline experience replay methods (PER, EBU, DisCor). EDIT: Having seen the additional experiments and clarifications the authors have added, showing that TER could be used in situations where reward is concentrated in states rather than just in terminal states, I raised my score from 6 to 8. And is it the RGB rendering of the map, or the tensor representation? Overall I believe the paper is good enough to be accepted.<|endoftext|>The paper proposes a new sampling strategy to improve the sample efficiency of Q learning based methods. The authors also provide numerous additional experiments attempting to elucidate the merits and limitations of the approach. Although the experimental setup is a bit limited,  the results look good and the proposed approach has the potential to improve future work in this space. Weaknesses  The experiments are somewhat limited to goal oriented tasks. Although the authors more or less limit the scope of applicability of their method to grid like environments, it stills shows that such inductive bias can be useful for complex environments. Could the authors elaborate on why this works in this setting, as well as elaborate on what setting would we see this choice be a bottleneck? I would suggest clearly mentioning the values of $\eta$.<|endoftext|>The authors propose a topological experience replay (TER) method to perform Q value updates in a reverse sweep style. However, I have the following concerns:Q1. The authors should clarify it in the abstract and introduction sections, and discuss how to extend TER to general RL tasks. Q2.I suspect that the hashing method may lead to an intractably large graph $\mathcal{G}$ especially for environments with large state spaces. Q4.The authors only consider the construction of undirected graphs, which makes their method not applicable to MDPs where states form directed graphs. Although the paper is well written, the current version does not convince me to recommend acceptance.<|endoftext|>This paper proposes a new sampling schedule for the experience replay buffer often used by reinforcement learning algorithms. What on policy methods? What is the role things such as eligibility traces and importance sampling can have in this analysis? This drastically reduces the applicability of the proposed method. How much TER is a general approach and how much is it a method to leverage very specific features of a subclass of problems? In that case, it is not clear how often one observes common states in different trajectories. I don’t think the single experiment n Figure F.9 is enough to justify this is not an issue. The text has several issues in terms of precision or correctness.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; This paper decomposed out of distribution into texture and semantics and proposed a model that extracts the texture and semantic information separately and then combines them via normalizing flow based method. My biggest concerns about this paper is the correctness of assumptions, derivations and fairness of comparison.<|endoftext|>While this certainly makes some sense, I think this perspective is a bit too narrow: all of practical OOD detection in computer vision would not fit into one of these two buckets. In Eq.(1), I think one cannot drop the $p(x)$ term from the denominator, since we are modelling a (conditional) distribution on the variable $x$. It is also not completely clear to me that texture and semantics should be treated as independent, as in the MNIST KMNIST experiment (regardless of their conditional independence given x), because a change in semantics ought to also entail a change in textures most often (since a novel object will most likely have a different texture associated with it). From Eq.(9) it appears that \lambda   0.0 corresponds to “semantic” shift, but the experiments seem to suggest the reverse. The experiments seem to suggest this can lead to fairly good performance. An ablation study is required for supporting the proposed choice of initialization for multi SVDD.<|endoftext|>The paper proposes an OOD setting emphasizing on texture and semantics. The authors propose an OOD detection method which disentangles texture and semantics. Strengths:  The separation of texture and semantics is reasonable  SoA performance   Easy to understand writing and illustrationQuestions and weaknesses:  Is there any case in which texture is part of semantics? How do we know the optimization of (2) will extract semantic information but not the texture information? However it still lacks comparison with some latest methods.<|endoftext|>The paper propose to decompose the out of distribution sample detection task into two fold: texture and semantic. The experiment is extensive and sufficient to justify the proposed idea. The paper assumes that the texture and semantic information are independent. In my understanding, this assumption is a little strong. This may need some justifications. Overall the paper is good and ready for publication.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 8; This paper proposes a numerical algorithm for solving a class of  nonlinear generalized eigenvalue problems  that arises from the Hartree Fock approximation of the Schr\"odinger equation. Efficient computation of the Schr\"odinger eigenvalue problems  is a fundamental problem in computational  physics is certainly  worth to study. Here are some concerns and comments. However, this was only demonstrated in empirical results, but not justified rigorously. In the present paper, the authors seeks an  Hartree Fock approximation of the Schr\"odinger problem and considers a finite dimensional discretization of the Hartree approximation. Can the authors comment on whether (and how) the proposed method, especially the idea of Eigengame can be extended to solving the original  (infinite dimensional) Schr\"odinger problem without resorting the Hartree approximation? If so, please  state it in this way. This paper provides an interesting attempt for accelerating the computation of nonlinear eigenvalue problems relevant to the Schr\"odinger equation. However, the contribution of this paper is incremental and may not be sufficiently novel for ICLR.<|endoftext|>It proposes the Self consistent Graident like Eigen Decomposition and shows that it achieves better numerical permanance for solving Schrodinger equations. The statement of the proposition itself does not make any sense. If so, then F   0 everywhere and the proposition is meaningless. It seems that the authors are trying to prove something along the lines of "a stable local convergence point is the solution of equation 1", but I am not sure. Can the authors give more information on the full solver case? Equation (2) (3) can be written in the matrix form to be more consistent with the rest of the paper. I consider this paper to be marginally below the acceptance threshold.<|endoftext|>This paper considers a gradient like eigendecomposition algorithm for solving nonlinear eigenvalue problems. Some experiments have shown that it is more efficient and effective. Some experiments have shown that the proposed method outperforms the traditional heuristics based initial guess methods in terms of accuracy and efficiency. S2.Sufficient technical details of the gradient like eigendecomposition algorithm are included. The authors just simply apply Oja & Karhunen s gradient method algorithm to solve their nonlinear eigenvalue problem.<|endoftext|>It regards F(V) as a special “online data generator”, thus allows gradient like eigen decomposition methods in streaming k PCA to approach the self consistency of the equation in an iterative way similar to online learning. This paper demonstrates SCGLED can replace traditional heuristics based initial guess methods with large performance advantages and is capable of finding highly precise solutions independently without any traditional iterative methods. This paper is interesting that the proposed SCGLED can replace traditional heuristics based initial guess methods with large performance advantages. I recommend this paper to be published after addressing my below comments. My comments are as follows:1. For Equation (3) on Page 4, v_i’ is used, but in the description v_i_t is used. It shows it not only can simply replace traditional heuristics based initial guess methods with large performance advantage, but also is capable of finding highly precise solutions independently without any traditional iterative methods. In summary, I recommend its publication in ICLR after my comments are addressed.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; I believe this to be a well written, strong paper, with an elegant and simple approach. Second (and a core contribution of the approach) treat language prediction as an auxiliary task, predicting a sequence of language instructions that specify how to perform the task (which I liken to “subtasks” or some compact language describing an immediate outcome to optimize for, though the authors should please correct me if I’m misinterpreting!). Centered around an evaluation of various tasks in the BabyAI complex grid world suite, as well as the Crafting environment introduced by Chen et.<|endoftext|>The authors show that using instruction modeling is able to improve the performance in planning environments when training with a limited number ofdemonstrations on the BabyAI and Crafter environments. The authors use language as an auxiliary task to improve planning accuracy. Could the authors explain the difference between the claimed "non Markovian" and the commonly used "partially observable Markov decision process (POMDP)"? Why do the authors use "non Markovian" instead of "partially observable Markov decision process (POMDP)" in the paper? This paper is easy to read and has no significant fault.<|endoftext|>This work proposes using instruction modelling as an auxiliary objective to improve long term planning. These are also good related reference for the paper, which this draft doesn t include   some of the references this work does include would also be good test beds   because they also contain more complex grounding challenges due to visual observation complexity. All of these previous works include dynamics and language instructions more complex that the ones studied in this paper. For example the instruction "go to the red door" is perfectly recovered with the binary features (goto, red, door). Can the authors please comment on this?<|endoftext|>This paper investigates the usage of language prediction for imitation learning. The authors distinguish between a goal (a short description of the task) and an instruction (a detailed description of subtasks that span a sequence of observations). They experiment with minigrid and crafting environments to show the efficacy of the proposed approach. I have some questions about the approach. * Section 4.2, "Prior work has not use" should be "Prior work has not used".
Reject; rating score: 3; rating score: 6; rating score: 6; rating score: 8; The authors argue and propose to compress the neural networks using an additive combination of TT decomposed and sparse tensors/matrices. Authors provide compression results that are interesting (improved accuracy for a given compression rate), however, the method has an important shortcoming that requires setting the compression parameters by hand for each layer (ranks, sparsities) that limits its practical applicability. Additionally, authors miss a large body of related work on: a) additive combinations of compressions b) low rank and tensor decomposition methods. Authors argue and empirically demonstrate that an additive combination of compression is a good choice for the neural network compression. By itself, this would have been a good contribution if executed perfectly, however there are several shortcomings that prevent me from accepting the paper. In other words, an empirical study on a single network should not be generalized for all networks by saing "...L+S is the best choice...". Here is one practical argument againts L+S or any additive compression scheme: the overall compression ratio of any additive combination scheme is limited by the compression ratio of the worst term. Clearly, there is a compression error interplay  between using additive scheme vs using any single scheme, which should be studied more formally to make any long standing claims. 2.The formulation and the choice of hyperparamters. 3.Missing details on reported quantities. Authors report overall compression ratio, however, do not report how these values were computed. Please report clearly how the measures are being computed. Authors should mention these works and factor paper s contribution into the existing literature in a more rigorous way. My rating of the paper is based on the following issues:1. Literature review and positioning<|endoftext|>This paper presents a survey of methods for enforcing low rankness and sparsity in neural network weights and proposes SPARK, an alternating algorithm for creating low rank and sparse weight tensors from a pre trained network. I would heavily encourage the authors not to create more confusion about terms than there already is, especially when working in the same field, and rename this method to something else. The fact that it is not mentioned makes me worry about how many other references are missing and potential comparisons. 3.The sparsity literature is vast and includes some extremely relevant contributions, all of which have been omitted. There are more in the references they list and the ones listed are too similar not to be compared.<|endoftext|>This paper introduces a new DNN compression technique. It consists in approximating the weights of a trained DNN by the sum of a low rank and a sparse tensor. The key elements of the method are clearly discussed and supported (even too heavily in my opinion). The proposed technique is challenged against state of the art for three common architectures on CIFAR 10 and ImageNet. In my opinion this is an interesting contribution and I support its publication. But I feel that the paper could have been more convincing if providing more extensive experiments, especially because the novelty is somewhat limited (the paper combines already existing techniques, even if this is done cleverly). And this appears to be one of the main contribution of the paper. While I agree with the three asked questions, I think that the answers could be better supported. What makes them really different is the way they are parameterized, and it is often easier to set a constrain parameter than a penalty weight.<|endoftext|>The paper proposes a novel approach for  model compression : reducing the size and computational cost of a neural network model by converting weight matrices to (a) be sparse and (b) low rank. While prior works have considered both sparsity (i.e., pruning) and low rank ness before, the proposed method utilizes both simultaneously: approximating the weight matrix as a sum of two matrices, one that is low rank and one that is sparse. This leads to an improved performance compression trade off: indeed, in some cases, this approach seems to have a regularizing effect and actually improves the accuracy of the model. I especially appreciated the thorough discussion and analysis in Section 3. There are only two compressed versions reported for each architecture/benchmark. While that would be a separate algorithm and out of the scope of this paper, it would be helpful to give readers a bit more intuition on which approximation causes a higher performance loss (and if it varies from layer to layer). Overall this is a great paper, and I believe clearly meets the bar for ICLR.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; rating score: 6; Following in the success of boosting methods for tabular data, this paper introduces a new boosting approach for data that graph based with tabular features. The proposed approach, efficient bilevel boosted smoothing (EBBS), has convergence guarantees as well as empirical successes compared to competing methods. The proposed approach is an end to end, bilevel, combination of label propagation and boosting. **Merits** I believe that this is a strong paper that clearly outlines a proposed approach for boosting in this non iid setting of graph data. The proposed approach has a convergence guarantee and is shown to be very effective empirically. The algorithms and theoretical results are discussed as well. * Figure 1 would be easier to read if Y axis was the same in both plotsThis paper provides both theoretical and empirical results for a boosting method for graph structured data. The results appear to advance the state of the art and the submission seems to have valuable contributions.<|endoftext|>After the rebuttal I have strengthen my opinion on the quality of the paper. It is also shown how the meta loss introduced by the authors provides convergence given some moderate assumptions. I believe it is a nice contribution to the field. The work is well structured, with a good theoretical basis to support the proposed methodology. The empirical results are very promising, although the small amount of datasets combined with the lack of confidence intervals does not allow for meaningful conclusions to be drawn. Although the authors give their own explanation of why the test nodes should also be used during training, i.e.for the propagation of information in the graph, if the test labels are used during the train there is no longer any separation between train and test. I have this doubt because the labels are used to calculate function space gradients. "Although tabular graph data for node classification is widely available in industry, unfortunately there is currently little publicly available, real world data that can be used for benchmarking." "revealing that it may be more robust to non ideal use cases".<|endoftext|>In this paper, the authors present a new approach to combine the boosted decision tree classifiers with a graph propagation model, which is important in handling table input data. The approach casts the graph propagation as an optimization problem, where the input node features are generated by boosted decision trees. The final algorithm is shown to minimize the unified loss in a principled manner. Strength  The approach nicely defines a single objective that the model (graph propagation + decision trees) optimizes. Does the framework supports any propagation rules beyond (6)? I would be curious to see how general the method is. Overall, the approach seems sound and principled, although the scope is a bit narrow.<|endoftext|>Strengths:  This paper proposes EBBS, efficient bilevel boosted smoothing, a novel way to combine GNN and Gradient boosting for learning tabular graph data. The addressed problem of integrating boosting into GNNs is very interesting to me. Also learning over tabular graph data should receive a wide audience given its importance in the industry. Weakness: In my opinion, the main weaknesses are in writing/presentation and reproducibility. Second, it seems that the proposed method EBBS will be incorporating test nodes during training. Will this cause test information to leak into the training process? The paper proposes a novel way to address this problem, which is based on a principled meta loss. Empirical results show the effectiveness of the results. I feel the paper can be improved more by iterating on the formulations in Sec 3.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; rating score: 6; This paper proposes a  so called  NAS Bench 360, which is quite misleading. The original NASBench idea (brought by NASBench 101 Ying et al.) is to exhaustively train all (or a substantial amount of) architectures within a search space and record their training / evaluation statistics for researcher or NAS practitioners to evaluate their search algorithm in a fair manner. However, this work mis use the concept. See above, this paper mis use the concept of NAS Bench that is common to NAS researcher. After reading the entire paper, I do not find it interesting as a NAS practitioner. Evaluating some algorithms on different tasks is a good empirical report but cannot be qualified as a top tier conference paper due to lack of novelty and effort.<|endoftext|>The authors argued that existing NAS benchmarks targets the well studied tasks, and thus proposed that we should evaluate NAS on diverse tasks. And thus the authors collected 10 tasks and did some analysis on these ten tasks/datasets to compare several NAS methods and some strong baseline DL models, such as wide resnet. In this work, the authors thrown out an issue that the existing NAS benchmarks focused on the popular tasks   image classification and needs more diverse tasks for the NAS evaluation. First of all, the existing NAS benchmarks have covered at least 6 different tasks/datasets and if we counting the dataset/tasks explored in the HPO works (which also try to find some architecture options), we have more architecture datasets/benchmarks in the NAS community, which is strong and diverse. So that some claims in the paper may be exaggerated. Unfortunately, this work does not provide such architecture dataset. For example, some datasets only has about 1K data but try to do some 2D dense prediction. Compared to this option, the only benefit of this NAS Bench 360 would be the calibration the NAS methods in the existing application papers. This NAS benchmark lacks architecture datasets for the collected ten datasets and technical novelty.<|endoftext|>This paper proposes a new benchmark for NAS methods, which is called NAS Bench 360. This paper has tested several standard NAS methods on the proposed benchmark and confirmed that there are many gaps among the ten tasks and NAS methods. + The proposed benchmark is much diverse compared with the existing ones. It indicates that it is necessary to provide the rank or something related to the rank to appropriately evaluate NAS methods, not only performance. But this paper uses the limited search space for the tasks, thus I am wondering if this benchmark can appropriately evaluate NAS methods. It seems that designing search space itself is needed for diverse tasks and evaluation of NAS methods on them. It would be nice to provide the detail of the search space in the main paper. Hopefully, the authors can address my concern in the rebuttal period.<|endoftext|>The search space are not limited to architecture topologies, but also hyper parameters. Strength:1/ I think the problem this paper raises is very meaningful. 2/ The NAS benchmark proposed in this paper is flexible and general enough to include architecture search, hyper parameter tuning, and different base architectures. 3/ The conclusion or take away message of this paper is useful for practitioners. Eventually, the reason to have a benchmark is that 1) the benchmark itself measure some metrics we care about, or 2) performance on this benchmark can generalize to a wider range of tasks that may arise in practical applications. In this paper, scenario 1) obviously does not hold. But for scenario 2), this has not been demonstrated in the paper. There is no experimental or theoretical support for this, which limits the contribution of this work. 2/ The technical contribution of this paper is limited. This paper did not propose new methods or other novel contributions.<|endoftext|>Below I list some more details comments:**Pros:**(+) I think the set of collected tasks will be a useful contribution to the NAS community in order to shift the attention to other tasks that do not only involve image classification. (+) The paper is well structured and easy to follow. **Cons:**( ) I think the paper could benefit more by experiments that provide further insights for the NAS practitioner. For instance, performance predictors are commonly used nowadays in NAS [1]. ( ) I think there is some text and results in the main paper that can be moved to the appendix and make some space for additional empirical results. For instance, the authors can move Table 3 to the appendix, since it is somewhat redundant with Table 2. How much does the inductive bias affect the ranking of different NAS methods evaluated throughout the 10 tasks? **Other comments:**  I think the term "NAS Bench" is reserved for tabular/surrogate benchmarks and using it for this submission might be confusing at first for many readers, since NAS Bench 360 in principle is only a collection of datasets and scripts for running methods (search space + NAS algorithm) on them. I would suggest the authors to reconsider changing the name of their suite. I agree with the authors that as an AutoML practitioner, in general I am also concerned which will provide the best output: 1) optimizing the architecture when keeping the hyperparameters fixed or 2) the hyperparameters of a fixed well performing architecture. Even though there is no novelty per se, I think the NAS community will benefit from this curated collection of diverse tasks.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; This paper focuses on stable and effective adversarial training and improving generalization using it. The experimental results are ok but not strong enough to be accepted. There is no theoretical analysis, and therefore this is basically an empirical paper and requires strong empirical results to be accepted. 2.Author analyzes the behavior of FGSM style adversarial training and proposes a method from the lens of perturbation deterioration.<|endoftext|>**Summary**The paper address the issue of catastrophic overfitting in robust training. This replaces one problem with another. Thus, since there are no guarantees and that the key element here is replacing the tuning of a scalar to tuning a scheduler is not considered as a major contribution. I may be wrong or missing something here. I hope authors correct me on this. The paper then goes about resolving this issue by strengthening the augmented adversaries during training by learning the hyperparameters of FGSM adversaries, namely the step size and the initialization. The key argument in the paper for catastrophic overfitting is the deterioration of the augmented adversaries during the training time. 2.The other key contribution to this paper is the algorithmic part.<|endoftext|>This paper focuses on an emerging topic in adversarial training, which is about catastrophic overfitting. Besides,  the results in this paper are similar to an existing work [1]. So, the authors introduce an adaptive adversarial training method, called APART, which adjusts the parameters of the perturbation generator. The observation results lack reasonable verification.<|endoftext|>Experiments: The experiments validated the paper s claims. It is unclear whether the authors explicitly thought through this point. After seeing others  reviews, while I do believe the main writing it clear, the authors may fail to justify the most crucial part of their contribution (how perturbation distribution and robustness are linked). I think such related works (maybe ones that are more immediately relevant to adversarial training) can be discussed to better motivate the current work.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; The paper studies training of models that are robust to spurious correlations without any supervision on spurious correlation revealing example groupings. Also, CNC is shown to be more robust to noise in spurious attribute recovery than the existing methods. Spurious correlation avoidance without assuming any prior knowledge of spurious features is an important problem with wide impact. For example, the smallest group in CelebA (with supervised group labels) has only around 100 examples. Expect to see std. Environment inference for invariant learning. Looks like all your numbers are from a single run, given the typical std. The differences to me look only superficial. The authors state that the prediction accuracy of the spurious attribute in the case of CelebA is only around 59%. I wonder what is the significance of ERM base model at all and what would happen if we instead simply regularize the distances for any pair of points of the same true class (this would be similar to [2]). **Sensitivity to Stage 1 prediction (Sec 5.3):** I do not understand why in Fig.7 (d), we see the average accuracy also decreasing with p. I expect the average accuracy to remain stable or increase as the worst group accuracy deteriorates.<|endoftext|>Empirically, the proposed method demonstrates improved worst group performance over existing baselines. The proposed method achieves SOTA worst group performance to be close to GDRO which uses the true group labels. There’s also a recent work [3] that applies contrastive learning for doing so. * Though the proposed contrastive method leads to improved worst group performance, it seems to decrease the average case performance compared to baselines. More crucially, neither part of the two stage method is justified with sufficient motivation and empirical evidence, as detailed below:    * Using ERM prediction as the group label is not convincing enough, and it is not clear how it would affect the contrastive part. It could be interesting to more extensively analyze how the label prediction affects the improvement given by the contrastive method, probably using a scientific setup where the label prediction is controlled. * For the contrastive part, the current empirical comparison obfuscates the advantage on its own. To decouple it from the effect of wrong group prediction, it is important to compare in the setting where group labels are available, i.e., GDRO vs GDRO + contrastive. Also, there could be a lot of choices of negative selections but only one is used without sufficient explanations, it would be great to include more explanation or compare with some other possible choices as an ablation study. Thus, I recommend a ‘reject’ for the paper.<|endoftext|>Is there a way to incorporate spurious attribute information into the positive and negative sampling methods if it were available? This paper considers the goal of training classifiers that achieve strong worst case performance across groups with different spurious features, without assuming access to supervision based on the spurious features. In all this is a nice piece of work and I am cautiously happy to recommend its acceptance at this point. I do, however, have a couple of questions I would like to see some answers to (see below). The method addresses an important problem, and is well motivated and evaluated. The method specifically samples hard positive/negative samples. This is potentially an even more important point on the novelty of the method than is currently emphasized [see the second main question below, about SupCon as a baseline]. The worst group performance seems to decay slightly better than JTT as the level of spurious correlation increases (Fig.7).(For weaknesses, see the questions below). It would be good to see SupCon as a baseline in Table 1. I am not per se asking for any response from the authors, but offer them up in the spirit of constructive feedback:  What if you iterate your method?<|endoftext|>This paper discusses a two stage method for improving a model s subgroup robustness, first training an ERM classifier and then performing contrastive learning on the representations. there are some training details buried in the appendix which seem worth discussing   for instance the clustering based prediction from the first step ERM model seems like an unintuitive step which may be fairly important to the functioning of the method. In my experience, clustering approaches can be quite helpful for these types of problems and I would like to know a bit more about the role it plays, given that it is far from the first thing you would think of doing (which would be just using the standard linear layer)Other thoughts:  In Figure 3, the relationship I would really like to see is L_align vs Accuracy: this is the one that makes your point most compellingly  In Fig 3c, I disagree with the characterization that high worst group accuracy corresponds to a combination of high I(Y,Z) and low I(A, Z). It looks like WG accuracy is mostly (but not fully) invariant to I(A, Z) in this plot, with the level colour sets extending horizontally (more or less) across the plot  Some of the notation in the proof in 3.2 is a little sloppy   in particular, y is overloaded in the definitions of L_wg and L_avg, both being used inside the scope of the expectation and outside itThe paper can use some cleaning up but the idea is interesting and clearly communicted enough to be of value to the conference.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; rating score: 6; 2.Empirical studies show that AQE achieves a much higher average return compared to SAC, TQC and REDQ in most environments. One of the major concerns is the similarity of this paper to a prior work REDQ. The similarity is of various aspects, including the algorithmic tables, sentences, results, analyses and even the format! Compared with the baselines in that figure (SAC 5, TQC 5), REDP sounds much better in terms of reducing the Q estimation error. I am quite curious how REDQ on the bias. 3.It’s kind of strange that a model with low bias and std provides poor performance sometimes. The authors don’t provide an explanation. [typos]Not sure whether or not the authors intended to say “AQE improves the sample efficiency performance of TQC and the asymptotic performance of REDP” in the abstract? Because TQC provides SOTA asymptotic performance while REDP achieves high sample efficiency... Overall I think the contribution of this paper is not significant to the high standard of ICLR.<|endoftext|>The paper proposes a new method for mitigating the overestimation bias in Q learning like algorithms. Empirical results are provided to support the claim. As shown by Figure 5 of Henderson et al.(2018), 5 random seeds are clearly not enough for empirical comparison. I cannot convince myself of any conclusion with only 5 domains, especially when the performance of the proposed algorithms and the baselines are of the same order. Further, under this assumption, I believe Theorem 1 is just trivial. I do appreciate that the authors achieve good performance with simpler approaches than existing work.<|endoftext|>629This paper studies data efficiency and asymptotic convergence rate of deep reinforcement learning and proposed a new algorithm called AQE and claims the algorithm are both fast in both senses. The paper is mostly empirical with results on Mujoco, and compared with TQC and SAC and showed competitive performance. Your theorem 1 shows this is positive, then how bigger is it than zero? I ve read the authors  feedback and other reviews. Your result 2 in Th1. There are also other problems (presentation, algorithmic discussions and comparisons) that can be much improved. The results show sample efficiency. The asymptotic rate is not very much supported. The theory s message isn t very clear. This is an interesting alternative. The results were run for 1 million frames: this may not be able to support asymptotic behaviour. What does the approximation mean here?<|endoftext|>**Summary**This work introduces Aggressive Q learning with Ensembles (AQE), a simple model free reinforcement learning algorithm. **Pros*** The proposed AQE algorithm is simple and seems to be effective based on the presented empirical results. The proposed algorithm and the empirical results are presented in a clear way. * The connection between AQE and REDQ is not clear for someone that is not familiar with the REDQ algorithm. According to the paper $G$ should  be greater than $1$ ($G>1$), but it would be really interesting to check the impact of updating the model multiple times on the performance of AQE. My main concern as regards this work is its novelty.<|endoftext|>In this paper, the authors show that the asymptotic performance of  REDQ is improved by replacing the minimum of Q network outputs with the average of the multi head Q network outputs. + the proposed method is simple. Weaknesses:   Incremental improvement from the REDQ paper: 1. The main difference between AQE and REDQ is that AQE uses (1) the average of their outputs (i.e., model averaging) to target and (2) multi head Q functions. However, it is not clear whether the performance improvement shown in the experimental results (e.g., Figure 1) is very meaningful. Why Mheads are better than one: Training a diverse ensemble of deep networks. Theorem 1 and its proof are not correct:  At the beginning of pp.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; This paper proposes an elegant two stage pipeline to jointly solve the three tasks of point cloud densification, denoising and reconstruction at one time. This paper mainly has two strengths:First, a two stage pipeline is innovatively proposed, which can enable the network to solve multiple tasks such as point cloud denoising, completion and reconstruction at one time, and the reconstruction accuracy and visualization results of this method can surpass the latest methods in various sub fields. It would be better if the paper could explain why the proposed new method can achieve such strong generalization ability, and why the generalization ability of the existing methods is poor.<|endoftext|>The paper proposes an end to end unified approach to solve the subtasks in point cloud completion. The main selling point of the paper is that combine two existing methods for point cloud densification  & denoising (Stage 1) and point cloud completion (Stage 2). My understanding is that the network majorly befits from stage1 as a lot of qualitative results concentrate on completion. 4.Qualitative examples are good, and show the performance improvement but are limited, and would like to see more of them in the supplemental (which is missing)Weakness: 1.<|endoftext|>This paper presents a unified framework for doing point cloud upsampling, denoising, and completion jointly. It outperforms single task baselines on multiple benchmarks by a great margin. It s not enough to only demonstrate the proposed method out performs single task baselines. The proposed framework greatly out performs single task baselines. I have worked in the field of point cloud recognition and pretty familiar with the recognition frameworks. Or something else that is more accurate like "point cloud refinement"?<|endoftext|>The paper proposes a new method to jointly address the tasks of point cloud denoising, completion and upsampling. However, due to the lack of a significant body of works in the related work section as well as the missing evaluations I can not recommend this paper for acceptance at this stage. Missing related work   There is a very large body of recent works on neural shape implicit representation which focuses on the surface reconstruction task. First, there was no unified approach used (denoising + completion + sampling). Second, It would make more sense to me to compare the proposed method on the other s task since it is expected that a denoising method will not do well for upsampling. Finally, as mentioned in the related work comment, the paper lacks comparison to implicit representations surface reconstruction.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 6; Hierarchical DivNoising is based on earlier published DivNoising architecture. The authors compared the performance of their HDN approach to the closest state of the art unsupervised as well as supervised denoising methods on an impressive set of twelve datasets.<|endoftext|>This work is the first to use hierarchical Variational Autoencoder for the task of unsupervised image denoising. The method is also shown to remove structured artifacts on three real microscopy datasets. This work advances the state of the art in terms of use case novelty and performance improvement.<|endoftext|>The paper proposes a hierarchical VAE model that produces good denoising results. This is not a condition for publication, I think the method is interesting in any case. Strengths:The proposed model is a novel unsupervised technique that is competitive or outperforms the state of the art using a multiscale variational autoencoder framework. I think this is a meaningful contribution, and therefore I recommend publication.<|endoftext|>Introducing the idea of hierarchical representation of latent variables analogous to VAEs, the proposed method improves the denoising performance of DivNoising (DN). In addition, the authors propose a method for artifacts removal based on the analysis of the image components represented by the latent variables at each layer. It is recommended to denote e and n in different words.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 8; The paper tackles the difficult problem of learning to segment objects from an image using no supervision during training. 3.The paper introduces a new synthetic dataset of images taken from scenes with multiple objects with varying shapes and textures (11 and 15, respectively). Figures 3E G are also helpful showing the failure modes of the proposed method. I am not fully convinced that the comparison to the baselines is entirely fair. How does [2] compare to the method presented in this paper? 4.There are no ablation studies reported in the paper.<|endoftext|>I also got lost in the angular velocity equation. The paper also introduces a new synthetic dataset where prior methods do badly, and the proposed method does slightly better. Overall, this paper is messily written, and proposes something that only works marginally better than prior methods, on a synthetic toy dataset. The performance in Table 1 illustrates this: the standard deviation of the segmentation IOU (0.34) is about even with the average IOU (0.35)! Statistically maybe these are equivalent. (If you do expect it to work, please try it out and add the results to the paper.) These things should be re arranged. Where is this coming from?<|endoftext|>[2] Kabra et al., 2021. The main contribution here is an optical flow based method to warp the image at time t using the predicted object location/pose/depth to predict (some of) the pixels in image at time t+1. However, the empirical evaluation is very limited and this makes it difficult to evaluate the full merit of the proposed approach. I think the main concern with the paper is limited experimental evaluation. The model is evaluated only on a single, rather simple dataset (that was generated by the authors). There are many inline equations; these make it difficult to parse the text visually. Figure 1 is great but again hard to parse.<|endoftext|>The results are good. This new setup requires different data that what has been used in this space, and the authors contribute a synthetic dataset as well. In some sense, the methods are not really comparable as the other methods do not have a viable way to make use of the additional training data. But if the number of objects is known, then the comparison to other work that infers it might not be fair. K is in the pseudo code, but does not seam inferred. Should K be an input. The paper could use some polishing. Also, I am guessing that the "Objects" box is the LSTM. The authors do not say whether they will release their code. While this might be the standard for this sub area, real data from a robot or car should be relatively easy to get. Using just two time points is both a strength and a weakness.
Accept (Poster); rating score: 8; rating score: 8; rating score: 5; The paper proposes a new distributed optimization framework COMP AMS based on gradient averaging and compression with convergence guarantee. In particular, theoretical discussions have shown that the proposed algorithm shares the same convergence rate as AMSGrad with linear speedup effect.<|endoftext|>This paper suggests a gradient averaging strategy for distributed adaptive optimization. Convergence analysis results demonstrates that the proposed strategy has a linear speedup as the number of workers increases while the same convergence rate as standard AMSGrad. Strengths  The topic considering communication efficiency for adaptive distributed optimization is of timeliness and importance.<|endoftext|>The authors propose the COMP AMS algorithm for a distributed optimization framework. The algorithm is based on gradient averaging and adaptive algorithms. The application of gradient compression helps to reduce the communication complexity, and the tool of error feedback is used for the bias correction. The theoretical results are justified by the numerical experiments. The convergence analysis implies a linear speedup in terms of the number of workers, and shows that in the single machine case, it can achieve the same convergence rate as the standard full gradient SGD. The paper is well written and easy to follow.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; The paper proposes a graph neural network based forecasting algorithm for battery state of charge. This is a small addition to the algorithm.<|endoftext|>This work proposes a new method based on graph neural network for Lithium ion batteries parameters estimation. The goal is to predict multiple future states for all the time series data. It outperforms the compared state of the art method. This paper is an application paper, on Lithium ion battery parameter estimations. There are many existing temporal graph neural networks for time series forecasting [1]. You should better clarify the notations and be more consistent.<|endoftext|>The paper proposes to incorporate a graph autoencoder method from the field of causal structure learning for improving graph based forecasting, with a focus on battery parameter estimation. In Section 3, $A$ is not guaranteed to be acyclic, different from [1], since the author does not enforce the acyclicity constraint.<|endoftext|>The paper proposes a new graph autoencoder time series estimation approach that learns the underlying relationship between variables in battery measurements. Weaknesses:My major concern of this paper:1. The technical contribution is limited. Considering that the architecture of RNN+GNN is not novel, the main contribution of this paper may come from the application of graph sampling for learning the relationship between variables. [KDD20] Wu, Zonghan, et al."Connecting the dots: Multivariate time series forecasting with graph neural networks." 2.The necessity of graph structure learning needs to be proved.
Reject; rating score: 3; rating score: 5; rating score: 6; They compared with alternatives for DS reconstruction with an overall better performance. I think the paper is well written and easy to follow. The spline basis extension is straightforward and explained well by comparing it with PLRNN. The spline basis expansion is quite common in many problems, like in mixture models, instead of using one model, people extend to mixture models with a linear combination; in kernel study, instead of using a single kernel, people use a mixture of kernels. In early papers such as [Chan et al ACC 1998], people have studied spline bases to model the nonlinear relationship between input and outputs in RNN. Therefore, although spline basis performs well in the context of PLRNN, I don t think the technical contribution is much significant to the community of ICLR. Some detailed comments:1. I think the equivalence is only true when the high dimensional PLRNN has a unique structure of the connection weight W as shown in eq.40.Therefore, although a dendPLRNN can be formulated as a high dimensional PLRNN, the opposite direction is not always true. 2.Following point 1, at the end of page 6, "it becomes clear that the basis expansion enables to reduce the model’s overall dimensionality without compromising performance." I think this is only true if the structured assumption of W is valid. If the authors want to compare PLRNN and dendPLRNN in the same space, e.g., M*B, it s needed to show that how much approximation error there will be. The spline basis expansion works empirically but the technical novelty is not strong enough.<|endoftext|>This paper focuses on the nonlinear dynamical system on time series data. The training method of the proposed model is through variational inference, and BPTT with teacher forcing. The proposed model is a simplified hybrid linear and nonlinear dynamic system. The reasoning of coupling this model with dendritic computation is not clear. The claimed interpretability of the proposed model lacks both theoretical and experimental justification. Eq.(2) and (4) refer to the same function denoted by \phi, which incurs confusing definitions. In time series analysis, "piecewise" often refers to the method that separately models the segments of time series [1]. In this paper, the implication of "piecewise" is not well explained. The proposed model is a simple hybrid dynamic system and the training method is based on existing VI and BPTT and thus the paper lacks novelty and technical contribution.<|endoftext|>The paper proposes to modify the nonlinearity of neuron couplings in PLRNN with a linear spline basis expansion. This modification is claimed to boost PLRNN s capacity of capturing arbitrary nonlinear dynamics in low dimensions. The proposed method is evaluated on various nonlinear dynamical systems. The authors have updated the manuscript and added experiments. I will not change my recommendation. * To demonstrate the expressive power, apart from chaotic attractors, there are many dynamical systems such as fixed attractors, continuous attractors and etc that are commonly used to describe e.g.neural computational (Wang 2006, Mante 2013). Such experiments could be useful for qualitative demonstration. Hopefully the authors can address my concern in the rebuttal period.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; The author proposed a GNN based to estimate influence (as an upper bound). Based on the estimated influence, the authors use CELF optimization to find the optimal seed set. It is an interesting and practical idea to consider learning based approach for cascade problems on social networks. Some of the writing in the paper is not clear or misleading. First, it is not clear what is the input to the GNN. First, RSS based methods are not included in the influence estimation part.<|endoftext|>This paper considers using learning methods to solve the well known influence maximization problem. The paper proposes to estimate the upper bound of the influence by using graph neural networks, which can be used in subsequent steps for selecting the seed nodes through either Q learning or a greedy algorithm based on the learned representation. Experiments on various datasets have been provided to evaluate the accuracy of influence estimation as well as the effect of influence maximization. Using GNN and Q learning is reasonable but not novel. The presented experiments consider influence estimation from small seed sets. Notice that IMM might be the state of the art method with approximation guarantees, but efficient heuristics are possible (as discussed in [e]).<|endoftext|>Authors propose several neural network model based approaches to influence maximization (IM). First, GLIE estimates influence using a GNN, which can be plugged into optimization algorithms like CELF. Finally, PUN avoids the cost of having to estimate the influence for every seed node by approximating the influence with features from GNN hidden states. The IM problem has a strong structure (submodularity), and strong non learning algorithms are already established for the problem. Therefore, it is quite surprising and encouraging to see that GNNs can actually be quite competitive on this problem. Since this paper also proposes a learning based approach for submodular optimization, this approach should ve been discussed and compared against as a baseline. For a submodular function, we should be able to evaluate for any set in the domain of the function.<|endoftext|>Also, it seems that the main novelty of the paper is in proposing to estimate the influence via graph neural networks and the proposed influence maximization methods use already existing techniques in combination with this influence estimation method. The paper is overall well written as easy to follow. In my opinion, the main weakness of the paper is the missing details in the experimental section.
Accept (Spotlight); rating score: 8; rating score: 6; rating score: 6; rating score: 6; rating score: 6; rating score: 10; The paper extends the recently proposed E(n) GNN (Satorras et al.) This allows the easy incorporation of geometric and physical information in the MPNN framework in a steerable manner. The experimental results reported in the paper are quite encouraging, and for me the hallmark of the paper. It is also well motivated and well written and is definitely valuable. phi_m and phi_f are required to be equivariant, specifically to the group E(3). Section 3 goes over the related work on viewing message passing as convolution. The exact form depends on the type of input data.<|endoftext|>This paper introduces a kind of Steerable E(3) Equivariant Graph Neural Networks (SEGNNs), exhibiting these merits: 1) Compared to EGNN (Satorras et al., 2021) and other concurrent works, SEGNN does not restrict the message passing to be scalars but to be steerable features, and it also permits the injection of geometric and physical quantities into node updates. 2) Compared to previous steerable architectures (such as the works by Thomas et al., 2018 and Fuchs et al., 2020), SEGNN employs more general steerable convolutions, built upon the message passing network (Gilmer et al., 2017). The authors are suggested to separate the introductions of the preliminaries, related work, and the proposed methodology, instead of introducing them as a whole (section 2 and 3). Just justified from the performance, it seems on QM9, the performance between SEGNN and EGNN is close. Given that more complexity is involved in SEGNN for the computation of Spherical harmonic and Clebsch Gordan products, the benefit of involving steerable features is not well verified experimentally. Besides, on OC20, there is no comparison with EGNN, why?<|endoftext|>This paper proposes SEGNNs, a message passing neural network which is equivariant to 3D rotations. Equivariance is achieved by enforcing SO(3) equivariance on the node and edge networks using what the authors call Steerable MLPs. SEGNNs may be considered non linear convolutional networks with equivariant anisotropic messages. ### Strengths  There are several existing equivariant graph neural networks including TFN, SE(3) transformers, Schnet, and E(n) Equivariant GNNs (EGNN). ### Weaknesses  The paper suffers from some lack of precision and formalism. Section 3, in particular is very difficult to read. Section 3 is also claimed as a contribution in the form of “unifying view.”  As it stands, I would not consider that to be the case. ### Questions  Why are TFN and EGNN not used as baselines for OC20?<|endoftext|>In addition, the MLPs in message passing should also be replaced by steerable MLPs. The experimental results on N body, QM9 and OC20 demonstrate the effectiveness of proposed method. This paper is well motivated and well written. I have few concerns below for the authors to address. perform well on OC20 IS2RE.<|endoftext|>This work extends the concept of steerable kernel to nonlinear transformations in MLP. The steerable MLPs are then incorporated into recently introduced equivariant message passing layers thus forming Steerable E(3) Equivariant GNNs (SEGNNs). When applied to molecular property prediction tasks, it achieves mixed results on QM9 data and state of the art results on IS2RE task of OC20 dataset. The paper is well written. The main strengths of this paper are the strong empirical results on recently introduced benchmark (OC20) for learning on small molecules. Although I enjoyed reading the manuscript I think it would benefit from a clearer presentation of the novel technical contributions. Some datasets such as GEOM Drugs contains max.<|endoftext|>Namely, through steerable node attributes a, either derived from the physical setup (forces, velocities) or from predictions (similar to gating), the MLPs can be applied node wise and generally used in steerable feature fields as non linear activations._A very strong paper that provides an elegant framework for building E(3) equivariant graph networks. The proposal is very general and well motivated; it is also well situated within the literature. The exposition was very clear throughout, but especially in the description of the model, the introduction of steerable vector spaces, the affine transformations, and related work. I would almost encourage the authors to publish a longer form version of this paper with everything in the main text. 5.The empirical results seem very strong and three pretty different datasets. The submitted code uses e3nn, which (while very nice) is a pretty complex codebase.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 8; rating score: 8; This work proposes modifications to the highway connection networks (HCN) through the introduction of masks for selective weight updates and a layer wise normalization to mitigate internal covariate shifts at feature level. Experiments were performed on the Permuted MNIST, incremental CIFAR 100 and incremental CUB200 by incorporating this approach alongside EWC, ER and HAT. * Comparison with relevant work is missing: The idea of sparse, selective update of parameters to mitigate catastrophic forgetting and encourage task separability has been explored previously. The UCL approach was also mentioned in the paper but not compared against. * Continual learning approaches are typically evaluated in both task  and class incremental learning settings, it’s not clear from the experiments about which scenarios is considered. * Classification accuracy might not be the best metrics to evaluate the continual leaning potential. * Do the results assume a certain task ordering or have they been chosen randomly. The results look promising, but the novelty of this approach seems limited since the use of learnable masks, gating mechanisms, and layer wise normalization have been previously proposed in the continual learning context. Moreover, the experiments are limited in terms of comparison with other state of the are continual learning methods, evaluation metrics (such as backward and forward transfer) that characterizes the forgetting behavior.<|endoftext|>The paper investigates the backbone networks that are less prone to catastrophic forgetting. Two modifications to existing backbones are found to be useful: a mask attached to the gate function of the Highway Network, and layer norm (without tuning parameters). Experiments on top of existing popular learning algorithms designed for continual learning (EWC, ER and HAT) show that these modifications work. The findings in the paper are useful, adding evidences to the existing literature on continual learning that we need to pay attention to the interplay between learning algorithms and backbones. The introduction of the mask on the gating function in Highway Networks is interesting. However, this seems to be specific to this particular architecture. Also I would like to see more theoretical analysis of the empirical findings. However, the novelty seems to be limited as it is based on well known architectures and ideas.<|endoftext|>On the other hand, Layer Wise Normalisation is a new normalization of activation in the neural network. The paper is hard to read and uses very strange notation. The modification proposed in the paper seems small and can be understood as an effect of architecture search. The most interesting part is the experimental section. The authors have verified six classifier networks with four continual learning setups over three datasets. Unfortunately, it is not clear why the authors use SGD instead of adam. It is unclear why we do not have results on a classical convolutional neural network with 5 10 layers. 1 on Perm MNIST wins LWN in MHC (70%) and on Inc Cifar100 and Inc CUB200 LWN in MHC (10%). The proposed modification seems to be small and not significant. The experimental section looks nice and is convincing, but authors should use some additional architecture and show how the method works on adam optimizer.<|endoftext|>This paper proposed two architectural improvements for networks designed for continual learning. The value of each of these elements is made clear from thorough experimentation. This paper excels at making clear the importance of each of the proposed changes to HCNs. The experimentation is thorough, showing strong performance of the proposed methods on several datasets. The motivation for masking and layer normalization is made clear and is supported by empirical measurements. In general, this paper makes a compelling case MHC and LWN. The improvements proposed show strong performance in every case considered. Figures 3 and 4 are a bit hard to read, and perhaps representative quantities could be made explicitly clear in the text rather than only using phrases like "deeper colours" (and less critically I think the fonts should be larger in those plots). These minor points do not affect my score. (i) In Section 1, the past and present tenses are both used to describe things done in this paper   this should be consistent. (ii) In the beginning of Section 4, it says "Overleaf in the top half of Figure 2... " and this may be a typo. (Note that I am not an expert in continual learning and none of my own research has been in this space.With that in mind, I admit that I am not up to date or extremely familiar with existing related work.) I think this is a strong paper with compelling experimental results.<|endoftext|>These modification introduce fairly simple masking rule for selection of portion of weights to update and normalisation that (presumably) further adjust the internal representation to be more selective in which weights to update for a given task. I find the general approach of "baking" in the ability to handle continual learning into the network architecture quite an elegant way of solving the catastrophic forgetting problem. Though the proposed modifications to the HCN architecture are rather simple, I think this paper is worthy of note, because these changes make the network quite more robust for continual learning, and thus present a relatively straight forward, headache free (other than deciding on the MHC threshold) architecture that seems significantly less vulnerable to catastrophic forgetting. The paper would be stronger if there was a bit more theoretical explanation of how the internal dynamics of the masking and normalisation "select" weights in a way that does not destroy what the network has learned. As it stands, the explanation is mostly based on intuitive understanding of the dynamics and empirical evidence, and quite general principles, akin to the argument that HCN is good for catastrophic forgetting, because it multiplies the derivative by a fraction. It seems that these two statements contradict each other. While overall the paper is pretty well written, some parts could be improved for clarity. Mathematical notation seems cumbersome (why use $L_\gamma$  etc.subscript everywhere to indicate layer index...when just the $\gamma$ subscript will do for index identifying the layer. Why the neuron s output is labelled with $a$ for MLP and with $k$ HCN based models? Also, HCN could be explained a bit better (in Section 2).
Reject; rating score: 5; rating score: 6; rating score: 6; Lack of literature review:In general, this paper aims to solve the early/optimal stopping of the Bayesian optimization (BO). Plenty of related works are even not mentioned in the paper, e.g.Freeze thaw BO [1],  Multi fidelity BO [2], Hyperband BO [3], and BOS BO [4], these works all discussed how to achieve efficiency and saving budget in BO. A detailed discussion of the difference between the proposed method and all the previous works needs to be mentioned, not to mention comparison experiments. The stopping criterion is mainly decided by Eq.(7), specifically, determined by the upper confidence bound and lower confidence bound. Through the paper, the authors do not mention the role of $\beta_t$ and its impact on regret. In Proc.NIPS, 2016. This paper needs a major revision.<|endoftext|>The paper proposes a new approach for automatic termination of hyperparameter optimization based on Bayesian Optimization (BO). The idea is to construct high probability confidence bound on the regret and then determine when to terminate the BO process. But the maths behind the proposed approach is not rigorous and confused. Overall, I think the main idea of the work is interesting. Also, by checking the Algorithm 1 in Page 12 in the Appendix, I have a feeling that this Proposition 2 is just a proposal from the paper.<|endoftext|>In particular, they construct a high probability confidence bound on the regret, and then the users can specify a desired tolerance that shows how accurate the final solution should be compared to the global optimal. The results demonstrate the effectiveness of their proposed approach. The problem setting of early terminating the entire BO process is interesting and novel. It is also reasonable to notice that overfitting can occur in HPO. The problem is also well motivated. This paper proposes a novel early stopping criterion for Bayesian optimization.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; This work provides a systematic way to adapt object detectors to an unseen target domain. Clarifications needed:  the bounding box adaptors take as input the foreground proposals, which are the crops of objects, as shown in figure 3. If the adaptors are trained to align the features such that the discriminator is confused, which means the source bounding box distribution needs to be matched with the target, hence biasing the regressor again. Would different domain pairs have different saturation iteration? How was this value chosen? the line on page 9: "The gain brought by box adaptation is consistent, for example when T   3, it can still improve the mAP from 47.0 to 49.1." Overall the writing and structure of the paper is good.<|endoftext|>The conventional domain adaptation techniques hurt the discriminability of the detector and ignores the adaptation on bounding box regression. The experimental results are very promising. The paper is very well written. 3.The decoupled adaption is a great option for avoiding the hurt to the detector s discriminability. Weaknesses:Will the authors release their code?<|endoftext|>The experimental results obtain SOTA for some combinations explored. It is not clear at this point of the paper what is the relation between domain adaptation and adversarial training. For clarity, the connection that currently is made  on "Related Work" should be presented as part of the motivation presented in the introduction to situate the reader on the challenges presented.<|endoftext|>This leads to the proposed D adapt, namely Decoupled Adaptation, that decouples the adversarial adaptation and the training of the detector. Experiments show that the proposed D adapt strategy achieves state of the art results on four cross domain object detection tasks. Paper overclaims its contributions. However, the novelty and significance are somewhat limited. Given these concerns, overall, I view the paper as being slightly below bar for acceptance. Authors have addressed a number of my comments in the rebuttal, and promised to addressed others (claims) in the camera ready revision.
Reject; rating score: 3; rating score: 3; rating score: 5; The algorithm can be combined with any compression algorithm for the gradient. ZeRO 3 allows the training of models with 10bn parameters or more on relatively few GPUs without a large communication overhead. I do not think this is the best dataset to showcase this method. I think the work is better positioned as communication efficient training rather than training to save memory. To me, it feels the rebuttal has been fruitful and could lead to a much better paper. I think it is better that the authors take their time to improve the paper and resubmit it to the next conference. Due to lacking expertise, I am not able to judge the theoretical part of this paper and I would refer to other reviewers for that part of the paper.<|endoftext|>In particular, the authors suggest compressing the error vector with a separate compressor, which results in the ConEF algorithm. Unfortunately, there are a few big issues with the proposed solution. I see two significant issues with this work. Secondly, the guarantees for the method imply that the methods are significantly slower than standard EF and may be even slower in runtime than SGD without compression. The improvement is significant only when the compression ratio is close to 0.<|endoftext|>The authors propose to compress the local error in communication efficient distributed training to reduce the memory to store the local error. Experiments show the training performance of the proposed method with different compression ratio. Besides, the scalability regarding the number of workers and the model size is not very convincing. extra memory cost. The model size of ResNet 18 on CIFAR 10 should be trivial compared with the activations.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 3; The paper presents a method (Zest) for measuring the distance or similarity between 2 supervised machine learning models while only requiring black box access to the model s inputs and outputs but while also providing various improvements over the method of simply comparing model outputs themselves on a finite set of data points. *** Update after author rebuttal *** The authors have added additional experiments in text and audio and have offered additional information regarding the significance of the application domains ( model stealing and unlearning). These improvements are achieved by fitting locally linear models in the region of various reference data points, following the well known LIME algorithm which was originally introduced for the purpose of model explanations.<|endoftext|>3.The experiments are carefully designed to demonstrate the benefits of the proposed method. This paper utilizes a model explanation methods called LIME to design a new method to calculating the distance between two machine learning models. Overall I think this is a good paper and can be accepted to ICLR. The paper proposes a new method to calculate the distance between two machine learning models.<|endoftext|>The authors propose a novel approach for computing an architecture independent distance metric to assess the similarity between ML models by comparing the global behaviour of the models as approximated through LIME. They demonstrate the method by applying it on instances of the CIFAR dataset, focussing on the two tasks of model stealing and machine unlearning. The paper is interesting and well written, although quite technical in scope.<|endoftext|>The paper mentions that Zest can also be applied to other domains like text and audio. More specifically, given each reference sample, it first calculates a linear weight based on a set of masked samples and their corresponding outputs for each model, then calculates the distance between the set of weights for all reference samples generated from the two models. Given that this paper focuses more on the empirical side, it would be good to include more diverse experiments and add applications in text/audio domains into the experiment section of the paper. More explanations on this may be required.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; The paper introduces a new method of doing curriculum learning for goal directed RL. It uses a notion of reachability traces to model "temporal closeness" of states to a goal state. The reachability trace function is then used to define a sequence of sub goals for which policies are learned iteratively, and then the final policy is learned  using advice from the decomposed sub policies. An interesting, somewhat novel approach, but experiments not thorough enough to show that method is truly robust; theoretical contribution seems completely wrong. The authors did seem to survey the related work carefully and situated their work well within it as being a new approach. One minor novelty in the experimental section was the idea of experimenting with misspecified reward function. The expectations will be different. In loopy environments I suspect you will find more cases of CL hurting.<|endoftext|>The empirical results on grid worlds and several maze environments show that the proposed method outperforms DDPG, RIS, PER, and EBU. * Pros  * The idea of discovering a sequence of subgoals and using it as a curriculum is interesting. It seems like the underlying claim of this paper is that it is useful to use the particular reward function for goal based RL, which is in fact a standard setting in goal based RL. Thus, I will keep my original score. **Update after the rebuttal**I m not entirely convinced by the author s rebuttal for the following reasons. This paper seems to propose a proper reward design (reachability) as a solution, but my comment was that a proper reward design seems to be on the problem side rather than on the solution side. It would is not only difficult to learn but also could hinder learning about the main goal.<|endoftext|>This paper proposed an automatic curriculum learning design method for goal orientated Reinforcement Learning. This paper is one of the papers that try to automatically propose a curriculum learning to improve the sample efficiency of the RL algorithms. See a new review paper https://arxiv.org/pdf/2003.04664.pdf. Is it supposed to be $\mathcal{R}_{\phi}$ instead of $r_{\phi}$ in Proposition 1? It is not clear what the authors mean by "This interpretation allows reachability traces to also potentially be learned via standard RL algorithms." The authors should compare to more baseline algorithms. I believe the idea behind the paper is not very novel and the performance is not very significant.<|endoftext|>The paper tackles the problem of curriculum design for single task/single goal reinforcement learning problems with sparse rewards. *Update*: Updating my recommendation to reflect the discussion by the others. I have some concerns regarding the empirical evaluations and the usefulness of the entire approach and look forward to engaging with the authors in the discussion period. However, I have concerns regarding the purpose of the proposed method (and hence the contributions) and the empirical evaluations. 3.[On “contributions”] I am not entirely convinced that the contributions [Section 1] are fairly evaluated in the experiments. The paper would greatly benefit by a set of explicit ablation studies that evaluate the usefulness of (say) the reachability traces in isolation from the proposed framework. I also have another small concern that does not affect my review but would be nice to have some clarification on:  7.
Reject; rating score: 5; rating score: 5; rating score: 5; A multi modal multi task model is proposed for UI understanding. The method is evaluated thoroughly on 5 different data sets and achieves good results. Strengths+ Formulated a multi modal multi task learning for graphical user interfaces+ A new two tower Transformer architecture is designedWeakness  By jointly trained on 5 tasks, the model is on par or slight improves over models trained on individual data sets. This does not match the observation on other image language data joint training where significant improvement is achieved. The 5 tasks are closely related, and it is hard to see how the model can generalize to a different task, such as generating/editing new UI layout  The main architecture of the model is new, while all the building blocks are not. The paper demonstrated an interesting problem addressed by a standard transformer based model. The potential of generalization to more tasks is not clear, and the improvement over single task models falls below expectation.<|endoftext|>This paper presents a multi modal Transformer for multi task modeling of user interfaces. The authors hence design a so called Versatile UI Transformer model which involves three modals inputs to handle five unique tasks. In summary, their main works can be summarized as followed:(1) The authors formulate multi modal multi task learning for graphical user interfaces and design a VUT model. The manuscript is easy to follow. The experimental results conducted on 5 distinct UI tasks demonstrate its superior performance over some state of the art methods. (1)	Tables 5,6,7 and 8 compare the multi task learning with the single task counterpart. However, their results show that the multi task learning cannot significantly facilitate the studied tasks. This is contrary to the original intention of this paper. More explanations should be given. (2)	There are some typos in this manuscript, e.g., “We experiment with VUT on 5 distinct UI task”. I am a researcher interested in UI and published many paper of UI on top conferences.<|endoftext|>I believe that this contribution isn t enough for me to recommend acceptance. The authors that with the proposed architecture, training on all tasks simultaneously is better than training on individual task alone. The authors also seem to have carefully designed their experiments. These insights might be relevant to the UI modeling community. The paper is well written, and the ideas are clearly presented but it lacks in technical novelty. 3.The authors provide several reasonable insights that might be relevant to the user interface modeling community — using object detection as part of multi task learning instead of standalone pre training task, design choices to create a single unified architecture for all the tasks, multi task learning outperforming single task learning etc. Overall, I thought the paper was easy to read and understand. Additionally, the paper lacks in a few other areas   comparison to single task models with respect to computational cost, inference time etc, the downstream tasks are similar but the performance gains aren t significant. Due to these reasons, I think the paper is marginally below the acceptance threshold. Are there any overheads/disadvantages because of multi task learning (Like a larger model size, inference time for individual tasks etc)? Such a comparison would highlight the advantages of the multi task model and would be helpful for the relevant audience. It is therefore not surprising that multi task learning should help these tasks. **Updates after rebuttal period** The authors addressed some of the concerns   showing inference time, model size and a discussion about training details and hyperparameters in the appendix. However, I am not convinced that the paper presents new insights that are relevant for the broader ICLR community.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; rating score: 6; rating score: 5; Finally, the paper provides theoretical guarantees for the proposed method and shows improved performance on a subset of D4RL tasks.<|endoftext|>[3] https://arxiv.org/pdf/2106.08909.pdfOverall, I think this is a good paper. Besides, experiments on D4RL tasks are provided to show the effectiveness of the proposed method. Besides, the proposed method is efficient and extensive experiments are provided to prove the effectiveness.<|endoftext|>* [Q2] AWR is a value based offline RL method and is probably the most relevant baseline. I think this claim should be more precise. Please clarify on this. This claim seems to be too strong for me.<|endoftext|>VEM is also not evaluated on the kitchen dataset. Pros: 1.The paper is clearly written and easy to understand. The authors provide theoretical guarantees of the convergence of the VEM and also show that the memory based planning module improves the convergence rate.<|endoftext|>There are quite a few contributions of this paper, but none of them are significant. The value based planning to conduct bootstrapping is efficient. 3.The theoretical analysis is convincing and enhanced the persuasion of the claims. 4.The experiments are extensive and supportive.<|endoftext|>The main contribution of this paper is the introduction of the expectile operator as a smooth interpolation between behavior cloning and value learning in offline RL. The authors provide theoretical analysis and empirical results for the developed method. (ii) The authors seem to assume that the reader is familiar with prior work on value based offline RL.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; The authors describe how to translate a probabilistic program into a normalising flow layer which maps samples from a unit Gaussian into samples from the probabilistic program. Results are generally positive for the proposed method. The paper is mostly well written. I m recommending acceptance because this paper makes a novel, yet simple and generic, way to impose inductive biases on normalising flows.<|endoftext|>This paper first proposes to take a probabilistic program and compile it into a sequence of invertible transformations. This is essentially a neural architecture search idea where the model is free to choose the prior or not. I greatly appreciated the new experiments and more discussion on why the structured layers are simple and have fixed parameters for essentially simple inductive biases. The empirical results seem relatively weak (especially because the baselines seem weak). Overall, I think the idea of the paper seems reasonable (i.e., to compile known domain structures into flows).<|endoftext|>This paper proposes a new type of normalizing flow for incorporating inductive biases into the model architecture. The paper claims to outperform the state of the art normalizing flows. Indeed, presenting training and sampling times of the proposed and the baselines models would strengthen the contribution.<|endoftext|>This paper tries to bridge the concept of the normalizing flow model with the structured layers that encode the domain knowledge (also called inductive bias here). Overall, I think this paper advances the research in this area, although more careful ablations are required to ensure the empirical results are indeed promising. and $a$ is undefined. The idea is novel and inspiring to me.
Accept (Oral); rating score: 8; rating score: 8; rating score: 8; A crucial property of H divergence is that it takes into account the decision loss; namely, it compares two distributions in a way that distinguishes them based on the optimal decision loss, i.e., "two distributions are different if the optimal decision loss is higher on their mixture than on each individual distribution." It also provides an empirical estimator for H divergence and studies its convergence properties. The paper studies several use cases of H divergence, including two sample tests. Strengths: Proposes a new type of divergence that compares two probability distributions from the lens of optimal decision making. However, there is still room for improvement related to the computation of H divergence.<|endoftext|>3.The paper is well written. I wonder if the result can be generalized to the case when the number of samples is different for p and q. Theorem 2 assumes that the same number of data points are sampled from p and q, respectively. Although many different types of divergencies have been proposed in the literature, they are typically decision independent.<|endoftext|>The conditions that the two probability distributions have non negative H divergence are given. Estimation and convergence of the H divergence are discussed. (e) The paper is clearly written. The proposed methods are applied to climate data for decision making in agriculture and energy production.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; This paper proposes methods for voice conversion. The representation space is divided into two parts   content and speaker, to enable disentangling of the content and the speaker. Objective as well as subjective scores are used to evaluate all systems. The paper proposes an approach for voice conversion. Here there are no separate speaker and content encoders and additional classifiers are added on top of speaker and content speakers. The paper proposes a method for voice conversion which aims to use 2 classification tasks to learn content and speaker embeddings. I think the paper in the current form is not ready for publication even though it appears to be a meaningful approach for VC. The paper talks about content and speaker information leakages into the embeddings. There are quite a few typos as well as language errors. 8.The experiments are also not very thorough and lacking in some aspects.<|endoftext|>The novelty of the proposed method is limited. To ensure the performance of disentanglement, a common speaker classifier and an adversarial speaker classifier are proposed. Experimental results validate the effectiveness of the proposed method. Hence, I do NOT think the paper should be accepted. (3) Two classifiers are proposed to ensure the disentanglement of speaker and content information. Hence, the novelty of the proposed method is limited. How about the other losses? There also many topos in the paper.<|endoftext|>The paper proposes a model for voice conversion. To achieve voice conversion, the approach is to separate linguistic content and speaker information into two embedding vectors. The theory in the paper is weak, if not wrong. Several weakness of the approach are not mentioned. The first and major assumption is that linguistic content can be separated from speakers. The approach has a few weaknesses. The first is about disentanglement. The introduction also does not motivate the proposed approach. This sentence does not have a verb.<|endoftext|>This paper presents a learning framework for Voice Conversion (VC) with single encoder containing both contextual and speaker information, along with some tricks on utilizing timbre and speaker wise information in both training and inference phases. The problem formulation is subtle and interpretative, given the premise theory constraints. 3.The ablation study is also a bonus point, especially analysis on latent variables. 3.Problem formulation shall not be included as part of methodology. Does not have to be extensive but need one. However on the other hand, more than several statements are questionable, along with wording and grammatical mistakes.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 6; The submission proposes a model of the dynamics of trust from a reinforcement learning perspective. I think understanding trust is of obvious importance, so from that perspective the work is well motivated. Similarly with the high level claims on the importance of trust to society   it s fairly obvious that understanding trust is important, and an extended discussion of this is not needed. Considering s  is not known, transition probability should be defined for each action given state s . Finally, there are some typos, including:   "choosen action"  > chosen. "agent choose action"  > chooses. I am not an expert in trust modeling, but even so I think the paper as framed is mistargeted for the conference.<|endoftext|>The paper presents a computational formulation of trust. There is repetition in the paper, e.g., some quotations are introduced twice, that should be removed. Ideally, the paper would simulate previously evaluated human experiments and compare those results with their simulation analysis. The metrics used would also need to be comparable. Perhaps, as mentioned in the introduction this might be relevant for multi agent systems.<|endoftext|>These concepts are not really explored in this work. The paper is not about trust in other agents. The reviewer wonders if it would be better to rewrite the paper with a focus on self confidence? The idea of modeling self confidence is indeed very interesting and fascinating. They appear as given "modeling assumptions" in a sense. The introduction of distributional learning is also not clearly introduced in the paper. The reviewer does not understand why you need a different notation for the single agent as well. It seems to me that this is an oversimplification in a sense.<|endoftext|> The paper proposed an algorithm to model trust numerically in parallel with the policy learning process in multi agent scenario. 3.This paper provides comprehensive experiments on evaluating hyperparameter sensitivity as well as simulating the DKE overconfidence effect and reproduce the results from other works, adding confidence to the experiment findings. For example, what is the benefit or application of constructing a trust model? Understand that this may not be the primary goal of this paper (also somehow related to the first question), but this would add to the completeness and perhaps spark future interests among readers. Questions for clarification:1. Can you elaborate more on this?
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 6; The writing of the paper could also be further improved. The proposed contribution on loss designs is limited to the locomotion tasks, and it is not clear whether these losses would generalize to other robots and the real world [1, 2].<|endoftext|>** The importance of features section is really unclear. The paper is interesting, proposing a clean framework to perform locomotion tasks on mass spring systems. * The experiment section is not really convincing and hard to follow. However the experimental section is weak.<|endoftext|>This paper provides a practical guide on how to train an NN based controller for locomotion tasks with robustness and efficiency. $g_v(n)$ as a velocity should have a dimension of 3 (or 6 if rotation is included). Maybe you mean $||g_v(n)||$. Why only target velocity is considered, how about the height $g_h(n)$? I did not find how Adam is modified.<|endoftext|>The second claimed contribution "end to end differentiable simulation environment, including mass spring and material point" is not well supported. So only a small subset of simulation will work. If the application of this paper is only locomotion, its contributions are quite limited. The paper demonstrates good results for learning soft body locomotion.
Reject; rating score: 3; rating score: 3; rating score: 6; This submission proposes to offer a better initialization by using image pyramids and Q learning (top down manner) for the original recurrent visual attention (RAM) model (bottom up manner). The proposed method has been tested on several image classification datasets, including MNIST, cluttered translated MNIST, SNHN (sequential multi digit recognition). They also test the robustness to adversarial attack (PGD attack) on CIFAR10. Now I do not know where the improvement comes from. But there is not any in depth analysis about it. BTW, I think a fair comparison to CNN based methods would be a spatial transformer network, or any other CNN with spatial attention. Tab.3: Seems to me that RAM based methods are very suitable for sequential digit recognition since they have the concept of glimpse. better report run time speed and computation cost for all the comparisons.<|endoftext|>This paper extends the recurrent attention model(RAM) with another extra top down attention. The whole paper is well written and well organized. In contrast, I suggest the authors can discuss multiple step top down predictions, and how to (dynamically) control the step of top down prediction or trade off the steps in two types of attention. 2.The comparisons with existing works are unfair. Instead, I suggest including the number of steps of the top down steps into evaluation. 3.Motivations of the proposed context constraint are not clear. The idea itself is interesting, and but I think these are several limitations of existing versions (cf.the weaknesses part), and the submission can be further improved by solving my concerns.<|endoftext|>In this paper, the authors propose a novel method to unify the top down and bottom up attention together for recurrent visual attention. They also propose two constraints in the bottom up recurrent neural networks for better balancing the trade off between exploration and exploitation when searching local regions. Weakness: The authors should give some visual examples to show the difference of the searched regions between the proposed method and other compared methods, especially the RAM model.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 5; Then, the authors could summarize their work in the conclusion instead. The authors propose a neural GAM (NODE GAM) and neural GA2M (NODE GA2M) that scale well and perform better than other GAMs on large datasets, while remaining interpretable. There is novel integration of 2 methods previously not combined before  A variety of datasets seem to show that the proposed method is useful. I would tend to accept this paper as it is novel enough and supported by empirical experiments.<|endoftext|>They show that their proposed NODE GAM and NODE GA$^2$M architectures perform comparably to the baselines of Explainable Boosting Machines, tree based GAM/GA$^2$M and the traditional spline based GAMs on medium sized datasets while outperforming them on larger datasets. They also show that NODE GAM can benefit from self supervised learning by pretraining a model to reconstruct the original input from a masked input. I believe this is similar to the approach taken in the NAM paper as well (https://arxiv.org/pdf/2004.13912.pdf; cited).<|endoftext|>The authors proposed novel architectures for neural GAM and GA2M, which preserves the interpretability of GAM and leveraging the deep learning architectures for performance gains. The performance of methods (NODE GAM and NODE GA2M) were comparable with the other GAM methods. Compared to other similar methods, the proposed method has a better scale ability, and better performance on larger datasets. For the design of the new architecture, the authors made three major changes to the NODE architectures. The paper proposed an interesting architecture for interpretable deep learning models. However, based on the empirical results, the method does not seem to be superior or only marginally better than traditional methods such as the spline model.
Reject; rating score: 1; rating score: 5; rating score: 5; rating score: 5; rating score: 5; ICLR 2021. There are various obvious weaknesses in this paper including the comprehensiveness of the experiments and the novelty of the problem definition and the proposed method. The proposed framework is not new. Semi supervised long tailed recognition has already been studied with the name of imbalanced semi supervised learning [1,2,3].<|endoftext|>This paper proposes a new setting semi supervised long tailed recognition. Stage 3 trains the classifier with class aware resampling on top of the refined feature extractor. The setting is more realistic than supervised imbalanced recognition in that imbalance usually comes together with unlabeled datasets. The authors introduce the setting and the motivations clearly. For standard semi supervised learning, the authors compare with pseudo labeling and mean teacher. It could e interesting to compare with this baseline.<|endoftext|>They consider both of the labelled data and unlabelled data exhibit long tailed distribution. The structure of this paper is also good. The improvement lies in classifier. It is actually more like considering the characteristics of long tail in semi supervised learning. It may not so significant by constraining that the unlabelled data also exhibit long tailed distribution in long tailed recognition. It seems contradictory with the motivation.<|endoftext|>The biggest weakness is the experiments. Most of the existing long tail methods after decoupling are not compared in the experiments (for example, BBN[2], logit adjustment[3], and RIDE[4]). (2020).Long tailed recognition by routing diverse distribution aware experts. Fixmatch: Simplifying semi supervised learning with consistency and confidence. Since the paper is not a theoretical paper, I think the novelty of methods should also be considered.<|endoftext|>However, I believe that a more straightforward baseline should be compared in order to justify the design of the alternate sampling strategy. This is my main concern about the paper. The finding comes with solid empirical evidence (Table 2) and analysis (Sect 3.5). For example, some recent works [1, 2] study the class imbalanced semi supervised learning as well, a discussion on these methods should be necessary.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; This paper presents a graph and attention based architecture to model the dynamics of complex physical systems. Their experiments indicate a significant (in cases, upto two order of magnitude) outperformance of a state of the art graph network baseline. The current state of the art of physical simulation with neural networks, is MeshGraphNets, which uses a GNN archictecture to learn one time step updates. These updates are integrated over time to produce arbitrary length simulations. Another key novel aspect of the paper is in the reduction step, where an input graph is reduced to a much smaller, but representative ‘summary graph’. The experiments and results presented in this paper provide compelling evidence to both claims, and as such I would recommend this paper for acceptance. Is there any insight on how succinct the learned representation might be?<|endoftext|>This paper proposes to use auto regressive sequence models with attention mechanisms to predict the evolution of physical simulations. The paper focuses on encode process decode structures, and a transformer is proposed as main method to predict future states in the latent space. However, what would be important to demonstrate is that the models are trained for sequences of length n, and then evaluated for much longer sequences, e.g.2n, 3n or higher for input parameters not seen at training time. The appendix gives additional details of the network architectures used. Post rebuttal: the authors have answered my questions, and added two interesting simulation results in the appendix.<|endoftext|>This work proposes a new algorithm combining graph neural network (GNN) and auto regressive sequence models for physics prediction problems. The authors first use GNNs to compress the physical graphs, then use transformers to predict the next steps of the compressed representations, and finally use GNNs to recover the graph representations from the predicted representations. 2.More results on how the performance of the GNNs for compressing the graph representations into hidden states are needed. Or is that newly proposed in this model? This new model uses graph neural networks to compress the physic graphs into lower dimensional representations and then uses transformer models to predict the dynamics. Through empirical studies on three physical scenarios, the authors show the advantage of this method compared to the previous SOTA.<|endoftext|>The paper presents a GNN based model with temporal attention to auto regressively predict flow velocity of 3 test cases in fluid dynamics. Prediction results and some diagnostic analysis on the components of the network have been done. 1.The paper s abstract starts with "Auto regressive sequence models for physics prediction are often restricted tolow dimensional systems, as memory cost increases with both spatial extents and sequence length." This is not true. 2.The cases presented are not that high dimensional. In fact, the cylinder flow can be decomposed to its first 3 POD modes which are enough to describe the dynamics of the system. 3.The authors should dig some literature to look at methods used in what can be considered state of the art in this field of data driven physics. I feel this section of the paper to be very insightful and interesting.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; They also propose to augment a pre trained LM with a dynamic entity memory and cross attention to improve the entity usage. (3) The proposed entity memory augmented model is reasonable and improved entity coherence/consistency in some of the experiments. Weakness:(1) The two proposed metrics about entity coherence and consistency are intuitive but not rigorous. Similarly, for consistency, the authors considered all verbs and adjectives as the attributes of the entity. Overall, this work proposed some interesting ideas to analyze and improve the entity usage in long narrative generation task.<|endoftext|>Considering little analysis that has been done on analyzing the ability of pre trained language models (LMs) to maintain entity coherence and consistency, the authors take narrative generation as a case study and analyse the long range entity coherence and consistency in generated stories. Overall, the paper is well written and organized. 3.There is a lack of experimental support of why the proposed entity consistency metric is  needed since it gives lower correlation with human ratings in Table 4. 2.The work proposes new automatic metrics for analyzing narrative generation where some show high correlation with human evaluation.<|endoftext|>In this paper, the main contributions are 1)analyze the entity coherence of  pretrained LMs by leveraging some existing metrics and also four newly proposed metrics 2) propose a new generative model to have a higher quality generated narratives from entity coherence and consistency perspectives. Pros:The motivation of the work which is relevant to a well known issue in existing long text generative models has been written clearly and figures are very helpful in this regard. Author try to contribute in two main problems first proposing automatic metrics to measure and analyze the coherence of generated texts by existing LMs and second proposing a model to improve the coherence of generated texts by augmenting an entity memory. Cons:There are some not clear sections that need to be explained more clearly. Assume we have one entity but in two different forms such as mom and mother throughout the plots, according to what has been explained in the paper model considers them as two separate entities. In the method section, since the main idea of the work is about entity memory, presenting some instances for plots/entities/attributes  can help author to better follow the steps and the proposed model.<|endoftext|>Further, the authors also propose using two new metrics for automated evaluation. Further, the dynamic nature of the proposed entity memory network allows multiple attributes to be associated with an entity. Further, the example provided in the paper Fig 1b seems to be an issue with local coherence instead of long range entity coherence. However, this approach does not take into consideration multiple entities in a sentence and attribution association in those cases. This paper tackles the problem of entity coherence and consistency in narrative generation and tackles the problem through the use of an entity memory network that augments the pretrained language model.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; In this paper, the authors introduce a new dataset (actully, a collection of datasets) called MetaShift. This dataset can benefit the community by allowing a more system evaluation of dataset shifts. The proposed dataset would be much more useful to the community if the authors would provide a much larger subset of “pre made” datasets for easy experimentation. + The proposed approach to leverage metadata (form previously published large scale dataset) to create datasets of domain shifts is simple and well motivated.<|endoftext|>This work proposes a collection called MetaShift to study the impact of dataset distribution. **[Strengths]** Good idea to study the impact of distribution shift. Section 3 gives a good introduction to the step by step construction of MetaShift. + Section 4.3 is interesting and the results give some insights. Again, more challenging multi classification setting would be very useful. Specifically, this work proposes a framework called Metashift, which contains systematic annotation about the differences between different shifts.<|endoftext|>Their contribution is that they have a large collection of 12,868 sets of natural images across 410 classes obtained from visual genome data and its metadata for annotations. This helps in accounting for large natural shifts in the data. They also provide a way to measure distance between two subsets to quantize the distribution shift between any two of its data sets. They provide the impact of shift distance on the domain generalization by keeping test set same and varying training subsets randomly. Overall, it’s a well written paper about the motivation, use cases, applicability, and generalizability of their proposed data set. Other obvious underlying data bias issue has already been acknowledged in the paper too and I hope the authors will research more into solving it.
Accept (Poster); rating score: 8; rating score: 8; rating score: 5; (4) It would be better if authors could provide more analysis and ablation study on the relationship between accuracy of camera pose estimation and distribution of hallucinated correspondence. It seems that I am not able to find it in the main manuscript. Strengths:(1) This work provides an interesting research direction correspondence learning, in which occluded and out of FoV correspondence can also help large baseline camera pose estimation. (2) Based on this insight: authors proposed a new model for hallucinating the correspondence based on attention blocks.<|endoftext|>This is an interesting re formulation of the correspondence estimation problem, the conventional formulation of which considers only keypoints that are visible in both images. The description suggests that this should be recall. I consider the use of four publicly available datasets, two indoor and two outdoor, sufficient. Generalization on additional datasets not used for training is also shown. *Other Comments*I find the term hallucination in the title marginally acceptable, but I think that it is abused in the rest of the paper.<|endoftext|>This paper uses a deep model to accomplish this goal. + The paper proposed a method that is based on correspondence map and neural reprojection error [Germain 2021] with sufficient details. As a result, this paper adapts [Germain 2021] to solve this problem. In figure 4, I do not find the precision for identified keypoints.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; The findings in this paper and the model would be useful to the community. strengths   VQVAE and VQGAN have shown impressive generative performances but they were still lagging behind CNN based GAN models such as styleGAN. Although the model is a relatively straight forward combination of ViT and VQGAN, its simplicity makes the model widely applicable  The tricks introduced to improve the model such as projection to low dimensional space for code index lookup and l2 normalization of codes are useful findings.<|endoftext|>After modeling the discrete tokens with an auto regressive transformer, we could observe that the generation results of the proposed ViT VQGAN are really amazing and beat most of previous works in various benchmarks. $\ell_{2}$ normalization on latent variables. The paper is also well and clearly written.<|endoftext|>Though the empirical results of ViT VQGAN are quite impressive, I still have a concern about the technical novelty. * I’m seeing the potential and practical benefits of the proposed method, even though it is a fairly straightforward approach by replacing CNN into ViT in the VQ GAN framework.<|endoftext|>This paper builds on top of VQGAN and DALL E like models to achieve SOTA FID and IS scores for image modeling and unsupervised feature learning. The description of the architecture as well as the presentation of the results are very clear. This paper achieves very good empirical results and build on top of several recent works on VQ + Transformers for image modeling. It is better to have a footnote for the completeness of the paper.
Reject; rating score: 5; rating score: 6; rating score: 6; Adding supervised experiments can add to the comprehensiveness of the experiments. The authors point out that despite the benefits of this for many general detection tasks, this will be a detrimental property when dealing with more colour dependant tasks. Planckian jittering is a physics based augmentation method proposed by the authors (although the formulations for it come from existing literature) to re illuminate the training images within a realistic distribution which leads to more realistic and constrained colour augmentations than colour jittering. Linear classifiers for CIFAR 100 and FLOWERS 102 classification tasks were then trained on top of each of the SSL models  features, where FLOWERS 102 is the task that is claimed to be more heavily colour dependant. However, the experiments are not comprehensive enough to strongly support the practical claims. Thus, my vote is for the paper to be rejected in its current form.<|endoftext|>Overall I find this work as an interesting exercise in the context of data augmentation, although the technique is not novel and the general consequences are not extracted. As it is presented now, it depends on the database or on the relevance of color for class discrimination. My opinion is that the proposed augmentation (as any natural data augmentation) is going to be more positive in too artificial (too restricted) databases where illumination is fixed or it has too low variability.<|endoftext|>The proposed Planckian jitter performs better with the recent contrastive and self supervised learning schemes. This color augmentation can be applied to a wide variety of tasks. Many tasks need color augmentation performed on the fly during training. If the proposed color augmentation is much slower, the impact of this work will be marginal. This paper proposed a general color augmentation that performs better than the typical color jittering and demonstrates better performance in contrastive and self supervised learning schemes.
Reject; rating score: 3; rating score: 6; rating score: 6; This paper proposes an automatic approach for attacking classifiers, by approximating a possible adaptive attack that can take place on a newly published defense. The methodology (MAMA) applies meta machine learning techniques, by training it on different defenses to let it grasp what might be a good signal to follow when attacking an unknown classifier. The authors then test this strategy and a naive version of it (that is, the one that is only trained on the attacks, and not fine tuned on a test set) against other attacks proposed in literature. Results suggest a modest improvement against defenses hardened with adversarial training. This is misleading, since the discussion should revolve around the efficacy of the proposed attacks against the other ones, and the table seems to suggest an improvement that is not real. **Only adversarial training is considered**While it is true that adversarial training (AT) is becoming widely used, it is not the only defense that has been proposed, or that it will be proposed in the future. Hence, how this method can adapt to a technique whose loss landscape is very different from an adversarially trained one? Or rather, this should be stated as limitations (that are not discussed). As it is now, the paper starts by describing BMA, and later MAMA, but the paper title is centered on MAMA. So, the discovery of BMA is a bit misleading while reading. I opt for a rejection, since I struggle to see a clear contribution w.r.t.the literature. Also, these techniques only adapt themselves on adversarial trained models, limiting the scope of the meta learner efficacy.<|endoftext|>This paper proposes a model agnostic meta attack and achieves promising adversarial attack performance compared with state of the art adversarial attack algorithms. The proposed algorithm overcomes many issues, such as the vanishing gradient problem, training instability problem, generalization to other defense models. I think the proposed adversarial attack algorithm makes a nice contribution to the adversarial machine learning community, and this method can be considered as the main evaluation for a newly proposed defense model. 3: The proposed attack can be applied to a large scale dataset (ImageNet as shown in Table 5)The weakness of the paper is the lack of novelty. Meta learning based adversarial attacks have been considered before and using RNN as an optimizer has also been considered. The contributions of this paper are those improvements made to improve the performance. However, the contributions of this paper are the improvements to the existing framework.<|endoftext|>The presentation of the paper was very good. My main reservation is that the improvement in performance is only minor. That said, since it comes at little extra computational cost, it is certainly not an argument to not use it. Related to the above comment, the deltas reported in Tables 4 and 5 are against the Reported accuracies from the source rather than the author’s discovered accuracies from their experiments. This seems to show their method(s) in a better light and so is a bit disingenuous. Same at the start of the Ethics Statement 	In Algorithm 1, n is undefined 	“be integrating” should be “by integrating” in caption for Table 2 	Are there missing bold highlights on some of the data in Table 3? It is more of a summary of the paper. More general to say “all other classes” of course. The paper provides only small improvements in performance. Given the importance of effective adversarial example generation in properly assessing the risk in usage of ML solutions, any contribution to improving the state of the art is welcome. The authors draw a distinction to a similar Learning to Learn approach from 2020, establishing that though their approach is not therefore strongly novel, is distinct and has merit in its own right. Hence I would recommend this paper for publication.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; The main contributions of this paper are a series of empirical results on smoothed attribution methods that are designed to show several smoothing techniques in the previous literature may produce worse explanations and these smoothing techniques are also not robust to non Lp attacks. ### Weakness  It is not surprising that Lp based defenses (smoothing) are not robust to non Lp threat models in the community.<|endoftext|>Please modify this sentence. For example, the authors claim that most of the smoothing methods are less sensitive to perturbation of model parameters so they may not be helpful to debug a model, and they may not be "robust" to non additive types of attacks. The paper should provide experimental evidence showing significant robustness reduction. Also in depth discussion about the experimental results is needed.<|endoftext|>The paper touches on a very important problem   the quality and understanding of (smoothed) explanations. It is very well written and an easy to follow. After reading the paper I don t fully have a clear notion of what makes an explanation of this form high quality and the exact properties that one should be looking for to assess it. The main to areas for improvement / clarification are as follows.<|endoftext|>With small sample sizes, most errorsunreported, only a single model and dataset, and only a single run, there is alot of room to improve this work. They evaluate the robustness via cosine distance and LPIPS to a target heatmap,and the quality via spearman correlation to the original under cascaderandomization of model parameters, the sparseness of the attribution maps usingthe GINI index, and the explanation infidelity. dist(h(x_{adv}, h(x)))  why is the segmentation experiment is not conducted for the ad hoc methods?
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; The main claim of this paper is not rigious at all. why not report the time in experiments? and what is the time cost?<|endoftext|>**Minor points**  There is a missing explanation of how to rank the attack in Figure 2. Overall, I recommend reject to this paper. 2.This paper is well written and easy to follow.<|endoftext|>In table 1, can SAT reach a similar level of robustness at the same (or at least close) performance drop? Quantitative results demonstrate the effectiveness of the proposed method. And the proposed LRAT addresses some existing problems in IRAT.<|endoftext|>The paper perform various experiments showing that GAIRAT overfits to PGD and increases the robust accuracy of PGD.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; The paper proposes looking at intrinsic exploration from a multi agent perspective. The motivation is that if there are multiple parts of the state space, agents can explore these in parallel, thus speeding up the wall clock exploration time. Weaknesses:* The method is very similar to prior work. I’d expect at least an ablation with different weights of $w_o$ and $w_s$. * The results are not apples to apples comparisons. * Figure 4: These heatmaps are interesting and shed a lot more light on the method. I’d be curious to see these heatmaps for MultiRoom (the environment in Figure 3) because it’s less clear to me what each agent would cover in this more sequential environment.<|endoftext|>This paper proposes a distributed algorithm for exploration in reinforcement learning, based off of the principle of "divide and conquer". These intrinsic rewards are built out of standard intrinsic reward methods for exploration, but are modified so each agent not only gets reward from it s own intrinsic motivation function but also gets reward by how their actions score with respect to the other agent s intrinsic motivation functions. To the extent that this work addresses the underlying problem it would be impactful and significant. The experiments show best of the population of the agents in the proposed method, introducing a selection bias. A more apples to apples comparison would run the duplicates like mentioned above. * It seems like the studies mentioned in section 5.3 "Ablation Study" are not actually ablations, in that they do not remove parts of the methods, but they visualize the method and check its sensitivity to changes in it s parameters. I will weakly recommend rejection, as it is unclear the extent to which the effect of this method is from the parallelism v.s.<|endoftext|>This paper introduces a method for performing multi agent exploration by supplementing any intrinsic reward method by providing each agent an additional reward that is the sum of intrinsic rewards of all other agents were they to experience the agent s transition. The experiments show that this divide and conquer strategy for multi agent exploration results in state of the art results on MiniGrid and VizDoom. The experimental setting consists of standard exploration benchmarks—MiniGrid and VizDoom. The method is simple and the results are strong in comparison to the intrinsic reward baselines. Weaknesses  The D&E method is only benchmarked for the case when the intrinsic reward is a simple count based reward. Including an ablation using only the simple count based intrinsic reward would make the results more conclusive. Including information about how the additional hyperparameters introduced by D&E were tuned and how sensitive the method is to these parameters would be useful. This paper clearly presents a simple multi agent intrinsic reward method for encouraging agents to not only explore regions that they have rarely encountered, but also those that the other agents have rarely encountered. If the authors address the weaknesses pointed out above, I will enthusiastically raise my score to an 8.<|endoftext|>This paper presents an approach to enable parallelizable "divide and conquer" style exploration in sparse reward RL tasks, where each concurrent process maintains its own intrinsic reward function but they are combined in a way that minimizes redundant exploration of the state space across processes. Most closely related is [1] as it addresses the same setting of multiple concurrent agents solving a single agent task by a divide and conquer exploration approach. * It s unclear that the D&E reward will always result in the desired behavior. * The inner episodic count mask seems like it would not have a scalable analogue in more complex domains. * The method referred to as "Count" in the experiments is a psuedo count method which derives an approximate visit count from a learned density model. A worthwhile baseline (for only the MiniGrid setting) would be to train with the same "real" count based intrinsic rewards without concurrent exploration and the D&E reward. "Coordinated exploration in concurrent reinforcement learning." International Conference on Machine Learning. This paper presents interesting ideas; however, they are extremely similar to prior work that is not discussed or compared to in the paper. It is difficult to judge the significance of this work without comparisons to (or at the very least discussion of) this prior work.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; This manuscript considers the adversarial domain adaptation training problem, specifically the gradient reversal method, from the perspective of game theory. The authors show that gradient based optimizers without an upper bound on the learning rate violate asymptotic convergence guarantees to local NEs. The authors further show that these constraints can be lifted by higher order ODE solvers. Furthermore, they show hyperparameter robustness of their method and finally, the method is tested on more complex image and NLP datasets and compared to current SOTA methods. Overall better results are achieved. Strengths:* Well written and understandable* Theoretical sound in both problem description and solution* Analyzing domain adaptation learning from a game perspective and analyzing the stability of the optimizer with the gradient reversal layer model is novel * Relevant related work is considered and compared* The experimental part is sufficient and supports the theoretical claimsWeaknesses:* The theoretical analysis is limited to full batch training while in practice as well in the experiments mini batches are used. However, the authors mentioned it in the paper. * As with gradient based methods, this method could also converge to a non Nash equilibrium as outlined in [1], however this is likely a rare case and not unique to this proposal[1] Mazumdar et al.On Finding Local Nash Equilibria (and Only Local Nash Equilibria) in Zero Sum Games. https://arxiv.org/abs/1901.00838The proposed method is theoretical founded and the experimental part supports the claims and the analysis is novel. I vote for accept, good paper.<|endoftext|>They show the optimal point in the latter is a Nash equilibrium of a three players game. From this perspective, the authors show that standard approaches, like gradient descent, cannot work in this setting as the method is known to be divergent in such a case. Instead, they propose to use Runge Kutta methods (for example) to discretize the ODE, which gives insights for novel algorithms with better convergence guarantees. The introduction of both fields of domain adversarial learning (sec 2) and game theory (sec 3) is done properly, with an adequate study of the related work. From this perspective, using known tools from game theory / variationally inequalities / numerical analysis for ODE discretization, the authors show necessary and sufficient conditions for the existence of local Nash equilibrium (prop.1 and 2), showed that a modified dynamic of the gradient flow (equation (8) ) ensure global convergence of the Euler discretization, and developed an algorithm (RK 2, generalization of extra gradient, equation (9)). In terms of novelty, there are no "new" theoretical results, properly speaking. The novelty here is the derivation of the domain adversarial learning into a game, which is not straightforward but simplifies its analysis. For this reason, and because the paper is particularly well written, I recommend the paper to be accepted. The authors presented a link between game theory and domain adversarial training. I believe this could be also the case here.<|endoftext|>This paper analyzes adversarial domain learning (DAL) from a game theoretical perspective, where the optimal condition is defined as obtaining the local Nash equilibrium. From this view, the authors show that the standard optimization method in DAL can violate the asymptotic guarantees of the gradient play dynamics, thus requiring careful tuning and small learning rates. Based on these analyses, this paper proposed to replace the existing optimization method with higher order ordinary differential equation solvers. Both theoretical and experimental results show that the latter ODE method is more stable and allows for higher learning rates, leading to noticeable improvements in transfer performance and the number of training iterations. For example, Example  2 should be an independent paragraph in the paper but not a "window" embedded in the other section.<|endoftext|>Theoretical results led to a breakthrough in practice   the Domain Adversarial Learning architecture (Ganin et al., 2016). The paper suggests looking at the paper from a game theory perspective. This is natural, as the objective is to minimize the loss on the source distribution while maximizing the distinction between the distributions. The optimal solutions of the game are characterized by the local NE. Motivated by the results from game theory, the authors suggest replacing Gradient Descent (due to its limitation in this optimization problem) with other optimizers   ODE (ordinary differential equation) solvers. Strengths :The game theoretical formulation is natural for this important problem. The experiments seem to support the asymptotic convergence guarantees of the optimizer. The empirical results look somewhat significant. The practical significance of this work reveals that using high order solvers instead of Gradient Descent (GD) in this setting leads to better results. Weakness:The technical novelty is only marginally novel and quite straightforward.
Reject; rating score: 5; rating score: 5; rating score: 6; However, this is not a extensive in multimodal learning. The key idea of the work is to minimize the Frobenius norm of a Jacobian matrix, so that the multimodal prediction is stabilized. [1] MMTM: Multimodal Transfer Module for CNN Fusion, CVPR 2020[2] Deep Multimodal Fusion by Channel Exchanging, NeurIPSThe paper proposed a simple multimodal fusion methods based on Jacobian regularization, However, there are minor issue in related work, and experiments are not extensive, i.e.only valided on smaller Toy datasets. The paper provide a theoretical error bound of the proposed robust late fusion method.<|endoftext|>In this paper, the authors proposed a training free late fusion method for robust multimodal learning. They also provide a theoretical error bound of their method. The experimental results outperfom other late fusion methods. Does the proposed method can handle this problem? It would be better to clarify in the equation as well. Is this the work pipline of the algorithm, at training time, we fuse $z_A$ and $z_B$ directly; at test time, we fuse $W_a z_A$ and $W_b z_B$ with optimized $W_a$? But of course this is not necessary to be true. I would consider to raise my score, if the authors can address my concern.<|endoftext|>In the experiments section, this paper s method could outperform baselines for most of the time. The authors state that their method is *training free*. The rows in *Table 1*, *Table 2*, and *Table 3* are not clear. Could the authors be explicit about what the role of the extra modality is? It also provides theoretical guarantee on its performance. Further, this paper conducts comprehensive experiments to verify their method.
Reject; rating score: 3; rating score: 5; rating score: 5; The paper proposes to infer a relational latent space in order to do multi variate time series forecasting. The main contribution is reducing the computational complexity of the inferred graph by converting the fully connected graph learning problem to a bipartite graph learning method in which the nodes are connected to K auxiliary nodes. The paper is fairly well written and its motivation is clear. It is also technically correct. The authors tried their proposed method on a few small datasets. The main argument of the paper is reducing the computational complexity of the relational inference, which I think is a valid motivation for this paper, but I would have rather expected to try a more challenging dataset with a large number of nodes for achieving the same goal, i.e.see if the reduction in computational complexity in this way is helpful in a practical manner or not. The computational complexity needs to be discussed in the main paper as it is the main argument of the paper.<|endoftext|>Length" the prediction window size? Additionally, they show hyper parameter sensitivity results for varying K (number of latent nodes in the bipartite graph) for one dataset. Also details about the evaluation procedure are missing   in particular, was a sliding test window used to evaluate the performance, how many test windows were used, was re training done before each, etc.? The approach is interesting and has some novel elements, especially using a latent bipartite graph as a step towards better scalability. As pointed out the key novelty is really using the bipartite graph to increase computational efficiency, and as other reviewers have argued little analyses around this is done, larger data sets should be included, and training time scaling reported. Despite the authors claim that larger data sets do not exist, and aside from simulated data, some larger ones exist and have been used in several other papers. The complete method details and procedure is not clear from the description, and code is also not provided, so as it is the work does not seem fully reproducible. This provides more interesting information and understanding of the proposed approaches. Or are the embedding values updated as part of training as well? They also should compare to the state of the art multivariate forecasting methods (including without considering graphs, to demonstrate the graph approach is helpful), and especially need to compare to those designed to scale to many time series, which also was not done. b.)The description is a little confusing in a few places. This could potentially also be generated from larger datasets if needed by sub sampling series, or just using the natural number of time series in different datasets.<|endoftext|>This paper presents a method to combine information of multivariate time series by extending univariate architectures. Finally, synthetic datasets were used to evaluate the adjacency matrices in control scenarios. The latent graph inference idea could be useful to automatically identify the relations existent in a multi variate time series model more broadly; for instance, in partially observed settings. With respect to the weaknesses, while the paper is relatively well organized, there are many details that are not clear. In particular, the architecture is presented and its components listed, but the connection of these components and the overall goal of the paper is not discussed in enough detail as to justify the choices. The paper has some strengths that could be useful in applications of multivariate time series forecasting.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 8; ### Post RebuttalI would like to thank the authors for the additional comments to answer my questions. The first s that I don t still find the task of generating symbolic expressions interesting. The second is about an application of the approach. I don t disagree with this claim, but the paper doesn t show that the proposed approach is indeed helpful for that task. I would like to see more discussions and evidence for the story to hold true (e.g., how the augmented data are labeled, whether they can improve the performance of neural models that compute solutions, etc.) The paper addresses a critical problem in symbolic reasoning domains, and theexperimental results are promising. Then, it is not clear  for me why training on such data can produce a model with the performance very  close to the model trained on data with correct labels (Table 2). For the former, the generated data could be mislabeled. A critical problem with the  trained model is that its prediction might be wrong.<|endoftext|>This paper aims to generate training data for symbolic reasoning by training GANs based on Transformers. Training these models on symbolic reasoning data that is randomly generated   LTL and Symbolic mathematics   is found to generate high quality formulas. The model introduced in the paper is well executed, and based on the generated data attached in the submission, the data quality appears to be good enough. There are still a few concerns I have in the motivation of the problem and experiments.<|endoftext|>Generating symbolic reasoning training data using transformer GANs. For all results in the paper, it would be best to perform multiple runs and report the standard deviations. How do you know the satisfiability of the generated samples? Does training on Uncert e improve performance on LTLbase (more so than training on generated?) However, their application is very interesting. General: The paper does not clearly separate LTL results and symbolic math results. This would make a good baseline (even if your models perform worse). If the authors can add the suggested results and show that their results are statistically significant, I would be very happy to increase my score. How did you decide to stop training? Does this happen for all runs?<|endoftext|>The authors apply (W)GANs with transformer encoders for data augmentation in two symbolic domains: LTL and function integration. There are many interesting findings, three of which stand out. Second, for LTL, a GAN trained on a dataset of size 10k can produce a much larger dataset, such that training on the new dataset is almost as good as training on the original distribution (when evaluating on the original distribution). I think the paper would be stronger with two domains throughout rather than one. If a larger dataset than in S4.2, did you do a similar check for duplicates between the generated examples and the training dataset? Minor comment: I did not find that it added much to the paper to consider both GANs and WGANs.
Reject; rating score: 3; rating score: 6; rating score: 6; rating score: 8; **Problems**  The main contribution is Theorem 2, but I think there might be some mistakes in the proof of Theorem 2.<|endoftext|>Therefore, it seems trivial to obtain the key result of the paper "bounding independent ratios based on the number of agents".<|endoftext|>4) The total variation reported in the second section of the experiments are based on samples collected by the behavior policy. 2) Minor presentation issues. (iii) In section 6.1, is there a particular reason for using u^k as opposed to a^k?<|endoftext|>Clarity:   The x axis on most of the SMAC graphs should be in scientific notation. This seems to suggest in turn that a large number of epochs should accelerate learning when the clipping ratio is 0.1; did you observe this?
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 5; The paper proposes SWAT to incorporate the agent s morphology into transformer based policy in multi task RL. [Clarification needed] In Transfer learning, the train set of environments is a bit unclear for Humanoid++ and CWHH++ test tasks. Two ablation experiments analyze the performance impact on excluding PE and RE.<|endoftext|>This paper proposes investigates the role of positional and relational embeddings in transformer based policies for multi task reinforcement learning (MTRL) across different morphologies. Overall, this is a well motivated paper with good performance. The proposed traversal based embedding seems new and achieves good performance as shown in the ablation studies. The paper is well written and easy to follow.<|endoftext|>This paper addresses the inhomogeneous multi task reinforcement learning (MTRL) problem. Therefore, they proposed to encode the morphological information via traversal based positional embedding and graph based relational embedding. Strength:    The paper is well written and easy to read. The experiments show that the proposed approach outperformed competitive baselines such as SMP and AMORPHEUS. Weakness:     The reviewer has some concerns about the novelty of the paper.<|endoftext|>This work proposes to use traversal based positional embedding and graph based relational embedding to encode morphological information for inhomogeneous multi task reinforcement learning tasks. Experiments with more seeds are needed to strengthen the paper. s3) The paper is well organized and easy to follow. Weaknesses:The major weakness of this paper is the experimental evaluation.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; They build on this block to produce an architecture that compares favourably against both well known CNNs and classical ViT solutions in terms of number of parameters and task performance ratio. (+) the paper is well written, the contributions are clearly stated and supported by empirical results. (+) with the proposed block the authors are able to reduce the number of parameters while preserving or improving performance on classification, object detection and segmentation, suggesting they are indeed providing a good backbone for many downstream tasks. (+) the core technical idea of the paper is simple and convincing. I think the analysis should either be fixed or replaced with a FLOPs count comparison. The contribution is interesting and claims are supported.<|endoftext|>The experimental results are quite promising. The paper mainly uses number of the parameters to compare model complexity. This is a theoretical metric. I would also like to compare GFLOPS between MobileViT and other competing models (e.g.MobileNet family models, DEIT, MnasNet, PiT). Table 1 (b), comparisons with heavy weight CNN are not quite convincing. This is important for the main results in Figure 6 and 7.<|endoftext|>It attempts to build a model which can combine the strengths of CNNs and ViTs to build a light weight and low latency network for mobile vision tasks. Besides, the author should provide more details about the competitors for the readers  convenience. Note that there is only a 0.1% difference between those two models. This raises the following two concerns: (a) the base model MobileNetV2 can achieve such promising performance. CVPR2017.Overall, I think this paper is well written and  organised. Besides, according to the reviewer s previous experience, the proposed framework can also work well in most lightweight backbones.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; The paper shows that approximate sampling using SGLD may result in an unbounded privacy loss in the middle regime, via Bayesian linear regression. So readers can position this paper appropriately. 2, Subsections 4.1   4.2 are proof sketches to show that approximate sampling of the posterior with Bayesian linear regression by SGLD is not differential private in some steps (Theorem 1). 4,  The manuscript is not ready and needs to be further proofread. The finding is interesting and useful. The paper should also be re organized and proofread before it can be accepted.<|endoftext|>This paper shows that even when the posterior is as private as targeted in the beginning, sampling from posterior with SGLS might not be as private as targeted. They prove that for n big enough sampling from the posterior is (\epsilon, \delta) differentially private (DP), but there is a step in which releasing a sample will not be (\epsilon^\prime, \delta) DP for \epsilon^\prime \omega(n \epsilon). This work is quite interesting and important in the sense that SGLD is used in many works in literature and it is before proved that SGLD with specific parameter choices provides (\epsilon, \delta) DP. However the structure of the paper can be improved. One weakness of this paper is the literature review. There are papers that uses SGLD for differentially private deep learning, it will be very useful to cite these works to understand whether these methods provide (\epsilon, \delta) DP eventually or not. It is important to show SGLD might not always give (\epsilon, \delta) differential privacy guarantee.<|endoftext|>This paper provides one concrete example, showing that revealing one posterior sample generated by SGLD has the risk of a privacy breach when the SLGD sampling iterations number is moderate, while the exact posterior sampling has little risk of a privacy breach. Does the same result hold if we use the common sample in each step of the SGLD? So I doubt that this example provides general insights. Thus, even for this particular counterexample, I don t think the result is practically meaningful. By definition, it is sufficient to find a pair of neighboring data sets to counterprove the loss of privacy. But the results also depend on the specific setup of the SGLD algorithm, which I believe is not very proper. Similarly, if SGLD preserves privacy, there are potentially some requirements on the algorithm implementation.<|endoftext|>Since the SGLD updates are stochastic, it is often thought the solution can be suitable for privacy preserving of the data that is used to train the algorithm. Using a counter example, this paper shows that it is not necessarily correct to assume so. The role of \epsilon and \epsilon’ does not seem clear. Are \epsilon and \epsilon’ same? This paper analyses the differential privacy of the SGLD algorithm. It uses Bayesian linear regression as an example to demonstrate that while differential privacy holds at the beginning of the SGLD updates and similarly at the convergence, but it may not hold during the intermediate steps of SGLD updates.
Reject; rating score: 1; rating score: 5; rating score: 5; rating score: 5; The paper generally is easy to follow and read. However, my major concern is the novelty of the paper, which is a combination of PU and GAN. There are GAN model for semi supervised learning.<|endoftext|>The pros and cons of the paper are described below. The reviewer understands that there may not be a proper data source of unlabeled data or an out of distribution data source for ImageNet.<|endoftext|>This paper proposes a unified framework to leverage Positive Unlabeled classification and conditional generation with extra unlabled data and benefits the two tasks simultaneously. But the novelty is limited and the lack some key experiments to show the improvements. ## Pros  The combination of positive unlabeled classification and conditional generation with extra unlabeled data is naturally and sounds reasonable.<|endoftext|>The authors show that their method is especially effective for handling out of distribution unlabeled data. 3.For estimating the transition matrix \tilde{C}, usually it is a not an easy task in label noise learning. 4.For experiments, many recent PU learning works are not compared.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; The paper proposes a modular architecture for lifelong learning of hierarchically structured tasks with some theoretically guarantees. In the current stage, the paper is missing several important details, making it hard to understand if the paper is novel/significant or not. 2.While the paper focuses on theoretical contributions, it does include some empirical results to show that the proposed approach works in practice. 2.Very limited related work   I understand that the paper is focusing on hierarchical task distributions but there is much more relevant work than just the 2 papers cited on page . Other works on lifelong learning should also be cited eg: https://www.sciencedirect.com/science/article/pii/S1364661320302199 (these are pretty much the first works that came up on a quick google search). 3.The paper is hard to understand with many important details/definitions missing. What parts of the algorithm depend on this assumption. Since the paper is focusing on the theoretical aspects, it is important to explain/outline the effect of these asumptions. I look forward to the author s response and interacting with them to understand their approach better.<|endoftext|>However, the experiments fall short from a proper empirical evaluation for a lifelong/continual learning paper. The primary contributions of the paper are proofs of the learnability of certain classes of tasks with three different choices of modular architectures. Given the fact that the theoretical results appear to be relatively minor, I would encourage the authors to carry out a more complete empirical evaluation, similar to that of other lifelong learning works. The algorithms are also evaluated empirically on toy supervised learning tasks. The submission proposes an approach to lifelong learning using a sketch based modular architecture. This is one of the few works that addresses lifelong learning from a theoretical perspective############## Weaknesses ##############1. The empirical analysis is minimal############## Arguments ##############I believe that the line of work proposed in this paper is quite interesting and relevant: modular lifelong learning from a theoretical perspective. I did not read the appendices in detail. This wouldn t necessarily be a problem if the submission included substantial empirical evaluations that validated the proposed approach.<|endoftext|>This work introduces a continual learning architecture that combines elements for sketches, hash functions, modules and routing. **Strengths**The authors provide some algorithmic theoretical guarantees in a field where they are mostly absent, namely continual learning**Weaknesses** I have found the architecture quite involved to understand. If the paper is addressed to researchers working on learning theory and/or sketches, then this is probably fine. If however, the intended audience is your average continual learning researcher, I think that the manuscript need works. Although most of the introduction was easy to follow, I found the remaining sections cumbersome (again, it could be because of my background).<|endoftext|>The authors additionally provide extensions to the proposed architecture in the case of tasks depending on other tasks and lack of clear task boundaries. The major selling point of this paper for me is that the authors actually provide theoretical analysis and error bounds for the learning of a modular learning strategy. Another thing that could really improve the paper in this regard would be to provide concrete connections to past modular architectures in the literature. While I appreciate that the authors also provide theoretical results, it is hard to get much out of these experiments. I think there are some major positive aspects to this paper including that the authors provide theoretical analysis of their modular approach, which is rare in the literature. Additionally, the authors discuss extensions of the approach to more ambitious settings. On the other hand, there is lack of clarity in the writing in some places and the paper lacks positioning with respect to existing modular approaches in the literature.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; To some extent, more qualitative results could make their way to the main paper, instead of predominantly relying on the supplementary material in this regard.<|endoftext|>I recommend Accept for this paper since the strengths outweigh the weaknesses.<|endoftext|>More justification of the design choices can turn it from weak to strong accept. The main area where I believe this paper can be improved is by providing deeper analysis of the novel parts.<|endoftext|>Therefore, compared with the initial rating, I changed my rating to a positive one. + Well experimented (but only with learning based methods)My main concern for this paper is about the main contribution.
Accept (Spotlight); rating score: 8; rating score: 6; rating score: 10; This paper proposes a method to model original texts and similar texts in a graph structure for language modeling with graph neural networks. The new model achieves the new state of the art on WikiText 103 and shows substantial improvements over other language modeling datasets such as One Billion Word and Enwiki8 datasets . Strength:  The authors propose an interesting hypothesis that referring to the training data could be helpful for language modeling, and they showed that the method is able to make considerable improvements over the vanilla LM. The paper proposes a retrieval augmentation to the language models and use GNNs on top of a graph structure of the original input and its neighbors and shows that it consistently improves over the vanilla lm model. The empirical results are good and the authors provide examples showing that the retrieved examples indeed help prediction in language models. The only concern is that through modeling additional neighboring contexts, the method introduces significant overhead in running time.<|endoftext|>The paper presents a GNN based language model where neighbor contexts are retrieved, encoded via a graph neural network, and used to enhance generation. 2. improvements over state of the art models on three benchmarks. The work is among the studies of enhancing language generation by context retrieval, and the new idea is modeling the retrieved neighbor contexts through a graph neural network. My concerns lie in the following aspects:1. Comparison: it seems that the proposed model is compared with a retrieval augmented baseline  only on WikiText 103. Is it because there are no available results on the two other dataset? What is the exact setting for this equation? 1.Relatively incremental technical contribution to the community. 2.Relatively weak comparison with baselines. ——————————————————————————————————————————————————————————————The authors  response answered most of my questions.<|endoftext|>This work build a novel GNN LM to do language modeling by using global context information. The proposed model is novel and quite different from previous LM structures. This work in my view draws the connection between traditional n gram language model and neural language model. Extensive ablation study is conducted to understand the model. I would recommend  accepting the paper based on the novel idea to build heterogeneous graph to do LM and the impressive model performance on PPL. This good combination of GNN and LM can be valuable to the community. I would consider KNN LM is a special version of GNN LM. Overall, base on the novel idea of creating global context graph, GNN LM and show the significant improvements on all LM datasets, I would like to recommend accept this paper.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 5; The paper is motivated by the need to account for continuous attributes in fair Machine Learning. Since the joint distribution between model prediction and sensitive attributes might not be available in practice, the paper also proposes two estimation methods: histogram estimation and kernel estimation. The kernel method leads to faster estimation error convergence rate in terms of sample size. The paper provides an extensive evaluation with tabular, graph and temporal graph data, synthetic experiments, and classification and regression tasks. Section 3: The definition of GDP is precise and relatively natural. However, some justification of this particular choice is necessary. I think the theoretical foundations are a nice contribution of the paper. Although I understand that this might be part of follow up work, I think it is important to include a discussion on how other metrics can be similarly extended to capture continuous attributes. Demographic parity is a natural first step, though. Section 6.1: why were these parameters chosen? Have robustness checks been performed and what were the results? In my opinion, the paper touches upon a very important and practical topic in fair ML. It provides a quite extensive theoretical justification and guarantees as well as a complete and convincing empirical evaluation with several datasets.<|endoftext|>This paper proposes the generalized demographic parity (GDP). It aims to generalize the existing definition of demographic parity (DP) to incorporate continous sensitive attribute while preserving tractable computation. Overall, this paper is a bit unclear in some parts and lacks a thorough literature review. The experiments are not convincing enough as well. Concerns  The authors claim that GDP is tractable. I wonder how it is tractable if we do not know the joint distribution of prediction and sensitive attribute (as claimed by the first few sentences in Section 4). However, I have several concerns about this manuscript as shown below. Why cannot we use the same techniques (i.e., histogram/KDE) to estimate the distributions needed for calculating mutual information (e.g., joint distribution of prediction and sensitive attribute,  conditional distribution of prediction given sensitive attribute, or conditional distribution of sensitive attribute given predictions)? The proposed method uses histogram/KDE to estimate the distribution of sensitive attribute. (1) The authors established the relationship between GDP and adversarial debiasing by Madras et al.2018.Why is there no experiments on comparing the performance between these two methods? I wonder the justification behind the choice of baseline methods. (3) Mary et al.2019 also works on debiasing the continuous sensitive attribute, yet there is no comparison. Enhancement of the Neutrality in Recommendation. In Decisions@ RecSys (pp.1 8).[Related to fairness with KDE]* Cho, J., Hwang, G., & Suh, C. (2020). A fair classifier using kernel density estimation.<|endoftext|>This paper proposes a generalized demographic parity for group fairness which is computationally feasible for both continuous and discrete protected attributes. 2.The histogram and kernel estimations are computationally feasible. 3.The connection of GDP and adversarial debiasing is built. 4.The comprehensive experiments on multiple datasets show the efficiency and effectiveness of the kernel estimation of GDP. 2.It is unclear how to calculate the underlying GDP in the experiments, or it is just an estimation of GDP. The experimental comparison is comprehensive and well designed. The connection between GDP and binary demographic parity is built, but I wonder about the relationship between GDP and discrete protected attributes with multiple values, e.g., intersectional fairness. The extension from binary to multiple to continuous would be more smooth.<|endoftext|>This paper proposes a new extension of demographic parity when the sensitive attribute is continuous. It proposes two techniques to estimate this quantity from data. The authors also consider adding this quantity as a regularization term to control discrimination while designing machine learning models. Overall, the paper is well written and easy to follow. I like the structure of this paper: it starts with a new definition for measuring fairness; introduces ways to estimate this fairness measure; and presents applications of this measure. 1.My main concern is the estimation error bound provided in Theorem 3. It would be great if the authors can provide an example to show that this Lipschitz condition is inevitable. The order of estimation error given in Theorem 3 is O(N^{ 4/5}). However, it is unclear to me if this is the optimal order. The authors only consider a single sensitive attribute. What if there are multiple sensitive attributes (e.g., age, weight, income, …). Can the authors specify what the optimal bandwidth is? The paper is interesting but the technical novelty may not be sufficient
Accept (Poster); rating score: 8; rating score: 8; rating score: 6;   The paper describes a new benchmark. It uses images from ImageNet, which now have annotations of regions containing features spuriously correlated with the ground truth labels. The dataset is used to evaluate existing models, showing they often rely on regions marked as spurious. (+) The method to propagate annotations from a small set of human labels is simple, effective, and based on observations from prior work that features of the last layer of a CNN are semantically aligned with human concepts. (+) But the authors performed a validation (Sec.3.4) by crowd sourcing that seems to support their assumptions. My recollection is that it was more of a thought experiment (to be verified) and/or a citation of the Terra incognita paper of Beery et al.Typo: casual instead of causal  The message in the last section that the standard accuracy metric does not tell the whole story is a very important one, IMO. Experiments with the new dataset already provides new insights about existing models.<|endoftext|>This work proposes a scheme to identify  causal  and  spurious  features with human supervision. The neural features are then ranked and visualized with the CAM feature attribution method (called neural activation map in this work). Overall, this paper produces an annotated version of the ImageNet dataset with causal and spurious features. This is then used to assess the performance of current state of the art models showing that these models show significant susceptibility to spurious training signals. This paper tackles an important problem and provides a convincing demonstration that current SOTA models are susceptible to reliance on spurious correlation in the training set. The Causal ImageNet here is a dataset that indicates which are  core  features and which are spurious features. I would encourage the authors to completely discard the notions of causality that they refer to repeatedly across the paper. Further, the definition of  causal  features in this work is quite vague, but acceptable.<|endoftext|>In the paper, the authors presented a novel scalable method to differentiate the so called causal or non causal attributes. 2.The Visualization is very intuitive, this also helps me to quickly pinpoint the spurious vs causal features mentioned in the paper. 3.The scalability contribution is well justified by the paper. But must this feature be spurious? What if the river is very quiet without any ripple and it is literally a mirror? I think a more precise discussion about causal vs spurious in vision scenario could thus be very useful. Could the authors enlighten me a little more on how we could potentially utilize this model to further improve either the interpretability or robustness of NNs. After reading the response from the authors, I am pleased with the paper revision and decide to raise my score.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 5; The paper studies the problem of multi agent Markov games with one leader and multiple followers. The algorithms proposed have guarantees in terms of regret which is sub linear in the number of episodes for the online setting and converges to zero in the offline setting in the size of the dataset. However, the structure of the paper would benefit from some adjustments. Also, the algorithms presented in the paper are hard to follow: it is hard to understand the meaning of instantiating a function. The main idea of the work is not well transmitted. The paper would benefit from more discussion on the related works and analysis similar to the one presented in the work by Balcan (2015).<|endoftext|>Based on optimistic and pessimistic LSVI, the paper proposes algorithms that achieve sublinear regret in the online and offline setting, respectively. This looks tabular, and for large state spaces, it is intractable. The paper closely follows (Jin et al.,  18; Jin et al.,  20b) and the technical contributions seem a little incremental. What is the prevalence of the problem that is considered in this paper? The literature review and notation are deferred to the supplementary material, and the paper is missing a conclusion.<|endoftext|>The paper studies multi player general sum Markov games with one of the players designated as the leader and the rest regarded as the followers. In particular, we focus on the class of games where the state transitions are only determined by the leader s action while the actions of all the players determine their immediate rewards. Of course this is not an issue of this specific problem but of the whole community. I feel that there is a huge lack of motivation in the problem. Interesting paper with important theoretical contributions.<|endoftext|>The paper studies the problem of computing Stackelberg Nash equilibria in a certain restricted family of Markov games, in which the followers do not control the transitions. Assuming an oracle that solves each stage game, the authors develop an algorithm for solving for Stackelberg Nash equilibria in the overall game. Strength: The paper provides a near optimal and, to my knowledge, novel regret bound for learning Stackelberg equilibria in the chosen setting. Weaknesses: The setting is not very well motivated. The authors stipulate very significant assumptions on the problem (the existence of a normal form SNE oracle, and the followers not affecting the state transitions) without justifying them. Also, the above two assumptions together seem to immediately sidestep everything difficult and interesting about Stackelberg equilibria and multiplayer games. As such, the problem is reduced immediately to a (single player) MDP.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; This paper studies efficient inference problem for large models. It proposes to train a small student model, and performs inference for easy data on the student model, and for hard data on the original large model. Strength: The paper is fairy well written and easy to follow. Weaknesses:There are multiple weaknesses.<|endoftext|>This paper presents a new method to do efficient inference with a teacher student setup for model distillation. It contrasts itself to traditional model distillation because it is optimized for the situation where rare and difficult cases can still be sent to the original teacher network. The paper would be stronger if it had more of these comparisons. However, in this paper, most of the solutions benchmark the method against variations on itself.<|endoftext|>This paper proposes a two stage distillation framework. In this paper, the authors proposed a two stage framework based on distillation which can achieve both the modeling benefits of the large models and preserve the efficiency with lightweight models. The key component is a well designed loss function for the student model which can help in the delegation process. Overall, I believe the paper is well motivated, the method is described clearly, and thorough experiments are conducted.<|endoftext|>The paper proposes a two stage distillation framework to improve inference efficiency and reduce the dependency on large teacher models. The goal of this framework is to only use the large/teacher model for difficult and rare examples and to use the student, smaller model for the more frequent easy examples. The results are not very convincing as the performance of the two stage framework is very close to distillation which is also efficient and does not rely on the teacher model during inference.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 8; This paper addresses the unconstrained stochastic linear quadratic dual control problem with online parameter identification in an online setting, where a near optimal control policy is sought to minimize the infinite horizon quadratic cost (i.e.stabilizing the linear dynamical system) while learning the initially unknown matrices A and B. The presented main contribution is the proposed switched control strategy (a stability augmented certainty equivalent controller) and a parameter inference scheme that estimates the impulse response of the dynamical system in contrast of the matrices A and B. The studied problem of online learning for LQR control is a very interesting topic and has gained significant attention lately. There are many existing results on the regret analysis in a probabilistic setting, e.g.using certainty equivalent control strategy. Compared to that line of work, the key difference in this paper is the modified version of the standard certainty equivalent control strategy, by allowing the system to take zero action for a certain amount of time and avoid potential destabilization. stability: it is a bit confusing to use the notion of safety rather than stability in the paper, since safe learning often refers to a different problem where a dynamical system is expected to avoid entering certain risky states during learning. The reviewer would suggest to use stable/stability that better describe the discussed problem here. Or in other words, will it disrupt performance and estimation guarantees if switching to the least square approach? Proof of theorem 1: One key claim is that all policies derived from the proposed learning process is stable, which seems to mainly rely on the assumption of the innate stability of the system without external control input. However, it is difficult to see how this could be justified given the switching control strategy which also has the normal certainty equivalent control component with exploration noise. For example, it seems the proof of Theorem 1 (Section A.2) only discusses the innate stability analysis when \norm{x_i} is larger than log k. Also, given the assumption of the innate stability with the matrix A, wouldn’t that be straightforward to simply take no action and thus converging to the origin easily? Correspondence to optimal rates with high probability: While it is claimed that the derived performance match up with the known optimal regret in the high probability regime, the reviewer does not find the analysis. For example, it is still unclear to me how the almost sure performance is achieved despite the stochastic noise in the dynamics. After Rebuttal  I appreciate the authors’ responses and revised content. Given the remaining concerns from the reviewer, additional feedback has been provided in the detailed response to help strengthen and justify the technical contribution further. The paper is in general well written, but the reviewer still has several critical concerns over the justification of the claimed property due to lack of enough details. Authors are encouraged to provide more discussion in the main context to justify how the proposed components lead to the improvement compared to the standard approaches. More comparative results to other baseline algorithms are necessary to demonstrate the claimed significance of the proposed algorithm.<|endoftext|>This paper studies an adaptive control for LQR and provides an algorithm to converge to the optimal policy almost surely in an asymptotic sense, where the convergence rate is also provided. The paper assumes that the system is open loop stable, and switches to zero control input (with exponentially decaying excitation noises) for a while when the state or control gain is larger than O(log(k)), which can steer the state to a smaller neighbourhood of 0 due to the open loop stability of the original system. Theoretically, the paper shows that any switched policy generated by the algorithm guarantees a finite infinite horizon averaged cost, which is called "safe" in this paper. Lastly, the performance is evaluated with different parameter choices of the algorithm. The paper is well written. However, I have several concerns listed below. In this paper, a time varying sequence of policies is considered, thus inducing a time varying system. It is well known that the stability of individual time invariant systems does not guarantee the stability of the time varying systems, even for linear systems, i.e.even if Ak is stable for all k, the linear time varying system x_{k+1} Ak x_k can no longer be stable. In fact, this is one of the major difficulties in adaptive control, i.e.how to guarantee stability while updating the policies. One approach to get around this is by considering piecewise constant policies (or episodic based policies) as in most papers on learning based LQR. 2.[Def 1 and Theorem 1] Further, for Def 1, the definition makes sense for linear systems because a finite stochastic LQR s cost happens to coincide with the stability of a linear policy u Kx. However, the policy designed in Algo 2 is essentially a nonlinear policy or piecewise linear policy. For nonlinear policies, a more common stability notion is input to state stability (ISS), which essentially requires that if the noises are small, then the states will be small. 3.[Algo 3 and Theorem 2] The Markov parameter estimation is very interesting. But I hope the authors can provide more explanation on why using Markov parameter estimation instead of the traditional least square estimation directly on A, B. From one side, since ordinary least square also has strong consistency, so least square estimation should also result in a.s. convergence in the asymptotic sense, right? I guess the result will still look like confidence bound with some probability in (0,1)? Could the authors provide more comments on this difference from the standard regret guarantee? it would be great to show that some scenarios where some existing adaptive learning algorithm does not converge but your algorithm does. However, the safety definitions introduced in this paper do not indicate the stability of the system in any sense, thus I found these safety requirements relatively weak. Since the least square estimation used in the current literature also converges a.s. asymptotically, I am not sure how much progress this paper provides compared with the literature.<|endoftext|>The paper studies reinforcement learning policies for an unknown linear system with quadratic cost functions. It is shown that the presented algorithm provides consistent estimates and the rates are provided. Update: After the edits the authors did, together with the edits they promise to do, the paper can be accepted. _________________________________________________________Original Review: Regretfully, the manuscript cannot be accepted due to different issues. Unfortunately, lack of novelty, lack of connections to existing work, incomplete literature review, unsupported and/or wrong statements, and subpar presentation, prevent the manuscript from being accepted. The claim that high probability results do not imply almost sure convergence is ONLY true if the failure probability is a design parameter. (Of course, even in this case, by letting it shrink as time grows, we obtain almost sure convergence). So, to see a work that failure probability is not a used in the algorithm, search for "Input perturbations for adaptive control and learning" and see the reference therein. Further, in the existing literature, the problem is addressed without the assumption of open loop stability (or initial stabilizer), and I recommend the authors to use google scholar to find the rich literature of adaptive stabilization. Here, I mention a few more issues, but there are multiple similar things the authors need to address for resubmitting the manuscript. The existing literature also relaxes the assumptions on the noise so that neither normality nor stationarity are needed. Also, there are results on worst case performance (ie stochastic sub optimality) which are stronger than the approach of average case (ie expected) sub optimality of the manuscript. I guess by "learning process" in Def 2 the authors mean  sequence of policies . It is unclear why line 7 of algorithm 1 is better than the least squares estimate (although it is very similar). About Lem 10, isn t it an immediate consequence of sub Gaussianity? Statement of Lemma 1 assumes full accuracy, and so is unacceptable and does not provide consistency (as the authors claim). "It should be noticed ..." on p8 is not accurate.<|endoftext|>Despite the restrictions of linearity, stability, and full state sensing, the proposed problem formulation is still challenging and I am unaware of any results which provide the same optimality guarantees provided by the authors in the online framework. The paper itself is unusually well conceived, well written, well structured, is clear, and makes very efficient use of notation. Although my overall impression of the paper is positive, there are several concerns which limit the significance of the result. First, while the definition of a "safe learning process" is clearly articulated in Definition 2, such a definition by itself does not ensure stability, as the act of switching between stable policies may be unstable   as observed by the authors. While the authors address this issue using inherent stability of the process and implementing what is effectively a kill switch, such an approach is clearly conservative, and, moreover, there is no statement in the main results directly establishing stability of the trajectories (although clearly such trajectories are bounded). This brings up the second point, which is that the proposed approach is rather cautious. Convergence in the simulation seems to require $~10^8$ time steps   presumably due to the rather large state dimension of the numerical example. Another point which I found significant is the omission of the vast literature on the problem of adaptive control (aside from the 1960 paper). While clearly the adaptive control approach is dated and only estimates a few system parameters, it considers precisely the problem formulated by the authors. Finally, I would have liked to see more justification for the estimate of the performance metric in the numerical analysis. It is unclear to me that Algorithm 2 actually produces a policy   as opposed to simply the next input value. What is being done here? It is unclear if the numerical examples include process noise and if so, what are the properties of this noise? Remark 2: Can the authors support "easily satisfied" with a proof or reference? The use of $J^{\pi_k}$ is a bit unclear. Define "almost surely" in Theorem 2. Typos: "The complete algorithm we propose LQR dual control is presented in Algorithm 1. The numerical simulations are non trivial and convincing. I have some minor concerns, but they can likely be addressed in the author response.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 8; Strengths:1) The authors did a good job in explaining and navigating the time series complexity and challenges posed by EHR data particularly the limitations of previous work and the results section. Weakness:1) My key concern about the paper is the lack of rigorous experimentation in designing the study cohort. The analysis presented in the manuscript lacks the experimentation with the sliding window and this further limits the scope for improvement and assessment of the models performance and determining the best possible subtype. This could possibly help future readers of this paper to approach/reproduce/handle missing data problems in their respective healthcare settings. Unfortunately, the novelty in the method proposed by the authors does not reflect in the model outcomes. Please cite them appropriately.<|endoftext|>This work proposes a supervised approach to phenotype patients given their EHR trajectory and predicted outcome. The authors argue that the lack of interpretability of current deep learning approaches does not allow clinically relevant phenotypes and propose a feature time attention mechanism to tackle this issue. This paper tackles the important problem of phenotyping with an attention based recurrent model. Additionally, the use of the  Occam razor  selection of hyperparameters needs to be justified, as it is not clear to me how it is optimized in a large hyperparameter space. It would be more interesting to show average attention for patients assigned to the same cluster. Mechanisms that leverage both temporal and EHR data have been recently presented [1].<|endoftext|>The paper describes a cluster based learning method for EHR time series of in hospital patients aimed to be interpretable and improve outcome prediction.\It claims to contribute with an interpretable framework and its training process, with a weighted loss to deal with the class imbalance and a custom loss to avoid cluster collapse. Strengths: \Writing is clear and well organized. Cons: \Only evaluated on private data. Is there a specific reason to use K Means? \The lack of ablation studies. I am not sure if the results are well evaluated so I expect to conclude at the discussion period.<|endoftext|>I believe that the work is interesting, solves an important problem and does well. The paper introduces time feature relevance maps, which enable domain experts to look at features time combinations which are in some important for their predictions. The authors claim that the superior results of TSKM are due to the metric bias towards convex clusters. For example, the could the authors come up with a way to use their proposed feature time relevance maps to characterize each phenotype? I believe that would be very helpful. 4.I am not sure if the baselines to predict outcomes directly from EHR data are strong enough. I recommend saving them as .svg or .pdf to ensure the same.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; This paper proposed a recursive disentanglement network (RecurD) for the learning of disentangled representations from information theoretic perspective. What is more, in Table 1, $\lambda_c  1$ for the original $\beta$ TCVAE in their paper. Hence the joint probability of p(a,b,c)   p(a)p(b|a)p(c|a) rather than p(a)p(b|a)p(c|b). The author only discussed and compare the proposed method with the baselines before the year of 2019. There are some recent works [3] [4] [5] on improving the disentanglement and reconstruction error. The authors should do experiments on 3D chairs or CelebA to demonstrate the good performance of the proposed method.<|endoftext|>The paper presents a VAE variant that disentangles the features of the inference network at every layer, with disentanglement defined in terms of mutual information between features. InfoVAE, for example. Could the authors clarify this? The paper appears to simultaneously give a clear slightly novel generalization to disentanglement losses of prior VAE variants, and to provide an architectural approach to implement their approach.<|endoftext|>The paper proposes a new approach for learning disentangled variational autoencoders. In addition to pushing the sufficiency, minimal sufficiency, and disentanglement of the latent representation, the paper proposes to also regularize those on earlier features in the network. * Section 2.1: what is the meaning of defining Markov Chain as \hat{x}  > x  > z, given that the generative process is actually x  > z  > \hat{x}?<|endoftext|>This paper formulates the disentanglement problem from an information theoretic perspective, but focusing on an objective that encourages a compositional disentangled feature space on the layers that precede the final latents. Some of the terms require mutual information estimation, for which they use MINE estimators. Overall, I am pretty happy with the paper. * Some ablations on the architecture itself also seem to be missing (ablations on the loss are good). The compositional objective is interesting and novel and the implementation method is clean. Overall, I am confident that the authors will be able to address the main issue above and that this paper will award acceptance in this venue.
Reject; rating score: 3; rating score: 3; rating score: 6; rating score: 6; Image augmentations have recently become a standard component of deep RL algorithms. This paper proposes to look at the distribution of statistics at a minibatch level in order to enforce consistencies. Paper shows results on standard benchmarks in discrete and continuous control (Atari and DMC). Strengths:Simple IdeaWell explainedStandard benchmarksWeaknesses:Not a significant improvement compared to prior work. The idea is simple and easy to implement and people can try to fork this in whatever they re doing. However, the improvements shown aren t significant to strongly push for accepting it.<|endoftext|>The method is very simple and effective but there has been a lot of prior work, some of which is not cited or compared. They both refer to the same 500k and 100k scores in the DeepMind Continuous Control Suite. But perhaps more surprisingly, the CURL scores are different in these two tables. This is a significant issue that needs to be addressed by the authors. The authors should provide an explanation regarding the discrepancies in results highlighted in this paper vs the RAD paper for the same benchmark tasks.<|endoftext|>This paper proposes a regularization method for reinforcement learning that encourages the Q value of the original image (i.e., original state) and the Q value of the transformed image (new state) to be the same. Discussion and comparison of the difference between related works, such as SAC and DrQ, is also provided. On the one hand, the proposed method is very simple. On the other hand, however, the method that pulls the output of an image and its transformed counterpart close to each other is already commonly used in computer vision methods, thus, the novelty of this paper is limited. Additionally, there are several small issues:1)	Figures in this paper are far from elegant. Symbols in Fig.1 is too large, while text in Fig.3 is too small. 4)	$n$ used in Eq.1 denotes the number of actions, not the batch size. This paper proposes a simple method to improve the performance of image based RL. More further discussion and study will improve the quality of this paper.<|endoftext|>This paper proposes to use data augmentation to regularize the Q distribution by matching the Q values between augmented and unaugmented states (Alg 1). The results are competitive on the DeepMind control suite tasks (Tab 1) and Atari (Tab 2). Data augmented methods for reinforcement learning have been increasing in popularity and this paper demonstrates another way of using augmentation to regularize the Q values that empirically performs well. The paper is generally well written and easy to understand the contribution. The definition of the Q value distribution at the bottom of page 1 is not a proper statistical distribution. Do I understand correctly that this uses the actions from irrelevant states in the minibatch to use for this regularization term? If so, is it interesting to consider other distributions over the actions to be sampled here? It s an interesting new usage of data augmentations that is nicely demonstrated
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; The authors introduce a neural network architecture based on Blaschke products for phase retrieval applications. I would suggest to conduct a few additional experiments to better analyze the model behavior. While the latter might not be a good choice due to the problems with the Riemann sum approximation for few datapoints, it could e.g.still provide a more complete picture. The section regarding model architecture would profit from additional information as well. It is, for example, unclear what activation functions were used in the BPNNs and fully connected NNs or what kind of optimizer was used for training the models. However, the evaluation of the model is quite minimal and could profit from a more in depth analysis in the form of additional experiments.<|endoftext|>In complex analysis, the Blaschke product is an important family of bounded analytic functions in the open unit disk that is constructed to have roots at prescribed complex numbers. However, while the paper compares against different neural network structures and setups, it does not compare to standard approaches for rational approximation or phase retrieval that do not involve neural networks. What is the main motivation for using piecewise BPNN? A potential weakness is that the Blaschke products have a fixed degree as the structure of the neural network is fixed. Can the authors comment on this? Since this is fundamentally a technique for approximating functions of one complex dimension, the application to phase retrieval seems very natural. In particular, I wonder how the author s approach compares to other rational function approximation schemes that do not use neural networks such as the AAA algorithm.<|endoftext|>To recover the phase in this setting the authors propose approximating the phase using Blaschke Products, which can approximate any continuous function on the imaginary line whose magnitude is one. I liked a number of aspects of this paper. Although I am not an expert on phase retrieval, the proposed method seems pretty elegant and clever. 1.Reading the paper, my main question was what the fully connected neural network brings to the table? If I understand correctly, it seems like the authors could have just regressed the coefficients of the rational approximation directly. 6.There were a number of places in the paper where the authors used terminology such as “it is easy to see that…” Typically I don’t find this style of exposition especially helpful. I also think the trick of using Blaschke products is something I hadn’t seen before and other researchers might find it interesting. A clever paper using a relatively uncommon mathematical trick to make progress on a niche, but important problem. While the paper could be improved, I do think the ideas here could be interesting and useful to the community. The experiments seem relatively compelling and code is included with the submission.<|endoftext|>The authors show that the unknown function can be expressed in terms of Blaschke products. **Strong Points**  Imposing priors on neural networks based on the physics of the problem is very important. Using Blaschke products to do this is an interesting idea. Why is a neural network used to predict the coefficients for the rational function? **Recommendation**My sense is that in its current form the paper should be rejected. I am not an expert on phase retrieval, so I am happy for the other reviewers and rebuttal phase to change my mind.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; The authors propose GEN3D, an ML based approach for 3D molecular structure generation. 1.The related work is pretty detailed but neglects some relevant related papers. ICLR 2020. 2.It is not clear how working with a graph is beneficial. GEN3D appears to be a competitive approach for molecular structure generation. The main weakness of this paper is, however, that it lacks methodological novelty.<|endoftext|>This paper proposes a generative model to sample a new molecule with its 3d coordinates. Is this applied to all datasets? The paper seems not self contained. Difficult to understand some of the details. To generate 3D coordinates, the model places a probability mass over the grids of distances and angles. It is unclear how the model handles this. This seems impossible based on the description provided in this paper.<|endoftext|>**Weaknesses*** The novelty of the work is limited. * The authors claim that the proposed model can generate 3D molecules with higher rates of chemical validity, better atom distance distributions, and is very cheap to train compared with previous models. * The paper proposes an autoregressive generative model for 3D molecular graph generation, by predicting the next atom type, next edge type, next atom distances, and next atom angles sequentially based on the existing graph. This is a minor suggestion, but I think it will improve the quality of the draft.<|endoftext|>The paper proposes a model to generate molecules using graph and geometric representations. It would strengthen the paper to see a simple demonstration of this application. All of the methods (w/ checks) produce valid molecules and have similar uniqueness and higher novelty.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; rating score: 6; The authors have performed theoretical analysis ofthe proposed framework, subgraph selection policies, and their expressivepower. The idea is to 1) represent graphs as bags ofsubgraphs 2) use permutation equivariant layers to encode the subgraphs 3)aggregate the subgraph representations into an invariant graph representation. The results are mostly positive, showing thebenefits of DS/DSS GNN architectures. It ll be better if the authors can perform a detailed breakdown. Architectural works well and when does it not? Also, it s better toshow how the method scales with graph sizes. This is a solid paper on improving the expressive power of graph neural networks.<|endoftext|>The proposed DS(S) GNN is proven to be equivalently powerful as DS(S) WL, which can be strictly more powerful than 1 WL. It also provides theoretical results about how design choices such as the subgraph selection policy and equivariant neural architecture affect the architecture’s expressive power. It tries to factorize the original graph into disentangled subgraphs, then to utilize the factor subgraph embedding to represent the whole graph. The presentation of this paper is clear.<|endoftext|>This paper proposed a novel way of increasing MPNN expressive power without using higher order node tuples representation, thus proposed method has lower complexity compare to naive powerful GNN methods. The core idea is to create a set of subgraphs for each graph in the given dataset by using single node or edge deleting,...etc policy. The main motivation and their approach is definitely eligible. This problem is one of the most important problems in GNN and recently there are a lot of different approaches. But at least in some dataset such as Zinc12K, powerful methods outperformed 1 WL ones with a huge margin. NeurIPS 2021. However, It is not new at all. Recently, one (maybe concurrent research) already published the node deleted subgraph idea in [1]. Though this shortcoming, it may gives comparable results in the benchmark datasets, but how can we say it is more powerful than 1 WL in terms of separation power?<|endoftext|>The paper addresses known limitations of the expressiveness of message passing neural networks (MPNNs). Key idea is to generate subgraphs from the original graph, to encode each subgraph with a GNN, and to aggregate the resulting set of subgraph encodings. The paper shows that their approach is more expressive than 1 WL. Weaknesses:  Even though the proposed method shows improvements, it should be mentioned that the improvements are rather smaller, especially given the increased complexity and computing requirements of the proposed method.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 8; This paper proposed an objective function for training neural MCMC proposal, called Ab Initio objective. The author demonstrated the effectiveness of Ab Initio objective on several benchmark models. Below are my major concerns. 1.Although the motivation for the new objective is quite general, the author ends up with a rather heuristic formulation of the Ab Initio objective matching the optimal acceptance rate of RWM for multivariate Gaussian targets. The general applicability hence is quite doubtful given the relatively simple target distributions used in the demonstrations. The author may try out more complicated models. 2.The computation of the Ab Initio objective is not clearly stated. 3.The author claimed (3) can approximate HMC proposals. The proposed Ab Initio objective seems to be an interesting addition to the current candidates of training objectives for MCMC proposals. However, given the heuristic formulation and inadequate empirical evidence, I am cautious about its general applicability.<|endoftext|>Namely, it tries to design an objective function that allows for the learning of a perfect proposal distribution in the Metropolis Hastings algorithm. This setting is very important for practical applications and should be explored empirically. The original paper [Titsias, 2019] solves this issue by adaptively setting the acceptance rate to some desired value. Instead of setting $\beta$ adaptively based on the acceptance rate, the authors propose to set it constant. The constant is then found by "manual trial and error" in such a way that the acceptance rate of Random Walk Metropolis Hastings is 0.234 for high dimensional standard Gaussian, which is known to be optimal for this task. The authors conclude that the proposed objective yields a robust measure of MCMC efficiency and can be used as a metric for comparison. The paper proposes and analyses a special case of the objective from [Titsias, 2019]. [Titsias, 2019]: Titsias, Michalis, and Petros Dellaportas. One may consider it as a special case of the objective from [Titsias, 2019]. The evaluation of the proposed approach is not practical.<|endoftext|>The paper proposes four desirable properties that should be satisfied by objective functions used to tune MCMC proposals. Using these principles it proposes a new objective based on GSM (Titsias and Dellaportas, 2019) that satisfies all proposed principles. This paper takes a first step in that direction. I m not fully convinced by the empirical evaluation. In addition, the method is not tested for online tuning of MCMC kernel parameters, which is how it would be used in practice. While the authors add this as future work, I think including it in this work would be quite good. Overall I think the whole idea of providing a set of guiding principles for the design of objectives to tune MCMC proposals is interesting and relevant.<|endoftext|>The authors design an objective function from first principles that can be used to optimize proposal distributions for MCMC. While I m a big fan of the approach I m somewhat confused about the actual proposed method. Lastly, I have some comments about the experiments. Also, I m confused about what resampling normalizing flow is (I also don t think it was described anywhere in the paper). There should be a section on how the ESS was computed. This can be easily fixed by fixing the manuscript and making these things very clear. I m also somewhat concerned about the practicality of the algorithm and I think this needs to be addressed by the authors in the manuscript.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; Balanced Replay, proposed on top of CQL, removes the CQL loss and train value function and policy as online SAC, and REDQ+AdaptiveBC gradually reduce the ratio of BC loss. I think the novelty and contribution of this paper are not enough. The focus of this paper, fine tuning in RL seems to be a significant topic towards real world applications, overcoming the bottleneck of current offline RL research.<|endoftext|>This paper proposes an update rule to adaptively change the regularizer term (BC here) weight during online learning for more stable fine tuning to improve over an expert. I would like to see a more principled solution that at least does not require such knowledge. Given the limited technical contribution, i would like to see a study on more complicated tasks with for example visual inputs where representation learning introduce additional challenges for finetuning. This paper considers an important topic for RL. However the proposed solution is heuristic and evaluations are not convincing given the simplicity of the tasks. Overall i think this paper does not offer enough contribution in its current form.<|endoftext|>This paper proposes a new offline RL with online fine tuning method. The method seems to be the same as hopper and also worse than Balanced Replay on three of the walker datasets. The authors conduct evalutions in D4RL mujoco environments and show that the approach is able to outperform prior methods in halfcheetah. I also have a few concerns.<|endoftext|>The authors combine REDQ and TD3+BC and propose an adaptive mechanism to update $\alpha$ during the online fine tuning. I agree that adaptively updating the behavior policy constraint is the key point in online fine tuning. My main concern is the novelty. REDQ and TD3+BC are prior work, and it seems that the main contribution in this paper is the adaptive mechanism of $\alpha$. The update rule of $\alpha$ is heuristic.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; This work proposes a new embedding for sets of features, an important problem since many data modalities can be seen as such (images, sentences, etc.). More precisely, a set is represented by the output means of an EM algorithm for fitting the input set with a mixture of gaussians. The authors draw a new connection to an existing method for set embedding (OTKE). Moreover, their method achieves good experimental results.<|endoftext|>This paper discusses that optimal transport kernel embedding (OTKE) can be regarded as a single expectation maximization (EM) step towards the maximum likelihood estimate of Gaussian mixture models under mild conditions. Motivated by the finding, this paper proposes differentiable EM, which can be regarded as a generalized version of OTKE with prior and several EM steps. The connections between OTKE and EM is insightful in set representation learning, and differentiable EM is well motivated.<|endoftext|>This paper proposes a novel set embedding method inspired by the EM algorithm. Overall, I like the paper; it is well written, and the interpretation of the set embedding procedure as an EM iteration indeed makes sense. It is also good to see the authors derive a novel algorithm from their re interpretation. 3) Have you considered using generative models other than a mixture of Gaussians? The paper proposes an interesting idea, and the experimental results are promising.<|endoftext|>In this paper, the author proposed an EM based algorithm, DIEM, for set representation learning. The author first provides the equivalence between the OTKE representation learning algorithm and a single step EM algorithm with extra balanced assignment constraints on the E step. The paper is well written and easy to understand.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; The authors proposed a new inference method for state space model with nonlinear latent states. They showed that their inference model can be incorporated with domain knowledge and optimized in a self supervised manner. They also demonstrated that their model can achieve competitive results through both simulated and real audio denoising tasks. weakness:  Is it possible to show plots on the estimated latent trajectory for the real audio denoising data? The authors clearly state the contributions of their paper compared to the previous literature. The proposed inference method is novel, and the math derivations look solid. And the quantitative comparisons in the experiment results look competitive. Overall, I recommend this paper to be accepted.<|endoftext|>The manuscript proposes to replace these unknowns with trainable neural networks. The input to the neural networks are the observations, and the output are the unknown parameters (or their factorizations, to ensure positive definiteness). The manuscript s proposal is reasonable and interesting, but I have concerns about novelty. Also, I found some of the discussion a bit confusing, and the overall organization of the paper can be further improved. Below are more specific comments : 1. You are proposing to replace some of the parameters/matrices that occur in the Kalman filter with neural networks, which is perfectly fine, and is a simple idea (simple good!). However, I have concerns about novelty. I also think the paper could be better organized.<|endoftext|>The paper proposes an approximate inference framework for state space models with nonlinear latent dynamics. The approach uses the classical Bayesian update rules with neural network for estimating the parameters of local linear transitions. I found the idea and its connection with the noise2noise approach interesting. However, I have concerns about the empirical evidence of the work and some comments on the structure of the paper:  Given that this area is an active area of research with a relatively large body of related work, I believe the related work deserves a separate sub/section. The “KalmanNet” paper, mentioned above seems to be closely related to the paper. As mentioned by the authors of KalmanNet, it can also be trained in an unsupervised fashion. I found the proposed approach interesting; however, I think the paper can be further improved in terms of format and empirical evidence.<|endoftext|>The authors describe a parametrized inference approach for nonlinear dynamical models with linear observations. They borrow ideas from the well known Kalman filter, and use Bayesian filtering and smoothing approaches to recursively estimate the states of the nonlinear model. The parameterization makes use of a locally linear transition between subsequent states using neural networks; thus the authors are still able to perform the Kalman like filtering and smoothing. Major:1) The authors keep calling the Kalman filter (and other variants such as the RKN) a  supervised  method, but these methods do not need a  ground truth  latent variables. They do, however, need a set of observations and a set of model parameters. 2) Needs more validation on real datasets, and more visualization for how it performs on the one real dataset that the authors did use. Figure 2 could apply to any state space model; the novel ideas take a while to get to. I would also recommend changing the terms to terms that are not as similar.
Reject; rating score: 3; rating score: 5; rating score: 6; The authors propose a geometry regularized method via meta learned orthogonal low rank embedding. In sum, the performance of the proposed method is somewhat weak compared with the existing FSL methods. [Strengths] + The paper is well organized and easy to follow. + The visualization of the feature space of z_p is good. + The leveraging of meta learning is interesting.<|endoftext|>This paper mainly targets few shot learning. Cons:1.The experiment results on single domain FSL cannot prove the effectiveness of the proposed method. The proposed modification of OLE in FSL is interesting. Meanwhile the performance is not convincing enough.<|endoftext|>This paper proposes Meta OLE, which imposes a low rank orthogonal geometry in feature space for few shot learning. Figure 4 is not referred to in the main text. The paper is well structured and easy to understand. +The proposed method outperforms the state of the art methods in few shot learning.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; The paper is well written and easy to follow. Which aspects of the results would continue to hold after time discretization ? A number of new results are established.<|endoftext|>One outcome is a characterization of how a neural network deviates, when in an underparameterized regime, from learning the eigenfunctions of the integral operator associated with infinite width NTK at rates proportional to the corresponding eigenvalues. I understand that this work trains both layers and there are many other important differences (well stated in the paper and thus omitted here), but these are mostly in terms of the results.<|endoftext|>There has been a lot of works that are studying the NTK regime. Considering the change of the residuals (admittedly, only on the empirical data) along the eigenvectors of the NTK was already considered in  Towards Understanding the Spectral Bias of Deep Learning’, Cao et al.While there is value in working out the technical details, I think the results in this paper are quite expected and not much surprising. The paper is well written and somehow clear and easy to follow.<|endoftext|>**In conclusion**, I am not saying that the results are not interesting, but in my opinion they are quite *expected* and are not conceptual novelties but rather technical ones. That being said, I have to say that the *story of the paper* is clearly written with a nice introduction that makes the reader eager to discover what is really in it. *However*, when it comes to the paper itself, I have to say that I was a bit disappointed.
Reject; rating score: 3; rating score: 6; rating score: 6; rating score: 6; This work first re interprets AdamW as an approximation of a proximal gradient method, which takes advantage of the closed form proximal mapping of the regularizer instead of only utilizing its gradient information as in Adam l2. However, in the paper, they actually did not solve or try to solve this problem. But they do not explain why different approximations give different generalizations? (1.2)	For the second contribution, AdamW is a “scale freeness” algorithm that may enjoy faster convergence speed. Secondly, For the “scale freeness” algorithm, it is only possible to achieve faster convergence rate. This is a big gap. (1.3) adam is  “scale freeness”, but its performance is usually worse than adam l2. (2)	For interpreting AdamW as an approximation of a proximal gradient method and showing the difference between AdamW and Adam L2, this seems to be very trivial in my view. Overall, this work does not provide very new insights for improving adamw or showing the better generalization of adamw over adam l2.<|endoftext|>An interesting finding shows that the advantages of AdamW compared to the vanilla Adam l2. The main contribution comes from providing the connection between the proximal operator and the AdamW updating, which is interesting. One problem is that the submission shows that scale freeness is essential for the optimizer. Do the author mean only scale the gradient of $f$. It seems that all the adam type optimizers all have this property. AdaGrad does not compute Hessians, and there is no reason to believe it approximates them in general. An interesting paper provides a good finding.<|endoftext|>They aim to unravel the connection between AdamW and AdamProx from both theoretical and empirical perspective. They delicately designed some probing experiments to verify their hypothesis. The math is not self contained and the logical flow is disconnected. For example, the line above eq(3) "This results in the following update rule ...". But I don t think it is trivial to derive it from previous formulations. I would appreciate if the author could help me to see the connections between eq(3) and previous formulations. Are you claim that when $\eta_t$ is small enough, then AdamW is an approximation of eq(5)? I feel the experiment section just carried out a comparison between AdamW and Adam $\ell$2. It supposes to prodive more experiments with AdamProx. I recommend to reject the paper.<|endoftext|>The paper give an analysis to the AdamW optimizer. Authors show that AdamW can be seen as an approximation of the proximal updates, utilizing the closed form of proximal mapping of the regularizer. 2.Authors analyze the scale freeness property of AdamW, and further show the advantages of the scale freeness property which is convincing to explain why AdamW gets better performance when the network is deeper or not equipped with batch normalization. Actually, a more realistic setting is to validate the vision transformer based architectures, which is more meaningful because (1) transformer is equipped with AdamW as a standard setting (2)  vision transformers are not equipped with the batch normalization. So I thought if the theoretical analysis can be validated in the transformer based architectures, and with larger dataset such as ImageNet, the results will be more convincing. should be modified. I lean to accept the paper. It will be more convincing to conduct experiments in more realistic scenarios such as transformer based architectures trained on ImageNet.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 8; 3) for the smaller graph, create dummy nodes so that the total number of nodes in smaller graph becomes N. 4) Mix (interpolate) the node features and edge connectivity of the nodes with same indexes from the two graphs. The main claim of this paper is that, such kind of mixing avoids the "manifold intrusion" problem. Extending Mixup for Graph structured data is an important open problem and this paper attempts to address this. Weakness:The main claim of the paper that the proposed method avoids Manifold Intrusion, is unsubstantiated. Thus it is impossible for the graph resulting from mixing to coincide with any other graph in the training set or with a mixed graph from any other graph pairs" This is the main claim of the paper but there is no attempt to explain this." Moreover, the edge connectivity can not be recovered from the mixed graph if the edges are weighted. Similarly the node features of the individual graphs can not be recovered from the mixed graph. If the authors can address this in the rebuttal, I am willing to increase the score. The paper proposes a mixing strategy for the graph structured datasets, but the main claim of the paper that the method is "Intrusion Free" is not correct in my opinion.<|endoftext|>Specifically, as claimed by the authors, the proposed method is manifold intrusion free. This is achieved by mixing two input graph pairs (some dummy nodes are needed, see Figure2) based on the standard mixup formulation. Strengths   the proposed mixup strategy is simple  motivation for alleviating the manifold intrusion issue is clear    I am satisfied with the parts on page 8 (the last sentence before section 4.2.3) and Figure 5, as the over smoothing problem has been well discussed. They demonstrate the proposed mixup method can work better when the over smoothing issue is serious. Weaknesses  in section 3.3, the authors proved the invertibility and concluded that the proposed method is manifold intrusion free. Can the proposed method be used for the graph with real value edges [0,1]? > 2) When using a graph with real value edge weights, does the proposed method still satisfy the invertibility? But in standard mixup, the mixed sample can be generated in the case of lambda being 0.5. Thus, the assumption in this proof may not be correct. > 4)The most important question in this part is how the invertibility can induce “manifold intrusion free”? The connection between them is unclear. As seen in Figure 5, baselines show better performance when using a shallower structure. it is unclear what do you want to prove in section 4.3.1 as there are no connections to the proposed method. In their experiments, as a general data augmentation method for graphs, node classification tasks are also needed.<|endoftext|>This work studied the research problem of data augmentation for supervised graph classification. The authors proposed ifMixup, which is a Mixup based data augmentation method on graph data. The authors also showed that ifMixup is free of the known manifold intrusion problem of Mixup based methods. 2.This paper is overall easy to follow. 3.The authors theoratically showed that the proposed ifMixup method is intrusion free. Cons:1.The performance improvements shown in Tables 2 and 3 are quite marginal. 2.The technical novelty is marginal considering the proposed ifMixup is basically a direct usage of Mixup on graphs. For example, the first feature of two different data objects indicates the same physical feature in regular data. However, the nodes in graph data are not nicely aligned like the features in regular data. Therefore, the way that the authors arbitrarily assign the node indeces (as stated in Sec3.2) is not very reasonable to me. This work is very interesting, but I have slight concerns about part of the model design.<|endoftext|>This paper shows a very simple technique for performing input space mixup on graphs, in which the vertices are put into an arbitrary order and dummy vertices are added to make the two graphs being mixed have the same number of vertices. Example of intrusion on mixed graphs is interesting, simple, and logical. Proposed algorithm is "Intrusion Free Mixup" ifMixup. Dummy nodes have no edges. This paper proposes a very simple and logical algorithm for performing input mixup on graphs, and achieves excellent results with this technique across a wide variety of datasets. It is also shown that this logically allows one to gain improvements from larger models and also from larger mixing rates. It suggests many natural and interesting follow ups, such as doing this same mixing in random layers, yet I feel this is a healthy and useful contribution for the field. The paper shows a way of interpolating graphs for graph classification and achieves excellent empirical results while showing that the interpolated graphs are invertible functions of the original graphs, avoiding the underfitting issues that may occur with other techniques for applying input mixup to graphs.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 5; The authors explore several explanation methods for image classifiersvia a user study. Evaluatinginterpretability methods is indeed difficult, and the authors clearlydid a significant amount of design, pre registration, usercoordination, etc. Overall, I think more work like this should appear at ICLR  explanability is an important topic in machine learning, and, while auser study is perhaps less conventionally presented at ICLR, I thinkit should be.<|endoftext|>The paper introduces an experiment design and an approach to synthesizing the dataset for the experiment. It is hard to generate data are on the manifold of the natural data for using the current approach to test the explanation results for real dataThe paper provides a  novel and clever experiment design to show that the limit of the current explanation approaches but shows limited potential to be used as a good protocol for large scale real data tasks.<|endoftext|>Weakness:  (1)	Is it fair to compare concept model in this dataset/ user study? The authors reveal an interesting research question and provide with a user study design. (3)	How did the classifier used for computing explanations as well as in baseline perform?<|endoftext|>And the proposed dataset is not very interesting or distinct compared to the existing. Then, two main user study is conducted. For example, for Peeky, does it mean as long as one pair of legs moving to the center of the body? Why not the upper left and upper right. I’m a little confused with this statement. What’s the question to ask so that they are asked to identify the class relevant features. Or is there an interface example?
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; The paper proposes an optimisation framework that uses K previous gradients stored in a buffer to compute an aggregated proxy gradient for the  use in other gradient frameworks (studied are SGD, SGDM, RMSProp, Adam) and evaluates it on CIFAR 10/100 as well as NLP tasks. The paper also presents a convergence proof on strongly convex smooth objectives. In almost all empirical evaluations the method shows an improvement in terms of iteration and performance and ablations are performed to show strong robustness to hyperparameters introduced. Overall, I like this paper as a simple to implement empirical improvement to other algorithm that just "makes sense": very large gradients should probably have an impact for a longer time BUT not all at once so as to not dominate fully.<|endoftext|>Bridging the advances from theoretical and empirical side, this paper proposes a framework for memory augmented optimizers, a strategy which can improve any off the shelf first order optimizers trading off increased memory with faster convergence. Strengths:  A general framework which can accommodate existing first order optimizers to trade off memory with faster convergence. Convergence proof for smooth strongly convex functions   Solid set of experiments    Sensitivity to hparams    Alternatives to l_2 norm for deeming gradients criticalAreas of improvements and questions:  Either make figure 1 vertical or at least make the text vertical so that its easier to read. In Figure 2, I see that several tasks degrade convergence. I will increase my score if this is done.<|endoftext|>The paper proposes a framework for memory augmented gradient descent optimizers which only keep a limited a limited buffer of gradient history and can be integrated with existing optimizers. The method proposed in the paper is centered around "critical gradients" with large l2 norm and only retaining them in the limited buffer. Experimental results show the faster convergence in terms of iterations compared to standard SGD algorithms without critical gradient buffers. The paper was fun to read and well written. The main concerns I have are:* Although the convergence speed in terms of iterations is faster, the wall clock time relative to baselines especially with large buffer sizes seems much higher Appendix D2 Table 4. The overhead seems to stem from the book keeping, replacement and aggregation strategy. * In Analysis there is nice study on the staleness of the gradients in the buffer. Especially given that there is a decay term when doing gradient selection for replacement. The proposed method of only storing critical gradients in a limited size memory buffer seems interesting. The experimental evaluation is comprehensive. However, the method only seems to improve in terms of gradient steps but not wall clock time to converge due to bookkeeping overheads.<|endoftext|>The paper proposes a memory augmented optimizer to make use of the history information of the gradients. Based on that, the authors propose a new optimizer that retains a limited gradient history and those gradients are selected by their importance. This method could be built upon existing popular optimizers like Adam and SGD and enjoys accelerated convergence and improved performance. Pros* The selection of the critical gradients directly depends on the $l_2$ norm of the gradients and the aggregation function is simple enough, so the idea itself looks reasonable and the implementation of the algorithm is straightforward. Furthermore, the optimizer could be combined with other popular first order optimizers which can make it more useful. * The experiment results are strong. Cons* Although it seems natural to choose gradients of large norm as the critical ones. I did not find any motivation or explanation for that. I guess such acceleration might depend on specific conditions and these conditions could show some improvements and limitations of the proposed algorithm. In general, the paper is well written and the experimental results look strong. However, the lack of clear motivation and explanation of the proposed method downgrades my rating to the paper. Furthermore, the theoretical part could be more detailed as the current one does not provide a convincing improvement over vanilla SGD method. I hope the authors will answer these questions in the future.
Accept (Oral); rating score: 8; rating score: 8; rating score: 8; rating score: 8; The paper introduces a general framework for designing message passing neural networks (MPNNs) stronger than the 1 WL distinguishing power via structural information injection to the aggregation scheme in the message passing framework. The authors show that under reasonable conditions the resulting architecture has a superior expressive power than the 1 WL test. As far as I’m aware, the aggregation scheme suggested in the paper is novel, providing a constructive way to design more powerful architectures than the 1 WL test. Do the authors have a conjecture as to why their proposed method circumvents over smoothing? A well written paper with strong theoretical exposition and results.<|endoftext|>Based on that, it carefully design GraphSNN with local structural coefficients to control message passing to obtain more expressive power than 1 WL GNNs. I would like to recommend to accept this paper, for its expressive, efficient and easy to use GNN component with intriguing performance. Based on that, it is persuasive to encorporate the three properties into structural coefficients. Reference:[1] On the bottleneck of graph neural networks and its practical implications.<|endoftext|>This paper proposes an efficient method for message passing that can incorporate structural information (that of neighborhood subgraphs) that is provably stronger than 1 WL. As compared to three strands of provably powerful (more than 1 WL) GNNs, the method has limited additional computational overhead, and can also show encouraging results on the oft quoted "over smoothing problem".<|endoftext|>This paper deals with the very challenging and important problem of designing Graph Neural Networks (GNNs) that are more expressive. The authors propose a new GNN framework, that injects structural information into a message passing aggregation scheme. The proposed architecture (GraphSNN) is shown to be more expressive than the 1 WL in distinguishing graph structure. The experimental results are quite convincing as they confirm the good performance of the method in classical datasets. I would however encourage the authors to work a bit more on the presentation of the paper.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 5; rating score: 5; The paper deals with the problem of domain generalization, specifically performing inference on a single test example. 2.The problem an interesting angle for viewing domain generalization. It might be interesting to compare their method of test time data augmentation, with your method of test time variational inference, in related work. # Review SummaryThe idea is interesting and the paper is well structured in general. That being said, the empirical results suggest that this method is superior to TTT, TENT and other baselines on benchmarks used in this paper. I am giving a weak reject at this point, but will be happy to receive clarification from the authors before the final decision. ## update after discussion periodThe authors have sufficiently addressed concerns raised by me and other reviewers and new experiments / analyses / comparisons have strengthened the paper. Or does the blue symbol indicate the subspace which the classifier predicts as the respective classes? Table 1 is useful for understanding how this work differs from prior art.<|endoftext|>This paper studied the problem of single test sample generalization. The paper formulated the single test sample generalization problem as a variational inference problem and proposed a meta learning framework. Pros.1.The proposed method is more flexible than the previous generalization  and adaptation based methods. Table 1 clearly shows the difference between this work and previous works in terms of training and test time settings. As indicated in the main paper, "both the meta prior distribution and the variational posterior distribution of the classifier are generated by amortized inference using the amortization technique", but why the amortized inference is applied? This paper addressed "single test sample generalization", which is an interesting and important problem in the field of domain generalization. I recommend acceptance for this paper. I highly suggest that the authors should widely discuss the remaining weaknesses raised by the reviewers in the final version.<|endoftext|>The paper proposes an approach to ensure a model trained on a set of source domains generalizes well to an unseen target domain based on a single unlabeled target sample. The paper is well written and generally easy to follow. The authors set this up within a variational inference framework that allows them to explicitly parameterize this conditional distribution to infer model parameters in a target domain. I will now highlight the strengths and weaknesses of the paper. Similar concerns were also shared by other reviewers and the authors provided sufficient experimental evidence to address the same. Additionally, concerns surrounding results with different backbones have also been sufficiently addressed (in my opinion) by the new experimental results. I particularly appreciate how the authors build up from the most rudimentary version of the proposed approach to adding more relevant terms to the objective that might further improve out of domain performance. Furthermore, in this space of test time adaptation, when compared to prior work which either rely on single or multiple passes over the entirety of target data, I think the paper is trying to address a relatively realistic and timely problem setting that the community might find useful. Adaptivity gap (as discussed in Dubey et al.2021) is a general statement about DG settings.<|endoftext|>  The paper describes a method for domain generalization that performs test time adaptation using a single test example at a time (as opposed to a transductive setting used in other works where a whole batch of test examples are used). The method is cast as a meta learning task. ( ) There is little analysis of the results to show why the method performs quantitatively so well. The paper describes an elegant method with good empirical performance, but it does not provide intuition/theoretical explanations/empirical analysis why it should/does work. However I still see important weaknesses in this paper that the authors may want to fix in the final version and/or in future work. 1.There is no intuitive or strong theoretical support for adaptation from a single test example. These **limitations** should be explored, discussed, or at the very least acknowledged in the paper. My impression is that the variational and meta learning aspects of the proposed implementation are eclipsing the more fundamental points on which the method relies. Figure 1 provides almost no information regarding (1) above.<|endoftext|>The authors propose to learn to generalize across different target domains with single samples by using a meta learning paradigm. Strength:* The paper proposes to use meta learning scheme to do one shot learning. * even though most experiments in the paper are considering multi source domain adaptation. It is unclear why not testing on the single source setting, since training the proposed method does not require multiple source domains* It seems that the proposed method underperforms the method proposed by Zhou et al.(2020a)  in all of the Resnet 18 experiments. and For some reason, for all Resnet 50 Experiments only for this baseline method is not conducted* it would be helpful to show how an ablation study on how exactly the proposed meta learning scheme improves performance vs non meta learning based one shoot learning, such as [1, 2], [1] Luo, Yawei and Liu, Ping and Guan, Tao and Yu, Junqing and Yang, Yi. 573 588.As the discussed above, due to weak performance, and lack of more empirical demonstration of the proposed method. I recommend marginal reject on this paper.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; In the paper the authors proposed the maximum deviation approach to study the safety of a predictive model by measuring its distance to an interpretable, "safe" function on a certification set. The authors explicitly showed how to compute such measure between function classes of linear, generalized additive, tree based, and piecewise Lipschitz models. However, the reviewer has some major concerns regarding the novelty and the development of the proposed methods. 2.The discussion regarding how to compute the maximum deviation can be rebalanced. The derivation in its current shape leaves a couple of questions.<|endoftext|>The safety, such as the possible maximum output within the prescribed input domain, of white box model, such as decision tree and linear model, are easier to inspect. In the experiments, the authors demonstrated that the maximum deviation are considerably large in practice. ### Strength#### [Strength1]The idea of inspecting the safety of black box models through the approximated white box model wills be novel and interesting. ### Weakness#### [Weakness1]The theoretical results on specific model classes in Section 4.1 and 4.2 on the deviation between decision trees and generalized additive models are straightforward and trivial. However, the main theoretical results are trivial or simple modifications of the existing results. Overall, I think the paper is interesting in its underlying idea.<|endoftext|>The authors posit that the demand for explainable/interpretable models in machine learning is linked to safety. This deviation is computed on a certification set of data points. Additionally, the authors show that their proposed maximum derivation can be computed exactly for some interpretable models, such as decision trees, rule lists and generalized linear and additive models. However, I do not see how exactly the proposed dissimilarity (the maximum deviation) can help a domain expert to retrain a safer model based in its output, which seems like the goal of such a metric.<|endoftext|>* The takeaway message is that these kind of computations are useful for evaluating the safety of the candidate model, b/c if it (i.e., its predictions) deviate(s) too far from the reference model, then that is a bad sign and someone should "investigate something". That paper produces bounds on the robust error. Is it a bad idea to just always use the black box methodology, by default? * To handle neural networks, there is some interesting recent work that reparametrizes a neural network in a certain way, winding up with a linear model. I like the paper and topic a lot.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 8; (I didn t aim to write such a short review, but this is a standard exploratory paper that scales up a well established setting.The experiment setups are well designed and I don t find any ambiguous point that needs clarification. Scaling up gives new findings that were not observed in previous (small scale) work. I am personally not strongly interested in this fully RL communication learning direction, but researchers that have good reasons for pursuing this topic may find the results very interesting.) The findings are novel and interesting, bringing out the need for conducting large scale experiments.<|endoftext|>Instead of simple synthetic objects, the authors consider scaling up to image datasets such as ImageNet and CelebA. Strengths:  This is a timely paper for a problem worth solving. In simple Lewis games, a high topo sim gives a strong clue about what the structure of the language is, but in this case, I have no idea. Questions:  You observe that scaling up the task difficulty entails unstable RL optimization. I would suggest adding a period (.) Measuring compositionality in representation learning. I think this paper is a great contribution, but I would appreciate some more understanding of the structure within the emergent languages.<|endoftext|>(4) It would improve the focus of the paper to state the work s relationship to the cognitive science and the RL sides of emergent communication more explicitly. Overall, the paper makes a strong case to scale up emergent communication experiments supported by empirical contributions. 2.Research is very well situated in the literature. For example, in Section 3.3 s voting, this is a technique that improves model performance but is likely not how language originated in humans.<|endoftext|>This paper provides an analysis of emergent communication behavior at scale and discusses training techniques to reduce instability during RL optimization. Given that the paper argues for scaling up a field, more information about compute usage and hardware would be appreciatedI strongly recommend accepting this paper, given the over emphasis of toy tasks in the emergent communication community.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; The paper focuses on the explanation of GNN performance, which is an interesting and critical problem. The paper proposes a reasonable method for the problem to decomposite the graph. The experiments are also comprehensive. For me, I have to search previous works to understand how to use it in GNN explanation issue. The paper is easy to follow and has a reasonable method. Besides, the comprehensive experiments, especially the visualization of results, verify the effectiveness of the proposed method.<|endoftext|>This paper proposes a decomposition based explanation method for graph neural networks. The motivation of this paper is that existing works based on approximation and perturbation suffer from drawbacks. To address the issue of existing works, the authors directly decompose the influence of node groups in the forward pass. The decomposition rules are designed for GCN and GAT. Further, to efficiently select subgraph groups from all possible combinations, the authors propose a greedy approach to search for maximally influential node sets. Experiments on synthetic and real world datasets verify the improvements over existing works. Questions regarding technical details.<|endoftext|>The first one lies in its ability to track contribution of components in the input graph. The second one is the algorithm for subgraph level explanation via agglomeration. In contrast of previously used approximation based, perturbation based, additive feature attribution methods, the authors claim that the newly proposed method has the advantages of higher fidelity and node level explainability. S2.Two popular GNN framework GCN and GAT are used as examples for decomposition operation, which demonstrates the methods compatibility to the current mainstream. Figures are provided with descriptive caption, including details needed for comprehension. Datasets includes both synthetic datasets and real world datasets for diversity. The paper innovatively proposes to decompose the information flow for explainability and provides a new algorithm to construct subgraphs to reveal more complex interactions. Thus, I am slightly positive of this paper.<|endoftext|>This paper proposes a decomposition based explanations method for graph neural networks. They demonstrate the effectiveness of the proposed method on synthetic and real world datasets. The main concern of this paper is the decomposition assumption. Most of their methods are proposed to employ score decomposition.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; As detailed below, the main aspects that attract me towards TP are absent from the proposed algorithm, and its merits as a generic drop in replacement for backpropagation are not strong enough or at least not yet well argued for. The authors propose a new prescription to compute targets in TP. The paper does not provide any theoretical guarantees that this is the case. Can the authors clarify this remark? There are numerous remarks on the vanishing/exploding gradient problem and allusions to the promise of solving this problem with TP. To me, GEMINI is more obviously related to synthetic gradients than target propagation, and it is not usually cited as LeCun s original TP paper. As an optimization algorithm, its guarantees are not sufficiently strong.<|endoftext|>The paper proposes the train the target propagation algorithm (as an alternative to gradient back propagation algorithm) through regularized inversion. Demonstration and comparison with traditional BP algorithm is illustrative. The target propagation algorithm is an interesting new directions that worth exploring. The main downside of the paper would be experimental demonstration. I understand the authors focused on the illustrative purpose, but that also makes it unclear how the model could generalize,  scale and apply to the realistic datasets. I m not asking for the SOTA performance, only the competitive results could be good enough to show the potential of the proposed work. (in the conclusion section), which might be misleading or over claimed.<|endoftext|>This manuscript proposes a new way of approaching optimization in RNNs based on Target Propagation. Second, there needs to be more complete discussion of this meaning of this Lemma, as the argument that TP is better than BP depends on the fact that TP calculates a \textit{different} direction than BP. How close is TP to a Gauss Newton method? Also, I found the phrase "In our experiments, we have not been able to get a non diverging sequence of iterates if the regularization was set to 0, which questions the interpretation of target propagation as a Gauss Newton method" peculiar. Am I missing something? (2) The empirical evidence, in my opinion, is lacking. This manuscript needs to explain more about what is different in the optimization to make it work better, and evaluate how  robust those improvements are with much more detailed and extensive experimentation, especially since the improvements are not fully explained from the theoretical side.<|endoftext|>The authors provide a modified version of target propagation that is easily implementable via autograd system. **Weaknesses:**  The process seems to have large variation on training loss and accuracy compared to back propagation, which might be an indication that the scheme is less amendable to theoretical analysis. **Correctness:**  There is no false claims to the best of my knowledge**Clarity:**  The paper is clear and structured  A typo in the paragraph under equation (2): "allow us tp interpret"  > "allow us to interpret"**Additional Comments:**  Is there any convergence result possible for the proposed target propagation scheme given that the backward updates are more tractable compared to the original version?
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 5; This paper proposes a simple but seemingly effective method for model based RL in cases where obtaining transitions for training from the real environment is extremely costly. The method is based on well founded principles of Bayesian experimental design. The empirical results sufficiently clearly show the advantages of the proposed methods. A nice reformulation of the problem as reducing uncertainty about the optimal trajectory, which leads to a decomposition according to the sources of uncertainty. While this is not necessarily an issue in the domains considered in the paper, generally MPC cannot be guaranteed to reach the same performance as an optimal solution. This is a limitation of the proposed approach. In the initialization, should s_0 be sampled from p_0? Then, X would be a pair of a state and an action.<|endoftext|>This paper considers the very relevant (in my opinion) problem of data efficient RL and is potentially applicable in nearly all situations where RL is to be applied in the real world rather than a simulator. The authors propose an acquisition function based on expected information gain (EIG) approach. The overall framework consists of a (1) Gaussian process model of the transition dynamics, (2) an MPC approximation of the optimal policy, (3) the EIG acquisition function where the target variable is the sequence of states visited by the optimal policy. Could multi task GPs be used here to improve the model’s capability? I’m curious about the sensitivity to the approximation of the optimal policy. Even though the motivation is for problems with expensive transitions, wall times for the new method should be included so that readers can get a sense for how computationally expensive the method is. Please see [2, 3] from the OR literature. I, however, have a few comments above that I believe would improve the paper.<|endoftext|>This paper uses insight from Bayesian optimal experimental design and define an *acquisition function* which quantifies information gain about an MDP s optimal solution given a state action pair. They propose a method for computing this acquisition function, and evaluate it empirically. The empirical results presented appear promising. Can the authors further comment on how hyperparameters were selected across each algorithm and whether they provide a fair comparison? ## Minor stuff which didn t affect the review:"the the" near the start of Section 3.1"evaluatation episdoes" near the start of Section 6Taking the above into account, I recommend acceptance of the paper. While I feel the empirical evaluation could be improved, I find the motivation and intuitive arguments behind the proposed algorithm convincing enough to be a useful contribution in the literature.<|endoftext|>This paper presents an active learning algorithm for model based reinforcement learning. The algorithm is based on a new acquisition function that finds the most informative transition to query an oracle model in order to achieve better performance with respect to the optimal task. Furthermore, the paper focuses on active learning for policy optimality, instead of the most traditional exploration/exploitation approach. However, I have trouble following the approach in the TQRL setting that the authors proposed. In that setting, there is already a generative model/oracle/simulator that allows querying arbitrary transitions. Being familiar with the RL and BO literature, the paper can be hard to follow. Being a novel setup I understand the complexity of a fair comparison and the addition of PILCO clearly solves some of the questions. Furthermore, given that the surrogate model in this paper is based on GPs, there should be a mention and comparison to GP based model based RL methods, such as PILCO, which has been, and they still are, the standard of sample efficient RL.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; This paper empirically studies six variants of prioritized experience replay, typically used in online RL, in a batch RL setting. I wish the authors the best to improve the manuscript. The experiments study the performance and bootstrapping errors. Among other things, it is shown that non uniform sampling strategies are also interesting in a batch RL setting. **Strong points:**  Sensible question: is a widely used method in online RL also useful in offline RL? The paper offers interesting perspectives for further work. The empirical experiments are well done, but limited in scope. The paper considers only one algorithm and three environments. I saw no mention that the code would be provided, although the paper strongly emphasizes the empirical validation. I am surprised this is not highlighted better as a contribution of the paper.<|endoftext|>This paper investigated the effect of non uniform sampling in an offline RL setting. Using TD3BC (Fujimoto and Gu, 2021) as a backbone offline RL algorithm, the authors applied prioritized experience replay (PER) to the sampling of TD3AC with variants of priority metric, including standard TD error, rank based return, pseudo count using a hash table, and the other three metrics. The authors insist that non uniform sampling can be helpful in offline RL compared with usual uniform sampling. They also found that there is no one outperforming metric for prioritized sampling in offline RL settings. Weakness  As shown in the numerical results, sampling in offline RL may not be a critical issue compared with online learning. Question  What will be the result if the following metric is used with proportional PER instead of rank PER: N step Return, Uncertainty, and Likelihood? Although this work raised an interesting question, this paper combines the existing algorithm and sampling metrics, which lack both novelty and empirical significance. I hope the authors to further improve this work in the future.<|endoftext|>In this paper the authors  investigate the application of prioritized experience replay (PER) applied to offline/batch RL. They trial a variety of different priority metrics from the literature on the TD3+BC model free algorithm and report results on the D4RL suite. Pros:* Overall the questions asked in this paper are worthwhile; I don t believe that PER has been applied to the offline setting before, so examining the efficacy of this is certainly an interesting direction. Cons:* This paper in its current state doesn t meet the standard for publication at a top venue as it lacks rigor for the following reasons:   * Only one offline model free RL algorithm was tested. Perhaps PER could be useful during model training? * The results themselves are not that compelling, and it doesn t appear that PER seems to help over uniform sampling much at all. Or perhaps work that seeks to explain why PER doesn t help that much, perhaps with some illustrative examples. For instance, when model based RL is introduced, the authors  cite recent work by Kaiser et al., but this is not a canonical nor an archetypal algorithm. pioneering  at the top of section 3.2, and the repetition of  metric  on page 9.<|endoftext|>The paper empirically investigates several sample selection strategies in offline RL based on TD3BC and the PER framework, including TD errors, N step return, Generalized SIL, Pseudo count, Uncertainty, and Likelihood. The paper finds that some sampling strategies improve the performance on D4RL dataset but they fail to avoid the bootstrapping error. If this is the case, the paper basically shows that different weighting results in different performance and I don’t think that is a novel or significant finding. 2.The empirical study is inconclusive. Since the goal of the paper is not to propose a SOTA algorithm on “deep” RL environments, why not consider some smaller offline dataset or even discrete toy environments and see if any clear conclusion can be drawn? Also, what are the reasons for using Likelihood? I hope the authors can keep improving the paper since it is an interesting topic.
Reject; rating score: 6; rating score: 8; rating score: 8; rating score: 8; The paper sheds an exciting light on the problem of producing a meaningful evaluation of GNN explanation methods (at least a subset of them). The idea is to introduce a deconfounder D to capture the effect of OOD explanations. The authors make an interesting example for a well known synthetic dataset where the weight of the explanation in the ground truth is lower than a clear non valid explanation when evaluated using the model to explain. The introduction of the deconfounder D creates a spurious path between the graph variable and the explainer variable. To mitigate this effect, then, they introduce a front door adjustment to the causal graph. The front door adjustment requires a graph generator and authors use a novel Conditional VGAE to generate graphs that will also cover the OOD case. As I have stated in summary, the paper has caught an interesting problem with current explanation evaluation methods. This aspect is a strength! Another strength is that they cast the problem into a causal framework making the reasoning behind the novel evaluation mechanism reasonably interpretable. The main issue with the paper lies in its clarity. It is not challenging to follow the technical details but rather to understand what the authors want to do. For instance, the authors title Section 2.1 as "Problem Formulation" but I do not see any problem formulated in that section. Also, the evaluation method requires using a generative model to generate enough samples to be able to apply Equation 1. For the above reasons, I would like authors to clearly state:1. Whenever a new model for explaining GNNs is developed, should one also generate graphs using CVGAE? The two baselines are very weak, in my opinion. The other one, though, is not conditional, which makes only explicit that a conditional generator is better. But I would have been surprised to find out that this was not the case. 3. what the full graph like  > what the full graph is like4. It harms the removal based evaluation of the explanatory subgraph  > I ve read this sentence over and over again but I ve not been able to understand what you actually meant5. well trained GNN predictor  > What "well trained" means? 6. it is rooted from G_s  > it is rooted on (?) Besides, we introduce ... generated graphs  > Cannot understand it11. Equation equation 1 (in a couple of places)Authors have addressed most of my concerns, except for the #1, which is still not 100% clear. After the rebuttal, I ve decided to raise my score. In summary, I like the idea but I believe the paper does not make a good job of conveying the message to the readers.<|endoftext|>This paper has done an excellent work of finding the out of distribution between the subgraph and graph as the confounder. Further, this paper proposes a conditional variational graph auto encoder in assessing the causal effects of subgraph on the prediction. They also introduce a surrogate variable to denote this out of distribution effect. Through adversarial training, the effects of the proposed model is correctly verified. This paper proposes a surrogate variable $G_{s}^*$ to denote the out of distribution effect and seems find interesting ways to evaluate the causal effects between the subgraph and full graph. *Strengths*:1. the out of distribution has not been explored before, as the paper claims. 3.I will the experimental settings, especially about the 3 insights. These results have sufficiently verify the claims and advantages of the paper. Based on the innovation, clear model description and solid experimental results, I recommend for accept.<|endoftext|>This paper presents a novel explainer agnostic method to adjust the biases of feature importance scores of feature attribution for GNNs. The paper first describes the feature importance scores of the GNN feature attribution framework have biases due to the out of distribution (OOD) problem. The subgraph important scores are calculated by inputting a subgraph instead of data graphs, but subgraph patterns can fall into regions outside the distribution of training data graphs. To address this problem, the paper proposed a method to generate surrogate graphs within the data graph distribution by CVGAE to make a front door adjustment for deconfounding these biases by distribution shift. The paper presents an interesting new method with a clear focus on debiasing GNN explainer subgraph importance scores by generating surrogate subgraphs to correct distribution shift problems. The paper is well written and easy to follow, and the core idea sounds very effective and the empirical study using three datasets provides useful demonstrations. Overall I liked the idea of the paper and found it nice work. But at the same time, a natural question will be: Is there any chance that we miss important features because the method to generate subgraph patterns didn t consider this OOD problem even if the proposed method can make a correction by posthoc processing. Or, either way, any subgraph patterns come from at least one of the data graphs, and so by recovering such data graph within the training distribution, we can approximately resolve the data distribution bias and there are technically no problems?? The surrogate graph G_s^* by CVGAE is basically intended to recover the original super graph of G_s in the training dataset, isn t it? It ll be very helpful to make sure this point in some way. Small questions are made to make sure1. Does this cause any problem? They are recovered original graphs containing the given subgraphs?<|endoftext|>In this paper, the authors use a causal view to investigate the OOD effect on the explanation evaluation of GNNs. They find the confounder between the extracted subgraphs and the model prediction, which makes the evaluation less reliable. To solve this problem, the authors proposed a deconfounding evaluation method based on the front door adjustment from causal discovery. To generate a reliable surrogate subgraph, they proposed a generative model, which contains three losses for training. The experimental results show the effectiveness of the proposed method (DSE). The paper is easy to follow. View GNNs from a causal perspective is not a new idea. However, this paper uses causal theory to investigate the OOD problem in DNNs is new and interesting. Weaknesses:My major concerns are as follows,1. In Section 3.1, the authors mention  our work is the first to adopt the causal theory to \textbf{solve} the OOD problem ... . However, I think this paper is a causal view on the OOD problem in DNNs instead of solving the OOD problem in DNNs. 2.In the adversarial training part, there are several hyper parameters such as $\gamma$, $\omega$, $\tau$, $\lambda$, and $\beta$. However, the authors do not provide any sensitivity analysis about these hyper parameters. 3.In Insight 3 of Section 4.2, the authors mention  The DSE based rankings are highly consistent with the references, . 5.Since the work of Causal Screening (Screener) [1] is very close to this paper, the authors should discuss more differences between this paper and paper [1] instead of mentioning it slightly in the Related Work. [1] Wang, Xiang, et al."Causal Screening to Interpret Graph Neural Networks." Some notations are not clear and there are some typos. b) Below Eq.(3),  Equation equation 1  should be  Equation 1 . The causal view on the OOD problem in DNNs is a new idea.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; For squared loss and independent data points, it theoretically proves that reweighing the data points has no effect on the final model learned for linear models and linearized neural networks. The paper is studying an important problem about the effect of importance reweighing in overparameterized models which has attracted a lot of interest recently and thus, would be of interest to the community. However, the paper has many serious flaws. So, there is this discrepancy between the works/experiments that this work is replicating and the theoretical results that are included. Moreover, this has already been studied theoretically for cross entropy and other general losses by [2] which has not been cited in this work. I would like to make recommendation for rejecting this paper.<|endoftext|>This paper mainly focuses on the reweighting algorithms (e.g.Importance Weighting, Group DRO) for the worst group performance. Specifically, the authors prove that under several conditions (e.g.assumptions for algorithms, wide fully connected neural networks, squared loss), the worst group test performance of the reweighting algorithm will converge to the same level as ERM. My feeling is that the conclusion is somewhat overclaimed. In both abstract and conclusion, it is emphasized that this work proves the pessimistic result that reweighting algorithms always overfit. The overparameterized models need to be linear models, linearized neural networks or wide fully connected neural networks, which are not commonly used in practice. The actual main contribution of this paper is proving that for linear overparameterized models, the current reweighting algorithms almost always overfit. Besides, there is a gap between theoretical results and experiments. In summary, the conclusion of the paper is somewhat overclaimed, and there is still a lot of room for improvement.<|endoftext|>(usually you need to do cross validation, or if not, argue that the test set is large enough...)The paper studies an important problem. They offer theoretical and empirical evidence that reweighting does not affect the final solution in this setting; in other words, the algorithm would converge to the same interpolators as that of basic ERM. The theoretical analysis is done for linear models, linearized networks, and wide fully connected networks. Many statements of the paper are misleading and should be addressed. It will help if the authors comment about the novelty of the proof techniques compared to the previous work. However, there are some issues that need to be addressed+ The abstract and introduction are quite misleading. + Given the discrepancy between the theoretical and empirical results, it becomes more important for the authors to make the abstract/intro/.. more accurate. If not, they should at least add a citation. + It looks like some of your experiments is similar to Sagawa et al.(2020a).Can you elaborate on the differences? + In Table 1, I believe you have left out the performance of ERM for early stopping.<|endoftext|>This paper provides a theoretical explanation for this fact (using linear models and linearized neural networks) and also illustrates it using numerical experiments on two datasets. Furthermore, the authors assess the role of $L_2$ regularization in training. The practical takeaway is that existing approaches based on reweighting require substantial regularization or early stopping to perform well in terms of test accuracy (in the overparamerized regime). I enjoyed reading this paper. 5) I like the experiments since they bring the main point of the paper across very clearly. One additional result I would have liked to see is an empirical illustration of whether the theoretical arguments hold, e.g., are the final solutions of ERM, DRO, and IW initialized at the same $\theta^{(0)}$ very similar (and more similar than solutions of the same method initialized at different $\theta^{(0)}$)?
Accept (Poster); rating score: 6; rating score: 5; rating score: 5; rating score: 3; The paper introduces A NHP and A NDTT that tackle the problem of continuous time sequences and neural symbolic computing with a time component using transformers and attention. The work seems primarily motivated by previous work on Hawkes processes and neural datalog through time NDTT. They achieve strong experimental results, especially in StackOverflow and RoboCup. While the paper is not too theoretical, the techniques do seem to appear natural and well motivated by the problem at hand. I did find the paper a bit difficult to read, but this may be somewhat unavoidable due to the probabilistic modeling. While there are some issues, the paper is generally well written, explores new techniques, achieves good results.<|endoftext|>The main contribution of this work is a new transformer based temporal point process (TPP) model. The proposed model defines the conditional intensity at each time $t$ as a function of all past events using the attention mechanism. This is different form existing neural TPP models that typically embed the event history into a vector. Additionally, it is shown how the proposed model can be combined with the Neural Datalog Through Time (NDTT) framework [(Mei et al., 2020)](https://arxiv.org/abs/2006.16723), which allows us to specify certain hard constraints on the event occurrences (such as "events of type x can only occur when certain conditions are met"). I have the following main concerns regarding this paper:1. 2.**A more thorough empirical evaluation is needed to support the main claims of the paper**  One of the claimed advantages of the proposed A NHP model is its **flexibility** (Section 1, point 3; Section 7). However, the experiments only consider THP and SAHP as baselines. Figure 2: The figure is difficult to read. Color coding or using symbols to denote different models in each plot would make it easier to understand. Another claim regarding the A NHP model that is not explored in the experiments is its claimed ability to learn long range interactions better than an RNN based TPP, like NHP (Section 1, point 2). It would be interesting to evaluate this aspect of the model and highlight these failure modes of RNN based TPPs. One idea would be to do this using multivariate Hawkes processes with long range triggering kernels and many marks. The proposed modifications to the NDTT framework [(Mei et al., 2020)](https://arxiv.org/abs/2006.16723) don t lead to significant improvements over the original methods.<|endoftext|>A model for sequences of events occurring in the continuous time setting is presented. It uses the self attention mechanism from the Transformer model, together with some domain specific masking operations, to create event embeddings that are contextualized by their relation to other (relevant) events that occurred. The model is then extended towards a generative process, which in turn is integrated into a neural datalog through time framework. * Section 5 is extremely dense and very hard to follow, lacking motivation:    If you are trying to introduce NDTT, you should provide an example that helps the reader to understand what you are trying to achieve, BEFORE introducing the formalism. * Experiments: no ablations of the architecture are studied, even though many choices were made that are non obvious. (i.e., why does the event type need to be part of parameters rather than part of data)Update after rebuttal While the authors provided extensive answers to my questions, the updated submission largely does not reflect these answers, nor is it obvious how it could (given the space constraints). Overall, I think the manuscript is not ready for publication at this time, largely due to a lack of clarity in the writing. Overall, I found the first four pages of this paper quite enjoyable, but found the remainder large impenetrable as someone unfamiliar with the area.<|endoftext|>The paper proposes a continuous time transformer architecture for event sequence tasks using the Neural Datalog Through Time (NDTT) framework for incorporating logic specifications into the model. The paper then experimentally compares the proposed model with point process models and standard NDTT. Overall, the paper provides some interesting contributions on modeling continuous time event processes and adding specifications via the NDTT framework. However, the paper also contains several weaknesses, which makes it not ready for publication in its current format. The writing needs to be substantially improved before this manuscript is ready for publication. It is unclear what the main contribution of the paper is. On the one hand, the paper proposes continuous time event transformers, while on the other hand, the paper proposes a neuro symbolic framework for embedding rules of which event can draw information from which other events. From the experiments  side, there seems only little to no improvement over standard NDTT.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; This paper proposes a selective token generation method for additive learning under pre trained language models. Overall speaking, this paper is somehow not qualified enough. The algorithm is the RL based method, the difference is that the author introduces a policy selection policy to decide the generation policy. If this is not the case, the results are also not surprising to see the improvements. However, it is missed, also for other parts in the model. The authors are highly encouraged to make a major modification of the paper.<|endoftext|>The reformulation of the proposed idea (selector to switch generation between a base PLM and a task specific PLM) as a reinforcement learning problem is interesting. However, I have some major concerns on the technical clarify/rigor and experiment presentations that hopefully authors could clarify and help me better understand your contributions. Therefore, I unfortunately cannot champion acceptance for this paper at this time. What is the "penultimate representations" in the paper? More below.<|endoftext|>This paper proposes STG, a method of adding a task specific head to a pretrained language model, and finetuning it with RL along with a policy selector that chooses when to add this policy specific logits to the pretrained logits for a given downstream task, targetting the low data regime. However I find the results unconvincing, primarily in their improvement over the base method. There are also some questions I have with the proposed method and how it is supposed to solve the overfitting issue like the paper claims.<|endoftext|>The paper is interested in the problem of exposure bias of sequential text generation: tokens are drawn from the data during training, while during inference, the tokens are sampled from the model’s distribution. The authors introduce a method that combines task specific adapters and reinforcement learning based training: at each token prediction, the model chooses between the distribution of the original pre trained LM and the distribution induced by the task specific adapters on top of the pre trained LM. Breath of results is also limited as the authors consider only two tasks  I would have appreciated seeing human evaluations.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 6; The paper proposes a new loss to improve test time BN adaptation for domain adaptation. They propose the hard and soft likelihood ratio for the confidence loss which has large gradients for high confidence predictions. It presents the analysis of the gradients of different losses for confidence maximization. Weakness:  The proposed diversity loss is not very effective on mini batch data. The novelty of the paper is relatively limited. My main concerns are that the effectiveness of the proposed $L_{div}$ on mini batch data, and the scale normalization problem of the proposed logits. Hopefully, the authors can address my concern in the rebuttal period.<|endoftext|>Studies the problem of test time adaptation across distribution shift, and proposes i) a new self training loss with better stability than entropy minimization ii) using a diversity regularizer and iii) an additional “input transformation” module. 2021.– The paper lacks a proper ablation study: while the input transformation module is ablated, what is the individual contribution of each proposed piece? – The proposed loss does appear to have certain limitations, as it is unbounded and relies on proper scaling via model design (eg.batch norm layers) to prevent logit explosion. – “the soft likelihood ratio loss creates lower amplitude gradients for low confidence self supervision”: this does not appear to match Figure 1 (right), where SLR is slightly larger than HLR for low confidence (<0.2). post rebuttal The author response had addressed my concerns about the behavior of the proposed loss. In light of the paper s empirical contributions but limited technical novelty, I recommend a marginal accept.<|endoftext|>In the spirit of full disclosure: I have recently reviewed this paper, and several parts of my previous review are still applicable, thus I am copying in these parts when appropriate. This paper presents a method for test time adaptation based on several techniques. Strengths The self supervised log likelihood ratio objective appears novel, as far as I am aware. And, for this problem setting, the combination of the aforementioned techniques is novel and leads to stronger empirical results than what has been previously reported. Weaknesses I think that the paper has improved on this point, but the motivation behind the general approach is still somewhat shaky. Imagine if the model was already very confident for the entire batch of data points, but there is a (predicted) class imbalance in the batch. Would it not be the case that the model would adapt in this case when using the proposed approach, even though it would make more sense to not adapt at all, which for example entropy minimization would (roughly) do? A final minor nit from my previous review: I would still like to know whether or not a confidence of 0.82 is "low" for the corrupted image datasets or other instances of test distribution shift.<|endoftext|>For Rebuttal  Please provide a control experiment for the choice of optimization. Please explain the data subset experiments in more detail (Figure 3). + The proposed extensions are still online and efficient, so this method seemingly could be deployed as easily as TENT. Would it not be better to always adapt on all the data that is encountered? Weaknesses  The novelty of the proposed extensions is diminished by intersection with prior methods. It is a pity that the most new part, the test time input transformer, is not further studied and improved to give this work a more independent dimension of contribution. The input transformation model is the most new, as input adaptation of this kind has not been done during testing, but there are close connections to training like ANT (Rusak et al.) Deriving an alternative loss without this restriction would be better to simplify the application of the method. Along with the more novel parts, such as input transformation during testing, this work also helpfully confirms and tunes other parts like the diversity regularizer in ways that future work can simply adopt.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; Overall I have nothing to complain about and think this is going to be a rich and interesting line of work   training LMs directly on structured text data. The resulting model is able to utilize the structure in HTML documents for a variety of tasks such as zero shot summarization, fine tuning, classification and more. The empirical analysis is thorough, the results look very good, and the writing flows very nicely. This paper introduces a large scale LM trained directly on the raw HTML in a large scale web crawl (the common crawl corpus).<|endoftext|>This paper presents HTML based large scale language model pretraining. The model architecture and pre training method are both based on BART. Are there any other tasks the model is trained on? 3. the proposed model seems to be a strong zero/one shot learner (both text generation, and classification) and appears to be on par with strong existing baseline. 4. the HTLM model can also be used via the more traditional pretrain+finetune paradigm and achieve good performance on GLUE set. I wonder if there is any correlation between the amount of pretraining data vs the zero shot or finetune accuracy? 5.What denoising pre training tasks are used exactly?<|endoftext|>This paper introduces HTLM which is a language model pretrained on a large scale web crawl hyper text data. 3.The proposed model achieves superior performance on zero shot summarization and does well on classification tasks. * A modified BART pretraining objective is proposed to inject noisy size hints to control the length of the span to be generated by the model during training. The model architecture and training objectives are largely based on BART, with some modifications to tailor for HTML format training data.<|endoftext|>The paper proposes a new large language model, called HTLM, short for Hyper Text Language Model. The language model has been trained on 25TB of HTML text. The authors argue that HTML tags provide valuable information regarding document level structure. The trained language model achieves state of the art performance on zero shot summarization by prompting the model to predict the text within the <title> tags.
Accept (Oral); rating score: 8; rating score: 8; rating score: 8; This paper proposes a Neural Voice Camouflage (NVC) method that has three important characteristics, which are essential for an NVC method to be used in practical scenarios: general, real time, and robust. Since the proposed method trains a model to learn predictive attacks without any constraints about input and output, it can be applied to any vocabulary in a real time scenario, and it is also difficult to defend the attack.<|endoftext|>The proposed method works in real time and is robust against some defenses. ### Strengths  The motivation for the problem, the formalization, and the experiments are clearly written and well organized. The proposed method is tested in various situations that reflect real world scenarios, and successfully deployed the method in a real room environment.<|endoftext|>This paper presents Neural Voice Camouflage, a real time attack method that disrupts in streaming ASR systems. This paper proposes a novel attack approach with a purpose of disrupting the automatic speech recognition system. The methodology is clearly explained and the main contributions are well supported by a solid evaluation framework and carefully designed experiments. It would be interesting to see the results of voice converted samples.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; rating score: 6; This paper proposes a Transformer based universal policy to control modular robots. Being the space of modular robots combinatorial, it is (almost) impossible to train specific policy for each sample of the "robot distribution". Therefore, the paper proposes a transformer based architecture design with explicit information about the robot morphology that, trained on a large number of possible morphologies. This universal controller generalizes zero shot to unseen robot designs belonging to the training distribution and can be efficiently fine tuned on new robots and tasks. The approach is thoroughly evaluated on different tasks. 2.The experimental results are solid (even though some things could be improved, more on this later). The following is a set of weaknesses that I think could improve the contribution. The robustness to morphologies in the same training distribution could be very valuable there I think. This naive baseline could give more intuitions on the advantages of the transformers for the task.<|endoftext|>In robotics, researchers primarily train a single robot for a single task, while modular robot systems suggest the flexible combination of general purpose building blocks into task optimized morphologies. The authors proposed a Transformer approach to learn a universal controller over a modular design space leveraging the “large scale pre training and fine tuning” scheme. The proposed claim is appropriate in that robot morphology can be understood as a modality on which we can condition the output of a Transformer.<|endoftext|>This paper presents MetaMorph, a Transformer based universal controller to learn behaviors across different robot morphologies. Moreover, results also demonstrate the generalization capacity of MetaMorph to different robots through zero shot transfer and fine tuning. It proposes an interesting way to use Transformers for learning a universal controller for different robot morphologies. Weakness:The contributions on algorithmic and network designs are marginal. Their approach is a direct application of Transformers and Reinforcement Learning approaches. Clarification: The description of section 4.3 can be improved, it is slightly confusing to follow. However, the overall contributions to algorithms and network designs are limited.<|endoftext|>The paper presents MetaMorph (MM), a transformer based architecture for generalization over robot morphologies. (The "morphology aware transformer" used by MetaMorph is just a standard transformer encoder, like that of Amorpheus.) The main weakness I see is a failure to adequately distinguish the proposed approach (MM) from the very similar AMORPHEUS approach (AM) by Kurin et al., 2021. Now to the terminology. The paper presents compelling experimental results, but does not adequately differentiate MetaMorph from AMORPHEUS by Kurin et al., 2021, and does not include AMORPHEUS as a baseline in the experiments.<|endoftext|>The paper uses transformers for incompatible multitask continuous control and shows impressive empirical results on a wide range of tasks in multitask training, zero shot generalisation and transfer. The authors propose morphology based conditioning and dynamic replay buffer balancing to increase the performance. Dynamic replay balancing proposed in the paper might be useful in multitask RL in general. Lack of clear comparison with Amorpheus (Kurin et al, ICLR 2021) that also uses transformers for incompatible multitask control. Dynamic replay balancing will only work if the environments are somewhat similar given the used metric (e.g.similar number of episode steps). How do you learn positional embeddings? When you study zero shot/transfer behaviour, what is the experimental protocol? Can you provide the results of this experiment?
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 6; rating score: 6; The paper provides several artificial examples on which SGD has an unintuitive behavior. There are also some issues with the presentation which I think can be fixed. This paper brings a very interesting viewpoint on the optimization process of SGD. There is one important issue which I would be happy to see the author’s response:The difference between the results in Section 5.2 and 5.4 is confusing. If it is really as defined there then it is extremely confusing and should be defined in the main part of the paper. I think the authors should provide some intuition for this definition. Is it convergence with probability? I think this is a non standard notation and should be defined.<|endoftext|>The paper constructs example optimization problems to illustrate that SGD may behave strangely, out of common expectation, when some assumption is removed, e.g., decaying learning rate and the noise nature. The abnormal behavior of SGD is out of expectation from the first sight. Strong points. Thus, this paper emphasizes the importance of the convergence with high probability. The paper has interesting observations including  for SGD, which are not carefully discussed in literature. Personally the reviewer would like to see the paper published on important venue like ICLR.<|endoftext|>This paper demonstrates on several fairly simple (e.g.1 dimensional quadratic) objectives that stochastic gradient descent may easily have very poor behavior: it could converge to a maximum, or diverge even in convex settings if the learning rate is too high. Note that the distribution (and $r$) depend on the learning rate. It is also shown that AMSgrad must converge to a local maximum on some non convex distributions. Then the central limit theorem provides an understanding of the limiting behavior of these iterates. The fundamental concept it suggests seems to be that we need to think more carefully about our assumptions and goals when proving results about optimization algorithms   e.g.it is often believed that convergence to critical points is fine since very likely adding a little noise will prevent converging to maxima or escaping saddle points. However, from looking at the constructions my intuition is that such perturbations may not change the overall message of the paper too much. Note: for proposition 3, I believe do you need to say that $a<0$? The paper introduces some interesting examples accompanied with relevant commentary.<|endoftext|>This paper is well written and has a good presentation. While the main point of the paper is very interesting, it is not clear what can we learn from these special example problems. The main point conveyed by the authors is that the population loss associated with eq(2) is a strongly concave function with a local maxima point at $w 0$, and if we perform SGD with the constructed Bernoulli data distribution, then it is possible that SGD converges to this local maxima w.p. Hence, one can find a regime of learning rate in which the gradient descent on the strongly convex loss dominates the strongly concave loss, and converges to the maxima. The other constructed examples in the paper have similar issues. Overall, the examples given in the paper may be too extreme to provide a good understanding of SGD in machine learning practice.<|endoftext|>If anything, it serves to _reinforce_ this wisdom by providing an interesting cautionary tale to the effect that "SGD _with a large, constant step size_ can converge to local maxima". The paper s results can be summarized as follows:1. Overall, I trust that the above should suffice to indicate the tone and tenor of the changes that would be expected from a revision. 2.They provide a quartic loss function under which SGD converges to the function s sharper minimizers (as measured by the trace of the Hessian at said points). [4] Stefan Vlaski and Ali H. Sayed, Second order guarantees of stochastic gradient descent in non convex optimization, https://arxiv.org/abs/1908.07023, 2019. Thus, while on the mathematical axis I consider the paper to be a good fit for ICLR (I would rate it between 6 and 7), the reported results do not suffice to support the paper s (overly ambitious) narrative claims, hence my overall reject recommendation. 1.The statement that "SGD noise is multiplicative and state dependent" is too vague and lacks context.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; rating score: 10; This approach can be used to generalize SVGD to both constrained and non euclidean settings. The paper is slightly too dense and somewhat unfocused. One of these, SVMD is shown to reduce to regular mirror descent when a single particle is used.<|endoftext|>The premise of the paper is clear and interesting : in traditional optimization, mirror descent allows you to optimize over constrained feasible sets, and this paper attempts a similar approach to sampling.<|endoftext|>The aim of this work is to extend the existing particle evolution method Stein variational gradient descent or SVGD to constrained domains and non Euclidean geometry. The second one is Stein variational mirror descent defined with some adaptive kernels and it is also applicable to constrained domain problems.<|endoftext|>The paper introduces new methods to run SVGD in constrained domains and non euclidean geometries. The theory proposed looks sound with a strong background and the experiments look very convincing. The paper is very complete and introduces all the different notions needed to understand the theory of the proposed methods.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; The paper demonstrates that the intermediate activations in Denoising diffusion probabilistic models (DDPM) can capture semantic information and thus can serve as representation for high level vision tasks. Can a flow based method also contain meaningful semantic information as DDPM? * Experiment setup / datasets  The number of test images with annotations is certainly small (ref.Table 1).I am not so sure if the conclusions that are drawn from the small scale test set can be consistent and generalizable to the large scale. It includes sufficient details for understanding the proposed method.<|endoftext|>This paper proposes to treat deep activations of denoising diffusion probabilistic models trained on image datasets as unsupervised pixel features. These pixel representations are used as inputs to simple classifiers that perform semantic segmentation. Figure 2 (unsupervised clustering of deep features) is excellent motivation for the proposed method, and the analysis (S.4.1, Figures 4 and 5) answered many of my initial questions about the results. An analysis of how the randomness in features affects training and evaluation would be beneficial. For the latter, only one self supervised model and choice of layers was used. The paper presents a new use of DDPMs as unsupervised representation learners.<|endoftext|>They are particularly interested in the semantic segmentation as a prototypical dense computer vision task. I also want to thanks for the reviewers for their extensive responses to questions raised by the reviewers. The paper is well written. Section 3.1 talks about  later  or  earlier  diffusion steps (associated with small and large values of t, respectively). Some discussion of this would be useful. Table 5 shows dependence of segmentation performance on size of the labelled training set.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; The paper contributes with an empirical analysis of the phenomenon as well an algorithm to address the issue. Works trying to tackle the problem in its fundamental aspects, starting from the weight adjustment algorithms are very much interesting for the community. It  provides empirical results showing that a certain amount of randomness reintroduced dynamically into the model helps in keeping the model able to learn. For instance, I would have expected to see at least experiments to rule out the fact that the inability to learn is due to gradient vanish (this can be easily plotted by monitoring the gradient magnitude). Currently, the paper is in between the two worlds, and unsatisfactory in the way it addresses the point above with respect to both. For instance, in the continual learning community, one might wonder on what are the relationships between the proposed method and other weight constraining/regularization approaches such as EWC and LWF. In terms of novelty, the solution proposed in the paper builds heavily on existing approaches, mostly on generate and test extended on non major aspects, such as support for multi layer perceptrons and non LTU activations. The empirical analysis, as anticipated, could make good use of experiments showing the behaviour of the gradient magnitude across learning iterations. Current experiments, although commendable in the fact that they cover both supervised and reinforcement learning, do not provide substantial proof of CBP being better than existing weight regularization techniques.<|endoftext|>This work highlights the shortcomings of the backpropagation algorithm for continual learning applications and proposed continual backprop that used a generate and test method to continually inject random features alongside SGD which enables better learning on non stationary data streams. The approach looks promising, but is limited in terms of the experiments and the comparison with existing approaches that makes it difficult to assess the true potential of this approach A comparison with these methods would help with better assessing the proposed approach.<|endoftext|>This paper investigates the problem of fast adaptation in a non stationary online continual learning(CL) setting. It argues that keeping weight randomnization is important to fast adaptation in CL. In particular, it proposes to evaluate the utility of each hidden unit   including importance to the current task and adaptation capability. The authors conduct experiments to evaluate the performance of the proposed method. Strong Points* The paper takes one of the most import issues in continual learning: non stationary online CL setting. * Although the paper focuses on fast adaptation to new tasks in CL. It s worth discussion the reasons for the results. I like the idea of continual reinitialization and handling it by the proposed generate and test method. Hopefully the authors can address my concern in the rebuttal period. [After rebuttal]:After reading the other reviewer s comments and authors  feedback, I would like to lower my score due to the same concerns as other reviewers as mentioned.<|endoftext|>They show a degradation in performance over time on permuated MNIST, non stationary RL problems, and the bit flipping problem. They demonstrate that utilizing this method, they can achieve better performance that does not degrade over time and that it works in more cases than l2 weight decay. The paper would benefit from more directly exploring their hypothesis of "decaying plasticity" of neural networks by exploring the change in weights, gradients, and loss landscape over time. Since the paper is largely empirical in demonstrating this new problem, it would be significantly stronger if it could demonstrate this problem on Deep convolutional models, and attention networks. The model sizes and datasets are also relatively smaller. Could this be a problem of overfitting and the proposed method is acting as regularization? The accuracy for the lowest in the current graphs seems to still converge quite quickly. 2019.This paper proposes and tackles a very novel problem in continual learning that would have great significance to the field and is important to explore. Post Rebuttal:I would again like to thank the authors for including the additional experiments on model size and dataset change speed.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 5; In this work, the question arises of a better way to evaluate different methods in Continual Learning. A minor concern I have with CLEVA Compass is the definition of the inner level points. Still, it is a concern that I have. The paper addresses a crucial topic in Continual Learning, and I think it may interest many people.<|endoftext|>This paper proposes an approach for a more nuanced assessment of continual learning which provides a visual representation that enables the identification of a given method s context with regards to the broader literature, and enables the comparison of two methods in terms of reported metrics. The transparent and multi faceted approach is interesting and unique, and can be truly useful for those working in the community. i.e."The CLEVA Compass should thus not be used to conclude a method’s superiority and its utility will depend on faithful use in the research community." why is this the case?<|endoftext|>The paper proposes how such a diagram could be used by future CL research and anticipates unintended uses. Standard deep reinforcement learning *is* a continual learning problem with its own set of trade offs due to agents being required to sample their own training data. The paper proposes a multi objective radar/spider web chart (CLEVA Compass) as a classification heuristic for CL research evaluation setups. This should be made clear upfront, with a discussion of all the assumptions such learning settings make.<|endoftext|>This work proposes CLEVA compass which provides visual means to easily compare different continual learning works and provides a checklist to promote results reproducibility. The main motivation and the benefits of the proposed compass is not clear. The figures have efficiently summarize the main components in the paper. As an assessment tool the main advantages that this work can provide to the community is not clear to me.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; This paper presents CrowdPlay, a crowdsourcing platform to collect human demonstrations for any MDP. The platform also provides an easy way to post process the collected data for learning experiments. I also appreciate the authors providing an anonymous staging for the platform, and that the entire platform will be open source. [2] is the state of the art offline RL algorithm on Atari games. Online and offline reinforcement learning by planning with a learned model. The platform makes use of good decision decisions. Therefore I recommend a strong acceptance for this paper.<|endoftext|>This CrowdPlay pipeline not only supports recruiting different users from different channels to collect multimodal behaviors and data but also designs diverse and real time incentive mechanisms to guarantee and improve the quality of data. Furthermore, the authors present a dataset, which is publicly available, along with benchmarks on Atari 2600 games, to enable further research on Imitation Learning and Offline Learning. Furthermore, the dataset provided by this paper is conducive to the research on Reinforcement Learning. There are several issues that could be further addressed:  Since some of the datasets provided in this paper are imbalanced, will the performance of the AI agents trained by these data be affected by the imbalance? The proposed framework CrowdPlay, including the software and the dataset, is interesting and attractive for researchers in the areas of Reinforcement Learning.<|endoftext|>The paper has to have hyperparameters for the experiments in Section 5. I encourage the authors to focus on the following two points:  Describing the dataset in more detail providing the insights about the data and their properties. Explaining why the dataset is challenging to the existing methods, and how it can drive the further development of the fields of imitation learning and offline RL. In this form, this is the paper about how to effectively collect a dataset rather than a dataset itself. Why are the existing datasets not enough to develop strong offline RL algorithms? Which paper it was first used in? Figure 3 uses the term "good data" that uses the term "task adherence". Not all RL environments are gym environments.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; This submission proposes a transformer framework for unsupervised domain adaptive classification tasks. In this submission, they conduct an exploration about cross attention layer and found that the cross attention layer is robust to pseudo label noise. Strengths:  an exploration about cross attention layer is conducted and it is discovered that that the cross attention layer is robust to pseudo label noise. a novel three branches architecture is proposed in this submission, and the source target branch is utilized as a teacher to guide the other two branches during training. the experimental results seem promising, which demonstrate the efficacy of this method. This work is of novelty and the experimental results are strong enough to demonstrate the efficacy of the proposed method.<|endoftext|>This paper proposes a weight sharing triple branch transformer framework, or CDTrans for unsupervised domain adaptation. A two way center aware labeling method is proposed to provide better pseudo labels. + The paper is well written and easy to follow. + The methods achieve SOTA performances in various UDA benchmarks. + It s interesting to see the performances of transformers in UDA. Two phase Pseudo Label Densification for Self training based Domain Adaptation, ECCV 2020I suggest the authors to compare their methods to other pseudo label generation method to see if the proposed pseudo label generation improves over them. (Although it is an arxiv paper, it s also good to include it as it provides better UDA performances on office 31/Home)I hope the authors could provide some insights/intuition why transformer can generalize well from source to target. And the comparison with other pseudo labeling approaches is still important to evaluate the contribution.<|endoftext|>In this work, the authors propose a method for domain adaptation by introducing a new way of generating pseudo labels and a cross transformer with classification and distillation losses. Since the transformers compare source and target in a patch based manner, the authors find it is more robust to false positive pairs. The authors do not focus on learning domain invariant feature representation and mainly solve the domain adaptation problem by pseudo labeling. This paper has the following strengths. The problem addressed is interesting for the community. The method achieves outstanding performance on multiple datasets. My main concerns are as follows:  Comparison with other methods is unfair. However, most previous works compared (e.g, SHOT, CGDM) use ResNet as the backbone. To better show the effectiveness of the proposed method, it is better to adopt the domain adaptation techniques of other works with the same backbone. Despite the concern on novelty, overall this paper lacks justification of their method to see whether the improvement is brought by the backbone change.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; In this paper, the authors propose an RNN architecture named SGORNN. The proposed model can be seen as an extension of the FastRNN model, the theoretical analyses of which also apply as a result. The proposed model is evaluated on a few tasks including the addition problem, HAR 2 classification, and PTB. However, the proposed method seems to be a simple extension to the FastRNN model, and has limited novelty. The experimental results could also be improved by adding more baseline methods, including RNNs with orthogonal weight matrices, as listed in the introduction section.<|endoftext|>This paper proposed a novel recurrent network architecture called Scalar Gated Orthogonal Recurrent Neural Networks (SGORNN). The authors theoretically studied the generation bound and gradient exploding condition of the model. Lastly, experiment results show that the proposed model outperforms standard FastRNN and a version of vanilla orthogonal RNN on three tasks: (1) synthetic adding problem, (2) HAR 2 classification, (3) Penn Treebank word level language modeling. Combining orthogonal transition matrices with other mechanisms in RNNs has been studied in the literature (arXiv:1706.02761), but the authors failed to mention any of them. However, this is far from showing the model is valid. The theory section in the paper has a meaningful contribution. The experiments are not convincing.<|endoftext|>This work proposes to approach and partially solve (under certain constraints) the exploding gradient problem of RNN. Demonstrations are given for the exploding issue and for the motivation of parameter constraints. The new model, called SGORNN, is then tested on two toy tasks and on a largish one (language modelling). Unfortunately, and appart from the first toy task, the experiments fail, imho, at giving insights on how well this model will behave in practice. We do not require a SoTA results, this is out of the scope of such a paper, however, it remains that a new researcher can not have a clear idea of if this idea will work and scale well in his/her own complex scenario. Experiments fail at proving the points addressed by the proposed method (at scale and with realistic conditions). The conclusion isn t really a conclusion but more of discussion + conclusion.<|endoftext|>The authors motivate their selections with gradient magnitude bounds and show proofs for them. The model architecture is relatively novel yet simple to implement, the bounds are easier to satisfy compared to FastRNN or VPRNN, but most importantly the model s performance is far superior to the baselines. The one issue I have is that perhaps slightly less relevant, harder baselines should have been chosen so as to show how much work there is to do to get a SGORNN like architecture to converge to performance in more typical models. All in all, I think this paper provides an interesting theoretical contribution that the community would appreciate. An interesting model with nice properties that does relatively well empirically, but more baselines (eg, orthogonal RNNs) should be included before the paper is accepted.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; There may exist schemes to generate these adversarial vectors which are compatible with the theorem formulation but cause a deviation in the median much larger than what the theorem promises. While the topic is interesting and the paper may have a valuable contribution, I don t succeed to verify the proofs of its theorems, and the way in which the theorems are stated lets me believe it is possible they are not correct. > send (plural)* Algorithm 2, part 2, step 2.<|endoftext|>Other comments:1. The novelty seems to be fairly limited as the paper mainly combines two known ideas: bucketing and secure median. The complexity of the proposed algorithm is characterized in terms of number of calls to secure comparison. 3.Assumption 2 seems to require that each client has a very large dataset, and computes gradient descent on the entire dataset.<|endoftext|>The approximation is based on bucketing, which divides each dimension of the model into buckets and sends a unit vector that has a 1 in the bucket containing the client’s update and the median is approximated as the middle value of the range of this bucket. My main concern is that the paper does not consider some closely related works. More experiments with large number of users need to be performed to show the effectiveness of the proposed approach in large scale systems. [2] H. Fereidooni, et al."SAFELearn: secure aggregation for private federated learning." Hence, I recommend revising the paper to include all relevant works and adding the comparisons.<|endoftext|>The proposed method includes two key ideas. The paper studies privacy preserving and robust federated learning. I think the paper would be stronger if my comments are addressed. Experiments were conducted to show that the proposed method is more efficient than cryto based standard median implementation. Basically pairwise comparison is adopted as a baseline for implementing the standard median. How about median of median? Overall, I think this is a promising direction. The paper picks Median, which shows some robustness but is not really robust.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; This model is optimized for image compression. 3) An efficient parallel architecture is proposed and is more time efficient than the serialized one on modern GPU device. The paper present a novel contribution by using first Transformer based method to image compression task.<|endoftext|>I think this paper is "marginally above the acceptance threshold" since the use of transformers as entropy models for image compression is novel and interesting. The paper needs a more thorough empirical evaluation to warrant a higher score.<|endoftext|>Overall, the idea is novel and interesting. However, there are some critical details missing. The same argument applies to Figs.
Reject; rating score: 5; rating score: 5; rating score: 6; Paper presents a method to push physical quantity to a physical neural network (PNN) and push the ‘additional quantity’ such as noise to a correction term virtual neural network (VNN). #2.Then the assumption is made for some nodes being observed and some not observed. The the new symbols are introduced [y_o,y_u]   f( [x_o, x_u]). Seems to me that y_o is a set instead of variables representing one specific node. Perhaps it is due to the confusion I brought up in #2. will do a more proper review after my queries have been addressed.<|endoftext|>The task of identifying a physical system on a graph is addressed. While the main part of the to be estimated model is assumed to be linear, the proposed model needs a nonlinear part (which is modeled by a neural net) due to the presence of unobserved nodes. The authors use a combination of a sparse linear model and a neural net, which is basically the same as the model in [Li & Weng, KDD 2021]. They consider some regularization terms to maximize the use of the physics part of the model. The motivation and the base problem setting are understandable, and the experiments are done on multiple datasets. (11) What is the relation between the proposed adversarial loss and the well known Wasserstein distance based on the Kantorovich Rubinstein duality (see, e.g., the paper of Wasserstein GAN)? Experiments lack important ablation studies. (12) What is the actual use of $Y$ in equation (3)? This sounds a bit weird because many physical systems are not usually modeled as a directed graph. I would like to know the difference between the cross validation for hyperparameter selection and that for generating the final test results. (4) I fully agree with the need for restricting the output of VNN and encouraging PNN to output as much as possible. In fact, similar ideas have been already investigated in the context of physical system learning in literature such as [Yin et al., 2021] and [Takeishi & Kalousis, 2021]. Do they refer to $x$ s (or $z$ s) on unobserved nodes? (5) In section 3.3, the skip connection of the two neural nets is "proposed," while it is just a frequent practice. However, in the early phase of training, since the models are not well fitted yet, perhaps there might be some practical difference.<|endoftext|>The paper proposes ATN to model and identify physical systems. While I slightly tend to accept this paper (thus I updated the review score accordingly), I would not champion this paper for the acceptance. The authors assume that there are physics bases that can represent the output with a linear mapping. The possible remainder (due to the noise or unobservable physics) is modeled via a more complicated neural network (VNN in the paper). These two networks are integrated into a single skip connection block. In addition, the authors suggest the use of siamese networks that measure the distance between PNN and VNN: then an adversarial learning loss is added to guarantee the outputs of PNN and VNN to be far from each other and play their respective roles more concisely. ATN outperforms its counterparts for some physical system datasets. ICLR 2021." This paper also decomposes the unknown physical system as a known physics part (however whose parameters are unknown) and the neural network part that fills the remainder that cannot be explained by the physics based one. Can the authors elaborate it more clearly? Currently, I lean towards rejection due to the concerns I raised. I would like to update my evaluation after discussing with the authors and my fellow reviewers. ***Post rebuttal:  I appreciate the authors’ clarifications on my questions.
Accept (Poster); rating score: 6; rating score: 6; rating score: 5; rating score: 5; The paper proposes a new simple black box adversarial attack algorithm, which utilizes surrogate models. * The proposed algorithm is simple but achieves state of the art performance. The authors also do not define "local information". The paper proposes a simple score based black box attack algorithm which achieves state of the art results. The simplicity and  effectiveness of the proposed algorithm is great for the research community. Moreover, the empirical comparison is not enough to show the advantage of the proposed method. The proposed algorithm is a simple switch algorithm of the above methods based on the update success, and thus the technical contribution is marginal. * The empirical comparison is insufficient, because the proposed algorithm was not compared with a variant, which only uses the surrogate’s loss gradient without the coimage of the surrogate’s local linearization (i.e., similar to [1]). Since the proposed algorithm is the combination of two existing approaches (i.e., gradient based and coimage based), the algorithm should be compared with both approaches. The right panel in Figure 3 implies that the variant will achieve very similar query efficiency with similar success rate, so the lack of the comparison is problematic. To show the advantage of the combination of both existing approaches, the evaluation in more difficult settings (such as under small norm bound and on adversarially trained robust models) is also important. It seems that Sec.4.4 just focuses on input specific priors and using surrogates, which are related to all local gradient based methods and not specific to GFCS.<|endoftext|>The paper proposes a novel adversarial attack algorithm, which behaves as twofold: (1) it tries to apply iterative ascent in the direction of the surrogate s loss gradient until it can and then (2) samples alternative directions from the coimage of the surrogate s local linearisation. The discussion flow is clear and easy to follow although there are a number of concepts that I am personally not familiar with, and so I believe it would help to define all the notation and the concepts studied (besides the basic definition of an adversarial attack) for a reader like me. Related work seems to be properly analysed. Furthermore, although I am not familiar with the standard experimental setup in this area, I should say the results look good to me and the advantage of the proposed algorithm over the state of the art looks convincing. I wonder if the authors tried to compare their approach to the approaches based on formal methods for reasoning about the model behaviour. Overall, I find the paper to offer a solid contribution that may be interesting enough for the community working in the area of black box adversarial attacks, if accepted to ICLR.<|endoftext|>The paper proposes a simple, yet effective adversarial attack in a black box scenario with available surrogates. The success of the proposed idea is surprising, while this effect is not explained by the experiments in the paper. if we don t consider benchmarks that use gradient steps that are taken directly from surrogate models. Common sense and previous work they cite (e.g.P RGF) also make similar statements. It would be interesting to consider the correlation between used surrogates and the victim model and quality of attack, as we can easly degrade the quality of a surrogate or maximize discrepancy between a surrogate model and a victim model by incorporating a specific term in the loss function. The idea of ensembles for the performance improvement seems interesting (also we can try to apply something similar to bandits to select the best surrogate during the optimization?). The authors suggest an interesting effect but provide a little investigation of it. The cause of this effect can be some specifics of used pairs surrogate victim or more general effect related to the correlation of deep learning models trained on large datasets and on general transferability.<|endoftext|>The proposed approach combines the ideas from transfer based attacks and zeroth order optimization methods for score based attack methods. Experiments on multiple architectures show that the proposed approach is query efficient while achieving similar success rates compared to recent approaches. Although there are several good things about the paper because of its simplicity and effectiveness, the authors should provide more experimental evidence to show the effectiveness of the proposed approach. The proposed approach is simple, effective, and query efficient. It would be interesting to see if the proposed approach achieves similar performance on other datasets e.g.CIFAR10.2.It is hard to compare the performance of different approaches from Table 1 as they all are performing really well. 5.I have some concerns with the setting of the experiments. The surrogate models are trained on the same dataset and use the same loss function which is already a lot of information for the adversary to attack the target model. 6.Since the transfer attack using the surrogate models are easy for the setting considered here, it would be an interesting ablation to apply FGSM or PGD attack using the gradient of the surrogate model(s) and compare the attack success rates.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; This paper proposes SpecTRA to construct spectral filterings on top of vanilla multi head self attention networks to have coefficients for each input graph. Specifically, SpecTRA computes the filter coefficients from the attention weights of vanilla multi head self attention networks by defining a mapping function (i.e., Equation 9) from the attention space to the filter coefficient space. + The paper is well structured. I can understand the motivation for filtering graphs for GNNs (Gao et al., 2021), but adapting this existing motivation to argue that the transformer cannot effectively segregate the noise from the signal is not enough. Equation 9 is the main contribution in SpecTRA, but why does SpecTRA need it? This makes a confusion as if it is correct, SpecTRA is an extension of Graph Transformer with additional using spectral filterings inspired from Gao et al., (2021). + The obtained results mainly come from the existing positional encoding schemes (Kreuzer et al., 2021). + Why are the GIN and GCN results on MUTAG and NCI reported in Table 1 different from the original paper? https://github.com/daiquocnguyen/Graph TransformerThe paper lacks the motivation and the proposed model SpecTRA is incremental.<|endoftext|>This paper proposes a framework that adds in an extra GNN layer after the Attention layer of Transformer, which enables better quality modeling in graph data. In particular, the GNN layer learns a per head transformation that imposes learnable filter coefficients, which demonstrates better performance than static filters coefficients. My main concern is that the paper itself does not compose a coherent story of why the GNN layer should be effective and makes a difference as a whole. Compared to simply injecting graph structures into positional embeddings, this method gives a novel way to fuse benefits from both sides. ## Cons* The descriptions and notations by the paper are preventing a coherent understanding. * The method itself is hard to be in the SOTA class. For example, in Table 1, even for SpecTRA+GCKN+3RW which is the best among 3/6 datasets, it can still not outperform the previous SAN Sparse one on the remaining 3/6 datasets.<|endoftext|>This paper proposes a novel model called SpecTRA to optimize the performance of the transformer model on the graphs. This proposed model induces a spectral module into the original transformer architecture to enable it to decomposite graph spectrum and filter the frequency domain and thus leading to more useful information. This paper has clear motivation and goal; that is since the transformer model naturally gives diverse attention sub spaces, which correspond to the multiple filters covering the spectrum of the graph. Under these settings, the goal of the model is then to learn the filter coefficients; this process also naturally provides interpretability. 4.Overall this paper is well organized where the authors clearly know what they are talking about. This variance in performance could suggest that the proposed model need certain prerequisite to be fully functional.<|endoftext|>For example, why do the authors learn the spectral GCN filter weights from the attention matrix of the transformer, which can have a completely different sparsity pattern than the input graph? Why not learn the filter weights from the graph itself, e.g., by using a GNN? While the authors claim that this theorem justifies the proposed architecture, it is not clear how. Yet, none of these position encoding schemes were proposed by the authors. Other comments:  The abstract and the introduction are difficult to understand and do not give a clear picture of the paper s contributions. However, other architectural choices such as learning the spectral GCN filter weights from the transformer s attention matrix are not well motivated.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; The paper proposed to parallelize the inference and training of GRU networks (a type of recurrent neural networks) at the `time` dimension. Since the parallelization technique in this paper gives best performance for recurrent neural networks with long sequences, the impact of this paper might not be that wide. My understanding of this paper is thus not very deep. The paper provides a new view to parallelize the training task of machine learning models.<|endoftext|>The authors describe the incorporation of a multigrid reduction in time (MGRIT) solver to speed up and better parallelize the application of forward and back propagation of information. Unfortunately, it is not clear at all what factors affect parallelism and scalability.  ) The appendix should provide guidance as to how the experiments are organized and might be replicated. The main drawback of this paper is novelty. This would be a very nice addition   note though that the related paper cited by the authors uses MPI+GPU with experiments on up to several tens of GPUs. )<|endoftext|>The paper describes a technique for evaluating GRU networks based on the multigrid reduction in time (MGRIT) technique. These techniques are not new, in general or to neural network training, but the contribution here is their application to GRU layers. Key idea is partitioning of a long sequence into shorter sub sequences. Obviously, a GPU implementation of this technique would be interesting as well, and the authors note that this is in development. Even without it, though, the results are compelling enough.<|endoftext|>Strengths:In this paper, authors propose a novel parallel in time (PinT) training method based on a MGRIT solver for GRU networks with long sequences. This method has been well motivated by illustrating the limitations and unsolved problems of the existing methods. The proposed algorithm enables the accelerated parallel training of GRUs on long sequence, which is a unique capability that permits growth in this dimension. Can you show the results/plot in the same manner or provide an explanation to it? Comments:Grammar errors(underline): Sec.4.1   “When training with MGRIT three levels are with a coarsening rate of 4 are used for the length 128 sequences. ”In Conclusion – using “6.5\times” would be better than “6.5x(letter x)”N/A
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; Overall, I vote for accepting. The motivations and contributions of the paper are very clear and important. Hopefully the authors can address my concerns in the rebuttal period. Finally, the paper also provides some results to show the transferability of the approach, and some analysis of learning dynamics. A minor comment, but the usage of policy/reward made me think that this paper uses some reinforcement learning approach.<|endoftext|>I will raise my score if the authors can conduct the ablation experiments and address my concerns. Because the technical novelty of this paper is limited (which is acceptable), empirical evaluation needs to be substantially strengthened. Some additional experiments are needed. Could the authors provide more intuition on this?<|endoftext|>The authors proposed a new reward function to learn the distribution of augmentation policies for NLP tasks, which can generate "difficult" and semantic similar samples to facilitate training. This paper proposed a simple and effective reward function for learning based augmentation selection in NLP.<|endoftext|>MixUp also shows good results for some datasets under Table 1. What are the ranges of the magnitudes? While it is a motivated decision to introduce these terms during the training of $f_\theta$, can these losses also contribute to the learning of better representations and lead to the improvement?
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; I was one of the NeurIPS reviewers for this paper. The key advantage of this paper is that the authors can use a significant amount of computing resources while other researchers can not. ###############################################################################################################The authors used a huge amount of computing resources to tune hyperparameters of Adam/SGD and claimed that they can match the performance of LARS/LAMB for large batch training. The performance of LARS will be much higher if they use the same amount of computing resources to tune hyper parameters. This means the comparison is not fair. "Table 2 shows that Nesterov momentum achieves higher training accuracy than LARS, despite similar validation performance. For BERT, I suspect the authors tuned the hyper parameters of fine tuning process, which means the comparison is very unfair as the LAMB authors did not tune it. The key technical part is just tuning hyper parameters.<|endoftext|>I might be open to increasing my score if those concerns are addressed. This paper doesn t add anything new to that discourse. Moreover, it highlights nuances in their hyper parameter choices which are very important to consider. * First of all, how should I interpret these differences in performance, are they significant or not? * If the hyper parameters were indeed tuned using a validation data set, how can the paper claim that the aim of the optimizers is to optimize sample loss? If I just cared about optimization, I would tune the hyper parameters to minimize the optimization loss. 2) LARS and LAMB were proposed to improve the batch size scaling while training. > Although the main concern of a practitioner is validation performance, the primary task of an optimization algorithm is to minimize training loss. It depends on how the optimizer is sampling, tuning its hyper parameters, etc. Overall, the comparison was not as comprehensive as [this paper](https://arxiv.org/abs/1811.03600). Although LARS obtains diminishing returns when increasing the batch size from 32,768to 65,536, future work could investigate whether Nesterov momentum drops off more or less rapidly than LARS. These are important questions. But it doesn t do much more. 3) How did the authors come with the 75.9% threshold?<|endoftext|>This paper takes a closer look at deep learning optimization methods tailored to the large batch size regime, namely LARS and LAMB. It shows that generic optimizers(SGD with Nesterov momentum and Adam, respectively) can achieve similar results in the largebatch regime given a careful tuning of their hyperparameters. *The paper is well written. While potentially eye opening, this is a very limitedresult. As stated above, the paper is well written and clear. This paper is not novel and it isn t trying to be.<|endoftext|>This paper revisits the effectiveness of the optimizers designed for large batch training such as LAMB and LARS by You et al.(2017, 2019) respectively. While it has been claimed and demonstrated (in perhaps limited settings though) that such optimizers can achieve better performance compared to other generic optimizers such as SGD or ADAM (in the sense that they don’t require a specific batch size), this paper re evaluates these optimizers while fine tuning all hyperparameters involved to potentially affect the result and finds that they do not work better as claimed; or, more precisely the standard optimization algorithms including Nesterov and Adam can match or outperform LARS as long as they are properly tuned. Understanding large batch training is quite important in this days of large data, and setting up a proper baseline result is an important.
Reject; rating score: 1; rating score: 1; rating score: 1; rating score: 3; The main structures of Fig.2 and Fig.3 has no difference with DANet. This version far from reaches the level of ICLR. Maybe the authors can improve it according to the weakness part.<|endoftext|>Between Intro and Related Works there is only a citation to a 2020 segmentation approach.<|endoftext|>Spatial attention and channel attention modules are used in the proposed method, as well as a multi scale context module. The authors should clarify the motivation in the introduction section.<|endoftext|>I assume it s channel attention from Sec.4.5.Eq.(10) and Eq.(9) are essentially the same, but applied to the outputs from different modules. I also expect the authors to oblate the losses, e.g.what if there s no CE loss (Eq.8).
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; A novel recurrant graph neural network, for spatio temporal encoding, coupled with a message passing neural network, for spatial decoding, is proposed for imputing missing values in a multivariate time series; The architecture uses temporal and spatial properties of the data. This paper is clearly written and covers a lot of related work.<|endoftext|>This paper proposes a method for time series imputation modeling the spatial dependencies with graphs. You focus on spatio temporal data. The derivations are sound and it is very well written.<|endoftext|>I found it interesting that spatio temporal methods to impute time series values do not take graphs into account. In this paper, the authors propose a GNN based imputation method for Time Series. The paper is well written, and easy to understand.<|endoftext|>This paper proposes to leverage GNN that takes available relational information for multivariate time series imputation. Limitation of the approach:The reliance on RNNs makes the proposed approach unable to handle irregularly sampled data. I am not convinced that the proposed method can work well when the observation becomes very sparse.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; rating score: 5; This work proposed a systematic framework of (unsupervised) domain adaptation for time series data, with various model selections and strategies. did not cover or compare with the recent work (DAF) of domain adaptation for time series by Jin et al.https://arxiv.org/abs/2102.06828    did not justify well why domain alignments are important well. what if using different backborns, such as attention based ones? difficult to interpret the experiment results  what is visual UDA in the table 3? why were average values reported? the observations are not much informative   one on imbalanced data is not connected to model selection criterions. What is the real take away out of this? may it recommend to use other possible backborn, e.g., attention based one? hard to understand why authors emphasize fair and realistic procedure. The paper is not well written especially experiments parts, and the message of our them are not clear in the line of overall paper take away.Therefore, the contribution seems to be not clear beyond aggregating all existing DA methods and systematically deploying them.<|endoftext|>The paper proposes a systematic evaluation framework named ADATIME, which systematically evaluates different unsupervised domain adaptation methods on time series data. The paper conducts large scale experiments adapting the state of the art visual domain adaptation methods to the proposed framework on time series classification tasks. The findings based on the experimental results reveal the key points of applying UDA to time series data. 2.The paper proposes a novel UDA framework for time series data, which may contribute to the community. The paper only talks about how the framework is designed, while not elaborating clearly on why the framework is designed like this. 2.The selected datasets are relatively too small and simple that the backbone network can only be a 1D CNN in order to avoid the overfitting phenomenon. The experimental results on these toy time series datasets are unconvincing. 3.The organization and writing of this paper should be improved. The authors should pay more attention to the motivation and the details of the framework. 4.It would be better if the authors explain some symbols like $X_{train}^{S}$, $X_{test}^{S}$, $Z_{train}^{S}$, $Z_{test}^{S}$, ..., etc. arXiv:2102.06828Minors:On page 2, “the the following questions"  > "the following questions"The findings are interesting and may contribute to the community.<|endoftext|>This paper explores the unsupervised domain adaptation of time series data (TS UDA). By standardizing the base model, datasets, and model selection, this paper provides a good benchmark of TS UDA. This benchmark can facilitate future research. Also, the paper proposes some findings. ## Strengths(1) This paper evaluates the previous domain adaption algorithms under a fair setting. Also, the experiment results present some competitive baselines to the TS UDA area. This benchmark will be helpful to future research. (2) Based on the experiment results, this paper provides some findings or analyses. (3) The paper is well organized and with clear clarification. Especially, the proposed AdaTime framework is also trivial. It is hard for me to distinguish them. That is why I think the AdaTime is not novel. (2) Some of the findings are also fragile. Model selection has a significant effect on performance. I think your experiment results are also affected by the limited data. (3) I am not sure about the contribution of this paper when the backbone is just CNN. Not only the numerical values but the relative performance is also changed. So I think the findings of your paper may also be changed under other base models. Secondly, as your mentioned, the time series area does not have a consistent backbone. Thus, the experiments are lacking in the backbone aspect. In conclusion, I think more baselines are needed if you want to obtain a general conclusion, such as the TCN. But because of the concern of technology novelty and the unconvincing evidence of some findings, I would like to reject it. after discussion The author addressed part of my question.<|endoftext|>The paper introduces , a standardized framework to systematically and fairly evaluate different domain adaptation methods on time series data. “Deep learning has achieved a great success in time series classification tasks, assuming access to a vast amount of labeled data for training” I am not totally convinced that this is true. I am very curious about how well one could do with a much simpler methods. It is easy to tell LAYING from any of the dynamic classes, simply by the fact that the variance of LAYING is less than one tenth of the dynamic classes. It is easy to tell LAYING from the other classes because G (the acceleration due to gravity), sifts from one axis to another etc. I understand that simply maximizing accuracy is not the full point of the paper, but I am still curious of we really need deep learning here. “In addition, we find that model selection plays a key role and different selection strategies can significantly affect performance.” It would be surprising of that was NOT true. While the novelty is low, the experiments as detailed and forceful, and the community may find them useful<|endoftext|>This paper presents an empirical approach for unsupervised domain adaptation of time series data. The paper points out some of the drawbacks of existing approaches due to inconsistencies in evaluation schemes, datasets, model selection rules, and base neural network architectures. The paper then presents adaptations of visual domain adaptation methods for time series data. Experimental results using ten state of the art methods on three benchmark datasets spanning fifteen cross domain scenarios are presented. Guidelines for future research are given. While the authors  efforts in systematic experimental evaluation of existing visual domain adaptation algorithms and time series unsupervised domain adaptation algorithms is commendable, there is no theoretical understanding at even the most basic level. hence the conclusions drawn from the experiments may be specific to datasets and baseline architectures used. The conclusions do not generalize.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 3; This paper studies how human interpretable are the concepts learned within the representation space of self supervised models. Instead, it proposes a “reverse linear probe” moving from which maps combinations of binary concept labels to the representation space, which is clustered using k means. The paper uses this linear probing method to propose a normalized mutual information metric to measure how well human interpretable concepts are encoded in the representation space. First, the paper finds that models trained on ImageNet images capture more than just semantic class labels. I also listed some potential questions for investigation.<|endoftext|>The paper proposes a method for characterizing the "meaning" of the representation learned by a given model (interpretability). Towards this goal, it proposes reverse linear probing, a post hoc method that aims at predicting a quantized version of the internal representation (as observed in specific examples) from semantic label. In addition, the proposed method is evaluated considering a good amount of self supervised learning methods. I find the way in which the paper approaches the idea of finding relationships between the internal representations learned by a model and a set of concepts very similar to [Escorcia et al., 2015], that did it at the attribute level, and to [Oramas et al., 2019], that further explore it towards interpretation of learned representations. While this indeed may seem as a positive and cheap means to further get annotations, as properly noted by the paper, it indirectly requires the existence of the expert predictors and the annotated data on which they were trained. In Section 4.4, it is observed that confusion between different clusters decreases when additional concept groups are added. Finally, it might be good to provide some details on the linear models mentioned at the end of Section 4.1.<|endoftext|>This paper presents an approach to investigate the semanticity of representations learned via self supervision applied to images. The approach measures (via mutual information) how well a linear model can map a vector indicating image attributes (e.g.objects, texture, etc) to clusters within the representation space. The authors apply this method to recent methods from the literature and interpret the resulting ranking, aiming to learn more about which methods produce representations that map well to human judgements of similarity. I enjoyed reading the paper   it is well written and the proposed approach is simple. Again, these differences may be alleviated with a more adequate grouping of approaches across experiments.<|endoftext|>The paper aims to measure the interpretability of the visual representations learned by the recent self supervised models. In doing this, the paper formulates the “interpretability” as the mutual information between the feature clusters and a set of visual concepts that can be interpreted by humans. The mutual information is approximated by the proposed Quantized Reverse Probing, a linear function that maps the concept vectors to the feature clusters. I’m afraid both linear probing and the proposed method have this problem, the proposed method relies on pseudo labels generated by the expert models that were trained with data annotations.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; The authors study these "limiting dynamics" in linearregression, which they model as an underdamped Langevin equation resulting in anOrnstein Uhlenbeck (OU) process and find an oscillatory dynamics in the(position, velocity) phase space. The authors discuss how to connect theirfindings in this simple model with the dynamics of Resnet18 (seebelow). I welcome the direction of the present study, trying to understand the dynamicsof deep neural networks via simpler models. Furthermore, modified losses arising through SGD dynamics have been studied in anumber of recent deep learning papers, some of which are cited by the authors Other claims about the significance of the results should equally be clarifiedin my opinion, for example:> The expectation that the training trajectory would reflect the underlying > anisotropy of the training loss driving the dynamics is also wrong;" (p. 9)In my understanding, this study is only concerned with the *limiting* dynamicsof learning, and hence conclusions about the training cannot be drawnimmediately?<|endoftext|>What are the practical consequences of these results? At the theoretical level the study focuses on linear regression. The authors affirm that the dynamics brake detailed balance. Also, what are the practical implications of braking detailed balance? Your model, by construction, is a correlated Brownian  motion with a drift, therefore the diffusion constant is 1. Although the result may be of limited practical interest, it is a new finding and would be interesting to understand causes and consequences of that (not discussed in the paper). I can guess that the next number after 10^4 would be 2*10^4 but this is not obvious a priori. An important aspect, that has not been discuss, is the effect on generalization.<|endoftext|>The experiments are thorough, and consistent with theoretical results;3. The contribution of this work are carefully clarified. **Weakness**The only pity is that all the theoretical results are derived from a linear regression model. Please can you elaborate them in order to raise my score? Their results certainly deepens our understanding on the learning dynamics of modern neural network.<|endoftext|>The analysis of the limiting cycles is nice, but I do not see what we learn more than was already presented in Ref.[21]. The author(s) (1) show some empirical findings related to the mean square displacement, (2) model SGD as an underdamped Langevin Equation, relate it to an Ornstein Uhlenbeck process in a linear regression setting, and use it to study the limiting dynamics of SGD, (3) use the Fokker Planck formalism to show that the steady state weight distribution obeys a modified loss which is isotropic in the absence of L2 like regularization, (4) provide empirical evidence that their findings are relevant also beyond the context of linear regression. (c) "we show that the key ingredient driving these dynamics is not the original training loss, but rather the combination of a modified loss, which implicitly regularizes the velocity, and probability currents, which cause oscillations in phase space"(d) "We identify qualitative and quantitative predictions of this theory in the dynamics of a ResNet 18 model trained on ImageNet."
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; The paper is overall clear and easy to follow. Incorporating variance information is a promising direction that can date back to the analysis of UCBVI, which can generally obtain better results. 2.There are some minor typos in the paper, e.g.in Section 3.3 “the proof can be found in Appendix D” rather than “can be bound in Appendix D”, for Theorem 3.5 the sum term should be in the denominator. 3.In the current presentation, it’s not clear where the improvement in Section 3.3 comes from. 4.Is the lower bound of Theorem 3.5 matches the upper bound of Theorem 3.3 up to logarithm factors and higher order term? I think so, but the authors don’t provide sufficient discussion on this. I hope the authors can continuously improve on these aspects, which will further improve the readability.<|endoftext|>The given work proposes the introduction of a pessimistic lower bound for reinforcement learning agents.The estimated variances are used to re weight the Bellman residual learning objective, such that the samples with higher uncertainty are correspondingly penalized. Weakness:*There is a lack of empirical evaluation carried out, to showcase how the lower bound estimation * It s hard to follow for those who are not familiar with this sub field. * Minor typo in Introduction(section 1): "stat"  > "state"Overall, the paper is well motivated, aiming to solve an important problem in the area of offline reinforcement learning.<|endoftext|>This paper studies an extension of value based pessimistic offline RL in linear settings, where variance information is included in the pessimistic penalty to provide a refined quantification of uncertainty. This work contributes to the study of value basic pessimistic RL by closing the gap in suboptimality bounds. It also related RL to OPE by showing the variance aware technique in OPE also applies to policy learning. Weakness / questions:The current setting of the paper is well studied and complete, but it might be a little restricted. I am interested whether such variance incorporated ideas can be used for fully data dependent constructions. In general, this paper is clearly written and elegantly presented. This work provides a refined treatment of pessimistic penalty functions, which leads to tighter upper bounds and nearly matching lower bounds.<|endoftext|>The paper is well written and easy to follow, with the key improvement (theoretically) easy to see. The authots propose a variance aware approach for offline RL, which leads to the pessimistic value estimation. The core idea of the algorithm is to use the variance term, to re weight the Bellman updates   and provide improved bounds for offline RL). This paper can be seen as a follow up along those lines, studying the limits for offline RL under linear MDP assumptions. What is the key novelty in the paper that leads to the improved bound? Can the authors point this out exactly? I do not think without experiments, the work has enough technical and theoretical novelty (in terms of proof details) for acceptance.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; This paper addresses the reward free exploration problem with function approximation under linear mixture MDP assumption. Can the authors clarify this point? Crucially, the proposed approach can work with any planning solver to provide an $(\epsilon + \epsilon_{opt})$ optimal policy for any reward function.<|endoftext|>The paper propose and analyze a model based algorithm for the reward free exploration setting. The analysis shows that the proposed algorithm is (nearly) minimax optimal. The proposed algorithm is agnostic of the planning algorithm that is used to solve an MDP constructed with the estimated model. There are many typos in equations.<|endoftext|>This paper studies reward free exploration with linear function approximation (i.e., under the setting of linear mixture MDPs). This paper addresses this technical challenge and makes a solid contribution to reward free exploration. The proposed solution to the technical challenge of the Bernstein type analysis under this scenario is novel. I would like to raise my score if the above concerns are addressed in the authors’ response. 2.Under the linear mixture MDPs setting, there is no concrete optimization procedure for the $\epsilon_\text{opt}$ optimal model based solver (a key assumption in this paper). Thus, this paper deserves to be accepted and I would raise my score if the mentioned concerns are addressed.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; rating score: 6; This paper critically re examines the rationality of domain invariant based method for DG (generalize to unseen domains without re training). It highlights that, tackling all source domains equally without taking the underlying relationship between them to learn domain invariant features can lead to a sub optimal solution. By this, the relationship between latent space and label space can be strengthened, thus the model can generalize better. The results show that LASSO (especially with threshold) can commonly lead to a marginal performance boost for popular DG benchmarks. Simply using the mixture sources domain may not reflect the true distribution of the target one. Besides there are some other weaknesses as follows:1. I can only find the author’s explanation rather than strict prove. This part is essential for this paper, besides, this is closely related to my given score. 2.In Section3.4.2, the author proposes to use sampling for model training. I wonder about the performance of doing this in a deterministic manner, just like using threshold at the testing phase. 3.Experiments are somewhat lacking, 1) challenging datasets (e.g., medical imaging for CV, reinforcement learning in MLDG) should be considered since the proposed method is more related with general machine learning,  2) only RSC is considered as the latest stoa baseline, which is not desired. It would be better to keep this unchanged. Though I am not an expert in this area, I have discussed my comments with my colleagues with rich experience in this area.<|endoftext|>This paper introduces a new method for domain generalizable classification. Although the paper tries to provide mathematical backgrounds and underlying theories of the proposed method, unfortunately, this reviewer finds no clear justification for learning and using the sub spaces. Also, the improvement by the method seems marginal (or it even underperforms sometimes) when coupled with small backbone networks. (1) How the diversity of latent representations could reduce the latent data shift issue: Theorem 1 and the first paragraph of page 4 suggest that to achieve domain generalization one has to reduce the latent data shift, which is unfortunately not straightforward since no target data is available in the task. The second paragraph argues that learning a single domain invariant model is not enough to resolve the issue, but such an argument is not theoretically proven. (2) How the sub space hypothesis could reduce the latent data shift:In page 5, the authors argue that the compactness of the sub space can reduce the latent domain shift, which is however not proven theoretically either. (4) How the independence of the masking weights leads to better generalization:The argument in page 6 about the advantages of such independence is not rigorous; the reason following the argument seems indecipherable. The first theorem resembles the well known theorem in literature of domain adaptation. It would be useful if their differences are well explained in the paper. Also, the practical value of the method seems limited as its performance is not impressive or even inferior to previous work when it is incorporated with small networks.<|endoftext|>The paper involves both theoretical and empirical results. Theorem 2 extends Theorem 1 but with latent subspaces. This seems to work similarly to the dropout, but with more structure and with learned and data dependent drop rates. The trend seems clear that for shallow network (e.g., AlexNet, ResNet 18) the performance is not necessarily better than previous methods, but with deeper network (e.g., ResNet 50) the improvement is  somewhat significant. Strength    The proposed method is solid and novel. The empirical results show solid improvement over baselines with a large network backbone. Weakness    Theoretical arguments are not particularly strong. While formulating a bound with latent space might be new, but it does not seem to add any better understanding for domain generalization. \Gamma is optimized based on Equation (7) and it is unclear what authors mean by "our principle is to encourage \Gamma_{d} to becomes more independent". Misc    It is unclear what it means by "but the gains shrink with ResNet50 since larger ResNet backbones are known to generalize better" in Section 4.1.3. The trend observed in the paper on the performance w.r.t.the network size seems interesting. Might be good to add more experimental results with deeper networks to see if trend holds true. While there is a question on the significance of the theoretical results, overall the paper proposed a new domain generalization algorithm that is both technically and empirically solid.<|endoftext|>Authors propose a new method which doesn’t have a single hypothesis shared among domains and give theoretical analysis of the proposed method. Authors also give results on benchmark datasets. 1) Authors correctly point out overly strict and sub optimal assumption that is popular in the literature currently. 3) Some benchmark papers in the literature (which don’t have an assumption of domain invariant features) are not considered [1 2]. 4) Theoretical analysis in the given paper helps to come up with a loss but not actually analyze the method. Like in [1 2], can authors comment on the learning theoretic study? 5) As we have a large number of domains and a large number of training examples then one should be able to have optimal error or loss. I could not draw this conclusion from the theory that the authors provided. 6) How were hyperparameters tuned for the baseline method? "Domain Generalization by Marginal Transfer Learning." Addition of suggested literature could further improve the paper.<|endoftext|>This paper proposes a latent subspace orientation algorithm for domain generalization, which is built on diverse latent subspaces and individual hypotheses with label informative features. Some theoretical analyses are given to support their learning scheme, and a very simple latent subspace orientation algorithm is proposed to tackle the domain generalization challenges. Experiments validate the usefulness of the proposed method on different datasets. Apart from these claimed theoretic analyses on the subspace indicator \Delta, the authors should give a more intuitive explanation of why it is useful, which can help the readers understand your work, rather than lots of equations. Moreover, the reviewer is wondering if we fix these indicator updates after some iterations, what will happen for the performance because the review doubt such an update is meaningless for this task in practical systems. Moreover, how could you set the value of \tau? Is there any adaptive learning strategy to determine it or empirically setting? This paper proposes a simple latent subspace orientation algorithm for domain generalization based on some theoretical and experimental observations. The technical parts with some complex theoretical analyses are satisfied, yet the model construction and experimental parts could be improved.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; This paper collects additional human annotations for Cifar10 and Cifar100. These annotations are also compared to synthetic label noise alternatives. A large number of baselines were implemented and compared in Section 5.1. Are there any important differences or modifications to the process the authors made? The "qualitative" section on instance dependent noise comparisons is confusing and does not help the overall flow of the paper. UPDATE after rebuttal  The authors have sufficiently addressed my concerns regarding the differences with respect to CIFAR 10H.<|endoftext|>The paper is easy to follow, and the observations are comprehensive. **Strength**  The paper systematically compares human noise and synthetic noise (including class dependent label noise and instance dependent label noise) thoroughly. **Weakness**  (minor) The paper can make some connections with the worker simulations in data labeling. They also adopt a similar class dependent scheme for worker simulations. To make the paper more complete, I suggest the author make more connections toward a broader community that focus on label noise.<|endoftext|>One of the main challenges in this area has been the absence of controllable real world noisy data. By comparing the results on the original CIFARs and the newly proposed CIFAR N, four observations are detailed quantitatively or qualitatively. In addition, they provide the performance comparison of more than 10 approaches on CIFAR 10 and  100N. The analysis of the memorization effect in human annotated noisy data is also discussed. In particular, the analysis with human annotated label noise (using CIFAR 10N and CIFAR 100N) provides some insightful information. Can authors report the imbalance ratio of the two proposed datasets? In addition, why does the DivideMix shows the worst performance in the CIFAR 10N aggregate dataset? I know, that human noise is like a feature dependent noise, thus it is much harder than the synthetic noise setup. In particular, resolving the above concerns will add more clarity and novelty to the paper.<|endoftext|>This paper proposes two smaller sized datasets with real world label noise. The datasets contain CIFAR 10 and CIFAR 100 images with human annotated labels. The paper analysis the noise pattern, and benchmarks existing methods on these datasets. This paper shows that real world noise pattern is different from synthetic noise, and the datasets proposed could be useful as a smaller scale benchmark for future researchers. The paper is also well written in general. While I find this paper to have a clear strength, the contribution is not enough to justify acceptance to ICLR.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; The paper proposes Markov game based framework for safe reinforcement learning. Cons* Some issues with the Markov game framework and no analysis for them. Safe RL problems are very important in many applications. With these two agents, the c MDP is transformed into a Markov game between the two agents. This idea of formulating a Markov game between two agents to tackle c MDP is very interesting.<|endoftext|> The paper presents an alternate approach to the problem of minimizing constraint violations during learning in CMDPs (safe exploration). ### Concerns:  *Missing related work:* The idea of having decoupled task objectives and safety planning has been explored in the community, although not from the lens of Markov games. Some of the works that come to mind are: [1] where barrier functions take the role of safety planner (Safety Agent) and can override the Task Agent policy,  [2] where pessimism is used for the Safety planning, and even in CPO based approached where there is a fallback policy when the agent violates the safety constraint that overrides the original task policy [3].<|endoftext|>This paper proposes a new framework for safe RL which is called DESTA. DESTA is a framework where two players called Safety Agent and Task Agent interact with the environment. First concern lies in Experiment. The last concern is reproducibility. In addition, although there is no theoretical results supporting the authors  claims, reproducibility is rather low.<|endoftext|>The paper proposes a Markov game based method for dealing with two competing objects: reward maximization and constraint satisfaction. This method incorporates the idea of the Markov game into the constrained Markov decision processes. Overall, I like the idea in the paper. My major concern is theoretical support and clarity. Hopefully, the authors can supplement some progress during the rebuttal.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; The paper presents a contextual bandit algorithm called regularized optimism in the face of uncertainty (ROFU) which is designed for general function classes. The experimental results of the paper may have potential. I feel that the experimental part of the paper is more solid and interesting than the theoretical part.<|endoftext|>The authors present an algorithm called regularized optimism in face of uncertainty (ROFU) for general contextual bandit problems. To me this is a very strong assumption. The theorem is also strange to me. Although I cannot come up with any previous work that consider similar algorithms, I think the authors should compare more previous methods to support the novelty of their algorithm. The strong assumption makes theoretical contributions of this paper incremental.<|endoftext|>The authors mention Foster & Rakhlin, (2020) once in the introduction but do not directly compare with their algorithm and results despite the fact that it is also a regression oracle based algorithm with flexible function classes. The paper also provides experimental results to support their claims of efficient implementation. Another drawback is the lack of comparisons with more recent work including  Foster & Rakhlin, (2020) and Simchi Levi & Xu (2020). Strengths:I think the paper is tackling an important aspect of the contextual bandit problem, aiming for flexible function classes and efficient implementation.<|endoftext|>This paper studies contextual bandits with general function classes. Contextual bandit with general function classes is an interesting problem in machine learning. However, I think this paper still has some concerns. However, it is not clear in the paper how to deal with the finite parameter space. 7.Some experimental results haven t converged. I think the theoretical part is not very consistent with the motivation and the experiments.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; This work examines the problem of the large number of iterations necessary to get high quality samples from denoising diffusion generative models. The paper mostly consists of two new components:1. I will retain my score, though I hope that the authors follow through on their claim that they will provide additional results in the camera ready submission. PMLR, 2015I find this paper to be very interesting and its results are quite good. I will address the main points of this paper as outlined above.<|endoftext|>I think the problem of accelerating diffusion models is important and believe that the paper makes a novel contribution towards it. **Strengths**The idea in the paper is relatively simple and straightforward, and has good empirical performance. The paper discusses how to address the engineering challenges in general DDSS search problems (discontinuity of timesteps, memory constraints). Post rebuttal review  I think the rebuttal addressed most of my concerns.<|endoftext|>So the improvements in FID/IS scores might be expected. 4.As mentioned in the paper, GGDP can use pre trained DDPM models. If that is the case, more datasets are needed to show the strength of the method. Is there a formal proof?<|endoftext|>In general, the paper s contributions are unclear, while the paper tackles a very interesting problem in the context of diffusion based generative models and provides empirical improvements.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 3; The key idea is to view the client selection problem as a facility location problem, where the objective is to approximate the aggregation of gradients of all the clients with that of a subset of selected clients. A greedy solution to the problem is derived, and the theoretical convergence analysis is also provided. Experimental results on synthesized and real datasets show the effectiveness of the proposed approach. ### Strengths  Overall, the paper is very well written. The proposed work is well motivated, as developing better client sampling techniques is one important direction in FL. Proposed approach is technically solid and seems novel enough. With some reasonable assumptions, the convergence analysis for the proposed approach is provided. Utilizing updates collected in previous rounds would be one possible approach, but it was not fully evaluated in the experiments in my opinion. Overall, I think this is a nice paper that addresses an important aspect of federated learning: improving a client sampling strategy. Experimental results are promising, though I would like to see how not limiting the number of local updates will affect the performances.<|endoftext|>This paper studies the selection of clients for federated learning under the assumption of full participation via submodular function maximization. Specifically, the goal is to obtain a subset of the clients such that the aggregation of the gradients of their loss functions approximates the full aggregated gradient. The minimization of this particular supermodular function can be equivalently posed as the maximization of a submodular function, which has been previously studied in the literature (e.g.[1] below). Finally, they test their scheme in synthetic and real data sets against other methods in the literature. The paper is overall well written and somehow easy to follow (see minor comments). I checked most of the math in the Appendix and it seems correct, but standard. Is there any connection between Assumption 1 and submodularity? The authors focus on the full communication case, can this approach be adapted to the partial case? Experiments: the authors should discuss Assumption 1 for the objectives used in this section, at least to have a sense of how large is $\epsilon$ for these applications. Also, it would be great to have an idea of the of time that each method spends. Appendix: $g_t$ and $\Delta v^k_t$ are not defined. The convergence analysis seems to be correct. However, the theoretical role of submodularity is not quite clear, besides the use of the greedy algorithm for the selection of clients. Assumption 1 seems quite strong, the authors should motivate this part more in detail. The experiments clearly show the advantage of DivFL over previous approaches in the literature.<|endoftext|>This paper studies the use of diversity sampling (based on submodular functions) for selecting clients who send updates to the server in a federated learning environment. As the authors show, this provides more efficient use of communication budget in terms of convergence and model accuracy. Subdmoularity based subset selection is widely used in many contexts, but as far as I can tell, it s the first time in federated learning. Theoretical and empirical results are significant. The paper proposes a new approach for diverse sampling of clients in each update round of federated learning. Though the idea is not particularly novel in the broader context, it does advance state of the art in federated learning.<|endoftext|>The paper studies a problem of improving the efficiency of federated learning by selecting asubset of clients whose gradients are representative. This is formulated as a submodular maximizationproblem, and the authors analyze a greedy algorithm for it. This is evaluated through experimentson synthetic as well as real datasets. Is there some special structure in the instance dueto which this holds? Assumption 1: this is a property of the algorithm. So I don t understand why this is an assumption. You need to prove that for the sets S_t chosen in each step t, this property will hold. They should say more clearlywhere that is (though an assumption in a prior paper doesn t automatically make it correct). In view of the issue with assumption 1, it is not clear if Theorem 1 is correctThe formulation in the paper has a lot of similarity with the setup in (Mirzasoleiman et al., 2020). The main contribution is the analysis of thegreedy algorithm, which has serious issues as mentioned above.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 8; The paper proposed a new model HetSNGP that captures distance aware model uncertainties as well as heteroscedastic data uncertainties. Strengths:This paper is well motivated. This paper combines the complementary benefits of these two types of uncertainty modeling and achieves a good calibration performance. The proposed method Is a combination of well known techniques. The novelty of this paper Is limited. There are many assumptions which are not sufficiently justified. This may limit the scope of application of the method. 3.The experiment results are weak. 4.This paper is hard to understand.<|endoftext|>The authors propose a method that can jointly learn the model uncertainty for out of distribution detection and data uncertainty for in distribution calibration. The effectiveness of their method has been validated on different out of distribution datasets. weakness+ The technical contribution of this paper may not be enough. Specifically, algorithms in this paper seem to be similar to SNGP [1]. I think that the major difference between the two methods comes from the different data generative processes. Table 2, Table 3, and Table 4 show that the performance of the proposed method and the baseline method SNGP are quite similar. Overall, I think this work is well motivated.<|endoftext|>This paper studies the combination between model uncertainty and data uncertainty based on spectral normalized Gaussian process. Theoretical results show that heteroscedastic SNGP allows for joint modeling of model and data uncertainties. However，the contribution of the whole paper can not meet the requirement of ICRL as (1) limited novelty. The proposed method is a limited improvement of SNGP. We can use methods in [1,3] to handle the data uncertainty, then use the method in [2] to handle model uncertainty. More discussion is needed.<|endoftext|>This paper presents a new method for model and data uncertainty estimation in deep neural networks combining the heteroscedastic method (Collier et al.2020) and the Spectral Normalised Gaussian Process (SNGP) method (Liu et al.2020).It is shown that the two methods are complementary and their combination outperforms state of the art methods on Out Of Distribution (OOD) detection on common image classification benchmarks. The paper is well written and the scientific contributions are clearly formulated and experimentally validated. The experimental results look quite convincing and show the benefits of this combined uncertainty model. This makes the paper difficult to follow.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; This paper introduces a VAE based model for interpolation of irregularly sampled time series. The temporal input data is mapped to a latent representation over fixed reference points with an attention mechanism, using an intensity network that allows to encode data sparsity information. Thanks to the intensity network and the heteroscedastic output layer, the proposed HeTVAE model can capture uncertainty estimates over the interpolated points. 1.Despite building heavily on the mTAN model, the new ideas introduced in the paper are novel and well motivated. The paper has extensive ablation studies that justify all the new components of the model  WEAKNESSES1.<|endoftext|>This paper introduces a novel model, HeTVAE, for probabilistic interpolation of time series that are irregularly sampled. HeTVAE builds on prior work by complementing it with a learned time dependent output variance in the VAE and architectural improvements. The performance of HeTVAE is evaluated on multiple datasets against various baselines and via ablation studies. The proposed model is for the most part interesting and well motivated, justifying the additions that are made on top of the prior mTAN. Accordingly, the paper is mostly well written and easy to read.<|endoftext|>The paper introduces a novel model of Variational Autoencoder that deals with irregularly sampled time series with a probabilistic approach to do time series interpolation. The main contribution is the architecture by itself, its components, and the training process. Multi time attention networks for irregularly sampled time series.<|endoftext|>Specifically, intensity encoding is introduced to make the model be aware of information about input sparsity. Does the heteroscedastic output means that you learn the variance and the homoscedastic output means you fix the variance? This paper also misses citation and comparison to a previous work NRTSI [3] that can also impute irregularly sampled time series. I suggest the author compare to NRTSI on the irregularly sampled Billiard dataset introduced in NRTSI. Does it mean "homoscedastic temporal VAE mTAN"?
Accept (Poster); rating score: 8; rating score: 8; rating score: 5; rating score: 3; The proposed method is based on ViT B (86M parameters) which is bigger than ResNet 50 (25M parameters) on which the other competing methods are based. Inspired by this observation, the authors propose a modification to ViT architectures that appending these token representations to the input and show that the resultant models generalize better for out of distribution data on ImageNet classification. The modification to the input is simple and can be integrated into variants of ViT. The author present an observation in this paper that discrete image token representations derived from a vector quantized image encoder is able to preserve shape and structure object information. The submission is pretty solid and the claims made in the paper are convincing. 3.The proposed method is simple yet effective as shown in the experiments. There are some observations unclear to me, but they are not critical issues given the solid experiments presented in the paper. It is a general approach to improve the robustness of ViT. The authors conduct detailed ablation study which are pretty interesting to learn. They also conduct insightful experiments to demonstrate this observation qualitatively. The quantitative evaluation on the ImageNet benchmarks seems to support this observation. 5.The authors present extensive ablation studies as listed below. The results are convincing.<|endoftext|>The authors have an observation that ViTs trained on ImageNet heavily depend on local features, but fail to use the global features (shape or structure). The authors claims that it can push the ViTs to learn the global information by this replacement. Experiments are conducted on ImageNet and other ImageNet variant datasets. Results show that the proposed method can improve ViTs  robustness on various benchmarks. The authors proposed a novel approach to improve the ViT s robustness, and made a comprehensive study on it. The paper is well written. Con:  The authors claimed that ViTs  fail to make adequate use of global context. From results of the paper, I believe that the proposed method can improve the robustness of ViTs, but it can not support the claim. Comprehensive experiments are conducted on various robustness benchmark. My main concern is that the major claim of the paper, that ViT s fail to make adequate use of global information, is not supported by the experiments. I d like to accept the paper if the authors can make the claim more supportive.<|endoftext|>This paper introduces Dr. ViT, which is conducted on 7 ImageNet robustness benchmarks. The motivation of this work is to demonstrate the robustness can be enhanced when adding discrete representation. Sufficient experiments demonstrate the effectiveness of this paper. Cons.1.#Abstract: (1) The claim ViTs "are overly reliant on local features" rather than "make adequate use of global context" leads ViTs to fail to generalize OOD, real world data. The (1)(2)(3) seems to be contradictory. 2.The number of baselines is very limited. Missing many important methods, such as CutMix, Mosaic, Puzzle Mix, Cutout, Manifold. 4.Missing enough discussion of the method itself. (1) After reading the paper, the reviewer still gets less information on the "codebook". 1.In Figure 3(b), the author noticed that the "Ours" model captures more structural and shape patterns. 2.Why using discrete representation can improve robustness? However, such an operation results in information loss. If it works, how about directly concat a global feature extracted by CNN? But the latter paragraphs could not work well in concert with the previous writing. After reading this paper, the reviewer could just observe "ViT + some global information would be better" rather than your method is necessary.<|endoftext|>Authors propose discrete tokens (instead of the standard continuous pixel values projected and fed as tokens to ViTs), to enhance the shape learning capability of ViTs, and thus make them robust against out of distribution data. Authors state that ViTs are overly reliant on local features and fail to make adequate use of global context. Inspired by https://openreview.net/forum?id Bygh9j09KX that shows CNNs are biased towards texture; [1] conducts an extensive study, and demonstrates that ViTs are less biased towards texture, compared with their CNN counterparts. The authors show that ViTs have shape bias, comparable to humans. [1] “Intriguing Properties of ViTs” Neurips’21 https://arxiv.org/pdf/2105.10497.pdfTo make a claim on texture vs Shape bias, the authors must conduct a principled study as in “ImageNet trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness”  https://openreview.net/forum?id Bygh9j09KXIn recent works, ViTs demonstrate better out of domain generalization, as demonstrated in extensive experiments conducted in Fig14 of https://arxiv.org/pdf/2105.10497.pdf  and Fig5 of  https://arxiv.org/abs/2106.09785In the paper, the experiments are conducted primarily on ImageNet robustness datasets. I am not entirely convinced by the shape vs texture bias claims made in the paper, due to lack of principled study as in  https://openreview.net/forum?id Bygh9j09KXAlso, it will be interesting to see if the proposed discrete tokens enhance ViTs robustness against common corruptions (e.g., artifacts introduced by rain, haze etc), and adversarial perturbations. The considered robustness analysis is too restrictive in my opinion.
Reject; rating score: 6; rating score: 6; rating score: 6; rating score: 6; rating score: 6; This paper introduced "inflation" (2D CNN kernel to 3D one by repeating on the 3rd axis) to transform a 2D image pre train backbone into a 3D version so that 3D point cloud task can be benefit from 2D image pretraining. Detailed experiments are performed to address this idea. The authors are kind of "surprised" by these results. Idea is good and helpful for the 3D point cloud field. Since such idea has already used in other 2D 3D domain, the novelty is limited. I doubt the fairness of the comparison with the baseline since it is possible that the performance gain is fully from the increasing parameters. For pointnet++, the training method is quite strange. This is an interesting idea. However, the discussion on why it work seems not quite convincing.<|endoftext|>This paper proposed a pipeline for transferring convolutional network weights that are pre trained on 2D images to 3D convolution networks. The experiments are conducted on 2D image datasets such as ImageNet, and 3D datasets such as ModelNet. The performance gain using the 2D pre training is notable and non trivial, showing the proposed weight transferring technique has the ability to convert useful learned information on 2D to 3D. However, as the authors pointed out, the visualization cannot show why the transfer works or what information can be transferred. This paper proposed a solution to a new problem of transferring 2D weights to 3D. Though some claims made in the paper are questionable, this paper deserves to be published at the conference.<|endoftext|>The paper proposes to transfer the 2D image features for 3D point cloud understanding tasks. Specifically, the proposed method first inflates the 2D convolution filters from a pretrained model to 3D and then optimizes only input, output and optionally batch normalization layer. The paper shows that using 2D pretrained features improve performance. ECCV 2020   Explanation why this kind of inflating 2D filter to 3D is reasonable. Although the experiments in the paper show that the proposed model can improve the performance and data efficiency compared to training from scratch, the overall experiments results are not strong enough.<|endoftext|>This paper described an experiment of transfer learning between image and point cloud data. The proposed method is also proven to be useful when the training data is limited by training under few shot learning setting. Cons: 1.The idea of inflating model weight from 2D to 3D is not new[1]. 2.The performance on point cloud segmentation on SemanticKitti dataset, while is reasonably well, still has gap to the state of the art(This one is minor)3. It will indicate which part of the pertained model is really useful in this transferred learning setting. "Quo vadis, action recognition? a new model and the kinetics dataset." The authors did a good job explaining the idea, concepts, procedures and experiments, showing the effectiveness of the transferred model on performance.<|endoftext|>Extensive experiments performed on the point cloud classification and segmentation tasks demonstrate the effectiveness of the work. Post rebuttalI have read the authors rebuttal as well as the other fellow reviewers  comments and the corresponding discussions. I agree with the other fellow reviewers  comment, 2D inflating to 3D has been studied in video action recognition. Therefore, I do think the paper is of novelty. Also I agree with the other reviewers  comments that the discussion of why the proposed method works does not been clearly addressed. It seems to the reviewer that this is the first paper to use 2D image pertained models for 3D point cloud understanding.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; The proposed method is based on the accuracy of the source target classifier to identify the covariate shift: if the source target classifier cannot distinguish the source or target data, then the proposed method considers they are generated from the same distribution and vice versa. positives:The authors investigate an essential problem in real world applications. negatives:  The proposed method attempts to adapt the offline covariate shift detecter to the sequential detection and learning scenario. However, due to the statistical nature of the two sample test, the proposed approach still need a relatively large number of data to work. This draft proposes an online covariate shift detection algorithm based on two sample tests. The theoretical guarantee is based on the size of sampled data, and thus, it is usually hard to make the error (false positive/negative rate) small in real world online applications.<|endoftext|>This paper proposes a new method to sequentially detect covariate shift which is the problem encountered when training and test covariates are not equally distributed. A lower bound on the probability of correctly accepting the null hypothesis is derived from the properties of the aforementioned interval. Nevertheless, guarantees on FPR are obtained under the assumption that the distributions are equal. This deserves further investigation since the given explanation based on multiple hypothesis testing is not convincing. Providing comparisons with some of the cited sequential two sample test would definitely make the paper stronger, justifying better the need for the proposed method and its advantages. The paper also  lacks comparisons to existing sequential two sample tests, which, even if  they do not provide guarantees on the False Negative Rate in non iid scenarios,  are natural competitors since they do provide FPR guarantees (under the hypothesis of equal distributions). Therefore, sequential two sample test are applicable to the scenario of this paper. Also, the power depends on the choice of the classifier. My concerns on FPR control have been addressed. This should be formalized. However, I can t see which multiplicity you refer to.<|endoftext|>However, in the real world, this assumption may be broken. For example, when the images change from daylight to night, the distribution changes. This paper presents a new sequential classifier two sample test to address these dynamic covariate shifts. In the experiments, they also show that the proposed algorithm efficiently detects covariate shifts on ImageNet. Thus, detecting such shifts is meaningful in many fields. 3.Experiments are conducted in many situations, which supports the effectivenss of the proposed method. As reviewed in this paper, kernel MMD is another type of two sample tests. In (1), what does \sP stand for? In the literature, concept drift detection and adaptation are very close to your problem setting. Mean shift? However, some points must be clarified before the possible acceptance.<|endoftext|>This paper proposes a new covariate shift detection method, which distinguishes whether training data and test data come from the same distribution. Experiments on ImageNet validate its effectiveness at detecting both natural and synthetic covariate shifts. ### Pros:1) This work introduces a novel method to train the classifier and detect covariate shifts in a sequential manner. ### Cons:1) This method evaluates each example before taking a gradient step on that example. It seems to be a great limitation of this method. 2) Besides, the experiments seem to be a litte unfair. The proposed method use the whole set for both training and testing while baselines sample some examples for testing and use the others for training. Though this paper is interesting and theoretically solid, the experimental parts do not convince me. I will raise my score if the concerns about the experiments are addressed. But I will not act as a champion for acceptance.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; Thirdly, I do not agree with Fig.1.Algorithms like DQN are in fact in the gray area where the state space is huge but the action space is just 4 dimensional. The authors claim to tackle the "gray area" where the state space is huge but not large action spaces as one of their key contributions. It would be great if authors could comment/explore this. So this goes back to my previous points of scalability and comparison with existing algorithms like DQN.<|endoftext|>I think that without some comment about the issue of how well the obtained decision tree based policy approximate the optimal LP based policy, the paper is weak. The idea is simple and the computational complexity analysis, though not surprising, is convincing. What is missing is an estimate of the efficacy of the decision tree approximation. The authors seem to completely ignore the issue of optimality.<|endoftext|>Post rebuttal  I thank the authors for their responses to my points, they make sense and I think including these explanations in the paper would aid in framing and justifying some of these decisions. Realistically the benefits of having a decision tree structure for a policy is so that the policy is interpretable and can be inspected and communicated well. I do not think the paper is ready for publication in its present form although the method is potentially promising.<|endoftext|>In total, I think that the approach has potential and is worth to be published at some point, but addressing the raised points from the other reviewers could indeed further improve the paper. The submission is clear and, as far as I can see, correct. state partitions to solve these problems. On the other hand, there are no comparisons to baselines on these problems. In general, the paper is well written. Namely all of the problems where the optimal policy doesn’t not have the underlying tree structure. And that the examples are simplistic. I agree with the optimality issue described by reviewer HjwV, and also with the author’s answer to this issue.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; rating score: 6; This paper proposed equivariant transformers   a neural network based algorithm to predict properties of molecules. The architecture is built upon the traditional transformer architecture, combined with some modifications specific to molecular property prediction tasks, such as exponential normal radial basis functions, SiLU activation function and design of update layers. [2] End to end symmetry preserving inter atomic potential energy model for finite and extended systemsL Zhang, J Han, H Wang, WA Saidi, R Car   arXiv preprint arXiv:1805.09003, 2018Overall, this paper is a good paper with strong experiment results on various datasets. The components of the model look very interesting.<|endoftext|>The authors introduce a novel architecture for ML force fields, the Equivariant transformer (ET). It is based on the Transformer approach and can be used to predict energies (and forces) and other molecular properties (e.g., QM targets). 3.Specification of the training details is welcome. 5.There is a typo in Table 3 (malonaldehyde). What did the authors learn about the model that would help them improve it, for instance? # UpdateThe authors updated the manuscript and addressed my questions. For this reason, I am willing to increase my score.<|endoftext|>The paper describes an equivariant neural network (ET) with attention mechanisms that is applied to the prediction of molecular properties. The demonstrated computational efficiency is impressive, in particular given the model size. The paper is well structured and the proposed method is described clearly. Update:Based on the response of the authors, I have raised my score.<|endoftext|>The paper presents an equivariant transformer model for predicting quantum mechanical properties from an atomic graph. The model obtains SOTA or near SOTA results on three popular datasets while maintaining good computational efficiency. While I would recommend the authors to clarify this analysis for the final version, I am leaning towards accepting the paper in its current form. This model is also well motivated by the underlying physics of the problem.
Reject; rating score: 5; rating score: 5; rating score: 6; In this paper, the authors propose 1 bit LAMB, which combines lamb optimizer with communication compression, with convergence analysis and experiment results. The paper is well written. In overall, I think this is a good paper. All the experiments are limited to BERT, which makes it unknown whether the proposed algorithms could be applied to other training tasks including cv and nlp. The main issue is in the assumption of the theoretical analysis.<|endoftext|>The paper proposes a communication efficient distributed LAMB optimizer with 1 bit compression. Given the prior work 1 bit Adam, the novelty becomes somewhat incremental. Experiments show training speedup due to communication compression. The paper propose a communication efficient large batch LAMB optimizer to accelerate distributed training.<|endoftext|>This work studies the problem of distributed training with large batches in a communication bottlenecked setup, where regular versions of algorithms such as LAMB become a constraint. The method also seems to generally follow from 1 bit Adam, with the exception of a gradient recovery mechanism (although authors indeed demonstrate that direct application of the techniques from prior work yields worse results). Authors also compare their method with several reasonably chosen baselines to demonstrate the importance of their contributions: while there definitely could be more pretraining tasks and models to be considered, given the space limits I believe the current scope of experiments is sufficient to justify the claims made in the paper. I think this is a quite important part of the algorithm which should be described in detail, since both the empirical results and the analysis (Assumption 1.3) rely on it.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; This paper studies the behavior of a three layer neural network trained by a projected SGD algorithm. Therefore, the generalization error is bounded by a minimum of many RKHS norms, and is proven to be smaller than the single NTK norm. For target functions in the NTK, the proposed bound is no bigger than the NTK bound. The paper studies the optimization and generalization of a three layer neural network trained by a projected SGD algorithm. In this setting, the network runs beyond the NTK regime. Specifically, the generalization error is bounded by a complexity depending on the minimum of a family of RKHS norms.<|endoftext|>The main result is introduction of adaptive generalization bounds that are defined for a modified SGD algorithm. When it is used in eq.9 it is not clear what is the number of intermediate features and how it is defined. This can be easily avoid by ~ or \hat symbols. Since I m not an expert in this, I ll defer to the other reviewers to check the significance and correctness of this paper.<|endoftext|>This paper makes contributions into two main category:1. 2.In terms of generalization, it proposes a new generalization bound with data dependent complexity measure that goes beyond NTK regime. Another generalization bound, based on a new function norm (minimum RKHS norm w.r.t.a family of kernels) is proposed. The results obtained here, especially on the proposed function norm (minimum RKHS norm) is interesting. It would be better if the authors can provide further comments on the key difference between the data dependent complexity measure bound and the function norm based bound.<|endoftext|>Related to this point, I believe that the discussion on adaptivity of the kernel is a little misleading. The paper provides novel results analyzing optimization and generalization of three layer neural networks. However, the kernel cannot be chosen freely, but from some restricted family of kernels. However, it is not clear to me why the new complexity measure offered in the paper improves over the NTK.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; This paper proposes a new algorithm for goal conditioned hierarchical reinforcement learning that is able to succeed in tasks with sparse rewards (differently from most other methods in the field). It does so through two innovations: (1) a representation learning procedure that is more stable, and (2) a exploration strategy that takes into consideration not only novelty but also reachability. The exploration strategy to sample goals to be visited is also novel. * *Importantly, in the ablations, were the parameters of the ablated methods tuned?<|endoftext|>1.This paper investigates learning stable subgoals within a deep hierarchical reinforcement learning setup. How does the asymptotic performance look like for this method? What is the maximum achievable reward for these tasks? This paper presents an interesting and novel idea at the intersection of deep HRL, novelty based exploration and reachability. The clarity of the paper can also be improved to more directly address the need and important of stability regularization. To handle this, a potential measure for subgoals is proposed which regularizes the novelty measure. The baselines also seem reasonable. The closest baseline in terms of using directional goal vectors is Feudal Networks. Currently it is hard coded to be c. What are the implications of this? It would have been good to more clearly understand the relationship between these measures. 3.Figure 4 is the main quantitative figure. 4.The qualitative analysis on the effects of the interaction between potential and novelty measure is quite sparse.<|endoftext|>The authors propose a hierarchical RL algorithm which augments an existing contrastive learning based subgoal representation objective with heuristics for exploration. Furthermore, the authors propose exploration heuristics that encourages the learner to explore in promising areas of latent space by combining count based novelty and potential measures. Comparisons between the proposed method and other hierarchical methods demonstrate that the algorithm results in better performance. The motivation on this point was not really explained in detail. Overall, I vote for a weak accept. The ideas in the paper are interesting, and the experimental evaluation is thorough and demonstrates the benefits of the proposed algorithm.<|endoftext|>The paper studies Goal conditioned Hierarchical RL (GCHRL) and proposes a new algorithm called Hierarchical Exploration approach with Stable Subgoal representation learning (HESS) to improve the stability of subgoal representation learning and strengthen the exploration at high level. Extensive experiments are conducted in a few MuJoCo environments with sparse reward, demonstrating the superiority of proposed algorithm and the effectiveness of different ingredients. This paper proposes effective active exploration for high level exploration of GCHRL. I vote for a borderline acceptation after discussing with the authors. I appreciate the combination of novelty and potential which properly takes novelty and reachability into consideration for an effective exploration selection. At a first glance, the representation regularization seems to be disconnected to the active exploration method. Later, I found that the stable representation learned is important to the effectiveness of novelty calculation. &nbsp;Besides, I have a few questions on the experiments.
Accept (Poster); rating score: 8; rating score: 5; rating score: 5; This paper proposes a new deep learning architecture called ST GNN that learns representations on graphs that evolve over time. This paper proposes a novel architecture based on graph shift, time shift and space time shift operators (also proposed by the authors in the paper) to handle time varying graphs. The theoretical analysis of this paper is very thorough and clearly elucidates the stability of their NN architecture.<|endoftext|>This paper introduces a space time convolution operator that is robust to graph perturbations and small variations of sampling rates. 3.Stable to small variations in graphs and sampling rates. 2.Lack of time complexity analysis. Although this paper theoretically proves the stability of the proposed convolution operator, it does not provide experimental evidence about the superiority of the proposed method against other baseline methods. Given this paper is not the first to study time varying graph signals, I think baseline comparison is necessary. Based on the above consideration, I think this paper is not ready for acceptance.<|endoftext|>This paper introduces a new spatio temporal Graph Neural Network, ST GNN, for making predictions on temporal network. The paper further proves that under practical conditions their ST GNN with Integral Lipschitz filters is stale to small perturbation in both time and graph domain. 3.I like the analysis of ST GNN’s stability to perturbations. I find it new and insightful, and especially interesting to read, though I am not an expert in this so I may be biased as well. Alternatively, the authors could be more explicit in restricting ST GNN s application domain.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 5; This paper presents a new method (as far as I know) of growing neural networks. Strengths:The work provides a novel, practical approach to growing networks. Its motivation and derivation are presented (mostly) clearly. This is an expression that decreases as the norm of the gradient increases."<|endoftext|>I will increase my score to 6. **Pros:**+ This paper is well written and neat. The motivation and the method are clear.<|endoftext|>This paper proposes a weight initialization method when growing neural networks. [Strengths]  The idea of the proposed weight initialization method, maximizing the gradient norm of the new weights, is intuitively reasonable.<|endoftext|>The paper proposes a method to grow neural networks by adding neurons in a specific way. On top of this, this could be turned around and part of the method: How can we add new neurons such that we change/improve the backward pass in way that improves the gradient norm of the whole network e.g.by backward computation that circumvents vanishing gradients.
Accept (Oral); rating score: 8; rating score: 8; rating score: 10; The paper proposes a diffusion probabilistic model based voice conversion method for one shot voice conversion scenario. Furthermore, to improve the real time factor, a novel stochastic differential equations solver is proposed which makes the diffusion model faster. The proposed solver is also suitable for other generative tasks. Strengths:(1) A diffusion probabilistic model based voice conversion method has been proposed for the one shot voice conversion scenario. Based on the above main review (especially the strengths of the paper), the proposed method is novel enough and the experiments can well support the effectiveness of the proposed method.<|endoftext|>Intuitions behind proof are also summarized. I think a lot more work could be done in exploring diffusion probability models on the lines of this paper, in regards to numerical integrators and formulation setup. It is rather remarkable how much progress has been made in the last few years in speech modeling. The main novelty in this paper is claimed to be in the decoder setup solving the reverse SDE.<|endoftext|>The authors propose a dedicated architecture able to generalize to unseen speakers naturally (without relying on Phoetic Posteriorgrams (PPGs) as in previous works) by conditioning the diffusion model on "average phoneme" spectrograms together with the target speech. This paper is well written and remarkable on many aspects:  A novel SDE solver than can be of interest for anyone interested in Diffusion models is proposed.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 8; rating score: 8; The authors propose a framework, which they refer to as GraphANGEL, for inductive relation prediction in heterogeneous graphs. The direction of the edges is extremely important in the benchmark graphs since it has explicit meaning. At least for real world knowledge graph completion tasks, the nodes can definitely be far away, with the hop distance (shortest path length) of the order of the diameter of the training graph. 3) The monotonic increase in the performance reported in section 5.3 clearly demonstrates the importance of the number of the subgraphs that are used. It will be interesting to see some empirical work for that in the future. Can they also provide some statistics on the number of 3 cycles and 4 cycles generated for the benchmark graphs? 5) Can the authors also elaborate whether they have considered the graphs as directed or undirected?<|endoftext|>The paper proposes GraphANGEL, a relation prediction framework that checks if subgraphs containing the node pair for predicting relation are similar to other subgraphs containing the particular relation. The framework considers heterogeneous graph based recommendation as well as knowledge graph completion. 2.Paper is mostly explained well and easy to follow. What about the graphs with noise since the majority of the real world graphs have errors or noise? 4.The points mentioned under limitations in the paper are quite important. Questions to authors: 1. 2.Did you consider cases where there are rare relations?<|endoftext|>The core idea of the approach is to compare a target subgraph (containing the nodes s and t between which the relation r is to be predicted) with other subgraphs in the graph that supports/refute the presence of the relationship. I think that these additions will make this paper more impactful. Considering specifically pairs, 3 cycles, and 4 cycles helps to keep the approach tractable. For instance, r1_unseen (e.g.basedin), r2_unseen(e.g.live) are 2 unseen relations observed during inference. But this may not be the case for heterogeneous recommendation settings, where I would expect that the graph to evolve very fast.<|endoftext|>The method works by first storing base patterns (limited to pairs, 3 cycles and 4 cycles to manage complexity) and building three sets of subgraphs on top of each of these base patterns, *target*, *supporting* and *refuting* patterns for a given triplet *<s, r, t>*. The final prediction is derived by calculating the distance between embeddings for *target* and *supporting* (denoted as *s+*) and, *target* and *refuting* (denoted as *s *) and feeding these numbers to a neural network, treating the final relation prediction problem as a binary classification problem. 2.While the gains in inductive setting are decent, the gains for knowledge graph completion and heterogeneous graph recommendation tasks are marginal. The experimentation done in the paper is extensive and shows the effectiveness of the method. Overall I feel the authors make a significant contribution to the relation prediction literature and I would like to recommend **accepting** this work.<|endoftext|>The paper proposes a novel inductive method for relation prediction over heterogeneous graphs. The method extracts supporting and refuting subgraphs to judge whether a relation exists between two nodes. The method is generalized to unseen relations. Experiments on graph recommendation and link prediction demonstrated the effectiveness of the method. The method combines the logic/subgraph patterns and graph representation learning together to build an explainable reasoning method for relation prediction, such that it can be generalized to unseen relations. The authors also pointed out this in the paper. I think the constraints of logical patterns are too strong. after rebuttal  The authors  response addressed my concerns. I would raise the score to 8.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; would that be a problem? here, it might be instructive to compare to the work of Schwaller et al.e.g.https://iopscience.iop.org/article/10.1088/2632 2153/abc81d/metahttps://www.nature.com/articles/s42256 020 00284 win this context it also make sense to compare to reaction fingerprints such as the one by Probst and Schneiderhttps://chemrxiv.org/engage/chemrxiv/article details/60e358fb379e8d3ba9f92d15https://pubs.acs.org/doi/abs/10.1021/ci5006614In my opinion, with these additional experiments the paper would stronger.<|endoftext|>The paper proposes a molecule representation learning method which is guided by chemical reactions. In particular, it leverages chemical reaction equations by forcing the sum of reactant embeddings and the sum of product embed  dings to be equal for each chemical equation. This idea is simple and useful, sharing the spirit of Word2Vec and TransE. In particular, the paper presents several fail cases and tries to explain them. This paper is insightful, clearly written and presents good empirical results.<|endoftext|>This paper proposes to use chemical reactions to pretrain molecular graph representations by 1) considering the reactants and products to be positive samples in the contrastive learning 2) pretraining using USPTO 479k reaction dataset. This is unreasonable as the position of a functional group will influence their molecular properties significantly. The idea of using chemical reactions to perform contrastive learning for molecular representation is novel, which offers alternative solutions to the problem by leveraging domain knowledge from the synthetic perspective.<|endoftext|>They do so by forcing the sum of reactant embeddings and the sum of product embeddings to be equal for each chemical equation. The strength: the idea of the paper using the reaction to embed molecules in a space where the summation of reactant embeddings is equal to the summation of product embeddings is very interesting.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; rating score: 6; This work introduces a new semi supervised learning problem that has more real world applications. The paper is well written and easy to follow. The proposed method and most of the baselines have utilized SimCLR pretrained weights. **Questions and Comments**  The results reported in Table 2, and 5 demonstrates that the accuracy on novel classes is higher than that of seen classes for the CIFAR 10 dataset. In parallel to the zero margin and fixed margin experiments have the authors conducted any experiments by decaying the margin for CE loss with a predefined schedule (linear, sigmoid, cosine, etc)?<|endoftext|>Although the paper has some weaknesses such as comparison in the conventional SSL setting, the paper proposes a new problem that considers a more realistic situation and the proposed method seems to work properly for the designed setting. The main function of the ORCA is balancing intra class variation between seen and novel classes with 3 kinds of loss terms. This paper contains a wide range of experiments to support the authors  claims.<|endoftext|>This paper studies a new setting for open world semi supervised learning. This setting extends the typical semi supervised learning by considering unseens classes in the test set. The results are evaluated on multiple datasets. Overall, this paper is well written. The idea of adaptive margin and estimating intra class variance using uncertainty is novel to me. The authors explained that this shows ORCA ZM is not able to reduce intra class variance. I recommend accepting this paper for publication if the authors could address a few issues described above.<|endoftext|>The proposed method can avoid the model assigning samples of novel classes to the seen classes. Pros:+ This paper is well written and easy to follow. + A new setting is proposed in the community of semi supervised learning, which jointly considers semi supervised learning, open set learning, and novel class discovery. + A simple but effective approach, Uncertainty Based Adaptive Margin, is proposed to address the introduced setting. It can avoid the model quickly converge the seen classes so that the model can well recognize both seen and unseen classes. If not, the authors should compare the proposed method with different values of the negative margin. For example, the proposed method may meet problems when learning on the large ImageNet which has 1000 classes.<|endoftext|>This is achieved by introducing pairwise loss and regularizing with prior class distribution. Strength:This paper proposed a new setting for SSL. Weakness:The expected number of novel classes is a strong assumption for open world SSL. I would like to see more details. Regularizing the average posterior w.r.t.a prior distribution is somehow a strong assumption in that the distribution for unseen classes are known in advance. As a result, the pair wise objective might heavily rely on the pre training. This paper provides a new perspective into SSL.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; * In Appendix B, the authors provided a complexity analysis of the algorithm, however, I think they should also include a comparison with respect to the standard SGD applied the non convex training problem and some baselines e.g., (Pilanci & Ergen, 2020) also proposed a convex approach with a polynomial complexity in the problem parameters. ICML 2020I think that the main contribution of this paper is incremental due to the following reasons. Do you need to solve additional optimization problems? Pilanci M, Ergen T. Neural networks are convex regularizers: Exact polynomial time convex optimization formulations for two layer networks.<|endoftext|>This paper considers the problem of learning the parameters of a two layer ReLU network with a residual unit: given samples of the form $(x_i, y_i)$, with $y_i   B^*[(A^* x_i)^+ + x_i]$, the authors provide an algorithm that provably learns $A^*$ and $B^*$, as the sample size grows. However, I guess that also the vanilla linear regression is a consistent algorithm, so proving just consistency appears to be a rather low bar. WEAKNESSES:(1) The whole paper (starting from the vanilla linear regression to the non parametric learning of the two layers) is based on the fact that $A^*$ has positive entries.<|endoftext|>This paper gives an algorithm for learning two layer neural networks with ReLU activation (realizable case). Strengths: The proposed techniques are simple and lead to easy to implement algorithms. I would have liked if the authors had done some experiments regarding this. Understanding theoretical properties of neural networks is a major unsolved problem in learning theory. Considering that, this paper makes an important progress using simple algorithms and techniques.<|endoftext|>In this paper, the author describes an algorithm that learns a two layer residual unit/network with ReLU activation in a teacher student setup, under the assumption that the first layer weights are nonnegative (without specific assumption on the data distribution), and shows that the student network is able to recover the teacher parameters in the infinite data limit (Theorem 6.2 and 6.3). The paper contains some interesting ideas that would be of interest in the study of more involved neural network models.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; The paper proposes a method to learn meaningful emergent languages with a continuous communication channel. The paper explores a two agent communication problem where the Sender agent sends a continuous waveform to the Listener who is tasked to reconstruct the original concept vector. However, there are many important details missing in the main text. Moreover, many relevant are not cited in this work (for example the above works). I would encourage the authors to do a thorough literature survey of the recent work done in the space of compositionality and generalization in emergent languages.<|endoftext|>This paper tackles the well established signalling emergent language game with the innovation of using a continuous, audio based channel for communication as opposed to the more typical discrete, symbolic channel. The analysis of compositionality given is insufficient. ## RecommendationI recommend rejecting the paper. While I appreciate the environment introduced and believe it to be promising, the experiments presented fail to highlight the novelty of the environment. Thus, the experiment could answer questions such as:      Does compositionality emerge at the same rate in continuous  and discrete channel games? multi concept and noise:    The noise you add to the channel has a number of different components; there should be an ablation study to illustrate the effects of these components. In other words, I see that the structure of the environment is unique, but what are the consequences which this structural difference begets?<|endoftext|>The paper claims to be an advance on Gao, but Gao addresses the problem of segmenting a speech stream into words when the listener does not have a pronouncing dictionary. Here the use of the continuous channel appears to me to be an unnecessarily complicated way of adding channel noise to what is otherwise a discrete system. There are many things that I do not understand about this paper. Wouldn t it be better to allow variable length and perhaps use the reward to encourage brevity. What is the statistical significance of the numbers in this table? If the Listener only has to consider a vocabulary of 50 words, there are only 2500 two word combinations which should be tractable for a DTW style recogniser with pruning. The experimental design appears to involve some unnecessary approximations.<|endoftext|>#### 4.2 Unconstrainted communication of single conceptsI realize at this point that somehow I missed what are the actual input data. On the whole, 4.2 seems like a superfluous debugging section, which doesn t really provide any information/signal, and could be omitted. The output is a prediction of the original input concept. noisy communication channel being needed for compositionality and/or generalization has been shown in a few previous works. I feel that this work as it stands is an excellent start in this direction. However, I feel that the experiments and analysis could be extended somewhat, into scenarios where it feels somewhat less predictable what the outcomes will be. Why use deep Q learning? ### 2.EnvironmentUsing a pre trained text to speech synthesizer seems to go against the goals of  emergent  communication?
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 6; To this purpose Firth bias reduction is proposed to be used. However it came with some drawbacks:  when using L2 regularization, the regularization could be non optimal. This results is also generalized to the context of using a cosine classifier on the features. Extensive experiments are carried out to show the benefits of this bias reduction on SOTA approaches and with various backbone architectures.<|endoftext|>This work introduces a method to reduce the bias of MLE for FSL problems. Authors developed the bias reduction formulation based on the Firth bias reduction method and extended it for cosine classifier as well. Would be very useful if authors can provide some insights about this. I believe the proposed method is novel and interesting.<|endoftext|>The submission examines the impact of Maximum Likelihood Estimation (MLE) s small sample bias in the few shot classification setting. The authors point out that while asymptotically consistent, MLE is biased in the finite data regime, and propose to apply Firth bias reduction to few shot classifiers to counter this bias. Firth bias reduction is shown to outperform other regularization approaches such as L2 regularization and label smoothing alternatives (confidence penalty and unigram label smoothing). I enjoyed reading the submission. The proposed idea is theoretically grounded and the experimental results are convincing. The submission is clear, well written, and it explores a simple idea with rigor.<|endoftext|>This paper tackles the few shot learning problem, which is a hot topic in the representation learning field. Specifically, the authors aim to train accurate classifiers using a small number of samples – Maximum Likelihood Estimators are biased for them. This paper has multiple advantages. This is the first paper addressing the effectiveness of Firth bias reduction in few shot learning. Vast empirical study validates the proposed method consistently improves the performance of the baseline method, and no performance penalty is introduced. Comprehensive discussion about other regularization techniques is provided. I also have several questions and concerns about this paper. Bias reduction of maximum likelihood estimates.
Reject; rating score: 3; rating score: 5; rating score: 5; The model is trained end to end to maximize the flow imbalance score (or its variants). **Strengths**Overall, the paper is well written. Usually, we start with a real, practical problem at hand, and then we try and develop a new method to solve it, if existing methods do not yield satisfying results. The experiments in Section 4 do not demonstrate the practical importance of clustering with respect to flow imbalances. More about empirical evaluations: The authors use 80% of all nodes for training.<|endoftext|>This paper introduces DIGRAC, a novel GNN approach for clustering based on the edges that are not present in a digraph. The architecture achieves this by finding node embeddings for digraphs that directly maximizes flow imbalance between clusters. The paper presents empirical results on both real and synthetic datasets, showing the effectiveness of the proposed approach. I am also wondering whether more can be said about when DIGRAC s model for clustering is appropriate for a dataset and when it is not. Apologies if I have misunderstood the empirical results, if so, I would like to better understand them. An interesting paper with a self supervised novel architecture for node clustering using an objective based on missing edges in a digraph.<|endoftext|>The paper introduces DIGRAC, an end to end model for clustering in directed graphs. Then, a GNN model is used to obtain the probabilistic assignment matrix that defines clusters. Strengths:  The paper provides an interesting pipeline for clustering in directed graphs. In particular, the idea of modeling the imbalance flow is very useful. Weaknesses:  The theoretical contributions of the paper are somewhat vague at some points, and overall could be enhanced. It would also be important to clearly present the constraints of the optimization problem.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 6; This paper proposes a novel framework for training flow models named Autoregressive Quantile Flows (AQF). The proposed method utilizes a new objective by evaluating forecasts with proper scoring rules, including the continuous ranked probability score and the check score. The strengths of this article are as follows:1. 3.It is also interesting to see that the proposed quantile flows could also provide the distribution estimation of model outputs, which can be applied to provide uncertainty of predictions.<|endoftext|>The paper proposes an interesting approach for estimating densities and providing uncertainty estimates for these densities. The paper extends normalizing flows in several ways: (1) by using objective functions based on proper scoring; (2) by using autoregressive quantile flows; and (3) by defining quantile flow regression. The generalization to quantile flows is well motivated and well described. The analyses of the UCI datasets are comprehensive.<|endoftext|>This paper proposed a quantile regression method for uncertainty estimation based on autoregressive quantile flow. The flow model can be trained in both forward and reverse setting using different loss functions, and the quantile flow framework can be combined with other linear or non linear transformations. It is also extendable to various different flow transformations and applicable to a wide variety of regression problems.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; This paper aims to explain why there is a generalization gap between Adam and gradient descent in learning neural networks, even with proper regularization. Based on this dataset, the authors prove that Adam is more likely to fit noise in data, while GD tends to fit real features. The theoretical results are verified by experiments on the proposed dataset. It would be interesting to see if some of the results still hold with weaker assumptions. Secondly, it seems the convolution part of the two layer model is only introduced to impose weight sharing, such that the feature patch and the noise patch have to compete for the same set of weights, which is not necessarily the case in practice. Thirdly, in practice, the generalization gap is mostly observed between Adam and SGD rather than GD, and it is well known that SGD usually generalizes better than GD; therefore making directly comparisons between Adam and SGD would be more convincing.<|endoftext|>This paper propose a non convex optimization problem where the batch version of Adam has worse generalization than GD. Note that the second layer weights are not learnable and fixed to 1. Adam will focus on the second patch which has lower signal, while GD will focus on the first patch and ignore the noise. The authors provide experimental verification of this fact. If it were learnable, then setting a negative weight on the second patch would remove the issue that the dimension with the answer has different signs in the two patches. The result is for batch GD and Adam. Notice that for the stochastic version, we wouldn t have such a strong difference between GD and Adam, because most of the time the gradient would be zero for the noise coordinates, and when they are seen, Adam would rescale by 1/sqrt(n) rather than 1/n with the batch version.<|endoftext|>The paper makes interesting observations about generalization properties of GD vs ADAM on a few settings. Strengths  Global convergence results for full batch ADAM and GD in non convex setting when proper regularization is applied. Weakness  While all their observations are interesting in their own right, to me, the main weakness of the paper is its limited scope (2 layer conv net, full batch ADAM instead of online setting), some of which the authors also acknowledge in Section 7. Since ADAM performs *better* than SGD in when used with transformer/lstms in language setting, a more complete analysis would be to also consider a data distribution/model where that can be explained as well. This paper makes interesting and novel observations about generalization gap between adam and GD.<|endoftext|>The authors also provide analysis for convex functions and some experimental results to validate their claims. The proposed analysis of the comparison between GD and Adam is, as far as I am concerned, novel. I have to admit that the settings in this paper are so novel that it looks a bit strange to me. Although I understand that it is hard to analyze neural networks, especially when the authors are trying to compare GD with Adam, which is notoriously known to be unnecessarily fancy, I still have the following questions/concerns. I do not see where the technical difficulty is. Although I find this paper to be novel and interesting, I have to admit that some of the settings in the paper look strange to me.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 5; The authors claim that in non iid federated learning setups models experience catastrophic forgetting of previously encountered data. Personalized Federated Learning with Gaussian Processes. These datasets are used along with the original data of the client to update the local model parameters. Strengths:  The problem presented in the paper is well motivated and the proposed solution for handling it looks promising. Generating the pseudo datasets at the client using the global model is a good idea and avoids potential private information sharing. Overall I think that this paper could be a nice contribution. The paper is technically correct. The results of the method are strong compared to the baseline methods examined. Full experimental details and code were provided. Why did you choose to generate the pseudo datasets via adversarial training? For example, how did you choose the hyper parameters based on the grid search? Did you have a validation set?<|endoftext|>The paper addresses the problem of federated learning, they claim the main issue is catastrophic forgetting, which they solve by generating a synthetic dataset per client. Strength: Empirical evaluations show strong results, in several experiments  Idea is novelWeaknesses:  My main issue is with the writing of this paper. 1 but should compare to homogenous data where the forgetting shouldn t happen  Also, they show good results also on homogeneous data (CIFAR 10 uniform) which again points at something else as the reason why their method works  It is also unclear why the synthetic dataset helps with catastrophic forgetting. The algorithm seems to work well, but I am skeptical about the alleged problem it solves and that it really addresses it. Writing is the main issue.<|endoftext|>This paper aims to alleviate forgetting in federated learning on non iid data. The method proposed FedReg focuses on regularizing locally trained parameters with the loss on generated pseudo data, which are based on adversarial examples of the global model in the previous step and the adversarial examples of the local model at current step. It shows improved performance and less forgetting during the training. Strength:(1) Very interesting and novel idea: it is the first paper I have read about showing adversarial examples of global model can help to prevent forgetting in local models. (2) good result comparing with other methods.<|endoftext|>This paper considers the catastrophic forgetting issue in federated learning. The authors observe that this issue is (at least partially) responsible for slow convergence of existing FL methods when the data are not independently and identically distributed (non i.i.d.) [3] Yuan, Honglin, et al."What Do We Mean by Generalization in Federated Learning?." This paper proposes FedReg, an algorithm to alleviate the catastrophic forgetting issue by regularizing the local model parameters on the generated pseudo data. This paper is clearly written and it is easy to follow. It is not clear why catastrophic forgetting could occur in federated learning as well. It would be helpful to precisely describe what “previous training data” means in FL. 4.The assumption (2) in Section 3 is highly related to the assumption that different clients should have similar examples (considering the perturbation). I am wondering how practical this assumption (2) is when the data is non i.i.d.across different clients. 5.The experiments showed that the proposed method converges faster than the baseline methods in terms of the number of rounds.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; This paper proposes a quasi newton method for neural network optimization. I would like the authors to explain why. Is it an operator? 5.Some of the experimental results are not convincing enough.<|endoftext|>The paper proposes to use a Symmetric Rank 1 (SR1) quasi Newton approach to approximate the Hessian and  to use an Adaptive Regularized Cubics (ARC) with an adaptive norm framework. To assess the performance of the new method, numerical experiments using autoencoders and feed forward neural network models are supplied.<|endoftext|>Numerical simulations are provided comparing the performance of the proposed algorithm to SGD, adaptive methods such as Adam and a naive L BFGS implementation. Therefore I recommend rejection. I am however open to update my score if the authors or other reviewers address my concerns. Cons:  The novelty of this paper is very limited. The experimental results are confusing and inconsistent.<|endoftext|>The algorithm is applied to training deep neural networks for image classification and autoencoding, and compared to L BFGS and various SGD variants. Second, the authors never specify the activation function they are using in their experiments. If the algorithm is the main contribution of the paper, a statement of these results would be nice to have. In the results, please do say more about the architecture (activation functions in particular).
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; This paper focuses on solving minimax optimization using Anderson mixing. The authors tried to use  Anderson mixing for the minimax optimization problem. They show that for the bilinear case, this algorithm converges to the optimal stationary point. Finally, the authors simulated their method on synthetic examples as well as image generation on CIFAR10. This is necessary to show the effectiveness of the method for GAN related applications(e.g., the one in section 6.2). 3)Author motivates the subject from a theoretical and practical point of view. Post rebuttal :I have read the authors response, and I increasemy score, but they should add the result on general convex concave problem and non concavesetting and the discussion about it in the final version.<|endoftext|>The authors propose to use the Anderson Acceleration on min max problem. They show two theoretical results: convergence rate on bilinear problems when using simultaneous gradient decent ascent, and convergence rate on bilinear problems when using alternating gradient decent ascent. Finally, they present numerical results in favor of their approach. *** Post rebuttal ***After a discussion with the authors, I updated my score to accept. They can be seen as an automatic way to find the optimal combination of the previous iterates. This result covers all the theoretical work presented in this paper. Moreover, I have a concern about a detail in the proof of Theorem 2.<|endoftext|>This paper proposes a new optimization method for min max optimization (That could actually be generalized to any variational inequality problem) based on Anderson Mixing. They eventually try their algorithm on toy bilnear problems and GANs. The theoretical results are sound   The method seems to scale to GANs (when combined with Adam)## Weaknesses:   The comparison with the related work could be improved (see my detailed comment)I think the authors could discuss more the rates obtained in Thm 4.2 and Thm 4.1 and the standard rates obtained in the literature (e.g.the ones presented in Remark 4.2). Also, one could compute the Taylor expansion of $T_p(1 + x)$ when $x\to 0$ to have an idea of the convergence speed. Also, I have some concerns regarding the theory and the comparison with respect to the baselines. (for instance, if everything is clarified I will increase the technical and empirical novelty score as well as the final score) For now, I recommend accepting this paper.<|endoftext|>This paper views the gradient descent ascent method as a fixed point iteration and solves it using Anderson Mixing to converge to the local minimax. They show theoretically that the algorithm can achieve global convergence for bilinear problems under mildconditions. Some numerical experiments have been conducted. The authors only discuss the convergence results for the simple bilinear games problems. They do not discuss the general nonconvex nonconcave problem or the convex nonconcave problem. The algorithms developed in this paper seem incremental.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 3; The paper proposes two losses for meta learning based few shot learning. The first loss is a triplet loss where the positive and negative anchors are replaced by class prototypes (averages of class members from the train set of each episode). * experiments, (1) miniImageNet only is not enough; (2) the baselines are not sota, please go to the PapersWithCode website and see real sota methods for miniImageNet   the reported results for the proposed methods are far below real sota on miniImageNetThe paper as it stands has major drawbacks in novelty and experimental validation, unfortunately in current form it will probably not be interesting to the general audience.<|endoftext|>This paper proposes a new model that takes part of the meta learning approaches based on metric learning. More specifically, this paper modifies triplet loss to make it more suitable for few shot learning task and proposes ICNN Loss to increase the inter class distance and reduce the intra class distance. This paper lacks comparisons with the methods after 2019. 2.The results of the experiment are not sufficient. This paper only provides the results on the mini ImageNet. 4.Since the motivations of proposed loss functions are the same, this article should further analyze the applicable conditions of Proto Triplet Loss and ICNN Loss. In addition, the applicable conditions of the two losses should be further analyzed.<|endoftext|>This paper applies two losses to the few shot learning model based on metric learning (similar to ProtoNet), which aims to utilize the intra  and inter class distances. The first one is based on the original triplet loss and adjusted for the prototype network. The novelty of this paper is incremental. It seems that the authors only apply two existing losses to the existing prototype network. 3.The experimental results on only one dataset are not enough to demonstrate the generalization.<|endoftext|>This paper discusses the few shot learning based on metric learning. In order to better measure the distance between samples of different classes, the authors propose two new metric loss terms considering both inter class and intra class distances of samples. The first metric loss is called Proto Triplet Loss, which improves on the traditional Triplet Loss. The motivation of this paper is clear.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; The proposed method is validated with leading performances and efficiencies on multiple datasets across multiple tasks. How sensitive is the performance of the proposed method to them? However, in the practitioner’s point of view, reducing inference time may be more critical. Also, the authors validate the proposed method across not only multiple datasets but also multiple tasks.<|endoftext|>The parameters are mapped into parameter groups through a preliminary training step and k mean cluster the layers. The method is tested in Low Budget and High Budget regimes and on different tasks. Adding pseudo code or algorithm would also improve clarity of the method. Figure 1. shows comparison between distillation and pruning which seems to be a different class of methods.<|endoftext|>The main concerns lie in the real application value and real benefit of the methods. 4.Is the proposed method specified to tasks? However, how about the real performance on the hardware platforms, e.g., bandwidth, memory consumption, training speed?<|endoftext|>StrengthsI commend the authors on extensive evaluation across multiple datasets, models, and tasks are convincing of the broad applicability of this approach. WeaknessesThe practical impact of this approach is overclaimed in this paper. are designed to have lower inference time, which SSN does not achieve. * In the discussion of results, acknowledging that these other methods reduce FLOps and training time, whereas the NPAS approach does not. The proposed method is methodologically an interesting contribution, however the practical impacts on training efficiency and model efficiency are minimal and overstated, which limits my recommendation.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; The main result states that the gradient flow training dynamics provably converges to a "maximum margin classifier". I feel that these results make the line of work on the convex formulation more complete, and will serve to further encourage the line of work analyzing neural networks based on some hidden form of convexity.<|endoftext|>The authors then analyze the dual extreme points of the convex formulation and show the implicit regularization of unregularized gradient flow as convex regularization. The flow of the section could be improved by providing additional information about which role they play in the overall proofs. The paper then demonstrates that this is the case under some conditions on spike free matrices and orthogonal separable data.<|endoftext|>The geometry discussion of the neural gradient flow is clear, especially based on Figures 1 4.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; rating score: 5; The paper formulate a general form of federated adversarial learning (FAL), based on which the authors theoretically analyze the convergence of FAL. * The authors have clarified most assumptions and conclusions. Specifically, how the problem is different from traditional federated with distributional shift [A] if the adversarial noise is simply treated as a bounded noise? * The technical implications of the main theorem (4.1) are not elaborated, either in Section 4 or later. The authors should try to clarify and establish the novelty of the problem contradicting related robust federated learning.<|endoftext|>I would like to see these concerns can be addressed by the authors  feedback. I tend to vote for a rejection due to the lack of novelty, clear theoretical justifications and experiments. What are the gaps that this paper addresses?<|endoftext|>This paper is a direct follow up of Zhang et al 2020b. These assumptions are easy adaptations from Zhang et al 2020b. However, after reading the author response my opinion remains the same, as the authors acknowledge that this work is an extension of Zhang et al 2020b and there is no experiment after the revision. Weakness: one of my major concerns is the novelty against Zhang et al 2020b. Given the above novelty concern and the lack of experiments, I would recommend rejection.<|endoftext|>As the authors stated, this FL gradient is a new gradient they use to tackle the difference between global and local updates. Weakness:1\  As the authors claimed in the paper, their analysis is not tight. 2\ Their technical contributions, despite rigorous analysis, is limited:a. b.Many of their proofs are built upon previous results. Overall I think the author proposes a useful framework with rigorous analysis but I am somehow concerned with the technical novelty of this paper.<|endoftext|>Minor:1.The two reasons in the abstract seem to be not consistent with the "involved challenges" in the latter of the introduction, are they refer to the same focus? Specifically, the authors provide a framework for analyzing federated adversarial training and present the convergence analysis in the over parameterized regime, the main results theoretically show that the minimal value of loss function in this learning paradigm can converge to $\epsilon$ small under certain circumstances. This paper provides convergence analysis for federated adversarial training, which theoretically shows the feasibility of applying adversarial training to federated learning.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 5; rating score: 6; My concern about the claimed "speed up" is still present, but the authors have stated why they decided to use FLOPs rather than actual runtime on a given hardware and implementation. The proposed approach is compared with fully Dense MoEs and the recent Switch MoE baselines in a language modeling task. The paper proposes a simple and easy to implement idea. Based on all this, I m slightly increasing my score for this submission. There are some results from the "effect of balanced loads" analysis (Section 4.2.4) which are also not clear to this reviewer, as well. In addition, notice that one of the main practical reasons to use a balanced load (e.g.in Switch MoE) is to efficiently use all available compute, which is not considered in the analysis presented. "Need We analyze it in token level s weights?". The main claim of the paper (the current approach of jointly training experts and the sparse gates introduce a negative impact on model accuracy) is not well supported by the experiments. Actual runtime comparison (on a particual hardware architecture) would be much better. Given all these issues, I recommend to not accept the paper.<|endoftext|>This paper studied the problem of training the gating network for the mixture of experts based model architectures. To make the gating network training more stable and robust, the authors proposed a dense to sparse gating training algorithm that uses Gumbel noise and temperature tuning. StrengthsThe proposed scheme to train dense gating then making it sparser makes sense and could potentially stabilize and improve the training of gating networks. Stabilizing the training of the sparse gating network has been the main challenge of the MoE based model architectures, there exists some research with similar ideas trying to make the training of MoE more robust. This paper uses a binary encoding and diffienable operators to smooth the learning of sparse gating, with initializing to be dense and later converging to sparse. Overall, I think the training scheme from dense to sparse for sparse MoE gating makes sense, and this is supported by experiments in the paper. However, I also think the technical novelty of this paper is limited given existing work with similar ideas on improving the gating network training.<|endoftext|>The paper proposes an algorithm, DTS Gate, to warm up experts in an MoE learning setup. Experiments and ablations in language tasks are provided to support the advantages of the algorithm. This paper tackles an interesting aspect of conditional computation: in the context of MoEs, how do we start training from scratch when individual experts can easily break down and collapse? The paper provides an intuitive answer: we can start by training all experts together (i.e.as a dense model), and relax this over time, hopefully, once all individual experts are enough developed. In some setups (which, interestingly, coincide with some of the original motivation ones for MoEs) this may not be possible, for example if the number of experts is just huge. Some recent work has shown the effect of increasing "k" a lot (see [1], Figure 10). Overall, I think the experiments in the paper do not show the proposed algorithm offers any advantage over standard top k routing (Switch, in the paper). For any FLOPs budget (vertical line), Switch offers better performance than DTS. Similarly, for any attainable performance level, Switch gets there at a lower FLOPs cost. Thus, I m not sure how the paper justifies the use of DTS. I may have misunderstood something though. For Figure 6, as expert id s are independent across layers, it would be nicer to sort experts per row according to their load, so that clearer patterns emerge. Have the authors tried this?<|endoftext|>Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks, JMLR 2021, https://www.jmlr.org/papers/v22/21 0366.htmlWhile there are some clear advantages highlighted by this paper in obtaining more efficient transformer models, overall, I believe that the paper is not ready yet for publication. This paper introduces the idea of using dense to sparse (DTS) training gates in mixture of experts (MoE) based on a gradual sparsification process. **Strong Points:**  using gradual sparsification for the MoE gates of a transformer model seems to offer a final performance (perplexity in this particular case) close to the MoE dense gates, while having lower computational requirements. At least in theory, as I believe that a binary mask is used to emulate sparsity. If the proposed method is particularly designed for transformer models then perhaps vision transformers shall be also studied. In parallel, sparse to sparse training has been studied for vision transformer in [2]. Please add a consistent paragraph to discuss the above directions and to highlight clearly what is novel in this paper in comparison with those works. Otherwise, the proposed method, broadly speaking, would be just a simple application of [1] (or follow ups) on MoE gates for transformers. Can you please add an algorithm to better illustrate the proposed method? How can one choose it? Can you train all models from Figure 3a for the same number of iterations?<|endoftext|>This work proposes the Dense to Sparse Gate or DTS Gate. It’s a simple idea of starting training with soft (continuous) routing to all experts and then gradually reducing to hard (discrete) routing. If this is the case, I will improve from a 6 >8. The authors claim that this improves the quality of traditional approaches. Notes:* The idea is an interesting one. If the quality of sparse models can be significantly improved through using more compute only early on in training, this could become a widely adopted technique. * Reasonably well written paper. Why are there periodic dips in the training perplexity curves? If this is from repeating the data in the same order, I’d recommend in the future that you shuffle data each epoch. * “experiments results show that the FLOPs efficiency can be optimized into time efficiency”  > Where was this shown? All the plots and data I see are for FLOPs, not wallclock time on a specific hardware. Also, as a nit, I’d avoid subjective claims like in using the word “amazing”. I’m positively disposed towards this work, however, my main concern is on the lack of a clear demonstration of the magnitude and the significance of the gains.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; Authors propose to solve an order fulfillment problem with imitation learning backed by GNNs. The second strength of the paper is the practicality of the problem addressed. Therefore, I expect the proposed method to generalize well for practical problems in the real world. I encourage authors to include more discussion on what versions of fulfillment problems are popular in which applications, and how representative the given problem formulation is. One notable weakness of this paper is the experimental setup. Some of the technical claims are questionable.<|endoftext|>Overall, the paper presents a relatively straightforward application of an existing idea (graph neural network with supervised learning). The problem is clearly and formally specified through a mathematical formulation  The model is clearly explained on the right level of abstraction for the reader familiar with graph neural networks  Experiments illustrate potentially good results, achieving good quality solutions in time orders of magnitude faster than MIP solving, and with better quality than a heuristic. Some claims are unsupported or incorrect, e.g."The order fulfillment problem is one of the fundamental optimization problems in supply chain management" could at least use a reference, "tens of thousands times faster" does not accurately reflect the results (200 5000x).<|endoftext|>A Graph Neural Network (GNN) model is designed to enable the supervised learning of optimal solutions for an order fulfillment problem in supply chain management. This is a problem that must be solved in real time, making a GNN whose forward computations are quick an attractive option. Clarity: overall, the paper is well written and easy to read.<|endoftext|>This paper proposes a heuristic graph based machine learning model, which can be used to solve the class of order fulfillment problems. The cost gap is only 2   3% compared to the optimal solution, and the inference time is minimal, similar to previous heuristic approaches. I could not find the generative process. Is there any unmentioned pattern in the training dataset, or the problem instances can be arbitrary? Have the authors tried their model on some real world datasets? I think the idea of converting an order fulfillment problem to a tripartite graph, which is then fed to a GAT, is interesting.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; This paper proposes a method for learning an object centric symbolic representation of an environment that allows for planning. While the final approach itself is still limited, in the sense that it relies on several strong assumptions, it advances what symbolic approaches are currently capable of. Further, by highlighting the trade off between creating object types and problem specific grounding it contributes an interesting problem that may not yet be on everyone s radar. The main improvements in this paper compared to prior work of Konidaris et al.(2018) are1.<|endoftext|>This paper introduces a method for learning symbolic, object centric abstractions from object factored environment observations for long term planning tasks. It extends the symbolic representation learning framework by Konidaris et al.(2018) by factoring the state into objects, learning object type abstractions, and “lifting” the model to operate on these object centric, typed abstractions. The problem of learning sample efficient abstractions for long term planning is an important and difficult problem, and taking an object centric approach to address this problem is very timely and of high significance. The proposed method is largely based on a framework introduced in prior work (Konidaris et al., 2018), but introduces sufficient novelty in terms of integrating object centricity in an elegant way into this framework. The experimental evaluation is carried out on sufficiently complex environments to demonstrate that the method can solve tasks that are not trivial. Overall, I recommend “weak accept”.<|endoftext|>The authors proposed a learning framework to learn object centric abstractions for high level planning, yet it does not solve the fundamental limitation of the manually designed planning domain, and it is more of an autonomous labeling tool that partially alleviates human efforts. This paper proposes a method to automatically learn a PDDL like abstractions for high level problem solving based on the different states (either symbolic representation, or high dimensional embeddings) of the objects in the environment. Although the proposed method, to some extent, alleviates human efforts to define the planning domain manually, it has two main drawbacks that cannot solve the fundamental problem of the human defined planning domain. Such object centric abstractions are still more of task specific representation, even for the same objects in semantics.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; rating score: 5; This paper proposes a novel problem to embed query graphs with edge qualifiers to query a hyper relational knowledge graph, and a solution extending an existing approach for a non hyper relational knowledge graph. They also consider three baselines that can show the characteristics of the proposed approach, including reification baseline that transforms qualifiers to nodes. The overall performance shows the proposed approach shows benefit over the baselines, and usefulness when edge qualifiers are available. S1.They proposed the first query embedding approach for a graph with qualifiers. S2.The proposed approach is compared with three baselines including the reification baseline. S3.A new evaluation dataset is proposed. There is some very minor presentation issues that can be improved such as adding the best hyperparameters in the main paper, or adding the missing node color in the legend in Figure 4 or maybe using "Oracle " instead of just "Oracle" which can be a bit misleading. But otherwise, this paper is well written with a clear goal and contributions, and evaluations. But I don t think this is necessary to show the value of this paper.<|endoftext|>The paper presents an approach for embeddings queries over hyper relation graphs. In order to evaluate this approach, the work defines a new dataset on Wikidata because of reified triples/qualifiers. Most existing approaches are focused on triple based techniques, however ignoring complex but more expressive representations such as reification and/or qualifiers. My main concerns of this work is as follows:1. 2.EmQL evaluates faithfulness of the overall approach. Have the authors tried to do this? If there are qualitative analysis on this it would be great. I am not convinced that it is an apples to apples comparison to the other rows in the table. The paper is well written and addresses an important direction is handling more complex representations of knowledge graphs.<|endoftext|>In this paper, the authors propose a framework to learn the query embeddings for hyper relational KGs, which allows QA over hyper relational KGs with more complex questions and makes use of the qualifiers. They also introduce a new hyper relational KG QA dataset, WD50K QE, for the evaluation of their proposed method. Strength:  The paper is well motivated, and the authors propose a reasonable solution to the hyper relational KG QE problem. Experiments show that the proposed problem outperforms most of the previous methods that only use triple only graphs. The authors prepare a new dataset for the hyper relational KG QE problem. Weakness:  The description of their proposed method is not very clear. The authors should explain that “X” can be E, R, etc. Questions:  Could you clarify whether “StarE like” in table 2 is the same as “StarQE” in table 1?<|endoftext|>This paper studies how to embed and answer hyper relational conjunctive queries based on recent advancements in Graph Neural Networks and query embedding techniques. The challenge of multi hop logical reasoning is not clearly stated. The abstract of the experimental results is not specific enough. It is recommended to ensure the latestness of the literature when investigating related work, otherwise it is difficult to persuade the work to be novel. This paper propose a method to answer such queries and demonstrate in their experiments that qualifiers improve QA on a diverse set of query patterns. However, in the process of demonstration, there is a lack of description of the challenge and thinking about how to solve the problem. I think the authors needs to improve the writing.<|endoftext|>This paper studies the multi hop logical reasoning problem with hyper relational knowledge graphs. The authors constructed a new dataset, WD50K QE, and the experimental results show that StarQE can effectively model the qualifiers in the dataset. This paper proposed StarQE for question answering over hyper relational knowledge graphs. The task is well motivated and interesting. 2.The proposed model is simple but performs well on certain types of queries. 4.The paper is well written and easy to follow. Currently, only one dataset is used. It is not sure that if this method can also be used on other datasets or domains. While some existing methods [a][b][c] can handle such information in knowledge graph embeddings. I believe some improvements can be made to strengthen the paper.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; This paper considers the problem of learning a goal conditioned policy effectively. The downstream effects of this new architecture are validated experimentally fairly well. ## Main ReviewThis paper studies the structure of the function approximators used to learn goal conditioned policies. ### Strengths* The paper presents a valid hypothesis that a structure that separates the task specific information from the action specific effects should lead to better learning. * The experiments validate one part of the hypothesis by showcasing faster learning with the separated networks on a few of the domains tested on, as well as better transfer to goals in parts of the state space that the agent has not trained on. It also does not attempt to generalize and extract insights beyond what are applicable to the few domains that it evaluates on. * The paper mentions that the value network is decomposed into the bilinear networks the paper studies. * The discussion section seems like an extension of the experimental section. There is a variety of possible generalizations and implications of this work that should be discussed which are ignored in the discussion section for going over a few more experimental details. Perhaps it might be useful for discussion.<|endoftext|>The paper proposes a modification to goal oriented universal value functions that split the neural network into two parts. One network accepts states and actions and outputs a vector representation, while the other outputs a vector given a state and goal. Part of the contribution of the paper is the interpretation of these two vectors   the first captures the notion of what actions can be taken at a given state, while the second represents the direction the agent should move towards to achieve the goal from its current state. Empirical results on several simulated robotic domains demonstrate that the method outperforms other UVFA architectures, while ablations are provided to capture the most important aspects of the method. On the one hand, the approach is a slight modification to the kind of architecture proposed in Schaul et al (2015), which the paper acknowledges. The experiments were all well executed and showed improved empirical performance, and ablation studies are provided to help understand exactly which design decisions are responsible for improved performance. ***********************POST REBUTTAL***********************The newly added experiment in 4.5 strengthens the paper in my view, since it highlights the particularities of the method. As a simple example, we could imagine an unbounded 2D plane with some goal. \citep is used in the first sentence of Section 2.1 instead of \citet  4.<|endoftext|>This paper proposes a new decomposition for the universal value function that disentangles local and global components. The main contribution of the paper is proposed modification of the universal value function approximation implementation by making the goal dependent component also depend on the state so that now it is f(s,a)*\phi(s,g) instead of f(s,a)*\phi(g). Some details of the method implementation and experiments are missing. It seems to me that the decompositions into a global component that depends on goal and state and local that looks at state and action has been already used in the literature (maybe without stating it as the main contribution). How would the proposed method perform compared to such a baseline? Also, maybe other decompositions could be considered, such as f(s,a)*\phi(a,g)? The benefits of the method in terms of data efficiency and transfer to new goals are clear.<|endoftext|>The paper proposes a new approach to Universal Value Functions in Reinforcement Learning by decomposing monolithic universal value functions into two components and exploring different ways of combining the results of the decomposition. The paper shows that this leads to better sample efficiency and has desirable properties for generalization. PRO* well written and easy to follow* simple method with good motivation* good ablation studies and discussion for the different aspects of the proposed methodCONS* it s not discussed why the method performs worse or on par in some cases* only tests the method with DDPG and in two domains. Is it important that this is bilinear? The idea presented is quite simple but does show promising results in the experiments.
Reject; rating score: 5; rating score: 5; rating score: 6; My initial assessment is to not accept this paper. This is a super interesting question, and the answer this paper appears to give is "yes". 2.3.2: What is the motivation for this spatiotemporal transformation? 2.4: There are a lot of details here that are important to document, but distract from the main point of the paper.<|endoftext|>The paper also performs additional analysis to interpret the weights learned by the generator and discriminator networks, as well as assess the quality of the networks  reconstruction of the neural activity. It is also not clear how this analysis is generally applicable, or what insights can be gained if it were applied to neural data from a different task. It would be nice to see the pre  and post learning neural activity for precisely those neurons that the discriminator assigns attention to in Figure 4.  the description of the results was confusing, and many of the plots that the conclusions here rely on are in the supplementary section, making it hard for a reader to follow any reasoning based on these plots. Heatmaps in Figures 2 and 4 are hard to interpret without colourbars2. A lack of clarity in the methods and results section exacerbates this.<|endoftext|>This paper uses CycleGAN to map neuronal activities of mice (as measured by Calcium traces) pre  and post learning. Using paired synthetic data to show the effectiveness of applying CycleGAN. Essentially disentangling spatial and temporal information, modeling the spatial relationship as a graph with a learned adjacency matrix. In this way, neurons will be permutation invariant/equivariant, and the sorting is not needed, and the whole model can be learned end to end. Although the architecture or method novelty is not significant, and there is some unclarity of the writing, it can be a good starting point for further explorations.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; This paper follows a recent and interesting line of research on strategic approaches to GAN training in which the optimization problem of the players is lifted to the space of models employed by the discriminator and generator. Overall I appreciated the contribution, but consider it as a borderline. You work on a Banach space. The topological details would be quite important to see here. Is it important for the metric? In particular, there a several points the authors should take care of: There are several technical problems with the paper. •	Section 6: It would be nice to have some (perhaps speculative) discussion why GDA does perform worse than natural gradient in this example. Also, what would happen if the equilibrium is at the boundary? The key point is that the map $L(f,p) log(f)+p log(1 f)$ is convex concave. In general, the Lyapunov functions proposed here can also be found in that reference, as well as the invariance functions for matching pennies (and other zero sum games).<|endoftext|>The paper studied the dynamics of a natural gradient flow in hidden convex concave problems, and showed that the natural gradient flow attains global convergence to stationary points. Understanding nonconvex nonconcave games is pretty important, here the topic is pretty interesting. My main concerns are as follow:1. Compare with existing works. It would be better if authors add more discussion on the difference compared to GDA. Here authors provide some evidence in experiments. While here it is natural gradient flow, which requires a matrix inversion. Generally I think the paper is interesting, but it would be better if authors can provide more insights on it. I will appreciate the authors to address my confusions, and definitely reconsider my decision.<|endoftext|>The authors study a generalized Gradient Descent/Ascent flow in a class of non convex non concave games known as hidden convex concave (HCC) games. The paper focuses on the particularly interesting class of games known hidden convex concave games. As the authors point out, this class encompasses many important applications within the setting of min max optimization, including adversarial example games and GANs. First of all, on of the key points stressed by the authors is that they establish global convergence guarantees. There have been some techniques to address such issues in the literature (e.g.using sketching), so the authors should definitely consider addressing this intractability issue. Overall, the practical importance of the results is questionable. I would recommend clarifying further the results of Section 5 for the revised version. The importance of these results and a comparison with existing works appears can be further highlighted.<|endoftext|>This paper studied a non convex non concave Zero Sum game setting called Hidden Convex Concave game (HCC), as shown in Def. The observation is that although the min max objective is non convex non concave w.r.t.the parameters (e.g., weights of neural networks), it could be convex concave of the outputs/strategies of the players $F$ and $G$. The authors then connected NHG to replicator dynamics (RD) in literature, under a certain metric of geometry. strengths:The observation that a lot of practical problems including GANs and AEGs are HCC games is interesting and useful, which motivated the proposed NHG method in this work. The proposed NHG is guaranteed to converge to a Nash equilibrium in the HCC game setting and it can be used to explain cycling behaviour of RD by using a certain metric. 1.The proposed NHG method is a preconditioning gradient update such that $F$ and $G$ are updated following standard gradient descent ascent update. Overall, this paper makes an interesting observation about the HCC game setting and proposes a reasonable NHG method, reducing the non convex non concave problem to a convex concave problem. Therefore, I make a borderline decision for the current moment.
Accept (Poster); rating score: 6; rating score: 5; rating score: 5; The architecture is applied to ImageNet 1k classification, COCO detection, and ADE20k segmentation with good results relative to model size and performance. Strengths:* The approach is an interesting combination of existing ideas. * The presented experimental results are competitive and cover three tasks. Weaknesses* The novelty of the approach seems somewhat limited. I think the updated paper and the authors  comments have addressed some of the concerns that the reviewers have raised. An interesting combination of existing ideas evaluated on three computer vision tasks. * The main idea of the "axial shift" is not introduced with sufficient clarity in my opinion. "the first work to apply MLP based architecture to the downstream task"   There seems to be a strong relation to the recent CycleMLP, which may be concurrent work, but should nevertheless be cited and contrasted, I think.<|endoftext|>The proposed method performs much better than previous MLP based methods on ImageNet 1K and on par with Swin transformer. The paper is well written and presented clearly. 2.The proposed axial shift module is simple and elegant. 3.Experiments are done extensively beyond ImageNet. Limited technical novelty given the fact that the shift operation (CVPR 2018) was already presented as an alternative (though similar) operation to convolution. The shift ResNet is already spatial convolution free except the first stem layer. 3.Another intuitive baseline is simply to use depth wise convolution with a depth multiplier of 2, instead of the 2 parallel axial shift module. Possibly not. I think this paper is around the acceptance threshold, mainly due to the limited technical novelty besides applying Shift, an alternative to convolution, to MLP mixers, and achieving on par performance as previous transformer/convolution methods.<|endoftext|>Strengths:The work shows good results on different vision tasks. Especially, the proposed work is very similar to S2 MLP and, furthermore, the authors proposed even an improved version, S2 MLPV2, which reports even better results for 224x224 image resolution. However, when the shift size is (1,1)  there is no spatial interaction in the architecture (all the information exchange is between the channels at the same spatial location). The authors claimed that introducing locality into MLP based architecture “enables the model to obtain more local dependencies, thus improve the performance.” But the authors did not actually prove this very clearly. The novelty of the work is limited and the authors did not show the results when using a standard MLP approach on the spatial direction.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 5; The authors propose a novel way to learn new recommendation deterministic policies from feedback collected under different recommendation policies. To this end, the authors propose a novel way of re weighting and regularizing empirical risk that encourages the search for policies that have better reward (lower risk) and are still overlapping in evidence with the current policy. Strengths:* The subject of the paper is quite relevant and the identified issue of non overlapping support for RW recommendations serious* The application of existing methods from causal inference to deterministic recommendation policies is valuable and the empirical section is well developed and the results significant Weaknesses:* The motivation for the particular approach is not extremely clear. The authors build the argument around “transportability of patterns”, but I feel the point is never fully explained. If an action is never taken under a certain context in the past, (meaning is in the non overlapping set), the algorithm will have to extrapolate the outcome. In order for the extrapolation to hold some assumptions need to be made about the reward model, and I find the authors do not really spend time on this. Furthermore, I think borrowing either the counterfactual notation or the policy learning notation and vocabulary would help quite a lot in explaining the approach. Overall, I liked the paper and I found the approach interesting. I think the authors should link their approach a lot more to the work presented in [1] and also covered in two of their references [2,3]. I think by doing so, it clarifies that the main difference in the problem domain is that we are in control of which of the counterfactual worlds we want to compute the risk in. Otherwise, all of the vocabulary and the core explanations of the soundness of the approach remain the same. Generalization bounds and representation learning for estimation of potential outcomes and causal effects.<|endoftext|>This paper proposed a new perspective for recommender systems: rephrase optimizing recommendation as finding an intervention that best transports the patterns it learns from the observed domain to its intervention domain. To optimize the recommendation algorithms in this setting, the authors proposed a transportation constraint risk minimization objective and convert it to a two player minimax game. The paper views recommender systems from a new perspective, which is similar but different to existing works in causal collaborative filtering. The experimental studies are conducted in synthetic data, offline data and online data, which can well demonstrate the effectiveness of the proposed method. It seems to me that the proposed method can outperform IW methods when the overlap between the source and target domains are insufficient. However, there is no measurement about the insufficiency. 2.It is hard to directly compare the bounds derived by the paper, e.g., between Theorem 3 and Proposition 1. But the same method is not evaluated in the offline setting, which seems unreasonable. 4.The proposed method tries to optimize recommendation algorithms via a minimax game, which could be much more complicated than other baselines. Thus, it would be necessary to analyze the efficiency of the proposed method in the experiments. In summary, this paper targets at an interesting problem and shows promising theoretical and empirical results, which could be helpful for the community of recommender systems.<|endoftext|>In light of this, this paper aims to propose a novel method to optimize recommendation results by solving such insufficient overlapping problems. Specifically, a principled transportation constraint risk minimization objective function is devised to optimize the recommendation results, which is able to transport the learned patterns from the source domain to the intervention domain better. Extensive experiments were conducted on both real datasets and semi synthetic datasets to show the superiority of the proposed method. The theoretical proof is given to show the uncertainty of the results. 2.Generally speaking, the paper is well written and well presented. Though many mathematical theorems are contained in this paper, it is easy to follow for readers. 3.The proposed method is novel and can inspire future research in recommender systems. I am confused as 20 is pretty high, and maybe here is (0.20) or (.02). items should be unrelated items. Also, the same questions occurred in other parts. 2.\mu is also an important hyperparameter, and authors also mentioned that a suitable \mu will give us the opportunity to better explore the whole feedback data. I recommend that the parameter sensitivity of \mu should be conducted. 3.The main contribution is to address the insufficient overlapping problem that causes the uncertainty of IW and DA based methods. However, in the experiment part, IW and DA based methods are not compared with. 4.The motivation is somehow unclear and the presentation of introduction could be further improved. However, the motivation is not quite clear, and  more IW  and DA based methods are expected to compare in the experiment.<|endoftext|>A more detailed analysis of the time complexity would be desirable. The authors propose a new adversarial learning method based on transportation constraint risk minimization to address this challenge. Generally, the overall framework is novel. The authors give some theoretical analyses to the proposed transportation constraint risk minimization. My major concerns and questions are as follows:(1) The motivation is not clear to me. One of the claims of the paper is that the proposed algorithm can address the insufficient overlapping problem. However, the authors do not give a clear motivation of why this problem is important in real world RL systems and why the proposed methods can address this problem. Specifically,  Theorem 1 and Theorem 2 are only the finite sample bounds under $P$ but not the transfer generalization bound w.r.t the target distribution $Q$ like Proposition 1. I think this paper is potential. The proposed domain transportation strategy for the recommendation is quite interesting. Given above, I think the theoretical part is not well written and it would be great if the authors can provide some interpretations of the theoretical results. (3) The literature review is not very good, and the context is missing about how the research work stands in the agenda of each subfield such as Wasserstein Robust Optimization, Unbiased Recommendation, Off policy Learning and Causal Inference with Missing data. The authors also need to discuss the differences between this paper and other Distributionally robust optimization papers with Wasserstein distance in terms of the generalization bounds. It is difficult to figure out what s new and what s already a part of the existing literature. (4) The experimental comparison is also not sufficient. This paper lacks some baselines and results on benchmark datasets. The authors also need to evaluate their methods on Yahoo and Coat datasets which are the benchmark datasets for counterfactual learning in the recommendation.
Accept (Poster); rating score: 8; rating score: 5; rating score: 5; The paper provides theoretical results that favor MLE estimators, in terms of the excess square loss risk,  compared to empirical risk estimators under mild assumption. The theoretical novelty of the paper lies in the fact that it is the first work that determines the performance of a MLE in terms of the performance of existing estimators using finite samples, as opposed to asymptomatic convergence to the true value. (5) Moreover, I think it would be useful if the authors could summarize in a Table n and d for the datasets considered.<|endoftext|>This paper promotes the maximum likelihood estimation over target metric optimization. One disadvantage of MLE is the likelihood is different to determine and it is often misspecified in practice. The paper discusses this point and recommend using a mixture of distributions. Here are some examples. Due to these issues, I was not able to fully understand and appropriately evaluate the significance of the theoretical results, and I have reflect this in my confidence level. I am not clear how to interpret the main result. I am not sure if it is true that the proof for discrete distributions can be easily extended to continuous distributions. Minor issues:    "w.p" should be "w.p."<|endoftext|>This paper compares two inferential methods for regression models, the maximum likelihood estimation and the estimation based on loss functions. The point distribution and negative binomial distribution are discrete distributions, while Pareto distribution is a continuous distribution. I am worried from the discussion in Section 5 that this approach is used for the maximum likelihood estimation of the mixture distribution. There should be more explanation about the maximum likelihood estimation of this mixture model. Since the two estimators are obtained through different loss functions, I think it is difficult to compare the goodness of the estimators through the single quantity such as the quantity (1). Or is there any special meaning to use the quantity (1) to evaluate the MLE and TMO? **Other Comments:**(g) I am confused about the MLE for regression models used in the paper. (h) p.2, Section 1, Competitiveness of MLE, l.2 up: MLE can competitive   > MLE can be competitive(i) p.3, Section 3, Notation, ll.1 2 up: the sphere centered at ...  > the ball centered at ... (I think this expression is less confusing.) Some new theoretical results are presented on this topic.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; The paper tries to relate multiple unsupervised NLP embedding models to the problem of stable coloring. The writing of the paper can be improved. The authors repetitively claim that they "define a mathematical framework that defines continuous stable coloring on graphs and develops optimization problems to search for them", but there is no validating algorithm at all. From a theoretical point of view, how could the proposed mathematical framework help us better understand NLP embedding models? Is G_{DH} a weighted graph? This is not properly typed. Section A.6: "with the parameters mentioned in the table ??"<|endoftext|>In the paper the authors define a new problem called the continuous stable coloring (CSC). The authors then provide an objective function that could be minimized to obtain a CSC. However, this connection (while not presented as CSC) has been known. Further, my main issue with the paper is that the connection between techniques and word2vec has been made at a surface level in function space. Further, the more interesting question in deep learning and the usage of neural networks is not the connection in function space, but the connection in parameter space. However, the connection could bear fruit if the authors do provide deeper connections either in parameter space or provide results on the properties of the embedding learned. DOI:https://doi.org/10.1145/3375395.33876412) **Missing Details**I also think the paper is missing details. While this is not too big a deal, it is an issue for a theoretical paper.<|endoftext|>It is also not clear and does not read well. Major comments   One of the main issues with this paper is that writing is not clear. The paper is pretty difficult to read. Overall, the paper seems to be proposing an interesting contribution. However, I have several concerns about this work. The writing is not clear, while the significance of the theoretical results seems to be low. The authors provide no discussion on potential applications of the proposed methodology, while it is not clear whether it can be useful for domains in which there is already a wealth of embedding approaches such as in natural language processing or in graph representation learning. It would strengthen a lot the paper if the authors could apply the proposed methodology to some real world problem.<|endoftext|>In this paper the authors proposed continuous stable coloring (CSC) as a new framework to unify the understanding of several existing unsupervised learning algorithms, including Word2Vec, BERT, and Node2Vec. The authors also show how to reduce the existing approaches to CSC. This paper provides a novel perspective for understanding unsupervised learning algorithms, which could inspire new research directions. However, the theory of this paper is not proven rigorously and it lacks new method or empirical results. Typos and Grammars:   As mentioned before the graph embedding architectures captures capture...   $\mathcal{C}(\mathcal{N} (u))   C(\mathcal{N} (v)$   $\mathcal{C}(\mathcal{N}_l (u))   C(\mathcal{N}_l (v)$   Firstly, describe the construction of the graph for a particular domain and sub graph induction based on the sample in the data. ???(missing citations)the paper is inspiring but major revision is needed
Reject; rating score: 1; rating score: 3; rating score: 5; rating score: 5; The paper is a survey of methods in evidential deep learning. I wish this paper had some deeper insights on  this matter. It would have been different if the paper, in addition to being a survey, presented some fundamental insights into the nature of these methods, which could count as a novel contribution. I think survey papers are not suitable for conferences as they need to be evaluated using a different set of criteria.<|endoftext|>Having said that the paper touches the important topic of making a new model family more accessible to the audience, I am afraid I do not think it does it in a way that would add sufficient value on top of one reading the material from the original papers. This is a survey paper without any conceptual claims of novelty. Under these conditions, I am not able to recommend an accept for this paper in its current shape. The paper classifies evidential deep learning approaches into categories and points out the strengths and weaknesses of each category.<|endoftext|>**Strengths**:* In general, the paper is well written, structured, and easy to understand. * There is significant value in surveying evidential deep learning techniques as there has been some parallel work in this space that is not often connected. With improvements to the broader insights and takeaways from the survey as well as paper presentation, this work will become a good contribution to the academic community. I am missing further insight and takeaways beyond the existing collection of papers surveyed. **Suggestions for improvement**:* My main recommendation is to work into the survey further insights and context from beyond the surveyed papers.<|endoftext|>This paper surveys a collection of existing works that the author frames as evidential deep learning.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; This would not be a concern if the method was a useful tool for drawing even more insights into how models work, but this is not fleshed out in the experiments. The authors show this can also be used to explicitly encourage specific frequency sensitivities, which they call “spatial frequency regularization”. However, the paper neither justifies its practical applicability, nor does it develop the methods into a fleshed out model analysis to yield insightful conclusions. This lowers my confidence that the method has practical applicability.<|endoftext|>As the result, the empirical results seem weak. The presentation of the main idea can also be improved. In this case, the authors might be able to argue that their method is good enough since AugMix is a very computation heavy method specially designed on the benchmarks used in the paper (with evidence that AugMix is not that good for Table 1), but the authors seem to prefer to present the paper in a way that they seemingly want to compare to AugMix where AugMix is designed with their own method inspired by something else.<|endoftext|>This paper proposes a novel spatial frequency regularization technique that improves the robustness of training neural networks against superficial fourier statistics in a dataset. There do seem to be slight performance gaps from the SOTA AugMix method in image corruptions. However, the method is considered to be simpler than AugMix and still outperforms other baselines in many cases. The results in the robustness against the fourier filtering are convincing. Is AugMix not applicable in this setting?<|endoftext|>The authors only show results on CIFAR10/100, but other baseline methods (e.g., AugMix) show at least some ImageNet results. Based on this measure, the authors propose a family of spatial frequency regularization techniques to suppress the model’s sensitivities to certain spatial frequencies. It is also generic to be applied to any differentiable model. My major concern lies in the insufficient experiments and unconvincing results as stated in the weakness section.
Reject; rating score: 5; rating score: 6; rating score: 6; The submission introduces a generalization bound for Prototypical Networks that does not depend on assumptions on the class conditional distributions. This bound decreases as the variance in norm of the feature vectors decreases, and the authors empirically investigate five feature transformation approaches that are meant to lower this variance: L2 normalization (L2 norm), variance normalization, Linear Discriminant Analysis (LDA), Embedding Space Transformation (EST), and EST+L2 norm. In terms of significance, I would argue that "comparable with ProtoNet and the fine tuning approach" is not that high a bar. Baseline and Baseline++ with no additional feature transformation and a prototypical classifier on top of the extracted features. I would say however that I am not entirely sold on a prototypical classifier having more desirable properties than a linear classifier or an SVM (like is done with MetaOptNet (Lee et al., 2019)). The topic studied in the paper is relevant to the few shot learning research community, and the prospect of applying prototypical classifiers to a wider range of pre trained feature extractors is appealing. It would flow better if it was consistent in its verb tense use. The main issue I have with the submission is clarity. Since the drawbacks of Cao et al.s bound are discussed in this work, it would be good to show the bound in the submission. Is it because they need to be applied during pre training? If so, it should be made clearer. The same can be said about FC100 vs CIFAR FS. Is it possible that Baseline++ was not pre trained properly in their re implementation?<|endoftext|>The theoretical analysis of the variance of the norm to impact the bound of the prototypical classifier is interesting. The feature is trained with cross entropy loss with a linear projection layer. This paper derives a novel generalization bound for the prototypical network and shows that focusing on the variance of the norm of a feature vector can improve performance. The proposed upper bound is a modification of (Cao et al., 2020), and it does not require the features distributed on any specific distribution. [Strengths]  It is interesting to focus on the different loss functions in feature training (softmax loss) and testing (Prototype classifier). L2 normalization is well known to improve performance. The paper explains how the variance of the norm is related to the bound of a prototype classifier. The L2 N is applied only to EST. The discussion of Sec.4.4 is not completely validated. These values before and after the normalization also should be compared. However, it seems that the variance and mean of Gaussian distribution are used when expanding the results of one sided Chebyshev s inequality, eg., Eq.(22). P.6  The authors wrote that “Regarding equation 9, the ratio of the Euclidean distance between the class mean vectors and the between class variance is supported to be constant.” Why can it be supported to be constant? The performance is not completely the same as fine tuning nor meta learning.<|endoftext|>In this paper, the authors have derived a generalization bound for the prototypical networks, which provides some insights towards improving protonets without finetuning using simply normalizing the feature vectors and reducing the variance of the norm. To justify the claim, the authors experimented on several feature transformation methods on standard few shot benchmarks. The interesting aspects of the paper are as follows:1. The theoretical analysis just relax the assumption of having particular class distribution (in both class distribution and class covariance matrix), but the overall approach is marginally novel compared to Cao et al.2.The theoretical results derived in this paper are quite intuitive though, e.g., L2 normalization of features is effective. These are quite well known tricks. I was looking for more interesting insights which are non trivial. 3.Variance normalization performs worse   can you connect this with the theoretical analysis performed here? Several feature transformation methods are evaluated, however, there is no guarantee that the chosen feature transformation method is optimal. Overall the theoretical analysis is interesting, however the novelty seems to be marginal. The insights provided in the paper are quite intuitive  and proper connection with theory and experiments need to be explained better. I would like to see how the authors response the raised issues.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; This paper addressed an interesting problem of action anticipation. To this end, the authors proposed a unified recurrence model that generates the graph representation of the video sequence. Extensive experimental results have shown the effectiveness of the proposed method. The method section is not well structured. ECCV2020The overall idea of using message passing for constructing graph representation for video action anticipation.<|endoftext|>1) The main novelty of the paper seems to be the proposal of a method for action anticipation based on a graph representation and message passing. > this sentence is also incomplete.<|endoftext|>The paper is not ready for publication in its current state; however, additional ablation studies, experiments on additional datasets, and removing unnecessary sections can significantly increase the paper s chances for publication. 2) The input graphs are fully connected, but affinity matrix (or edge embeddings) influence the message passing function.<|endoftext|>Paper proposes a novel architecture for video action anticipation task. The proposed method used a graph representation via message passing. 2.Method is clearly and methodically described. 3.Experimental results are strong on the challenging EK55 dataset. While it is an important task in vision, it is not clear if it is of interests to the wider audience in the ICLR community. I recommend to accept this paper as the technical contributions are quite strong. However, its appeal to the wider audience in ICLR is a minor concern.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; This paper proposed AdaMomentum, which uses the EMA (exponential moving average) of the square of EMA of gradient as sthe denominator, while Adam uses the EMA of the square of the gradient. The authors also tried to provide theoretical analysis. The authors try to understand both the convergence and generalization of the algorithm, and the experiments also cover a wide range of neural network architectures and tasks. The authors need to consider this in the statement and experiments. Theory:1.Fig1 as a demonstration of the idea does not make sense to me. I m using Jax with the AdaBelief in Flax, all default hyper params. So the ImageNet results look unconvincing to me. [1] Zhou, Pan, et al."Towards theoretically understanding why sgd generalizes better than adam in deep learning." arXiv preprint arXiv:2010.05627 (2020). 4.The current version of AdaMomentum scales lr up by a factor of $1/(1 \beta_1^t)$ (see comment Experiment 1 below for an example), which contradicts findings in RAdam that warmup from a small lr to a large lr would stabilize the training. I wonder what s the authors  comment on this? First I don t think the equation above Sec3.2 is correct, it should be $v_t^\prime v_t + \epsilon/ (1 \beta_2^t)$, as $t\to \infty$, $v^\prime \to v_t + \epsilon$.<|endoftext|>This manuscript proposes to substitute the gradient in the second moment estimation term with the momentumized version, and show that it improves both optimization and generalization. Some theory is presented, and there are also some promising deep learning experiments on different tasks. Strengths: the empirical results are good for language modeling, and it is comparable with SGD with momentum on vision tasks. 3.In Theorem 1, the authors require the iterates to be bounded. 4.In Theorem 2, the authors assume $\alpha_t/\sqrt{v_t}\geq \alpha_{t+1}/\sqrt{v_{t+1}}$ to prove the convergence in non convex case, which is problematic. For example, [Reddi et al., ICLR 2018] provided a non convergence analysis of Adam and they showed that this inequality does not hold at all. The generalization analysis using SDE comes from [Zhou et al.NeurIPS 2020], and the optimization analysis is based on a problematic assumption pointed out by [Reddi et al.ICLR 2018]. There are also some unrealistic assumptions (e.g., Assumption 5) which needs to be further justified.<|endoftext|>In this work, the authors proposed an improved version of Adam by using a momentumized version of the second moment. The authors test the proposed methods in several deep learning tasks and show the improvement over some existing methods. The technical contribution seems to be reasonable. The empirical evaluations show that the proposed method is robust. Readers may not fully understand the statement made in the section Benefits of substiuting g with m.### 2. The authors should tell readers if the analysis gives a better bound compared to Adam or other variants. ### 3.General non convex optimization (related to Sec 5.1.1)Adam and many diagonal adaptive gradient methods do not work well for general non convex functions. For example, Ackley function and  Rosenbrock function are multivariate (d>1) objective functions. In Sec 5.1.1., the authors only consider the simplest case when d 2. The paper is in general well organized. However, as pointed out by other reviewers, the empirical evaluations are not fair and the technical statement could be incorrect.<|endoftext|>Nevertheless,  they discover that substituting the gradient in the second moment estimation term with the momentumized version in ADAM can well solve the weaknesses of ADAM which  generalize worse than stochastic gradient descent (SGD) and tend to be trapped in local minima at an early stage during training.Their intuition is that gradient with momentum contains more accurate directional information and therefore its second moment estimation is a better choice for scaling than that of the raw gradient. The paper is well written. 2.The intuition of estimating  second moment by momentum  is interesting. 3.Theoretical results and experimental result is provided. The assumption in Theorem 1 seems wired, where it requires the   distance between any θt generated by AdaMomentum is bounded. I am not sure whether this assumption is a widely accepted assumption or it is a special assumption required by the proposed algorithm. 2.The assumption in Theorem 2 seems has the same confusion, where it requires alpha_t/sqrt(v_t) > alpha_(t+1)/sqrt(v_(t+1)). It will be much better if the authors can clarify this in their paper. The intuition, theory, and experiments seems OK.
Reject; rating score: 3; rating score: 3; rating score: 6; rating score: 6; In particular, the paper investigates using a vanilla transformer as well as a more customized variant called  Simulation Transformer . In the abstract the sentence  most existing particle based simulators adopt graph convolutional networks to model the underlying physics of particles  seems off. Using transformers for solving physics is an interesting research direction. (2) With 12 pages the Appendix provides a significant amount of additional material, some of which can be considered quite relevant for the main contribution of the paper (e.g.Table 4).This indicates that the contribution of this paper may better be discussed in a format that has less restrictive page constraints. There are reasons why papers have page limits and I have the impression that for this work the authors are trying to circumvent them by providing a lot of material in the Appendix. that have all not been introduced up to this point and it is not clear what the contributions of the proposed paper are compared to existing concepts. Therefore, I don t think the paper is ready for publication. S3.1  The goal of a simulator is to learn a model ... In other literature simulators are commonly not described to  learn a model . Why?It would help to add a few more details here.<|endoftext|>The paper studies the use of transformers for particle based physics simulation. The main contributions are as follows. Since the authors are proposing a new neural network layer, it would be more useful if, rather than compare to different baselines each with their original hyper parameters from their papers, etc, the authors took the approach of integrating the different parts of the newly proposed layer into existing architectures (possibly including non simulation settings), and try to understand better that way how the new layer may help in a more apples to apples comparison.<|endoftext|>In this paper, authors propose Simulation Transformer (SiT), a transformer based approach for particle based fluid simulations (in contrast to all the existing approaches which are overwhelmingly based on Graph Convolutional Networks). Authors evaluate the proposed SiT model on diverse environments and compare against several state of the art fluid simulation models and also demonstrate generalization across different materials. Paper is well written, well organized, and narrative is coherent. 3.Specifically, an interesting contribution of the paper is the use of explicit (disentangled) "abstract tokens" that encode material specific properties. The authors demonstrate (through multi material simulations like a solid cube floating in a fluid) that the SiT method is able to leverage the material specific properties learned in the "abstract tokens" to better simulate the properties of the solid interacting with the fluid. The authors have performed a rigorous comparison with state of the art models and the achieved improvements and the explanations rendered for the improvements are sensible.<|endoftext|>The paper proses the Simulation Transformer (SiT) to simulate particle dynamics, using the Tranformers  attention mechanism to attend to critical particle interactions. SiT is evaluated in four simulated environments: FluidFall, FluidShake, BoxBath, RiceGrip, though it would be nice to see environments with more non liquid multi object interactions. SiT provides a novel Transformer based architecture for predicting particle dynamics that outperforms previous graph convolutional and Transformer based architectures.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; In this paper, the author investigates whether importance weighting is incompatible with the training of overparameterized neural networks. In contrast to the recent observation that  importance weighting is ineffective in current deep learning paradigm (Byrd & Lipton 2019), the authors show that it could actually be helpful using polynomially tailed losses. The proposed polynomially tailed loss could be a very practical tool for importance weighting. Both theoretical and empirical results are provided to corroborate the claim.<|endoftext|>Previous works pointed out an interesting incompatibility between importance sampling and training overparameterized neural nets. Thus the choice of $\alpha$ cannot be arbitrary, and a polynomially tailed loss with a high degree can perform as badly as the exponential loss. The theoretical analysis is only for linear classification. Although this paper is mainly a theoretical paper, the authors do claim that polynomially tailed loss has practical value, and the current experiment results are not strong enough to support this claim.<|endoftext|>The authors showed that in contrast to the prior works that showed importance weighting is ineffective due to implicit bias of converging to max margin, switching to poly tailed loss circumvents this issue, both theoretically and empirically. Pros:Overall, this paper is very well written and the theoretical results are sound. Cons:There are some obvious limitations, for example, the gap between linear classifiers and neural networks used in practice, which is a minor issue due to the intractability of analysis.<|endoftext|>The paper shows both theoretically and experimentally that importance weighting is not incompatible  with the training of overparameterized models provided that the training loss is appropriately modified so that it does not have an exponential tail. Experimental results are limited to a comparison between weighted cross entropy and the new loss. Strengths:  The paper is well written and enjoyable to read  The theoretical claims are sound and well presented  Theorem 4.2 is interesting: It extends the analysis by Chatterji & Long to imbalanced setting and the finding about the scaling of the weight being cubic in \tau is interesting. Specifically, for binary classification the authors propose a new loss function with polynomial tail decay.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; The authors thoroughly show that this inductive bias is a property of the parameterization of neural networks, and so remains resilient to changes in optimizer or initialization distribution. What kind of trend does it have? To make claims about low rank solutions, it is quite important that the authors actually illustrate what goes on with the true rank, or even say numerical rank (which thresholds at some level). This is backed by various experiments and ablations, and is in itself convincing. (Note: I am not against linear over parameterization as I also mention in the  Pros  above, over here the concern is the claim about the benefit of low rank inductive bias of depth.) (d) In all the comparisons, are the different networks at a comparable loss value? Related work: Overall, this is covered nicely in the paper, but I do still have some minor comments.<|endoftext|>The results does not extend to network architectures with skip connections such as ResNets. The authors claim that this low rank bias of the embeddings help generalization on natural datasets, and show that a reparametrization of common architectures where the linear layers are replaced by products of linear layers improves test accuracy on CIFAR 10 and CIFAR 100. The paper is well written and the claims are clear. However I m not sure that the main claim is not trivial, after all, when matrices are multiplied, rank can either decrease or stay the same. The connection to Solomonoff s theory of universal induction that the authors make seems far fetched: in the Solomonoff ensemble shorter programs consistent with the data are preferred, while in deep learning deeper neural networks, which are longer programs, are preferred, which suggests that neural networks operate far from the idealized regime of Solomonoff induction.<|endoftext|>The paper argues that modern deep neural networks are effective in part because they incorporate a bias towards low rank Gram matrices (where the Gram matrix is the product of the features with themselves). By this point, we know quite alot about the biases towards low or high rank in finite and infinite neural networks, and the paper lacks alot of this nuance. 2.Infinite neural networks don t have a bias towards lower rank as they get deeper. 3.Low rank fixed kernels aren t particularly likely to improve performance (which is why the lack of low rank kernels in deep infinite neural networks is a good thing). This is much more accurately described as just representation learning [3]. In particular, work comparing finite and infinite networks would lead us to expect a simpler top layer kernel for a single, fixed, random initialisation of the weights. Why bigger is not always better: on finite and infinite neural networks.<|endoftext|>This paper conduct several controlled experiments and arrive at the following results:  Deep networks (both linear and non linear) are biased towards learning low rank embeddings at initialization. Strengths:  The paper is very well written and I enjoyed reading it. It starts with a conjecture and then validates that through experiments. I liked it very much that the authors investigate that whether the bias is due to initialization or the training procedure itself. This paper also does a good job at proving an overview of previous related work. I believe it is not the loss but the accuracy. On page 3, the paper says: *Since the dimensionality of the Gram matrix depends on the dataset size, we can compare neural networks with different modeling capacities in the zero training error regime. * \I found this a bit confusing. I believe the reason is that "it does **not depend** on the number of parameter".
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; This work proposes to explain an RL policy by clustering states into meta states and presenting strategic state(s) for each meta state. This clustering is performed based on policy rollouts, balancing likelihood of paths within a meta state and number of paths from states within the meta state. SSX is "local" only in the sense that, for larger domains, approximations are made to address scalability issues. The authors present a new way (SSX) to create a certain style of explanation, but the style itself is not novel. This is not acknowledged as a requirement/shortcoming of SSX.<|endoftext|>The paper presents an explanation method for deep reinforcement learning. Strength: (1) The paper proposes a new explanation method against deep reinforcement learningWeakness: (1) The utility of the explanation is vague and needs more details. It seems the method cannot handle exponentially large state spaces. I would like to see the proposed method applied in more sophisticated games (e.g., Pong and Mujoco games) in which the states cannot be numerated.<|endoftext|>This paper proposes a novel explanation method for DRL policies that clusters environment states into meta states and identifies strategic states from meta states as explanations. The paper empirically compares its proposed explanation method with some existing methods on three games and designs a user study to demonstrate its utility. Due to some unclear terminologies, the proposed technique is not clearly described. 4.It s not clear the problem space of the proposed technique or, in other words, what types of RL tasks are suitable for the proposed technique. AAMAS 2018This paper studies an important problem from an interesting perspective.<|endoftext|>The paper proposes an approach to interpret a black box control policy of a reinforcement learning (RL) problem such that its interpretations can be understood by a human user. I encourage authors to comment on this. Consider rewriting the sentence “… which act as intermediate goals for states belonging to a particular meta state”. Figure 2 is very confusing. My overall impression of the paper is positive, but I have several concerns. In this case, is not it enough to restrict the computation of Γ to such states? Moreover, the convergence results in Algorithm 1 should be presented when Γ is estimated through policy simulation and not when Γ  is known exactly (please see my comments above). Some theoretical or empirical studies are useful here.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; This paper seeks to answer whether modern DNNs are modular and proposes statistical methods to quantify modularity. Given this, the merits of using partitioning to analyze/interpret DNNs remains unclear. 3.The question of detecting modularity is a very interesting one  Critiques: 1.<|endoftext|>To this end, the authors propose to cluster the neurons of the network using spectral clustering applied to a graph that is weighted by similarity between the neurons.<|endoftext|>The aim of this work is important and surely relevant. However I have some doubts on the methodology that is proposed in this paper. 1) Construct an undirected graph from the nodes and weights of the trained network2) Divide the graph in groups using a spectral clustering algorithm3)  Define ‘subclusters’ as the sets of all nodes that are in the same cluster and in the same layer4) For each subcluster compute an *importance* score and a *coherence* score5) If the sub clusters are more important and coherent than randomly defined groups of nodes of the same size than the network is said to be ‘modular’. Detecting modularity in networks is surely an important and timely problem in deep learning.<|endoftext|>This paper uses graph based clustering methods to identify the humanly comprehensible modularities in the neural network. The methods and results are convincing and clear. 3.They also provide a statistical method to identify the significant modules automatically. And I assume that is the reason why the p value does not work. And using the impact to reflect the connections of the neurons.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 10; The paper explores the forgetting effect as a positive effect for learning. The hypothesis is empirically evaluated by improving iterative algorithms in two cases: image classification, and language emergence via the Lewis game. There are some impracticalities of this work, that I would appreciate if the authors can address. Strengths:* The ideas in this work are novel to the best of my knowledge. The work does not prove the hypothesis, which jeopardizes the generality of the method. In language, it has been proposed that a network could be pre trained for shorter sequences and later on with longer ones (e.g., [1]).<|endoftext|>This paper proposes a curious new perspective on forgetting that can have significant potential impact on the deep learning research in general. Since forget and relearn is a general framework, it can improve a broad range of deep learning applications: not only low resource image classification and language emergence, but a myriad of others including more practical vision tasks, nlp tasks, speech processing, RL applications and untold others. However, there are significant issues in the current version of the paper that prevent me from recommending acceptance.<|endoftext|># Interesting Idea, but needs major revision and additional results In this work, the authors introduced forgot and relearn framework to unify disparate existing iterative algorithms. Did you tune it? If not, reporting numbers on their benchmark would be handy. * I agree work is interesting and offers a unique direction for training DNNs, but more work is needed to back a few claims presented in this work. Suppose the hypothesis shows that forgetting and relearning is essential. It may be true in some scenarios, but not for all. MNIST, there are very few difficult examples; even those examples share most of the representation with other not so difficult examples.<|endoftext|>The authors articulate their view in the *forget and relearn* hypothesis and extensively test it through the paper. The results of the experiments in the entirely different scenario of language emergence give further evidence to the generality of the LLF approach. I would undoubtedly follow new works in this direction. I think there are many:  The accuracy gains persist with larger datasets such as Imagenet? This hypothesis is well supported by experiments and offers a straightforward method to reduce overfitting, effective in image classification and language emergence problems.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; This paper proposes the PI GNN (Pairwise Interactions in Graph Neural Network) model for semi supervised tasks for nodes with noisy labels. It employs a new auxiliary task to predict PI (pairwise interactions) labels using a pair of nodes. on various noise levels and settings. Technical novelty on the model might be overclaimed. "Iterative deep graph learning for graph neural networks: Better and robust node embeddings."<|endoftext|>This paper focuses on semi supervised node classification with noisy node labels. The authors propose a novel learning objective called the pairwise interactions, which encourages node pairs holding positive PI labels to have close node embeddings. It would be more convincing if the authors could further elaborate on that or provide some theoretical justification. Extensive experiments show promising results. The problem is well motivated. 3.It is better to consider some larger datasets. My major concern is on the novelty of the paper. However, the idea of modeling pairwise interactions is also not very new in the field of graph machine learning.<|endoftext|>This paper proposes a new GNN approach which they call PI GNN that aims to improve performance in presence of noise or corruption. I give a borderline evaluation for this paper. Ideally, a node pair has positive PI is more likely to have the same labels. More convincing results or a strong reason why PI GNN is a good candidate in similar tasks should make me improve the evaluation.<|endoftext|>Experimentally, the accuracies of the proposed method are higher than baselines in the case of noisy labels. Is it a semi supervised way? 2.How about the complexity, no analysis is given. DBLP and WikiCS datasets should be evaluated on ablation study. 2.The novelty is limited. Essentially, the proposed pair interactions belong to contrastive learning. 3.The experiments show better performance.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; The paper studied a challenging and important problem in efficient attention mechanisms. I think the overall method is novel and the experimental results are promising, which leads me to a positive rating. Results show that the proposed approach matches the state of the art with much fewer FLOPs and model parameters. Designing an efficient attention mechanism in Transformers for vision tasks is definitely an important problem.<|endoftext|>The paper proposes an efficient attention algorithm based on quadtree for vision transformers. Strength:+ Introducing the quadtree to the vision transformers is interesting and makes sense. It achieves competitive performance in various vision tasks.<|endoftext|>Empirical results also demonstrate that the proposed approach achieves good performance with less computation on many vision tasks. This paper proposes a new attention mechanism in vision transformers, which is called quadtree attention. 2.The quadtree attention can be used in both self attention and cross attention, which leads to a wider applicability. The design of pyramid tokens is unclear:    (1) In Figure 1, Top 2 patches are used to compute finer attention. In summary, I think the proposed quadtree attention is a reasonable efficient design of attention mechanism.<|endoftext|>Experiments are performed on several tasks, e.g.feature matching, image classification, and objection detection. Superior results are achieved on these tasks. From this point, the idea of quadtree b attention is very similar to [1]. The paper proposed an efficient way to process long distance and short distance attention. However, the authors only compared with the PVT. ## Questions:  How about the real runtime when applying quadtree attention to PVT architecture? However, by shifting the windows in the Swin Transform, it enables that information exchange between windows for Swin.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; This paper proposes the use of multi parameter persistence (an emergent research topic in topological data analysis) to capture latent time conditioned relations among nodes in a GNN. Also, the authors make it clear how this work differs from previous contributions. The paper is technically sound. This is a well written paper and generally well structured, making good use of appendices. Some statements are not adequately supported by empirical evidence.<|endoftext|>The paper introduces multipersistence into graph neural networks (GNNs) to render GNNs capable of finding hidden time conditioned patterns in spatio temporal graph data. The proposed model, Time Aware Multipersistence Spatio Supra Graph Convolutional Network (TAMP S2GCNets), outperforms other state of the arts methods on various spatio temporal graph data. So the inefficiency is a big problem of this paper.<|endoftext|>The authors suggest the use of multiparameter persistence for explicitly capturing he latent time dependencies in spatio temporal data. They propose the Time Aware Multipersistence Spatio Supra Graph Convolutional Network that uses the mutlipersistence as extracted by the Euler Poincare surface and allows for time aware learning. technically sound. Thorough experimental part. The motivation and is well stated and supported by also a very well presented methodology. Some details here would be nice. A very interesting paper that proposes an approach that I expect to be impactful for future research.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; The paper investigates if synthetic datasets obtained from implicit generative models can be used for representation learning in place of the original dataset. Strengths:  The problem of learning with only generated data is very relevant, and the authors achieve reasonably good results. The methods are very clearly exposed, the results are comprehensive and the experimental protocol is very detailed. This approach is novel and the empirical results support its importance. The paper is a step forward towards training only using synthetic data, a long standing goal that has been of interest since the introduction of deep generative models. With the rapid advances in representation learning, this submission providing a strong and comprehensive empirical study is likely to be valuable to the community.<|endoftext|>### Strengths  The paper is well written and easy to follow. While performance is **slightly inferior** to training with the full dataset, the proposed approach **only requires storing the model weights** of the generator instead of the full dataset. While this work focuses on more stable GAN training and not representation learning, it takes away from the novelty of the proposed method. The experiments in the paper appear sound and provide **interesting insights**.<|endoftext|>The results are not earth shattering, but solid work like this should be published and discussed. Weaknesses: The elephant in the room is that the results depend on the quality of the IGM (implicit generative model) itself   by quality i mean how well has the IGM learnt the data generating distribution. The experiments appear to be well executed and thought through.<|endoftext|>I ve upgraded my rating from 3 (Reject) to 6 (Lean Accept). * The authors have shown more promsing results on pretrain LSUN + transfer on Stanford Car classification. Even though, the results are somewhat underwhelming on ImageNet itself, this work can serve as a promising and strong baseline for future work in this topic. Here are my concerns:* It seems that the SimCLR baseline is very weak. Even though achieving state of the art performance is not the goal of this paper, the authors should strive for a reasonable baseline.
Accept (Poster); rating score: 8; rating score: 5; rating score: 3; The paper introduces a novel way to solve Full Waveform Inversion problem which is a common problem in geological surveys. I am very positive about the paper and would recommend acceptance, but authors could also considers comments/points mentioned above. The power of the method lies in unsupervised learning which allows one to use more data without expensive data labelling. The paper is also well structured and well written.<|endoftext|>The paper presents a method for full waveform inversion (an inverse problem in seismic imaging) that combines a convolutional neural network (CNN) with a physics based forward modeling operator. My main criticism is that the novelty of the approach seems to be low. It s not clear what supervised method is being referred to here: "...26.77% smaller than that of the supervised method"  Full Waveform Inversion or Full Waveform Inversion? The authors evaluate their method on a new simulated dataset and show that it can outperform other supervised approaches.<|endoftext|>This article studies the Full Waveform Inversion (FWI) problem. In real seismic inversion for exploration, the acquisition geometry may be non uniform. **(4)The novelty of the unsupervised learning paradigm:**Is the proposed learning paradigm model agnostic? The topic is of general interest and the paper is mostly well written. However, there exist concerns about a lack of novelty and incremental improvements. The authors are strongly encouraged to address these shortcomings by:i) provide a better justification of the OpenFWI dataset, especially on how OpenFWI bridges the gap between simulation and real seismic inversionii) motivate the idea Neural Network+PDE  betteriii) improve the experimental section by studying if the proposed method is model agnostic
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; The paper also provides evidence that easy to learn cues tend to converge to relatively flat minima and models that prefer these cues are more abundant in parameter space. It provides an interesting and insightful analysis of model preferences for various visual cues. Given the potential impact of the analysis, I recommend the paper to be accepted.<|endoftext|>The authors propose a framework for studying the tendency of deep neural networks to preferentially adopt "cues". Weaknesses:Having said that, this paper fails to deliver on its promise of intricate analysis. The authors themselves acknowledge that there might be other "shortcut cues" outside of the selected ethnicity and age cues. I would appreciate it if the authors can offer some insight into this. The ethnicity solution doesn t seem to be "characterized with a flatter and wider surface" as compared to age and gender solutions in Fig.4.If there are more obvious examples, the authors should use them instead.<|endoftext|>The path of investigation is fundamentally valuable in the sense that models of biases and generalization of networks are helpful tools. [W2] A somewhat methodical weakness of the paper seems to be the problem statement. I do agree that this is consistent with the model preferring the property of scale in making the decision. Perhaps the authors can rephrase the contributions more clearly towards the benefits? The paper addresses an important topic, but I feel that the overall impact, measured by depth of presented contributions is too shallow.
Reject; rating score: 3; rating score: 5; rating score: 6; They proved the Lipschitz continuity of distributional RL , provided convergence condition for TD update under noisy states, and conducted sensitivity analysisThe idea of understanding the distributional robustness of distributional RL seems novel and has not been done before. Moreover, In RL, we often consider bounded reward, so if we consider the expected RL with squared loss function, it still enjoys all the properties of KL loss arising from distributional RL   Disconnected results: How were the analysis in Sections 4.2 and 4.3 related to distributional RL? After the sensitivity analysis, the paper did not give any actionable insights from their analysis except that "the degree of sensitivity is heavily determined by the task" . This conclusion is not helpful and does not need any such sensitivity analysis. From my own experience with this experiment, I don t think so. **Minor comments**:  Eq (4) can be simplified   Section 3.1: "the state space go to infinity": unclear   The idea of exploring adversarial robustness of distributional RL is interesting and novel, but the analysis presented in the paper are marginal, disconnected, and does not support the understanding of the robustness of distributional RL.<|endoftext|>This work presented State Noisy Markov Decision Process (SN MDP), where there is a noise generating mechanism (either from the environment noise or from the adversary), and the theoretical properties (such as convergence and contraction) for corresponding (expected) Bellman operator and distributional Bellman operator were proved. The theoretical analysis was done for both tabular and linear funcion approximation settings. Especially in function approximation setting, authors characterized the robustness blessing of distributional RL based on histogram distributional loss and analyzed how the noise factor affects TD learning by using influence function that utilizes the perburbation method. However, I believe authors should have given experiments on the usefulness of this framework (e.g., authors may suggest more practical but simple problems where training observations are noisy as stated in the introduction). Also, only 3 runs are used for each experiments, and all results are reported without standard errors, which means that we cannot evaluate the statistical significance just by using the reported results.<|endoftext|>This paper studies the robustness of distributional reinforcement learning, in particular the robustness on state observations, which have been demonstrated in a few papers on adversarial attacks to deep reinforcement learning. Theoretically, the authors find that distributional RL can be more robust under this setting, via the lens of Lipschitz continuity of the loss function and the influence function. The results are quite promising and show that distributional RL might be more robust than expectation based RL agents under noises on state observations. 1.The paper is not well organized and writing can be improved. Is it a plotting error? I feel this paper can become a good paper if the weakness and questions mentioned above can be addressed.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; This paper considers the problem of “one shot” set/graph generation, which involves learning a probabilistic decoder that maps latent vectors to sets. First, the authors extend the usual definition of equivariance for a function to a learning algorithm. Next, the authors propose Top n, which is a new set creation mechanism which learns to select the most relevant points from a trainable reference set, in a deterministic and non exchangeable fashion. Experimental results are provided for SetMNIST reconstruction and generative tasks for a synthetic molecule dataset for sets, and the QM9 chemical dataset for graphs, demonstrating that Top n is competitive with or outperforms a number of existing generative approaches. Strengths:  The authors provide a good overview of prior work on generative models for sets, and in particular for the one shot generation problem. It appears to address problems with some existing set creation mechanisms, such as random iid generation and First n creation, and empirically performs well. Weaknesses:  It would be helpful to include an experimental comparison with at least one recursive set generation method.<|endoftext|>In this paper, the author proposes that exchangeability is unnecessary for the generative model in the domain of set and graph generation. The definition of equivariance is generalized to learning algorithms, which is appropriate for generative modeling. Then, a method called Top N which can be used in classical generative models, such as VAE, GANS, is proposed. The paper makes significant contributions:1. The experimental results in this paper using different models are compared on the basis of the generalized definition for equivariance. It is better to compare the traditional method which has the ability to generate exchangeable distributions with the one proposed in this paper. But for more complex generating tasks, for example, the task which should satisfy the symmetry of rotation, permutation, translation simultaneously, it is more difficult to design the full architecture. 3.How to choose the size of the reference set? Overall, I recommend accepting this paper but the authors are expected to address the concerns.<|endoftext|>This work proposes a new deterministic set sampling mechanism, Top n. Top n learns to select the best  n  points from a trainable reference set. Unlike the previous set sampling mechanisms such i.i.d.sampling, First n and MLP projection, Top n do not suffer from collision problem and can generate sets of various sizes (unseen during training). Top n can be incorporated for one shot sampling in VAE and GANs like generative models. Experimental results on standard benchmark for set and molecular graph generation, suggest improved performance in comparison to prior sampling mechanism. The paper is well written. How are gradients propagated ? 2.For Top n algorithm, please elaborate on the dimension of W_i. 3.For reference points, how is  angle  vector related to corresponding  representation  vector ? There has to be some theoretical lower bound connecting number of reference point with performance on each dataset. Please include additional results with similar reference set as First n model. I suggest that all the equations in the paper be numbered. And so it is not possible to map noise vector.<|endoftext|>This submission discusses probabilistic models that generate sets and graphs conditioned on latent vector representations. On SetMNIST, the results for TSPN are not the same as reported in the paper (as mentioned by the authors). * First n generation, using a learnable reference set of a maximum number of nodes/set elements, and concatenating the latent set representation to each element of this reference set, and then picking the first n element of this reference set as node representations. They furthermore adapt first n to the method top n with differentiable sorting of the representations of the elements in the reference set based on cosine similarity with a vector that depends on the latent vector representation. The proposed method is benchmarked on set and graph generation tasks : SetMNIST, synthetic molecule like 3D structures, the QM9 dataset. * new ways to handle permutation invariance and equivariance in graphs and sets is also a timely topic. I welcome clarifications by the authors. Paragraph above def 1: "this definition can unfortunately not be used for generative modeling: since the input is a latent vector and not a set, it cannot be permuted". This statement is very unclear.
Reject; rating score: 3; rating score: 3; rating score: 8; Going back on that trend now only to pursue the polynomial time nature of the running algorithm would in my opinion require far more diverse evaluation examples, backed by a stronger motivation highlighting real world threats of all the other MARL algorithms taking longer than polynomial time. As is, SOP CG does not contend amazingly against other MARL algorithms that chose the "NP hard? Curse of dimensionality? Fine.We ll approximate, approximate, approximate." path rather than the "Polynomial time is our topmost priority; function expressiveness can wait." path.That leads me back to the question of why pursue polynomial time at the cost of losing both the function expressiveness and the peak performance in the apparent trilemma.<|endoftext|>This paper proposes an extension of deep coordination graph, called Self Organized Polynomial time Coordination Graphs (SOP CG). Instead of pre specified graph topology used in DCG, their method allows graph topology to be state dependent, which is achieved by a coordinator agent, and the optimization of this agent is incorporated in a modified temporal difference learning paradigm. Two pre specified undirected acyclic graph classes are used to ensure polynomial time graph selection and accurate greedy action selection. The result on sensor network, grid world and MPE shows that such a trade off between the representational capacity of graph topology and the computational accuracy can improve the performance of MARL and learn meaningful graph topology. State dependent coordination graph needs to be determined at each time step in both training and execution,  which means SOP CG is a centralized method. 2.It is not clear how $q_i$ and $q_{ij}$ are learned. Are they parameter sharing for agents? 4.Since both DCG and CASEC include SMAC experiments, it would be better to also include it here to show the performance of SOP CG in complex environments. In summary, it is currently hard to see the benefit of determining the coordination graph in a centralized way. It seems clearly below the bar of ICLR.<|endoftext|>This paper introduced a novel method called Self Organized Polynomial time Coordination Graphs (SOP CG), aiming to handle the decentralized constraint optimization problem (DCOP). This paper is well organized and the experiments are explicitly presented. For example, in the Background section, the meanings of some symbols in the model are not clear. Or, is there any intuition at all? How did you come up with that idea? Have you borrowed this idea from somewhere else? It is better to give a detailed explanation about polynomial time coordination graphs. Whatever techniques are used in the manuscript, there is a need to tabulate computational cost of the proposed algorithm in this paper. 5.In the Experiments Sections, the authors claimed that the graph structures learned by SOP CG definitely match the ground truth demands for effective collaboration. However, it is not clear to us why the method can be used in demonstrating the ability of the proposed approach to organizing coordination relations. Therefore, the authors should provide a detailed explanation about the above issue. The work presented is indeed interesting and relevant to the real scenarios.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 6; rating score: 5; This paper develops equivariant architecture for RL, specifically DQN and SAC. The theoretical exposition is general and a specific SO2 instantiation is shown to outperform baselines on image based RL tasks. Strengths+ Very clearly motivates sample efficiency + generalization and articulates inductive bias from the perspective of adding equivariance to the model vs doing data augmentation. (iii) both Fig 5 and 2b are very idealized and in practice many other visual distractors would be present. A more nuanced coverage of limitations would be helpful to constraint where this method works and where it doesn t, rather that surface level limitations of learning in simulation and delta state based action.<|endoftext|>The paper uses rotation equivariant CNNs for model free reinforcement learning. More specifically, it is argued that for robotic manipulation tasks the corresponding MDP is invariant under translation and rotation, and therefore one could more efficiently learn equivariant/invariant policy/value functions. This idea is applied to both DQN and SAC for control with finite and continuous action spaces respectively. The main strength of the paper is the experimental results which are very supportive in a range of different tasks. Its main weakness is lack of novelty, on the theoretical side. However, given the importance of this domain, I believe the focus on this group and proposed methodology still has interesting contributions (e.g., in learning from demonstrations).<|endoftext|>The authors propose two main contributions: 1) define and theoretically characterize an important class of group equivariant MDPs, 2) and integrate equivariant variations to DQN, SAC, and LfD. The paper provides many different sets of experiments. The results are promising which shows benefits of the proposed approach. Strength:  Sound theory of group invariant MDP. Applications on realistic robotic manipulation  Showcasing on different RL (SAC and DQN) and robot learning (LfD) algorithms. Regarding to the invariant MDP, the proposed approach only deals with visual state spaces, so is it possible to extend to include proprioceptive information in the state space? The paper proposes interesting ideas that might be useful for robot learning tasks.<|endoftext|>This paper defines and theoretically characterizes a class of group equivariant MDPs and studies its invariance and equivariance properties. 2.The claims made about the proposed method are validated through extensive experimentations and comparison against strong baselines. It shows the improved performance of an equivariant DQN on these tasks and introduces an equivariant SAC model. 4.Overall presentation of the paper is goodWeaknesses:1. Is the claim of proposing an equivariant version of DQN as a novel contribution correct? 2.The improvement in the reward shows the ability of the Equi DQN and Equi SAC to faster learn the policies but can it be ensured this is because of the inductive bias of the model architecture as claimed? What is the significance of the extra two fully connected layers? Does this mean the Equi DQN needs more parameters to learn compared to the baseline DQN. If so, then there should be a baseline DQN that has the same number of parameters. The paper has an interesting premise about the advantage of encoding symmetry as an inductive bias in the data. It is novel in terms of introducing equivariant SAC but the claim of introducing equivariant DQN is ambiguous as it has already been introduced in previous work (Mondal et al.2020).Their experiments show improved performance of equi DQN and equi SAC over other methods for rotationally symmetric problems in robotic manipulation (which is different from previous works). Additionally, in order to ensure fair comparison, their equi DQN needs to be compared with a baseline DQN model with the same number of parameters.<|endoftext|>In this paper, the authors present an application of equivariant neural networks to reinforcement learning. They demonstrate new DQN and SAC architectures that make use of equivariant representations in order to improve sample efficiency. Results on manipulation tasks demonstrate sample complexity reduction compared to other techniques. The authors have provided a definition of G invariant MDPs and demonstrated their effectiveness in visual tasks. This definition rather introduces restrictions on the class of MDPs considered. 2.Similar question as the above goes for the action spaces. What assumptions are we really making on the reward and transition functions? Or is it that the type of discontinuities present in ATARI prohibit the application of the proposed method?
Reject; rating score: 5; rating score: 5; rating score: 5; This paper studies the correlation between the attention mechanism and many prior arts. Heuristically, this paper links the currently hot topic, attention mechanism, with many milestones works in the past decades before the deep learning era, including subspace learning, sparse coding, kernel regression, non local means, etc. ICCV.The framework by Hamburger[3] is strongly correlated with the self expressiveness in this work. The minimizer to the optimization problem can be treated as the output of attention modules, and the optimization algorithm can be understood as the ``architecture  of attention. Considering the close connection between attention and Graph neural networks, here is a list of GNN papers inspired by classic methods. In general, this is an interesting paper with good insights. However, I would not like to recommend acceptance because of the lack of theoretical and practical evidence to support and verify some basic claims as well as substantially ignore the progress within the deep learning community to bridge the attention method with prior arts listed in this paper. Many ideas are clearly of merit to inspire further research but not formal and supported enough as scientific work. I sincerely recommend submitting this work to the ICLR blog track this year, if the authors can sufficiently revise the writing style to suit a blog. The intuition behind the connection between attention and prior arts is clear. Especially for many researchers starting their careers after the booming of deep learning, the discussion sketches a holistic and detailed perspective towards the essence of many previous works and how they are related to attention. This will be of interest to the community. End to end learning and optimization on graphs. Also, integrating these principled methods into neural networks is not such easy, especially due to the curse of training issues, computation budget, etc. 2.Although this work itself sufficiently discusses the correlation between attention and classic methods and clearly explains why these classic methods are related to attention or vice versa, it ignores many related works in the community that have essentially similar ideas or contributions. Nyströmformer: A Nyström Based Algorithm for Approximating Self Attention.<|endoftext|>This paper surveys several lines of prior work which has connection to the self attention module in transformers. Specifically, the authors show that self attention has the similar form with the kernel regression and non local mean algorithm. They also demonstrate that locally linear embedding and self expression algorithm for subspace clustering have the form of representing one data point by weighted sum of other data points, and thus are connected to the weighted sum of values in self attention. Based on these observations, the authors argue that the innovation of self attention is not modeling the long range relation, which is also proposed in prior work, but the learnable parameters and the multi head design. The authors also suggest several directions for future work, such as using self attention for manifold clustering. Strengths:+    The paper does a literated survey of the prior work that has connection to self attention. +    The proposed directions for future work are quite interesting, including using self attention for manifold clustering, and connecting sparse reconstruction with self attention. Weaknesses:The main concern for me is the contribution of this paper. First of all, the relation between self attention and kernel regression and non local mean denoising has already pointed out by previous work, as cited by the authors. Second of all, the relation between self attention and locally linear embedding and self expression in subspace clustering seems not so close to me. For example, self expression estimates the affinity of the points sampled from the same subspace. However, the authors didn t take a step further into any of them. The contribution would be much bigger if the authors could dive deep into one of the ideas to potentially improve self attention or apply it in other problems. For example, the authors show that self attention is related to non local means and also to low rank subspace clustering. Is it a possible way to explain that transformers are more robust than CNNs, as stated in recent works [1]?<|endoftext|>This review paper tries to study the principles behind attention and its connections with prior art using the self expression methods (e.g., kernel based regression, non local means, locally linear embedding, subspace clustering). This is a review paper to discusses the relationships between the attentions and the traditional (coding) methods, including attention and kernel based regression, masked attention and locally linear embeding, self attention and self expressiveness. ves: + The problem of the principles behind attention seems interesting. + The authors show some future directions on how to improve self expressiveness using self attention and vice versa. The self expression is that “each data point in a union of subspaces can be efficiently reconstructed by a combination of other points in the dataset.” (Elhamifar & Vidal 2009; 2013) This leads to the reconstruction loss in the Eq.(12).However, self attention is to use itself to choose itself, that is, the calculation formula is defined in the Eq.(4), and it’s loss depends on the different tasks. It is clear that the self attention does not need the self expression or the self expression is one case of the self attention. In the subsection 5.3, the authors note that self expressive coefficients (SEC) are more general than self attention coefficients. However, SEC still have some shortcomings. 1) SEC must result in over high time and space complexities because it considers self expressiveness. Thus, the current SEC methods cannot apply into large scale datasets. 2) Could you provide some rough experimental results to verify the effectiveness of the Eq.(18)?I recommend this score since this paper is to consider an interesting problem, some suggestions are hard to implement in deep models on large scale datasets (millions), and there are no experimental results to support the arguments.
Reject; rating score: 3; rating score: 5; rating score: 8; This paper proposes the Virtual MCTS (V  MCTS), a variant of MCTS that mimics the human behavior, and is 50% more sample efficient, by performing a type of forward pruning. The proposed method uses forward pruning/early termination in UCT to reduce the search effort. This idea has been shown to indeed improve performance in certain situations. The proposed approach appears to be a variation on these older experiments. The authors have  not presented comparisons to these earlier works in non deep learning environments. I would be very interested to learn of a comparison of their new approach to these earlier selection rules. I would consider such comparisons essential for considering the new rule for publication in ICLR. There is one unresolved reference to a section in the paper. The paper presents an interesting new termination criterion for MCTS. The work should include comparisons to other selection rules, and misses references to some of these. Without these comparisons, it is unclear how substantive the contributions are, and I do not recommend acceptance.<|endoftext|>The paper proposes Virtual MCTS, an early termination rule for MCTS to improve its efficiency. Roughly speaking, the termination rule prunes the search process when the final policy at the root node is unlikely to change by too much from the current policy. But I think the proposed termination rule follows my intuition and the paper could be improved if the authors provide more detailed analyses on that. The main contribution of the paper is an early termination rule for MCTS.<|endoftext|>This paper proposes an approach for a significant speedup of Monte Carlo Tree Search (MCTS) at a relatively small cost in playing strength. The basic idea is that, when the change in the distribution of visit counts between two different time points $\frac{k}{2}$ and $k$ is less than some constant $\epsilon$, it can also be shown that the remaining change in distributions between $k$ and $N$ (where $N$ is the maximum visit count allowed by some budget) will be bounded below some value, and if we consider such a maximum possible error to be sufficiently small we can just terminate the search early at time $k$. 2) The main ideas of the paper are clearly described. 3) The proposed approach looks interesting and useful. **After response from authors:** I am satisfied with how the authors have addressed the issues and updated my score accordingly. They re too similar, and often in other work the hat actually signifies an "approximation" whereas the version without a hat signifies a "theoretical" or "true" value. Should this be "games" instead of "pieces"?
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 5; I recommend the authors may investigate more in depth interpretations. For example, does EN+BN recipe benefit from the implicit regularization effect of BN?<|endoftext|>the smaller the better? All underlying computations are built on previous methods.<|endoftext|>The relevance of the work is somewhat limited since the reported performance improvements are relatively far from current state of the art. All reported performance values lack error bars.<|endoftext|>Given the current weakness of experiments and theory part, I tend to weak reject this stage.
Reject; rating score: 5; rating score: 5; rating score: 6; This paper proposes "S3" framework for learning with noisy labels. To this, I appreciate the efforts and contribution. For the DivideMix that the authors mention, I admit that DivideMix is also a combination paper. The proposed "S3"  framework achieves very good performance on both close set label noise and open set label noise. Each part of S3 along with the hyper parameters are analyzed. I do not think "S3" is a simple framework or simple method. It contains two stages with multiple techniques such as KNN voting, relabelling, sample selection, mixup and consistency loss, which make it hard to understand which part exactly contributes most to the performance. 2.KNN voting based relabelling, mixup and consistency loss have already been explored in the literature of learning with noisy labels [A1, A2, A3]. ICML 2021**Pre rebuttal:** S3 achieves very good performance but the technical novelty is limited. It is relatively a hard decision for me.<|endoftext|>This paper focuses on learning with noisy labels problems, which constructs an iterative learning framework to refine labeling set and train the model parameters. It demonstrates its advantages by comparing with current baselines on a range of datasets. The novelty of this paper is ad hoc, which stacks the benefits of sample selection and contrastive learning. Considering several previous works have explored them [1, 2, 3] and the proposed method in this paper has minor difference with previous works, it is below the bar of ICLR for the area of learning with noisy labels. However, the novelty of this paper is ad hoc, which stacks the benefits of sample selection and contrastive learning.<|endoftext|>The paper proposes a two stage approach to learning with noisy labels (LNL). The proposed method is novel and interesting and shows promising results in various settings. **Post rebuttal/revision**The authors have done good job addressing my concerns (as well as other reviewers ). The authors explain well known concepts like softmax gradient but some details of the proposed methods are explained briefly or not at all. 1 but not in main comparison. I assume you meant something like "The vector $\pi$ contains the class probabilities in the whole dataset, and we denote by $\pi^{ 1}$ its elementwise inverse". **References**Nishi, Kento, et al."Augmentation strategies for learning with noisy labels."
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 8; For open set recognition (OSR), the authors propose using background data to represent instances from unknown classes. Based on LDA, they proposed using the distance from an instance to the class mean to estimate the posterior class probability via softmax. Strengths:The main contribution is a loss function that includes background class regularization (BCR) based on probability of inclusion estimated by the CDF of Weibull distribution, though BCR and CDF of Weibull distribution are not new. Empirical results indicate that the proposed method compares favorably against a few existing algorithms on a few datasets. The paper is generally well written. Also, a more detailed analysis and discussion of why L_bg,k is needed would be helpful. Minor:Paragraph next to Figure 2: Eq 7  > Eq 8 for P_IWhile the empirical results are favorable, justification and analysis of including L_bg,k could be more detailed.<|endoftext|>This paper introduces a distance based background class regularization (BCR) method for open set recognition (OSR), in which the distance based classifier that uses the principle of linear discriminant analysis is utilized to limit the feature space of known class data in a class wise manner and make background class samples far away from the limited feature space. Experiments show the robust OSR performance of the proposed method. + A distance based background class regularization (BCR) method using the principle of linear discriminant analysis is proposed to address the problem of open set recognition. The experimental evaluation of this paper is not sufficient. However, the novelty is limited and the experiments are not sufficient.<|endoftext|>In this paper, the authors propose a new method for open set recognition. To this end, the authors use distance based classifiers and they minimize the distance between the class samples and their corresponding means for the labeled data samples and try to enforce the background data samples to be outside the class acceptance regions. Then the samples coming from the unknown classes can be rejected based on the distances to the centers of the known classes. Also, the authors use a completely different setting (using a background class) in their experiments and therefore it is hard to compare the results to the some published related methods. The proposed method is compared to related open set recognition methods and better accuracies are reported. The authors use ImageNet data samples are used as background class samples. I was wondering if the overlapping classes between tested datasets and this one are removed. Lastly, an ablation study showing the importance of the utilized background class is necessary in my opinion to judge the effect of background class samples for returning more compact class acceptance regions. The authors cited some of these studies, but some important references are missing. Also, more comparison to the related methods and some ablations studies are needed in experiments. The authors use a similar idea, and they enforce to minimize the distances between the known class samples and their corresponding class centers. Therefore, the overall novelty is limited in my opinion. This is unacceptable. Note that Izmailiov et al.use this setting in the generative models and this is not appropriate for the classification setting. Furthermore, they also recommend the updating class centers. The authors report that updating class mean vectors yield worse results than fixed class mean vectors.<|endoftext|>The proposed OSR is based on background class regularization (BCR) where data of known unknown classes are utilized in the training phase of the classifiers based on typical deep neural network (DNN) architectures. This is an important and practical problem and can bring new insight for the field. I recommend the paper to be accepted. Proposed approach is experimentally compared to three previous works applying BCR based OSR with SoftMax classifiers. Different performance metrics of classification accuracy, area under the receiver operating characteristic curve (AUROC), and the open set classification rate (OSCR) are utilized in comparison. A group of different techniques from prior work are combined in a novel way and each part of the model is justified and the whole model is experimentally verified against state of the art. The claims of model properties are experimentally validated against three previous approaches, and it is shown that open set recognition rate is improved, and the similar or better close set accuracy is achieved simultaneously. To strengthen the presentation and claims, it would be good to show how computational complexity and efficiency of the proposed classifier in relation to training and inference times compares to previous methods utilizing SoftMax classifiers. In ablation studies related to classification accuracy (with conventional loss function), it would be useful to collect these results to a new table for comparison  How is the best/optimal lambda selected in experiments? Could this be automated? The paper presents an interesting and novel approach combining several techniques to improve DNN based open set recognition with distance based classifier.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; This paper proposes a new few shot learning method for NLP problems by incorporating a simple,effective framework. Strengths:   The paper is generally well written, with excellent motivation and empirical setup/analysis   The overall strategy of differentiable prompt optimized to maintain fluency is reasonable and novel. The ablation experiments and optimized prompt analysis are insightful. The choice in deciding how many template tokens are used is unclear. An additional ablation experiment trying different number of tokens to optimize could be illuminating (even for just 1 dataset). The ablation study would be better if specific numbers were provided. While it has a couple minor issues, I think the broader community would find this paper interesting.<|endoftext|>This work reduces the need for prompt engineering for few shot tasks by optimizing only the word embeddings of unused tokens in the LLM. The exploration on what exactly are learned in the prompt was also interesting. Strengths  Very clearly written, the key ideas were well explained and simple. Weakness  I am bit unconvinced about claiming that this work requires no external architecture in Table 1. There are new parameters that are getting trained/fine tuned before we can do inference. It would ve been nice to see what happens if we constrain the additional tokens to be a part of the same vocabulary. This is more or less the approach taken in AutoPrompt, except with the current training methodology. If they are learnt from scratch, then how is this different from regular fine tuning? Is this fair to say?<|endoftext|>The paper proposes a new approach called DART (Differentiable Prompt) which can perform few shot fine tuning without any prompt engineering (main difference w.r.t.previous works). This is achieved by optimizing the prompt template and the target label with backpropagation. Since the proposed approach doesn’t use any extra parameters, it can be easily used for any pre trained language models. Further, the empirical evaluations suggest that it does better few shot learning than previous works. Strengths:1) The paper proposes an interesting framework for few shot fine tuning without any prompt engineering. 2) The problem space is well defined (and also well contrasted with previous work), and the paper is clearly written and easy to follow. It would be great to further discuss these results in detail. Overall, the approach is technically sound and is also very easy to apply to other pre trained language models.<|endoftext|>This paper proposes a new soft prompt tuning method called DART. The authors claim that training label representations for prompt tuning is important. Without the addition of a task specific architecture, DART outperforms previous state of the art prompt tuning in few shot performance on 9/14 NLP tasks. Strengths of the paper:  Clearly written and easy to follow. Comments:  In section 4.5, the authors said that their method requires no external parameters. Comparing with Prefix tuning and WARP in experiments would be great. But this is not mandatory. The proposed DART has similar motivation. However, DART is more efficient than previous prompt tuning methods and is on par with them.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; This paper studies the ability of shallow and deep neural networks to approximate Korobov functions, and analyses their representation power in terms of the number of used parameters. The authors provide simple constructive proofs of the theorems, whose high level ideas are well explained in the main text.<|endoftext|>The bounds in Th. This gives a complete picture for how Korobov functions behave wrt to function approximation with shallow or deep nets. For example, the Conclusion states: "This work therefore contributes to understanding the practical success of neural networks theoretically"  > How is this the conclusion?<|endoftext|>This paper studies what function class can be efficiently approximated by neural networks. This is a local constraint on the local smoothness of the function space. The paper gives matching upper bound and lower bound for the $X^{2, \infty}$ space. This makes it less motivated to study this function space. * The approximation issue is not the main concern in deep learning theory. The authors have addressed my main concerns.<|endoftext|>The paper proves upper and lower bounds on neural networks for approximating Korobov functions $X^{2, \infty}$. The scope of network architectures discussed is extensive, including shallow (2 layer) networks, deep networks with ReLU( like) activation functions and Sigmoidal activation functions. On the other hand, extending to bounded $L^p$ derivatives with $p < \infty$ can be interesting. Compared to Sobolev spaces, we see the approximation of Korobov functions is free of the curse of data dimensionality. Thanks for authors  detailed response.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; This paper proposes a unified interface for access to a collection of Neural Architecture Search (NAS) benchmarks. With experiments performed on across multiple search spaces and datasets, the authors show that some conclusions drawn from a small subset of benchmarks do not generalize across diverse datasets and tasks. The results are interesting and very useful for the community as it is   and the authors should be commended for their effort. However the work lacks deeper insights from the experiments performed that could guide the community towards better methods. Minor: clarify whether $i$ and $j$ are columns or rows in Fig.5. In my view the two main contributions of this work are (1) showcasing the limitations of drawing conclusions from single benchmarks; (2) providing the community with a larger analysis tool. I expect that the community will welcome these resources and thereby **recommend the paper is accepted** (provisional score: 8)<|endoftext|>This paper investigates the drawbacks of current existing NAS benchmarks and the works that evaluate on them. Then it presents NAS Bench Suite, an extensible collection of NAS bench marks which is easy to use to facilitate reproducible, generalizable and rapid NAS research. There exist many NAS benchmarks currently across diverse tasks. However, researcher find that NAS algorithms may perform differently across these benchmarks. Sometimes even reverse conclusions are drawn. This paper investigates this problem and points out that the differences may come from the different search space design, training pipeline and hyperparameters, via comprehensive experiments across many existing benchmarks. It finds that conclusions from a few benchmarks do not generalize well to other benchmarksFurther, the paper proposes NAS Bench Suite, a collection of NAS benchmarks, with a unified interface, which is easy to use, to facilitate reproducible, generalizable and rapid NAS research. The motivation of the work is clear. The claim is sufficiently addressed by the method. Overall, this is a good work for the NAS community. I think the paper is of good quality and will contribute to the NAS community.<|endoftext|>This paper present a wrapper for 25 NAS benchmarks and provide insightful analysis on NAS and predictor performances across the benchmarks. They show, among other things, that no NAS method is best across all tasks, that hyperparameters of NAS are not robust and that small tabular benchmarks are not representative of NAS method performances on larger benchmarks. Strengths:  Very important topic. Benchmarks are critical to assess performance of NAS algorithms. Provide statistics across a wide array of NAS benchmarks, such as the distribution of accuracy across the search space. 1) Whether NAS results generalizes from the small 101 and 202 benchmarks to larger ones and 2) NAS have robust hyperparameters. They show that 1) NAS algorithms do not generalize well from small 101 and 202 benchmarks to larger ones,      2) Hyperparameters are not robust for NAS algorithms. 3) Is not true obviously since hyperparameters of NAS algorithms are not robust. They contribute a valuable wrapper for 25 benchmarks, although it is not clear to me how the search space representation with NetworkX allows for a dynamic search space definition. And the experimental answer to this question is not clearly outlined in the paper neither. I am quite concerned about the maintainability of the library. Some even contain specific slurm configurations for the cluster of the researchers. The wrapper of benchmark is a valuable contribution by itself, but this paper illustrates well the benefit of such benchmarks by providing insightful analysis of NAS algorithms.<|endoftext|>The authors collected most of the existing NAS benchmarks to construct a new benchmark. A unified API is provided to use these existing search spaces and architecture datasets. Based on this the authors re analyze some NAS algorithms on this new large and comprehensive benchmark and have some interesting observations. strengths:  The paper is well motivated and written. This new benchmark can be used to do a more deep analysis for the NAS methods. The table and figure captions are too short to explain themselves. Please at least explain the abbreviations used in the table or figure. The authors did much engineering efforts and emprical analysis, whereas the technical novelty is a little bit weak.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; The paper tackles the problem of learning robot controllers that can handle changing or unknown environments. It proposes to use differentiable physics for online system identification and reinforcement learning for offline policy training. The differentiable physics module estimates simulation parameters from robot history and feeds this to the controller that is parameterized by these simulation parameters. They use domain randomization to ensure the universal controller conditioned on simulation parameters is robust to changing environments (simulation parameters). At test time, the differential physics simulation is used to estimate the simulation parameters to bias the controller to output controls for the  correct  simulation parameters. The proposed approach outperforms baselines in two benchmarks.<|endoftext|>In this paper, the authors propose UC DIFFOSI, which combines a differentiable physics simulator for system identification, and a universal controller which takes the identified parameters and passes them into the neural network to output actions. This will encourage the development of differentiable physics in the future. The paper s novelty is limited. It would be better if the authors can consider environments with more dofs. For example, they can consider identifying the Inertia matrices of the robot arms. However, employing differentiable physics for system identification has only limited novelty. The experiments are a little bit weak with only 2 simulation tasks and no real world environments. I think the paper s contribution doesn t pass the bar for acceptance.<|endoftext|>This paper proposes an algorithm to control robots with a universal controller conditioned on the robot parameters, that are identified online using differential simulation. The idea is simple yet interesting: giving the controller explicit information about the system could definitely help performance. Estimating those parameters with differential simulation definitely makes sense. The approach is evaluated on a series of rigid body control tasks, where it is on pair or better than previous solutions. The paper idea is overall interesting and fits very well with the current developments of differential simulation systems. The main strengths are:1. It empirically shows that domain randomization is insufficient to solve complex control problems with varying dynamics. The experimental results are not sufficient to back up the claims made in the introduction. 2.There are no state of the art model based reinforcement learning baselines. As mentioned above, they would probably need many samples to adapt to changes, so this could be a selling point for the approach. * What would happen if the policy input were very high dimensional, for example, images?<|endoftext|>The paper proposed to use differentiable physics engine for online system identification, which is then combined with controllers trained in a wide variety of tasks for robotic control problems in environment with unknown dynamics. The main contribution of the paper is the usage of a differentiable physics engine for online system identification, which supplements a universal controller(UC) trained offline. The main concern is in its current form, the paper is not clear about the trade off of the proposed approach. The paper could improve its quality if the following questions can be answered (1 is the major one while 2 and 3 are minor):1.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; This paper theoretically shows that the mean field equation for a certain family of Boltzmann machines with hidden variables, called the monotone DBMs, can be modeled as the recently proposed monotone Deep Equilibrium (DEQ) model. In particular, the practical advantage of the proposed monotone DBMs is not clear. However, there are no such comparisons in this paper. I am happy to increase my score if the above my concerns are properly addressed by the authors  response. This paper potentially includes an interesting technical contribution, while the significance is not convincing and the evaluation is weak.<|endoftext|>In this paper the authors propose a restricted parameterization of the Boltzmann machine that guarantees that for any set of observations, the mean field objective has a single global optimum. This means that, as a modeling tool, the proposed BM with restricted weights might be less flexible than a DBN. So it is difficult to gauge the practical advantage in the provided examples. The paper is well written and easy to follow.<|endoftext|>This paper is well written but its contributions are incremental with somewhat weak experimental results. Sadly this does not seem to be the case:  The practical benefit of deep Boltzmann machine compared to more traditional neural architectures (e.g.CNN for image classification) is not clear to me and has not been highlighted in the paper. When would someone use deep BM instead of the alternatives? This seems like an ad hoc fix for a method that doesn t really work due to the monotonicity constraint. For the patch case, the model works better without the monotonicity constraint.<|endoftext|>On one hand, this paper has some significant strengths. First, the paper is fairly well written in general. Second, while this work is heavily inspired by Winston & Kolter (2020), I find that the connection between mean field and monotone DEQ is quite interesting (although relatively straightforward), and the proposed method is theoretically well founded. The proposed model is theoretically sound, but it is not clear why one should use it. Even though this is just a minor detail in the current paper, I would like to take this opportunity to raise an important issue regarding credit assignment. This is not fair, and I think this happened because some previous work didn t cite them correctly or in a misleading manner. But of course it is up to the authors to decide.
Reject; rating score: 1; rating score: 5; rating score: 6; The authors propose a method called Quasi Rejection Sampling (`QRS`) which is a relaxation of Rejetion Sampling (`RS`). ## Weaknesses1   The authors propose a relaxation of rejection sampling which is using an arbitrary parameter $\beta$ instead of the true upper bound of the ratio $\frac{p}{q}$ when the latter cannot be computed. 4   In the abstract the authors require the proposal distribution to upper bound the target everywhere which is not true as the authors themselves clarify in the text. They rely on importance sampling, which begs question 1. The reviewer strongly recommends rejection since the paper lacks both significance and novelty.<|endoftext|>This paper introduces Quasi Rejection Sampling to balance sampling accuracy and efficiency for energy based models. By doing so, a bound on the importance ratio is not needed but the target distribution is no longer preserved. Besides, the hyperparameter beta and the proposal distribution need to be carefully chosen. 3.Algorithm 2 with automatically tuned beta is interesting since beta is an important hyperparameter and may require a lot of tuning to make QRS work. It could have potential usage on many applications. But the empirical demonstration is not convincing, especially missing the comparison with MCMC methods.<|endoftext|>Exact rejection sampling requires the user to know some upper bound M on the likelihood ratio between the desired distribution p(x) and the chosen proposal distribution q(x) [M > p(x)/q(x) \forall x]. The proposed method, Quasi Rejection Sampling (QRS), alleviates the need for this bound and replaces it with a tunable parameter \beta. This comes at the cost of increased rejection rate which decreases the efficiency of the sampler. The authors demonstrate the effectiveness of their approach in a number of controlled text generation settings and demonstrate that it can lead to improved samples over prior works (at the cost of additional computation). QRS gives more to the user in this way. The same cannot be said for MCMC methods. Another nice property of the approach is that it can be easily applied on top of any new advances in learning the proposal distributions. Weaknesses:The main weakness in this work is in the empirical evaluation.
Reject; rating score: 3; rating score: 3; rating score: 6; rating score: 6; rating score: 6; This submission contributes an approach to handle missing values in Neural networks by replacing inside the architecture the missing values by placeholders which cancel the role of the feature in the architecture. However, the approach here is based on handwaving, with intuitions that seem fragile. Page 3 claims that the approach enables relaxing the MAR assumption, but there is no legit argument to this claim. The goal of neuralizing the effect of missing values on activations inside the architecture is not a desirable one. Should X2 be missing, an architecture attempting to make a prediction solely from the observed data and using the same logic add the fully observed case, would fall, given that the fully observed case relies only on X2. Figure 2 should show other approaches, such as mean imputation. It does not really perform better than mean imputation.<|endoftext|>Experiments are conducted comparing this so called "(m)PROMISSING" method with other methods for treating missing data. In fact one can propose a more general version of what the authors propose by having the concatenation (x,m) of length 2p as input to the network. When m_i   1, then x_i takes on a value of 0. If m_i   0, then x_i takes on its observed value. One has weights to the k th hidden unit from both x_i and m_i. The paper then conducts 3 sets of experiments, on simulated XOR type data, OpenML data, and a clinical application. The XOR data is a simple mixture of 4 Gaussians dataset, but is perhaps a poor choice for a missing data experiment, as with only two input variables either 0 or 50% of the inputs are missing. While both PROMISSING methods do as well as can be expected on this dataset (Fig 2b), we really should also be given results for competitor methods (zero imputation, mean imputation, KNN imputation, MICE imputation etc). It would be most natural to consider the differences between different methods on *each* dataset, and then produce some aggregate summary. Overall, this paper proposes the (m)PROMISSING methods for handling missing data in neural networks.<|endoftext|>The handling of the missing values is located in the neurons of the first layer and each missing value is "replaced" by a neuron specific neutralizer in its activation function. NeuMiss networks: differentiable programming for supervised learning with missing values. ## Strong points  The computationally and conceptually light approach to handle missing values in neural networks appears to be an interesting alternative to overly simple imputation (such as mean or 0 imputation) and more costly imputation strategies (iterative imputation, multiple imputation). This is an interesting feature and I would encourage an analytical/theoretical assessment of this aspect in a revised version or future work in this direction. The experiments and results of Section 3 are well presented and commented and the details provided on the method and the simulations allow for easy reproducibility of the results. It would be interesting to rerun the experiments and plot a similar figure but in the case of MNAR data where the missingness pattern can indeed be predictive (to some extent) of the outcome. p.9: _several analytical and empirical_ aspects of PROMISSING _remains_ unexplored $>>$ _analytical and several empirical_ aspects of PROMISSING _remain_ unexplored (I didn t find any analytical aspects of PROMISSING discussed in the paper)######################################### Post rebuttal updateI thank the authors for their detailed and timely responses. I followed all exchanges between the authors and the other reviewers who criticize the lack of contributions/novelty of the proposed method.<|endoftext|>In this paper, the authors propose a method titled PROMISSING; this provides a new approach to handling missing data. Rather than imputation, a complete case analysis, or inverse probability weighting, among other methods, the authors advocate for learning a problem specific numerical representation for unknowns. Strong points:  The paper is well motivated and the method is interesting. I appreciate that the authors have included a NN architecture implementing the proposed approach for use with a popular NN modeling software. The data analyses are fairly thorough, but could be improved (see below). What is the motivation behind the  neutralizer  value? I m not convinced that keeping unknowns as unknowns allows the MAR assumption to be relaxed (page 3). The take home message from the simulations and data analyses, in my view, is that PROMISSING does no worse than methods that use imputation. In the simulations:      what does performance look like if the data are MAR? What are the magnitudes of the difference in performance between approaches? I suspect that the differences between methods are quite small relative to the error. I vote for accepting the paper subject to some additional experiments, based on my review above.<|endoftext|>The author(s) tested the method on simulated data as well as real data on a group of classification and regression tasks. I would first like to confirm if that were true. The main weakness of the model is the lack of significant improvement in performance in comparison to the baseline models but this should also be commended as the author(s) did not cherry pick the results where the model is well performing and rather opted to report all their results. This would give the paper more strength especially to make sure that your model outperforms others in the conditions mentioned. While there are mitigation strategies for that in regular neural networks, this could also be a strength point for your model. This is not entirely accurate given that the training phase will be affected by the missing values so it will still work best when the data is MAR similar to how regression based imputation methods perform best at those conditions. In addition, in table 2, there are some datasets that have zero features, is that a typo?
Reject; rating score: 1; rating score: 1; rating score: 5; This paper proposes a sampler for LSTM video models. In short, no novel contribution is presented in any way since the method is just a sampler that repeats frame in a trivial way.<|endoftext|>This paper mainly provides a stepped sampling method only for video detection and only adopted in LSTM structure.<|endoftext|>The authors present a stepped sampling to improve the learning capabilities of neural network models such as LSTM. Specifically, the stepped sampling procedure repeats the same input data in multiple batches, in other words, the batches "overlap" with one another in terms of the contained input data. The proposed methodology is technically sound and clearly presented.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; rating score: 5; rating score: 5; Could the authors clarify on this point and give examples. c.  Also, although the paper states that there have been earlier work (Saxe et al., 2017; Haarnoja et al., 2018; Van Niekerk et al., 2019; Hunt et al., 2019; Peng et al., 2019) on skill composition, there seems to be no discussion on why there is no comparison with them. This should be defined explicitly. If this is the case, I believe that the set of tasks in Sec.2.1 should be named differently. Overall, the paper is well written and it is easy to follow (if typos are not taken into account). Experiments are performed to verify the theoretical results in both transfer learning and lifelong RL settings in the PickUpObj and Fourrooms domains in the minigrid environment. The proofs of the theorems also seem sound. It would also be nice to see a discussion on the differences/similarities of this framework and the proposed one. On the proposed framework:	a. 5, does the agent sample a task or is a task provided by the environment? Overall, I think the paper can be a nice contribution to the field of lifelong RL, however, due to my concerns detailed above, I am currently voting for “marginally above the acceptance threshold”. Another big concern of mine is that although the experiments support the claims of the paper, I find them to be on very limited settings of the proposed domains.<|endoftext|>Good work on this paper. I appreciate the background session as I had forgotten lots of details from the boolean task algebra, since I read it. Within the problem formulation, the empirical method works well enough. Some improvements can be made on the paper though:1. The same applies to similar claims around the paper, for example, "which aresub logarithmic in the size of the task space" (abstract), 2. It s often not feasible to do that in some scenarios, so it can hinder the further applicability of this method, for example, some goals are defined as continuous regions of the state space. This is a good paper with a sound method and good results. A important step of the background is missing.<|endoftext|>The authors provide some theoretical guarantees on the performance of the algorithm, as well as empirical evidence that it can work in a toy setting. This work is sound and can potentially shape future lifelong reinforcement learning (LRL) research. assuming the agent can interact with the task as long as it needs is quite an unrealistic assumption in LRL. all experiments on toy tasks. there s no LRL baselines. I m giving a weak reject because I would like to see more motivation for this work and how it could be adapted to use cases. Upon acceptance, I still encourage the authors to add such an experiment.<|endoftext|>g \neq s$; or $\exists s \in \mathcal{G}. * The theoretical results may be of interest, although I had difficulty understanding some of the main statements, which hopefully I can clear up during the rebuttal period. * This comment is about boolean task algebra for RL in general, not just the present work. This would be “zero shot” transfer in the sense of this paper. * I also have some confusions surrounding Theorem 2:   * Why are the cardinalities all that we care about? * As a delta on the previous work of (Nangue Tasse et al.2020), I am not sure if this work constitutes a substantial enough standalone contribution for an ICLR paper. There are advances to be sure, but they are relatively incremental. * The experiments are good to illustrate the main ideas, but they are not a substantial standalone contribution, since the domains are very simple and the baseline comparisons are very limited.<|endoftext|>They provide theoretical bounds for their approach, and provide empirical evidence that their approach generalizes and transfers well in practice. Is the reasoning that most or all of transfer learning in RL can be framed as goal based tasks? Point 3 of rebuttal: The revised version is much more clear, thank you. (See my clarity concerns below, the setting is still not clear to me.) I was also confused about how a task could be “bounded”. Because of the cumulative effect of the Section 2 issues above (particularly the notation issues), I am not able to give constructive feedback on the theoretical contributions of Section 3. Please see Fig 4 of Tasse et al.if that helps. Since Section 3 proceeds to build upon Section 2, I think that the issues with Section 2 will significantly harm the readability of Section 3 for many readers. I would encourage the authors to take the time to run 30+ runs in the future to make this claim more substantial. This paper is fascinating and well motivated.<|endoftext|>### Strengths* The paper is generally well written and easy to follow, and explanations are intuitive. I think that this is an immensely interesting research direction. * I like the theoretical results, even though they are a little unsurprising. ### Weaknesses* Some important details are missing from the main paper: the exact algorithm proposed, and the exact setup of the experiments. * The paper s experimental section is fairly weak in that all experiments are on the same, fairly simple task. ### Detailed questions/comments* While the paper is properly anonymized in theory, I looked up "Nangue Tasse et al (2020)". The fact that they use near duplicate language strongly suggests the same authors (or a violation of attribution). If so, please train with more data/longer.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; rating score: 6; To improve the generalization in unseen domains, the paper proposes a regularizer based on align ing the gradient direction between training environments. The proposed approach achieve non trivial improvement on bench marks such as WILDS. The paper address an important problem in OOD generalization. The paper is well written and the contributions are stated clearly. 2.Despite the simplicity of Fish, it achieves non trivial improvement on WILDS benchmark, outperforming important baselines such as IRM. I think objective (4) shares similar motivation to IRM in the sense that we want the model to update in directions (use features) that are beneficial for all domains. It would be great to provide more discussion and comparison w.r.t.IRM.2.I wonder whether the inner product of gradient in equation (4) could be related to bi level / meta learning? For instance, check equation (12) in [1]. In particular, IRM defines its objective by measuring optimality, which is highly related to optimization. Note that this is different from analyzing approximation of IDGM such as Thm 3.1. Overall, there is no significant flaw in this paper. Although the proposed approach is not well analyzed, the empirical results are still impressive.<|endoftext|>This paper proposed inter domain gradient matching for domain generalization. They also approximated the proposed model with a simple first order algorithm to avoid costly second order computations. Pros:(1) The theorem of FISH looks interesting, which can avoid costly second order computations. (3) This paper is well written and easy to follow. (4) Experiments on the WILDs and DomainBed demonstrate the advantages of the proposed methods in a practical. (3) Some sentences are not accurately described. For example: In the related work 5. Not all ERM algorithms fail [1][2]. Need more explanation and the difference of MAML. Reference:[1] Gulrajani et al 2020. As stated in the main reviews, the motivation of using a matching gradient is not particularly impressive, but overall the paper does provide some interesting analysis that may encourage new ways of thinking about domain generalization problems. As such, I think this paper may be of interest to the ICLR community.<|endoftext|>Authors propose an inter domain gradient matching objective that targets domain generalization by maximizing the inner product between gradients from different domains. Authors also give a computationally efficient optimization for the proposed method and give results on various datasets. Authors worked out the Taylor series expansion of the gradient and came up with a linear approximation. Authors modified the Reptile algorithm which works on improving the GIP inside a particular task (or domain) to improve the GIP across different tasks (or domains)3) By the virtue of their simplification, their model update process is much cleaner than it would have been had they used IDGM itself4) This linear approximation also helps to not worry about saving intermediate gradients of all the past steps as might be the case in Reptile. Weakness: 1) It is not clear if the improvements given by the proposed method are statistically significant. 3) Comparing Fish and IDGM might be necessary to confirm that the approximation works. Review:1) How is the concept of using gradient inner product different from using kernel methods? If given infinite resources, can authors improve their scores further? If authors can show this on at least a few dataset, it will really help to understand the method better. arXiv preprint arXiv:1806.07572 (2018). For better understanding slow but exact algorithm could be tested on small dataset.<|endoftext|>This paper proposes to maximize the similarity of the gradients of the classification loss for different domains to learn domain agnostic features. This is an interesting way to address the domain generalization problem. 2.This method also provides a effcient way to approximate the inner product between gradients due to the large computation cost. A theoretical proof is also given for this approximation. Cons:1.The improvment is minor on DomainBed in Table 3. The results of baseline methods seem to be inconsistent with the results by facebook? It would be better to have an explanation for this observation and full comparison with DANN. Update:I have read the response by authors and other reviews. But I still share the same concern with another reviewer that it is unclear why this method works. If some domain specific information is useful according to the response, then why the gradient matching helps preserves such information? But the improvment is minor and some critical explanations to the results are lacking.<|endoftext|>The main claim of the paper is that by maximising inner product  between gradients from different domains leads to better learning of domain invariant features. The paper is well written and is easy to follow  Multiple benchmarks are used to evaluate the method Concerns:  the claim that feature learnt are domain invariant is not really backed by a theoretical explanation or empirical. One way would be to show using tsne plot, that indeed features align as the gradient have same sign. Regarding just empirical results Fish seems competitive but it is not clear in what cases it the best. Coral.Why the data augmentation based methods are left out in the comparisons? Several comparisons are presented but paper lacks explanation on the central idea, that why invariance is achieved.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; Authors tackle the problem of solving a capacitated vehicle routing problem (CVRP) when the cost of introducing an additional vehicle is introduced. It is nontrivial to extract the feasible solution from the probabilistic predictions, so greedy decoding, repair procedure, and post processing with OR tools is employed. One strength of this paper is that it identifies the common limitation of existing approaches in the literature: that they do not actually solve CVRP with hard constraints on the fleet size. Also, a good range of reinforcement learning baselines are considered in this work. In fact, a heuristic repair procedure is applied to meet the constraint, and then another heuristic post processing with OR tools is used to improve the quality of the solution.<|endoftext|>This paper is the first to address the capacitated vehicle routing problem with a hybrid machine learning (ML) and algorithmic solution. Also, the “pseudo” aspect of the decoding isn’t explained. Based on the ablation study in Appendix B1 for the auxiliary losses, it doesn’t seem like there is a significant difference in Cost/Cost_v performance. ##########################################################################Cons:  The main contribution appears to be a formulation of the CVRP as a supervised ML problem. I welcome a response from the authors, particularly pertaining to my concerns related to the motivation and contributions. The claim that RL methods are cumbersome to train is not well supported in general. (Granted, it cannot solve the problems with fixed fleet size.)<|endoftext|>This paper proposes a supervised learning approach with a permutation invariant network to solve the Capacitated Vehicle Routing Problem (CVRP). For large size CVRP, it is hard and time consuming to get a near optimal solution. For training time comparison, I think the authors should also include the time of generating training sets for your supervised learning model, as other RL based algorithms do not need. In equation (16), $b_k$ is defined, but I cannot find it in the equation. The paper proposes a supervised learning algorithm to solve CVRPs with fleet size constraints and vehicle costs. The authors design a novel permutation invariant network to deal with the combinatorial structure.<|endoftext|>The paper propose incremental improvements to improve the work of Kaempfer and Wolf and adapt it to tackle the CVRP with bounded fleet size problem. In particular, I am curious to see how much of the improvement they get comes from the post processing step as opposed from the learnt representation. Since the technical solution is derived from the work of Kaempfer and Wolf, I would have liked to see the authors compare their solution against that of Kaempfer and Wolf both on the mTSP problem that Kaempfer and Wolf used in their paper and on the CVRP problem tackled in this paper.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; Comparison and discussion of this work should be included though it can be to some extent be considered contemporaneous, and does not perform as extensive comparisons on tabular data. On balance the relative novelty and strength of the experiments lean me towards acceptance. Strengths:  Interesting use of contrastive learning for anomaly detection on tabular data  Strong empirical performance outperforming recent baselines; extensive comparisons on ~30 datasets   Ablation studies and sensitivity analyses for hyperparameters are performed  Paper is well written and easy to follow; limitations and assumptions are discussed  Code is provided for reproducibility Weaknesses:  My major concern is that this work is very similar to the recent ICML 2021 paper "Neural Transformation Learning for Deep Anomaly Detection Beyond Images", which aims to learn the appropriate masking strategy for tabular data; this is in some sense a more general formulation compared to the fixed masking strategy used for contrastive learning here. I think the paper is a clear accept now.<|endoftext|>Authors provide very complete baselines and also compare in other datasets, which greatly reduces the concern that the proposed rule overfits to the ODDS benchmark. ## SummaryThis paper learns a contrastive representation that helps differentiate between normal and abnormal data in the tabular data. I am very surprised this method performs so well, since this k sliding window does not seem to make sense for tabular data since heterogeneous types of features exist (e.g.discrete, continuous or skewed distribution) and the order of features should not matter and thus the sliding window approach should not work unlike images. But my concern is much relieved after seeing the complete set of experiments and the stability analysis, as well as some limitations the authors admit in the discussion. Since it s in the tabular data, I hope the authors can compare with long lasted traditional baselines such as RRCF and KNN in ODDS benchmark (authors can use the default hyperparameter in the sklearn). This might help improve the model to learn better representation based on index j without learning independent model for each j. The text is quite confusing right now. I think the claim should be changed to "a default rule of hyperparameter selections performs well".<|endoftext|>  The authors propose a novel anomaly detection framework that can be useful for tabular data. The authors utilize the contrastive learning to learn the relationship between features in the tabular data. 1.Assumption  In the introduction, the authors assume that the feature relationship is class dependent. It is actually my biggest concern on this paper. Please see my detailed comments above. Some additional baselines are needed to complete the experiments. Some concerns (e.g., assumptions, consecutive features, and additional baselines) are resolved. Maybe we can set the negative pair as a_m^j where (m is not j). There are some rules of thumbs in page 4. Although other reviewers are leaning to acceptance, I will stay with my original scores (5) until those above concerns are resolved.<|endoftext|>This paper proposes a contrastive learning method for unsupervised anomaly detection on general multivariate (tabular) data. *Pros*+ Anomaly detection on general multivariate data (where few prior knowledge is available) is an important problem with many applications (fraud detection, etc.) that is relevant to the community. + The paper is overall structured well and easy to follow. There is not much of an explanation or intuition as to *why* the proposed method seems to work well. Learning and evaluating representations for deep one class classification. For this reason, I am leaning towards accepting this work, though I have some remaining questions and concerns (see above).
Reject; rating score: 3; rating score: 8; rating score: 8; This paper describes a learned similarity metric for spatial scalar and vector fields obtained from physics simulations. The metric is like a perceptual loss but for physical fields. The training is self supervised by comparing one parameter families of fields obtained by modifying initial conditions or restricting or resampling. If the authors propose their metric as superior, why does the method rely on PCC as part of its construction of the ground truth? And if this approach would be better, why not use it?<|endoftext|>A new similarity model based on the entropy of a physical system is proposed, which in my opinion is the paper s major contribution. Strengths:The paper is very well written and easy to follow. Weaknesses:Overall I m positive about this paper. But there re still some aspects of the method that I hope the authors could clarify in the rebuttal. The paper presents an iterative method for determining the simulation step \Delta. This limits the method to be only applicable to comparing data of a known physical process.<|endoftext|>A formulation of a similarity measure between a reference state and any other microstate in a dissipative physical system is presented. This similarity cannot be used as a metric as is, but can be used to train a Siamese CNN (VolSiM) that is introduced in this paper. The authors formulate a learning task by generating a sequence of physical systems. The literature review nicely places the proposed work within its field. The argumentations in the paper are well funded and design choices motivated. Regarding the equivariance of the proposed CNN. I think the semi supervised approach to learn a metric for comparing simulation or measurement data based on the proposed similarity model is useful for a wide range of applications and believe the authors also present a nice line of argumentation for design choices which are applicable to other tasks, hence I recommend accepting the submission.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; By stacking multiple layers of these  monotonic continuous conditional swaps  the authors obtain a sorting network for which all outputs are monotonic in all inputs. It is obviously biased but it provides the correct direction, so in a sense is similar to monotonic differentiable sorting?<|endoftext|>The conclusions on using well suited sigmoid functions are interesting and relevant to the body of work on differentiable sorting networks. The paper presents an overview of differentiable sorting networks and the construction of soft min/max functions used in the swap operator.<|endoftext|>This paper introduces monotic differentiable sinkhorn networks. It would be good if authors can elaborate on broader impact of their work. Nice contribution.<|endoftext|>This paper presents monotonic differentiable sorting networks. Then the paper provides a necessary condition for monotonicity and study bounds on the error of continuous conditional swaps.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; This paper aims to enable effective meta RL in the offline setting. The problem this paper aims to address is practically important for RL, that is, how to effectively utilize offline datasets to meta train policies and enable them for fast adaptation to new tasks. 2.This paper identifies a specific problem to offline Meta RL : distribution shift of the exploration policy for collecting data for adaption, which looks interesting. However, synthetic reward learning is not novel and looks straightforward. How will SMAC perform in settings where tasks may have different dynamics? 4.From the experiments, the outperformance of SMAC is mainly due to the unsupervised online data collection, which may not be a fair comparison to baselines. 5.This paper only evaluates the proposed method in three scenarios. This paper is generally well written. The experimental evaluation also needs to be improved by including more settings.<|endoftext|>The policy is conditioned on the task feature z, so it is important that the transitions collected by the exploration policy are informative enough to extract the feature information. This paper proposes an additional online training process (without the reward info) to fix the issue of "distribution shift in z space". This paper is well written and easy to understand. Weaknesses:The problem of "distribution shift in z space" might require deeper discussion. In the online training process, is the distribution of tasks the same as tasks in the offline training dataset? Is it a fair comparison? Is it possible to demonstrate the advantage of the proposed method on standard offline meta RL benchmarks (e.g.D4RL)?The results on standard benchmarks with commonly used offline datasets will be more convincing.<|endoftext|>This paper introduces a reward prediction module to produce reward labels in a meta RL setting. Here the offline RL part requires reward labels while the online portion can be done in a self supervised manner by generating reward labels using the offline trained module. + Related work is well covered. For example, in a set of tasks how different can any two tasks, is the difference at the level of one being picking and one being opening a drawer or is the difference at the level of different pick locations in a pick task. If the reward decoder is not well trained or reliable then wouldn t the bad labels skew the learning in the wrong direction? The idea is simple and works on the problems tested. The larger applicability and constraints of the approach are not clear.<|endoftext|>To this end, the paper proposes a framework that is robust to the distributional shift by introducing a self supervised learning phase. In this phase, the proposed framework leverages online data without reward labels and generates synthetic reward labels for it based on the labeled offline data, allowing for smoother adaptation. I believe this work tackles an interesting and promising research direction (i.e.alleviate the distributional shift issue in offline meta RL) and proposes a convincing framework to address it. **Novelty**  I believe the introduced online self supervised learning phase is novel. Approach, notations, and figures are well explained. **Experimental results**  The presentation of the experimental results is clear. I wonder if the proposed framework works better partially because of the suboptimality of this dataset. **Online interactions**The proposed method needs additional online interactions compared to offline meta RL prior works.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; The motif based heterogeneous graph construction method proposed in this paper is interesting, but the experimental results are difficult to prove the effectiveness of the method. The paper uses graph neural networks to learn and combine atom level and motif level graph feature representations, so as to improve the representation of molecules. At the same time, the paper did not give a detailed explanation on why GIN was chosen as the feature representation of the learning graph, and from the results in Table 1, GIN does not have obvious advantages compared with other methods.<|endoftext|>This paper propose the Heterogeneous Motif Graph Neural Network (HM GNN) which is based on a motif level graph representation that takes into account commonly occurring motifs like rings in molecules. The core strength of the paper is the strong experimental results. Overall, I will say that this is a paper with a lot of potential, and many of the ideas are likely to be useful in graph machine learning and molecular representation learning in general. As of right now, this paper is not in publishable state. After some revision however and some improvement in their exposition, I believe this can be a good paper.<|endoftext|>The learned features are concatenated with the molecule feature learned from a traditional atom level graph neural networks and fed into an MLP for property prediction. The experimental results show that the proposed method can improve the performance significantly;3. In summary, I think the idea of using motif in molecule representation learning is interesting, but the propose method has a lot of design issues that are not well justified or well explained. Moreover, the proposed method is not tested on commonly used datasets. Overall, I think this paper is below the borderline and I tend to reject this paper. The authors do not test their method on some datasets that are commonly used for molecule property prediction, which makes it hard to comprehensively compare their method with the literature;6. Moreover, it is also interesting to investigate if there is any theoretical relationship between the two graphs in terms of running GNN on them. Therefore, it is always a trade off, which should be discussed in the paper.<|endoftext|>The paper is well organized. In other words, is sharing a motif a clear indication that two molecules share common properties? For example, the sizes of the feature vectors of a motif node and a molecular node should be different, and how they are put together for message passing is not clear. How efficient is HM GNN compared with GIN in terms of memory consumption? The edge sampler seems to be not much different from what is done in GraphSAGE. Overall, I think this paper is at the borderline.
Reject; rating score: 5; rating score: 6; rating score: 6; This paper presents theoretical results to explain the effectiveness of contrastive learning (CL). Numerical experiments validate the theoretical finds. Overall, I appreciate the plentiful theoretical analysis in this paper. However, some concerns are preventing its acceptance. And it is good to see that the upper bound of CL is lower than the lower bound of AE (with the increase of d and n). The authors can further elaborate on the difference or merit of their error bound compared with those existing works. Thus it does not really explain the effectiveness of the existing supervised CL model. Actually, we know that the supervised CL can also be regarded as a (supervised) similarity metric learning problem that has been widely studied in both theoretical and experimental aspects.<|endoftext|>This paper studies the generative power of contrastive learning from a theoretical perspective. To enable the analysis, this paper considers the linear model with random masking augmentation. Training data are assumed to be generated by the spiked covariance model. I have some detailed questions about the comparison between autoencoder and contrastive learning (Thm 3.2 and 3.3). Does $c $ in Thm 3.4 depend on $r$ or $r_c$? If so, can we say that $c $ is worse than the bias term (first term) in Thm 3.3? Minor comment: The appendix is not well organized. Overall, I think this paper is good and interesting. My main concern is 1) whether the comparison between autoencoder and contrastive learning is fair, and 2) how much insight can be extended to the more realistic scenarios.<|endoftext|>In such a setting, the following results are shown theoretically (a) contrastive learning with a particular data augmentation can learn much better feature than an autoencoder by virtue of learning the underlying low rank signal as features, (b) supervised contrastive learning can do better than contrastive learning by getting rid some bias that data augmentation introduces, (c) for transfer learning, a combination of unlabeled contrastive loss and supervised learning can do better than either of them individually. **Weaknesses**While the simplicity of the setting is not a huge weakness, it certainly raises the question of relevance of the analysis to practice. One way to justify the setting would be to experimentally demonstrate relevance of some of the theoretical components on some real benchmark datasets. Could this have any connection to its superiority in practice? Overall the theoretical results are quite interesting, for a very relevant problem of contrastive learning. The main negative point is about the relevance of the results to more practical settings.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 3; The paper mainly investigates the effect of permutation in the class label assignment in the tasks for the MAML algorithm. First, the authors show that MAML requires a higher number of inner loop updates than what is commonly used. Then, they show that MAML is sensible to the permutation of the class labels in the tasks and experimented with diverse methods to alleviate this problem. I never saw performance as high in other papers using MAML with a ResNet12. Maybe the difference come from the pretrained weights ? All the experiments are detailed as well as the thought process behind them. I would appreciate more information on the full training process to improve reproducibility.<|endoftext|>It shows that both the inner update iterations and task label assignment have a clear influence on the performance of MAML. The paper is interesting for me. 3.MAML was implemented originally not only on few shot classification tasks but also on reinforcement learning tasks. While it is clear that this new UNICORN MAML may not be straightforwardly used for RL, as it studies the specific problems in the image classification settings. I have a few concerns about the significance of this current version of the paper because of the poor performance or the arbitrary design of experiments. I would like to see the comments of other reviewers and the discussion with the authors to make a final decision.<|endoftext|>First, they study the number of inner gradient steps. However, it seems that during testing this could lead to different test performances on a target task. They performed very systematic and reasonable experiments. The second part of the paper that looks at the permutation of the labels is interesting. [2] Snell J, Swersky K, Zemel R. Prototypical networks for few shot learning. It is a good improvement on MAML that is based on a very detailed systematic study.<|endoftext|>However, I have some concerns that needs to be addressed:     In equation (1), the gradient is taken with respect to the $\theta^{‘}$ as we are in the inner loop? How do you relate this finding to your intention in this work? Table 6 does not provide a fair comparison with current algorithms for cross domain few shot classification. My *major concern* is about your motivation for this study. It improves the performance of the MAML, however I have some serious concerns regarding the importance, the validity of the motivation and the fairness of the results for this study. So, you need to change your motivating example and make it more general. The procedure to produce the results in figure 3 is not clear to me. Please elaborate on this.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; This paper proposes a function space variational inference method for classification tasks. Pros:  The paper is well organized and easy to follow. Intuitively the ideal posterior predictive Dirichlet mean at $x_A$ and $x_B$ should be $(1/10, \cdots, 1/10)$, but the predictive Dirichlet precision should be different since we see more data at $x_A$. In summary, I raise several concerns about the theoretical soundness of the proposed method, which currently prevents me from rating this paper higher. I will consider raising my evaluation if the authors address my concerns or point out my misunderstandings.<|endoftext|>As such, while I still like the paper, I cannot clearly recommend acceptance because I would have these questions open as it stands. I suggest working on fixing these aspects to have a more complete version of this manuscript and hope to see it again as a reviewer when that is the case so I can strongly support it. As the evaluation currently stands, I understand that this works compared to weight space VI, but not how it works compared to the methods in Sec.A with various Bayesian models. 3.In the experiments, the authors repeatedly point out that in CIFAR 100 weight based models do better due to the strong regularisation effect of the Dirichlet prior used here when having this many classes. I appreciate that insight and hypothesis, but could we please test it? I appreciate that the authors claim it still works, but it would be instructive to get a better sense of this.<|endoftext|>The paper introduces a function space variational inference algorithm ("fVI") that uses a Dirichlet predictive prior, and approximates the output of a (weight space Bayesian) neural network as a Dirichlet distribution. Do the authors have some insight as to why it is not helping in distribution? My main review outlines my biggest issues (especially points 1 4), these tend to be about the practical implementation not aligning with what I view as the biggest benefits of the theory (and the biggest differences to past works), as well as questions about experimental results. Post rebuttal: Please see discussion. I am sticking with my score for now, despite some of my initial concerns remaining (and additional valide points raised by Reviewer rF5E). I like this idea.<|endoftext|>I find the claim that this work provides "a unifying view of prior work which use the Dirichlet distribution and function space regularization" to be a slight over statement. A method that combines concepts from two areas is not necessary a unifying theory thereof. Given the technical quality of the paper, I am leaning toward recommending acceptance of the paper. I am not fully convinced about the results shown in Section 4.4, in particular in Figure 5 for CIFAR100 where the accuracy and log likelihoods are not distinctly better across corruption levels, particularly with Ensembles.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; rating score: 5; This paper proposes a method for noise removal from the FIBSEM images. The paper proposes a network architecture and loss functions for this purpose. The approach has been tested on two real datasets. + An unsupervised technique for denoising is also highly desirable. The SEM and TEM imaging approaches, that are widely popular because their capability to record larger tissue volume, has a z resolution of approx ~30nm. There is also some inconsistency in the statements and figures.<|endoftext|>Novelty: The method proposed in this paper is a variation of Noise2Noise. Moreover, the paper is rather limited in its application (is specifically designed for FIB SEM images) and is unclear if it can be extended for any other domains. 3.Baselines: The proposed method has not been compared to many important baselines. The paper misses comparisons against many important and recent unsupervised denoising works which are directly applicable for the presented application domain without needing any modifications (see my comment in weaknesses).<|endoftext|>The paper proposes a neural network architecture (NRRN) implementing a variant of the noise2noise scheme, and applies this to section triplets of a FIB SEM stack. The denoising results are compared to training free methods, as well as supervised baselines. What is the total number of images and Mpx contained in the dataset?<|endoftext|>This paper proposes an unsupervised denoising convolutional neural network model for Focused Ion Beam Scanning Electron Microscopy (FIB SEM) images. The results are demonstrated on two datasets, one public and one private, which outperformed both traditional and learning based baselines. The authors clearly demonstrate their expertise in FIB SEM imaging. Weaknesses: Although this biomedical application is meaningful, the novelty behind this method is limited especially for a major ML conference. However, the novelty is limited and more experiments should be performed to show the utility and advantage of the proposed method.<|endoftext|>The authors back this up with state of the art results on a real and a noisy simulated dataset. `NRRN` is a novel approach to address the challenging application of denoising *FIB SEM* images.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; This paper proposed to train several parallel sub networks in the slimmable networks framework, which could run in parallel on different devices subject to runtime hardware configurations to reduce latency. * Strength: Similar to slimmable networks, this approach could train all the networks at the same time and retraining is not necessary when deploying. * Reasons to accept:  * The parallel distribution to speed up model deployment can help reduce latency. Is the computation done in a mixed serial parallel style, or is the computation repeated? However, the improvement has not been well empirically evaluated and justified. The main discussed scenario is distributed embedded computing, but important topics in this scenario like mapping between different traits of sub network and the capabilities of devices, has not been covered.<|endoftext|>The authors propose ParaDis, a parallel and distributed version of universally slimmable networks. How do these other types compare to ParaDis in terms of accuracy at different latencies? Weaknesses  The major evaluation figures seem to not be highlighting the correct aspects of the method. The paper needs more baseline techniques, more model architectures, and larger parallelization factors for me to fully support acceptance. This is shown in the figures. Then, the comparison to US networks could either show improvements over previous work, or equality with improvements in latency or other aspects. This isn t directly captured by the FLOPS. Given the relatively poor single configuration results on some switches, these data points may want to be run multiple times and listed with error bars. I assume this was to demonstrate that it doesn t have to be explicitly trained to perform well, as long as its submodels are trained in other switches.<|endoftext|>The paper proposes ParaDiS, a slimmable network that can be executed across multiple devices, and be transferred across different devices without re training. To train ParaDis, the authors propose to distill from a wider network, and combine activation knowledge distillation to improve the performance. While the idea of parallel execution is promising, there is little detailed discussion on that point. 2.The paper is well written and easy to follow. Methodologically there is no fundamental differences from slimmable networks. The paper proposes 1) distilling from wider networks and 2) distilling feature maps, both of which are not new. 3.No simulation results on parallel execution of ParaDis. From to Figure 1, is it necessary that all neurons in the first width configuration are fully connected (in terms of the performance)? 2.Figure 4: what about Slimmable and Slimmable Wide IPKD for MobileNet experiments? 3.What is the training cost compared with vanilla training or US training, if all switches are enumerated in each iteration?<|endoftext|>The paper proposed an algorithm that distill from a larger teacher to a set of parallely distributable student models each of which only compute a fraction of channels in the network. The combined output of the student models can be used to compute the final class prediction. 3.The practice of distilling a bigger teacher to the student model is well established in the literature (like noisy student). A careful study of distillation algorithm for the mobile/edge device deployment. The whole paper is well written with experimental results supporting the main contribution claimed by the authors. However, the significance of this approach is a bit limited due to the scalability issue (only works with convolutional models and  a very small set of switches.)
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; This work tackles the problem of unsupervised dynamics generalisation in model based reinforcement learning, improving approach introduced by additional of inferring a latent context which conditions the dynamics model and captures the variation in dynamics between environments. They introduce a set of auxiliary losses based on relational intervention and causal reasoning to encourage the inferred context to be the same in trajectories from the same environment (even when the environment identification is unknown) through inferring which environments are the same and weighting the context similarity loss by this environment similarity. The method shows improved prediction error and test reward on a range of continuous control tasks compared to SOTA baselines from MBRL, Meta RL and dynamics generalisation MBRL specifically. Visualisations qualitatively show that their method clusters transition segments from the same trajectory together better than previous methods, which could explain their improved performance. The method is interesting and novel, and seems well designed to tackle the problem being addressed  The empirical results and visualisations both support the claims that this method is performing better than previous SOTA methods due to the better clustering of contexts. There s not much that can be done for this, unless additional benchmarks were created and assessed on (which is clearly a lot of work). Is from different subsequences of transitions from a single episode, or multiple episodes, or both? Also, the explanation of the method is sometimes difficult to understand.<|endoftext|>Previous methods learn to predict a vector $Z$ that characterizes a particular environment dynamics from past transitions. However, as the environment id or label is not available, this vector inevitably contains redundant information, which might hurt the generalization of the model. The paper therefore proposes an interventional approach to estimate the probability that two vectors $\hat{z}_i$ and $\hat{z}_j$ belong to the same environment, and then uses a relational head to force similarity between them. While other works have used similar ideas to characterize the environment dynamics using a vector $Z$, the paper presents an interesting idea to reduce the redundant information in $Z$, or in other words, learn a better representation of $Z$. The improvements of the proposed method are significant compared to the existing baselines.<|endoftext|>Figure 1 shows that a PCA visualization of the learned context vectors using the proposed method separate the context vectors into distinct clusters. In this problem there are a set of train MDPs and a set of test MDPs, all with the same state and action spaces, but with different dynamics functions. The authors propose a loss (equation 6) that pull $Z$ s from the same trajectory together and push away $Z$ s from different trajectories together. This likelihood is used as a weighting coefficient on the loss: the authors use this weighting coefficient as a "soft" indicator variable to distinguish examples from the same environment (positive examples) or examples from different environment (negative examples).<|endoftext|>One of the standard ways to approach the problem is given by inferring a latent variable Z encoding each task and then conditioning the dynamics model on it. The authors propose to encode segments of state action trajectories into Z vectors and maximize similarity between Zs from the same trajectory. The paper compares the method against the baselines across 3 axes: in terms of dynamics prediction error, in terms of returns on testing environments, and in terms of separability of the inferred Z for different environments. NoveltyTo the best of the reviewer’s knowledge, the paper is fairly novel. Would the conclusion that Zs for TMCL are not separable hold for a non linear method for visualization (say, t SNE)?
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 5; After author rebuttal: thank you very much for the detailed response to both my and the other reviewers  comments. This paper proposes an approach to compressing the Transformer family of pretrained language models via tensor decomposition. The approach, when combined with distillation, outperforms existing methods for compressing BERT models on the popular GLUE benchmark. Strengths:  The paper is well written and the motivating analyses (e.g.Fig 1) are interesting. I also appreciated the thorough appendix. The main technical contributions of the paper (i.e.using all the matrices to perform matrix decomposition, using a bank of matrices) is novel to my knowledge. However, I have some reservations about whether this is sound (see weaknesses). However the results only seem to be based on one of the methods (i.e.IV).I realize that some of variants have already been studied in the literature (e.g.Mao et al., Noach and Goldberg), but since the setup is not identical, it is crucial that the proposed approach is compared against both II and III. If not, do you obtain better performance by normalizing the matrices such that they are in the same scale? And it is furthermore not clear that the improvements are coming from the proposed approach as opposed to something else, since an ablation study across the different decomposition methods is missing.<|endoftext|>This paper proposes to use tensor decomposition to compress the multi head attention (MHA) and FFN layers in transformer architecture. Overall, I think this paper is an incremental improvement to previous state of the art. Tucker decomposition has been used extensively in NN to compress RNNs, CNNs and Embeddings. Thus use of tucker decomposition and its ability to compress BERT MHA and FFN layers is incremental improvement over the previous results, especially given the fact that prior work has also shown that MHA and FFN layers can be decomposed in a low rank structure and talked about the redundancy in the parameters in those layers. Proposed Solution:Based on this observation, the authors discuss the merit of various methods for decomposition of MHA and FFN matrices. However, the results of the paper are interesting from an engineering point of view. The results of compression are strong. The use of tensor decomposition for compressing neural networks has been explored extensively for CNNs, RNNs and Embeddings. Decomposability and low rank nature of FFN and MHA layers has been discussed previously in the literature. In order to improve the paper, I recommend providing more insights into the workings of the method and the bias that the fixed structure like tucker decomposition can lead to. Further, I would encourage the authors to explore and understand why finetuning with KD in Table 6 leads to such large accuracy drop. GD+TD should lead to better accuracy, but improving accuracy by 40% is an interesting data point. 2.Table 4 should have comparisons to sparsified BERT, esp for data points with 2 3x parameter compression. Both structured sparsity and random sparsity could achieve said compression.<|endoftext|>This paper explores extreme parameter compression for pre trained language model, especially BERT. The compressed BERT model achieves much smaller size with promising performance. Large scale pre trained language models have demonstrated their effectiveness. However the large model size makes it difficult to deploy and compressing such models have drawn a lot of interest. It introduces several decomposition methods and makes a comprehensive comparison among them from the perspective of compressing Transformer layers. The Tucker decomposition is chosen to be the final solution due to its compression ratio. The experimental results demonstrate the effectiveness of the method. Especially, the compressed model size is really competitive. I know that this can make the size of compressed model really amazing (e.g., 1.8M) and the compression ratio amazing (e.g., 86M/12.3M 7) but is not fair as the whole model including the embedding layer are used when deploying. 3.Some other method(s) are missing in the related works. Section 5.1, "...are not exactly equal to the the raw weights...", duplicate "the"? The paper presents extreme compression on pre trained language models. Though the introduced methods are not new, the adaptation to the Transformer layers and the analysis are interesting, and the experiments are convincing. Though there exist some weaknesses, I think the paper is of good quality, if the authors could mitigate them.<|endoftext|>This paper proposes to use tensor decomposition to jointly compress the model weights in all attention and FFN layers of a Transformer model, which reaches similar performance as the original BERT model while marking the model much smaller. Reasons for score:I think the idea proposed in the paper is novel, but some design choices can be further elaborated and there should be more experiments on larger models and more ablation studies. This is a valuable contribution to the model compression community. Weaknesses:  The paper proposed multiple potential ways of compressing weight matrices (matrix decomposition and tensor train decomposition) as some alternatives to the proposed Tucker decomposition. The paper only performs experiments on BERT base and TinyBERT models, but I believe that the compression method proposed in the paper should be more demanded by larger models. Other comments:  How does this method compare with previous works in terms of training/inference latency? The paper would be better with more experiments on larger models and ablation studies. Also, the presentation and the rationale behind the idea are not clear to me.
Accept (Oral); rating score: 8; rating score: 8; rating score: 8; rating score: 6; There are minor presentation improvements to be done but I think this is a strong paper otherwise. The authors first derive an algorithm that enables learning when $ \omega $ is made endogenous in the case of (weighted) pre training. The authors apply TAWT on 4 NLP tasks, using BERT as their base model. Critically, the authors also show the importance of varying task weights throughout training. Pros:  The method is well motivated. The added theoretical guarantees are welcome. The experimental setup is convincing and the results confirm the soundness of the approach. The appendix is very rich in additional interesting experiments.<|endoftext|>The paper discusses an approach that learns to weight data from different tasks in pretraining or mutli task learning. Finally it describes experiments on a number of NLP problems. It would be useful to note this. This is a minor thing, but I think that was not specified earlier in the paper? An interesting algorithm, theory, and results.<|endoftext|>The paper proposes a new weighted training algorithm, TAWT, to learn the task aware weights on tasks for better using the cross task signals. The paper can bring a new research interest for multi task learning and transfer learning. 2.The used four NLP tasks are closely related. The weighted training is very important but there lacks good work in this direction. Therefore, this paper is great to give an attempt for task aware weighted training.<|endoftext|>They provide both theoretical guarantee and empirical evidence to corroborate the proposed method. Strengths  The proposed method is novel and well motived. The authors provide a theoretical guarantee of the proposed method. Empirical results indicate that the proposed model greatly outperforms the vanilla methods in both pre training and joint learning. Although this work is designed for "cross task learning" instead of "multi task learning", the existing methods can be straightforwardly used to address the same problem. This is a good paper with novel ideas, but it can be further improved by adding more backgrounds, related works, and comparisons with other state of the art methods.
Reject; rating score: 3; rating score: 5; rating score: 8; The paper should be regarded as a theoretical paper. The main results are stability bounds of SGD for convex and nonconvex ERM schemes. In short, I cannot agree with the authors that it is the normalization operation that leads to the results developed in this paper. In addition, in my opinion, the presentation of the paper needs to be greatly improved. However, the definition of normalized loss function was only introduced in Section 4. Based on my comments above, I cannot recommend its acceptance.<|endoftext|>This paper considers the problem of understanding the generalization of SGD using the stability framework. The well known result in this line of work is the paper by Hardt 16. The main observation by the authors in this paper is that in many cases, the loss function is invariant to the scaling of weights. Their main results are the new stability analysis for this new notion for convex and non convex settings. 1  The definition of the stability is for normalized surrogate loss. 2  In general, I find it difficult to interpret the results in this paper. However, the theoretical result is obtained for vanilla SGD.<|endoftext|>This paper provides new generalization bounds for SGD using the stability analysis framework for both convex and non convex cases using a normalized loss function, stability as measured in the form of anglewise stability as supposed to standard euclidean distance. The paper is generally very well written. I feel the theoretical results are strong and the authors provide sufficiant empirical results to validate their theoretical findings.
Accept (Poster); rating score: 6; rating score: 6; rating score: 5; rating score: 5; This paper presents a new framework to learn object centric representations. The idea of using the network weights to represent object centric representations seems new and interesting. 2.All experiments in the manuscript are conducted with synthetic data, which is not that convincing. In the supplementary material, the authors have presented some real world examples. However, it still lacks qualitative results.<|endoftext|>In this paper, the author proposes a novel framework named object pursuit which can continuously learn object centric representations with streaming training data. The author collected data from interactions and the proposed framework can imporve the label efficiency in downstream tasks. This paper has a good formulation. However, there are some small issues that the author should address:1. 2.Somehow the generated dataset is small. I m a little bit worried about applying it to a large scale (real world) task. Some small issues remain but they do not affect the overall quality of the paper.<|endoftext|>This influence is not discussed in the manuscript. Model evaluation:(1) The model is only quantitatively evaluated on a synthetic dataset with a limited number of objects, lacks quantitative evaluation on a real dataset. Writing issue:(1) For equation 1, the symbols for sample and population are not distinguished. Do they refer to types of objects and the number of learned objects? (4) Does the alpha in equation (3) and equation (4) refer to the same alpha? (5) In Section 4.5, (Fig.??)refers to which figure? (6) In section 4.3, table3 should be table5? Does it refer to the number of learned objects? It seems that mu does not exist in the objective term. They were set positive in Training details but were claimed to be negative in section 3.2.<|endoftext|>The experiments study the impact of \tau under different metrics. It is reasonable for setting a \tau in the formulation to identify the unseen objects. As stated in Table 1, "if an object is learned and added to the object list, it will be claimed as seen by Eq.2, and the re identificationaccuracy is always one." It is a bit unclear how this can be extended to real world data where a robot agent needs to obtain the instance segmentation mask through interaction for a large diversity of objects. This paper is a comprehensive pipeline for continuous object learning and presents some valid designs for this specific problem. Some of the evaluation metrics are not very clear to illustrate the model performance.
Reject; rating score: 3; rating score: 3; rating score: 6; rating score: 6; (xi) It is not clear, how Assumption 3 on page 15 relates to the universal approximation theorem (as stated below), which deals with uniform approximation of a continuous function on a compactum rather than convergence of gradient descent. The gradient of such loss (for a mini batch of observations) w.r.t.to the neural network parameters, coined physical gradient, can then be used to optimize the neural network by standard variants of gradient descent. (xiv) Typos: *an* generic inverse solver; *we* found that second order optimizersThe paper proposes an interesting adaptation to the supervised approach of solving inverse problems by means of deep learning. In addition, there are several issues regarding the presentation of the material. The former, however, would drastically slow down the neural network training when using physical gradients as the iterative solver needs to be applied in every optimization step and the latter is not explored in the paper. **Empirical evidence:**    Unfortunately, the numerical experiments do not showcase the potential strengths of the proposed method. While a crucial part of this method is the initialization of the iterative solver using the current network prediction, this seems to only be used in the first toy example. However, for all but the last example, an explicit solution is used instead of an iterative solver and there is no numerical evidence of how the method depends on the accuracy of the iterative solver. However, apart from the toy example in Figure 7, it seems that there is not enough theoretical or empirical evidence for that statement. (iii) I guess that $\mathcal{P}^{ 1}_{n^*}(y^* | x_0)$ should, in general, not be equal to $x^*$ as stated in the introduction of Subsection 2.1. It should probably only be an approximation of the latter. (v) It seems that the derivative of the forward operator is missing in the update rules in Figure 1 and also the expression for the Hessian given in the caption should be derived in more detail. (ix) The first example does not fit into the framework described in Equations (1) and (2) as now $\mathcal{P}$ is parametrized by $a$ and $y^*$ is fixed.<|endoftext|>The paper proposed a new update method for neural network solving inverse problem by incorporating the physical information as prior into the gradient computation step. It leads to faster convergence and better convergence performance. I am happy to adjust my scores when the concerns are addressed and there is a high chance that I did not understand vital parts of this paper. The inverse problems are usually framed into two different frameworks:1. This is consistent with the paper s equation (1). However, this is not what this physics gradient is meant for. 2.Mapping learning framework where we learn (usually using NN) either a probabilistic mapping or deterministic mapping from target y space to the control parameter space x. This seems to be what the authors are trying to illustrate from Equation (2). Secondly, I didn t quite understand the assumption of having an inverse problem solver P^{ 1} in the first place. If we have a numerical inverse problem solver, why do we need to use a neural network to solve the inverse problem anyway? The assumption of having access to the forward numerical process P is ok, but the access to an oracle P^{ 1} really made me struggle to understand. This is the biggest concern I have, this method proposes to solve inverse problems using neural networks, but it needs access to a numerical inverse solver from domain knowledge, which seems to beats the whole purpose of using neural networks. However, I would assume that the numerical inverse solver P^{ 1} would take a much longer time in wall clock sense than a neural network update. My apology if I missed crucial parts of this paper, but I did spend some time on it and still struggle to understand things I mentioned above. If the above concerns are ultimately addressed by the rebuttal, the clarity of the delivery would need a lot of improvement.<|endoftext|>This paper looks at the (inverse) problem of finding the some initial state given a final state of a physical system. It proposes training neural networks to predict the initial state for a given final state. These networks are trained iteratively by using inverse solvers (based on domain knowledge) that provide physical "targets" that are closer to the desired initial state than the current prediction. The case n 1 used is probably the most interesting, because the inverse step is indeed simply providing a slightly improved target to supervise the training of the network, without the need of labeled data and without having to run a full inverse solve. The experiments performed, though on fairly simple domains, demonstrate that the method works and is able to generalize at least in the given domains (ie interpolate), given that data is sampled randomly. As with many of these types of applications of machine learning to physical inference, it is questionable if the benefit of the fully trained model achieved at the end compensates the cost of training it for a long time on a limited domain, while also having to provide an already existing inverse solver (or something capable of approximating this inverse). It would be good for the authors could discuss further what use cases they foresee for this type of model in practice, where the costs of training and formulating a known inverse could outweight simply using a traditional methods. Despite some of the comments above, I believe the conceptual contribution and the experiments presented in this paper are of enough interest and robustness to warrant its acceptance.<|endoftext|>However, there are significant issues with overselling, substantiation of claims, and clarity that mean I cannot recommend acceptance of this paper in its current form. The crux of this approach involves rewriting the training loss function to capture the difference between (a) the NN s estimate of the problem input and (b) the solution obtained when using this estimate as an initial point for an approximate inverse problem solver. This paper proposes an approach for learning neural network based approximators for inverse problems in physical domains. This approach is demonstrated on several physical systems, and shown to avoid issues due to multi modality (which supervised training schemes fall prey to), instability in the gradients through the physical process (faced in some settings by first order training methods applied to the "standard" inverse problem), and issues such as expense of computation or ill conditioning (faced in some settings by training method using e.g.inverse Hessian information). Instead, the original inverse problem is rewitten such at the gradients that "fall out" have physical intuition. However, $\mathcal{P}_n^{ 1}$ is just a single step of gradient descent, and while this is mentioned, it is not emphasized. Notably, all experiments use this "single step of gradient descent" interpretation, and the physical interpretation of the resultant gradient depends on this as well. This point should be made much clearer. * Relatedly, the abstract indicates that the training approach "combines higher order optimization methods with machine learning techniques." To the best of my knowledge, this does not seem to be true. * Certain claims made in the paper are not fully substantiated. (Also, the last paragraph of page 4 mentions that x_n can be taken as a constant   this is, importantly, not the case, but requires digging into Appendix A.1 in order to understand.) * Above equation 3: The authors might consider using a different subscript than "sup." Presumably this is meant to stand for "supervised," but could be confused for "supremum" given the optimization setting. In addition, interpretation of the legend in e.g.Figure 2 should be made more self contained. * In Figure 3, it is not clear what the x axis is.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; One of the confusions I had as a reader: The authors talk a lot about task synergy and competition (e.g., sections 2.2 and 2.3, theorem 2, figure 3, etc.) I would appreciate a bit more discussion about this. If not, how is the preceding discussion relevant? I think this is an excellent paper.<|endoftext|>I would consider increasing my score if these issues are largely addressed with revisions. Could it not be that, to paraphrase Tolstoy, each unhappy task is unhappy in its own way? "iCaRL: Incremental classifier and representation learning." The definition of forward transfer is unconventional in that is simply the “accuracy on a new task when it is first seen”. None of the competing methods are reimplemented   all results are taken from the original papers or other ones that have reimplemented them.<|endoftext|>This could be a great paper. That is fine, as long as the setup is clear from the start, and the tone of the paper reflects the reality of results, i.e.a simpler problem can be solved better than much harder problems. I will (reluctantly) not oppose publication close to current form, but I cannot confidently recommend acceptance before the writing is improved. The authors describe different viewpoints as an “intellectual gap” in research, which is not very helpful. The paper does not discuss this, or report such results.<|endoftext|>I am happy to engage in a discussion with the authors, to see whether my concerns can be taken away or whether the paper could be improved. I could not find any description of this in the paper, and also in the submitted code there does not seem to be any code for these other methods. This seems problematic, as the mentioned publications use different architectures and optimization methods. I think it would be good to either modify the title, or to justify and discuss the reasons for this comparison in the text.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; This paper computes/estimates the mean and higher moments of the length distortion of fully connected ReLU networks at initialization, assuming typical random initialization of the weights. In particular, it shows that length distortion does not grow with the depth of the network (as previously believed). The paper also presents analog results for the average distortion of higher dimensional volumes. This paper is well written and addresses an important problem using very careful derivations. It is a welcome addition to the literature, since it was not obvious that typical network initialization yields a non explosion of length distortion as the network grows deeper. The following are minor comments that I have. Otherwise the caption can be confused with the main text. Similarly, Figure 3 is far from the place where it is actually referenced.<|endoftext|>The work improves on (and corrects) related existing results for random neural networks. This existing work concludes that the length distortion grows exponentially with depth, but 1) depending on the weight normalization it is straightforward to see that this may happen; one should thus choose the standard 2/fan in variance for initialization, 2) it contains a bug which makes it seem that even for the standard He initialization the length distortion explodes with increasing depth. "—one could imagine that a network does something really bad to a curve while keeping its length intact. The results make use of estimates from Giryes et al.(2016) and Hanin & Rolnick (2018) for random ReLU networks. Perhaps first define the tangent space. The formal results and the proofs are as far as I can tell correct and they agree perfectly with the numerical simulations. The paper is very well written and easy to follow. In the proof of Lemma C.3, should the integral in the first display be over $B$? This is a very well written paper about an important topic which clears up some confusion caused by bugs in prior work. The arguments are clear and simple and likely to be useful to other researchers. I therefore recommend acceptance. While the result for curves ($d   1$) gives (implies) matching upper and lower bounds on distortion, the result for general $d$ dimensional volumes is only an upper bound. What is the significance of the bound on $m$ in terms of the layer widths?<|endoftext|>This paper presents results on the expected trajectory length of the outputs of a neural network at initialization with respect to the trajectory length of the inputs. In addition, the authors point out at a missing coefficient in prior results on trajectory length that lead to very different conclusions; and they also provide results on higher moments of trajectory length as well as on the multi dimensional case of expected volume. # Specific commentsIn Section 2, when you say that "it has been shown that it is possible to set the weights of a deep ReLU network such that the number of linear regions computed by the network grows exponentially in the depth", you should also mention negative results in depth for linear regions such as the depth width tradeoff discussed in [1]. As you discuss looking into more detail on convolutional networks, perhaps it would be worth citing related work on linear regions for CNNs [3].<|endoftext|>The paper s main focus is on the following question: "How the length of the output of a NN is distorted with its input length". We would like to note that this is a standard initialization that appears in the literature and implementation packages. The authors also provide bounds for higher moments and prove similar results for higher dimensional manifolds (instead of 1 dimensional curves). The bound is roughly the ratio of the number of neurons of last and first layer and actually the length does not explode as the number of layers grows. This fact is claimed to be surprising by the authors (though I disagree, see my comments below). The problem is interesting and the authors have motivated it well; However the results are far from surprising or novel. and then use independence. What seems quite frustrating is that the authors mention in the abstract "it is widely believed that length grows exponentially in network depth". Moreover, I would like to note that even the result about distortion on manifolds may seem novel, it is basically reduced to the 1 dimensional case in a straightforward manner. Distortion on Manifolds and higher moment analysis makes the paper weak accept if my comments are addressed.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 3; The authors consider Navier Stokes equation in 2D and the 1D Burger s equation. Missing references in the introduction  Open problems in the field are not discussed and motivation for the proposed model is missing. Poor English throughout the paper and generally a poor presentation. The author should care to explain what is the output of the network. The proposed model may have some selling points to it, but the presentationis so poor and so many details are missing that it is impossible to tell.<|endoftext|>In addition to these technical unclearnesses, my main concern is the novelty of mixing multiple popular topics. I believe something significant will happen in this particular overlap but not in a "1+1 2" form. The author did not provide any explanation on the intuition behind refactorizing the channel and spatial dimensions. Is it for memory efficiency?<|endoftext|>It is not relevant to understand other parts of the paper. Hence, the research question that the authors address is of interest. The authors introduce a new architecture for PINNs, which can be seen to be novel up to a certain extend. It is not clear to follow the logic and what the motivation for the design of these modules is, in the specific context. Also, no research code is provided to reproduce or better understand the proposed architecture. The overall quality of this paper is below the acceptance threshold for ICLR.<|endoftext|>6.On page 7, Table 1 shows the performance comparison of the proposed network with previous state of the art. 2.The definition of t_u_i is given right after Eq.(18), but it is not used in that part. I would not recommend this paper to be published in the current form. should be explained respectively instead of as a whole.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; rating score: 6; In this paper, the authors propose an MDN based adaptation model to e2e communication system. Specifically, it transforms the inputs to the decoder such that their class conditional distributions are close to that of the source domain. Strength:(1)	The paper proposes a new MDN based adaptation model for e2e communication system. (2)	The proposed method is verified by both synthetic and real world datasets. (4)	Diagonal covariance implies the inputs are independent, which is a quite strong assumption, it is necessary to work on the general covariance case. This requires the data size of the source and target to be the same. Is the source data size reduced to match the small number of target data points.<|endoftext|>The paper considers an application of deep learning to communication systems. Due to the nature of deep neural networks that always learn to work on training distribution, the current deep learning based methods often perform badly when there is a domain shift between training and testing data. This method is well suited for the communication setting, where the distribution of target data changes rapidly (e.g., a wireless channel), making it challenging to collect a large number of samples and retrain. The authors propose a method for adapting the auto encoder without modifying the encoder and decoder neural networks, and adapting only the MDN model of the channel. Weaknesses  On the theory side: the proposed domain adaptation method involves maximizing a regularized log likelihood and is optimized using known methods such as BFGS quasi newton methods.<|endoftext|>This method is specialized for e2e learning for a communication system using an auto encoder. They evaluate the performance of the model on several adaptation scenarios in wireless communication and show a good advantage over baselines when enough target samples are not available. Their proposed method is simple, but shows solid gain over baselines when target samples are limited. It was confusing. The work may have high importance in the application to wireless communication, but lacks discussions related to existing domain adaptation work and comparison to them.<|endoftext|>The paper includes results on simulated and real data. However, I have some reservations. What is the focus of the paper? The authors assume that the source and target distribution have the same number of components with a one to one correspondence. It would be interesting to include cases in which the domain adaptation fails.<|endoftext|>This paper proposes an end to end learning framework for communication systems using autoencoders. 1.The authors only mentioned adapting the MDN model in the abstract and introduction. But they have the same problem here, they only have a tiny dataset to train the adaptation layer, why overfitting will not happen if they only have such a small dataset? But collecting the dataset too frequently will leave not enough time to transmit actual data. The dependency of lambda choice on the source target distribution pair makes the proposed solution very impractical. There are existing solutions that retrain the entire MDN.
Accept (Poster); rating score: 8; rating score: 8; rating score: 5; The authors propose replacing the diagonal gaussian prior in a variational autoencoder with an Ornstein Uhlenbeck process for fitting a generative model to biological sequences. They they apply their approach to ancestral sequence reconstruction. Pros:  I think the authors approach is statistically well grounded and applied to the appropriate problem. I think it is interesting that some of the other approaches can t even run with the size of the multiple sequence alignment and sequences! Could the pairwise distance of latent variables be compared to the distance when fitting a proper phylogenetic tree? Since these sequences are aligned (thus all the same length), why does a GRU decoder need to be used? Their approach is statistically well grounded, and they apply their model to a relevant, difficult task in biology and improve upon it.<|endoftext|>The authors introduce a VAE for modeling individual protein families that incorporates phylogenetic trees through an OU process on latent space. They also use a sequence likelihood which does not factorize over positions. The authors claim these two advances represent a more expressive and efficient model of protein evolution and apply it to ancestral sequence reconstruction. Strengths:  The technical novelty of relaxing independent sites is interesting and important. The use of a tree structured OU process over latent space is novel and natural for this problem setting. The model is thoughtfully implemented and presented, but the evaluations do not enable strong conclusions to be reached yet. More careful ablations and comparisons are needed to understand implications of this work for scalability, representation learning, and evolutionary modeling. The exposition of the model itself is easy to follow and well written.<|endoftext|>The paper develops a new generative model for sequences using Variational AutoEncoders (VAEs) with the key difference that the latent dimension is modeled as a tree structured Ornstein Uhlenbeck (OU) process which captures the phylogenetic tree of the sequences. A more quantitive analysis similar to Figure 3 would improve the paper. Therefore, it can be the case that the latent dimension learned in the proposed VAEs provide some evolutionary information beyond the usual phylogenetic trees. How easy is it to construct the tree? Overall, the paper tries to address an important and interesting problem in learning generative models of homologous sequences informed by their phylogenetic structure. However, the results in particular are not convincing enough to grant acceptance.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 6; This paper compares methods by which to assess the value of data points for classification. However, insufficient analysis of surprising findings and a lack of comparison to a well established method that assesses the value of data points (SVMs) weaken the contribution of this paper significantly.<|endoftext|>Yet, the main concern is that the paper lacks formal justification and the experiments are done in a synthetic setting. The paper presents a comprehensive overview of data valuation ideas. [post rebuttal] Thanks to the authors for the response.<|endoftext|>There are many more experiments mentioned there, but it makes it very hard to follow the paper. The intuition and the methodology that the authors are using are in the right direction, but it is half baked.<|endoftext|>Experiments are convincing and supporting the authors  claims, although limited to a single model type and simple 2d synthetic dataset. Throughout a suite of experiments by MLP in different setups they found that most methods agree on miss classified points to be important.
Accept (Oral); rating score: 8; rating score: 6; rating score: 3; rating score: 3; Operators are key elements in deep learning (TensorFlow, PyTorch, etc) or scientific computing (NumPy) frameworks. ** First of all, the paper provides a handful of case studies with insightful analysis onuser experience on the existing design of operators, and then based on those studies it derives a seriesof notations for highly user friendly operator design. The paper also explores some techniques like caching to reduce the execution overloadof runtime dispatching. The typed operators, instead of typed tensors (e.g.NamedTensor),provides the guarantee that users only need to type important part of their code as a drop in replacement of the existingcode fragments without having to worry about refactoring the entire codebase. ** With the proposed notation, compiler based approaches, e.g.TVM, MLIR, haven t been explored to further boost performance,although such opportunities are real and tangible. For example in TVM, the Tensor Expression (TE) can be considered as a generalized form of EinOps, and it could bring extra performance gain to the users. Therefore, it would be desirable to integratewith those compiler backends to assist with potential performance. The normal definition of the EinOps notation, while demonstrated in many examples, are not carried out formally. It may refer to tiling one dimension, or composition of new dimensions,or removing a dimension, etc. Another example that needs more clarity is stack/concatenate where there are multipletensors involved, and might be desirable to state formally on the constraints implied in this particular case. Last, while the notation is very flexible in expressing layout related operations, it is yet under explored onexpressing the common operators that deep learning researchers may care about, namely, many variants of convolution. In summary, the paper demonstrates clearly an elegant and novel notation to represent a large portion of operators in deep learning workloads, and developed a meta framework with solid engineering. The reviewer recommends to consider the acceptance of the paper as a novel good deep learning meta framework.<|endoftext|>In this paper, a new Python toolbox called EINOPS (Einstein Operations) is introduced. The paper illustrates about current issues on available solutions, typically in numpy and shows how EINOPS solves those issues. The usage of indices names is more flexible compared to already existing ones in the einsum operation in numpy and Pytorch, which are known to have some limitations, e.g.number of dimensions is limited by the number of letters. It is very useful for using Tensor Networks where number of interacting tensors are usually big and can have a large number of dimensions. Weaknesses: 	The paper does not provide any advance in theory or new algorithm for machine learning. It is limited to introduce a useful and appealing new coding tool. The paper does not mention its application for computing and manipulating Tensor Networks, missing a very important usage for which there is a growing audience eager to have such convenient tool. The paper is focused on deep learning applications. However, I am sure it is also very useful for dealing with Tensor Networks (TNs), where many core tensors are interconnected. The paper lack of a description of its application to TN contractions, for example. It seems that current implementation only consider operations on single tensors. I would suggest to include also operations on two or more tensors, similarly to einsum in numpy and Pytorch, but using a more flexible indices labelling. A comparison with previously available solutions, via numpy or Pytorch, for example, in terms of computation cost is missing in the paper, making it difficult to evaluate if there is some price to pay.<|endoftext|>Highlighted advantages of einops include:  + providing semantic check for tensor operations; + including high expressiveness and flexibility for tensor manipulating interfaces;+ supporting multiple backend runtimes efficiently; + making tensor manipulating code more readable and reliable, and etc..First, this paper studies efficient programming paradigms for multi dimensional array operation, which is an important intermediate layer for modern ML systems, especially for deep learning systems. Personally I tend to buy these advantages of einops claimed by the author, including:+ providing semantic check for tensor operations; + including high expressiveness and flexibility for tensor manipulating interfaces;+ supporting multiple backend runtimes efficiently; + making tensor manipulating code more readable and reliable, and etc.. These claims sound very promising and valuable as a ML toolkit. However, there are some fundamental concerns I have for the paper: + The writing is problematic as an academic paper. Comprehensively, this paper reads like a technical blog, which tries to introduce and advertise a Python library; some statements in this paper are casual, i.e., in the end of Section 7, the author says "We intentionally omit discussion of user conveniences provided by einops package: anonymous axes, ellipsis, list inputs and neural layers. ", although I would not doubt about it if reasonable amount of evidence (e.g., empirical study or analysis) was presented, this sentence sounds a little ungrounded. + There is a lack of reasonable amount of empirical study to justify the statements about the contribution of the work. The current Section 5 can be considered as good motivating examples, but it is not sufficient to support this claim following the principle of scientific study. The efficiency evaluation of the implementation is also missing, e.g., it is important to learn how much overhead is introduced by imposing einops over the backend systems, e.g., what is the runtime gap between einops implementations and direct usage of the backend systems  interfaces. As far as I learn, the interface looks similar (at least from coding examples). As so, I would expect some clear and concrete statements about the distinguish components of einops (different from einsum). However, there is a lack of solid empirical study to validate the effectiveness and efficiency of the design and clear discussion about the difference from the existing tool, i.e., einsum.<|endoftext|>I should mention that, since no code was included in the submission, my assessment of this work is based entirely on the paper and the ideas expressed therein. I could see myself, as a practitioner, using the "rearrange" function (I thought that the example of reshaping an array of images into a 4x4 grid on page 6 was nice). 1.While I agree that much of machine learning is based on tensor manipulation, this paper is not suitable for publication at ICLR. I realize that libraries like TensorFlow and PyTorch had a paper published at some top conferences like ICLR, but it s clear that their novelty and impact on the community was incomparably larger at the time of publication. Also, the authors claim that they "align the interface with einsum to allow smooth simultaneous usage" but this is misleading. This is also misleading since einops is merely an interface/API to existing functions. It is also not clear what kind of "name checks" would be prevented by assigning fixed names to the axes in convolution, and how einops overcomes this issue. I might have misunderstood this whole paragraph, so please correct me if I m wrong. I am obviously not implying anything about the programming and engineering abilities of the authors, I am merely pointing out that claims like this should not be made in a scientific paper. Figure 1: the example assumes that the reshape is done by hardcoding the dimensions and that the programmer will make a typo that "luckily" does not crash. 7.Page 4, third bullet point: the authors imply that the code should crash, but in fact it is a perfectly well formed expression that should not crash. Again, the authors should not assume the likelihood of a bug or what the users want to do. This is a claim in support of the readability of einops, although it holds for einsum already. **Suggestions**After revising the paper, the authors should consider submitting it to an appropriate venue for open source contributions. The authors should make the paper more rigorous: the notation should be formalized, the claims about usability should be removed unless backed up by user studies, and the motivation of the paper/library should be rethought. The paper could also focus less on examples (which are more suitable for the documentation). **Positive aspects**  The proposed extension of einsum is useful and I can see it having large adoption in the community of scientific computing. The motivation for the paper is weak and much of the advantages brought by the proposed "einops" were already brought by the original "einsum" notation. The presentation is, at times, confusing. There are many non rigorous claims that cannot be tolerated in a scientific publication.
Reject; rating score: 5; rating score: 5; rating score: 5; In other words, it is not clear how the authors measure negative performance. In summary, I cannot recommend this paper for acceptance at its current state. However, it is currently unclear if the choice of using the infill or outline reward something that is manually assigned and whether this means that the same policy is trained using two different types of rewards? Finally, the performance on the physical task shows some good results in terms of average offset improvement but those are not reflected in Figure 15. **questionable improvements on the physical task**: Although the paper reports on the average offset improvement of the proposed solution with respect to a baseline controller, the provided photos of the results from the physical experiment do not seem different to the baseline controller.<|endoftext|>This allows for closed loop control that outperforms the state of the art in terms of printing quality. Given that this paper is application focused and light on algorithmic contributions, I think there is should be a higher burden for it to be clear and readable. For this reason, I will recommend rejection, but will remain open minded to the author rebuttal and the opinion of other reviewers.<|endoftext|>Very clear description of the proposed system, both in terms of hardware and software. That being said, I think that the paper s main weakness is its very fine grained specialization to the task of additive manufacturing. Overall, this is a well executed and evaluated system paper. However, at the moment, it reads more as a technical report than a scientific paper.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 6; The authors proposed an efficient transformer layer based on a dictionary of shared parameters instead of standard self attention. The goal is to reduce redundant parameters in transformer models. Clear exposition of the proposed model. Weaknesses  It is not clear how the initialisation of hyper parameters affects model performance. Please address the following questions during the rebuttal:  Does parameter initialization could affect model performance? However, how expensive could this exercise become? Please speculate on how attention representations behave across layers. I recommend acceptance given that the paper clearly describes related work, and proposed model. The authors proposed an efficient transformer model that can be trained with less resources.<|endoftext|>This work proposes a modification of the original Transformer architecture by replacing attention layers and layers in its Feed Forward Networks across all of its blocks with learned shared dictionaries. The proposed modification to the Transformer architecture is novel and I believe would be interesting for the community but the methodology and motivation could be explained more clearly and provided with more context, including more details on the hyperparameter selection and on how the DictFormer is trained. #### Updates during paper discussionBased on the author s responses to the reviewers  questions and updates to the manuscript (including clarifying some of their methodology and statements and including confidence intervals in the results section), I ve decided to increase my score. My understanding is that this is not directly optimized for in the model. What is meant by “high quality performance” of the shared dictionary projection? Adaptively sparse transformers. *Additional questions*  Is it necessary to have the dictionary size less than the embedding size, namely $m < d$? Have the authors tracked whether all columns of the dictionaries are used in practice? Have the authors tracked what percentage of the $t$ coefficients are non zero on average?<|endoftext|>The core “dictionary” technique isn’t really explained at a high level before the paper plunges into the details. It seems to be something like the approach in [1] but it’s difficult to be sure (I gave up on section 3 after a while). This is hard to understand: “	In this paper, the #Params omit word embedding size that would highly dependent on the sentence length and would significantly differ for various tasks. The total #Params in this paper includes the model size of word embedding.”  It’s difficult to align table 3 with figure 5. You should include a line corresponding to the point in figure 5 with highest BLEU (higher than anything that appears in table 3). Potentially a great paper, but if so it deserves to be much better explained.<|endoftext|>Then, the method can approximately recover the original Transformer models by simple additions and multiplications. Experimental results show that the proposed method seems to reduce model sizes and computations successfully while preventing considerable performance degradation (in some cases, the proposed method appears to improve the performance). The idea of the proposed method is interesting, but there are a few concerns in terms of the presentation. If the paper lacks explanation, this can be an apparent problem for this paper in terms of completeness. * Additionally, it seems that there is no description about what the "Improved Embedded" is shown in Table 3. Therefore, it is hard to judge whether this paper has enough contribution for publishing as the conference paper. However, the idea itself of sharing the parameter is not very innovative. ### 2, Notation and equation* The notations are incredibly messy and hard to understand. The authors need to make notations much simpler for better understanding to readers. * In Table 1, it says the results for WMT De En and WMT Fr En. However, at the beginning of Section 4, the experiments are conducted on WMT "En De" and "En Fr," which are not "De En" and "Fr En." However, I could not find the precise experimental settings used in this paper. I recommend clearly showing the model configurations and hyper parameter settings for keeping reproducibility.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; This paper proposes a new task of modeling editing processes to model the whole process of iteratively generating sequences. The Experimental results show that modeling editing processes improve performance compared to previous single step models of edits on related downstream tasks and the proposed task. The paper is well organized and easy to followWeaknesses:1. My main concern is the motivation of this work. It is not clear why interactively editing is better than single step editing. The authors need to elaborate on the source of the gain in more detail. 3.In section 6, the paper only compares their model to a limited baseline, making the experiment less convincing. The paper needs to add some baseline, especially in the dataset CODEREVISIONS. Please refer to the Weaknesses in Main Review, I hope the authors could give a clear answer<|endoftext|>There are also additional related work on editing based method for text generation. Do those apply to the edit generation problem here? It also shows more edit history is helpful in prediction, therefore EditPro based on three previous revision history is better than that based on one immediate previous version. The paper also propose three downstream tasks for evaluation: editing given the text comment (from the commit message), edit conditioned generation, and edit condition classification. Previous approaches only consider predicting editing based on the current text (therefore just one revision history). This paper considers more revision to predict and it show more revisions (up to three) are beneficial. 4.The paper shows improvement on the metrics on WikiRevisions dataset. One of the metrics proposed in the paper is slightly strange. If you directly use Transformer encoder decoder to predict both the operation and the generation text, how is EditPro compare to this baseline? It could be a good paper if the authors could fix some major concerns above.<|endoftext|>This paper considers the task of explicitly modeling the process of editing sequences in an iterative manner, while the prior approaches for sequence editing are usually single step editing methods. Thus, the proposed method is significantly novel. In my opinion, the experimental section can be improved significantly. It currently lacks sufficient comparisons with prior work to demonstrate the efficacy of iterative edit modeling over prior methods of single step editing. Comparisons with prior work: The paper compares with only one baseline, while it does acknowledge well known and recent papers in the domain of sequence editing. The proposed evaluation metrics also seem to be directly aligned with the design of the proposed model, hence comparisons with other models on these metrics might be unfair. (However, the model s design explicitly favors these metrics.) However, the datasets considered in this paper may not be well aligned with this motivation. Hence, I feel that the proposed model requires a more rigorous evaluation. In the related work, authors do acknowledge prior work on sequence editing.<|endoftext|>The outset of the paper is valid: Conditioning on the complete (or longer) revision history can potentially model the editing process of documents more accurately than models that just take the latest history into account. I think that it is hard to justify an approach with perplexity improvements only. Tab.3 does not compare with related work on these tasks. This subtle difference is quite important so the wording should be more precise. Is this true for both WikiRevisions and CodeRevisions tasks? x_0 is not empty in Fig.1.While there are some potential merits to the idea of modeling multiple revisions, I have doubts whether the evaluation demonstrates them in a convincing way.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 8; This paper proposes a symbolic system in Quasi Natural Language, MetaQNL, which is compatible with both logical inference and quasi natural language expressions, and where the basic building blocks are sentences and rules. The authors also propose MetaInduce, which learns to generalize a set of rules that explains the examples in MetaInduce. 2) the authors apply a symbolic procedure called anti unification to generate abstract rules from concrete rules. 3) and finally, proof paths are encoded in MAX SAT and a subset of all rules are solved using a MAX SAT solver to find minimal possible explanationsThis paper evaluates its methods on two synthetic datasets, and the authors claim to learn compact models with much less data, and produces answers as well as proofsstrengths:1) this is indeed novel research problem   using a learning system together with a maxsat solver to make logical inference and identify rules. In the end, it depends on a maxsat solver, which will become intractable quickly when there are more rules. In summary, if the authors can present a real world application that can potentially benefit from their system while other learning systems fail to do so and evaluate their method on a small real world dataset, I would be less concerned. The authors need to be specific what on tasks. In short, while I acknowledge this work is novel, its setting is not very practical. This paper would benefit from presenting a potential real world application, or else some theoretical generalization results on how well their systems can learn.<|endoftext|>This paper proposes an algorithm that learns rules from natural language data, and a symbolic system for manipulating these rules, where existing provers can be applied. The objective is to maximize the number of examples in a test set that are consistent with the proposed mode while minimizing the number of rules in the model. Pros:   The learning algorithm is interesting and the problem of automatically discovering prepositions from examples is important for ATP in formal or informal languages. The paper clearly defines the terms used and explains the methods and experiments well. One result of this is existing methods already achieve good performance, and it s not clear that the proposed method results in better performance. None of the components (theorem prover, rule abstraction, rule pruning) are novel individually. Questions:1) Are TRUE FALSE and MAPS_TO the only special symbols? It would be helpful to state this. 2) Are there any unprovable examples in SCAN? The paper proposes an interesting solution to an important problem, but as is the experimental settings and results are not compelling.<|endoftext|>The work proposes two new concepts:+ MetaQNL: a symbolic system in Quasi Natural Language. An interesting property of the MetaQNL representation is that it allows to perform backward or forward inference by substitution of variables with sentences. The authors assume that texts can be translated into the MetaQNL format and thus solving the reasoning problems with text input is possible via mining rules from text and backward/forward reasoning with the Quasi Natural Language. It is a bottom up rule induction approach where it includes a Rule proposer which propose a concrete rule from a training example and an anti unification module to abstract the concrete rule with more generalised rules. Overall the idea is novel and I like the paper presentation. I wonder how it will work when the sentence is really complicated with real natural language? MAJOR CONCERN 2:I worry about reproducibility as the source code is not open but the details explanation of the proposed approaches are missing. > This is a strong statement, it requires a support with real natural language examples rather than synthetically generated sentences in the experiments. Definition 6: what happen if there is no proof for a goal and the goal is proved via the close world assumption? The paper proposes a new concept called quasi language which allows representing rules in an new informal format that still allows to perform forward or backward reasoning while it is assumed to be mined easily from texts. The experimental results with some datasets with synthetically generated texts show that the methods work very well and advance state of the art results.<|endoftext|>Traditional research on symbolic reasoning assumes input data are already translated into a format that complies with the formalism of the underlying system. A major struggle for symbolic reasoning is to handle the data coming in a natural form (e.g.images/text) and perform reasoning on the same. It does not scale to millions of training examples. The formalism is also proposed by this paper and is called MetaQNL. See my comments in the main review. This formalism supports the natural language sentences with variables within them. 2.It generates abstract rules from concrete rules via a symbolic procedure called anti unification. 3.It encodes the proof paths in MAX SAT and solves for a subset of all rules using a MAX SAT solver. This paper benchmarks the proposed method on 2 tasks   learning compositional instructions and logical reasoning. For learning compositional instructions, it works on two standard benchmarks: MiniSCAN and SCAN and recovers precisely the ground truth rules. For logical reasoning, the proposed method achieves SOTA on the RuleTaker dataset. In my view, it certainly adds a dimension to the literature on symbolic rule learning. Although there are some weaknesses of the proposed approach, I still feel this is a novel idea and has the potential to yield something big in the future. __Strength__  A well written paper. It will be good to shed some light on this.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; Experiments on the benchmark datasets show competitive results compared with baselines. 1.The motivation of this manuscript is not clear. The authors should clearly claim the challenging issues in previous methods. 2.The authors complement the theoretical explanation of the success of the proposed approach. 3.While the online coreset selection method adopted in the manuscript seems plausible, it is not exciting. In general the paper is well organized and clearly written. The technical details are easy to follow.<|endoftext|>2) The core set problem is an important and under researched area in continual learning, especially in its online form. 4) The introduction of diversity as a criteria for core set selection is interesting. WEAKNESSES:1) As the term “online” appears in the title of the paper, this modality should be better introduced and motivated. However, the term remains not properly defined and seems to be related with the problem of imbalance. 2) Figure 2 shows a dataset that is not addressed by the method and is somewhat misleading. This may partly explain the not improving performance on CIFAR 100. 5) In the reviewer’s opinion the ablation study of the effect of the gradient is not sufficient to justify its usage.<|endoftext|>This paper addresses the problem of coreset selection for realistic and challenging continual learning scenarios. The authors did not explain how big the difficulty is to adapt existing algorithms to online learning. The three adopted selection strategies do not demonstrate enough novelty, either. In a nutshell, the reviewer regards this paper as a borderline paper, given the limited technical innovation. I read the authors  rebuttal. However, I think that the core technique of this work does not advance the research in continual learning significantly.<|endoftext|>The author propose a novel approach for online coreset selection, i.e exemplars used in the rehearsal process of past tasks in a continual learning framework. Positive aspects:  The proposed approach presents scientific novelty  The related work section covers the most relevant papers in the field  The experimental validation is extensive and the authors demonstrated the superiority of their approach. Negative aspects:  The idea is in general well explained, although the paper lacks clarity in some aspects (see the detailed comments below), therefore it could be further improved  There are also some issues with the proposed approach which are not clear enoughPlease find below my main concerns:1. What do you mean by  target dataset  in a continual learning framework? It is confusing: If I have to learn 10 tasks, which one is the target dataset? Therefore, please use this formulation instead and change all over the document. How do you guarantee the class balance of the selected core set? Or you consider the similarity between a samples and the batch average? Something is missing there. You refer as  cross batch , but the eq.4 is about the similarity between samples in the same batch!
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; This paper presents an unsupervised learning method to identify skill correspondences for translating skills between robots with different morphology. The approach is inspired by unsupervised machine translation, and uses an unsupervised objective to learn a skill translation model to match the distributions across domains. The novelty of the paper is not clearly discussed and justified in the paper. The insight/premise that differently morphological robots follow similar strategies to address similar tasks seems questionable. But for robot locomotion as an example, this assumption may not hold: robots can perform locomotion by walking, crawling, hopping, and wheeling, based on the morphology of the robot. The terms (skill, strategy, and mode) are confusing, and it is hard to imagine these terms in the context of a robotics application. “Analysing qualitative skill translations” in the experimental section 4.1 has no experimental support. No experimental results are provided to justify the statements in this subsection. The Sawyer robot and the Baxter robot’s arms also have similar design and configurations. In the application of robot manipulation, suction cups, soft grippers and traditional mechanical grippers can be considered as morphologically different robots, or maybe robot arms with obvious differences, e.g., in terms of degrees of freedom. How the proposed approach can be generalized to transfer skills among these robots with obvious morphological differences? While the idea of transferring skills between robots with different morphology is interesting, the paper is preliminary and will need improvements on explaining its novelty, premise, and terminology; more convincing experiments using robots with obvious morphological differences are also expected.<|endoftext|>This approach to learning skill correspondences is framed as a problem of matching distributions of sequences of skills across robots. The paper proposes an unsupervised objective, inspired by work in unsupervised machine translation, that makes the skill translation model learn to match the distribution of skill sequences. The idea of being able to reproduce skill sequences between different robots based on videos or visual demonstrations is a very hard problem with potentially very impactful applications! Does it refer to different kinematics or robots with different DOFs? Depending on this definition, the assumption: “different morphological robots use similar task strategies (in terms of sequences of skills) to solve similar tasks” might not always hold. It would be useful to clearly emphasise the contributions of the approach, as many parts rely on TVI introduced in Shankar & Gupta (2020). However, I think there are several older related works, which would help provide context and give motivation to the proposed approach, such as:  Movement primitives:    Kober, J. and Peters, J. Learning motor primitives for robotics. It would also be beneficial to show the performance of the proposed approach in comparison to simple skill transfer or learning from demonstration approaches as a baseline. Update related work discussion<|endoftext|>The paper proposes a novel idea of learning skills correspondences between robots with different morphologies in an unsupervised way, borrowing ideas from machine translation. The contributions are solid, with a few minor evaluation weaknesses as highlighted in the main review above. The paper proposed to learn skills correspondences between robots of different morphologies in an unsupervised way   without requiring paired data from the robots. The proposed approaches hinges on the assumption that robots with different morphologies follow similar high level strategies when performing similar tasks. Instead they provide correspondences over skills, which is much easier to specify, even by non robotics expert users. Weaknesses:Although the Baxter and Sawyer have different dynamics, their kinematics are not that different. I would have loved to see experiments on robots with different number of degrees of freedom as that is more challenging. I would have loved to see an ablation study on the quality of skills across the robots. I would have also loved to see an analysis of the training time of the translation model in relation to the time it takes to learn the skill from scratch with the target robot. One important aspect of transfer that is usually ignored, is the cost of transfer compared to learning from scratch. Of course, using data already existing avoids this but is still a problem for robot for which data doesn t exist.<|endoftext|>This paper frames learning skill correspondences as a problem of matching distributions of sequences of skills across robots, and presents an unsupervised objective that is able to learn semantically meaningful correspondences between skills across diverse robot domains. The paper is written clearly and is well structured. The contribution is presented in a clear way, the technical formulation is sound and claims are supported by experimental results. Implementation details are provided and comparisons with alternative methods allow to appreciate the benefits of the proposed method. It could be argued that the reach skill is not complex enough to appreciate the accuracy of the proposed method in translating from one domain to another. Can you discuss these in more detail? How do you expect your method to transfer to real robots?
Reject; rating score: 1; rating score: 3; rating score: 6; rating score: 6; There are multiple textbook chapters (and entire textbooks) that address this issues. This paper has some interesting observations, but is not ready for ICLR acceptance. However, from a research perspective, this paper has very limited novelty. With respect to class stratified evaluation (or combining them into a single score), I have seen hundreds of reports that already do this.<|endoftext|>Recognizing that the problem is interesting, however, taking into account the above, I found that the contribution of this work is insufficient for this conference. Note that the classic probabilistic model of the classifier is based on the minimization of loss function, which leads to the minimization of the overall risk (expected value of the loss function), i.e., one can say that the problem is solved and the proposal contained in the paper does not bring anything new.<|endoftext|>The paper presented a simple yet general purpose class sensitive evaluation framework for imbalanced data classification. Their framework is designed to improve the grading of multi class classifiers in domains where class importance is not evenly distributed. The paper addressed a known problem by introducing weights to the rare class and explaining a new theoretical finding to better grade the models applied.<|endoftext|>However, the paper still exists the following problems:The major problem of this paper is that the importance of each class is difficult to define. Specifically, the proposed evaluation metric is designed to improve the grading of multi class classifiers in domains where class importance is not evenly distributed. Moreover, the writing in this paper is very well and is very easy to follow. In this paper, the authors have proposed a novel evaluation framework for imbalanced data classification.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 6; What is interesting is that optimization of the decoder and encoder parameters to optimize the ELBO degrades the FID performance. In this last case, the form derived from the EF VAE differs somewhat from the "standard" version, by using word counts rather than frequencies.<|endoftext|>Or is it a combination of the factorized encoder plus the ELBO? As a minor point, it would have been nice to see implications of theorems 1 and 2 explored in the experiments to see that exponential family VAEs remain close to linear parameterized exponential models. Overall, I think this is a well written paper with an argument for the limitations of an important class of models.<|endoftext|>I think this is a very clear experiment to show that failure of VAEs is often not caused by optimization issues, but rather the inherent limitations of ELBO learning with inflexible encoder distribution.<|endoftext|>**novelty & insights** The interpretation and insight provided in theorem 1 is an interesting addition to the literature on analyzing VAEs, and is novel to the best of my knowledge.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; In this paper, the authors presented a principled approach to identify global interactions by casting the problem as a multi arm bandit problem and proposes a solution using UCB algorithm. Furthermore, the authors showcase the importance of the learned interactions by proposing a new deep learning model based on these interactions and showcase the improvements in model size (thereby competing against pruning methods) as well as in accuracy (thereby competing against generalization methods). Such methods are of high importance and can be used in settings where the model is black box by design or by method of delivery (e,g.a binary file)  The simplification of the problem to best k arms leads to a more computationally feasible solution.<|endoftext|>The paper handles the important area of model that can be interpreted using feature interactions   aligned to theme of explainable deep learning. The authors chose to use the problem of multi arm bandit, solving it by UCB algorithm with good speed and accuracy. A lightweight and interpretable deep learning model (called ParaACE), built using alternating conditional expectation (ACE) method is the crux of the work. It is shown that the proposed method improves accuracy by 26% and reduces the model size by 100+ times as compared to its Teacher model over various datasets. The paper has extensive supplementary material, as well as shared code. is this assumption valid? upper bound of m in practice? explain more on the cross domain practical applications  recommended to compare with other state of art approaches in real analysis   using Matlab logo as function was cool : )This paper is recommended for acceptance at ICLR.<|endoftext|>The paper proposes to detect pairwise estimation using a more efficient evaluation of Hessian values via sampling based on the multi armed bandit approach. However, the paper doesn t focus on the effectiveness of this multi armed approach. * It would be interesting to consider simple architectures as an alternative to ParaACE, as other architectures seem to be too complex to learn well for a dataset of size 800. Please, provide a reference to them in the main text. The authors propose a more efficient method for hessian matrix estimation with little novelty and not comprehensive experimental results. If experimental issues are resolved with experiments on real datasets of high dimension and comparison with a proper baseline, then there is a change for the acceptance.<|endoftext|>The authors propose two key ideas. I think the work proposes several interesting ideas; these ideas may be based on prior literature and may seem straightforward in hindsight, but I think they involved creative thinking to propose in the first place. Although I am not convinced by the computational efficiency of the proposed UCB algorithm and think a more computationally efficient baseline is missing from the comparisons, I think the idea of taking a multi armed bandits approach to model interpretation is still clever, and the UCB algorithm could likely be extended/modified to leverage a more computationally efficient way of evaluating interactions. In experiments, the authors show the UCB approach is effective at identifying feature interactions compared to alternative methods, and they demonstrate that ParaACE offers strong gains in model compression compared to other competing approaches, and often improves performance relative to its overparameterized teacher model. Overall, I rate the paper above the acceptance threshold as is. This difference would be even more dramatic for experiments with even more features. I think it s important to report the number of pulls. However, I recognize that, depending on the perturbation size h, this saturation issue could be overcome.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; This paper designs a new agent with the ability to estimate the local observations and potential goals of other agents, conduct message passing of such local estimates with neighboring agents, then use the outcome of message passing in a two level hierarchical policy to select its own goal and take primitive actions conditioned on that goal. Experiments and ablations were done in the cooperative navigation benchmark and a multi sensor target coverage task, in comparison to previous multi agent communciation baselines. How many primitive actions are taken per sub goal? Or are there hand designed rewards for each sub goal? Of course, some of these questions may be addressed by the paper somewhere, but the fact that I still have these questions after multiple readings shows that the paper s organization really needs improvement (not just minor local changes) and the responsibility isn t entirely on the reader s part. During rebuttal: authors have provided detailed answers to my questions and revised the manuscript to improve clarity. Even after reading to the end of the methods section, I still can t find the answer. The accuracy of this prediction needs to be shown in an experiment.<|endoftext|>The paper presents a new method for communication and cooperation in multi agent settings. The method relies on modelling other agents  intentions and internal states using Theory of Mind based neural nets. The predictions from the ToM model are used to decide how to communicate and coordinate with other agents. The authors evaluate the approach on two synthetic tasks: cooperative navigation and multi sensor target coverage. If the authors believe that their method is inapplicable to the domains, then I would have liked to see a discussion on how this approach could be adapted. I would have liked to see a discussion on such approaches.<|endoftext|>This paper proposes a new algorithm called TOM2C to solve the multi agent reinforcement learning problem. To achieve goals in the MARL problem, communication between agents is important. It would be much convincing if results of other benchmarks are provided. To solve this problem, the authors adopt the Theory of Mind to multi agents. The agent infers the mental states and intentions of others upon partial observation. The authors also provide a communication reduction method based on CTDE. The approach that combines MARL and Theory of Mind is interesting. In addition, performing two tasks (CN and MSMTC) for performance evaluation and choice of the baseline is sound. The ablation studies show how well the proposed model is structured. Also, more experiments are needed to persuade the reviewers regarding its performance. Does TOM2C have a coordinator who gives rewards to all agents? The input to the agent will be the global information. 6.Section 4.1 describes the cooperative navigation task and multi sensor target coverage task.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 5; rating score: 5; This paper studies multi agent distributed optimization under general consensus type interactions between agents. The main contribution of the paper is to study linear stochastic approximation in a multi agent setup without using bi directional communication among agents. Maintaining a consensus algorithm using bi directional communication is rather straightforward and, by now, well understood, but as the authors argue, relaxing this assumption poses some challenge. First, I personally believe that this literature does not represent well distributed, multi agent RL. Minor points:   terms such as “mean square” and “mean squared” are used to denote the same notion.<|endoftext|>This paper addresses a consensus problem in stochastic approximation, which has application to policy iteration in reinforcement learning. Under certain assumptions, the authors prove convergence of their proposed algorithm in a certain sense. Also assumption 4 has some discussion which is not part of the assumption. In addition, the lack of a numerical illustration seriously undercuts the claims of significance. This brings up the second problem with the paper, which is the lack of exposition and rigour in the definition of the problem and presentation of results. This lack of exposition continues to the main results in Thms 3/5. This brings up my third concern which is that a certain lack of rigour impedes understanding, verification and interpretation of the results. As a result, there are numerous terms in the Thm statements for which must guess at the meaning. The use of assumptions is generally not rigorous, but the particular usage here does not even make it clear what these properties of which variables are being assumed.<|endoftext|> The paper presents a finite time analysis of distributed linear stochastic approximation (non doubly stochastic interconnection matrix) in the presence of Markovian Noise. A major weakness of the paper is the lack of numerical examples in the distributed TD setting. This is where it would have helped to have a motivating example. While it is not a weakness that should limit the evaluation of the paper per se, it is nevertheless important to be highlighted and I thank the authors for the same.<|endoftext|>The manuscript considers the linear stochastic approximation problem (Srikant & Ying (2019)) for a multi agent system. The authors introduced RL and specifically TD as examples of the theory developed in this paper. Even after reading Appendix C, this reviewer is not convinced that Assumption 6 is a proper assumption. Given that all agents reach consensus on the same $\theta_t^i$, this reviewer suggests dropping Assumption 6 and modifying the statement in Theorem 2 to some time varying ODE with $\pi_t^i$ in the definition. To understand the role of the ODE, the reader is required to read the Appendix. This requires clear explanations. The paper is marginally novel and the contributions are fair.<|endoftext|>An interesting submission dealing with distributed stochastic approximation driven by Markovian noise, in which the communication topologies considered among agents are captured by a stochastic matrix (in contrast to the doubly stochastic matrix studied in the related literature). Both asymptotic and finite time error bounds are established and it was shown that the algorithm converges to some unspecified convex combination of the equilibrium points of local tasks. Originality: This paper studies distributed Markovian stochastic approximation algorithms with stochastic interconnection matrices, which has not yet been covered in the related literature. The algorithm proposed is fresh and new as well as interesting. The results are nicely organized and the contributions relative to existing works are clear too. After rebuttal: After reading the other reviews and authors response, I have updated my score.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 5; Ivanova et al., 2020 effectively demonstrate that the multi demand network uniquely responds to code. However, it is not clear to me what new findings are proposed in the current work beyond this. Re Experiment 2:   What layer of the models were used? Having said that, I agree with the other reviewers that this has been a productive discussion and the paper could benefit from more additions  particularly, stronger/cleaner results to show what information is differentially processed in MD vs. LS etc.. ##########Strengths: The paper was well written and the research question is interesting + useful. Do the authors mean brain systems or code models? 2.For discrete properties, this is clearly not regression.<|endoftext|>This paper introduces a systematical framework to discover the relationship between the brain representations of programs and their corresponding code models. This framework helps us to understand the code properties encoded in the human brain so that we could evaluate whether ML models faithfully represent human brain representations of computer code comperhensions. Then they demonstrate another ridge regressor that can map brain representations to the corresponding learned representations by computational language models of code with different model complexity. Major comments and questions:   The results of experiment 1 show the accuracy of the brain representation of code properties. They also indicated that LS is responsive to Python code, while visual areas are responsive to ScratchJr. Minor comments:  The explanation in the paper is fine, however, adding more diagrams of the whole procedure of the framework as well as mathematical descriptions of the models and their input and output as matrices/vectors would have helped a lot to understand the whole idea faster and easier. The paper is well written, the motivations are clear, and the references are enough.<|endoftext|>This paper investigates encoding of computer code in the human brain. The author build decoding models for fMRI responses to predict 1) various properties of python code and, 2) representations of python code derived from different machine learning models. Strengths:This paper is very clear and well written. Weakness:I am not sure whether this paper actually provides a lot of new insights for both audiences from the neuroscience community and machine learning community. It is not surprising to me that the Multiple Demand system in the brain is able to provide above chance decoding performance of the selected properties of the computer code. Therefore it is unclear to me how the integration of information over time or length of program is going to affect the brain mapping to these representations. The mapping from brain representations to context window is also potentially helpful for machine learning researchers in designing better models for code representations.<|endoftext|>This work examines the relationship between fMRI recordings of people who read short programs and different properties and representations of the programming code. The authors say that the output of the encoder was used, but what about for auto regressive models (i.e.the last state, or something else?)? what were the subjects instructed to do? Overall, the paper investigates an interesting and timely question, but offers limited technical and empirical insights. The paper can benefit from a more thorough discussion of what is learned by the comparison with the language models. Another related model property that may affect the relationship with the brain recordings is the dimensionality of the output embeddings did the authors also examine this property (it s somewhat related to the number of parameters in the model but is not a perfect predictor, i.e.BERT and GPT 2 have the same embedding size but different numbers of parameters overall)?
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; Strengths:  Characteristics of the backfill dynamics problem is very well motivated with observations from COVID 19 data that the authors have collected. The authors then propose a Back2Future (B2F) pipeline that can be used to refine predictions from a model when backfill dynamics are present. Due to the high novelty and very general proposed B2F pipeline for refining model predictions, this paper could have high impact, although it may not be the best fit for the ICLR audience.<|endoftext|>The authors deal with the problem of revising previous recorded data and its effect on timeseries predictions. Towards that, they propose a novel deep learning approach, the Back2Future, that refines the model predictions using backfill dynamics. The authors present clearly the backfill problem and its effects on the predictions thus enabling the reader to understand the importance of the suggested solution. Thorough experimental analysis and strong results. > This needs clarification. The proposed work is well motivated, well written, sound and contributes a model that solves a very interesting problem in timeseries prediction with strong results.<|endoftext|>This is a very well written paper with good explanations, proper motivation, clear objectives, and results. The work is very relevant to the current situation and ongoing researchMinor comments:  is it possible to give some reasons for the revisions in the case of covid? The paper proposes an approach to overcoming the so called backfill problem when time series data are revised to correct previous recorded values. I also appreciated the careful preliminary data analysis of the covid data that motivate and back the model proposal.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 6; This work provides a compressive analysis across multiple (18) different hardware performance predictors by   (1) collecting their performance under different amounts of training data and different input network structures and showing each prediction method’s advantageous/disadvantageous scenarios  (2) analyzing the  prediction accuracy s influence on selecting subsequent hardware architecture and giving the insights on how to pick/design hardware performance predictors for the NAS. This work gives reasonable explanations/insights for most of the phenomena in the conducted experiments. **The insights are shallow and obvious**: Even though the paper presents multiple insights from the experiments, most of them are rather shallow and obvious. The underlying reasons or hypotheses are not provided. **Unclear on the aspects of how to counter the imperfect predictions on guiding the network architecture selection**: One of the most interesting aspects about this work is that it proposes imperfect predictions from the hardware predictors can also lead to decent selected architectures. For instance, in page 7, the authors mention “verifying the hardware metric predictions of the 13 architectures”; which 13 architectures? It was never mentioned elsewhere.<|endoftext|>This paper provides a large scale study of hardware metric predictors on 2 recent tabular benchmarks: Trans NAS Bench 101 and HW NAS Bench. While I think that the motivation to have a large scale study of hardware metric predictors similar as White et al.do for performance predictors is a reasonable one, however I think that this paper needs to improve its presentation and clarity. Considering that the novelty and the results presented in the paper are not surprising, I think a more thorough empirical evaluation is necessary. Are they the architectures in the "simulated pareto front" evaluated on the true HW NAS Bench? I think the authors can gain some space by moving them to the appendix and adding additional experimental results in the main paper. There is also quite some work to be done in order to improve the structuring and clarity of the text in my opinion.<|endoftext|>This paper systematically evaluates a wide range of hardware performance predictors across different networks/devices/tasks and analyzes the influence of such predictors to the architecture selection process. 2.The evaluation across different predictors and tasks is solid and the codes are provided. 3.The simple but effective insights can benefit the NAS community. The major concern of this paper is the technical novelty since no new technique is proposed and the provided insights are relatively intuitive. More novel insights are expected. 2.The averaged correlation in Fig.1 across different devices (and tasks) may not be the only metric for evaluating the predictors, i.e., the device specific correlation is also important. For most deployment scenarios with determined types of devices, such device specific discussions could strength the contributions of this paper and provide more detailed insights for the community. 3.The discussions about the influences of the predictors to the architecture selection should be NAS algorithm specific while such discussions are missing in the paper.<|endoftext|>Also, it could still be useful to add some analysis on all 27, in addition to keeping the experiments that are already there. This paper gives an empirical study of 18 different performance predictors on NAS Bench 201 and TransNAS Bench 101, evaluating the rank correlation of the predictors on different datasets and predicted metrics. Although there have been experimental studies on regular performance predictors, not much was known about hardware metric predictors until this paper, which is a more realistic setting. The analysis is good. It is interesting that in Section 6, the authors simulate the predictor (now there is a prediction of a prediction) to speed up results. Why not also use a predictor for the accuracy, and then compute the error of the Pareto front after that?
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; In particular, the authors are interested in including a divisive normalization like the one characterized in the primary visual cortical era and to see the influence of this mechanism on the performance of the AlexNet network. Is the choice of the AlexNet model important for the generality of the results or does this also apply to other networks such as VGG? In general the paper is very well presented and the results are very convincing.<|endoftext|>Visual information flow in Wilson–Cowan networks. I advocate for the publication of this work in the conference when the authors address/comment on the following concerns:A. C. OTHER ANALYSISPrevious comments imply that other analysis could have been performed in the final model:  What are the features that fall within the same neighborhood?. Are these receptive fields similar? I am persuaded that (1) using more general (and "explainable") divisive normalization schemes and (2) considering visual tasks that require the specific adaptation that can be obtained from "explainable" relations between "explainable" features, would certainly make a big quantitative difference. This will lead to better information transmission and eventually better classifications.<|endoftext|>The authors study the effect of divisive normalization on AlexNet. They show that, when combined with standard normalization schemes, it increases performance. More generally, there is a mixture of ideas and analyses which are interesting but somehow not well connected in this work. Following the authors  response I have increased my score form 5 to 6. This would seem more easy to follow for an ML audience. Small suggestions:NN s  > NNsThe paper explores an interesting but not novel idea.<|endoftext|>2.More clarity on connecting the representational analyses of divisive normalization models and other baselines with functional advantages (such as the performance gain on ImageNet and CIFAR 100). I am updating my initial opinion of this paper, in my opinion the paper performs interesting experiments and analyses on OOD generalization and feature selectivity benefits produced by divisive normalized networks. I do not recommend acceptance of this paper at the current stage, but I am willing to change my decision if my comments above are addressed by the authors during the rebuttal phase. The authors must better attempt to address why each of these findings are important and how they relate to the improved classification performance.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; The authors propose to add an autoencoder on top of a pre trained encoder to reduce the encoder’s output dimension and allow to significantly reduce the size of the decoder. The motivation for using autoencoder is not quite clear. The paper writing can be further improved. Figures 1 3 have been shown in many papers. And also missing some strong Seq2Seq baselines with knowledge distillation losses.<|endoftext|>This paper proposes to compress the transformer based summarization models with well pre trained autoencoders (AE). The results of the transformer baseline is far from previous literatures, such as [Li et al., 2018; Lewis et al., 2019; Wei et al., 2020] that achieves at least 33.42 R L score, while this paper report a 31.2 R L score with the transformer baseline. 2.There is no any comparisons with existing compression methods, including pruning and distillation.<|endoftext|>This paper investigates the use of a pre trained autoencoder to reduce the output dimensionality of a pre trained transformer (such as BERT or BART) so that a lower dimensionality decoder can be used. The way these are used is not very clear but I assume that the linear AE maps each symbol independently whereas the LSTM and CNN see the whole sequence? The results show that you can reduce the decoder size by up to 40% without sacrificing too much in performance. However, this does nothing to change the size of the pre trained encoder and it is the problem of the ever increasing size of the pre trained encoder which is highlighted in the introduction to the paper. This paper describes a simple approach to reducing the dimensionality of the decoder to be used for a pre trained encoder.<|endoftext|>The paper proposes a new autoencoder based seq2seq model for text summarization tasks. The paper conducts extensive experiments by evaluating the loss of accuracy with ROUGE. The paper also presents the generation results and another related experiment in the appendix. The scale of those dimensions is not even linear. 2.The rouge scores might not be good  It would be better to incorporate other metrics such as BLEU, BERTscore.
Reject; rating score: 1; rating score: 3; rating score: 3; rating score: 5; This sentence confuses my understanding. The composition and details of the paper are quite unusual. After all these factors, it has been hard to  not  recognize that the authors had struggled to fill the paper with somewhat redundant information. It is hard to accept the experimental results performed on the author s custom datasets.<|endoftext|>Its impact to a larger scope may be limited. I suggest the authors to evaluate their approach to more diverse applications and compare against state of the art continual learning methods on public benchmark datasets. The technical details are all clearly presented and technically sound. * The contribution of this paper is very limited.<|endoftext|>This submission modified REMIND with different dictionary update policy to keep track of previously stored centroids and partition the datasets to subsets to optimize memory usage. Better state more clear as in Sec.4.4 and 4.5. ### Weaknesses#### Major  This application is too narrow. The modifications are not coupled with this wireline auto spooling task and can be applied on many other continual learning tasks (e.g.ImageNet ILSVRC 2012, CORe50, CLEVR, TDIUC tested in REMIND paper).<|endoftext|>In this paper, the authors proposed a method called the modified REMIND (MREMIND) network. It is a replay based continual learning method with a longer memory to historical data and no memory overflow issues. experiment section shows that the modified REMIND network gave the best performance among all other tested solutions. Weakness:  The challenge introduced in this paper is to overcome the current drawbacks of the REMIND network (Hayes et al.2020).The latent code dictionary update and the memory overflow. Overall interesting paper.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 8; rating score: 8; This paper introduces continuous Generative Cellular Automata (cGCA) that is a generative model for continuous 3D reconstruction / shape completion. What is the number of parameters in their model? This is also similar to my understanding of “probabilistic scene completion”. 3.The paper is well written and easy to follow in most parts.<|endoftext|>Dongsu Zhang and Changwoon Choi and Jeonghwan Kim and Young Min Kim, "Learning to Generate 3D Shapes with Generative Cellular Automata", ICLR 2021Strengths1. 2.The proposed PointNet based encoder and implicit function decoder is new and inspiring as an autoencoder for dense point clouds or surface. 3.This paper delivers proof and derivation for the continuous version of GCA. While as the discussion of the limitation of GCA is out of the scope of this paper, this is not a big issue. This paper presents a way towards large scale high resolution scene generation/completion.<|endoftext|>This paper proposes an approach for scene completion which allows for sampling plausible completions from a learnt probabilistic model. The idea is to learn the state transition of a Markov Chain and use this during inference. The proposed formulation is based upon the Generative Cellular Automata where the representation of the surface is a discrete occupancy grid. The state transition is learned using an adapted version of infusion training. However, they are all in a fairly dense range. What are the runtimes of the proposed approach?<|endoftext|>Summary: The paper presents a shape completion method for large scale 3d scenes. Combining these approaches leads to impressive performance on recent 3D scene datasets   ShapeNet Scene and  3DFront. On ShapeNet the model outperforms Generative Cellular Automata (GCA) on reconstruction metrics, but not diversity metrics. Writing: The paper is well written and complete. The figures are sufficiently high resolution and demonstrative of the improvements due to the proposed method. Although it would be interesting to see how the authors managed to train GCA on such large scenes. It would be interesting to see the performance from such a model.<|endoftext|>The submitted paper studies 3D scene geometry reconstruction and proposed a pipeline for predicting signed distance fields from incomplete pointcloud data. The pipeline consists of two major components: an autoencoder that maps between the latent space and the voxel representations, and a Continuous Generative Cellular Automata (cGCA) which progressively and stochastically generates more complete data at each step.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; This paper proposes a new language conditioned imitation learning task based on the Atari Frostbite environment. Why is there a need for this benchmark and how is it different from existing language instruction following benchmarks from the embodied learning literature (Eg.VLN benchmarks like Room to room and object interaction benchmarks like ALFRED)? * Models and training approaches considered in the paper seem straightforward. What are the new technical contributions of the work? * Weak experiments I would encourage the authors to think about what’s new and interesting about the proposed new dataset, what makes it challenging and how it is related to and different from other existing benchmarks. I didn t learn much from these experiments. The experiments are weak and do not provide new, meaningful and practically useful insights.<|endoftext|>No comparison to prior tasks, unclear what new proposed environment tests and how it compares to prior environments. There has been several works, old and recent, on instruction following. In order to propose this benchmark, the paper ought to compare the proposed environment with these existing work   why is there a need for this new benchmark? This crucial question is not addressed in this work. These related work and many others (https://arxiv.org/abs/1906.03926) are not even cited. What does this mean? It seems like there are only a few hundred labeled trajectories. Do the authors means they have a couple hundred labeled trajectories, each with 10k steps (where the task is fixed)? If so, I think this is at best misleading   no other dataset for language grounding that I am aware of lists annotation size like this. The authors mention 500 tasks but there are only  8 listed in the appendix.<|endoftext|>This paper presents a new multimodal benchmark for language conditioned RL settings, where an agent must complete tasks specified by text instructions in the Atari Frostbite environment. 2.The authors use Text DT and TDT interchangeably in their papers, is there any difference? In addition, the authors explain the proposed model based on the decision transformer, but it is not considered to have much related to the decision transformer. However, It seems that the contribution of the proposed model, Text Decision Transformer (Text DT), is slightly marginal and Text DT seems not related to Decision Transformer.<|endoftext|>This paper introduces a corpus of text instructions for the language guided Atari Frostbite environment, and also presents a study of pretrained Text Decision Transformer on the corpus, shows that it can improves upon baselines, yield better results in the low data regimes. This paper introduces a quite large corpus of text instructions for the language guided Atari Frostbite environment, which is good resources for the community, and it also presents baselines for pretrained text decision transformers (TDT), and shows some interesting empirical results. For example, 1). In Table 4, it looks more context does not help for Medium tasks?
Reject; rating score: 3; rating score: 3; rating score: 3;  The authors prove that a discrete approximation of a class of optimal control problems can be recasted as an RL MDP. The authors prove that the original NAF formulation of the Q function cannot approximately solve the MDP defined above, and hence propose a new quadratic formulation of the Q value. The authors evaluate their proposed agent over 4 optimal control environments. NAF also seems to have very high variance and hence needs more runs. (Please do at least 5 seeds for all other baselines, if not 10 seeds for NAF)  In section 4, the new class of Q functions seems to be arbitrarily proposed. I believe this is the biggest contribution of the paper, so I recommend making this class of functions more intuitive (perhaps explain in Appendix how this class of functions are conceived of). Questions:  What are the limitations to this approach (beyond limited to a certain class of problems)? Overall:Based on my limited understandings and mathematical background, the authors propose modifications to NAF, prior work, and prove it can solve certain classes of control theory problems.<|endoftext|>The paper applies the method of quadratic approximation of Q functions to a continuous time optimal control problem by discretizing the time. The manuscript cannot be accepted for publication in the current format. First, the setting looks artificial and non realistic, and the title is misleading, as the continuity is converted to discreteness! I do not think that solving the problem in (1) is equivalent to those above (1), as the authors claim. Note that it is assumed that the value function is known. The proposed framework, by (11) and Thm 2, is applicable only to small T setting, which is not the common setting in control or RL. Putting this together to the other assumptions, the main claim of the paper is that Q func is continuous with time. First, the setting looks artificial and non realistic and the problem is not well explained. The main claim of the paper is that Q func is continuous with time, which seems like an immediate consequence of smoothness of the value function in Thm 1. Some concepts and quantities are not defined, and here are several issues with the presentation.<|endoftext|>The paper investigates the applicability of quadratic approximations to the advantage term in the value function of RL problems in continuous action spaces, and the effectiveness of the previously proposed NAF algorithm for finding optimal policies for such problems. The main contribution is identifying a class of optimal control problems where the cost is a quadratic form of the control effort (commonly known as "minimum effort problems"), and proving theoretically that the value function for such problems can be approximated arbitrarily well by the kind of quadratic approximations eteh mployed by the NAF algorithm. By assuming that the advantage term of the value function is quadratic in the action, this optimization problem can be solved much more efficiently, thus speeding up policy computation significantly. In this paper, the authors prove that if the cost functional of the optimal control problem contains a term that is quadratic in the control effort (action), then the approximation is reasonable, and the true value function of the decision problem can be approximated in this way arbitrarily well. This is a good result, because this kind of "minimum effort" problems with non linear dynamics but costs quadratic in the control effort are very common in practice and of high practical significance. Other than this proof, the contributions of the paper are rather limited. Also, I am not entirely sure what this paper has to do with the main topic of the conference, learning representations. Several modifications to the previously proposed NAF algorithm are described and tested, but it is not clear how much better or faster they learn in comparison with standard RL algorithms that do not make use of such quadratic approximations.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; Given that this super image will be a larger image, the paper leverages the more memory efficient Swin Transformer [1] as an image classifier to perform action recognition. This is an orthogonal approach than having explicit components for modeling temporal relationships. The ablation studies on layout and ordering are interesting. ### Typos  P5: “o”ur approach has linear computational complexity… (capitalize)  P5: We demonstrate later in Sec.4 “thata” larger windowOverall, I think the proposed method, though simple, leads to surprisingly good results. It not only provides a new way of thinking about modeling temporal relationships, but also better connects action recognition and image classification.<|endoftext|>However, the paper proposes to explore whether an image classifier (instead of a video or spatiotemporal based classifier) would already be enough to accomplish this task. The method described is simple and achieves surprising results.<|endoftext|>I believe it would inspire others in this research field. In this paper, the authors propose a simple but effective approach for video action recognition by casting the problem as an image recognition task. Transformer based and CNN based models are both tested on public benchmarks and good experimental results are reported. This paper provides a novel idea for video action recognition.<|endoftext|>Strength:1) Transforming a video to an image for video recognition is a good direction to explore and has potential application field in the future;2) This paper is well written and the experiment is extensive;3) The visualization provides interesting insight of this task;Weakness:1) The novelty of the proposed method is limitted.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; This paper proposes an alternative greedy operator for RL which selects actions proportional to the optimality gap of value functions. Strengths : 	  The resmax operator achieves best of both worlds   an epislon greedy like operator and the softmax operator, which takes into account the optimality gap of value functions. The usual drawbacks of the softmax operator, as discussed in the paper, are known but does not provide new insights to existing literature. During the early training stages, the optimality gap is rather poor (and in practice we do not have a good approximation to the optimality gap either). The proposed operator is not theoretically justified. Experiments do not well justify why the proposed resmax operator may be helpful. Can the authors comment on the use of entropy regularizers with resmax operator based action selection? I do not think this paper has enough novelty for acceptance. The softmax operator is well used in the policy gradient literature   and it is surprising that the paper completely ignores any issues or discusses anything about the effectiveness of the resmax operator in PG learning context.<|endoftext|>This paper introduces a new soft operator, resmax, for mapping Q values to action probabilities. This operator is designed to replace softmax in Boltzmann style policies while having the non expansion property which enables the convergence of Q learning. The paper provides theory demonstrating the coverage and non expansion properties of resmax, as well as somewhat more heuristic evidence that resmax enables more exploration than softmax. * The proposed operator has nice mathematical properties and seems well behaved (unlike softmax). On some environments resmax leads to better performance, and it seems potentially less sensitive to its hyperparameter (although maybe the hyperparameters just live on different scales), but there is no clear empirical analysis of the amount of exploration, e.g.state coverage. * The crucial question for a method like this is whether or not it makes a difference overall when used more widely. The new resmax operator proposed in this work seems interesting and could be a more robust choice for Boltzmann type policies. ### After responsesAfter reading the other reviews and considering the authors  responses I am leaning towards rejecting this paper.<|endoftext|>The paper introduces a new operator for exploration in reinforcement learning. Overall, it is useful to have an additional operator/policy that is somewhat similar to softmax but with its own strengths and weaknesses. I find it useful to make a distinction between the operator used in value function optimization (such as softmax), and the policy used for decision making (such as soft argmax). In fact, previous work has shown the impact of using Boltzmann/mellowmax just for value function optimization and not in decision making (See Song and others "Revisiting the softmax bellman operator: New benefits and new perspective"). So parts of the paper need revision to clearly state this point. Going to the main contribution, namely resmax, it is interesting to note that the behavior of resmax can be qualitatively different than epsilon greedy when more than one greedy actions co exist. More about this on my note about the proof of the non expansion. That said, i feel like the proof could have been generalized with some moderate work. The paper provides experimental results on 3 toy domains as well as 3 Atari domains.<|endoftext|>The most commonly used exploration policies for SARSA/Q learning are epsilon greedy and softmax sampling. The problem with epsilon greedy is that it explores obliviously the action space which may slow down convergence of the Q function estimate, especially when the environment requires long sequences of actions to reach high rewards. This is a reason for me to support the paper. This article propose a novel exploration policy called ResMax. According to the authors this overhead was too costly to integrate mellowmax to their benchmarks. It may improve some RL algorithms. Did you try this as well ? For these reasons I am still wondering if the proposed method is a clear improvement against mellowmax and its tuned softmax variants. Doubts/Questions:  I did not really catch the overemphasis argument on Figure 1a. What about a softmax where another convex function is used instead of exponential ? What about an ordinal "mean of top k" operator ? Minor remarks:p2l 1 more greedy  > greedierp3l26 b   \arg\max  > b \in \arg\max (or the uniqueness assumption should be set on the table)p3l 7 "with results in a small environment" : ??
Reject; rating score: 3; rating score: 3; rating score: 5; I am not absolutely familiar with the literature of escape time theory for SGD. An archetypal example of this fact is that the authors claim to have developed a "novel quasi potential theory that rigorously describes the escape of SGD": in fact, quasi potential theory is a *theoretical methodology* that helps in exhibiting escape rates, whereas it is often presented as a contribution in itself in the article (even if obviously large deviations is an important theory that should help understanding some of the behavior of SGD).<|endoftext|>This work proposes to use the notion of quasi potential to analyze the dynamics and escape properties of SGD from local minima. There is a deep inconsistency in the theory proposed in this paper. Authors expect to use the quasi potential theory to obtain a quantitative criteria for the ability of escape from local minima.<|endoftext|>This paper uses the quasi potential theory to formalize the escape behavior of SGD happening while training a deep neural network. How can one explain this discrepancy? 1  The paper is well written and easy to follow.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 3; I have updated my score accordingly. The paper presents an interesting theoretical analysis of co distillation for two different kernel regression models. The approaches are analyzed theoretically and empirically. The paper thoroughly analyzes the dynamics of co distillation and the proposed variants in a limited setting with two clients. Here, a broader empirical analysis would have improved the significance of the contributions. To that end, the paper details the dynamics of all three approaches and shows that the straight forward approach may degenerate, and that the ensemble approach is optimal in the limit. Moreover, it gives conditions under which the averaging approach degenerates. These theoretical results are interesting and insightful. This hints at the generality of this analysis. While there is good reason to analyze only two clients theoretically, it would be straight forward to apply the proposed methods on larger numbers of clients empirically. The paper would benefit greatly from a broader empirical evaluation. ******** after rebuttal **********The authors have addressed my questions to my satisfaction. While I agree with my fellow reviewers that this work is limited in its scope, I do enjoy this novel theoretical take on distributed learning with different model types via knowledge distillation.<|endoftext|>This paper analyze knowledge distillation based model agnostic federated learning. They consider simple two agent kernel regression scenario, where each agent has its own dataset and predicting function. They provide another algorithm ensemble AvgKD which can actually avoid this issue and converge to optimal solution. Pros:1.This work gives the first theory analysis for model agnostic FL, even though the setting is very simple. They also use the negative result to motivate the improved algorithm ensemble AvgKD. The whole story is complete and clear. What is difference between AvgKD and their work? In practice, there are multiple agents, and the machine learning model is way complicated, that may not have closed form solution. 3.In addition, the paper only consider optimization perspective, but I am curious about the generalization ability of AvgKD. Overall, model agnostic FL area is lack of theory results, so this work has its own novelty. However, the setting is somewhat toy, so I am afraid it may not be very helpful towards understanding model agnostic FL in practice (e.g., multiple clients and complicated models that do not have closed form solution.)<|endoftext|>This paper focuses on the setting of federated learning where the two agents are attempting to perform kernel regression using different kernels (and hence have different models). Their theory also shows an interesting connection between AKD and the alternating projection algorithm for finding the intersection of sets. Based on the problem setting, the proposed method is only applicable for the case of two agents, while the problem of federated learning is usually for multi agents (more than two). Can this proposed method be extended to a more agents case? This result cannot be directly observed from the figure. Ex2: The analysis for Figure 4, 5, 6 is missing, which can make the readers confused about these results. * Minor problem:Some of the notations in the paper are a bit confusing to me, especially those seem to have similar meanings. I would suggest that if have the same meaning, they can be represented by the same notation， if not, more explanations for their differences can be offered.<|endoftext|>EXPERIMENTI find the experiment somewhat strange. It implies the initial local model of client 1 is already on the same level with the centralized model which means distillation does not help at all. On another note, the flaw setup on AKD is observable from the reported results. However, in terms of its practical significance, I am not convinced because of there is too much gap between the theoretic results and real world setting, which is elaborated below. In fact, on the same model, same data setting, it is noticeable that client 2 is much worse than client 1 at the initial round. This is problematic because the result derived in 2 client setting is very specific to the iteration between the two clients and how they average between the local data & the distilled prediction of the shared models. 1.The theoretic setup started with 2 client setting but has not been extended to multi agent setting. If there are more than one shared models, how would the client set up their training output for the next iteration? 2.Another peculiar restriction is that the presented result is tied to an obvious flaw in the way AKD is set up in this paper. Thus, while this could be the beginning of an interesting theory, more work is needed to complete the idea. I suspect this is a main cause that leads to AKD s degeneration. 5.The result is largely based on a formulation of regression model and the result is certainly tied to this specific regression form. 6.Have the authors considered the communication cost beyond the 2 client setting?
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 6; This work proposes a continuous disentanglement variational autoencoder. The approach is an extension of the disentangled variational autoencoder proposed by Sha & Lukasiewicz in which they modify sampling layer to use weighted sum of multiple distributions instead of single style vector distribution. The model shows perfect correlation between latent values and true generative factors on the synthetic data. The novelty of the approach is very thin. Model is a simple extension of disentangled variational autoencoder using a weighted sum. However, the experiments are only on individual synthetic datasets and are limited to disentanglement use case.<|endoftext|>Still, none of the experiments in the paper touch upon this area, which convincingly is an area where categorical annotations are difficult. In experiments on synthetic datasets, the model is shown to provide the specified properties. The proposed method is as a minor variation of Sha and Lukasiewicz (2021). Specifically, the main modification is that instead of sampling from only the ground truth latent distribution for the decoder, a weighted sum is used over all style values. However, no other was compared to it. This can be done on downstream tasks, where it can be compared to many other works. The second issue is that the experiments rely on datasets that were created for this paper specifically and not standard benchmarks.<|endoftext|>The model effectively learns a single continuous embedding for examples from which all label sets are easily predictable, effectively unifying the different label sets. It s a small modification on the overall architecture, and the evaluation is done only on a single synthetic dataset. I was a little confused also about the distinction between a single multidimensional style vector and many scalar style values; the paper seems to argue that multiple scalars provide better disentanglement than a single vector but I couldn t follow what this means in practice in terms of the model equations. The evaluation is very sparse; it s unclear to me how to interpret these results on small synthetic datasets. I don t see the novelty or significance here.<|endoftext|>The authors consider multiple disentangled factors in a variational autoencoder. Then, the final representation is a weighted sum of all the factors. The results show that the method is able to encode multiple factors. The proposed method is effective in learning disentangled factors. It would be better if the authors evaluate the methods on emotional data, as they mentioned in the abstract and introduction. However, the experiments on real data is suggested.
Reject; rating score: 1; rating score: 3; rating score: 3; rating score: 6; New independent SVM classifiers are trained over the class probabilities and their results are combined using voting to perform the final prediction. The authors report experiments on two datasets, comparing with versions of ResNet trained from scratch, in which SVMNet shows better results than ResNet when using fewer training examples, For one of the datasets, the ResNet could not converge, while SVMNet obtained up to 80%. Strenghts:  the paper presents an ambitious idea by designing SVM classifiers that operate locally. Therefore, claiming it is a network it is a bit farfetched. the choice of the datasets is also debatable: why not using MNIST, Fashion MNIST, ImageNette, or others, which have many results reported in previous papers and would allow for a better comparison? Also, the baselines were not adequate, and the choice of datasets to compare was limited.<|endoftext|>Each SVM is trained is a small patch or window from the input imagery and some classifiers can be eliminated from contributing to the predictions. Results show better performance by the model in the small data regime compared with larger convolutional neural networks trained from scratch. Sensitivity analysis was performed for the window/patch size parameter. Authors tested the approach in multiple datasets. Weaknesses:  The evaluation framework is inadequate for the proposed apporach. More so the larger the network architecture is, and it is unfair to compare that setup to an SVM ensemble. I do not think the paper is ready for publication and I recommend the authors to retrink the evaluation framework and add relevant baselines and method for comparison.<|endoftext|>The paper argues that one of the major drawbacks for deep convolutional neural networks (DCNNs) is the need for large annotated training sets. Also, it’s not clear how the CPU hardware used to train SVMnet compares against the GPU used to train ResNet. The SVMnet architecture is composed of one or more stacked "SVM layers". Each SVM layer is composed of a set of independent svm classifiers, where the input to each svm is a patch in the image and the output is a probability estimate for the image class. Finally, the predictions from the last SVM layer are tallied to produce a majority vote for the image. In my opinion, the experiments presented in the paper fail to demonstrate or give any meaningful evidence to the claims made by the authors regarding the proposed method. I will divide my review into three sections, addressing different aspects in the paper: motivation, method and experiments. Also, I believe the claims could have been better supported with a wider discussion demonstrating how the proposed method relates to other previously published work on training deep models in settings with limited training data. Overall, the motivation for the proposed method and its importance compared to existing work was not well founded.<|endoftext|>The result shows that the SVMNET outperforms other deep convolutional neural networks such as ResNet 50 with less training time for cases in which the number of labeled training samples is small. For most cases, such amount of training data is not so difficult to generate. So the advantage of SVMNET might be limited. 2.According to the paper, all ResNet models were trained from scratch rather than starting with pre trained weights derived from ImageNet. In this paper, the author proposes a novel deep learning architecture that uses ensemble of numerous simple SVM classifiers as network layers. For two of the three datasets, SVMNET has better performance when the training samples are less than 200 compared with ResNet models, but the ResNet catch up to the SVMNET quickly after the sample increases. Also it would be better if they can do comparison with pre trained ResNet and show the results in their paper.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 6; This paper looks at the problem of meta learning with heterogeneous tasks. This is an interesting problem that is receiving increasing attention in the past two or three years. Several previous work address this problem by clustering task representations and let the meta learner exploit the information about task cluster. The authors claim that, for the first time, they cluster tasks not only at the level of the input representation (features), but also at the level of the optimization trajectory in parameter space. My opinion is that the work has some significant positives, for example:  The paper is well written, mostly clear besides some small details (see "Minor points" below)  The authors report experiments on several benchmarks and several ablation studies. Visualization of the rehearsed learning paths (figure 3b) is nice. The work of https://arxiv.org/abs/2103.04691 is conceptually very similar, they run gradient descent in the inner loop for a few steps and they cluster the resulting trajectories across tasks. 2) I m not entirely convinced that the 1% or 2% gains with respect to other methods reported in table 1 and table 2 are due to the superiority of the proposed architecture. Is the capacity (e.g.number of parameters) of the different models taken into account? 3) In section 4.4., while deriving the "shortcut tunnel", the authors justify their method by stating that "tasks with similar feature assignments will also have similar path assignments."<|endoftext|>This paper proposes a clustered task aware meta learning algorithm. The task feature is derived from the weighted sum of the soft cluster centers. Experiments on image classification and cold start recommendation demonstrate superior performance compared to baseline algorithms. The proposed idea that adopts a learning path to improve the parameter initialization is interesting. Some of the critical technical points are not sufficiently justified, e.g.(1)	why is using the trajectory of rehearsed path helpful to learn a good parameter initialization? (2)	in section 4.2, the reason for modeling task representation is explained as encoding the predictive distribution. But I can t see the connection between them. The proposed algorithm is interesting and well supported by experiments. My concerns are that a more intuitive description should be added to make the motivation, rationality, and insight of algorithm design easy to follow.<|endoftext|>This paper proposes a meta learning approach based on MAML but with a task conditioned initialization, which takes into account both information extracted from the features of the task (support set) data, as well as geometric information from a “rehearsed” learning path (obtained by gradient descent on the support set). They then combine the path and feature embeddings via an additional neural network to generate the final task specific initialization. They experimentally evaluate their approach on few shot classification and cold start recommender problems and show performance gains over similar MAML based approaches. The paper is also nicely written, well organized and for the most part easy to follow. Overall, while the paper presents some nice ideas, is technically novel, well organized and easy to follow, I have some concerns primarily about the relationship to prior work that make me lean towards rejection at this stage. Despite the nice ideas presented in this paper, the significance of the work is low if the results aren’t comparable with conceptually simpler and computationally more efficient baselines. How many steps of gradient descent were performed in these experiments?<|endoftext|>This paper builds on prior work on the feature based task characterization to integrate the rehearsed task gradient descent trajectory into task representation. As the rehearsal task is computationally expensive, the authors learn a different network to estimate the rehearsed task trajectory characterization from the feature representation. The proposed framework is tested on few shot image classification (meta dataset and miniimagenet) and cold start recommendation tasks. The paper is well written. It would be interesting to see the results of these deeper networks based meta learning models on the meta dataset curated by the authors. Overall, it is a well written paper with interesting and, at times, conflicting ideas. post rebuttal updateThank you for the clarification on the  shortcut tunnel assumption. The assumption results in estimating the path cluster assignments through feature cluster assignments.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; Even though the author gives the reason for R^3 embedding space, I am wondering if how the embedding space dimension will influence the results. I am generally positive about this paper since it proposes a novel method to single view reconstruction using few shot learning.<|endoftext|>In this paper, the authors provide a novel idea for shape reconstruction from a single image. More results are needed to make it better. b) The author  generalize the differential learner with kernelized algorithms.<|endoftext|>Currently I recommend "weak accept". Have the authors tried to interpolate shapes with different topologies and were the results plausible?
Accept (Poster); rating score: 6; rating score: 6; rating score: 5; rating score: 1; This paper proposes to adapt the RTRL technique when training RNNs to make it usable in practice. Notably, this method can be used in two ways: * estimating the true gradient by computing its projection on a random low dimensional subspace of the parameters; * improve an existing estimation $\hat{g}$ of the true gradient $g^*$ by computing the projection of $g^*$ on $\hat{g}$: this projection has better properties than $\hat{g}$. I did not find any reference to methods mixing "directional gradient/derivative", "Monte Carlo estimation of the gradient", etc. However, I might have missed some references. The idea of projecting the true gradient on an estimated gradient is the main practical application of the paper. The reader may expect more datasets for the same setup, in order to show that the results are consistent over *several* tasks and datasets (and not only one per setup). # ClarityOverall, the paper is easy to follow. Pseudo code would be preferable, and this except could be put in the Appendix. 2: titles: "max_truc_len"  > "max_trunc_len"The idea is new, simple, relatively easy to implement, and seems to improve the estimation of the gradient.<|endoftext|>The paper proposes a new gradient based learning algorithm for recurrent neural networks, making use of the directional derivative along a candidate direction. The candidate direction can come from various sources such as randomly sampled directions, truncated backpropagation through time (BPTT), the “Reptile” meta learning approach by Nichol et al.(2018), or synthetic gradients. Weaknesses:  One of the main concerns I have with the paper is the assertion that computing the directional derivative has “computational cost the same as the forward computation of the function”, which the authors mention several times in the paper, referring to the cost of computing the directional derivative with forward mode automatic differentiation. This might be correct, but it doesn’t mean that the directional derivative can be computed for free with a “computational cost the same as the forward computation of the function” which to me seems to imply that both the function and the directional derivative can be computed without any extra computational cost compared with just the forward computation of the original function (without the derivative). In other words, I believe that when computing the directional derivative and the original function together, as it happens during forward mode automatic differentiation, the cost is indeed more (up to double) compared with the computation of the original function (without the derivative). I believe there is a lot of value in revisiting earlier work such as real time recurrent learning (RTRL) (Williams & Zipser, 1989), which the authors correctly identify as a related work and explain in their manuscript.<|endoftext|>The paper proposes directional gradient descent as a way to approximate the gradient in recurrent learning. Directional gradient descentuses a candidate direction (e.g., a random vector or a biased gradientestimate) and then "corrects" that estimate by scaling it with thedirectional derivative, i.e, the inner product between the candidate direction and the true gradient (or an unbiased estimate thereof). This idea has been known in the optimization literature as a "sketch andproject" gradient estimate (see, e.g., [1] and citations therein). This line of work should be cited and duly acknowledged. To my knowledge, it hasn t been studied specifically in the context ofrecurrent ML architectures, so a paper like the present one could still have its merits. The experimental comparison is a bit lacking in my opinion. On every problem, only one competing method is compared to. Except for the lacking discussion of computational cost, the paper is clear and well written. [1] https://proceedings.neurips.cc/paper/2018/file/fc2c7c47b918d0c2d792a719dfb602ef Paper.pdfEssentially, the paper is applying the known "sketch and project" trickto recurrent learning, so the novelty is rather limited. I am recommending rejectionfor now, but would encourage the authors to respond to my concerns in the rebuttal.<|endoftext|>The paper applies the direction derivative to compute the learning problems. The paper is not written well. The paper discusses the possibility of applying direction derivatives to improve the gradient descent method. There is no concrete calculations and analytical examples to support the idea.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; The paper proposes a method that designs molecules by combiningfragments along "editable" (single) bonds. The model is able to propose reasonable molecules in 3Dconformations within a ligand pocket using a docking score forguidance. Since theauthors seem to have used docking together with the 2D models, whydidn t they also use the 2D generator models on the 3D benchmarks oftable 1? (Incidentally, in Table 1, the largest score in column Divseems to be the one for GraphAF, not liGAN.) What targets did the authors evaluate in table 2, 3; and why are thebaseline results different from those in table 1? I have concerns about the reproducibility of this work and I find alot of details lacking. The English needs some work; some figures andtables have unexplained or mislabeled data.<|endoftext|>This paper proposes a method to optimize the multiple properties of a molecule. The proposed model outperforms other baselines in the multi objective molecules optimization benchmark. 2.This model doesn t rely on a data driven biological activity predictor. The model seems to be incremental. The good thing about using a data driven biological activity predictor is that binding site information is not required. However, when using docking software, data driven predictors of biological activity are not required, but the 3D structure and binding site information are essential. I have some questions/suggestions;1.<|endoftext|>The authors claimed this is an unsupervised drug design method that the training is conducted in a purely training data free fashion. Therefore, it is important to report the cost and the comparison with other baseline methods. The representation of the molecule is modeled at atom level and fragment level. 6.The training cost is also important since this is a self learning method and the training data is generated by the model itself. The method is evaluated on the drug design task on two sets of target datasets, by generating the molecules (drugs), the results are evaluated by multiple metrics. The performances are improved compared with other baseline methods (including both 2D and 3D methods). The main idea of the paper is summarized in the above sentences. As for the knowledge, as far as I read, including (point out if I am not correct): autodock vina (for target ligand energy), QED (drug likeness), SAscore (for synthetic accessibility). The editing method is in a reasonable design, which selects the bond and the fragment, then generates a new molecule. Besides, the authors even do not talk about RL at all.<|endoftext|>The authors propose an approach for drug design that generates molecules by simulating adding or deleting parts of the molecules, and using graphnets to capture atom and fragment level information and construct new molecules. They use simulated annealing to ‘edit’ the 3D structures, and docking simulations, drug likeness and synthesizability to provide information back into training. The authors compare with multiple baselines on a test set of 12 targets, including the current SOTA model, and report improved performance.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 10; The paper proposed differentiable scaffolding tree (DST), a novel method for molecular generation, that allows for a continuous gradient based optimization of selected molecular properties. The method also required less oracle calls (and majority of them before starting the optimization), compared to the other methods. The results reported by authors are really promising. Thus there is no need to train a latent space model before optimization. Cons:  In my opinion the section that describes the experimental settings  is not sufficiently developed. E.g.I cannot see how many molecules were generated and how many optimization iterations were performed for every molecule (the authors wrote that DST reached the highest scores within T   50 iterations, but they do not specify whether they use this amount of steps in every experiments). The only drawback is the weak section describing the experimental settings, which, however, can be easily corrected.<|endoftext|>The paper designs a novel model to optimize molecular leads with a differentiable scaffolding tree. DST implies a novel direction of resolving the problem of lead optimization. 3.**Determinantal point process**: The paper leverage DDP to keep expected properties as well as the diversity of generated molecules. **restricted backbone**: The backbone of the oracle model (as GNN used in the paper) must be a differentiable model. For example, if the oracle response comes from the real wet experiments, the proposed method is unable to get gradients from the wet experiments. 2.**Local optimum**: SInce the proposed method generates new molecules step by step, it is possible that the selected pathway of generating new molecule is not an optimal solution. 3.**Costly assembling process from scaffold tree**: it is also a problem of JTVAE by practice. Assembling molecular graphs from scaffold tree is very time consuming. This could be easily achieved by adding constraints to learnable N, A, and w. [1] Jin, Wengong, Regina Barzilay, and Tommi Jaakkola. International Conference on Machine Learning. Overall, it is a good paper with some interesting designs like DST.<|endoftext|>The contribution is chiefly in the mutation step of the GA, which roughly uses a pre trained graph neural network to take the derivative with respect to the adjacency and node identity matrices of a graph to produce a probability distribution over mutated graphs. They show results on several different molecular optimization tasks which suggests that their algorithm outperforms several baselines. I thought that this paper was well written and had a lot of genuinely interesting ideas. Why should I care about novelty and diversity? Why were your baselines chose, beyond just being other methods which work for molecular optimization? I think it is a very interesting contribution  the details of the DST method are very well described in section 3. I think it would be possible to re implement their method based on the information given, which is a good sign  Choice of experiments seemed good (I m glad this is not yet another paper maximizing logP). Cons    Looking at algorithm 1 (the main contribution), it is clearly a genetic algorithm. Tentatively I think I will recommend rejection due to these concerns, but I would be happy to raise my score if they are addressed. I found it slightly odd that the authors did not point out this connection, or use any terminology related to genetic algorithms. While this is sensible, I can think of many similar alternatives which are much less complicated (see below). In general, I think it is more fair to show comparisons like in Figure 3, which plots performance as a function of the number of oracle calls. This is the approach taken by most GAs. Aware that there is a page limit, I nonetheless think that the following things needed to be in the main text:  1.<|endoftext|>The molecular optimization is an essential chemical science and engineering task that has important applications such as drug discovery. Difference to related works: Most of the existing deep generative models do not scale well on large datasets due to non differentiable nature of molecule structures and they often heavily rely on brute force enumeration. Limitations: More quantitative performance comparisons with related works are needed. Why did the authors allow for single atoms to be utilized as "substructures"? This paper created a molecular graph locally differentiable by using a differential scaffolding tree (DST). The authors developed a generic molecular optimization approach based on DST, which was validated through comparison when previous methods and benchmark datasets. The paper appears well written and the results are promising.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; However, for NMT efficiency attack is not as valuable as accuracy attack. As mentioned in the paper, efficiency is dependent on the length of the output sentence, which is controlled not only by the NMT model itself but also by some heuristics (such as length penalty and max length). To this end, it proposes a method to guide the input modification such that the probability of EOS is as low as possible. The paper is well written.<|endoftext|>This paper focuses on the efficiency of neural machine translation (NMT) systems, proposing a novel attack approach to test the efficiency robustness. In the paper, the authors repeatedly mention the relationship between NMT efficiency and output length. The paper is easy to follow and the motivation is very clear.<|endoftext|>Weaknesses:  The paper considers the white box attacking scenario, which is less significant than black box attacking which fits the real world application more. The paper proposes an interesting idea of a new attacking target of NMT models, but has some major flaws such as limited contribution and inadequate evaluation. Minor points:  The insight behind of Equ (2) is not introduced. Therefore I suggest the authors add this part to show that the proposed method can increase the robustness of the NMT model w.r.t the computation complexity attacking.<|endoftext|>Raising this question and demonstrating an effective attack would be a valuable contribution to the community. First, all NMT systems in practice use a maximum sentence length, which is usually set to a factor times the source sentence length. I still think that the paper has merits, it just needs to be more upfront about the details and the limitations of the results.
Reject; rating score: 5; rating score: 6; rating score: 6; The authors propose a risk sensitive reinforcement learning algorithm based on DDPG for the problem of risk averse option hedging and equal risk pricing based on dynamic expectiles. The paper addresses the important issue of risk sensitive RL. Usually, when it comes to methods targeting a specific application, the paper must at least achieve one of the following two:1) Can the proposed approach be easily applied or generalized to other problems or application domains ? This is not obvious. The dynamic risk measures have very good theoretical properties   everything is Markov, and therefore very convenient for dynamic programming based solutions. I can accept these as significant but the evidence presented in the paper is insufficient.<|endoftext|>This paper targets a fundamentally important issue: risk aversion in pricing and hedging. The authors claims the first algorithm to identify optimal risk averse for option hedging strategies, which are time consistent with respect to a dynamic convex riskmeasure, etc., those are impressive. A first question I would hope the authors to clarify is that: from the organization and presentation of this paper, this paper is more like an application paper of DRL to pricing and hedging tasks. If it focuses on one particular application, then an ML&finance conference or journal would be a better choice for this work.<|endoftext|>The paper offers a solution to the problem of Equal Risk Pricing (ERP) by framing it as a risk sensitive MDP formulation. The work is novel since, as highlighted by the authors, they provide the first DDPG like algorithm optimizing a dynamic risk measure. The paper has also some other passages which may be difficult to follow, for instance in the derivation of the DDPG like algorithm and in the experimental analysis. The possibility that a seller’s policy may actually increase risk when applied to an option with shorter maturity is clearly problematic here", however, it is not clear to me how this is apparent from the results of Table 1. The paper offer a novel perspective on Equal Risk Pricing for option hedging using reinforcement learnin and dynamic risk measures. The contribution is mainly an algorithmical and experimental one, but the proposed approach seems to be sound and effective. I would like to propose a borderline accept to this paper.
Reject; rating score: 6; rating score: 6; rating score: 6; rating score: 6; rating score: 8; The paper introduces a geometric perspective on VAEs trained with a Gaussian mixture prior (standard multivariate or learned mixture of Gaussian): the covariance matrix encoded for a given input image in a VAE corresponds to a Riemannian metric (inverse covariance) around the local neighborhood of the data point encoded. The authors suggest sample generation in VAE  should be based on this covariance induced distance (Mahalanobis). International Conference on Machine Learning. PMLR, 2020. Experiments using samples generated using this method shows improvements in FID and precision recall measures in MNIST, SVHN, CIFAR10, and CelebA datasets. The authors review Riemannian geometry concepts that are relevant to the method proposed and it is easy to follow through this part of the paper. Experiments show improvements over RAE’s, VAMP, and RHVAE in terms of FID score and classification with a VAE generated set of data. It might be interesting to see if the proposed sampling can be used to improve other VAE models. More critically, what if the covariance matrix is not invertible? This is by no means specific to this paper but I would be interested in knowing the author’s perspective on this.<|endoftext|>It is a theoretical paper that discusses the Riemannian structure of the latent space of a VAE. The measure allows to define a Riemannian normal distribution on that space/manifold, to compute geodesics but also to "better" sample. The idea is to equip the latent space with a Riemannian structure, the measure of which depends on the inverse of covariance of the posterior. Before sampling, the distribution needs to be approximated using MCMC. * The baselines are chosen accordingly to the theoretical line of the paper. ** "Now that the VAE has learned and capture the intrinsic geometry of the data within the latent space seen as a Riemannian manifold," **    Why is the variance based measure necessarily the one that captures the "intrinsic geometry of the data"? Such a situation is not allowed by the theory developed here. **  VAE+GMM is almost as good as the proposed method in Table 2. Nevertheless, I have doubts about the importance of the contributions and their impact on the field.<|endoftext|>The paper proposes to analyse the VAE framework using differential geometry tools. More specifically, they propose to impose a (Riemannian) metric in the latent space of a VAE. This work proposes a novel interpretation of variational encoding by the prism of differential geometry. In particular the data $x_i$ provides two main elements, the “position” $\mu(x_i)$ and instead of interpreting the estimated $\Sigma(x_i)$ as the variance of the normal law i.e of p(z|x) it is interpreted as the value of the Riemanian metric at $\mu(x_i)$. In that perspective, the authors propose to sample not from an isotropic Gaussian of $\mathbb{R}^d$, but rather using uniformly from the learned manifold. On the Experiments:1. 2.Because the sampling method in the latent space is dependent on the chosen support vectors, did the authors observe any variance in the quality of the samples depending on the choice of the K centroids (something like variance of the FID at a defined number of support vectors)? I would be glad to increase my score if the major concerns are answered. Yet, the theoretical link between the VAE and the Riemannian distance based VAE requires a more thorough discussion.<|endoftext|>The paper proposes to generalize the vanilla VAE by formulating the conditional distribution and the marginal distribution using the Riemannian geometry. The paper proposes a general perspective on VAEs through a geometrical perspective. It generalizes some papers on VAEs with various latent manifolds (e.g., hyperspherical VAE, VAEs with Poincare disk). If data is binary, the Bernoulli distribution is taken; if data is continuous, we can pick from a plethora of distributions. Second, a very common choice to model RGB images is a discretized logistic distribution rather than a Gaussian distribution. The authors mention that they use Eq.(7) as the Riemannian metric and the HMC sampler for sampling, however, it is not necessarily obvious what they precisely use later on in the experiments. I like the paper and the idea of using Riemannian geometry in VAEs. However, the paper presents the idea at a pretty general level and it is hard to see the details that matter from the computational perspective. This is my main problem with the paper, namely, the reproducibility. Since it is unclear how to sample and how to calculate various components of the VAE, it is hard to assess how “heavy” the method is and whether it could be scaled up. Nevertheless, the new additions (especially Appendix C) make the paper better.<|endoftext|>To construct a Riemannian manifold, the authors interpret the covariance of the posterior of each latent as a local Riemannian metric. #### Strengths  The stated contributions of the paper are substantiated by the experiments. I would have liked to see some discussion of the cost incurred from HMC sampling  I would have liked to see some discussion of the relationship of the proposed approach to [1 2]. Joint European Conference on Machine Learning and Knowledge Discovery in Databases. The sampling approach leverages a novel theoretically based interpretation of the VAE latent space structure. It also appears the authors may have neglected a related body of work.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; The four types are designed to reflect different incremental learning scenarios with different semantics.The authors have also conducted extensive empirical validations, including pre training >500 models on four categories of pre training streaming data from ImageNet and DomainNet, and evaluating them on three types of downstream tasks and 12 different downstream datasets. I have some concerns and comments as follows:    1. The task studied in the manuscript is highly practical. 3.Intuitively, why do self supervised models forget less than supervised models?<|endoftext|>This paper studies this kind of setting and says that in supervised learning, the gap isn t big (https://arxiv.org/abs/2010.08127). Finally, they reduce some of the gaps with data replay and regularization. I am unaware of previous thorough empirical studies on the effectiveness of self supervised pre training on streaming data. There isn t much in the way of explanations of the observations, and the methods aren t novel, but the observations are valuable. Some evidence for this: there is a larger gap between streaming and non streaming when the downstream evaluation is few shot learning (less finetuning) in Figure 3.<|endoftext|>This paper studies how models perform on downstream image classification tasks when they are trained via self supervision using streaming data. I would suggest the authors to experiment on much longer sequences such as 10 or 20 chunks. In the case of the latter, my concern will be that the setup is not really working on disjoint chunks and the observations may not be valid. I find this analysis particularly insightful. The authors have addressed some of my concerns. This information can be useful to understand in what type of sequential self supervised training scenario, there may be a need to experiment with continual learning approaches.<|endoftext|>This paper explores a more realistic situation for self supervised representation learning of training with streaming data. Although exploring the problem of sequential self supervised learning is meaningful, it is hard to find insights beyond empirical results to make improvement on the top of the discovery of this work in this paper. However, I am open to any objection and further discussion, if the authors make their point and insight of this paper clearer. It seems that the analysis results in Section 5 are not very helpful for understanding.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 8; This paper provides generalization guarantees for exponential family sgld which is a more general setup than standard gaussian noise langevin dynamics as it allows for noise settings from any member of the exponential family. Overall, from useful empirical observations it seems that the bounds proposed by the authors are tighter than these other bounds. As such there are several tools for such an analysis that are pretty well known now. As such the main message/contribution of the paper and the empirical section seems disconnected. I am happy to be corrected. The main contribution of exponential family dynamics seems not very significant. Gradient discrepancy is sidelined as a minor contribution but is focussed on a lot more in the empirical section, which makes the paper seems disconnected.<|endoftext|>The analysis of the error bound is based on the control of the expected stability. Strength.1.The paper proves the generalization error of  existing  noisy stochastic iterative algorithms to have a $\mathcal O(1/n)$ sample dependence. The generalized error bound is based on expected stability, which seems to bring new ideas for general exponential family noise. Weakness.1.The proposed algorithm does not show state of the art performance. Without strong support from the empirical study,  it is not convincible to believe why one bothers to study such more complicated cases. 2.Adding to the above point, the paper only reviews the SGD in the literature, there have been very recent works based on Langevin dynamics, e.g.replica exchange Langevin dynamic SGD, second order Hamiltonian Langevin dynamics. The paper seems to provide a new family of SGD algorithms. The theoretical analysis is recognized and deserves their own interest.<|endoftext|>This paper improves generalization bounds based on the notion of stability. It generalizes the analysis to noises in an exponential family rather than Gaussian noise. This paper provides several technical improvements to a recent line of work that I am not familiar with. The authors largely comment the articulation of their work with similar works, but almost never comment the relevance of their work for a wider community. As a naive question, I would like to ask why expected stability bounds are preferred. Or in theory, to analyze the performance of some algorithms? In Section 2, I find difficult to determine what are your contributions and what was already known. Similarly, to what extent was Proposition 1 known? I am not confident with my review as this paper is quite technical and I am not familiar with this line of work.<|endoftext|>This paper studies the generalization of iterative noisy learning algorithms. They they use this bound to provide a generalization error guarantee for a broad class of noisy iterative algorithm which is defined as W_t   W_{t 1}   eta_t, where eta_t conditioned on the past iterated follows an exponential family distribution which for instance include SGLD. Finally, the authors provide numerical study of their bound for several benchmarks and show that their bound achieves better estimate of generalization error. The dependence of the learning rate on the optimization trajectory: In your bound alpha_t depends on the dataset and trajectory. Therefore, I think it can limit us to use very small learning rates. Recently, the bound in Haghifam et al 2020 is extended to SGLD in [1]. Applying your framework for analyzing SGD:  What are the roadblocks for applying your bound to analyzing SGD? [1] "On random subset generalization error bounds and the stochastic gradient langevin dynamics algorithm", Borja Rodríguez Gálvez, Germán Bassi, Ragnar Thobaben, Mikael SkoglundVery interesting and solid contribution!
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 8; Assuming the given model class satisfies realizability, regularization property, bounded Eluder dimension, low covering number, and the true dynamics is the stochastic control model under gaussian noise transition (eq 5), the authors show the polynomial Bayesian regret guarantee. However, I believe that model based learning might be more suitable for this paper. One major difference between the current paper and the prior representation learning/reward free learning work is that the algorithm in this paper can only handle a single reward while previous model based or model free works can learn a representation that tackles multitasks (multiple rewards). I think such differences should be discussed in the paper. Moreover, some claims are not accurate. It is mentioned that “These algorithms learn a uniformly accurate model through a reward free exploration, upon which decouple the learning from the exploration”, but Modi et al., 2021 is a model free algorithm and does not try to learn a model. Modi et al., 2021 first proposes to solve min max min optimization instead of min max min max optimization as one operator has a closed form solution. For the theoretical part, the assumptions are not written clearly. For example, I believe equation (5) is a crucial assumption, but it is not discussed explicitly in Section 4.1. The related discussion about the Bayesian regret and the definition of Bayesian regret E_{Pf} are missing. I feel the title "a free lunch from the noise" oversells the paper. The algorithm is the standard model based TS algorithm. # Experiments #I think the most significant contribution of this paper is the experimental results. In the experimental part, it’s unclear how the algorithm is implemented. How do you choose the function class F and the prior distribution p(f)? I believe the clarity of the writing can be improved. The theoretical contribution is rather limited, and most significant contribution seems to be on the empirical side (I have to say I m not familiar with the empirical work).<|endoftext|>This paper studies representation learning in the context of reinforcement learning, and observes that under some noise assumption, the linear spectral feature of corresponding Markov transition operator can be obtained in closed form. Then the paper proposes the so called SPEDE algorithm that enjoys good theoretical guarantee and empirical results. The paper studies an important problem in reinforcement learning, and shows some interesting observations. 1.In my opinion, this paper is more like one on model based learning since Equation (5) is the main structure studied in the paper. For computational efficiency, [1] also proposes a Thompson sampling (TS) like algorithm that does not need such an oracle. In my opinion, it might be straightforward to show the Bayesian regret bound for this TS like algorithm using the results in [1] and the standard and well known results in [2]. 3.In terms of the transition dynamics, I think it would be beneficial to the readers if the authors could say more on the dynamics that covered by the paper but not covered by [1]. In my opinion, providing some (simple) examples could be helpful. I think this is an interesting paper, and I would like to understand better the technical novelty.<|endoftext|>On the theory side, the paper considers the setting where the state transition is a nonlinear function of the past state action plus additive noise, and develops a no regret algorithm. On the empirical side, the paper shows that an adaptation of this algorithm on real world RL tasks could perform better than existing model based algorithms. **Strengths**I think overall this paper considers an interesting problem (learning representations) that is of interest to the community, given that a major part of the RL theory literature focuses on learning with a known linear representation or structured function class, and much less is known when it comes to learning the representation. It is a plus that this paper contains empirical results too on real RL benchmarks, which I do appreciate as for a paper with a bulk part being theoretical contributions. **Weaknesses**One of my main concerns about the theory part is that this theory may be a somewhat cute but very specific consequence of the Gaussian noise assumption. Also, from the current paper’s presentations, I find it quite hard to situate the current contributions in the context of representation learning theory in RL, and compare with related work. The real problem setting considered in this paper appears only in equation (5) and is not emphasized as a problem setting. Could the authors discuss a bit more in detail how the proof compares with the standard Eluder Dimension proof (e.g.of Wang et al.)? Currently the proof sketch does not say much where the bounded Eluder dimension is used, and how is the application different from the existing Eluder dimension proof. (This one is about experiments) When deployed into practice, the main difference between SPEDE and existing model based algorithms is just that the parametrization (3) and (4) is used? Overall, I think this paper makes interesting contributions on both the theoretical and empirical end of representation learning within RL. However, significant work needs to be done in order to clarify its problem setting, results, and position within related work.<|endoftext|>This paper proposed a practical exploration algorithm for finite horizon RL problems and provided a theoretical guarantee for its algorithm when the transition kernel of the RL problem can be encoded with RKHS. Strength.This paper extended the model in LC3[1] to kernel setting, and it also used Lemma 16 to remove the dependability on the global Lipschitz constant. It also conducted experiments to show that its algorithm is practical and efficient. Weakness.After going through the proof, I notice that the regret bound is proportion to $\sigma^{ 1}$, which means that the bound will be bigger when the noise level decrease and should be emphasized in Theorem 5. Although there are some details in Section 3.2, there should be more detail about the posterior sampling in the experiments section since this paper claimed to provide a practical algorithm and this part is not trivial. Information theoretic regret bounds for online nonlinear control. This paper is significant, in the sense that it extends LC3[1] and provides a theoretical guarantee for a setting that is more complicated than [1]. It also uses a novel technique to utilize the noise in transition to remove the dependability on the global Lipschitz constant. Although the dependency on noise might have room for improvement, I think it is a good paper and could be accepted.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; The paper considers a variety of speech degradations   additive noise, reverberations, clipping and limited bandwidth. 8.While such a two stage approach makes sense, the connection between human hearing and the proposed approach is rather weak. Experiments are done using the VCTK dataset and experiments are done using single distortions as well as combinations of all 4 distortions. 2.Overall, I find very little to no novelty in the paper with respect to the proposed method. Experiments are done using only the VCTK dataset. The analysis and synthesis approach as well as the method for each stage are not particularly novel. Overall, the paper lacks strength on both fronts in the current form. Considering that the two stage approach and the methods used in each stage have been done before, this would have helped establish the significance of different approaches for analysis and the synthesis stages. In the latter case, these are also two stage approaches ?? 7.The paper claims to be solving general speech restoration. This could have been a focus of the paper.<|endoftext|>1.Using a two stage model, which to my understanding is presented as part of the novelty of the paper, is a common strategy among speech enhancement systems that tackle more than one distortion (see refs.[4 9]).Usually, those are already composed of an analysis plus a synthesis step using a vocoder. 1.Architectures for the analysis and synthesis modules are known and the authors do not perform any substantial modification (or at least they do not mention them nor evaluate their worth). In this sense, a more specialized venue could be perhaps more appropriate (e.g., InterSpeech, ICASSP, WASPAA). Lack of novelty from a machine learning perspective. [$ $] No details on subjective evaluation. The paper positions itself as "propose[ing] a general speech restoration (GSR) task" (example sentence from the Abstract). However, the task has been proposed for quite some time now; the only difference is that perhaps the 4 specific distortions considered in the paper have not been considered together by previous work. Only in the tasks of super resolution and dereverberation the proposed system seems to have a winning edge. This is not bad per se, but puts into perspective the value of the approach.<|endoftext|>In order to solve the problem of the general speech restoration task, the authors propose a U Net architecture which is trained on all of these tasks simultaneously during training time. The experimental results show that the proposed VoiceFixer combination of the model and the analysis synthesis procedure are capable of effectively removing the speech distortions and in some cases outperform previous approaches in the literature. It is also important that the authors have conducted subjective evaluation experiments to show how their algorithm performs. I think that the paper could have a great potential in the field, however, in its current form, the paper has several limitations which downplay its true potential. Moreoever, there are a few misconceptions in the experimental section that authors need to address before the paper is ready for a publication. This is extremely odd and is not at all consistent with Figure 6b.<|endoftext|>In addition, it also presents a generative framework called VoiceFixer consisting of analysis and synthesis stages to address the general speech restoration task. Their idea was well described and the experiments are systematical and extensive. The contribution of this paper is to incorporate a variety of speech restoration tasks including speech denoising, super resolution, dereverberation, declipping, etc. They also propose a two stage based generative framework called VoiceFixer to address the general speech restoration task. The authors report that VoiceFixer outperforms the GSR approach in the experimental results. A weak point may be that VoiceFixer restores speech as a synthesis form by employing a neural vocoder which inevitably entails some degree of distortion when compared with real speech. The reviewed paper proposed two new approaches for speech restoration.
Reject; rating score: 3; rating score: 6; rating score: 6; rating score: 6; This paper proposes a new active learning strategy that samples examples which would help the network converge faster during training. The paper suggests that faster convergence during training typically leads to better generalization performance. I found the paper s experimental section to be insufficient. This speed up is measured by approximating the derivative of the of the loss with respect to the training iteration t (which they call training dynamics). Weaknesses:The main weakness of this paper (and the reason I am not recommending it for acceptance) is its experimental section. The theoretical section is well written, but the problem is completely non tractable and cannot be theoretically analyzed in a realistic setting so the experimental section is instrumental and necessary to support the paper s claims. However, I don t think the experimental section is thorough at all. The only difference between your experiments and those in the CoreSets paper is that you don t retrain your models from scratch at each round and instead you continue training from the previous checkpoint. However, using a small initial budget benefits methods that don t rely heavily on the classifier s feature extractor. In International Conference on Learning Representations, 2018.<|endoftext|>The authors address the problem of active learning in the context of deep learning. Thus, they propose to optimize the "training dynamics", which is the time derivative of the loss function in the ultra wide limit. The paper looks good. Some of the results rely on approximations or numerical evidence which might not be universal, but I find the method quite interesting, and it paves the road to new possibilities in deep active learning. They then show that this is also true in the active learning setting, where the data is no more iid, with the help of some empirical evidence. From there, everything descends smoothly, providing a clean story and a method that seems to work well. Even though the theoretical analysis is only justified in the ultra wide limit, the empirical evidence provided (on satisfactory models and datasets) suggests that the method is still good for typical use cases. In particular, it is not clear that the dominant term of the population risk is given by B (which then goes as 1/Alignment). Then, they show empirically that MMD<B for several values of b, on the first query round:  They state that  the MMD is always much smaller than B, but the blue and yellow bars in figure 1 look similar. This can be expected, since with a bigger b the two distributions are more different. Would it be more interesting to compare at constant budget size, as done in table 1?<|endoftext|>This paper provides a theoretically motivated active learning algorithm based on maximizing convergence speed, which is justified by the "train faster, generalize better" intuition. The paper has limited but favorable empirical results. This paper has a nice derivation of the dynamics of the training loss and provides an algorithm to maximize the rate of convergence. My biggest concern is the empirical evaluation. I think the experiments require a larger amount of data (at least up to 10k labeled points for CIFAR10 and SVHN). Other comments:In equation (15), is the gradient squared supposed to be in the summation? How does maximizing over a restricted (smaller) set result in a larger value? The theory in this paper appears good, though I am not very familiar with NTK analyses which makes me hesitate to say more.<|endoftext|>This work proposes DynamicsAL, a novel AL criteria that selects new training example base on its ability to maximize the training dynamics $\frac{\partial}{\partial t} l(f, y)$. The authors supplied a practical algorithm (Section 3.2) and compared the proposed criteria with existing methods (Section 3.3). Strength:The paper is technically clear and well written, with good result on toy benchmarks (i.e., clean, small images with mostly balanced distribution). The method seems to be a good continuation of Ash et al., 2020; Liu et al., 2021 and built a theoretical bridge between these objectives with the training dynamics and generalization in deep learning theory justification. Weakness:* The infinite width assumption: The main theoretical result (Theorem 1) is derived under the assumption of infinite width model. While I understand the authors are building their results based on classic NTK theory, it is still worth addressing (can be non rigorous or based on small empirically verifications) cases where the infinite width assumption does not hold. * The Pseudo Labeling approach: While I understand this is a technique that is being used in the previous works, I wonder what would  would happen if model prediction is wrong, such that the training dynamics is computed against the wrong label? A paper that continues the investigation of active learning using gradient/influence function signal (Ash et al., 2020; Liu et al., 2021). I personally find this work to be a nice value add to the community, both in terms of the theoretical contribution and the practical utility of the algorithm.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; rating score: 5; This paper studies Markov games with large number of agents. The authors start by proposing a novel discrete time formulation for graphon mean field games. They show that the mean field game exists, and that finite graph games converge to the mean field game when the number of agents goes to infinity. They also provide an algorithm that learns an approximate Nash equilibria. The results in this work are novel and important. The paper is well written.<|endoftext|>This paper studies a class of games with a continuum of agents that connected by a graphon. This corresponds to the limit of games with a finite number of players connected by a graph. In the recent literature, such games have been considered in continuous time, and here the authors focus on a discrete time version. They first analyze the game by showing existence of a Nash equilibrium (defined in a suitable sense), and they prove that using the equilibrium policy from the game with a continuum of players in a game with a finite number of players provides an approximate Nash equilibrium. Then, two numerical approaches are proposed, both based on fixed point iterations that alternate between updating the distribution and updating the optimal control. Furthermore, the authors provide rigorous proofs for the statements of their main results. The proof techniques are quite classical in the mean field game literature, but it is nice that the authors provide detailed proofs in the appendix.<|endoftext|>This paper studies discrete time dense graph mean field games (GMFG). It discusses the settings of both finite agent graph game and GMFG and shows that under mild Lipschitz continuous assumptions, the Nash equilibrium of the GMFG exists. It then proposes two algorithms for solving the NE of GMFG, one with the idea of discretizing the graphon index, and the other utilizing the equivalence of GMFG to a classical MFG with an extended state space. The proposed algorithms are tested on an SIS Graphon problem and an Investment Graphon problem to validate their performances. The authors should give some discussions on when the assumption holds or how to verify this assumption, especially the existence of the Lipschitz mapping $\hat{\Phi}$, which is an element chosen from a set mapping (e.g., how it is chosen). This paper provides some nice contributions to the theory of graphon mean field games in discrete time settings.<|endoftext|>This paper proposes a discrete time graphon mean field game (GMFG) framework as a limiting model to approximate large population dense graph games. Existence of graphon mean field equilibrium (GMFE) is proved by reformulating GMFG as a classical MFG, and a graphon mean field approximation result is also obtained. The authors then propose two learning algorithms for computing GMFE, one based on discretizing the graphon index, while the other based on the aforementioned classical MFG reformulation. Some preliminary numerical experiments are also provided to demonstrate the theoretical findings and to validate the proposed algorithms. [Major Pros]This paper proposes a novel GMFG framework, with a relatively complete characterization of the GMFE existence and graphon mean field approximation errors. "Sequential decomposition of graphon mean field games."
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; This paper considers the problem of differentiable neural architecture search for RL applications. It applies existing DARTS to the RL setting and conducts extensive experimental studies to examine the performance and behaviors of RL DARTS. In addition, the work also carried out extensive experiments to examine the behaviors of DARTS for RL. Weakness.The main weakness of the paper is its lack of novelty and insufficient technical contribution. The work is mainly an application of standard DARTS to the RL setting, without additional contributions in the algorithm side. While it is indeed interesting to observe that existing DARTS from SL could be readily applicable to RL, it does not play a major contribution for the current paper.<|endoftext|>This paper proposes to combine differentiable neural architecture search (DARTS) with standard reinforcement learning (RL) frameworks by searching the model structures (convolutional cells) for the policy and value functions. The authors have applied the method to infinitely procedurally generated Procgen benchmark and demonstrated the benefits of DARTS in RL on search efficiency in terms of time and compute. [Strength]This paper aims to tackle interesting but less explored questions: while there have been extensive studies on searching network architectures for supervised learning, it is generally less understood whether and to which extent architecture search would benefit reinforcement learning pipelines. Are there any similar tradeoffs we can make in the RL setting, e.g., would smaller/sparser neural networks allow higher feedback control frequency such that control becomes easier? However, the statement may not be well grounded without context, e.g., to which objective is the "optimum" referring? As shared by most reviewers, the novelty of this paper is a bit limited, and as I have stated in my main review, I m not against A+B types of research (in this paper, A Reinforcement Learning, B Differentiable Neural Architecture Search), but I typically have high expectations from such straightforward combination.<|endoftext|>This paper studies the use of DARTS within the RL setting. The proposed approach is evaluated using the Procgen benchmark, and compared to the IMPALA CNN architecture. The authors give a nice overview of common issues of using DARTS within the RL framework, specifically compared to supervised learning settings. I found the explanation of the training procedure a bit unclear, and it would be beneficial to focus on this more, as this is one of the main contributions. The performance does not seem to be better compared to the Vanilla benchmark in 4.2.<|endoftext|>To bypass the bilevel optimization target in DARTS, which could prove to be intractable for RL context, authors propose to optimize the loss on cumulative returns of the RL problem by optimizing the loss on architecture and weight parameters from the replay buffer samples. 3.The experiments are indeed quite extensive and the authors have attempted to study the effect of apply end to end DARTS in RL from many possible dimensions including ablation studies on how the responses change in different kind of convolutions. but prohibitive search space becomes a bottleneck in most cases. 1a.One clear example of this is the fact that authors point out some interesting challenges wil RL earlier in the paper such as "RL training curves possess considerably more variance and noise than SL curves". This is quite a big problem and RL DARTS formulation does take care of this as well. JUst stating that it was integrated with minimal code change is not sufficient. The formal algorithm provided here is also a surprise. It literally has 3 steps. Diversity in domains is needed. Procgen is a great becnmark indeed.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 5; rating score: 6; On the task of breast cancer prediction from mammography images, the experiments demonstrate that the proposed periodic sampling procedure leads to better generalization performance than existing sampling procedures. Weaknesses:  The empirical evaluations are not sufficiently convincing – the work should demonstrate its utility and generalization beyond the task by evaluating on other standard tasks and public datasets. This paper proposes a new sampling procedure to tackle the challenge of dataset heterogeneity in medical imaging and illustrates its generalization performance on breast cancer prediction with mammography images.<|endoftext|>This generally questions contribution of this work in the wider context. Minor comments:  The criteria for splitting into DSs provide an important discussion, yet are not very clear and therefore not very generalizable. The authors mention that a "surprisingly nice validation metrics" is a good indicator to divide into DSs.<|endoftext|>It shows that naive approaches of training models by sampling from the various subsets perform poorly. The authors propose an algorithm for mini batch sampling that overcomes this challenge and evaluate it on a large unspecified mammography dataset. It highlights an important problem in the medical imaging domain, not limited to mammography. Research methodology is good. Results are given only on one unavailable dataset (or set of datasets).<|endoftext|>More experiments are needed to quantify its benefits for other datasets. This paper argues that training mini batches should not only be label balanced, but also balanced in terms of biases that may exist in the dataset itself. This is a good observation that stems from their experience with mammogram data. The proposed PS method is simple and easily reproducible, so most of the paper is devoted to describing the data and the experiments. The paper only experiments the method on one mammogram dataset.<|endoftext|>The paper has proper definitions for new terms and explanations for the dataset, methods and evaluations. The authors do not mention if this could be extrapolated to various other datasets. DSs are homogenous subsets of a dataset that share some attributes. There are some typos in the paper that would need to be corrected.
Reject; rating score: 1; rating score: 3; rating score: 3; rating score: 3; This paper introduces an interest based item representation learning method with a multi interest capsule network. The authors use an auxiliary task to learn interest based item representations, which are further combined with item representations learned from their features for final recommendation. 2020.The motivation of this work is not clear. The coverage of related work is quite limited. The authors do not provide evidence on this point. It seems that the proposed method is in fact a kind of ensemble. The authors should track the recent advances in the recommender system community.<|endoftext|>The paper proposes an interest based item embedding representation to enhance the performance of recommendation model by jointly learning iitem based item representations and interest based item representations. Novel model to jointly learn of item based and interest based representation of the item for recommended systems2. Practical application in recommended systems with real datasets. 2.There is no theoretical justification to support the advancement of the proposed model compared to the previous techniques.<|endoftext|>This paper studies the item recommendation task. The MICN can improve very slightly on the Amazon books dataset. Well organized: This paper contains basic elements that are essential to a research paper. Weak baselines: The baselines included in the experiment part are not very strong. I suggest the authors proofread the draft more and fix all grammar issues and typos. 2021.This paper studies a practical yet well studied problem by applying existing techniques. The reviewer finds the paper well organized. However, this paper has limited contribution, insufficient experiments, and missing details.<|endoftext|>This paper proposes a user multi interests capsule network (MICN) to improve better item representation. The intuition of the proposed model is unclear. S3.The model architecture is simple and easy to understand. I recommend that the authors consider recent publications for the CTR problem as below. W3.The technical novelty is incremental and unclear.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; This paper presents an interesting idea of using graph based neural networks (GNNs) to find an optimal layout path during de novo genome assembly using the Overal Layout Consensus (OLS) method. Existing methods perform this step by using heuristics to simplify the graph (removing edges/nodes)  instead of finding a path to assemble the genome from the assembly graph. These probabilities can help in defining a path on the assembly graph that can lead to faster de novo assembly during the Layout phase. Strengths:+ The paper presents an interesting idea of using Graph based deep learning to learn the path for genome assembly thus extending the previous neural network based assembly idea to graphs. Weaknesses:   The results in the paper are not comprehensive enough to support and highlight the claims in the introduction. Is the inference of a model trained from simulations then generalizable to another chromosome or another genome with actual reads? How will such a framework work for de novo assembly? It would be useful to explore the potential cause of the differences in alignment performance of the GNN method (and others) across 2Mbp, 5Mbp, and 10Mbp mini references. The proposed idea in the paper is interesting, however, the limited set of results are insufficient to support its claims and highlight its usefulness for de novo genome assembly.<|endoftext|>The authors propose a graph neural network approach for genome assembly. In theory, one desires a Hamiltonian path that visits every node and thus forms a reconstruction of the full genome. The benchmarks aren’t sufficient to demonstrate the potential for this approach. & Chikhi, R. Minimizer space de Bruijn graphs: Whole genome assembly of long reads in minutes on a personal computer. The use of only simulated data is another problem. There is often a very large difference between solving this problem on simulated versus real data. For this reason, the field has needed to hold competitions to properly benchmark methods, such as the Assemblathon. I completely disagree with the authors that this assumption is not “far fetched”. If they believe error correction methods are a useful stage in a full pipeline, they are welcome to include such a method.<|endoftext|>In this paper, the authors proposed a novel method that reconstructs the original sequence from the assembly graph by using a graph neural network (GNN). The authors demonstrated the effectiveness of the proposed method by comparing reconstruction time and accuracy (i.e., length of the reconstructed genome) with one of the de novo assembly software (Raven) on human genome reference data. On the other hand, it would be great if the authors address issues as follows:●	Related workI think this paper is one of the applications or the extensions of existing work [1], [2], [3], [4]. ●	Experiment (dataset)In this work, the authors used the human reference genome (CHM13) to demonstrate that the model has a capacity to reconstruct the reference sequence. However, as reference genomes such as CHM13 and GRCh38 already exist, biological researchers and practitioners do not need to assemble the human genome at usual times. Thus, I think the authors should supplement other accuracy metrics in the revised version. This paper proposed a novel neural network based de novo assembly method.<|endoftext|>This paper introduces a graph convolutional network to assemble a genome from long, perfectly accurate sequencing reads. The work was demonstrated on synthetic data without any read errors and thus serves as a prototype for real de novo genome assembly problems, which are much longer and contain noise. While there are many good things about this paper to be excited by, critically, a demonstration on real data is needed. The authors should include simulations with varying levels of noise in order to test the efficacy of the proposed method versus Raven. Thus, it is unclear how well the GCN can assemble realistic data (not HiFi) for which there is plenty of data. A proof of principle using real data would make this paper more compelling. This paper provides a promising approach to use GCNs to assemble genomes. To the best of my knowledge, this is novel.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 6; The paper proposes a neural network architecture, inspired from factorization machines, for classification task from tabular data. On the other hand, F2NN, and one of the methods (SAINT) seem to be properly evaluated. This can also be misleading, since these are tasks with different properties, and the performance of different methods may substantially vary between them. IMO, analyzing and reporting them separately, not only will be more correct, but it also may provide other insights related to the performance of F2NN (which is given in Appendix C, but not discussed). There are several  deep  FM/FFM approaches highlighted in the related work. The related work can also be improved in order to better serve the motivation for the work. It is unclear, whether the referred experiments (that dispute this) concern F2NN or the transformer based models, since it seems SAINT was tested with the same architecture, and TabTransformer not evaluated.<|endoftext|>  The authors proposed a novel deep learning model for tabular data classification. What are the main advantages of the proposed method in comparison to SAINT? We can see it in Table 1 (left) but not in Table 1 (right). 3.Figures.It would be good if the authors provide a figure that overview the proposed networks. In that point of view, can we extend this method to the regression problem? I carefully read the authors  responses and other reviewers  reviews. This is also my main concern about this paper.<|endoftext|>This paper presents a general neural network framework for tabular data classification, that can recover a range of popular classification methods. This paper may pose new directions to improve current popular models given the theoretical connection. Increasing lowered rank in factorization can be explored. In Appendix A, it said "FM, FwFM and FmFM, learn C vectors for each feature", should be "FM, FwFM and FmFM, learn C vectors, one for each feature"? The paper provides new insights on a unified neural network framework that can cover a range of popular tabular classification methods. The derivation is dense but solid.<|endoftext|>The paper describes an approach for tabular data classification. Do the other tested methods outperform them? In my opinion, with a careful revision of the formalism in the description of the method, the paper can be accepted for ICLR. They have compared their method with other classifiers in a fair way, with the experiments that are well designed and well described with the right amount of details useful for the reproducibility (I hope the authors share their code). Other comments:  In §2.2 insert a reference to the Appendix where $M$ is appropriately described. The authors should describe the motivation and the validity of this operation. Before the Equation 4 authors stated that: $E_fx_f$ is the embedding of the one hot vector $x_f$ but, if $x_f$ is column vector, it should be $E_f^T x_f$.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 5; This paper shows that the geometric random walk kernel is differentiable using implicit differentiation and can hence be used in a graph neural network to match an input graph to a set of learned graphs. The experimental results show that this method is competitive with a variety of other methods. Or perhaps there is a way that this work could be extended? The authors refer repeatedly to Bai et al.(2019) for the implicit differentiation. A well written and decent paper that struggles to meet the bar for novelty and significance.<|endoftext|>Crucially, the N graphs are learned using end to end learning. Further, the authors show how to compute the RW kernel efficiently, building on previous work  (Vishwanathan et al.2010), and how to do differentiate through the architecture efficiently, building on (Bai et al.2016).The paper can be seen as an extension of the RWNN model (Nikolentzos et al.2020), which uses a finite length random kernel. Simple architecture that give good empirical results 2. E.g., by quantifiying the graph structures captured by the infinite length RW models but not by finite length RW models. The paper is well written and easy to follow. However, the idea is incremental, building largely on previous work.<|endoftext|>They work by generating features for graphs by comparing random walks present in each graph compared to a set of trainable hidden graphs. It is stated in this paper that an advantage of random walk kernels is that they are end to end differentiable, but current implementations have the weakness of only being efficient for walks of very short length. They introduce a way to efficiently extend walks to ‘infinite length’, theoretically making features generated by the kernel more informative. Otherwise, I would have concern that the reported numbers are misleading the audiences and reviewers. Although the authors make great contribution to the technical part, the experimental results have big flaw, which needs to be addressed.<|endoftext|>In this paper, the authors proposed an interesting geometric random walk neural network for graph representation. Pros:1.The idea of this work is interesting and has sufficient novelty. 3.The paper is well written and thus easy to follow. 2.The experimental results in Table 1 and Figure 1 do not show the superiority of the proposed method. [a] Xu, Hongteng. Additionally, some related graph representation strategies are not considered or discussed.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; To achieve this, a drop in replacement for spatial convolutions is designed which uses pooled local and global frame descriptors to produce the temporal calibration weights. Strengths:  The main strength of the paper lies in the novelty of applying calibration weights directly to the spatial weights in the temporal dimension. It is clear that this dynamic temporal adaption is something which can be beneficial broadly in convolution based video understanding models. The introduction to the paper is not precise on the motivation for the work. I believe that this work presents a novel idea of weight calibration that has benefits in terms of performance and model efficiency that for video understanding tasks. Its use as a drop in replacement shows that it improves performance for existing architectures.<|endoftext|>It is designed to incorporate both the local and global temporal context by using stacked two layer 1D convolutional operations and global average pooling. On top of this, the authors construct TAda2D networks by introducing the temporal feature aggregation module that is based on a strided temporal average pooling. Many experiments performed on video/action classification and localization demonstrate the effectiveness of the proposed module. I like the general idea of adaptive weight calibration and its simplicity. The paper was easy to read and understand, and the experimental results are extensive and show good performance. For example, is TAda2D the most effective way to calibrate the convolutioanl weights? I believe that TAda2D should be compared more deeply with TANet. 4) I think it would be better to add a column for GFLOPS and #Params for Table 5 and A2 because the paper put weight on TAdaConv s efficiency. It would be better to add example values as already done in footnote 2. The paper is well motivated and easy to understand.<|endoftext|>The authors proposed a novel approach for video understanding. They present temporally adaptive convolutions (TAdaConv) based on dynamic networks by adapting the convolutional weights on each frame with its temporal contexts. The paper is well organized and easy to follow. TAdaConv can be easily plugged into any convolutional network based deep models for sequential modeling. I would like to see more ablation study of kernel size over temporal dimension of TAdaConv (e.g., 3 vs 5 vs 7). 4.According to the recent progress on neural architecture search for action recognition [1], I would like to suggest the authors adding TadaConv into search space in the future work. 2021.Overall, the paper is clear and the author propose an efficient and practical approach for video understanding. So I would like to provide a positive score in the initial stage of the review and keep tuning during the discussion period.<|endoftext|>This creates a set of weights for 3D convolution that are dynamic for each frame. The approach is evaluated on multiple video datasets, and shows some benefit over their baseline. **Strengths**  The paper is well written and easy to follow and understand. The idea of generating weights is interesting and has not been greatly studied in video recognition. This should be corrected to clarify the connection and differences between the proposed approach and convolutions. The approach is not especially novel, the idea of generating temporal weights from 2d weights has been shown before. It seems the main difference is that the temporal weights are dynamically generated here. Adding this experiment would strengthen the paper. The claims of state of the art performance are wrong.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 3; Overall, I like the paper and enjoyed reading it. Based on this idea, they propose a new method (SNS) for generating counterfactual examples. In my opinion, the main novelty is proposing Stable Neighbor Search (SNS), which finds counterfactuals in more locally Lipschitz regions. A second contribution is the discussion of the relationship between Lipschitzness and counterfactual consistency. In adversarial example literature, it is well established that improving the Lipschitzness of a network will improve its adversarial robustness. However, I am not sure whether the proof of theorem 2 is correct. line 46: $d\sigma (x,w)$ can be both positive and negative, right? As such, I think there should be an absolute value sign around $d\sigma (x,w)$. **Writing**:Overall, very well written and easy to follow. The authors are explicit about this later on, however, I think the introduction should be written more carefully.<|endoftext|>The authors study how counterfactual explanations of a Deep Neural Network change if the initial training settings are slightly changed. They propose a method for computing more robust (i.e.consistent) counterfactual explanations. Pro:   Interesting and relevant topic   Well written (good structure and easy to follow)   "Sound" evaluationCons:   Relevance of counterfactuals could be made more prominent (in the introduction). Why are they so popular? This specific number looks like a magic number that needs further explanations. I miss some comments on the computational complexity of the proposed method (SNS)Overall an okay paper with only minor (not too critical) issues as pointed out in the main review.<|endoftext|>The goal is to minimize the invalidation rate. Different from previous work, they focus on DNNs. Would this result in any issue if we aim to find CFE with high confidence? They find a bound on the change of DI in terms of the model s Lipschitz continuity on the support of $\mathcal{D}_x$. Note that only binary labels can be used. Only relatively small tabular datasets are used, which is not the most widely used case for DNNs. There is no causal model built and therefore, why the CFE in this paper is a counterfactual.<|endoftext|>The method is based upon a K Lipschitz constant. However, the reviewer believes that the claims relating to counterfactual are based upon a not so accurate interpretation of causality. Firstly I would invite the authors to substitute definition 1 with the appropriate counterfactual definition from the textbook by J. Pearl, 2009, chapter 7. The claims regarding to counterfactuals, explainability and causality I believe are a stretch. As such I recommend that the paper is changed such that it is in the proper context
Reject; rating score: 6; rating score: 6; rating score: 6; rating score: 6; 2015.In particular, that paper proposes to learn multiplicative weights in Fourier space instead of learning image space convolutional kernels. I see these for the other Fourier architectures but not FourierNetRGB in the Appendix. Weaknesses   The paper has limited technical novelty when compared to a previous paper which implemented Fourier domain networks, and does not currently contain experiments showing that the differences in approaches provide a meaningful improvement (see Detailed Comments).<|endoftext|>The main challenge is that the decoder needs to be able to handle the global PSFs used in the optical encoder in 3D snapshot microscope. The authors also applied the technique to lensless imaging. * The paper has a nice section on limitations. First, the 3D snapshot microscope is evaluated only on simulated data. * The result from UNet is unclear to me. This is the opposite of what is claimed in the paper. This paper proposes an interesting architecture for the end to end optimization of the optical encoder and deep learning decoder for 3D snapshot microscopy. So the decision is weak reject.<|endoftext|>This paper proposes a neural network architecture that applies a series of convolution kernels on the Fourier domain to focus on global information of the captured images. I do not believe that this question comes as a surprise to the authors, but a more detailed discussion on this potential problem is worth to be given in the paper. In other words, I wonder how the performance of the proposed method would vary as the pixel count changes. Although the proposed architectures are simple modifications to the existing CNNs, their motivation is clear and demonstrates the advantages in synthetic experiments.<|endoftext|>They demonstrate their approach for snapshot 3D microscopy as well as the reconstruction of light fields from input captured using lensless system (with randomized optical elements). Positive:* The paper is nicely written, illustrated. Questions:* I would assume that Fourier transforming the signal is not a new concept in the machine learning community. I would appreciate if the authors could clarify the relation of this work to those. That said, I m not an expert on the application (though it does feel like a bit of a niche). So I m looking forward to reading the other opinions on the work.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 8; rating score: 3; In addition, by assuming the Hessian is close to a random matrix, they estimate its lowest (non zero) eigenvalue and show that their bound can reflect double descent behaviour at the anticipated interpolation threshold. This work derives an interesting and to the best of my knowledge new, lower bound on DNN generalization and draws connections with a timely topic   the behaviour of finite width DNNs. Notwithstanding I found the work lacking in the following aspects   ** Assumptions: To derive their bound, the authors make 6 assumptions (A1 A6). Some of which seem quite reasonable however for some I didn t find sufficient support. I could not find where this is shown or implied. In addition, the authors argue that the loss term ($l_i$) would be close to zero near the interpolation threshold   but on what scale? The authors justify this using experiments, however, due to the hardness of computing the low Hessian eigenvalues   these experiments are quite limited in scope. This again is a very crucial assumption   it allows them to glance over the difference between trained DNNs and DNNs at initialization. ** Tightness of lower bound   in their experiments the authors compare the lower bound with the actual population risk. In addition, while it may or may not be that the minimal Hessian eigenvalues of the true Hessian will provide a second change of trend   I find it hard to see how the analytical approach, modelling the latter as a random matrix, will provide such a second scale.<|endoftext|>Post rebuttal: The authors addressed most of my concerns, as well as most if the other reviewers  comments and questions. This analysis is built on: * expressing the population loss based on a "add one in" procedure (a symmetric procedure to the more classical "leave one out", which allows in a second stage to use influence functions to provide a lower bound of this loss,* proving that the lower bound on the population loss diverges at a certain threshold, by analysing the spectrum of the Hessian at the optimum.The paper also studies the influence of different loss functions on this behavior. In particular, the authors provide an analysis for the case of cross entropy. In  particular,  the  analysis  in  terms  of  the Hessian  spectrum  and  rank  and  the  approach  taken  by  the  authors  to  do  so provide explanations to many of the latest observations in the double descent phenomena (e.g.the effect of regularization [Nakkiran et al., 2020]). While the work can be of high interest to the community, it has some weaknesses, in some of the theoretical results and in the empirical validation that can limit its impact.This is why a recommend acceptance, but I think the paper can be improved.<|endoftext|>The paper studies the double descent phenomenon in a very general scenario: general loss functions which include but are not limited to MSE and cross entropy, and general finite width neural networks (i.e not limited to linearized   NTK / RF  regimes). The end result is that the authors manage to lower bound the population risk by a term which depends on the trace of the inverse of the hessian of the loss function at the optimum. Please choose between use of "double descent" and "double descent". (remove stray space)Other minor issues   "Thanks to the influence function of the parameter estimate (Eq.4 for D   D_n)" <  Eq.4is in the appendix. This paper presents a completely new approach to understand the phenomenon of double descent, for very general models / loss functions (including finite width neural networks). I have a few concerns about the assumptions being used in the paper though (as explained further above).<|endoftext|>The paper discusses the phenomenon of double descent in the framework of finite width neural networks trained with various loss functions. The analysis of the authors relies on the use of influence functions. They provide intuition on the dependence of double descent on the number of samples in the overparametrized regime and on the number of parameters (in the under parametrized regime) by highlighting the dependence of the population loss on the inverse of the smallest eigenvalue of the Hessian. Although we still have a bound in the inverse of the Hessian. Introduction   I would remove the sentence “as well as can cater to the idiosyncrasies that arise with the usage of neural networks”.<|endoftext|>This paper derives an expression of the population risk with influence function, and then use that to derive a lower bound on the population risk, which provides an explanation for double descent phenomena in the finite width regime at the optimum point. The paper provides experiments to support the theoretical arguments. Moreover, the authors investigate how the choice of loss functions affect the double descent phenomena. The topic and the goal are both very important. The proof of Theorem 3 is very unclear regarding the O(n^2) term. The modes studied in this paper are essentially still in the lazy/kernel regime.
Reject; rating score: 3; rating score: 5; rating score: 5; This paper presents a new hierarchical classification method. Empirical results on a custom ImageNet dataset and the BREEDS dataset show improvements over baseline approaches. **Strengths**   The problem is of great interest to the vision community. **Weaknesses**   The proposal hierarchical classification method is not completely novel. The authors may want to comment on how the proposed approach apply to these non perfect but practically common situations. It would be more convincing if the authors include comparison with other hierarchical classification approaches in the literature. Also, limiting the method to subpopulation shift problems seem unnecessary.<|endoftext|>Created a new label hierarchy for a subset of ImageNet3. Evaluated the performance of their new architecture under subpopulation shiftStrengths:  This paper is well written. The architecture and evaluation methods are clearly explained and the research problem is well motivated. The proposed architecture is simple and computationally inexpensive which means it could be easy applied to any object recognition problem. On the datasets tested the authors demonstrated a performance benefit from using their training method. Weaknesses:  The proposed mulitheaded architecture is only evaluated on datasets containing images and a label hierarchy of living things. More experiments in this direction would provide a more convincing argument that your performance improvements are not only a result of the particular set of classes and hierarchy structure that you chose to use. Overall, this paper demonstrates some promise that there could be an advantage to explicitly specifying a label hierarchy during training.<|endoftext|>The authors propose infusing knowledge both about these subcategories and about super categories of the labels as additional supervision signals. For this purpose, they define a conditional training framework along with an adapted architecture and a hierarchical loss. The results are unsurprising given how the custom setup better addresses the goal of generalizing to subpopulation shift. In fact, several aspects claimed to be novel have been addressed several time in the literature over the past decade. With that, the current contribution is rather incremental. I would have deemed it otherwise had the authors provided theoretical insights about the problem of hierarchy, (e.g.as in the work by McClelland, Saxe and others). for Toxicity classification  > why capitalized?
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; This work analyzes the accuracy privacy trade off in ensemble learning by performing model inference attacks. These observations can be very interesting to the whole of MI literature   because they suggest that all MI attacks in practice do as well as "Gap Attack" (when not considering an ensemble). Unfortunately, they do not see any discussion in the paper. 2.The experiments are comprehensively evaluated across a wide variety of datasets, training settings, defense settings   which helps to be confident about the generality of the results. Since this paper is about "deep ensemble models", it appears incomplete without attention to ensemble based defenses such as those highlighted by the authors   like training the ensembles on different subsets of datasets ( which are not necessarily disjoint)  Salem et. It appears to be the reverse of what was followed in the main paper. Consequently, the average gap of correct agreement between train and test set widens. Hence, the wider *the* generalization gap of base learners is, the more effective the membership inference attack would be on *the* ensemble.<|endoftext|>This paper provide a systemantic analysis on the accuracy privacy trade off for deep ensmebles. They show that the effectiveness ofmembership inference attacks is likely to increase when ensembling improves accuracy. The authors further study  the impact of various factors such as prediction confidence and agreement between models that constitute the ensemble. 2.The conclusion on the effectivess of the level of correct agreement among models is important and likely to motivate more related future work towards this direction. In this case, it is not clear whether the conclusions in the paper will still be applicable. It is interesting to see whether the conclusion still holds for more advanced and generic ensemble settings.<|endoftext|>This paper describes and analyzes an interesting and curious phenomenon: a standard ensemble of models is typically more vulnerable to membership inference attacks, despite being less overfit than any single model. The graphs in the paper are overall a little hard to read, as they tend to show too many things at once. It would be useful here to also provide some quantitative measure of the distinguishability of these distributions (e.g., a simple TV distance). This paper s main insight that ensembles can be more vulnerable to membership inference is interesting and a priori somewhat surprising.<|endoftext|>The authors in the paper perform empirical studies to investigate the trade off between accuracy and privacy (measured by membership inference attacks) in deep ensembles. They find out that the level of correct agreement among models is the most dominant factor that improves the performance of MI attacks in deep ensembles. They support their claim by visualizing the distribution shifts of correct agreement in train/test examples. They further implement a variety of existing defenses, such as differential privacy and regularizations, etc., to investigate the ​​effects of existing defense mechanisms. Overall, the paper is well written and the experiments are well conducted.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; This paper proposes a revised version of score based models for representation learning. The encoder can be further conditional on time. The paper is easy to follow and well motivated. Did you use the adversarial training and initial noise schedule tricks that are introduced in section 3? However, I feel the writing and experiments can be improved before it gets accepted.<|endoftext|>Authors propose a simple idea to extract multiple scale features with denoising score matching. 1.Using "diffusion based" and "score based" interchangeably in the paper makes it very confusing. 5.The adversarial training of lambda divergences is not principled because the unknown constant of denoising score matching can depend on lambda.<|endoftext|>Despite I admit the idea, presented in the paper, is interesting and novel, the evaluation does not seem conclusive for me. I ask the authors to motivate the choice of the baseline and explain what the use case of the presented approach is. Qualitative experiments on MNIST and CIFAR 10 show that the encoder can learn some meaningful features. The motivation behind the method, taking its origin in the rethought conditional score matching objective, is clearly explained. Therefore, it is rather difficult to realize which context the authors put their work in. a.It is claimed in Sec.3.1. that $\lambda$ divergences can express any $f$ divergence.<|endoftext|>The paper presents a novel technique for representation learning by means of generative models. In particular, non adversarial score matching methods are improved to control the level of detail in the representations and learn a infinite dimensional code. Results on the generation task do not seem particularly impressive but it is fine for it not to be the main focus.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; This paper provides theoretical analysis on its advantage over behaviour policy and the meta policy. This paper proposes a new model based offline meta rl algorithm under the actor critic approach. The difficulty of balancing the exploration with meta policy and exploitation with offline dataset is an interesting observation, and the proposed solution is a reasonable remedy. Could the authors comment on this option? While the authors show 0.4 is a good value for all the experiments, I doubt if we can trust it in general. The paper has a good motivation and provides a reasonable solution to the problem in offline meta RL.<|endoftext|>This paper studies an interesting question of offline meta RL and presents promising experimental results. The ablations do a good job of demonstrating that each component of the method contributes to the success of the overall method. One concern with the paper is that the motivation for the method is rather unclear. Although the results are positive, the method only mildly improves over FOCAL, and the paper would be strengthened by comparing to MACAW and BOReL, as those methods have been shown to perform well in these environments. “we study a more general offline Meta RL problem.” I do not see how the problem statement in this paper is more general than the offline meta RL problem present in past papers. One limitation of the approach is that RAC seems to depend on having a behavior policy to regularize against, making it impossible to use the resulting policy for online meta RL. The paper does not provide compelling evidence that their method addressed a critical problem, and the experiments make it difficult to know if the method proposed really outperforms current meta offline RL method since details on tuning and important comparisons were not included. Edit: Based on the rebuttal, I ve increased my score up to a 6.<|endoftext|>This paper proposes a model based meta RL approach to offline learning over a distribution of MDPs (offline meta RL). The proposed method is inspired by COMBO and in fact is a direct extension of it to the multi task setting. They further add a proximal RL policy improvement operator, where the prior policy is meta learned over the task distribution. This could be made much clearer, which would not only help place the contribution in proper context but also make the paper much more readable. This observation holds also empirically, as the authors demonstrate that their method is at least as good as using only one of the two regularisers under various assumptions of data quality. Overall, while I think the presentation of the method can be made much simpler and clearer, I believe this paper presents interesting findings for offline RL and has a strong proposal for an offline meta RL algorithm. I recommend acceptance of this paper.<|endoftext|>Targeting offline meta reinforcement learning, the work proposes a model based method called MerPO, with conservative value evaluation and individual policy improvement method with the tradeoff between the meta policy and the behavior policy influence. The main algorithm MerPO includes an initialization step and a two loops meta learning approach. The initialization step learns the model of the meta model and dynamics for each task. The main contribution claimed in this paper is that the proposed method is a more robust method with the design of a regularization term involving the behavior policy, which improves the meta learning policy when the behavior policy is actually better than the current meta policy. Theoretical guarantees are displayed and experiments are conducted to show the performance of MerPO. The main contribution of this paper is well claimed and verified.
Reject; rating score: 1; rating score: 3; rating score: 3; My opinion is that the idea in the paper is at best a nice engineering idea, which aims to improve on the class imbalance problem, which typically lead to lower accuracy for those classes. The novelty of the paper is limited, due to reasons listed above. However, in the experimental results, it is not clear which and how many classes in those datasets were selected by the model as the lower accuracy classes, hence were augmented into the dataset, and how that affected the performance.<|endoftext|>The paper is not novel. Results the feasibility and perfection of the idea. Several experiments on datasets such as CIFAR10 is done. Besides, as I understand, the low accuracy will be associated with the class with a limited number of samples.<|endoftext|>The proposed method in this paper can be regarded as an engineering trick on training procedure of CNN models. This makes the method show limited technical novelty. The proposed method make sense and boosts the performance. 3.The experiments are not convincing.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; The paper introduces PF GNN, a new method through which to individualize node representations with the aim of increasing GNN expressiveness. It is inspired by graph isomorphism algorithms  individualize and refine (IR) approaches, which individualize individual nodes then refine colorings, and then aggregate all colorings across the search tree of possible individualization paths. Specifically, PF GNN approximately emulates IR by repeating a random sampling k times and returning an aggregate coloring from each sample. Finally, the model is empirically evaluated on a series of datasets and compared with existing standard GNNs and individualization techniques, e.g., random node initialization (RNI). In these experiments, PF GNN is shown to improve model expressiveness, is more resilient than RNI for larger numbers of nodes, and achieves strong performance on real world datasets. The paper introduces a novel, principled means to perform individualization, inspired by IR approaches within graph isomorphism solvers. Finally, the paper is clearly written and its main approach is well presented. However, I have some concerns regarding the novelty and significance of the approach, which I enumerate below:   The paper clearly explains and motivates its particle filtering mechanism, as well as its inspiration by IR. Hence, it is not at all clear how or why PF GNN achieves these improvements at present, and thus it is not sound to attribute these improvements solely to the PF mechanism and its data driven approach based on current evidence. As mentioned earlier, the paper only compares with fully randomized RNI. Hence, the authors should consider running experiments on standard benchmarks, such as those in OGB [1], to further corroborate this point.<|endoftext|>Summary:\The paper is well written and easy to follow. In my opinion, the originality of the paper is high since it is both technically rich and the empirical results are strong. Then, they show how the distance of the generated embeddings is approximated by sampling a number of paths from the search tree. One of my concerns with this paper is that PF GNN is mainly evaluated on synthetic datasets, while only three real world datasets are employed in total. It is not thus entirely clear how effective the proposed model is in real world scenarios. I would like the authors to provide some explanation. The paper proposes an original contribution for the graph representation learning community.<|endoftext|>The authors propose PF GNN for graph level tasks. They propose sampling process with particle filter updates to alleviate the high complexity issue. ## Pros:(+) The design and results are theoretically sound. ## Cons:( ) The clarity of the paper can be improved for non expert readers. ## Detail comments:I am not an expert in the graph isomorphism test and particle filter approach discussed in the paper. The experiment results demonstrate the superior performance of PF GNN. Overall, I think it is a good paper, but the clarity and the way the authors explain their approach can be more friendly to non expert readers. I understand that the embedding of GNN can be used for various downstream tasks and is not restricted to graph isomorphism tests. However, I wonder what is the time complexity of PF GNN on the other tasks and datasets such as ZINC, ALCHEMY, and QM9. The paper is theoretically sounded and the empirical performance of PF GNN is impressive. Still, I think the explanation can be improved and I also have some regarding the experiment. Together I lean to vote for acceptance of this paper.<|endoftext|>This paper presents a neural version of individual refinement (IR) architecture for improving the expressiveness of GNN in terms of isomorphism test. As IR is the dominant approach of practical graph isomorphism test, adapting IR to GNN is a novel and important idea. As IR suffers from the exponential number of branches, the paper adapts particle filtering algorithm to sample K paths to approximate the full version of IR algorithm. Simulation and real world datasets are used to demonstrate the improvement over base GNN. Adapting IR to neural and GNN area is a novel and important contribution. The designed algorithm does improve over base GNNs. 2.The particle filtering algorithm is a elegant and low complexity realization of IR algorithm. 3.A bad property for sampling method is that for two same graphs, the algorithm may judge them as non isomorphic. 5.The major weakness is the scalability and practical complexity. The designed model is T times deeper and larger than base model, and K path sampling needs K times larger memory for parallel computation.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; Experimental results show that ALiBi has significantly stronger extrapolation capability compared to other positional encoding methods. Pros:  Injecting temporal bias to attention is a neat idea for the language model extrapolation problems. This paper presents comprehensive experiments on comparing the proposed method with existing positional encoding approaches. The idea of adding temporal bias to attention is similar to the forget gate in LSTMs. I raised my rating to acceptance. This paper proposes an interesting and novel idea for enhancing the extrapolation capability of transformer based language models. A few additional experiments and discussions will make the paper stronger.<|endoftext|>The method is simple and quite effective. ALiBi’s inductive bias also improves the accuracy. It would be much better to provide theoretical explanations more than empirical proof on why ALiBi enables better extrapolation and higher final accuracy. ALiBi is only evaluated on language modeling in this paper. Each head has a different slope for the linear bias, so I expect that heads learn different patterns. An analysis of that would be interesting. ALiBi improves the efficiency of language model (or transformers in general) training.<|endoftext|>This paper studies input length extrapolation for Transformer language models; i.e., how Transformer LMs perform on test sequences that are longer than training sequences. Models with sinusoidal and rotary position embeddings do not extrapolate well, while T5’s position dependent attention mechanism (dubbed T5 bias) enables better extrapolation. The proposed ALiBi mechanism is simple to implement and computationally efficient. Experiments confirm that the proposed method enables length extrapolation for language modeling. The proposed method is simple to implement, so I imagine it would not be hard to add a few more tasks. The paper studies a novel problem, input length extrapolation in language modeling, and proposes a simple solution with good empirical results. The paper is also well written. One way to further improve the paper is to add experiments on other tasks. Overall, I recommend acceptance of this paper.<|endoftext|>Therefore, I would like to see this submittion at ICLR2022. end of the update 1. My point here is that the submission could have been more generalised in a way that, say, as long as the bias terms are fixed before training and they have an impact on the attention scores or the probability maps, the model will extrapolate to very long sequences. This would ve been a stronger and more generalised message. The submission proposed a simple yet effective method that helps pre trained language models to extrapolate beyond the sequence length used in the training, but I think the paper could ve delivered a stronger message.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; This paper is a follow up paper of Zhang et al.(2021).In Zhang et al.(2021), the authors proposed a new network architecture, l_infty distance net. By construction, the network is 1 Lipschitz w.r.t.l_infty distance. This paper resolves the issue by a new loss design of scaled cross entropy loss + clipped hinge loss. Without using MLP on top of the l_infty distance net backbone, the proposed new training method outperforms the original one in Zhang et al.(2021) and improves over the state of the art by more than 5% for 8/255 and other radiuses. Theoretically, the paper shows the expressive power of l_infty distance net for well separated data.<|endoftext|>The paper proposed a new loss to improve the performance of Linf distance network a new network for Linf robustness and achieved impressive empirical results. * The paper proposed multiple modifications on the loss function, including the scale factor on cross entropy, clipped hinge loss and weight coefficient. * After checking the hyper parameters, I found that the authors have tuned the beta_2 and eps of Adam optimizer. The paper is well written and the proposed method has an impressive empirical performance. My concern is mainly on the experiments and reproducibility.<|endoftext|>This paper proposed a simple modification of $\ell_\infty$ net training, which boosts the accuracy for certified robustness under $\ell_\infty$ attack. Major comments:  The main focus of the paper is on the training of $\ell_\infty$ net. The authors emphasize that the original training is hindered by the  $\ell_p$ relaxation, and they use clipped hinge loss to relieve the issue. The authors also implemented scaled cross entropy loss. How should we decompose the effect of scaled cross entropy and clipped hinge loss? The empirical results of this paper seem pretty decent to me. How large is $\theta$ in Figure 1(a) and 2(a)?<|endoftext|>The authors study to improve the certified robustness of $l_\infty$ distance nets by introducing a regularization term to train the network. The paper overall is easy to read and follow. This paper follows a similar setting with prior work (Zhang et al.2021) published in ICML 2021, specially targeted in $l_\infty$ distance net for improving certified accuracy. The proposed method serves as a regularization term for the hinge loss to train a more robust network. Apart from that, this work has moderate novelty which shows a large portion of prior work with the same setting and experiment design. What about other bigger datasets? This paper is largely based on prior work (Zhang et al.(2021))  which follows the same setting and experimental design.
Accept (Spotlight); rating score: 8; rating score: 6; rating score: 6; rating score: 10; The authors propose new interpretation of the Information Bottleneck (IB), dubbed PAC Bayes Information Bottleneck (PIB). The authors show that PIB is a bound on generalization error. The experiments are well done, thorough, and support the main claims of the paper. **Weaknesses**The main weaknesses of this paper are language and clarity. "optimal posterior of PIB," does PIB have a posterior or is the posterior over the weights? Figure 2.IIW only shows compression phase, can the loss also be included in these plots? I believe the main weaknesses can be addressed prior to publication.<|endoftext|>The idea to limit the information about the sample that is contained in the weights is not new (the authors cite several works that bound the generalization error via this information), but this is the first time that I have seen a corresponding cost function implemented in practice. Connected to this, is it safe to call the resulting cost function an information bottleneck cost function? Is this connected with the proportionality symbol in (14)? I assume that this is better called an IIW regularization rather than an IB cost. Unfortunately, the paper is not perfectly clear throughout all sections. This would be at least intuitive from a channel coding perspective, where a Gaussian channel input is known to maximize the mutual information through a Gaussian channel, and which is then known to produce a Gaussian channel output.) What is the exact meaning of splitting the IIW between layers in terms of the generalization bound?<|endoftext|>This paper introduces the PAC Bayes Information Bottleneck (PIB). Starting from the generalization bound Eq.4 which shows that the generalization gap is upper bounded by a function of I(w;S), the authors proposes PIB which has an additional regularization term of \beta I(w;S). The experiments show that the proposed \tilde{I}(w;S) correlates with the generalization gap, and helps improving the performance. Weakness:In order to make the computation of I(w;S) tractable, the authors make several important assumptions. Ideally we should see that with PIB as the objective, the generalization gap is much smaller than the other methods. With this, we can then be confident that the improvement is due to reduced generalization gap instead of better training. In summary, this paper is novel, but the experiment should be strengthened as detailed in the main review.<|endoftext|>This paper proposes a new version of the Information Bottleneck objective for training neural networks. This is in part motivated by previously derived PAC Bayes bounds on the generalization error that are proportional to the square root of the mutual information of the weights and the training dataset: I(w;S). As far as I know, the contributions here are novel and will be of high interest to the community. The authors build on previous work by showing how their IB objective addresses the shortcomings of previous work in this area. Specifically, this objective is motivated by a PAC bound (the tightness of which is not clear) and various approximations are used to estimate I(w;S) (the accuracy of these are not immediately clear). The experiments address these issues by showing that the motivation and the approximations are reasonable. For example, the paper provides an algorithm for sampling from the weight posterior p(w|S), but how does this compare computationally to standard training of a neural network, or a estimating the posterior in a Bayesian Neural Network? An excellent paper with exciting ideas, clear presentation, and technical depth.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 3; The main idea is to decompose the approximation of unknown dynamics into two parts: a parametric physics based model $h_k$ (used to incorporate prior knowledge, or to identify parameters), and a non parametric statistical model $h_u$ (used to fit the unknown, or unobserved degrees of freedom). The authors show that this modification implies well posedness, and subsequently design an alternative minimization algorithms to solve for $h_k$ and $h_u$ iteratively. The proposed method is sound and quite easy to implement, and the performance is also good as presented. Can the authors comment on the sensitivity of tuning parameters with respect to the good results shown in this table? This is a good paper tackling a relevant problem. Some theoretical justification on convergence is also presented.<|endoftext|>Authors propose a general framework for learning and ensuring identifiability with hybrid models. Experiments confirm that the proposed framework increase interpretability and maintain high prediction performancesAuthors propose a unifying framework for hybrid models, generalizing recent methods. Here are two points that I think need to be discussed more extensively in the paper:   The paper proposes two models (with and without auxiliary data). Moreover, equation 5 is the equivalent of equation 4 for auxiliary data. However, the model uses alternate optimization to converge.<|endoftext|>The paper proposes a method for hybrid model based/machine leanring learning, wherein a model is decomposed into an interpretable parametric prior and a residual (typically represented by a neural net). The paper demonstrates that prediction error minimization does not accurately identify the parametric component and proposes an alternating optimization method that augments prediction error loss with component specific losses.<|endoftext|>This paper investigate a novel method to recover well posedness and interpretability for the problem of learning a physical and machine learning hybrid model, by controlling the ML component and the MB hypothesis. The authors also study the case where auxiliary data are introduced. The experiment results of this paper is very strong and comprehensive.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; The paper proposed a set of modifications to the structure of the Transformer layer that improves FLOPS utilization. Grouped convolution has already been used in Evolved Transformer [1] and Primer [2]. The reviewer is not sure how useful the customized model is when the hardware becomes GPUs or TPUs. Grouped FFN and grouped convolution are not new. The paper lacks a comparison on MLM perplexity and downstream tasks with stronger baselines, such as evolved transformer and Primer where grouped ops are used. Add more tasks (GLUE or superGLUE)  Target general accelerators such as GPUs.<|endoftext|>It designs grouped transformation and grouped convolution for efficient training. Therefore, in this paper, authors design GroupBERT, which introduces a set of modifications to the structure of the Transformer network. The novelty of this paper is limited. I think authors should provide these differences, not just compare original BERT. It seems the proposed architecture is hardware specific. 3.In addition to the comparison of MLM loss in the training stage, the results on downstream tasks are also very important, especially for pre trained model. 5.According to the ablation study in Table 1. it seems that the Conv contributes more benefits than grouped FFN modules.<|endoftext|>Moreover, since SQuAD v2 is more challenging than SQuAD v1.1, it is important to report the results on SQuAD v2 as well. In this paper, the authors propose applying both grouped feed forward layers and grouped convolutional layers to improve the computational efficiency of the transformer model. However, both papers claimed huge improvement over the BERT baseline. In addition, ConvBERT is a pre work that should be compared to.<|endoftext|>The authors propose a new Transformer architecture. :1) grouped FFN and 2) a convolution module. Weakness:Since previous approaches have applied grouped matrix multiplications in Transformer, the idea of GROUPBERT is not very novel to me. The comparison focuses on training/validation losses, rather than downstream results, which is not very convincing to me. It would be better to report more solid comparisons on popular GLUE datasets.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; al.2016, in which heuristic functions called labeling functions (LF) are used to label training data. Weaknesses:  (W1) Lack of adequate comparisons to related work: There are many weak supervision approaches that have been proposed since the original Snorkel paper referenced and compared to in the experimental results; some of which are referenced in the paper, but never actually compared to. (W1a) Additionally, this work is very similar to classic label propagation techniques in semi supervised learning, and should have been compared to these as well. In the main results, this is apparently "chosen by a data scientist".<|endoftext|>This paper describes a method to improve the coverage of labeling functions in weak supervision. The method uses similarities between features of points for the gravitation based method. This paper discusses an interesting method and is well written and clear to follow. This suggests that the authors did not run Snorkel correctly. Chen et al also focus on improving labeling function coverage by replacing abstentions based on similarity in feature space to other points.<|endoftext|>The work may be interesting for the automatic label annotation area, but the idea novelty is a little trivial and experimental results are not convincing. Is there any improvement for the supervision performance if the authors incorporate their weak supervision approach (or pseudo labels) into a supervised setting? 4.The labeling functions seems to be rule based functions that maps the features to the pseudo labels. The technical novelty of this work is limited as the proposed reinforced labeling is similar to KNN with pseudo labels.<|endoftext|>This paper tackles the problem of designing data programming, which is a practical approach in weak supervision. The authors state that prior effects neglect to utilize data features during the generative process, and therefore have suboptimal performance. The experimental results are promising. For example, this paper tends to augment the outputs of labeling functions for better generalization. Also, the explanations are not sufficient. The current version is hard to follow.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; However, I find the scope of the paper to be too narrow. It is only a marginal transformation of an existing model, the theoretical contributions are quite elementary and the experiments do not really suggest wider applicability of the model in practice. While technically sound, I don t advocate for this work to be accepted at ICLR as the contribution is too marginal.<|endoftext|>This paper is motivated by LNN and aim to enforce momentum conservation in learning based on the Lagrangian framework. MCLNN conserves energy and momentum simply because it is designed to do so. The proposed model is tested on three Interacting particle systems with homogeneous pairwise interaction. The generalizability to unseen system sizes is only valid for homogeneous pairwise potential energy. What if the authors assume that masses are not known and instead need to be learned? The description of the generative model to "generalize to unseen system sizes" is missing. This is a big limitation on the claim of "generalization to unseen system sizes" and should be pointed out clearly in the paper. If we compare an MLP, a LNN and a MCLNN.<|endoftext|>These symmetries are associated with the conservation laws of linear momentum and angular momentum, respectively. Thanks to this explicit formulation, the proposed method conserves not only the system energy but also linear/angular momentum, as confirmed by the experiments. **Pros**The paper is well organized and self explanatory. The theoretical backgrounds such as Noether s theorem and the momentum conservation law are introduced in detail and in a reader friendly way. (2020) "Lagrangian neural networks". ICLR 2020 Workshop on Integration of Deep Neural Models and Differential Equations. (2019) "Hybrid neural network potential for multilayer graphene". However, the contribution of the present study is directed to the computational science community rather than the neural network community.<|endoftext|>This paper enhances Lagrangian neural networks by adding conservation of the angular and linear momenta. The simplicity of the idea is a plus and it is clearly introduced. It is indeed a good inductive bias to put in the model when we can do it. As said in the previous point I think the contribution is very incremental.
Accept (Spotlight); rating score: 8; rating score: 8; rating score: 8; rating score: 8; It is clearly written, the empirical evaluation is fair, and presents a novel perspective (rotation) to reduce inter task conflict in multi task learning paradigms. 2.Introducing a geometric rotation in SO(d) transformation after the final shared feature map and before the task specific layers. The concept of applying (and learning) a geometric rotation to align the optimization trajectories is novel; although, the concept of gradient magnitude homogenization has been well studied, with the presentation offered by this work feeling very reminiscent of GradNorm. 2.Extremely well written and polished paper. 4.Experimental results are compelling.<|endoftext|>The main idea is to avoid conflicting gradients by making them homogeneous both in terms of magnitude and direction. Why do you need to specifically enforce $R_k$ to be a rotation matrix? Have you tried experiments in this direction? The paper is solid and it looks has received several iterations. It is a nice contribution to the multi task learning field, although I think there are still some questions to be addressed first.<|endoftext|>This paper proposes a novel algorithm to tackle negative transfer in MTL optimization, by homogenizing the task gradients’ magnitude and direction. The paper tackles an important problem and proposes an intuitive approach for alleviating the problem. The motivation for the method is clear. Missing citation to relevant works: [1]  propose a method to handle conflicting gradients in MTL. Specifically, while different significantly than the proposed method, their covariance alignment approach appears to relate to the rotation matrices in the proposed method (although being applied to the input instead of the shared feature space).<|endoftext|>Considering a common shared representation from which multiple task specific subnetworks branch, the approach aims to homogenize task specific gradient magnitudes and directions at this branch point. The proposed optimization scheme appears to be a unique technical approach among a larger set of recent work that attempt to deconflict gradients in multi task learning.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; However, the presentation is a bit hard to follow and the empirical evidence are not strong enough to support the claims. So using this synthetic example as the main experiments makes the paper less convincing.<|endoftext|>Perhaps the authors should change the title accordingly. Also, the paper is difficult to parse in parts, not because of mathematical formalism but because of the use of physics jargon with a lack of clarifications.<|endoftext|>What is a "functional landscape"? "We propose a theoretical basis ..." I did not see where the theoretical study is provided in this paper. ## ConsThe paper is poorly written and I find it hard to follow the authors  ideas.<|endoftext|>Sections of the paper seem disconnected or repetitive, and the methods sections lack details. The authors propose a framework and selection criteria.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; Overall, the paper proposes an interesting approach to combine information bottleneck and model based domain generalization ideas recently used in [Ahuja et al.] In this work, the authors study the problem of domain generalization. The paper brings together ideas from rate distortion and information bottleneck principle (recently used in Ahuja et al.for domain generalization) and the work of model based domain generalization [Robey et al.]. In Ahuja et al., the authors combine invariance and information bottleneck to address domain generalization. It is a good idea to combine the ideas introduced in model based domain generalization [Robey et al.] The paper is easy to read as well. **Weaknesses:** I highlight concerns with the paper below in a pointwise fashion1. 3.**Regarding experiments** I would encourage the authors to modify their method to incorporate $G$ model in it.<|endoftext|>I am quite familiar with the related work in this area and I believe that the authors should put in more effort to identify where their approach fails in the real world and subsequently see how they can relax assumptions made in the paper by analyzing these failure cases. The central idea is not really inspiring, albeit the connections to rate distortion theory are interesting. The authors construct several hand crafted problems where indeed this assumption (and the proposed algorithm) work well, and also provide some experiments on MNIST type data. The problem of domain generalization suffers specifically from this issue, where a lot of the proposed work is valid only under certain restrictive assumptions, and usually fail when deployed in practice.<|endoftext|>The paper proposes a method for domain generalization. They formulate the problem as a rate distortion problem and introduce the information bottleneck penalty. It is good that the proposed method consistently improves the corresponding baseline methods in two experiments, linear unit tests and mnist type datasets. The paper proposes an interesting approach based on the information bottleneck. However, I think the discussion and comparison with related work on more realistic datasets is necessary. After authors  response  I appreciate the authors  feedback. They provided additional results on more realistic datasets (PACS and OfficeHome) and results for different $\mu$. After reading the feedback as well as other reviews, I would like to keep my score. As also mentioned by other reviewers, the theoretical guarantee of the algorithm is based on quite strong assumptions, which do not likely hold under practical conditions, and I am a bit skeptical about its significance.<|endoftext|>Authors address a problem of domain generalization using  information bottleneck method. Authors assume that learning domain invariant features is a key to solve the domain generalization problem and then prove that  information bottleneck penalty guarantees that domain invariant features can be learned. 3) Weak experiments section. 2) Strong justification on why their method could work. These references also should be cited in the current paper. 4) Assumption 4: “Strictly separable invariant features” This looks like a very strong assumption. 5) The experimental section is not complete as it does not contain most datasets that recent papers use: For example, PACS or VLCS datasets are missing [6].
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; This would of course make both the trained model and the chosen criterion advantageous over others, but it would be the same if any other criterion was chosen. Hence my initial grade is a clear reject. I am having hard time to make sense of the criterion proposed in Eq 5. iii) The paper does not report experimental results that give a decisive outcome about the central claims of the paper. If all other models are also trained further with Eq 8, the comparison is still unfair, because they are trained further with a loss that is designed for another OOD criterion than they are designed for.<|endoftext|>An extra network KLoSNet is further trained to align with the evidential training objective. I would like to hear the authors address my concerns raised above. The figures are effective in conveying the key idea and intuition. Concerns:   Authors highlight that the uncertainty estimation should consider both class confusion and lack of evidence. However, this may not be always desirable for OOD uncertainty estimation. In contrast, the performance for the baselines methods seems low across the board (~70 75%).<|endoftext|>This paper suggests training an auxiliary DNN to predict the uncertainty associated with a given example. Overall, while the experiments suggest that the proposed method is doing something reasonable, it s not clear to me that it s doing something reasonable for the purported reasons. The primary claim is that "[...] leveraging the second order uncertainty representation that evidential models provide, KLoS captures both class confusion and lack of evidence in a single score."<|endoftext|>Then, the weights are frozen and a 5 layer MLP is trained to predict KLoS* from the final layer of the original network. This procedure makes it difficult to compare the performance in Tab. The proposed training scheme relies on initialization of KLoSNet with the original classifier. Is this important to learn a proper measure or does this simply speed up convergence? Additionally, I am not fully convinced that a joint metric for OOD and misclassification is desired (W2). Finally, some ablation experiments are needed to understand the effect of certain design choices (W3).
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; The paper intends to improve a recent offline meta RL algorithm (FOCAL) through a modified objective and architecture. In addition, the authors modify the objective used in FOCAL to form a tighter bound on a "supervised" contrastive loss than prior work, though it s not totally clear how meaningful this supervised form of the loss is. The technical novelty is limited (essentially, adding attention)  It s very difficult to interpret the significance of the main theoretical claim; can the authors justify why L_{sup} is a meaningful lower bound? Why was this form chosen? In particular, I didn t realize that we were trying to come up with a surrogate objective for the "supervised loss" until the middle of page 6. 2021.I have mixed feelings on this paper.<|endoftext|>This submission tackles the problem of offline meta RL, where the goal is to learn a policy that can quickly solve new tasks. This policy must be learned on an offline set of (pre collected) trajectories from multiple tasks, unlike in online meta RL where data collection and learning are interleaved. Specifically, the authors focus on improving a previous method, FOCAL, in 2 aspects:1. Wang et al., "Alchemy: A benchmark and analysis toolkit for meta reinforcement learning agents", ArXiv 2021. They add an attention mechanism to improve the estimation of the context vector identifying the task. The authors show that this mechanism can reduce variance when estimating task context. However, I have some concerns on experimental and theoretical results. If instead FOCAL++ was competitive with or outperformed those methods on (sparse & dense reward) tasks, this would be a strong selling point for the paper.<|endoftext|>In this paper, the authors focus on the problem of meta learning for offline reinforcement learning. To this end, they build on the FOCAL algorithm by adding to key contributions. First, they add an intra task attention mechanism. Second, they addinter task contrastive learning. The authors provide a theoretical foundation for their approach and evaluate on control tasksin MuJoco. They demonstrate competitive performance over the state of the art. The paper focuses on a very important problem in reinforcement learning: meta learning. 3.The quantitative results on the given Mujoco dataset are competitive and show promise. The conceptual novelty is limited. In general, the authors have proposed a modification to an existing algorithm, FOCAL, which incorporates attention and contrastive learning.<|endoftext|>Theoretical analysis is done to show the proposed objective is closer to the supervised contrastive loss and the variance of task embedding with an optimised encoder is smaller than the one without batch wise gated attention. This paper provides a descent improvement for the SOTA offline meta RL algorithm, FOCAL. The main contribution is a combination of three existing techniques that are widely used in other domains, and therefore the novelty / significance is somewhat limited. In the experiment section, it seems the contribution of the contrastive loss is only marginal from Table 1 and 2. The architecture improvement on FOCAL is a good contribution in practice, while the theoretical analysis part requires some work for more solid discussion and proof.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 8; The paper introduces Safe Exploitation Search (SES), which achieves a balance between maxmargin solving (guaranteed to be safe, but non exploitative) and unsafe subgame solving (unsafe, as the name suggests, but exploitative of the opponent s initial distribution at S_top) by mixing their objective functions. The main contribution of the paper is only a minimal step on top of known ideas (a convex combination of unsafe and maxmargin subgame solving). The theoretical and experimental results are not at all surprising either.<|endoftext|>The paper proposes a technique to smoothly interpolate between computing a Nash equilibrium strategy and computing a best response. Experiments on two poker variants are presented showing that the technique can indeed interpolate between the Nash equilibrium strategy ("safety") and the best response ("exploitation"). I believe that the idea behind the paper is simply optimizing a convex combination of functions, and that the search formalism that was imposed on top is mostly an implementation detail from a theory point of view. Other than that, I do not believe the paper to be technically flawed or problematic in its claims.<|endoftext|>Experiments are sufficient. One area this paper could do a lot better in terms of clarity. For example, P2’s infosets should stretch over both the left and the right branches (which is briefly alluded to in point 1). Hence, tau is not known, or may not even be estimated using data (this admittedly is a problem for any type of opponent modeling). The authors present experimental results with varying epsilon (estimation error of opponent strategy).<|endoftext|>The submission proves that, for small enough alpha and posterior error, this gadget game produces a safe (non exploitable) strategy. Throughout the paper Safe Subgame Exploitation is improperly capitalized. The term "exploitability" is not generally used in a way that depends on the opponent s "true distribution". ### Exploitability TheoryI think the exploitability results are a bit confusing as currently written. It would be relatively to substitute this estimate for the "known" component of the strategy required by restricted Nash response (RNR) and test the exploitability.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 3; The paper demonstrates an algorithm to remove the effect of some training examples, S , on a learned model. In particular, it estimates the shift on the model parameters when the examples S  are removed, where the shift models the information contribution from the removed dataset. From the evaluation, the quality of the method also seems to be effective. In addition, this new definition helps to evaluate the quality of the unlearning algorithm. Is this estimated through samples from the p_S? Although I really like the metrics from the epsilon knowledge removal terminology, some case study would make the evaluation more convincing as the problem of removing training examples is a real world problem. There seems to have some benefits of modifying each sampled modeled parameter using some scalar multiplier, as it also helps to adjust the geometry of the distribution, i.e.variance, which should be changed with the number of training examples? The problem of removing the effect of some training examples on a trained model is well motivated by the paper. At the same time, the reviewer is not able to verify the proofs of the theorems in 4.3 and 4.4.<|endoftext|>This paper approaches the problem of how to make a machine learning model forgets some data samples it was trained on. This is of high relevance in the context of “the right to be forgotten” legislation. In this case, this work approaches this problem for machine learning models represented as Markov Chain Monte Carlo, which is of relevance in the context of Bayesian learning. The presented approach is based on the formalization of the problem as an optimization problem. The  “the right to be forgotten” is part of the legislation of many countries. Previous literature do not cover the case where the machine learning model is represented as a sequence of samples (i.e.Markov Chain Monte Carlo)  The proposed method is sound and provides theoretical guarantees about the capacity of the presented approach to implement this task. Your approach provides theoretical guarantees that it will erase this information from the model (with an epsilon error), but this guarantee only applies if a set of assumptions are met. I think they present a solid contribution to a relevant problem. Even though, there are some limitations that should be discussed by the authors.<|endoftext|>1.SummaryThe paper proposes a novel method for knowledge removal in a MCMC context. The contribution is four fold: First, a theoretical analysis relates the MCMC unlearning problem to an optimiziation problem introduced. Second, the conversed problem is related to a MCMC influence function. Third an abduction of theoretical results is done. Finally real world data experiments are performed showing that the proposed approach can indeed tackle the problem introduced. 2.Rationale for the scoreThe problem is very nicely introduced and well justified with recent (GDPR!) developments in application of IT / AI System. The paper is well structured, graphical contributions in the main part as well as in the appendix serve to enable a good understanding of the approach done. I can t see any weaknesses.<|endoftext|>This paper proposes the first method in MCMC for the problem of knowledge removal (meaning that the sampled posterior must be modified so that some subset of the training data is "forgotten"). The paper introduces the concept of epsilon knowledge removal to measure how closely the modified posterior corresponds to the exact posterior that would have been obtained based on only the remaining data. Strengths:  the problem of knowledge removal seems like it may see a lot of application, and introducing the first algorithm for MCMC is an important  step  the theoretical development of the method looks solidConcerns / questions:  The proposed algorithm works by translating the posterior without otherwise changing its shape. This would imply that many aspects of the data are not forgotten. For the BNN experiments (Table 1), none of the columns give information addressing this issue. As epsilon becomes large, the concept of epsilon knowledge removal becomes meaningless: at some point, even the original posterior (which didn t forget anything) satisfies it. Furthermore, I think the paper should have discussed these issues with the proposed approach.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; This paper studies the problem of online tuning for offline decentralized MARL. The online transition correction technique is proposed to correct the bias of the transition probability function between offline dataset and online dataset. Novelty: The bias of transition dynamics in offline decentralized MARL problem is interesting. Soundness:1.This paper studies the deployment constrained setting. However, BREMEN and MUSBO are never compared in the experiments. It is obvious that modifying the transitions given the new data improves the learning performance. 2.It seems that the OTC technique can also be applied in the single agent setting. Does OTC performs better in MARL than SARL? 3.In each iteration, OTC needs to find the most similar samples for each sample in the mini batch, which may incur large computation cost. Significance: The performance improvement of OTC compared with BCQ is convincing. But baselines such as BREMEN are missing. A transition modification technique, OTC is proposed to correct the transition bias during the learning. However, the experimental evaluation is not convincing as important baselines are missing, and the proposed algorithm seems to be too straightforward due to lack of theoretical analysis. Thus, I recommend the rejection.<|endoftext|>This paper studies the discrepancy between the online and offline transition data in multi agent RL training. The author proposed OTC to correct the biased transition dynamics and propose a training framework that can leverage offline data for more efficient online training. * The author studied an important problem in offline MARL. When the opponent agents  actions are not accurate, it can cause mismatch between the transition dynamics in the offline data and the transition dynamics in real online settings. As a remedy, the authors proposed to solve this issue by bridging offline training with online tuning. Though the proposed work is sound on the technical side, I feel not surprising about the proposed method. Given the novel setting of offline training with online fine tuning, the author maybe want to consider transfer learning cases like few shot settings, and experiments on other multi agent environment such as StarCraftII will also be favoured.<|endoftext|>The paper studies offline training and online tuning for multi agent systems, which focuses on the decentralized cooperative setting where each could only access to its offline dataset. Authors discover that the transition dynamics in the offline dataset and online execution can differ very much, and propose to bridge this gap by introducing online transition correction by modifying the sampling probabilities. Authors conduct extensive experiments on multi agent MuJoCo tasks, and show that OTC outperforms baselines (BCQ and AWAC). Pros:(1) Authors study an interesting and practical setting, offline training and online tuning for multi agent tasks. (2) The paper is well written, with a clear description of the proposed method. (3) Authors conduct extensive experiments to evaluate OTC. Cons:(1) Related work: The idea of correcting sampling probabilities is very close to that in (Jiang and Lu, 2021), and authors should discuss more about MABCQ in the paper. In the d function part (Equation 7), it leverages the difference in value functions for measuring the distance. However, Q(s, a) can be over optimistic for unseen actions. (3) Experiments:   There is a missing strong baseline MA ICQ (Yang et al., 2021) which also studies the offline MARL setting. Given Figure 4, there seems to be very large variance, and I m not very confident about the conclusion. Can the method scale with more agents? The paper studies an interesting setting, but should explain more about the potential problem of the method (please check the main review) and compare it with a more recent MA ICQ method.<|endoftext|>The paper proposes a novel MARL method to bridge offline training and online fine turning. More specifically, a transition function is maintained and updated according to the data obtained from online deployment. The paper is well written in most of parts. 1.For the high level idea that only using similar transitions to train the policy, is there any analysis? Intuitively, online exploration aims at finding novel samples that not exist in the offline dataset since these novel samples are helpful to know the environment and build a accurate model. However, the novel samples are not prone to be chosen in the algorithm. Why can the transition probability be modified by the Eq.3? 4.For the multi agent setting, do all agents observe the state rather than their own observations? It seems that the method can be directly applied to single agent setting. 5.For baselines, some offline RL methods are ignored like [1]. I would not decrease my score due to this reason. Arxiv 2021Although the paper is interesting, I still have some concerns on multiple aspects. Three important issues are: 1. The details of the algorithms. 2.The unique design for multi agent settings.
Reject; rating score: 3; rating score: 3; rating score: 5; The authors propose a method for using real world patient data to generate (semi )synthetic privacy preserving data on which to evaluate methods for causal effect estimation. **Strengths**:  The problem tackled is very relevant for causal inference problems, as there is almost always a lack of ground truth available to assess how well different causal inference methods perform. **Weaknesses**:  When the inclusion and exclusion criteria are applied to the original dataset, the authors are potentially introducing selection bias by conditioning on downstream variables such as hypertension. Shouldn t it be the other way around? Is the propensity model the same for both methods? Perhaps there is some particular structure in the data that favors the simpler propensity score stratification. If not, what is the difference? page 8, third line after Figure 3: small typo, "We" should not be capitalizedThe authors have presented a simple and potentially interesting approach for generating privacy preserving synthetic data from a real world dataset, but I am concerned by the fact that selection bias was introduced when the authors applied inclusion and exclusion criteria on the patients from the original dataset.<|endoftext|>This work proposes a method to generate a set of binary datasets that are realistic and include counterfactual outcomes so that they can be used for evaluating causal inference algorithms. It also addresses a very important, long lasting problem in the literature; that is, generating datasets that include realistic counterfactuals. That being said, I think there is a fundamental problem with generating the counterfactuals from a model trained on data: this is the whole reason why we develop causal inference methods. If we already have an algorithm that can predict the true counterfactuals, then the problem has been solved (and we no longer need datasets for evaluating causal inference methods)! Please see my other comments below:  Please provide a reference for the statement that the ACIC dataset is “limited by non representative populations”. Or am I missing something here? The paper provides little discussion on many design choices (e.g., why ADS GAN or WGAN GP was selected, etc.). Minor comments:  The abstract is too long; consider reducing it to one third. Page 5, line 2: don t → do not  The “subset of” sign in page 5, lines 9, 10, and 15 should be reversed.<|endoftext|>The paper studies the problem of generating synthetic patient data for the evaluation of causal inference models. The generated patient data is expected to highly mimic the distribution of the original dataset while also taking patient privacy into consideration. The purpose to simulate patient data is to evaluate causal inference models with generated ground truth. However, it is hard to guarantee that the generated treatment effects are close to the original treatment effects (both the effect size and sign): one can only guarantee the similarity between observed outcomes but not counterfactuals. The technical contribution of this paper is not enough. The following potential outcome generation is based on a feed forward neural network, which is a widely used backbone in causal effect prediction. More baselines (e.g., IPTW, matching based methods) should be considered to examine the goodness of created dataset. An interesting paper, but it seems that the proposed problem is not well addressed by the method.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 8; In this paper, the authors analyze the relation between GNN robustness against attacks with heterophily and propose a method to improve GNN robustness by separating the aggregators of ego  and neighbor embeddings. In general, the authors focus on an interesting and novel problem and the paper is of good structure. However, there are some issues the authors need to address. The authors only show the performance gap between before  and after attack on different GNNs that integrate the proposed technique. The novelty of the proposed method also needs to be justified. Using stacked ego embeddings for prediction has been applied before to solve issues like oversmoothing.<|endoftext|>In theory, this paper proves under simplified settings, for homophily graphs, the attack will increase the level of heterophily, and for heterophily graphs, this change will depend on the degree. In practice, this paper advocates a design principle of GNNs that separate aggregators for ego  and neighbor embeddings. Cons:  The major concern is the novelty and significance of this paper:    1. And the main contribution from the theoretical view, as the authors claimed, is the first formalization of the connection. 2.The design principle mentioned in the paper has already been proposed by previous works, e.g., GraphSAGE, and the connection between this principle and the aforementioned theory results is vague. This paper aims to investigate an interesting question about the relationship between robustness and heterophily of graphs, however, the novelty and significance of both theory and empirical results are not clear.<|endoftext|>The authors claim that effective structural attacks on GNNs for homophilious graphs lead to increased heterophily level, while for heterophilious graphs they alter the homophily level contingent on node degrees. This work studies the relation between graph heterophily and the robustness of GNNs and theoretically show that effective structural attacks on GNNs for homophilious graphs lead to increased heterophily level, while for heterophilious graphs they alter the homophily level contingent on node degrees under some specific assumptions. The paper is limited in novelty: while the paper investigates the relation between graph heterophily and the robustness of GNN and show that several heterophily inspired GNNs work more robust to GCN and GAT, there is no new methodology proposed in the paper.<|endoftext|>The authors present a study on the robustness on GNNs on homophilic and heterophilic datasets. ### Strengths* The paper has a thorough and well designed experimental study. * The paper studies empirical *and* certifiable robustness, which are both interesting and important. * The authors compare a broad range of different models. **Robustness in homophily based GNNs. In summary, I appreciate the thorough experimental study in the paper and believe that – even though the paper does not propose a new method – the findings have relevant implications for GNN design choices. There are some additional experiments and discussions I would be interested to see (see main review).
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; rating score: 6; The paper addresses the problem of modeling human behavior in a case where their deviation from the optimal behavior is consistent over time, rather than independent (systematic suboptimality). They claim that systematic suboptimality can be modeled by predicting policies rather than trajectories. To this end, they introduce Boltzmann policy distribution (BPD), as an alternative to Boltzman rationality, as a prior over human policies. I believe the paper is a good paper and authors have taken time to write it in an elegant way and design insightful examples to motivate their problem and show the advantages of their method.<|endoftext|>The paper proposes a new model BPD to account for the suboptimality of human behavior, and provide an approximation inference method for BPD. I’m overall positive about this paper. The intuition of the model is sort of based on “people are consistent over time”. The model has the potentials to be applied and further adapted to other settings.<|endoftext|>The authors propose an imitation learning algorithm that uses the known reward function to form a prior over human policies, the Boltzmann policy distribution (BPD). It’s unclear why continuity would be a problem here, and I suspect this is not true. Firstly, imitation learning from systematically suboptimal human behavior. I have concerns about the behavioral cloning baseline and the relevance to learning preferences. 2) One of the motivations of the paper is to understand human preferences, but the proposed method assumes that the human reward function is already known. I’m referring to the first sentence in the paper here.<|endoftext|>* This paper proposes an interesting approach to model and predict human behavior. * This approach predict human policies, rather than trajectories, so that it can capture the (systematically suboptimal) human behavior that is reflected in the human action choices over time. * The base measure, `p_base(\pi)`,  is defined as the optimal human behavior based on the known human reward function with suboptimality parameter \beta (Eq4). Generative adversarial imitation learning. * In my opinion, this work is similar to GAIL. * Minor:    * The comparison between the proposed method and behavior cloning is a bit unfair, since the proposed method has the knowledge about the human reward, while the behavior cloning does not. I think if the human reward can be updated, then observing that the human choosing route A could provide information.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; The authors propose a new penalization framework for the stochastic multi armed bandit problem with fairness constraints. The authors propose a new framework with penalty terms to model fairness constraints, which is different from previous fairness MAB formulations. Although the proposed penalization framework makes sense, it is unclear why we need this new framework. 2.The idea behind the hard threshold UCB algorithm seems to be very straightforward: just adding penalty terms to the original UCB values to favor arms that have not satisfied the fairness requirements. It is better to show some experiments with real world data. This paper introduces a new penalization framework for the fairness MAB problem. Besides, the motivation for introducing such a new framework and its potential real world applications are unclear in the current version.<|endoftext|>The authors deal with the stochastic multi armed bandit problem with constraints in the least number of arms pulls for fairness. Strength:  The authors propose a novel algorithm for the penalized variant of the fair MAB problem with the tight analyses of the gap dependent and  independent regrets. Weakness:  The definition of the fairness constraint is not reasonable. The paper lacks the rigorous guarantee of fairness. I recommend the rejection of this paper due to the lack of a reasonable fairness guarantee. First of all, the main issue of this paper is the problem formulation in Eq.2, particularly its constraints. That is, the proposed algorithm has a chance to guarantee the anytime guarantee of fairness. See the detailed comment.<|endoftext|>The authors propose and analyze an algorithm where the UCB index of an arm $i$   is increased by a specified amount (which is a parameter for the algorithm) $A_i$ for the time instances where the arm is lagging behind the fairness constraint. This algorithm is also compared with two recent state of the art algorithms for this problem in synthetic examples and illustrates superior performance. Strengths:      The idea of the algorithm is quite intuitive, however seems novel (not been proposed and analyzed in the literature). Experiments with a more reasonable range for the parameters of the state of the art need to be presented. A solid analysis of a novel and intuitive algorithm for the MAB problem under a type of fairness constraints, however the comparisons with related work are rather weak.<|endoftext|>The paper studies one variant of fair multi armed bandit (MAB) problem: For each arm $k$ played, there is a known fairness parameter $\tau_k$, which gives the fraction of times that this arm should be played, and if the total time $k$ is played by time horizon $T$ is less than $\tau_k T$, a penalty $A_k$ is added to the regret term for each insufficient play. The paper has positive contribution to the community on studying the penalty based fairness condition in MAB, but due to the weakness pointed out, its contribution is limited and is not easy to understand. Strength of the paper  Considering fairness in MAB is an important direction, and the current paper studies the variant where the fairness consideration is put into the objective function as a penalty, in contrast with the previous study that treat it as a hard constraint. It is unclear from the discussion in the paper whether it is due to the analysis or the algorithm. Page 5, Theorem 2In the line before the third equation in the theorem, notation $k_j$ seems to be wrong. I guess it should be $k$.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; The paper proposes a frequency based SGD method, where the learning rate is inversely proportional to the token frequency. The theoretical results is interesting in that the proposed method outperforms SGD *WHEN* skewed distribution appeared. I just have a few comments/questions:(i) it d be great to have a slice performance analysis, which may reveal that the improvement in infrequent items is even larger. though the generalization property is beyond the scope of the paper. The paper nicely introduces frequency information into SGD, with various interesting theoretical analysis and empirical results.<|endoftext|>The proposed algorithms are easy to understand and implement, and the theoretical analysis for the bounds are provided. WeaknessesThe experiments are insufficient. It is unknown if FA SGD outperforms the baselines in offline evaluation. In addition, only the cross entropy (point wise) loss is tested in the experiments. This paper proposes to integrate the frequency information of tokens into the optimization algorithms for a fast convergence. The idea is novel and the proposed algorithms are easy to implement. I recommend to accept it. Besides, more loss functions should also be investigated.<|endoftext|>The proposed method is very simple and easy to implement with provable benefits. The paper is well written and the proposed method is well motivated. The paper demonstrates that the proposed method has provable benefits over SGD. The title/abs/intro of this paper covers the various areas of embedding learning while the experiments are only on recommendation. It would be interesting to see a curve plot of the learning rate for each token v.s. the frequency of the token. The authors also show provable benefits over SGD. I think the paper can be further improved by changing the pitch to focus more on recommender systems and enriching the experiments as said in the main reviews.<|endoftext|>This paper proposes a counter based learning rate scheduler for SGD. This algorithm is designed based on the long tail distribution in recommendations and languages. Simultaneously, the authors demonstrated that the proposed algorithm achieved comparable empirical performance in two recommendation datasets. It is hard to distinguish each method in the current y axis scale. 4.Any results for languages? This paper is well written and made solid contributions for adaptive learning rate in long tail distribution.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; The authors propose a recourse methodology to deal with model biases/fairness issues in producing equitable outcomes for all users (classes). The paper addresses an important issue of dealing with model bias in producing fair user outcomes. My main concern with the paper is the lack of clear supporting arguments on why the choice of cost models (including knowing the distribution over the cost functions) is the right one? I am not clear about the assumptions made in the proposed cost function that each user adopts. Next, how is the recourse set itself guaranteed to always produce at least one reasonable counterfactual in the set? More specifically, even as the authors acknowledge knowing the exact cost function by each user is difficult, their explanation of using the recourse set to get around this problem with high confidence is unclear to me. Intuitively, it seems a measure like diversity will be more effective when the cost functions are unknown/private to the user.<|endoftext|>This paper aims to find algorithmic recourse that has low cost to the users. Unlike previous work, the authors do not assume that there is a known global cost function that is shared by all users. It is mentioned in the related work, how the closest literature to the paper is other cost based approaches to finding recourse and different from those approaches, this paper drops the assumption that there is a known global cost function that is shared by all users. Having said that, the proposed cost function has a certain structure and it is still novel: (i) authors propose a hierarchical cost distribution as the particular $\mathcal{D}$ they consider and (ii) by only considering the element with the minimum cost for each sample from $\mathcal{D}$, they exploit the fact that each users only really requires one recourse that they are happy with to be satisfied. Note that the current experiments are not helpful in comparing against other cost based objectives proposed in previous work: cost functions of the users are simulated according to the proposed cost function, then of course, a method that optimizes it would perform better than methods optimizing other cost functions. Some of the conclusions made in the results section suffer from user preferences being simulated as well. For instance, at the end of "Q2," the authors conclude that high diversity is not necessary to satisfy individual users; this is of course true for the simulated users since their cost function is designed to ignore diversity in the first place. However, it still introduces an interesting new objective to optimize for when finding algorithmic recourse.<|endoftext|>This work introduces a new method for identifying actionable recourses for users with user specific cost functions. Users’ cost functions are hidden from the recourse method. The paper proposed a discrete optimization algorithm COLS to solve the objective EMC. I enjoyed reading this paper in general. In Section 4.1, it is assumed that there is a distribution over all the cost functions D_c for the population. Is the distribution D_c known or unknown? For different users u, it is assumed that C_u follows distribution D_c. However, it seems inconsistent between the motivation and the assumption. Why do all users share the same distribution of the cost function? Theorem 4.1 does not imply that, but it is a very important question. 3.Why choosing Equation (3) and Equation (4) as metrics to measure recourse quality? 4.In the numerical experiments, could you compare with other functions that measure the recourse quality in previous recourse papers? I think my main concern is on FS@k. Is using FS@k equivalent to the following: assume there exists a black box algorithm that can output the indicator that if the total cost is smaller than k, then the distance function can be used to measure the recourse quality. Could you comment more on this part to motivate? This paper studied an interesting problem.<|endoftext|>In this paper, the problem of algorithmic recourse is studied where the goal is to find best recourse (counterfactual set) that is optimized for user cost. The author proposed new user incurred cost evaluation method, Expected Minimum Cost (EMC), which approximate user satisfaction without assuming a fixed global user cost function, and instead consider user cost functions as hidden and user specific. To cover diverse user cost functions, they propose to model user cost distribution with a hierarchical sampling procedure and estimated expected minimum cost by drawing samples from it. They introduce three new metrics for user satisfaction (all related to MinCost): FS@k, Coverage and PAC. Pros 	This paper proposes a new way of evaluating user satisfaction which differs from existing methods that measures on heuristics such as distance/diversity or assume a fixed global user cost function. It is more flexible and realistic, and thus could be an interesting direction to follow. The proposed formulation is quite novel and technically non trivial, with some theoretical grounding. The experimental results are very strong on the 3 newly proposed metrics. The discussion is pretty comprehensive, and they also included a fairness analysis. It is certainly good that the paper contains a lot of information, however currently it seems that the main text is a bit too packed such that very limited detail about the main methodology is provided. Is there any way to evaluate on more realistic user cost rather than simulating it with the same distribution as the one used in EMC? The authors talk about distributional shift regime in the appendix, that still the ground truth distribution is from the same family of the EMC distribution (mixture of percentile shift and linear cost). In the problem formulation in (2), does it mean that the best recourse set would consist at least one desired outcome solution but it may not be the one with lowest cost? The technical contributions are solid and the results are rather promising despite the potential bias toward EMC.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; While there are multiple theoretical results presented, the connection between them could be further elaborated, so that the takeaway messages can be clearly conveyed. The paper is hard to parse from time to time, and the organization of material is more or less confusing. If so, there is a worry of the tightness and the practical significance of the bound; if not, which notions could fit in the proposed framework?<|endoftext|>The paper studies the generalization properties of PAC learning with fairness constraints. Sections 2 and 3 are quite well written, easy to follow and provide some intuitive results. This is especially due to two reasons:  It is not clearly stated what the purpose/goal of the paper is.<|endoftext|>In particular, the paper analysis this under a PAC learning setting and a sample limit (asymptotic) setting. Some experiments are included. Strengths:    These bounds, although perhaps not surprising, are useful theoretical upper bounds for generalization under fairness. Majority of the proofs are application of Rademacher inequalities and union bounds. The "asymptotic regime" is quite limited in its discussion and analysis. Seems to be consistent with definition of $ h^{*} $.<|endoftext|>This paper establishes the guarantee for the generalization of fairness aware learning in binary classification under PAC learning and a more practical asymptotic framework. Considering practice, additional theoretical results or more diverse experiments expected to be added.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; rating score: 5; The key mathematical results seem to be correct. While the resulting algorithm is a straight forward adaption of RS, I think the conceptual and mathematical arguments on how to apply it in this setting are the key strength of the paper.<|endoftext|>I m not very familiar with the Neyman Pearson Lemma, but it seems unintuitive to me that in Lemma 3, one can provide a robustness guarantee against an adaptive adversary from the construction of a much weaker adversary. The exposition can certainly be improved to make the paper more self contained and readable. An adaptive version of the Neyman Pearson Lemma is proposed.<|endoftext|>The theoretical treatment of the problem is sound, and clearly stated. Cons:In multiple sections of the manuscript, authors claim that the proposal of policy smoothing is a novel contribution of their work.<|endoftext|>An adaptive Neyman Pearson Lemma is developed and a robustness guarantee in terms of the cumulative reward is obtained. But I agree that the authors adequately discussed the connection and difference between their work and the related works.<|endoftext|>This paper studies certified robustness of a policy in a reinforcement learning setting. I recommend the authors to carefully revise this paper in order to be published. The authors provide theoretical evidence and experimental results.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; rating score: 6; The authors propose a new method called ED2 that utilizes their insights from the empirical study. The paper is also well written and easy to understand2. I believe the paper studies a problem that s important and significant in the rl community and very relevant to the venueweaknesses/questionsIn general, I think the results should be treated more carefully and some theoretical motivation is needed. Following are some of my questions. 3. fig.4 again, why not compare the ensemble baselines?<|endoftext|>The paper also introduces ED2   an ensemble method utilizing the design choices from the study which is demonstrated to achieve SOTA results on Mujoco benchmarks. The ideas are well articulated and clear. This has been a fixture is most DRL algorithms. Would perhaps a better comparison be against an ensemble of SAC runs? The main contribution of the paper are (1) the empirical study into the various design choices within DRL algorithms used for continuous control settings and (2) an ensemble approach that integrates these learnings. While the ensemble method achieves SOTA results on many tasks and the empirical study presents some riveting results, the novelty in the paper is quite limited.<|endoftext|>This paper presents a deep reinforcement algorithm, Ensemble Deep Deterministic Policy Gradients (ED2), for continuous control tasks. The algorithm is empirically derived and is claimed to represent SotA performance on several tasks and while providing more stable results. On Walker it appears as though SUNRISE actually outperforms ED2. Given that the algorithm is entirely empirically derived and the empirical results are not compelling, I have given this paper a reject. Much of the paper is dedicated to information that would be better in the appendix.<|endoftext|>This paper has two main contributions: it introduces an ensemble based actor critic method, and it answers some pertinent questions in policy optimization by focusing on its different components. The ensemble is different from multi actor learners that interact with multiple environments simultaneously, violating the standard RL setup. It is also revealed that actor initialization affects performance less than critic initialization. Moreover, some key insights on deep policy gradient methods are presented such as the contribution of actor and critic initialization. Another strength of the paper is its focus on various details such as ideas that didn’t work as well as ablative studies in various manners. Weakness:The main weakness of the work is the fairness of the experiments. Even if there is a case, isn t it quite restrictive? I would be willing to know from the authors their thoughts on it.<|endoftext|>This paper conducted an experimental study over a range of tricks that are often exploited to facilitate ensemble deep reinforcement learning. Meanwhile, experiments show that the initialization of critics perhaps has a higher impact on learning performance than the initialization methods adopted for actors. While this paper seems to show some interesting new results related to ensemble reinforcement learning, there are several issues with this paper in this current shape:1. It was claimed by the authors in this paper that ED2 brings together existing RL tools in a novel way. However, it is unclear which part of the design of ED2 is truly novel. Hence the novelty and technical contribution of this paper may need to be improved. The experiment results in the paper only revealed some insights. Consequently, the technical contribution of the corresponding experiment results does not seem to be sufficiently strong.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 8; This paper proposes to represent point cloud data as samples drawn from statistical distribution, and project the data to an underlying statistical manifold on which various analyses can be conducted in a manner that is more favorable than doing so in a regular Euclidean space. The autoencoder with the Fisher information metric as the metric of the latent space is an intriguing proposal. The description of representing points as a distribution is accurate, yet has already been explored in registration. Finally, the authors transfer the learnt representation between dataset to demonstrate that the proposed metric space is suitable for representation of various point cloud data.<|endoftext|>The paper proposes treating the point clouds as probability distributions and equipping them with a Fisher Information Metric, providing a Riemannian geometric structure that can be used as regularization in AutoEncoders for training and latent interpolation. CONTEXTThe paper does not offer a proper context around the work. This would give insight into the proposed metric s behavior on different geometrical features (e.g., different curvature and sharpness).\c) Only rigid objects have been shown, all with the same distributions, while one of the paper s claims is that defining probability distribution is "likely quite natural and intuitive to the user". Which are other reasonable choices for probability distributions that respect the given assumptions?<|endoftext|>The paper studies 3D point cloud data. The core message is that one can regard a point cloud as a collection  of samples from an underlying distribution and, hence, one can imagine a manifold of point clouds and construct an associated distance metric via the Fisher information. If one claims practical utility, then one should nowadays really show at least "semi real" point clouds (e.g., Scannet, semantic3d.net, KITTI, ...)A rather theoretical paper, the proposed view is in some sense elegant, but not as revolutionary as claimed.<|endoftext|>The paper concerns data sets of point clouds in Euclidean space and proposes to analyze those as samples from underlying probability distributions. Each point cloud being represented by a probability distribution can now be seen as a point in a statistical manifold on which the Fisher information metric provides a natural Riemannian structure. Overall, I find the idea novel, the presentation clear and interesting.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 3; This paper discuss a concept of magnitude vector of images. The authors provided a fast algorithm for computing the magnitude vector, and discuss the potential applications in edge detection and adversarial robustness. Strength:The paper discuss a concept that is relatively new in the machine learning community. The experimental results are not strong enough to demonstrate the benefits of the concept of magnitude vector of images.<|endoftext|>The authors consider a relatively new finite metric space quantity known as the "magnitude" and its applications within computer vision. The reason that I have recommended this paper are several fold. This is not a conclusion, it is an assumption.<|endoftext|>This paper explores the application of the quantity “magnitude of a metric space” for images. International conference on machine learning. Section 3.2 applies the method for edge detection and compares it to Canny detector. Section 3.3 uses the magnitude for adversarial robustness and claims gains. Theoretical contributions of the work are not clear. My understanding is that the contribution in the Section 2.2 is the particular definition of metric space for an image (Def.4).The rest of the section applies prior tools to this metric space. The significance of Section 3.2 is not clear.<|endoftext|>This paper applies the machinery of magnitude vectors of metric spaces to extract information from individual images (which can be themselves be seen as finite metric spaces having one point for each pixel). Here are some additional minor comments:  After Definition 1: a *finite* set of vectors  Lemma 6: this is obvious, there is no need to cite a 2009 preprintThis paper presents an application of magnitude vectors to image, which is not convincing both from a theoretical perspective (why should the given definition be useful?why should the given speed up algorithm work well?) and from an empirical perspective (the results on edge detection and adversarial robustness are weak).
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 8; This paper intends to investigate and analyze the phenomena of adversarial examples through the perspective of Fourier Analyses. It claims three major contributions:1) they claim that adversarial examples is neither high frequency nor low frequency. In particular, the author list three contributions in the 1st Section. Yet, most content in this section is about related works. The author only bring up their own work at the end of this section with three very brief descriptions. The first contribution (neither high nor low) is somewhat interesting but not well supported. The author explain this argument in Section3 but I cannot understand their claim. What do you mean by "as one cannot verify the con  verse setting of blocking low frequency components. This is because low frequency components are inherently tied with labels (Wang et al., 2020b), conflating the two phenomena." For the other two contributions, I think the authors themselves did not know how their work can help the community. After all, adopting the Fourier perspective is not the original idea of this paper. If there is any, the author should highlight these parts during rebuttal or maybe put them into the 1st Section. I decide to reject this paper for now.<|endoftext|>This paper presents a frequency based understanding of adversarial examples in deep neural networks. The main observation is that the adversarial examples are neither in high frequency or low frequency components but dataset dependent. This paper presents some interesting observations of adversarial examples in the frequency domain. This work is motivated by “the common misconception that adversarial examples are high frequency noise”; however, such an understanding has already been questioned in the literature. Therefore, the motivation should be better justified. The contribution is more like additional evidence of the ongoing debate, but rather a new frequency based understanding of the “common misconception”. This should not be overclaimed. More datasets should be considered including the simplest one (MNIST), and others such as CIFAR 100, SVHN, Fashion MNIST. 4.It claims the “observations overlap with insights from the concurrent work by Bernhard et al.(2021)”; however Bernhard et al.(2021) appeared on arXiv in April 2021. The authors may want to highlight the differences and justify this point. 6.The measure of average noise gradient over the entire dataset should be further justified.<|endoftext|>In the paper, authors investigate the questions of adversarial robustness through the lens of spatial frequencies. In particular, contrary to a popular misconception, adversarial examples are not always related to high frequency components. Carefully crafted experiments validate their findings and suggest a more effective approach for adversarial training. Strengths:1) Interesting experimental setups and findings2) Insightful conclusion and practical takeaway for better and more efficient adversarial trainingWeaknesses:1) Minor grammatical errors   “vs˙robustness”2) Presentation of figures is a little inconvenient, they are usually presented earlier than when mentioned in the text, so one has to go back to previous pages each time to see them while reading. The paper negates the widely held belief about high frequency nature of adversarial examples through interestingly designed experiments with practical consequences of more effective adversarial training, and, therefore, I recommend an acceptance. However, the presentation of figures and minor grammatical errors should be addressed.<|endoftext|>This work explores adversraial robustness from a frequency perspective. * The paper is well written, easy to follow, and provides extensive supplementary material. * The experimental design in section 6.1 (Weighing the contributions of different frequency components) is a nice idea providing interesting results. # Weak points* In some sense, the finding that the frequency properties of adversarial perturbations are dependant on the dataset is somewhat common sense. This work mainly explores adversarial examples for models trained on natural images, which are generally more low frequency. I assume that the frequency property of adversarial examples would be different for models trained on relatively more high frequency images, such as radar signals or vibration signals. Such an experiment could also be insightful to further support one of the main points of this work. Hence, this work also provides only a partial view of the frequency perspective of adversarial robustness. The authors should discuss or point out this limitation of their work. The authors should discuss the differences.
Accept (Poster); rating score: 8; rating score: 8; rating score: 5; rating score: 3; **Guidline for future research**: The paper summarizes several best practices to create surrogate NAS benchmarks for new search spaces. It is particular important as there are many promising architectures that are not currently covered in DARTS, or NAS Bench 1/2. For example, research in transformer like architectures are increasingly popular. This paper provides a feasible method to fast evaluate new search spaces. Cons:  **Search trajectories**: The paper has done lots of experiments to show that the search trajectories running on the surrogate benchmark closely resemble the ground truth. However, it is unclear if the searched models (via surrogate and via ground truth) are also similar. How about the unseen models in the DARTS search space? Overall, the paper is very well written. It is clearly motivated and supported by lots of ablation studies. The dataset and search space might not be SOTA, but it does provide a promising practice for future NAS research. There are a few questions that I would like the authors to clarify, otherwise, I would recommend acceptance of this paper.<|endoftext|>The new expanded benchmark is named as surrogate NAS benchmark. In general, this is a good NAS paper that explored a new direction   surrogate NAS benchmarks. This work brings some fresh ideas and artifacts to the NAS community. We can see many NAS benchmarks were accepted in the top venues this year. The contribution, novelty, analysis of this work is above the most recent published NAS benchmarks. Weakness:  As surrogate HPO benchmarks have been proposed, it would be good to have a deep analysis on the comparison with that. Minor issues:  In the first paragraph, the authors cited (Hao, 2019) for carbon emissions.<|endoftext|>Overview: This work proposes a new NAS benchmark based on the results of surrogate models prediction. This surrogate model is able to predict all architectures in DARTS search space, which is about 10^18 possible architectures. The author compared the predict performance among different type of surrogate models and also leveraged surrogate models to investigate different NAS methods. Compared to the prior submission, the authors have included results on CIFAR10, CIFAR100, and ImageNet. I thank authors for doing that, however, there is still some concerns for me to see this work get published. However the selected benchmark baselines are pretty old. 2.All the results are plotted by the wall time, which is a bit tricky in NAS. The costs of querying a surrogate model can be cheap, but sample efficiency (#samples over accuracy) is far more important to NAS as the cost of evaluations are the main bottleneck, not the search. Please correct me if I m wrong, but I don t think you consider the cost of evaluations in the wall time. 3.Please diversify the tasks. Here I mean different tasks, e.g.detection or segmentation, rather than image recognition with different datasets and models. However, the current draft can not truly reflect the recent advancement in the field, and it may mislead the future. Once the author have address my concerns 1, 2, I m open to accept this paper.<|endoftext|>Overall, the paper is well written but it does not contain significant contributions. This has been done earlier for hyperparameter optimization as pointed out by the authors themselves and NAS is nothing but a specific type of hyperparameter optimization. Additionally, Yan et al.presented with their paper "NAS Bench x11 and the Power of Learning Curves" already one way to create surrogate benchmarks for NAS. There is no reason to believe that a larger search space is more interesting than a smaller one if not chosen correctly. Concluding, I do not see this benchmark to be particularly useful. The authors don t discuss all the shortcomings of their benchmark. Methods such as DARTS or Hyperband search differently and it seems like that these methods cannot be evaluated on this benchmark.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; Here are the details:1. I suggest the authors check the notations in the paper to increase the readability of the paper. I think $\eta$ should be related with $T$ or $\epsilon$. 6.The following papers also adopt sketching to reduce the communication cost of federated learning. The paper lacks empirical results and the theoretical analysis is somewhat weak.<|endoftext|>The paper studies the application of linear sketching algorithms to reduce the communication complexity in federated Learning problems. The idea of using random matrices to reduce communication complexity in federated learning is not new at all. Thorough proofreading of the paper should be able to fix this. B.6 It is disappointing that there are no labels to identify the parts in the appendix as proofs of the results presented in the main paper. As I pointed out in A2., properties similar to Definition 4.1 have been highlighted by many papers since 2017. **B.Presentation and clarity**B.1 The authors can significantly improve the presentation and the clarity of this paper.<|endoftext|>This paper proposes a sketching based federated learning algorithm to address the communication and privacy concerns in the federated learning setting. 2) The paper makes several interesting technical contributions in the federated learning setting. Weaknesses:1) There is significant room for improvement in the presentation of the paper. However, there is significant room for improvement to make the contributions of the paper clear. Especially, the discussion around privacy guarantees needs to be improved. It would be nice if the authors can connect the supplementary with the main text with more references to the supplementary. 6) There is scope for improving the discussion of related work.<|endoftext|>The paper proposes a sketching approach to reduce the communication cost of federate learning, and prove the convergence of their proposed method for convex function. But as other reviewers pointed out, I suggest the authors include a more detailed discussion on related work. Weakness.There is no experiments, and the total communication cost does not decrease.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; This paper proposes to learn disentangled representations via contrastive learning on well pretrained generative models. Extensive experiments are conducted on various datasets and the results validate the effectiveness of the method. Learning disentangled representations via contrastive learning on pretrained models is an interesting direction for this community. 3.Extensive experiments on disentanglement learning and high resolution datasets are conducted with satisfying results obtained. 4.The introduced entropy based domain loss and the hard negative flipping technique are effective and practical. 5.The proposed method is general for multiple generative models. The introduced method is simple, effective, and general for disentanglement learning. I recommend accept for this paper.<|endoftext|>The paper proposes a novel representation learning technique to disentangle the latent space of pre trained generative models, by discovering semantically meaningful directions in them. I raise my score to accept. First, random samples are perturbed along the directions obtained from the navigator. The output is in the variation space, where a contrastive learning technique clusters together the samples that were perturbed with the same direction. Quantitatively the proposed method shows better performance than the baseline for many datasets and 3 different kinds of generative model: GAN, VAE and Flow. This is impressive and shows the methods generality. Bad:Although the paper explained the method well from the perspective of reproducibility, it does not explain why the method should chose semantically meaningful directions. In principle the training loss could be minimised with this solution as well (?). "(ii) the factors are embedded in the pretrained model, severing as an inductive bias for unsupervised disentangled representation learning." One might think that the gains come from the extensive tuning rather than the proposed idea itself. some typos and grammar could be fixed, e.g."... generative model are two ford: ..."I think the paper contains good practical ideas and a very extensive experimental evaluation. The authors have clarified some details in the experiments.<|endoftext|>This paper proposes DisCo, a framework that learns disentangled representations from pretrained entangled generative models. Extensive experimental results show that DisCo outperforms many baselines in both quantitative and qualitative evaluations. Could the authors tell which sub metric of DCI you are using for evaluation? 2) Extensive experiments and ablation studies. Cons:1) There are still some flaws in the proposed method. 3) MIG and DCI metrics are out of date and may not well characterize disentanglement. 1) Novelty:  The authors propose a novel and interesting idea of learning disentangled representations by reusing the decoder/generator of pretrained generative models and only learning the directions that lead to disentanglement. Why are the sizes of the query key set and the positive key set different? They should be the same in contrastive learning. Could the authors elaborate more on this? 3) Clarity in presentation:  It seems that the flipping of negative samples into positive samples in Eq.7 plays an important role in the model’s performance but is not carefully analyzed in the paper. How does the flipping rate change during learning? How MIG and DCI are computed for discovering based methods that only use GAN? The problem with MIG and DCI is that MIG was shown to capture only modularity, DCI consists of 3 separate sub metrics and each of them only captures one aspect of disentanglement [2].<|endoftext|>This paper presents a framework to model disentangled directions for pretrained models. The underlying idea is contrastive based: similar image variations are caused by changing the same factors in contrast to the remaining image variations. Strengths:  The approach does not require any specific training. There is no fixed generative model type: it can be applied to GANs, VAEs and Flow models. The method is quite stable to random seeds. The authors provide a thorough ablation study, report the model accuracy with std due to random seeds, check the model sensitivity to the values of hyperparameter T.Weaknesses:  The approach requires many  tricks  and parts to work: Navigator, Contrastor consisting of two weight sharing encoders, contrastive approach, hard negatives flipping. Each component requires its own set of hyperparameters. Overall, the proposed method is outperforming previous approaches in terms of disentanglement scores.
Reject; rating score: 1; rating score: 1; rating score: 1; rating score: 3; rating score: 3; This paper proposes incorporating (a large amount of) human knowledge into policy network design such that it solves the problem directly or within a few iterations of training. Strengths:  Throughout the paper, the only thing that makes sense in my opinion is that the goal of incorporating human knowledge into the network design. Really hard for readers to understand. The network is designed case by case with heavy human effort, I don t see any learnable component in the network that makes sense and effectively supplements the human design. This is probably because the technical approach is not reasonable. A clear rejection based on the weaknesses mentioned above and no intuition to the community.<|endoftext|>To create an interpretable system, the authors propose to manually construct neural networks such that each neuron corresponds to an interpretable concept. The interpretability of the proposed system is not demonstrated. 1.This work does not present a systematic approach that can be broadly used. This work is not compared to other approaches for hand specifying policies.<|endoftext|>The paper proposes the Self Reward Design method for learning a reward function and designing an interpretable neural network for a specific task. In fact, the paper suggests that the network weights do not explain the failure modes of the policy: "We see that even failure modes can yield weights profile that look similar to non failure modes". The neural networks for the two environments seem to be hand designed by the authors, and it s not clear how they propose to design a network for a new problem. The general form of the SRD method (independent of the task) is not explained in the paper.<|endoftext|>This paper proposes using individually designed, interpretable neural networks to solve a given task. Each neuron in the network is hand designed to serve a specific task dependent purpose. The neuron level design of the network is used to provide human interpretability, as the behavior of the network can be understood based on the activations during environment interactions. Interpretability for deep reinforcement learning is an important and significant topic of research. Some of the major concerns with the paper are:1. The mechanism is not scalable.<|endoftext|>This paper proposes an interesting idea of designing specialized neural networks for maximum interpretability, so that the human designer can inject as much semantic information in the feature space as possible to simplify learning. Explainable machine learning is an important topic of research and designing ways to allow humans to inject as much domain knowledge as possible for learning agents is also an important direction for allowing machines to quickly acquire knowledge from their human teachers. However, the method proposed in this paper seems a bit too ad hoc for specific problems and grants no generalizability to complex real world problems. The writing of this paper is also not very professional and was hard to follow.
Reject; rating score: 3; rating score: 5; rating score: 8; The article would benefit from a more in depth discussion of the fundamental differences between the approaches considered and what it means for the term "saliency". The article would benefit from some sort of evidence that the produced taxonomy is of value to the end users. The paper does not provide actual evidence (for example, use case or user feedback) of this added value. Similarly, saliency is defined as an abstraction in the introduction, but an abstraction to what is not clearly discussed, nor is the fact that there may be a disconnection between the expectations of the user and the actual quantities being calculated by different methods.<|endoftext|>The paper is entirely a survey/review of existing methods; it does not propose any new XAI algorithms. However, I found the proposed dimensions and comparison of the saliency methods along these dimensions very interesting and informative. I could see this framework being useful to researchers in XAI as well as end users. The paper is also very well written and clear. The main weakness of the paper in my opinion are:(1) The methodology is not entirely transparent   it s not clear how the models are ranked on these dimensions or how a "score" would be computed for a given dimension. The authors note that some dimensions like "perceptual correspondence" are difficult to measure and may require large scale user studies; other dimensions may be measured in multiple different ways and different methods could give different ranking results.<|endoftext|>While it is reasonable to focus on those that have established themselves over time as impactful, it would be helpful to incorporate a few more recent methods to ensure that they still fit into the framework. While the paper should be quite useful for researchers and practitioners of saliency algorithms, it does not present novel work to build upon. Some may find this insufficient for ICLR. These types of cards have proved to be useful in other domains, as the authors state, and should be quite useful for user facing algorithms such as saliency.
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; The authors learn show that an optimal solution for a specific reward vector can be found by minimizing a constrained quadratic optimization problem. I didn t find major drawbacks in the paper, and overall the theoretical results seem correct (yet not very novel or fundamental). Overall the work seems solid to me. 4.The authors wrote: "We start by interpreting the optimal solution of Problem PA as a function of the policy parameters $\omega^*   \omega^*(\theta)$". Can you elaborate on this? See review<|endoftext|>The proposed approach is simple and proved to be effective in the proposed experiments. While the idea is interesting, my main concern is about the novelty and the appropriate reference of previous work. As far as I know, the first order necessary stationary condition was already used together with REINFORCE in previous work (see [31]). Finally, there are a few typos in the paper that makes the reading complicated, especially in section 3. Same for the multi objective case. Eq.3 is not clear.<|endoftext|>* I am a bit confused by the results. 2) The first order condition is a necessary but not sufficient condition for Pareto optimality. The idea of the paper is interesting, but there are many things that the authors should clarify, both in regards to limitations of the methods and evaluation (metrics, domains, comparison against other baselines). The original has a convex frontier, which PPA cannot solve. It would be helpful to compare PPA with other stronger baselines, such as PGMORL (Xu et al., ICML 2020).
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 3; This paper study a novel contextual bandit algorithm: Neural LinUCB. As in (Riquelme et al 2019), the idea of this algorithm is based on decoupling deep representation learning and exploration. The proposed algorithm is also an improvement over NeuralUCB (Zhou et al 2020) for two reasons: the computational cost of the exploration is lesser, since the exploration is only done in the last layer of weights, and the regret upper bound is tighter. Indeed, in contrast to (Zhou et al 2020) it does not depend on the dimension of the tangent kernel matrix, which can be in O(KT). The theoretical results are interesting. If Neural LinUCB reduces the computational cost of NeuralUCB, it remains very high in comparison to LinUCB due to the episodic retraining of the deep neural network. Neural LinUCB cannot be used if T is too large. Do the authors have any leads for handling this practical problem? The analytical results on the contextual bandits with Deep Neural Networks are not totally news, as well as the algorithm with shallow exploration on the last layer, but together, they constitute a significant step. As a result, a clear improvement of performances in comparison to LinUCB, and moreover this fixes a known issue of LinUCB: due to the matrix inversion, LinUCB cannot process more than few hundreds of features. An efficient Neural Contextual Bandit algorithm is analyzed, and this could have a deep impact on the bandit community.<|endoftext|>Authors tackle the setting of contextual bandits, using deep representation learning combined with an upper confidence bound algorithm. The main contribution of this work is to provide a regret bound for the setup which decouples the representation learning from the UCB search, by searching only over the last layer of the network. For instance, the results are given for UCB only   could a similar analysis be extended to the Thompson sampling case (i.e., that used in Riquelme et al.2018)?There is also some concern about the requirement on the width of the neural network in thm 4.4 being so large in order for the main O(\sqrt(T)) bound to hold, rather than a maybe more honest O(m^{ 1/6}T) bound, but the authors do at least provide some empirical evidence in the appendix that suggests the performance of their algorithm is not heavily reliant on very large values of the network width. Experimental results consider several reasonable domains+baselines and support the claims.<|endoftext|>The paper presents a new neural bandit algorithm with shallow exploration and provides a regret bound for the proposed method. The existing approaches have introduced deep neural networks based bandit algorithms to learn reward functions, in which exploration takes place over the entire network parameter space, which can be inefficient for large size networks which are typical in NTK based approaches. The authors address this by taking an existing approach that decouples the deep neural network feature representation learning from most of the exploration of the network parameters by only exploring over the final layer of the network. Although the combination is not novel, the authors make a theoretical contribution by demonstrating that the combination has sub linear O(\sqrt(T)) regret.<|endoftext|>Compared with existing neural contextual bandit algorithms, the proposed algorithm attains computation efficiency. The studied neural contextual bandit is very interesting and important in the area of applying neural networks to online learning. Weakness:While the proposed idea, i.e., transforming the raw feature vector using the last hidden layer of a deep ReLU neural network and adopting an UCB approach to explore in the last linear layer, is simple and effective, this idea is inspired by existing Thompson Sampling based work [Riquelme et al.2018].In addition, several techniques in the regret analysis of this paper are similar to existing works, e.g., [Riquelme et al.2018; Zhou et al.2020], which reduce the technical novelty of this paper. While Riquelme et al.(2018) is a Thompson Sampling based algorithm and you design a UCB based algorithm, I do not think that adapting an existing idea in Thompson Sampling type algorithm to the UCB type algorithm is a significant contribution in algorithmic design. 2.Theoretical analysisI appreciate that you propose the first theoretical analysis for the idea of decoupling representation learning and exploration on your UCB type algorithm, while prior work [Riquelme et al.(2018)] does not give theoretical analysis on their Thompson Sampling type algorithm. Additional Response I have read the authors  further response. Overall, I think that the studied neural contextual bandit problem in this paper is very important.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; The paper introduces a new kernel, the neural Fisher kernel (NFK), to learn data representations for generative models in both unsupervised and supervised settings. NFK extends the Fisher kernel to neural networks, and a low rank approximation of NFK is used to scale to large datasets. The paper proposes a formulation for the NFK for unsupervised (GANs and VAEs) and supervised neural networks. The kernels are defined as inner products of the Fisher vectors (derived from the Fisher score) computed from pre trained neural networks. Strength:•	Nice idea to see extensions of kernels, here the Fisher kernel, to neural networks. Are there other references? The paper presents interesting ideas of extending kernels, here the Fisher kernel, to neural networks. My grading suggestion is a 6, as I believe the paper presents nice ideas but would require more work. Could the authors comment on that? •	Low rank structure of NFK and Alg 1: How does one choose the feature dimensionality $k$? How is this different for NFK? The way I understand “low rank” is that the data has its own rank, which is low, and could potentially be learned. The goal of the paper is to present a method for supervised and unsupervised settings, however in the results an example on semi supervised is also presented.<|endoftext|>The paper proposes a new strategy for extracting compact and intuitive representations of the data from a neural network. Simplifying the representation through low rank approximation of the kernel matrix is an interesting technique and may have a good impact on representation learning. Using the power method for computing the low rank approximation through automatic differentiation could have applications that go beyond the proposed approach. For example, it looks like using Fisher vectors in representation learning and approximating kernels through truncated SVD are not new ideas. Also, the authors could have spent some more words to outline the main differences and advantages of using the Fisher kernel instead of the standard Tangent kernel. It is not clear if the main contribution of the paper is the extension to unsupervised learning or the proposed low rank approximation. questions:  Is the approximation of the fisher tangent kernel the key novelty of the paper? I like the idea of using automatically computed Jacobians in the power method. Can this be applied to the standard tangent kernel? The experiments do not compare the proposed approach with non fisher similar methods.<|endoftext|>The paper investigates the representation of modern neural networks from the perspective of kernels by extracting features from pre trained network models. This paper is interesting and technically sound, its novelty comes from two perspectives. The proposed NFK serves as a unified kernel for both supervised and unsupervised learning models. Besides, the authors take advantage of the low rank structure of NFK to avoid directly handling unmanageably high dimensional vectors. 3.Some figures could be used to help to better explain the framework of the proposed method and also its effectiveness.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 8; This paper developed a new decentralized adaptive gradient descent method to address the data heterogeneity problem. 2.This method can only converge to the neighborhood of the stationary point. However, existing methods can converge to the stationary point. It conflicts with the common practice that $\beta_2 0.1$ or $\beta_2 0.01$. Thus, this convergence rate is problematic. 5.Which value is used for $\beta_2$ in the experiment? According to Theorem 1, $v$ should be smaller than $\epsilon$. Thus, I don t think the hyperparameters in Theorem 1 are reasonable. The problem studied is interesting. But the method is not novel and the theoretical analysis is not solid.<|endoftext|>This paper proposes a decentralized adaptive method for distributed deep learning, termed DAG Adam. Convergence results are provided for smooth non convex objectives under a bounded gradient assumption. However, I do not find the claims in the paper to be well supported. I would expect the heterogeneity to affect the convergence rate, but not the final validation performance. To me it seems that the reason your proposed method seems to resolve the obvious issue of adaptive methods with linear consensus updates is that the consensus update is incorporated into the adaptive momentum buffer (i.e., tracking not just moving average of gradients, but also the consensus constraint), as opposed to swapping the order of adapt then combine vs combine then adapt. In short, I find the paper very well motivated, the problem is very important, authors show a familiarity with related work, and the method is sufficiently novel.<|endoftext|>This paper proposes a new variant to decentralized Adam with a strategy called Communicate Then Adapt. The paper provides analysis and toy example to illustrate why a traditional Adam would fail to converge to the exact solution, and provide experiments on CV/NLP tasks to substantiate the theory. This may be unfair in the comparison to other algorithms   it is possible that DAdam can perform equally good if the mixing matrix there is also tuned by $\nu$. However, it s not clear to me how these results interact with the main theorem. As these are non convex, and there is no $x^*$ anymore, it s unclear whether these improvement comes from the strategy of "Communicate Then Adapt", or just the fact that gradient tracker is used/mixing matrix is tuned. On a side note, I recommend the authors to include std for each table, as the results are pretty close.<|endoftext|>This algorithm also cannot converge to the optimum with heterogeneous input. Finally, they discussed the loss of efficiency of recent GT DAdam in the same setup. These arguments motivates authors to design the algorithm with communicate then adapt. In Theorem $1$, authors present the theoretical result for their algorithm that converges: with the constant learning rate the augmented gradient converges to 0; with $\gamma   1/\sqrt{T}$ the global average of iterates converges to the stationary point. The heterogeneous input is very important and I think that the adaptation of Adaptive Distributed Adam to it is an important step. This can save a lot of communication cost and make an algorithm even faster.
Reject; rating score: 3; rating score: 3; rating score: 5; This paper formulates a hierarchical RL agent that learns to request information from human assistant, when task critical information is missing. 7.The data presented in tables would be better readable as a plot. The authors do a good job implementing existing algorithms in the context of the given task, however the applicability of this framework is very narrow, to a context of running RL agents with a near perfect incomplete policy to evaluation tasks with missing annotations. The paper claims that the agent that can request assistance from a human to reduce uncertainty performs better on the given task, than an agent that can not request assistance   this claim is correct, however it also seems obvious. Page 2: "The agent achieves this level of performance while issuing only 4.8 requests to the assistant on average, representing less than 1⁄4 of the total number of actions taken in a task execution." 2.Related work is very brief, and given in the end of the paper   instead of at the beginning to motivate the current work. However, it seems obvious that an agent with less uncertainty will do better.<|endoftext|>I am also not entirely convinced the method is that different from the action advising framework. The paper is kinda Robotic Navigation oriented by the way tasks and "additional information" about the environment are described, as well as in the empirical evaluation. Therefore, my grades are maintained so that the authors can improve the manuscript for the next submission. Paper investigates an interesting and relevant topic. However, there is a major omission of related works. Therefore, my recommendation is to reject the paper. Works that focus on the specific points claimed as contributions in the manuscript also exist, such as considering as a MDP the problem of learning "when to ask for help" [4, 5], or adding a hierarchical component to task selection before the low level resolution of the task [6]. "Simultaneously learning and advising in multiagent reinforcement learning."<|endoftext|>The objective is for the learning agent to seek help from the assistant while solving a task, and the decision of what to ask the assistant and when to do so is made by learning an interaction policy using hierarchical reinforcement learning   in particular using a variant of POMDPs. The assumption is that the learning agent s operational policy on the task environment is not changed and an additional interaction (hierarchical) policy is learned on top of the operational policy which dictates when and what to ask the assistant. **Pros**:* Writing and Presentation: The paper is well structured. * Baselines: The rule based baseline seemed to be particularly weak in the context of this work. However, the experimental section could be made clearer since the results presented look promising. At the moment, both the baselines are weak (no assistance/rule based assistance). Hence.I am inclined to reject the paper at the moment.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; 3.The paper is clear and well written. The authors explore using CLIP as a visual encoder in two settings, plugging its features directly into task specific fine tuning; and combining CLIP with intermediary vision and language pre training before fine tuning on downstream tasks. While this paper does not introduce a novel method, it provides important experimental information to many in the community. 2.The experiments presented in this work are solid, informative and representative.<|endoftext|>Overall, this is a borderline paper to me as it does demonstrate the potential of CLIP with good results. Experimental results show that CLIP pretraining leads to competitive performance and further combining it with V&L pretraining can outperform existing methods. For a purely empirical study on an existing technique,  I expect more in depth analyses than simply showing a collection of results.<|endoftext|>E.g., does 0.037 mean that one can simply flip the model’s answers? The resulting performance in VQA, Visual Entailment, V&L Navigation suggests that CLIP is a viable alternative to the existing visual representations (e.g., ResNet based models pre trained on ImageNet). Thus, we propose to integrate CLIP’s visual encoder with previous V&L models.” The prompt engineering considered for VQA is not persuasive and does not imply the same straightforward introduction of CLIP based backbones to VLN / Image Captioning tasks.<|endoftext|>I think not many insights besides "switching the visual encoder to CLIP s bring the performance boost" are given. This paper examines how well CLIP s visual encoders are transferred on vision and language (VL) tasks. I must credit the paper for solidifying this tendency, but I am doubtful whether this paper opens any new directions to the community.
Reject; rating score: 6; rating score: 6; rating score: 8; rating score: 8; This paper proposes a branch and bound attack (BaB Attack) to solve hard instances efficiently, where none of the existing adversarial attacks can succeed. Experimental results show that BaB Attack outperforms the existing attacks in both attack success rates and efficiency. Strengths:  This paper utilizes several technologies to accelerate the branch and bound procedure on finding adversarial examples for deep neural networks. Compared to the baseline attack method (MIP attack), the proposed BaB attack achieves higher attack success rates within much less time. Compared to the gradient based attacks, which takes only a few seconds to generate adversarial example, the time cost of the BaB attack is not negligible. The BaB attack consists of several parts, including bean search guided by neural network verifiers, diving in branch and bound, and large neighborhood search. However, no ablation experiments were presented to verify the effect of each part. In the design of the BaB attack algorithm, many are heuristic and lack theoretical analysis. Overall, I think this paper is marginally below the acceptance threshold.<|endoftext|>Authors proposed a novel branch and bound attack, which searches adversarial examples in the activation space of binary variables in a mixed integer programming formulation. They also present several heuristics such as top down beam search, diving, and bottom up large neighborhood search guided by the adversarial candidates pool to efficiently obtain strong adversarial attacks better than standard MIP solvers and other SOTA methods. **Strengths:**1) New interesting way to approach adversarial attacks from a mixed integer programming perspective (although not fully clear why). 2) Presented several heuristics that improves practicality and speed of the adversarial attack search. Why integer programming and such an esoteric formulation for adversarial attack search? What are the benefits of this formulation and method compared to the standard gradient based approach? Since it is hard to capture the main point of the paper and the problem it is trying to solve, I tend to be on the rejection side.<|endoftext|>In this paper, the authors propose to solve the adversarial attack problem of ReLU neural network by using a branch and bound procedure, and search adversarial examples within the activation space corresponding to binary variables of a mixed integer programming formulation. The experiments showcase that such adversarial attack framework enables them to find adversarial examples for hard instances where existing adversarial attacks fail. This paper is neatly written, the intuition and heuristic method is easy to follow, and experiments are convincing. However, this paper is purely heuristic based where some theoretical results would be much appreciated, and experiments are not comprehensive, I feel like by keep working towards either one of these two aspects will highly improve the quality of this paper. Also, while the adversarial attack is a very important problem, I think the basic idea presented in this paper is not fundamentally novel and in that regard, the contribution of this paper seems a bit weak. 1.Throughout, "branch and bound" and "branch and bound" are both used. You should be consistent, and the first one is more commonly used. Also, its abbreviation is usually denoted as B&B. 5.Second paragraph, page 2: You should specify that $N$ is the number ReLU neurons here. 9.Page 3: "can be provable obtained" should be "... provably ..."10. 11."Adversarial common pattern" on page 6 should be highlighted in italic. 12.Second paragraph of page 7: "T percent of" doesn t make sense, it should be "100*T percent".<|endoftext|>This paper introduces an efficient way to apply a branch and bound procedure to solve the mixed integer programming formulation of the problem of creating adversarial examples. Strengths:  The paper has good novelty, combining techniques from several other papers into an interesting new technique  The proposed method is shown to provide a significant speed improvement over a vanilla MIP approach on the given tasksWeaknesses:  The experiments are only run on the adversarial examples which could not be solved by any other method, and only compared to the vanilla MIP approach. I would like to see some numbers comparing the proposed approach to some different methods for time and efficacy (the proposed method is slow so I would not expect to see this tried on a large dataset). Overall the paper is good, demonstrating a novel method to quickly produce adversarial attacks in hard cases. It could be improved with the inclusion of more extensive experiments.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 5; The paper proposes a centroid based approximation to evaluating the likelihood function of latent models   a rather incremental approach. This allows for faster evaluation of the likelihood function. Also, according to the authors, the LDM model (with and without fixed effects terms for each node), is able to achieve higher performance in different tasks (classification and link prediction) than other popular embedding methods (like DeepWalk or Node2Vec) using just 2 to 3 dimensions for the node embeddings. The main problem is that the final choice of the clustering procedure   which is the crux of this paper   is not clearly described. Moreover, the clustering approach itself, and the need for the clusters to be balanced, is one that I have objections based on reasons described before.<|endoftext|>The main strength of the paper is that it is faster/more scalable than the traditional LSM and that the method appears to perform quite well in experiment. The authors introduced several heuristics to approximate things/speed things up: a) approximating the non link term using blocks b) using a multiresolution KD tree c) minimizing with a Euclidean norm instead of the exact expression, which they propose an auxiliary function optimization procedure for. b) how does the resulting model after applying these changes relate to the original model? 2) are these changes to the LDM *necessary* and *unique*? For example, why the multiresolution KD tree? A lot of the decisions/choices made in this paper are not sufficiently motivated/justified. But I don t recall seeing such results in the literature. If that is the case, then where is the gain of SH LDM coming from?<|endoftext|>The focus of the paper is in suggesting a node embedding method that uses hierarchy to ensure scalability. In particular, here, __representing the edges as sampled from a Poisson distribution__ with parameter $\lambda_{ij}$:$$ \lambda_{ij}   \gamma_i + \gamma_j   || z_i  z_j||_2 $$Finding a solution in terms of $z_i$ s of this model   which is not new   is the object of this paper. __Component 3: Scalable division of the data in clusters__ The clusters are not known. This results in a reduction of  the total time and space complexity of the LDM to O(N logN)). The authors then proceed to validate their method using a set of experiments: 1. node classification 2. edge prediction 3. clustering and hierarchical structure recovered in the latent space. with reasonable performance on a subset of the dataset. The main take away here is that they use a Poisson regression model rather than logistic regression framework. The comparison table should list running times and complexities for all of the algorithms, instead of simply the performance.<|endoftext|>The paper studies the node level representation learning problem. It proposes SH LDM, which combines the embedding and hierarchical representations for scalable graph representation learning. The hierarchical structure in SH LDM reduces the time and space complexity of the LDM to linearithmic in terms of the number of nodes. The proposed model works well on link prediction and node classification with low embedding dimensions. It is novel to use LDM for hierarchical node structure representation. Motivations and intuitions should be provided. 5.Some important hierarchical embedding approaches are missing as baseline models in the experiments (e.g., HARP and MILE).
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 3; This paper tackles the challenging model of editing a potentially very large pre trained model in a way that is specific (local) yet comprehensive (general). The problem of editing models accurately with minimal side effects is an important one, and I believe this paper presents an elegant and effective method for doing so that will also be of great interest to the audience at ICLR 2022. There is no need for the original training dataset  There are no modifications to the original weights until the edit occurs. 2.Sec 5 "Comparison of Model Editors": It is not clear if the **fine tune** and **fine tune + KL** models operate on the same subset of parameters as MEND or on all the parameters of a Network. There is only one example of drawdown post MEND edit (Table 2 input 3b), and in this case it was not harmful. If so, this could be an additional impact point of the paper. The paper overall is well written, and this reviewer did not notice grammatical mistakes. The paper proposes an effective, novel, and general technique for manually editing pre trained models.<|endoftext|>This paper studies the problem of "model editing"   altering the model predictions on local examples without affecting the global behavior. Authors tackle the challenge of editing very large models with billions of parameters, such as T5 or GPT J, where editing is particularly important, as it is impractical to re train such models from scratch to correct every mistake. As a result, methods such as Editable Training (referred as ENN) would only make practical sense if one needs to edit a (small) model on device, where re training is infeasible. In contrast, MEND could make model editing much more widespread since (1) MEND can be used for models that cannot be simply re trained (I was able to run MEND for a GPT2 like model with 41B parameters using a 4x A100 80GB pod with tensor parallelism; that model was trained on 128 GPUs for over a month) and (2) MEND can be applied after the fact without fine tuning the original model, meaning that anyone will be able to train and publish model editors for repositories of pre trained models, such as HuggingFace Transformers, torch.hub. ## Strengths  (as described above) the proposed technique has a great practical potential  the technical contributions made in this paper are solid and significant  the core method utilizes a clever trick (rank 1 gradients) that significantly reduces the complexity of model editing. Perhaps, refer them to an appendix where you show this to be the case in a simple MLP example. To be fair, MEND can t be held solely responsible for this since it is not the only model editing method. I deeply appreciate the practical contributions of this paper and believe that it can have significant impact on the applicability of the large pre trained models. That said, I do have several minor issues with the experiment design.<|endoftext|>This paper deals with the problem of enabling both developers and end users of such models to correct inaccurate outputs while leaving the model otherwise intact. If presented with only a single problematic input and new desired output, fine tuning approaches tend to overfit. To enable easy post hoc editing at scale, this paper proposes Model Editor Networks with Gradient Decomposition (MEND), a collection of small auxiliary editing networks that use a single desired input output pair to make fast, local edits to a pre trained model. This leverage results in an efficient lightweight model editor networks to produce edits to a pre trained model’s weights when provided with the standard fine tuning gradient of a given correction as input. In its current form, it is difficult to understand the exact loss function of the model editor networks. Further, there seems to be a lot of notations in the paper, some of which are redundant. The paper provides an efficient approach for correcting inaccurate outputs of large nlp models while leaving the model otherwise intact.<|endoftext|>A fast model editing method is proposed in this paper to edit pre trained models at scale. The paper claims that the MEND method has reliability, locality, generality. MEND is empirically validated on some curated datasets. The problem of editing a pre trained model is interesting. This technique is reasonable, but was originally proposed by Goodfellow et al., limiting the paper s novelty. In addition, it is strange to see reported results as "<0.001". 2.One claim of this paper is the ability to edit large scale models like T5. However, only "the MLP weight matrices in the last 2 transformer blocks" are edited, which makes the contribution an over claim. Since training with only one sentence may lead to severe over fitting, it is natural to fine tune only several blocks. So the baselines may not be plausible. 4.The significance of the problem is unclear. This way, the editing task can be accomplished. Did the authors try this approach? Since a model is edited, the locality cannot be guaranteed. The empirically results are not strong. A concern of fair comparison is raised.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 8; The authors might want to add some discussions about the connection to the notion of saddle point escape in the literature (e.g  Jin et al.2017).(f) After reading the paper, I am still not sure how the effect of the learning rate and momentum was "disentangled" in the analysis. I see some analysis about the behavior of SGD, ADAM, SGD+momentum at critical points. A variant of Adam is given in the end. The presentation and statements are confusing in my opinion. However, a detail derivation to support the claim and the proof of showing that the SDE is a valid one for Adam are missing in the paper. Indeed, it can be seen from Page 5, where the authors said they are analyzing an "idealized" Adam. The paper should provide a detailed derivation of showing that (8) is really about how the distribution evolves when the underlying dynamic is (7). (b) There are two approximations on (3). There are some descriptions below (3) but are not very clear. What is the definition of "flat minima"?<|endoftext|>The paper focuses on the understanding of the effects of adaptive learning rate and momentum. It also shows that momentum helps the training process by passing through saddle points and without affecting the minima selection.The paper also proposes a new adaptive algorithm, named Adai (Algorithm 2), which uses parameter wise adaptive intertia to accelerate the training and finds flat minima as well as SGD. However I find that the presentation of this work is not very clear and somehow confusing. In particular, the structure of the paper and the results presented in sections 2 and 3 are difficult to absorb. See my comments below. I understand the motivation of the authors and what they tried to communicate but i find that there is no satisfactory explanation of the results presented in sections 2 and 3. What are the mathematical expressions of these assumptions?<|endoftext|>This work analyzes the dynamics of momentum SGD and Adam on escaping saddle points and sharp minima, which is based on the diffusion theoretical framework proposed in (Xie et al.2021b).The authors prove that momentum provides a drift effect around saddle points and does not affect flat minima selection (for SGD), and while Adam escapes saddle points efficiently, it does not favor flat minima as well as SGD. This paper is generally well written. The diffusion theoretical analysis does provide some insight on the empirical performance of momentum SGD and Adam. The authors also put in efforts to conduct numerical verifications to their theoretical statements, which is highly appreciated. However, I think that this work does not completed  disentangle  the effects of adaptive learning rate and momentum since the work analyzes Adam, which fuses these two algorithmic components. The construction of Adai is interesting, and its effectiveness is justified by the empirical experiments. This work provides some new theoretical insights for momentum SGD and Adam, which are interesting and important. The authors then propose adaptive inertia based on the insights, which shows good performance.<|endoftext|>This paper disentangles the effects of adaptive learning rate and momentum in Adam learning dynamics, and proves that adaptive learning rate is good at escaping saddle points but not good at selecting ﬂat minima, while momentum helps escape saddle point and matters little to escaping sharp minima. Based on the analysis, the authors propose a novel optimizer, Adai. Compared to SGDM, Adai parameter wisely adapts the momentum hyperparameter to the (approximated) Hessians of saddle points, and is proved to fast escape saddle points and sharp minima. The theoretical analysis of the Adam optimizer is based on the SGD diffusion theory, and the results confirms and explains the observation that Adam can sometimes converge faster but generalize worse than SGDM. The proposed Adai optimizer is theoretically sound, and demonstrates slightly better generalization performance than SGDM (and significantly better than Adam) on image classification tasks. Despite estimating the moments in a similar way as Adam, the proposed Adai optimizer seems more akin to SGDM, with the only difference being its adaptive momentum; and it doesn t use adaptive learning rates, which is a main feature of Adam. This paper provides new insights into the performance of Adam, and proposes a novel optimizer that both converges fast and generalizes well. Further improvements can be made by comparing the proposed method to SGDM more thoroughly.
Accept (Poster); rating score: 8; rating score: 8; rating score: 5; This paper proposes and analyzes a generalization of the randomized SVD to incorporate any covariance matrix and to Hilbert Schmidt operators. Extensive numerical experiments further strengthen the case for this generalization.<|endoftext|>This paper studies the randomised SVD algorithm with non standard Gaussian vectors which is then used to approximate Hilbert Schmidt operators using a new kernel based on out products of weighted Jacobi polynomials.<|endoftext|>This paper proposes a generalization approach for the randomized SVD. The proposed approach uses a multivariate Gaussian distribution with a covariance matrix instead of the standard Gaussian random matrix in performing SVD. Real datasets should be used in the experiments. Besides, this paper is well structured, and the theoretical background of the proposed approach is well described in the paper.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; However, as far as I can tell, $e_t$ is not referenced after its definition, so I do not know how it is used in the full algorithm. The final stated contribution of this paper is a study of an important initialization method called T Fixup. I did my best to keep pedantic issues off of that list and focus on areas where I genuinely found clarity issues.<|endoftext|>Weaknesses:1) Re the transformer use: TrMRL proposes to use transformers exactly as if they were RNNs in RL^2. I think that it is very unlikely that TRPO based MAML or PPO based RL^2 (both are on policy) can outperform that dramatically SAC based PEARL (off policy). While that paper studies an important direction in RL of adopting new network architectures, such as transformers, the paper, unfortunately, brings very little novelty or insight. The proposed method largely builds off of the existing work (such as RL^2) and essentially proposes to replace RNNs with transformers. Moreover, the experimental methodology is a suspect as it doesn t provide an extensive ablation or support all the claims.<|endoftext|>There are few typos here and there but nothing that affects the readability of the paper. As RL^2, it builds the policy from the episodic memory generated by the transformer, instead of RNN. (2) The experimental results are not convincing. If I do not misunderstand it, I think N is the sequence length (correct me if it is wrong). (2)	N 2 for all experiments? However, a few major concerns are as follows. Is it possible to try different N and compare with PEARL? Overall, it is a good paper, but not good enough for publication.<|endoftext|>Unfortunately, several state of the art approaches are missing in the evaluation part. The approach is evaluated on Metaworld in various task transfer settings. Weaknesses:* Several important baselines are missing like (https://proceedings.mlr.press/v139/dance21a.html and https://arxiv.org/abs/1801.01290 )* The novelty of the model remains quite limited compared to the SoA Transformer current utilization in sequential decision making* The experimental results don t seem significant in ML45 settings which is an important transfer setting of Meta World*  Maybe a connection to auto prompting for zero shot adaptation in controlled generation could have been interesting to develop (https://aclanthology.org/2020.emnlp main.346.pdf). The problem addressed in the paper is important and up to date.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; rating score: 5; Quantum neural networks have two parts: a quantum embedding circuit that takes in classical data and embeds it into a quantum state, and a variational quantum circuit that learns a quantum circuit for evolving the quantum state before measurement. The authors propose to encode the classical input data into the parameterized angle in the quantum embedding circuit using a tensor train network (instead of a dense neural network). For example, in [1], they have considered using ResNet for doing the encoding. It is not clear from this work that using the tensor train network is beneficial compared to the plethora of existing methods (such as ResNet). Furthermore, we can see that the prediction performance on MNIST is significantly worse than the state of the art results (> 99% accuracy). We consider the single qubit quantum circuit to act as an identity (so the quantum circuit is not doing anything). Together I think the main claim that "Our experiments on the MNIST dataset demonstrate the advantages of QTN for quantum embedding over other quantum embedding approaches." Hence, the main claim of this work is not justified.<|endoftext|>As it was already studied in the literature, one way to implement a machine learning classifier in quantum computers is to convert classical input data like images into quantum states that are fed to a Variational Quantum Circuit (VQC). However, current quantum computers have a small number of qbits, which requires to perform a dimensionally reduction of the input datasets as a preprocessing step. In this paper, the authors propose to use data compression based on the Tensor Train Network (TTN) model, which is a very well known compression technique. They compare TTN compression against a straightforward dense layer and the PCA based dimension reduction techniques. The paper includes experimental results on MNIST dataset by simulating quantum circuits with up to 8qbits for noisy and noiseless scenarios. Moreover, the selected TTN structure seems not to be optimal for image treatment. Also, it would be useful to report results with larger TT ranks too. For example, using a very simple Logistic Regression classifier can provide testing accuracy around 92%. In fact, the proposed embedding of input data was already used in many papers, see for example [Stoudenmire, 2016]. This paper shows one potential way to use quantum computers for machine learning.<|endoftext|>A two stage framework for quantum machine learning is presented, which uses a tensor train network (TTN) to reduce the dimensionality of input data, whose embedded form is fed to a variational quantum circuit (VQC) and measured to obtain an output. Several theoretical results about this framework are presented, and experimental results show the improved performance of this framework against a pair of similar baseline models. The experiments show an improvement in performance from using TTN for dimensionality reduction, but these results would be stronger if another dataset besides MNIST was used, and if less trivial baselines were used. There is very little new theoretical contributions. The tensor product encoding presented in theorem 1 has been extensively used before (since \[Stoudenmire and Schwab 2016\] at least), and theorems 2 and 3 are simple consequences of the universal approximation properties of dense shallow neural networks and tensor trains. The use of TTN as a dimensionality reduction method is poorly justified in the paper, and not compared against sufficient baselines in the experiments. In particular, given that the dimensionality reduction is intended to be fully classical, it would make sense to use less trivial classical models as experimental baselines (e.g.a neural net with more than one layer). The biggest strength of the paper is in its experimental results, but these consist of a few experiments on MNIST with a limited number of baseline models.<|endoftext|>This work proposes an end to end learning framework TTN VQC for quantum neural networks. The main problem is that current quantum computer cannot handle large number of qubits. The idea of this paper is applying QTT to perform dimensional reduction followed by quantum encoding TPE, which can be combined with existing quantum neural network (VQC) as an end to end learning model. This paper combines QTT and VQC into one framework. VQC is well known quantum machine learning model. TPE is a simple method to encode classical data as quantum state, which is also proposed by quantum inspired neural network (NeurIPS 2016). The experiment validation is done on MINIST dataset, however, the performance is far from the state of the art performance by DNN, which cannot show how useful of quantum neural network. But how about computational cost and convergence rate when comparing with dense network? Other experiments on larger datasets like CIFAR is expected, which is related to the scalability of the propose algorithm. But the paper has not presented any algorithm related study and discussion. Let us assume TTN with tt ranks are all 1, can we achieve same upper bound as dense layer? Typo in the line below eq.(4), the core tensor is X rather than W.   Eq.(6) and (9) are same. However, novelty is fair, and the experimental evaluation is less convincing.<|endoftext|>Summary:This paper is dedicated to designing an end to end learning framework for quantum neural networks. The authors design a novel quantum tensor network for dimension reduction and quantum embeddings generation, since it is quite related to the practical usage in real world applications where only a small number of qubits could be supported on available NISQ computers at this moment. The key contribution of this paper is to leverage a tensor train network (TTN) to replace the dense layer for dimension reduction, which enables an end to end training process fully conducted in a quantum computer. Pros:1.The paper tackles a real limitation for QNN by replacing the dense NN with TTN and enabling end to end training. Since the TTN is the key proposal of this paper, it should be explained more in the related works. For example, as an existence module, how it works in the convolutional neural network and recurrent neural networks. 3.The technical contribution seems limited. Since the authors do not touch the QNN and only use an existing module TTN to replace the previous dense layer for pre processing high dimensional input.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 6; This prints *(Hinton et al.,* 2006*). However, the majority of them should actually use the `\citep` command. # Post rebuttal updateI thank the authors for their response and improving the manuscript based on my suggestions. I therefore recommend that the authors continue improving the paper and submit it for another review process at one of the upcoming conferences. What does "outperform" even mean in this particular experiment and how is it measured?<|endoftext|>This is quite critical for general adoption of open ended learning in unsupervised learning. The paper considers leveraging open ended learning to try to learn a more diverse set of skills, since automatically generating challenging environments can enable new skills to be discovered. While the premise of generating new environments to enable more diverse skills to be learned is intriguing, the experiments do not convincingly demonstrate this. In the experiment section, the authors compare the performance of ROEL to that of DADs on held out environment variations, and argue that ROEL is superior.<|endoftext|>## Post rebuttal updateThank you for the response. The evaluation against DR is a step in the right direction. However, the evaluation still needs to be more mature (see my second point above) before I would be comfortable accepting this paper. I encourage you to keep working on it and submit to another conference.<|endoftext|>This is not evaluated, though I understand that the original version of POET is restricted to one environment. ## JustificationAlthough I point out a number of weaknesses with the evaluation, in particular, I believe the evaluation is an intrinsically difficult step for this type of workThe evaluation that is performed supports the claims of the paper, and the overall quality is still sufficient for acceptance.
Reject; rating score: 3; rating score: 5; rating score: 6; This paper presents a Wasserstein distributionally robust optimization (WDRO) framework for federated learning to hedge against statistical heterogeneity. The problem under investigation is important. The duality result (5) used by this paper was not new. One of my major concerns is in (6), where the authors did not solve \gamma to optimality, but instead considered it as a hyper parameter. Also, at the end of Section 2, the authors mentioned several related works exploring Wasserstein robustness in FL. It would be good to expand a bit on what is the difference between this work and those previous works, what makes your paper novel and what are your contributions. FedAvg itself is not a robust algorithm, and thus I do not think comparing with it gives any insights on the robustness of the proposed approach. Did you set it to n_i/n? Several theoretical results are already well known by the WDRO community, and the proposed algorithm is not very different from FedAvg.<|endoftext|>The paper proposes a Wasserstein based distributionally robust federated learning (WAFL) framework to address the statistical heterogeneity problem, which is also applicable to distributional shift setting and domain adaptation. However, a larger uncertainty ball may not be an advantage, it will also leads to over conservative decisions. Therefore, a crucial question for the authors is: for what types of application can WAFL outperform existing method? If so, is there any theoretical or empirical evidence? It provides a comprehensive study of the proposed framework. However, in my humble view, the motivation of the paper is not well supported, and the comparison against existing methods is insufficient. Therefore, I rate this paper as a marginal paper.<|endoftext|>The paper proposes a Wasserstein robust Distributional optimization scheme to provide robust approach for empirical risk minimization. While the ideas implemented are not surprising, appearing previously in Wasserstein Adversarial learning algorithms, the authors do a decent job of explicating the details of the method. The SGD algorithm via usage of duality properties is standard and the theoretical justification is solid. Overall, I would say this work is a reasonable contribution. 3.Please provide the computational time comparison with relevant methods.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; rating score: 6; This paper attempts to deploy the boosting technique in the supervised learning on policy learning in the RL setting. I was called to do an urgent review on this submission and I did not have time to check the mathematical details in full. It is difficult to catch the main idea of this paper, and it is dubious if the proposed algorithm can be implemented in practice. How does one know that a policy $\lambda$ that satisfies the definition 4 (shrub) exists? The meaning of notations is inconsistent from place to place.<|endoftext|>This paper studies boosting like RL algorithms using the most boosting techniques from online learning. It advocates the advantage of the proposed algorithm as the sample complexity s independence of the number of states. The idea of transferring the most recent online boosting technique to RL is interesting. If this paper aims to deal with the large state space problem in RL, then it misses some important related work in state compression techniques. Hazan and Singh 2021 also gives some experiments.<|endoftext|>The paper proposes boosting (using Frank Wolfe) weak RL learners to get a strong (in a concrete sense) policy. What norm is this? I like the inclusion of the proof sketch, but it should be even more detailed and should provide much more intuition. A big benefit of the proposed algorithm is that the final complexity bound does not depend on the number of states, which is a very interesting result. However, I have no idea how that came about.<|endoftext|>In this paper, the authors study boosting in RL, i.e., how to convert weak learners into effective policies. The authors provide an algorithm that improves the accuracy of the weak learners iteratively, and the sample complexity and running time do not explicitly depend on the number of states. Weighing the strength and weakness of the paper, I am recommending weak accept.<|endoftext|>This paper proposes a new approach for solving RL problems with sample complexity independent of the number of states. The proofs seem reasonable, but I did not carefully check. The sample complexity result is competitive and does not depend on the number of states, under the assumption of access to weak learners. When used in the theorem, is this meant to be $\Pi_W$?
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 5; The main idea is to use meta gradients through the first few steps of optimization to determine which weights to prune. 1) Since the current methods are insufficient to enable this optimization and lead to a large degradation in model performance, the authors proposed a new method to identify a fundamental limitation, namely that their saliency criteria look at a single step at the start of training without considering the trainability of the network. 2) The paper is well organized and easy to follow.<|endoftext|>This work proposed a new efficient pruning method, by leveraging both loss sensitivity ("saliency score") during a few initial training steps. The authors leveraged meta gradients with appropriate approximations to stabilize and speed up the pruning. Strength:The overall method is well explained and easy to follow. This is actually more GPU memory efficient since no extra computational graphs are stored.<|endoftext|>According to the proposed first order approximation, it seems that the proposed method is limited to apply with the optimization method SGD. Strengths:+ This paper is well written. 6.The related work of meta gradients misses a related work, that is, meta gradient in semi supervised learning [r1]. I looked into the introduction and the related work of pruning at initialization.<|endoftext|>This work focuses on weight pruning at initialization. E.g., "Many methods attempts...", "...the original, unpruned, model" and "Previous works that prune at the start have training have not been...". This paper proposes to use meta gradients through the first few steps of optimization to determine which weights to prune. [Strengths]  The proposed ProsPr is a simple and effective pruning method that prunes weights at initialization.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 8; This paper proposed a new version of deep Q learning, by avoiding the use of target networks in Bellman errors. Finally, the authors showed in experiments that the proposed method achieved better or similar performance, when compared with DQN and DDQN baselines. It was believed that the target network needs to be periodically updated for stability, so this paper brings a new perspective on the target network in DQNs. The performance gap indeed shows the promise of the proposed method. Given that Q_{\bar{\theta}} corresponds to a lagged version of Q_{\theta_t}, the assumption may not hold, since covariance functions are not necessarily the same and equal to the identity matrix. This statement is a bit problematic: Since rewards are randomly sampled from a replay buffer in DQNs, it s difficult to guarantee whether the used rewards can be newly encountered or not. 3.The experiments did show the advantages over DQN and DDQN baselines. However, it lacks the comparison against a very related work as follows, which also removes the need for target networks in DQNs. This paper brings a new perspective for the use of target networks in DQNs, and the experiments show some promise. However, the interpretation on why it works has some flaws, and a more rigorous analysis could make it more convincing.<|endoftext|>In Q learning, a target network is often used to define the Bellman loss. Empirically, the authors show that this can improve performance over 7 Atari tasks and the four rooms environment. The idea of using the target network for “functional regularization” is simple (in a good sense) and well motivated. The novelty is somewhat limited, but for empirical papers I think performance is more important than novelty. Given that the paper presents no theorems, I think the sections which mathematically compare the proposed methods to previous ones could be made a little smaller. ## Four roomsThe four rooms environment is a simple toy environment. It is reasonable to use such an environment as a sanity check to e.g.show that some theoretical property holds. However, the proposed method lacks theoretical grounding in the form of theorems or guarantees and thus relies on empirical testing to demonstrate any virtues. ## AtariThe first issue with the atari environment is the number of environments. The authors use 7 tasks considered by previous researchers and use 10 seeds for each of these. I think it would be better to use two seeds in each Atari environment (I think there are ~55). This is understandable, but what can be even more impactful is improving the performance of competitive methods. I would suggest that the authors consider ONE state of the art baseline that is more competitive and show that their methods improve it. For such methods, the authors would also not need any parameter tuning. The paper is well written and easy to follow. The authors consider the toy environment of four rooms, which I believe is too simple. The authors also consider 7 Atari environments, I think it would be better to consider all environments from the Atari suite. Most importantly, for the Atari environments, the baselines considered are very simple, and improving upon them does not show that the state of the art is being advanced.<|endoftext|>Given that in Q learning, the Bellman error (which we optimize) involves the target Q values with ever changing parameters, two major approaches have been proposed to address the instability: (1) have a copy of a slightly old version of Q (i.e., periodically update the parameters to estimate target Q); (2) use a moving average of parameters. Learning is slow in both cases. For example for (1), suppose the Markov chain has N states and we update the target network once every H steps, then it would take NH steps for a sparse reward (right most reward of the chain) to propagate to the beginning. Therefore, the authors propose deep Q learning with functional regularization (FR DQN). See Eq.(5).Essentially, they replace the target network Q’ by the current network Q (and adding stop grad); and they add the regularization term at the end, i.e., (Q   Q’)^2, so that the up to date parameter can be used in the Bellman error. The results in the four rooms task show good performance: compared to DQN and Polyak DQN, the proposed approach can recover the optimal value function quickly (given that the optimal value function in this task can be computed using tabular methods in this task). The proposed approach can match and sometimes get better than DQN / Polyak DQN on Atari tasks. Comments/concerns: More detailed analysis on how FR DQN performs on tasks with very sparse rewards would be interesting (perhaps also with a relatively large action space), given that this is one of the motivations. Will FR DQN actually perform better when there re sparse rewards? How does FR DQN compare to "increasing the update frequency of target nerworks"? More principled / automatic ways of adjusting kappa in Eq.(5) would be interesting. It s not clear to me why FR DQN needs such a "target update period" shown in Table 1 in the appendix. More experiments on complex tasks with relatively large action space, with sparse rewards will be great.<|endoftext|>The paper is theoretically justified and puts the scheme into the context of delayed target networks and polyak averaging (that both instead introduce a lag to the Q values estimated by the neural network). The paper provides experiments on some Atari benchmarks against a DQN and DDQN that use polyak averaging and delayed target network updates. I liked reading the paper as its motivation is clearly lined out from the beginning and the claims are also supported by a theoretical justification. Target networks often provide an easy solution to get an RL agent running but properly tuning their update frequency remains difficult in practice. This is also due to the fact that all their implications are largely still unknown. That being said, the authors could also highlight this a bit more. From an experimental point of view, the used environments are sufficient. Atari serves well as a benchmarking environment and comparing the approach to DQN and DDQN also seems sufficient (first I wondered why the other improvements are not being used but those really should not affect each other, so it is better to keep it as simple as possible, DDQN makes sense; also, I wondered how results in DDPG should look like – however, the results should also hold here). However, what I am missing a bit is an experimental benchmark against related approaches that have been proposed (e.g., Shao et al.(2020) and Kim et al.(2019) which have both been discussed as similar approaches in the paper). I don’t expect them to be much worse or much better either. Some minor comments: 	p4: In practice, deep Q learning methods are known to be unstable, [to] not approximate the true value function, and [to] sometimes even (soft )diverge (Van Hasselt, 2010; Van Hasselt et al., 2016; 2018). Does it reach the plateau in the end (looks like it will but not sure) 	Sec.4.2.1: “Complex DQL methods currently hold state of the art results in this benchmark (Hessel et al., 2018). Fig 2, d) Success rate is quite high for different kappa (in [0.7, 1.0]), but still somewhat “sensible”. In the last part of Sec.6 Discussion they claim “[…] the performance does not dramatically change for small changes in kappa […].”; Also: It would have been nice to see convergence speed for different values of kappa (as opposed to only success rate) 	p4: Typo “sectionSection 3.2”The paper is easy to read and provides a significant contribution to a very widely known problem. The solution can easily be integrated in existing applications and frameworks and is hence of broad interest to the community.
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 8; The paper proposes to use the zero cost proxy in the operation selection in differentiable neural architecture search. By introducing the zero cost proxy, they first find the proxy could yield a better ranking compared to both Darts and Darts PT in the Nas Bench 201. They also experiment with different proxies proposed by previous work and find they all perform better than the Darts PT. In the Darts CNN and S1 S4 experiments, they select one of the best performed proxies and find it could yield better accuracy than other baselines. 2.The experiment results look promising and interesting. It could achieve better accuracy across several benchmarks and some spaces, which could be useful to the community. Cons:1.The paper s novelty is quite limited. 2.At the same time, all the zero cost proxies are proposed by other works and there is limited understanding of why some specific proxies are clearly better than others in the differentiable setting but not in the one shot setting. Although the paper has shown us the zero cost proxy has great power in operation search in the differentiable neural architecture search, the paper stills lack insight on analyzing how it works. Therefore, I rate it as a borderline paper.<|endoftext|>This work combines training free proxies and perturbation based subnetwork evaluation to achieve efficient NAS. The author also conducted comprehensive study of training free proxies under different training iterations. Strength:This work studied different training free proxies and leveraged perturbation based methods for NAS. If I understand correctly, the proxies listed in Table 2 are all from previous works. The authors did not claim any novelty in these training free proxies either. 2.The operation sorting method (Eq.5) is also similar to the perturbation method proposed by [1]. The main difference is that the authors adopt this perturbation at supernet s initialization. In addition to the novelty issue mentioned above, some results and description of this work also makes me a bit confusing. Why does the author want to conduct this experiment given the "zero cost" motivation in this work?<|endoftext|>In this paper, the authors introduce new training free proxies to the context of differentiable NAS, which can speed up the search process while improving accuracy. Further, the authors propose, evaluate and compare perturbation based zero cost operation scoring (Zero Cost PT) for differentiable NAS. The experiments show that the perturbation is more effective than discretization and propose Zero Cost PT. Extensive experiments empirically show that the proposed method outperforms the best available differentiable architecture search in terms of searching time and accuracy. This work only borrows it into differentiable architecture search, which is only a combination. Some annotations can be moved to the caption. Although there is a lack of theoretical contributions, a large number of experiments provide empirical contributions. It is difficult for me to put forward some new opinions about these large numbers of experiments. I give the above acceptance threshold, but reject is OK.<|endoftext|>Specifically, they formalize "operation scoring", and use zero cost proxies to score operations with the perturbation paradigm. This leads them to a new NAS method (Zero Cost PT) that outperforms existing methods. It is a natural idea to combine these two things, but also not trivial to do it well. Sections 3.1 and 3.2 are nicely done and make sense, and Fig 2 in particular is very interesting. But this is the smallest search space, and in particular, it is known that zero cost proxies do well on NAS Bench 201 and bad on larger search spaces (the authors mention this as well). NAS Bench 1shot1 is the best option because it is size 360k and can run one shot algorithms. TransNAS Bench 101 can also run one shot, and it is smaller but would add diversity. Finally, it might be possible to run some of the experiments using the 60k trained architectures from NAS Bench 301). Better ablation between the search and the validation stage. The authors did have an ablation, but it is only on NAS Bench 201 and does not cover search only ("validation only" is basically NASWOT). In order to have a better comparison between ZC PT and DARTS PT, the authors could run ZC PT once with no validation stage. The authors claim in Section 7 that they released their code in the supplementary material, but unfortunately there was no supplementary material submitted. If the authors provide the code during the rebuttal period, I will feel more positive about the paper (for example, anonymous github).
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; The paper proposes a new evaluation metric for scoring untrained, randomly initialized neural network architectures (zero shot NAS) in order to predict their accuracy/performance after training. The score is based on evaluating the gradient signs and is shown to outperform existing approaches on NAS benchmarks (NAS Bench 101, 201, NDS). Overall, I am quite intrigued by the method. So I do appreciate the very clean and insightful presentation of the results. ### Ways to improve my scoreI do hope the authors decide to engage during the rebuttal period.<|endoftext|>While existing gradient methods are based on heuristics, this work proposes a metric called Gradsign for model performance inference, which provides some theoretical guarantees and performs well in practise. The problem of automatically discovering efficient neural architectures is the foundation of work on AutoAI. Paper is clearly written and experimental results are convincing. The idea of Gradsign is novel and integrating it to existing methods seems to yield improved performance.<|endoftext|>This paper introduces GradSign, an accurate, simple, and flexible metric for model performance inference. Experimental results show that GradSign can generalize well to real world networks and outperform state of the art gradient based methods for model performance inference. The proposed metric is backed by theoretical insights. In the evaluation, the paper only compares GradSign with gradient based approaches. I would like to see a comparison against other sample based, learning based approaches as well. This paper introduces a simple yet effective way for model performance inference. It outperforms existing gradient based approaches in almost all experiments. I recommend accepting this paper.
Accept (Poster); rating score: 8; rating score: 8; rating score: 8; The authors propose Transformer Control Flow (TCF), a set of improvements to the Universal Transformer (Dehghani et al, ICLR 2019). The paper is very clearly written, and proposes an interesting solution to an important question. The tasks chosen are meaningful, and the experimental results suggest that the proposed architecture can solve the extrapolation problem. First, computational depth is a relative notion. It would be interesting to test this on TCF (and baselines). This means that the model can adjust to longer sequences by iterating for more "ponder time".<|endoftext|>Evaluation: Secondly, while not a strict requirement, there is no evaluation on language tasks / pseudo language tasks like SCAN   there is a length generalization benchmark within SCAN itself and it would be good to know how this method does on that. Analysis: In Figure 2 bottom, what does It is unclear what the y axis is. Overall, I enjoyed reading this work. The writing is clear and to the point, and the approach itself is very well motivated (see questions for more), and simple to implement without too many tunable hyperparameters.<|endoftext|>Does the performance of the proposed method degrade in that situation? 6) It is not clear if the considered assumptions are always necessarily and correct. The resulting method achieves near perfect accuracy on the considered benchmarks for length generalization, simple arithmetic tasks, and computational depth generalization.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 3; Overall my opinion is that the paper explores an interesting problem of learning latent domains present in data, when no explicit domain information is provided. I feel the experiments performed in the paper can be improved a bit. A gating function (based on sparsemax) is proposed to decide which of the K convolutional layers will be applied. To solve this problem, the paper proposes the data to be coming from 1 of K unknown (or latent) domains. 2.The paper claims to propose a new setting for learning from multiple domains, where domain labels are unknown. This is an important problem since labeling domains can be challenging and/or expensive in real world applications. The experimental setup used in the paper, as well as the baselines are not properly justified. is not properly motivated. While I was able to understand them by going back and forth between current paper and referred paper, it would be helpful if the authors clarify the notations.<|endoftext|>In this paper, the authors proposed latent domain learning for adaptation. Several visualizations also illustrate the effectiveness of the proposed approach. The proposed latent domain learning makes sense and technically sound to me. Is there any thorough methodology to pick a good value instead of using K from 2 to 5 or just 2 as experimented in the current draft? 2.For the results, it was averaged over 5 random initialization, what is the variance for each experiment? It is not sufficient enough to use the mean itself for comparison as there might be a very large variance that indicates the model is not robust. Based on the above analysis, I am leaning towards acceptance for now.<|endoftext|>The authors present the latent domain learning aproach to multi domain learning without domain annotations by introducing sparse latent adaptation (SLA) for learning sparse domain gates without domain labels. The paper is well written, easy to follow and the motivations and contributions are clear. How does it learn domain related gating without domain labels? The results are interesting and I feel the paper could benefit from a deeper analysis on what is being learned by the "domain" gates. If they are not domain specific is this really multi domain learning?. Overall a good paper that could be better with more detail.<|endoftext|>This paper introduces a new task called latent domain learning and proposes a baseline that extends an existing multi domain learning method. 2.Limited novelty:The proposed model LSA looks like an extension of the residual adapter (Rebuffi et al., 2018), dubbed RA in the paper. The use of gating variables could be counted as a contribution, but they are not a new idea as they have been widely used in other fields of machine learning. Also, I would note that some objections against RA in the paper look invalid and accordingly the contribution of this paper should be undervalued. 3.Experiments:Compared to RA, the improvement by the proposed method does not look sufficiently large. The new problem setting, latent domain learning, looks interesting and practically useful. However, the baseline model is limited in terms of novelty, its performance is not impressive (decent though), and the paper misleads readers about the important previous work (i.e., RA).
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; This paper proposes a novel approach for hierarchical reinforcement learning. (2) The two tested environments are similar, and do not support the generality of the proposed approach. As the title says, the authors claims that the use of evolution strategies is useful to train hierarchical policy.<|endoftext|>This paper proposes to apply evolution strategy to hierarchical reinforcement learning. It outperforms the baselines in one of the experiments. While the paper is clearly written, I have a few concerns and suggestions. In this paper, both are specified manually. 2) The result does not significantly outperforms the state of the art (HIRO). The paper claims that ES is more scalable.<|endoftext|>This is indeed a useful property in HRL, and one that should be exploited further. The main issue with the paper is that the degree of novelty seems limited, as it seems that in practice the proposed method is just Feudal RL optimized via ES instead of gradient based methods. Also evaluation is insufficient. :) <informal off>Weaknesses:  Evaluation is limited to just two environments.<|endoftext|>The paper presents the integration of evolution strategies with hierarchical reinforcement learning. Although the proposed methods are novel, it seems that the experimental results need to be improved. One of the main contributions of this paper is to show the effectiveness of ES in HRL problems. This paper is well organized and easy to follow.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 5; The authors verified the existence of highly sparse “winning tickets” in ASR task, and analyze its robustness to noise, transferable to other datasets, and supports with structured sparsity. The first investigation of lottery tickets hypothesis on automatic speech recognition. 2.Paper is well structured, well written, and easy to follow.<|endoftext|>The paper explores the lottery ticket hypothesis, i.e., the existence of highly sparse subnetworks that can be trained in isolation withoutsacrificing the performance of the full models within the context of ASR. This is a well written paper presenting several interesting results. The novelty might not be high, it s more like an application of the lottery ticket hypothesis to the ASR problem but the conclusions are interesting. The main weakness in the paper is that it seems the authors train and test the models (in the robustness study) with the same type of noise. It will be interesting to investigate the impact of this parameter as it seems it might be important.<|endoftext|>In this paper, authors propose to use lottery ticket hypothesis (LTH) for ASR model pruning. For example, it s better for you to provide "existence" tables on CommonVoice and TED LIUM as well, as Table 1. The model is first trained from scratch. The paper is well written with a clear and simple idea inherited from prior works. LTH does perform well on ASR and this is a very nice finding. The authors empirically show 1. 2.Compared to other pruning methods, LTH performs best. Though the experimental results are solid, the technical novelty of the paper is a bit thin.<|endoftext|>This paper investigates using the lottery tickets hypothesis (LTH) strategy for pruning neural network weights for speech recognition. The paper shows the effectiveness of the proposed method in the standard ASR tasks, pre training with other models, and transfer learning, especially in noisy speech recognition tasks. Section 2 "On the model level, compared to CV models, speech models are mostly based on RNN backbones": I m confused about this description. extensions for transfer learning scenariosweaknesses:  the novelty of the method itself is incremental from the original LTH and related work. The application is only ASR and it may not attract so many AI/ML researchers.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; rating score: 6; This paper proposes efficient packing methods for training sequences of BERT, such that the 50% of the padding tokens in the Wikipedia dataset is avoided to speed up the training. So this can be a useful trick but the contribution might be not significant enough to publish at ICLR. 2.The novelty of this paper is limited.<|endoftext|>This paper profiles the training data of BERT and finds out opportunities to reduce padding thus saving computation. By packing multiple sequences into one fixed length sequence (two packing algorithms were used), the author shows that a BERT training speed up of 2x can be achieved without loss of quality. Technical quality: The overall technical quality is limited because of the limitation mentioned in "significance" and "novelty". marginally below the acceptance threshold<|endoftext|>The paper proposed to pack sequences instead of padding to reach the max sequence length for each sample for BERT pretraining. On the Wikipedia dataset, the proposed method can achieve 2x speedup while achieving similar training loss. However, I have the following concerns and comments. However, since the sequences are not i.i.d.in the packing algorithm, I am wondering if the generalization performance on downstream tasks (such as GLUE) will be affected. Minor:1.`s_m` in Sec.3 is not clearly defined. Overall, the paper proposed a simple and effective method for speeding up BERT training especially on the datasets with a large variance of sequence length.<|endoftext|>This paper proposes two packing algorithms for bert pretraining, shortest pack first histogram packing (SPFHP) and non negative least squares histogram packing (NNLSHP). So, the findings of specific datasets are quite limited and hard to say it is a general finding. Such as sorting by length and then perform the training with as fewer packing symbols as possible in minibatch. 5.Also wonder if the packing algorithms can be applied to ASR tasks such as packing audios as well.<|endoftext|>The paper proposes a simple way to pack sentences together for efficient training. Strongness:(1) The method is easy to understand and easy to incorporate into the existing training pipeline. (2) Do you have a comparison of the two proposed packing algorithms?
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 3; rating score: 5; The key difference from previous methods is that this paper proposes a region conditioned transformation. The statement "different regions of a 3D shape vary in their geometric structures which makes it more sense that we have a region conditioned transformation instead of the shape conditioned one" is too vague to support this motivation. However, as the method described in Section 2.3, it just adopts k MLP functions on the global features to obtain k transformations. However, in Section 2.2, the authors suddenly used Pi, which I think the authors may refer to Si or Gi. This paper presents a region aware deep learning framework for the point cloud registration task.<|endoftext|>This paper studies point cloud registration with deep neural networks. The key insight is learning a region based transformation is more robust than learning a per shape transformation even though shapes are rigid. The proposed method follows a DCP pipeline except that it performs point cloud reconstruction as well as self supervised region segmentation. Next, a per region transformation is predicted based on the features of points in each region. The paper is not well organized and presented. It seems this paper proposes a pure self supervised region prediction module. 6.This paper only conducts comparisons to the original DCP and PRNet, which were presented two years ago.<|endoftext|>This paper proposes a method for the registration of 3D point clouds using a trainable cascade of MLPs. The pipeline first computes per point features, then proposes a transformation based on those features. The impact of some building blocks is validated in ablation studies. Does the method use some decomposition of the ModelNet40 models? Usually, one would sample two different sets of N points from the original shape, then transform one of those, to simulate the fact that two scans are typically independent samplings of a common shape.<|endoftext|>This paper addresses the problem of point cloud registration, namely, given two point clouds with the same rigid shape, they aim to determine the relative pose (R and T) between them. Why the comparison algorithms in Tab.2 is less than that in Tab.3? (5) In Sec.3.5, the authors emphasize that they do not use SVD as a fine tuning step and thus they achieve inferior results on translation estimation. What if the SVD is also applied to the proposed method? The paper is not well organized.<|endoftext|>This paper addresses the problem of global rigid registration which is a fundemental problem in computer vision or graphics and they proposed a region aware decoder and fuse the transformation predicted for all those regions. But I hope the authors could address my questions, I want to make sure this is no misunderstanding. Weakness: The writing is rather unclear and the paper is difficult to follow. Strengths: It seems they have got a big improvement over the DeepGMR and DCP method.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; The authors should give honor to the authors who first invented the model. The authors use the ALS algorithm as the core algorithm for TN decomposition. This algorithm, especially for the CP and TC models, often gets stuck in false local minima and prevents the greedy method from finding a good model. I also suggest the authors test their method for tensors associated with the multiplication of two matrices. This does not confirm the greedy algorithm works well for other difficult scenarios.<|endoftext|>The practical values of the proposed method are not clearly shown. The algorithm consists of bi level optimization, where tensor network structure is optimized in the outer loop and tensor decomposition is computed to approximate a given tensor in the inner loop. [*] https://home.csulb.edu/~tebert/teaching/lectures/528/greedy/greedy.pdfThis paper tackles an important problem and provides a handy algorithm. In this paper, a greedy algorithm that can find a structure of a certain class of tensor networks is proposed.<|endoftext|>This paper proposes a greedy algorithm to solve the tensor network optimization problem in a heuristic manner. The paper also discusses the relationship among different tensor representation, and the authors have a deep understanding of tensor decomposition an optimization problems. 5.Is the proposed method subject to the influence of noise (AWGN)? The paper is technically interesting, but the authors should carefully revise their experimental section to address the above concerns.<|endoftext|>This paper proposed an adaptive method that enables to optimize the best tensor network structure which can be used for tensor completion, decomposition, etc. The performance of the proposed method was tested on both synthetic and real data. However, the procedures in Algorithm 1 actually may not support their claim.
Accept (Poster); rating score: 6; rating score: 6; rating score: 6; rating score: 5; This paper considers the fair PCA with distribution ally robust optimization algorithm. The problem is of great interest. The distributionally robust optimization and its reformulation is developed for parameter estimation. The strength of the paper is the rigorous optimization framework to solve the proposed fair PCA, with a nice reformulation and theoretical property. The weakness of this paper is in the following folds.<|endoftext|>This is optimized with subgradient descent on the Stiefel manifold, and numerical experiments comparing the proposed method to previous fair PCA are conducted. The problem formulation and re formulation steps are not trivial. There needs to be a discussion on the assumptions for Theorem 3.2. Experiments and discussions on the robustness of the proposed method are lacking, compared to the fairness aspect. This paper is lacking a conclusion section. It would have made it stronger if the authors mentioned how they would extend this work to non binary sensitive attributes.<|endoftext|>This paper studies the formulation, reformulation and algorithm for distributionally robust fairness aware PCA. The paper is nicely written and easy to follow. The motivation is clear and the problem formulation is novel. Although this formulation is new, it seems a natural extension of existing frameworks. The results can be strengthened by testing on datasets of larger dimensions.<|endoftext|>In the case when the desired robustness is specified by Gaussian distributions, the authors show how to adapt a manifold subgradient descent to solve a non convex reformulation of the proposed fair pca formulation. The experiments are also inconclusive since the paper focuses exclusively on the binary sensitive attribute setting, and with just one metric. Strength: + Robustness criterion for linear transformation of fairness can be formulated as a single optimization problem as long as the ambiguity set is defined using a single normal distribution. To me, the biggest technical question that the paper fails to answer is the generalization to nonbinary features.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 6; post rebuttal response   I have read the other reviewers  comments with the authors  feedback, but still think the proposed verification method (FJC discriminator) should not be directly compared with other optimization methods (not limited to MTL). They are not designed to find the global Pareto front for low dimensional problems. + The call for new benchmarks for multi objective optimization other than multi task learning is valuable, also see a related work [1] (I am not involved in this paper). However, this discussion is in the appendix and not the main point of this work. Multi task problems are not multi objective. Since HNPF depends on random sampling, it is not surprising that it can only work for small scale problems. An efficient nondominated sorting algorithm for large number of fronts. 2: 182 197, 2002.<|endoftext|>This paper intends to find points on the Pareto frontier of a multiobjective optimization problem. Despite other baseline methods for this problem, they claim that their proposed algorithm is suitable for non convex optimization, and their solutions are evenly spread across the frontier. Learning the pareto front with hypernetworks. This claim is not true. The distinction between the contribution of this paper and other proposals should be clear, and the contribution of the literature should addressed fairly and clearly. Multi task learning with user preferences: Gradient descent with controlled ascent in pareto optimization.<|endoftext|>The authors claim the following contributions:  New strategy for weak Pareto front identification based on Fritz John conditions. My main concern about this work is its scalability. At first phase of finding the weak pareto front HNPF needs to sample the solution space in uniform manner. Covering the solution space for real life solution is not feasible (in terms of memory and runtime) as such models contain hundreds of millions of parameters. In my perspective the novelty of this section is limited, it can be applied to any MTL/MOO approach. The experiment part is a bit weak.<|endoftext|>This paper has two contributions. Second, it presents a two stage method for finding the Pareto front. Multi objective optimization as a field may be growing within ML, but I feel many work are not evaluated in a way that is conducive to a holistic understanding of the methods capabilities and limitations. The proposed two stage method is reasonable, and shows good performance on the proposed benchmarks. I wonder if the neural net for weak Pareto front will be practical in applications where it is difficult to compute the value in objective space. Some discussion would be good. Pareto density (Table 2) is not the only way to evaluate, and it does penalize methods like MTL. [1] Zhang & Duh.
Reject; rating score: 3; rating score: 5; rating score: 5; This paper has presented CoRe, Contrastive Recurrent state space model, for model based robust model based reinforcement learning for robotic control. In addition to this, a policy is being learned with SAC. Experiments on distracting control suites and several robotic control tasks demonstrate the better robustness of CoRe. In addition, both CVRL and the proposed method use the policy loss from dreamer for learning the policy network. Beyond what has been discussed in the paper, CVRL has mathematically shown that by replacing the generative observation likelihood with a contrastive objective, we can lower bound the original ELBO. There are some other weaknesses, but I believe the issues discussed above are sufficient to make it a clear rejection. The paper is highly similar to a prior work as mentioned above, and as a result, the contribution of the paper is very limited.<|endoftext|>The paper shows a latent planning RL model  tolearn control policies directly from pixels. The main contribution of the work is the addition of recurrence to themodel and the extensive testing and explanation on the reason it tendsto work better. The authors could have easily try to mask their similarity with someother methods in the literature but they were upfront with that. Further, the appendix provides even more ablations, making veryclear that a recurrent state has a major impact on the general performance. The authors address specificallyone point from dreamer: the contrastive learning strategy, which failed to produce betterresults as reported by [1]. However, I do believe that this closer look into thisspecific point can be useful for the community and insightful in general. I wonder how the performance of this method would be in a more open task than mujocostyle robotic control. I think this is a very useful and well written paper. Even though the scope is small, the results are convincingand it shows a very clear way on how to effectively use contrastiverepresentation learning methods  while learning to control directly frompixels. ## After RebuttalAfter reading the other reviewers comments and the rebuttal I see that i missed some literature that needed further comparison.<|endoftext|>The paper proposes a recurrent state space model that learns robust representations for robotic control. Different from prior work such as Dreamer and SLAC which rely on pixel based observation reconstruction, this paper highlights that a simpler contrastive loss for the next observation prediction achieves better results **if** a recurrent state space model is used for the latent space. # Weaknesses## Novelty* The novelty of the approach isn t quite clear. I am concerned about the lack of clear novelty in the paper, and other experimental issues highlighted in "Weaknesses". Or is the novelty only in the observation that recurrent models are needed for contrastive loss to work well? I will update my rating based on the author s responses. This isn t clear to me. This is very similar to the proposed method and should be compared with.
Reject; rating score: 3; rating score: 5; rating score: 6; This paper studies "zero shot recommendation" where source and target domain have no overlap in terms of user and items. Experiments are conducted on two offline datasets. This paper studies "zero shot recommendation", where there is no overlap between the source domain and target domain in terms of user id and item ids. The main idea is to use item content features that can be generalizable, such as using BERT over item descriptions. Users are represented based on the items that were interacted by them. Strength  The paper is clearly written and the authors clearly communicates what has been doneWeakness  The major issue of this paper is novelty. The reviewer agrees that the "zero shot recommendation" is probably an interesting future direction, but the way that this problem is tackled in this paper makes it not different from known problem settings. The model is learned on source domain and directly applied to target domain. Many recommendation papers use such datasets so it is a standard data setting. This makes the reviewer wonder how valuable this setting is   if user interactions are already available in the target domain, why "zero shot" is needed? Overall, the reviewer recommends the authors to think carefully what "zero shot recommendation" is a novel meaningful setting and why the proposed methods in this paper has any meaningful novelties. Reply to rebuttal:The reviewer acknowledge the response but not convinced. The reviewer s point is, the proposed method makes assumptions that make the "new setting" work only under scenarios that is virtually the same as cold start or cross domain recommendation. The authors keep using claims such as "completely different", "extreme cold start"   but what would happen, for example, if the model learned on MIND dataset is applied to Amazon datasets? How would the owner of a new website decide which dataset to use? The authors also argue that "Previous initial phase recommenders can only collect low quality interactions" and "Initial phase recommenders take much longer time to collect data because inaccurate recommendations are not appealing to users." The reviewer finds some arguments and the incremental training experiments confusing. In fact, one can argue that interactions not from the UI do not possess many kinds of biases and are of higher quality. Noticeably, the proposed methods have constant performance over the time period (while other methods get better as more training data is available). How could the proposed method work well when there are no interactions (as its inference depends on user s past interactions)? 2) If the proposed method has first 4 weeks data for inference, then other methods should use them for training   it is just the proposed method can t train with them. So the reviewer feels the experiments are not fair under the current setting. Please let me know if I misunderstood anything in terms of the experimental setting. Though the paper is well written, the reviewer finds it difficult to believe the "zero shot recommendation" is a novel setting and the proposed methods have differ from content traditional content based recommender systems in meaningful ways.<|endoftext|>In this paper, the authors identify a new and interesting problem called zero shot recommendation, where there is no overlap of users or items between a source domain and a target domain. The main idea is to bridge two domains via the attributes of the item and the users, which is called continuous universal item ID and user ID. Weakness:1 The idea is a bit too straightforward, i.e., using the attributes of the items/users and their embeddings to bridge any two domains. 2 The technical contribution is limited, i.e., there is no significant technical contribution and extension based on a typical model for the cross domain recommendation setting. The authors conduct experiments and show that the proposed zero shot recommendation framework work well. However, my main concern is that the idea to bridge two non overlap domains via attributes is too straightforward, and the technical contribution is limited.<|endoftext|>A great recommender system relies on great training set. However, at the beginning, there is no such data availability. If not, how is user represented, aren’t all uses will get the same results if no item is recommended? The two challenges are generalize to unseen users and to unseen items. For unseens users, sequential recommendation represents users as a sequence of their interacted items. 5.The limitations in terms how close the source and target domain needs to be are not discussed in the paper. As for items, the unique ids are not useful. Overall, the problem being studied is interesting and challenging. This paper proposed an approach based on hierarchical Bayesian model. The approach proposed in this paper make sense. Please clarify more for the authors. There are some areas for improvement like baselines, literature review, lack of discussion about . The problem of zero shot recommendation is interesting yet very challenging to solve. 2.The proposed approach with universal item embedding combined with sequential recommendation methods for user embedding levering the items that user have interacted with is a novel and reasonable approach for solving this problem. Cons:1.The literature survey does not cover a lot of more recent approaches from sequential recommendations. Although the proposed approach is model agnostic, it would still be good a provide a thorough literature review. Proceedings of the 28th ACM international conference on information and knowledge management. 2020.2.It would be good to add some discussions in terms of how easy/hard the proposed approach can generalize to other modalities such as video, image, attributes.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 6; As far as I can see, this work is about practical methods for distributionally robust optimization (DRO) with a special focus on trying to overcome the limitations of previous methods that have been proposed for this kind of problem. The paper is relatively well written (some editorial feedback is given below). I am willing to reconsider my evaluation if my feedback is addressed adequately and provided that no other flaws are found. Say $\hat{\mathcal{L}}_\mathrm{DRO}(\theta)$, where the hat indicates the empirical version where the expectation is replaced with average over the finite sample? To indicate the ratio based (R) parametric (P) distributionally robust optimization (DRO). "In this paper, we propose a new approach for DRO, called RP DRO, based on a key modificationof the P DRO algorithm: instead of modeling the worst case distributions directly, we reparametrize the likelihood ratio between the training distribution and the worst case distribution.<|endoftext|>The paper proposes a modficiation to the DRO/PDRO method based on a parameterization of the various distributions that allows the problem to be recast in an optimization directly over likelihood ratios which belong to the exponential family. Minor issues:Some of your figures have gridlines and some do not. Parts of the method seem a bit heuristic but the paper acknowledges this and the choices are largely justified by previous literature and empirical validation.<|endoftext|>The study of label noise is interesting in that proposes a theory on why the nonparametric robust optimization methods don t work as well. This paper tackles an important problem of data set shift. Some things should have been evaluated better. It sets up an adversarial min max framework where the learner needs to minimize a loss function but under a max for perturbations of reweighting the training distribution.<|endoftext|>This work introduces R PDRO, a distributionally robust optimization framework based on parametric likelihood ratios. The empirical evaluation is thorough. The paper is well written and quite pleasant to read. Ideas are presented clearly. What happens when the training distribution $p$ is distributionally far from the worst case distribution $q$?
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; My main issue with the article is that they never clearly acknowledge that they work with "simple" PDs that have a very particular form, and that this makes the problem much easier to solve. The presentation of the paper is clear and the paper is well written.<|endoftext|>Overall, the writing in this paper flows well and the language is clear. The same comments apply, *mutatis mutandis*, to the discussion of the  brain network data set. Here are two examples of this (additional ones can be foundthroughout the text; since some of them boil down to personalpreference, I will refrain from listing them):   cycle structure is ubiquitous   >  cycles are ubiquitous    takes infinitely large amount of work   >  takes an infinitely large amount Overall, this is a very interesting paper with a very relevant andtimely topic (potentially high impact).<|endoftext|>In this article, the authors propose a way to characterize graphs using topology and persistent homology. The authors distance is only presented on graphs with same number of nodes. [Post rebuttal comment]: the rebuttal was nicely done, and I think that the paper is now good enough for publication.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; The paper tackles the problem of curriculum learning and presents a new technique (boosted CRL) that provides a tighter bound on the approximation error of the action value function than standard curriculum learning. The authors show both theoretically and empirically the effectiveness of the approach over standard CRL. The presented methodology learns a function approximation based on the sum of `residuals . * BCRL Framework: The BCRL framework was interesting (in particular the idea of boosting using residuals). From the main paper, it was not clear what was meant by the LSPI applied to LQR   some of the details from the appendix can be brought to the main paper. I was not entirely convinced by the reasoning provided (on unlearning of Q function when the task was switched to the target task). **Additional Comments**:Can the presented methodology be extended to account for the increasing task complexity in the curriculum and weigh the importance of each task gradually presented in learning the state action value?<|endoftext|>This paper proposed a curriculum value based reinforcement learning algorithm. Inspired by boosting methods, the algorithm learns on the Bellman residuals between the value function of the current task and that of the previous task. There should be more discussions on $\epsilon_i^0$, which is also some approximation using finite samples. The method is tested on four different problems and is compared with two 3 baseline methods. As far as I know, this is the first paper that applies boosting to the RL curriculum learning. The experiment is solid and results are impressive. Here are the detailed concerns. The authors should make it clear about this point. 7.It seems there are two versions of $Q_{t}^k$ in the paper, one generated from the optimal Bellman operator and the other one from the empirical one.<|endoftext|>The paper proposes a new curriculum for RL method. The basic idea is to "learn" a residual for each task, modeling in this way how the tasks differ. The proposed method is interesting and creative, and seems to be beneficial to the learning process in the empirical evaluation. Curriculum learning for reinforcement learning domains: A framework and survey. Post rebuttal The authors tried to address some of my concerns, in special adding some text in an appendix commenting on the distinction of their paper from TL.<|endoftext|>The authors propose to use the sum of residuals trained on each task for knowledge transfer and develop a novel curriculum RL method called Boosted Curriculum Reinforcement learning. The authors also conduct interesting empirical investigations on fleshing out the importance of boosting and the curriculum components of their algorithm. The proposed algorithm is novel and justified both theoretically and empirically. The paper proposes an interesting and novel algorithm for curriculum RL. The paper is overall well written. In the introduction, the authors state that the existing (value based) curriculum RL methods are constrained to use the same function approximator throughout all the tasks; this is because the trained Q value function of the current task in the sequence is used as the initialization for the next task. There were some clarity issues in the experiments.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; In this paper, the authors propose the Gaussian Differential Privacy Transformation to identify which algorithms fall under the framework of of GDP. There is no formal result showing the improvement in utility for a class of algorithms. It is largely unclear what is the motive of the paper and what is the application. Thus, I think it is not good enough for publication.<|endoftext|>This work is an investigation into the newly proposed Gaussian Differential privacy (GDP) in the context of existing formulations of epsilon, delta differential privacy (DP). I would encourage authors to expand their Section 5 and provide explicit applications of their work in this domain. As a result, it is very difficult to determine the actual novel contributions that the work contains. I would suggest trimming the technical details down to make the contributions of the paper more convincing. In addition, DP is known to be detrimental to the utility of the trained model, which a more carefully selected formulation (such as GDP) can partially mitigate. does not specify the condition of ‘differ’: is this an add/remove a record or is it a replace a record? [2]   Bu, Zhiqi, et al."Deep learning with Gaussian differential privacy."<|endoftext|>## Summary of ContributionsThis paper considers Gaussian differential privacy (GDP) notion which is an extension of $(\epsilon, \delta)$ differential privacy (DP). Overall, I think it d be better if you show an application of GDPT that would be much harder to derive under the privacy profile notion. Previously, it was known that the former implies the latter if $\epsilon  \geq \epsilon$ and $\delta  \geq \delta$. In Theorem 3.1 of this paper, the authors show that the implication may even hold when $\epsilon > \epsilon $ but $\delta $ also has to be larger than $\delta$ by a certain amount (depending on $\delta, \epsilon, \epsilon $). Specifically, almost all the applications can be derived via the latter as well. Although the paper claims to provide "engineering tools" for GDP, no evaluation is given. The "utility improvement" from the non trivial relationship between $(\epsilon, \delta)$ DP s does not seem that appealing.<|endoftext|>Because of this, I will be ok if the paper were accepted to ICLR, but I will not push for it. * Concentrated DP was first proposed by Dwork and Rothblum (https://arxiv.org/abs/1603.01887). The paper makes a contribution to automating the privacy analysis of algorithms within the elegant GDP framework, but doesn’t manage to make a strong case for the usefulness and applicability of the new tool it develops. The main strength of the paper is in giving one such tool, and showing how it can ease the privacy analysis of several algorithms.
Reject; rating score: 3; rating score: 3; rating score: 6; rating score: 6; See the second paragraph of [1]Related work: I think [1], [2], [3], [4] should be included in the related work section, as they all propose methods to improve the efficiency of neural ODEs. Training continuous normalizing flows might be one application where a diffeomorphism is required, and experiments on this task would be interesting. Are there more fine grained measures of time cost of different models? The experiments are not extensive enough. Perhaps it might be worth trying some of the tasks investigated in [4]?<|endoftext|>In order to better learn differential equations that produce discrete data, this paper proposes to learn a coordinate transform (i.e.the `diffeomorphism’ in the title) in addition to learning the vector field. On the other hand, the latter is interesting but needs more work. This base is not new, and the contribution is to add a coordinate transform layer. Now both the differmorphism and the base dynamics are learned, and given one differmorphism there is a base dynamics. In addition, I’d like to better understand how the new method scales with the dimension. As this paper proposes an improvement of NODE, comparisons with NODE based approaches on machine learning baselines would be very helpful.<|endoftext|>The paper has been well summarized by the authors in their abstract. In this work, the authors propose to learn an ODE of interest from data by viewing its dynamics as a vector field related to another base vector field via a diffeomorphism. The strengths of the paper are:  it follows a series of work in this direction of ODE estimation, which has been validated as an important direction for ML  it is mathematically sound  well writtenThe weaknesses are:  from the limited experiments presented it is not clear how applicable the method is for complex practical problems  existence of an diffeomorphism is a critical aspect of the paper, it is not clear if such a tractable mapping that can be estimated using using a NN can really exist  related to the above, the choice of the base ODE is another aspect that might be non trivialClearly, the paper adds to the existing literature and is interesting.<|endoftext|>This paper presents an interesting perspective that connects ordinary differential equations with diffeomorphisms. The authors propose to learn the base dynamics alongside with the corresponding diffeomorphism. It would be nice to see a visualization of forward trajectories over time. This is mentioned nowhere in the main paper. I wonder how much of the "load" is on the shoulders of the base ODE system. I particularly enjoyed the preliminaries section and how the concepts are explained in sufficient detail.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; rating score: 5; The paper analyses cell based search spaces for neural architecture search based on the NASBench301 dataset. The main finding are that a) only a subset of the operations actually contribute to the final performance and b) well performing architectures exhibit common patterns from the literature (e.g res net style motifs), questioning whether we actually find entirely new configurations with current search spaces. # Merits  The paper addresses arguably one of the main challenges in current neural architecture search, the search space design. Also, the proposed techniques to analyse operators and subgraphs seem sensible. # Concerns  The paper focuses mostly on the DARTS search space and NB201, which is a simplified version of the former. While I do not think that it effects the main findings, it would be great if the authors could comment on that in the paper. In the long run, I think the paper can have a significant impact and help us to define better search spaces.<|endoftext|>2.Constrianting cells to similar patterns yields better results: By limiting the search space to important operations and similar sub graphs, random sampling in NAS performs on par with most other NAS algorithms. Cons:  **Open questions are not discussed**: Despite many interesting findings, the authors left many questions in the end. What is a good search space? This show that problem exists in the DARTS search space, but it doesn t suggest that cell based search space suffer from the same problems. For example, the skip constraint does not impact the performance as much as DARTS. It is ideal if we can find SOTA model in the search space, but the main point of NAS search space is to provide a fair common ground to compare NAS methods. For example, one may find a model with better FLOPS or latency. The paper is trying to provide some insights on search space design but I would like to see more elaboration in this direction for this paper to be accepted.<|endoftext|>By replacing the current operations with just this subset and also enforcing the skip connection to be used as a residual connection, they are able to achieve accuracy very similar to the networks discovered by other SOTA algorithms. They also demonstrate that one does not have to search for reduce cell and that it can just use the same architecture as that of the normal cell. The findings are very relevant to the NAS community. 3.Although the authors did not mention this, reducing the number of operations search space alleviates the problem of the ranking of the architectures sampled from supernet  based on their weight sharing accuracy and the ranking of the same architectures when trained from scratch is not highly correlated. How much is the run time reduced by? In Figure 10(b), they did train the architectures, but 30 is very small to make a conclusive decision. Also, for figure 10(a), could you increase the networks to 100? This paper focuses on an important topic of understanding the search spaces better.<|endoftext|>The paper performs a detailed analysis of the DARTS search space commonly used for weight sharing neural architecture search. In other words, the constrained random search in this work is in fact heavily based on the prior knowledge discovered by search. As far as I can tell the scope of the paper is restricted to cell based NAS. While I agree with the authors that cell based search spaces are still the mainstream for weight sharing NAS, it would be more comprehensive to discuss the aforementioned works. The message is not groundbreaking in my option given several existing works with similar implications, but the analysis and results are interesting enough to be informative.<|endoftext|>Reducing the complexity of search spaces in NAS is a strong motivation. It is not at all clear how a result like this generalizes beyond CIFAR. [*This simplification reduces the search space (but still highly complex),*] Unclear parenthetical. 2.[*it is fair to state that such cell based spaces currently dominates.] [*Note that we may not obtain NB301 performance prediction on these architectures, as NB301 requires all 16 operations to be enabled with valid primitives. While the discoveries in the paper are interesting and may be useful for searching CIFAR, it is entirely unclear how they extend to more practical NAS settings, e.g.tasks beyond CIFAR and constraints beyond just the DARTS search space. Indeed the findings may not be relevant if they do not hold in other settings NAS may be applied to. I view answering these questions as critical and so lean against acceptance.
Reject; rating score: 3; rating score: 3; rating score: 8; rating score: 8; This paper proposes two methods for estimating smoothness of the prediction function learned by a deep network :  1/ an extension of Rahaman et al.to multiclass classification, by adding a sine of desired frequency to the target vector, then training on the new target vectors. What is cumulative in this measurement? In fig 2. if both images of the same class, why is ||y_lambda   y_0|| high at lambda 1 (i.e.exactly the other image)   How precisely do you see the smoothness in these plots? In some cases it is rather ambiguous e.g.in fig 6.c bottom which experiment in the range [0.1; 1.6] is the smoothest one? ### Empirical evaluation   The paper is lacking experimental details, such as choice of optimizer and hyperparameters. You can also take inspiration from the method in [Zhang et al 2021] who study the spectral bias in the context of epoch wise double descent, also using training examples, but by performing a full Fourier decomposition. I think that the methodologies for measuring smoothness can be improved, and the experiments should include experimental details as well as an ablation study of the role of different hyperparameters on the smoothness of the function obtained after training.<|endoftext|>The goal of this work is to extend the spectral bias into practical image recognition networks, i.e., beyond the fully connected nets and the NTK regime that it was previously studied. What is the ‘effective amount of interpolation’ mentioned in sec.3.2?One of the most popular networks (and the almost default baseline) is ResNet, so it is surprising that ResNet is not used. The paper focuses on a single experiment, i.e.image classification on CIFAR10, which makes it hard to appreciate whether it generalizes beyond this setting. One implicit assumption of the paper is that CIFAR10 shares the same high frequency statistics as the datasets used in practice. The sec.3.2 (which is probably the most critical in the method) could be improved in terms of writing.<|endoftext|>This work presents an empirical study of how different training aspects affect the spectral bias of neural networks in practice. They also propose to measure the variability of the loss landscape in the linear interpolation path between two images as proxy for spectral bias. The spectral bias of deep learning is mentioned very often as a justification of many practical tricks in the literature, but we are lacking a proper evaluation of this bias in practice. **Evaluation metrics**: My main concern with this work is the fact that the choice of metrics used to evaluate the spectral bias of a neural network are highly subjective and difficult to interpret. Although I believe the proposed research question is of great interest to the community, and I admire the effort made by the authors in studying the effect of many training factors in the variability of the spectral bias in CNNs; I think that the overreliance on qualitative metrics and lack of quantitative results makes most of the observations in this work inconclusive. 2.**Inconclusive results**: Another source of concern for me has been the fact that some of the presented analysis are rather speculative and not based on facts.<|endoftext|>This paper proposes a set of tools to probe spectral bias of deep neural networks, that is their tendency to learn low frequency, simpler functions earlier in training, whereas high frequencies are fit later. Moreover, they introduce a linear interpolation technique on validation data to probe the smoothness of the learned function along paths connecting natural images. Authors perform extensive experiments to demonstrate how the proposed tools can be used to investigate the spectral effect of training parameters such as model size and different forms of explicit and implicit regularization. Strengths:Even though similar methodology has been used before (e.g.in Rahaman et al.), the proposed label smoothing technique seems to be novel in being applicable to multi class classification. The effects of training parameters on spectral bias (or its proxy) is investigated from various different directions. The key conclusion seems to be that  an ideal function should include high enough frequencies to fit the data but avoid unnecessary high frequencies that can harm generalization , which is somewhat evident and it is not clear to me what consequences it has. On the other hand, the core novelty and significance of the findings are somewhat limited in my opinion, therefore I recommend marginal acceptance.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; This paper presented an instance confidence embedding model (ICE), which is a variational approximation of the instance dependent noise (IDN). Given the observation that most columns of the IDN transition matrix have only limited influence on the class posterior estimation, this paper uses a single scalar confidence parameter to model the noise and uses a simpler transformation to transfer clean class posterior to noisy class posterior. The suggested method captures instance specific noise information and thereby improves classification performance when compared to previous methods based on the class conditional noise (CCN) assumption. 3.Why do you need $K\times K$ parameters to model the transition relation for each instance X while the $p$ and $q$ you defined are both $\in \Delta^{K 1}$? 4.The previous works employed some assumptions or are based on domain specific knowledge but they fully model the IDN. In your method, the X is replaced by one postulated variable C learned by embedding an index parameter. 5.The authors claim that most columns of the IDN transition matrix have only limited influence on the class posterior estimation and the transition function should be argmax preserving function. Does it mean this method can only deal with label noise with a low noise rate? Is it the $i th$ element of $u$, then what is $u$? 8.Eq.(9) needs to be elaborated. Why can this method work? All the IDN methods the authors mentioned in the introduction part were not considered as baselines. 11.Some grammatical issues should be fixed. Overall, the proposed method is too heuristic without enough justification. The experiments are not sufficient.<|endoftext|>This paper introduces ICE, an instance confidence embedding approach to learn with a challenging noisy label setting: instance dependent noise (IDN). Experiments results on various image datasets and text data show the effectiveness of the proposed method with the presence of label noise. $\textbf{Strengths}$* **Novelty** This work provides a novel and interesting approach to using only a single scalar confidence parameter to approximate the IDN transition matrix. * **Extensive empirical validation** The authors test the effectiveness of ICE in multiple image classification datasets and text classification tasks. $\textbf{Weaknesses}$* **A single scalar V.S. Or in other words, could authors explain why this approximation captures necessary/useful noisy label information at the instance level? * **IDN based baselines**  Learning with instance dependent noisy labels has become a popular topic in recent years. And several approaches have been proposed to mitigate the impact of IDN. To further validate the effectiveness of ICE, it would be better if the authors could compare with some more IDN based methods, for example, [1] [5]. It would be better if the authors could test the performances on real world noisy benchmarks, especially some small scale benchmarks such as Animal 10N [6], Controlled Noisy Web Labels [7], or CIFAR 10N, CIFAR 100N [8]. [7] https://ai.googleblog.com/2020/08/understanding deep learning on.html. This is an overall novel and interesting paper. Besides, I am not fully convinced of the effectiveness of ICE by referring to the baseline selections. I am willing to increase my score if some of my above concerns are well addressed.<|endoftext|>This paper proposes ICE (Instance confident embedding) method for learning with IDN (instance dependent label noise). ICE approximates IDN by using a single scalar confident parameter for each sample based on the assumption that $P(Y|x)$ is close to a one hot vector. Authors conduct experiments on both image and text classification to verify the effectiveness  of ICE and perform gradient analysis for further understanding the method. This paper is well written and easy to follow. The motivation is strong and interesting. It is of great importance to approximate IDN with a simple method since IDN is generally hard to model. This paper proposes a simple way to approximate IDN by using single scalar confident parameter. $C$ in Equation (8) (9) should be dependent on the feature rather than the feature index. 2.Many methods have been proposed in the literature to deal with IDN (A1, A2, A3, A4, A5). However, in Table 1, the authors do not compare any of them. Besides, the performance gain of ICE is minor compared to GCE on CIFAR10 and CIFAR100. Authors are encouraged to perform a comparison in the experiments. Does the network begin to overfit label noise as training proceeds? ICML2021A4: Learning with feature dependent label noise: A progressive approach. ICLR2021This paper has a strong motivation. However, it seems that the proposed approach (ICE) does not strongly connect to IDN and the performance gain is minor compared to existing approaches. Authors should provide more explanations of why embedding on the index can approximate IDN and perform more comparisons.<|endoftext|>This paper studies instance dependent noise (IDN) problems. It proposes a single parameter (confidence) model for the noise generating process. The experimental results on various image and text classification tasks confirm the effectiveness of the proposed method. Overall this work has enough novel and significant contributions, but some issues with the experiments weaken the work. The single parameter noise model and the use of ICE to learn with IDN are novel to my best knowledge, though aspects of these ideas exist in other literature. Strengths:  The single parameter noise model proposed in this work is somewhat novel, and the idea is relatively simple (which is a good thing). This noise model is well motivated. The use of ICE to overcome the issue where $T(x)$ could vary significantly for two adjacent instances is interesting and seems beneficial. Lack of theoretical results. Variational approximation in this paper is only used to justify that the proposed single parameter noise model leads to a valid lower bound. In fact, one can use any noise model and still get a valid lower bound using the same justification. The paper does not provide any theoretical analysis of the gap. It looks like the proposed noise model does not include CCN as a special case. 2.In the experiments, it is not clear how that clean labels and noise are generated. I am kindly asking the authors to provide this information. Instead of using the original labels, they used the sampled labels as clean labels. In Table 1 of this paper, how was the overall noise rate controlled? Please also list the controlled noise rate for each data set. 3.This paper did not compare with the method in Zhang et al.(2021a) without any justification, even though the paper used Zhang et al.s setting for noise generation. In Zhang et al., no such assumption was made. A related question: how is this assumption enforced in the noise generation for the experiments?
Reject; rating score: 5; rating score: 5; rating score: 6; rating score: 6; This paper presents a novel method for ensemble learning. By introducing a anchor scheme and specialization loss , the base learner are forced to be specialized. The method is validated on both tabular and image datasets. I don t work in this area, so I may be not familiar with the related work. For the experiments, I have the following two questions:1. If for a fair comparison, the authors should provide comparisons of boosting and various ensemble based on the same base learner. Since I don t work in this area, I rate this paper as borderline reject, and would like to participate the discussion in rebuttal.<|endoftext|>This paper learns an ensemble of models where each of the base models has a specialization in some latent sub region. The use of specialized models on sub regions can be very interesting for shallow models but for complex models such as deep networks, it is not convincing that the models specialized in sub regions can bring any change. How the authors found their results different than predicting a sample using only the subregions models by maintaining a mapping for a test sample to a sub region model.<|endoftext|>This paper proposes a method to train models that specialize in subsets of the data distribution by weighting the training loss based on how close example embeddings are to a learned “anchor” embedding. Experiments: the paper mainly focuses on tabular data and small image datasets, with small models (the largest used is ResNet 32). It also proposes to improve ensembling by combining multiple “specialized” models.<|endoftext|>Motivated by this, this paper presents the specialization aware method to improve the ensemble learning. To derive the correlation between samples and anchors, it utilizes the transformer like attention mechanism for learning the weights of base models. This paper demonstrates its effectiveness on several image and tabular benchmarks. This paper presents a new perspective   specialization   to enhance the ensemble learning. By the proposed approach, the problems of existing strategies can be solved. Although the analysis perspective is new, but the technique used in this paper are current existing one. The core for distributing samples in the latent space is based on the attention machanism. But I am conerned about the novelty of the specific technique adopted in this paper as well as the insufficient experiments.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 5; To avoid expensive second order approximations, the authors propose to use the evolution strategies (ES) based method to approximate the response Jacobian. Pros:* The idea of using ES based method to approximate the response Jacobian matrix is interesting. There have been earlier works on using ES based methods for bilevel optimization (e.g.[1]) that share a similar idea of approximating the Jacobian of the response function. Moreover, STNs ([2], [3]) use the ES gradient approximation method to approximate the response function (or response Jacobian) in an amortized manner. "Hyperparameter optimization with approximate gradient." * The key concern about the paper is the lack of rigorous experimentation to show that the proposed method is superior to other bilevel optimization approaches. The paper majorly focuses on the hyper representation experiments and compares the proposed method to only three baselines (e.g.HOZOG, AID CG, AID FP). It is still unclear if the proposed algorithm is superior to the previous approaches. I believe that there should more in depth empirical analysis to make the proposed method more convincing. Does this mean that the proposed method approximates the response Jacobian more accurately than CG? * What are additional hyperparameters introduced by ESJ S? * In algorithm 1, it would be helpful to identify variables such as K, Q* In section 3.2, “Differently from …” → “Different from …”* I believe that some information in the Appendix is outdated.<|endoftext|>This paper studies the evolution strategies (ES) based method to estimate the Jacobian matrix used in bilevel optimization. The main idea is to use first order information to estimate the second order information in the Jacobian matrix. **Things I like**: The paper proposes an alternative approach to bypass the need of computing the costly second order information in bilevel optimization and provides theoretical guarantees for the proposed algorithms. **Things can be improved**:**Limited technical novelty**: While this paper uses ES in a more careful manner in the bilevel optimization, it should be noted that the idea of using ES mainly follows from [Gu et al 2021] and the convergence analysis of the bilevel part mainly follows from [Ji et al 2021]. Specifically, compared with [Gu et al 2021], the proposed algorithm only uses ES on Jacobian in (3) not the entire bilevel gradient. By carefully looking at Theorems 1 and 2, to guarantee convergence, the theory requires the number of queries Q and the batch size S to grow with $\epsilon^ 1$, which makes the theory less favorable to practitioners. Such high complexity seems to be hidden in the simulations by choosing small Q and S, which makes the comparison with baselines less convincing. However, most of experiments are carried out on simple problems with small networks. Have the authors tuned the parameters well? iii) ResNet12 is not big enough to justify the claim on scalability.<|endoftext|>Crucially, these two algorithms forego second order gradient computation completely, favoring instead an evolution strategies approach. While this approach has been proposed before in this context, the new algorithms are more efficient because they do not treat the whole problem as a black box, and leverage the structure of the hypergradient. This paper tackles a very relevant problem. Further, this doesn t seem compatible with the claim that all the 12M resnet12 parameters are learned in the experiment. 3.Another overclaim is that the proposed algorithms are  efficient . Only requiring gradient computation does not make an algorithm efficient, particularly when it requires NQ *full* gradient steps per outer step. A comprehensive cost analysis with AID and ITD which use second order information is necessary here, to see when N and Q become too big for this approach to yield gains. 4.The theoretical results, while interesting, are not properly put into context. So the results are only valid when this constant is smaller than epsilon, which should be made clear. Finally, there seems to be a discrepancy between the analysis on strongly convex objectives, and the experiments using non strongly convex objectives. At this stage I do not recommend it for publication at ICLR, but I encourage the authors to take this feedback to heart and improve the paper so that it can have all the more impact on the community. Interesting new methods to tackle a very relevant problem.<|endoftext|>This work mainly focuses on proposing a computational efficient for approximating the response Jacobian matrix in the hypergradient of bilevel optimization with only first order information of the loss functions. The paper is generally well written. I found that method is novel and interesting, but still, have the following concerns regarding the contribution and technical correctness of this work:1) The proposed ES algorithm is quite similar to the zeroth order methods. This again contradicts the intuition that single loop algorithms would be more efficient. 4) The required step sizes for the ESJ S are a little wired. I think the step sizes are either decreasing sequences or depending on $\epsilon$. 5) In section 4.3, why not consider MAML and ANIL as baselines? Overall, I believe that it is an interesting work, but the proposed is only applicable for solving a class of problems given a special structure of hypergradient. Even the authors justify the computational efficiency of the proposed algorithm numerically w.r.t.running time, the theoretical justification of the efficiency of the iteration and sample complexities should be further addressed.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; In summary, I think this is a well motivated and elegant proposal that has been shown to be effective in a variety of experimental settings. This work introduces a straightforward development for set representation learning in the meta learning context based on the intuition that the sets encountered in real world meta learning tasks tend to have common attributes, as illustrated in Figure 1. The proposed method is simple and does not require complicated architectural or training adaptations to improve the models to which it is applied.<|endoftext|>It would be better to clarify what the contribution of this paper is and what the major difference is in contrast to the existing literature of optimal transport for representation learning. 2) The proposed algorithm is well motivated, and is effective to be implemented for real world applications.<|endoftext|>Indeed, it seems to me that some ideas presented here can be found in previous papers on set representation learning that are not mentioned. [2] presents a similar method in the context of graph representation. Pros:  The paper is well motivated and mostly clear. In particular, the proposed framework seems to improve existing methods for set representations (DeepSets and Set Transformer). Cons:  I have concerns on the novelty of this work.<|endoftext|>**Significance**The work is significant, but novelty is a bit limited. **Limitations**  The paper lacks theoretical analysis for the proposed approach. Error bars are missing and not reported in the results. ‌‌ **‌Originality**The paper is original and proposes an Optimal Transport (OT) based algorithm for improving existing summary networks for learning from set structured data.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; Strengths:  I like the idea of controlling the non stationarity of the policies to guide learning in the MARL setting. Particularly, the introduction and the related work assumes significant background knowledge. Some questions for the authors. Some comments about the presentation. I am happy with the way you addressed my two main concerns and I would vote for accepting the paper.<|endoftext|>This paper studies non stationarity in MARL. It is suggested that the authors clearly present their algorithm in a compact way. Although the research topic of this paper is important, some parts of the paper should be further discussed and re organized before it s ready to get accepted.<|endoftext|>The main algorithm, MAMT, is not presented in the main body of the paper and I think it would be more instructive if the authors could move it from the appendix to the main body.<|endoftext|>This paper proposes MAMT to address the important non stationarity challenge in MARL, which is inherently resulted from the learning of other agents. After reading the authors  responses to my questions, I am open to raising my score. **Weaknesses and Concerns:**1.
Reject; rating score: 3; rating score: 5; rating score: 6; rating score: 8; The dynamics of the neural network are stored in a time evolving graph. The paper s primary contribution is that the suggested rolled graph model for convolutional layers improves time and space efficiency in comparison to the existing unrolled representation. The proposed method is intuitive and lacks solid theoretical support. To facilitate interpretation, node centralities are used to indicate the graph characteristic. To make the results more understandable, the centralities can also be put into a more interpretable classification model, such as logistic regression or classification tree. Although predicting NN performance from the network structure and dynamics is an intriguing topic, it is not one that this research proposes. Apart from the rolled graph representation of the convolutional layers, there is only a little theoretical enhancement.<|endoftext|>The paper considers the problem of predicting test performance of a neural network model by using statistics about the parameter weights in relation to the network structure during the first few epochs of training. They also propose a rolled graph representation that uses fewer nodes to represent a convolutional neural network, thereby making the training and prediction process more efficient. Experimental evaluation shows proposed method 	having similar predictive performance as unrolled representation, while being computationally faster. The motivation for the problem is not clear to me. One of the key claimed contributions of the paper is the rolled graph representation. However, it is not clear what is the difference between a rolled representation and an unrolled representation (i.e., section 3.1). It would greatly help if the authors could explain (and justify) the differences better. It seems bias parameters are not involved while summarizing the neural network state. The paper seeks to predict the accuracy of a new instance N_tst, but the accuracy of N_tst after how many rounds of training is not specified.<|endoftext|>This paper tries to model the learning dynamics of a (deep) neural network using graphs. In particular, the authors propose to represent the learning process as a time varying graph, which is then used to capture the structural changes through some simple graph statistics (such as weighted degree and eigenvector centrality), and eventually to predict the accuracy of the neural network (through some simple ML models such as SVMs or MLP). The proposed method seems (experimentally) efficient in terms of the number of epochs and prediction performance. While Fig.1 is very useful, the paper is missing a connection between Fig.1 and the graph construction (Figs.3,4).In particular, it would be useful to have a picture that shows the graph structure for a  simple  deep neural network, with only a few filters and layers, as well as the number of edges corresponding to each step. Fig.3 is good, but it does not give the complete picture to the reader. 2) The authors should explain a bit more the intuition behind these graph signatures. What do they reveal about the architecture? 3) The authors claim that the method can be used in interpretability related tasks. 4) How do you define the training set for a specific architecture? pg.4: section 3.2: "the introduced time evolving graph representations" . However, from my understanding, no time varying graph representation has been introduced till that point.<|endoftext|>The paper proposes a graph based approach to assess the performance of deep neural networks in vision tasks. The approach is essentially based on representing the first few epochs of SGD based training of the neural architecture seen as a graph (where nodes stand for neurons and edges stand for connections among neurons), followed by feature extraction, signature construction and finally a classifier (to predict the high or low level of final accuracy) or a regress or (to predict the exact finally accuracy score) layer. The approach is assessed on several experimental settings, though limited to the case of static networks (e.g., without recurrent/recursive connections). The results seem encouraging and interesting* Strenghts  The paper is rather well written, clear and coherent (I could only spot a typo “ changes of rolled graph graph between”,  with the repeated “graph” word)  The empirical analysis is rather extensive and the results seem to support the claims* Weaknesses  Lack of theoretical insights makes the contribution slightly less relevant and appealing  Lack of asymptotic computational (time and space) complexity analysis would definitely add some more value to the paper, especially when comparing the behavior wrt early stopping and the literature alternatives. The paper is overall convincing, even though lack of theoretical insights and of computational complexity analysis reduce a bit my final evaluation.
Reject; rating score: 3; rating score: 3; rating score: 5; rating score: 6; This paper proposes using Koopman analysis to study the dynamics of neural sequence models. The paper applies this idea to two applications: RNNs trained on IMDB reviews for sentiment classification, and RNN autoencoders trained on ECG traces used for arrhythmia classification. If this were the case, I would be very excited about this paper! This means that the authors are approximating the nonlinear dynamics with a _global_ linear operator.<|endoftext|>The paper applies the developed theory of the Koopman operator to analyzing neural sequential models empirically, on two tasks: ECG and sentiment analysis, and derives some insights on what the models are doing using the spectrum of the Koopman operator. Applies the Koopman operator framework to develop some insights into two sequence modeling problems. 2.It seems like it would be a better fit for an applied conference focusing on the problems in question (e.g., sentiment analysis). However, even then, I think it would be necessary to apply the Koopman analysis to several datasets and have more developed insights using the Koopman analysis.<|endoftext|>The paper proposes a framework for studying sequence neural models based on Koopman theory. Their approach doesn t seem to be novel enough. Their results on the sentiment analysis problem, and the ECG classification challenge provide simple yet precise descriptions of the underlying dynamics and behavior of the recurrent models.<|endoftext|>The authors propose a method for analyzing NN applied to temporal signals (e.g., dynamical systems). The proposed method relies on the Koopman operator, which is widely used for studying dynamical systems. The proposed method is evaluated on two tasks, namely: sentiment analysis and electrocardiogram (ECG) classification. The authors propose a new scheme for understanding the representations captured by sequence neural models. However, I believe that some of the descriptions (mainly in the results section) should be strengthened before the paper can be published.
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 6; This paper focuses on the problem of uncertainty calibration under distribution shift. By using a domain classifier in the distributional robust learning (DRL) framework, the authors estimate the density ratio between the source and target domain to achieve well calibrated predictions under domain shift. A regularized DRL framework is further proposed to promote smoothed model prediction and improve the calibration. Experiments on Office31, Office Home, and VisDA 2017 demonstrate the superiority of DRL over empirical risk minimization (ERM) and the temperature scaling method measured by expected calibration error (ECE), Brier Score, and reliability plots. + They further integrate their method as a plug in module in downstream applications such as unsupervised domain adaption and semi supervised learning, leading to significant improvements. There are mainly two technical parts of this paper: the density ratio estimation method and distributional robust learning (DRL). In NeurIPS, 2020A uncertainty calibration method under domain shift with a distributionally robust learning framework that show competitive performance on comprehensive experiments but lacks enough novelty.<|endoftext|>In the paper, the authors propose a framework for learning calibrated uncertainties under domain shift based on a distributionally robust learning (DRL) framework. As a plug in module, the proposed method benefits downstream tasks of unsupervised domain adaptation and semi supervised learning in experiments. And the authors proposed a novel method based on DRL and density ratio estimation. The proposed method can be seen as a plug in tool for existing methods. 2.The paper may overclaim in the abstract that the authors "consider the case where the source distribution differs significantly from the target distribution". Does the proposed method still work when the support of the source and target domain are (largely) different? I have some doubts about how well the density ratio is estimated in the proposed method. The discussions and ablation studies may not enough to show this point. The research question is important and useful but the writing of the paper may overclaim the usage of the proposed method.<|endoftext|>This paper proposes scaled uncertainty prediction in the context of unsupervised domain adaptation. arXiv 2020This paper derives a new scaling for predictive uncertainty in unsupervised domain adaptation. A secondary problem then is the calibration of predictive uncertainties in the target domain. Results show better calibration under similar (slightly better) predictive performance. This paper proposes a model of uncertainty proportional to the (log) of density ratios between domains. Strengths:* The proposed use of density ratios makes an attempt to quantify the domain shift. arXiv 2019[4] Roelofs, et al."Mitigating bias in calibration error estimation." * Uncertainty is evaluated using both ECE and Brier score. I don’t agree with this intuition. What would be a counter argument to this statement and how does that influence the results? * The human selection scores seem to be a major motivation of this paper, but the figure misses error bars. When the ratio estimates change from 2.2 to 2.6, what is the noise in these numbers? * The calibration and training of density ratios is not well explained. Densities are known to be hard to interpret in high dimensions [3]. * Section 2.2: This section has many connections to the research in GANs, where a similar density ratio is estimated [1,2]. Use of papers on arXiv is not part of the review procedure.<|endoftext|>The paper proposes an uncertainty ( density ratio) of source target prediction to adapt for the target domain. The methods also validated in the robust semi supervised tasks. The paper is lacking in some of these aspects: Clarifications: What is the $\sum$ in Eq 3How to present work different/related adversarial domain adaptation methods; this discussion will clarify the paper further. using the Bayes theorem, the Differentiable density ratio estimation can be treated as the discriminator s prediction. ( discussed in sec 2.3) Do source and target predictors share some parameters? In Eq 6and  7, how to obtain y for unsupervised domain adaptation case. Performance: The performance is not compared with recent state of the art methods in domain adaption[a][b][c][d][f] in real workd dataset such as VisDA. I suggest authors to compare their methods with these related works in terms of performance. Some more clarification about the method will increase the readability of the paper.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; This paper is well written, and the proposed method is novel. The derivation and solution of the quantization are novel. Empirical results show that the proposed method outperforms recent data free methods. This paper provides a brilliant way to directly use discrete optimization for quantization instead of conventional training with gradient backpropagation.<|endoftext|>In this paper, the authors propose a data free quantization algorithm of deep neural networks called SQuant. In order to jointly optimize these three objective functions, a constrained absolute sum of error (CASE) is studied and a progressive algorithm is used. Strength: The main strength of this paper is the introduction of multi scale approximation of Hessian matrix. Weakness: I think there are some flaws in the derivation of the optimization objective. Experiment results show that the multi scale optimization objective lead to good performance after quantization. This paper will be better if the authors can illustrate these steps well.<|endoftext|>This paper proposes a new data free quantization method that does not require back propagation nor fine tuning. Then, instead of MSE, the authors introduce CASE (constrained ASE) of weight perturbation. This paper is clearly written and well motivated. The experimental results are impressive in Section 4. The quality of the proposed method would depend on the validity of the assumptions to ignore activation distribution.<|endoftext|>In the manuscript, the authors propose SQuant, which is a data free quantization method that can apply post training quantization (PTQ) without any backpropagation. Specifically, SQuant is taking advantage of approximated Hessian information. Based on the assumptions and deductions in the paper, SQuant tries to optimize constrained absolute sum of error (CASE) instead of MSE. The experimental results are quite good.
Reject; rating score: 3; rating score: 5; rating score: 5; rating score: 6; The paper aims to improve training of deep neural networks applied to sets. They also propose a modification to DeepSet and SetTransformer models inspired from pre norm ResNet and pre norm Transformer. The main weaknesses are: lack of statistical analysis of the experimental evaluation and limited novelty of the pre norm changes. Limited novelty of the pre norm changes: pre norm ResNet and pre norm Transformer are very common, the proposed addition of pre norm block to DeepSet and SetTransformer does not seem significant. In object detection models which operate on sets of input features and predict sets have been gaining popularity since DETR [1], whereas in trajectory prediction VectorNet [2] is an example of a regression model with set inputs.<|endoftext|>Strengths+ I enjoyed the discussion on design decisions around normalization layers and agree that set norm is better than what was used in the Set Transformer paper because it discards less information. Weaknesses  None of the results have error bounds, so it s hard to tell whether the differences are statistically significant. Looking at the learning curves in Figure 1, the vanilla and ++ versions of some networks seem to have similar performance on a few tasks. The paper increases the number of layers for both architectures, and this is never addressed in the experiments. I think Table 5 should be moved to the main paper, to show that permutation invariant networks compare favorably to traditional approaches in this real world setting. This paper proposes interesting design choices for permutation invariant networks, but I cannot tell whether the empirical results are statistically significant. Additionally, the paper does not show the effect of their components on the original shallow networks.<|endoftext|>What are the advantages of these alternatives? However, I do have some concerns regarding the current version of the paper, detailed next. As in test time, some constant inferred from the batch statistics is used. However, I am not convinced that the claim is True as existing architectures do incorporate normalization layers. For regression tasks, the units of the measurement should be reported and explained as well. It also does not fully answer the second question as it does not evaluate on different depths of networks. To my understanding, the main implication from Figure 1 is that with 50 layers and without normalization the learning performs poorly, which is not novel. * Lack of novelty and the benefit of the set norm is unclear* Missing discussion on the relation to graph neural networks* The experiments do not test properly the effect of depth on learning Moreover, existing architectures already incorporate normalization layers. *Relation to graphs*The current paper focuses on sets. Note that there is extensive literature on the more general settings of deep permutation invariant networks, which is the case of graph neural networks. I would expect a more detailed discussion about this challenge in graphs versus sets. For example, see implementation details in the supplementary material of [3].<|endoftext|>The paper has four main contributions:1. introduces the _set norm_ normalization layer in neural network models for set data, as opposed to feature normalization (aka.batch norm) and layer normalization layers;2. provides intuition behind a "cleaner" implementation of residual connections;3. implements set norm and the cleaner residual connections in modified versions of existing Deep Sets and Set Transformers models;4. introduces a new dataset called Flow RBC with over 100,000 examples. ## Strengths**S1) Overall well written and clear**The motivation for developing set norm is clear, and the comparison to feature norm and layer norm is also clear. **S2) Interesting new dataset**Admittedly, as I have no medical background, I am not a good adjudicator on the "value" of the new Flow RBC dataset. ## Weaknesses**W1) What is special about an equivariant residual connection? **It is unclear to me how an "equivariant residual connection" differs from a regular residual connection. If so, this needs to be made clearer in the paper. The paper is well written and provides solid empirical justification for specific choices of normalization (set norm) and residual connection design ("clean path principle") for neural networks that operate on sets. Overall, a solid empirical paper, but I hesitate to say that it is truly "novel."
Reject; rating score: 5; rating score: 5; rating score: 5; rating score: 5; rating score: 8; The paper proposes to learn latent variable model with mixed discrete and continuous latent variables. Post rebuttal: The authors  feedback clarify some of my concerns. Some places are unclear and confusing. the paper emphasized that the work is fully unsupervised and proposes the type preserving data augmentation. Could authors further elaborate on this point? (a) Since more arms potentially means more encoder/decoder pairs (i.e., more parameters involved), its better to also include the model complexity comparison with the baselines. I lean towards reject and may reconsider my rating based on authors  feedbacks.<|endoftext|>This manuscript proposes cpl mixVAE, which is a novel VAE formulation that attempts to improve clustering models by adapting ideas from consensus clustering. The paper proposes a new model structure and a novel distance to help encourage improved model representations. The reviewers have address some of my concerns. I have issues with the experiments, justifications, and theory in this manuscript. First, my major issue with this manuscript is that the proposed approach is not truly an unsupervised method, as is claimed. This theoretical section needs an increased discussion about these issues.<|endoftext|>The main motivation for such a consensus constraint is, if I am not mistaken, that it avoids the mode collapseproblem. * In the same figure, it seems that the discrete compoments that the proposed method uncovers are in agreement with the hierarchical clustering of the cells. So what we have here are two unsupervised methods the results of which are in agreement. It also comes with a quite extensive, though not complete, set of experiments and analysis. * The evaluation in figure 4 where a given continuous latent variable varies with the discrete component makes intuitive sense. On the same time, other than saying that intuitively the results of clp mixVAE look good it is hard to make an additional comment, because this requiresquite some expertise on the biology side. The paper proposes a generative model based on GAN VAE wherethe main desiredata is to perturbe the instances while not alterning their latent categorical code, the non observed class.<|endoftext|>They make a particular assumption about the factorization of the encoder function q(S, C|X) q(S|X)q(C|X) and they also take a mixture of “A” experts that are made to agree with their discrete assignment C.The paper compares the proposed method with a few similar methods, using MNIST but also using a dataset for single cell RNA sequencing data (a domain with which I am not familiar). The same paper can be referenced twice, but at least it should be referenced in the introduction in conjunction to the other papers on VAEs that the authors wish to mention. Hard to say how novel the contribue is.<|endoftext|>The paper considers the problem of generative modeling for mixed discrete continuous data. To this end, it introduces a novel variant of coupled variational autoencoders. This consensus constraint is enforced on the grounds of the Aitchison geometry in the probability simplex, which avoids the mode collapse problem. Strengths: The idea is quiet novel and smart. The experimental results are convincing, as they include a number of benchmarks, including some challenging ones, comparison to some SOTA methods, and an ablation study that offers good insights into how and why the method works.
Accept (Poster); rating score: 8; rating score: 8; rating score: 6; rating score: 5; This paper proposes a transformer style architecture, Scene Transformer, for joint trajectory prediction of multiple agents (including the autonomous vehicle). Overall, I think this is a very thought provoking paper with many interesting novel ideas and impressive results. I recommend this paper to be accepted, but I hope the authors can clarify my questions in the final version of the paper. The result is quite impressive.<|endoftext|>The model was tested on two standard datasets, and achieves state of the art performance. The scene transformer has its flexibility to switch between different prediction tasks by using different masking strategies. Overall this is a strong paper with sufficient novelty in its model architecture in the context of motion prediction for autonomous driving. The experiment results on standard prediction tasks on popular benchmarks are convincing. Despite some minor issues in presentation, the paper is above the acceptance bar.<|endoftext|>The paper proposes a new Transformer based trajectory forecasting model that can predict multiple agents in a scene. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2.The problem statement is justified and relevant in autonomous driving. 3.Results are good. **Weaknesses:**While the problem statement is important, the technical novelty and the analysis is lacking. For instance, see [0]. There needs to be analysis that advances the field forward. ADE/FDE results for the scenarios that WOMD labels as challenging or interesting.<|endoftext|>Section 3.4: The referenced  Figure 7  appears to be missing. Sections 4, B: Results of the various ablations are very sensitive to the model instance s training objective (marginal, joint, multi task) and the trajectory lengths (t {3, 5, 8}). The manuscript "[combines] a scene centric approach, agent permutation equivariant model, and a sequence masking strategy...". Section 4: The manuscript states: "Although no quantitative benchmarks are available in the community for quantifying GCP predictions, we take these results as positive indication that the model is responding appropriately and save further exploration of counterfactual analysis for future work." In this work, planning related experiments are not shown: it is another conditional trajectory prediction approach with a transformer based architecture. See main review, above. More discussion is needed to understand why.
Reject; rating score: 5; rating score: 6; rating score: 6; rating score: 6; Experiments are conducted over datasets such as MNIST and results compared against the k means++ algorithm (a popular initialisation algorithm). This is something that the paper does not elaborate. The discussion seems to be lacking on this aspect in my opinion. The suggested use of the algorithm is as an initialization routine for the local search based algorithm for k median.<|endoftext|>The paper proposes a new initialization scheme for the k median problem on graph input (or general metric spaces) using metric embedding tree structure. The work complemented these theoretical findings with experiments and show that the proposed initialization imporves the performance of k median++ initialization. The paper did well in presenting the algorithm s framework and the ideas there are interesting. Right now the problem setting and the results seem a bit confusing to me. For example, the major comparison, k means/median++,  is never fully explained.<|endoftext|>A discussion of runtimes as well as comparison of runtimes in experiments would be useful  Regarding the results for k means   moving the k means results to the main paper, as well as a brief discussion and comparison to k medians would be useful. This paper introduces a new initialization scheme for the k medians clustering problem in the general metric space setting. The approximation guarantees improve the best known results in this case.<|endoftext|>An extension to $k$ means is given in the appendix. This paper considers initialization methods for $k$ median clustering in both the standard setting and the differentially private setting. An empirical study is done on a class of synthetic graphs as well as the MNIST dataset. The main strengths of this paper lie in the differentially private version of their algorithm. Here we get both theoretical and empirical improvements over prior work. In terms of weakness, first there is some misleading language with how the result for the standard setting is presented.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 3; This paper incorporates convolutional models into self attention to explicitly handle word to phrase correlation, paired with a sparse masking strategy to balance between word to word attention and word to phrase attention. The model achieves good performance on GLUE and RE tasks. In short, the authors propose convolutional model for self attention, which is lack of novelty.<|endoftext|>The manuscript presents a multi scale self attention method for NLP tasks. The aim is to better extract phrase  and word level features. The contribution of the manuscript is minor and the novelty of the proposed method is marginal. The information provided in the current shape is very shallow. The authors should compare the proposed method on more NLP benchmarks with a comparison with the SOTA methods.<|endoftext|>This paper introduces a new network architecture based on a transformer. The basic idea includes1) Extract phrase information using a convolution operator and compute the attention between phrases and words2) Learn to predict a mask for word word attention to turn off word phrase attention when word word attention is highEmpirical results show that the proposed method improves the baseline BERT model when being applied to the features extracted by BERT. Despite being motivated by the attention between phrase and word, the empirical and technical novelty is limited. It is unclear whether the proposed method is indeed better when it has the same number of parameters and operations as the baselines. 3.It is not clear whether modeling phrases explicitly is indeed helpful, especially in a deep neural network. To improve the paper, the authors should try to1.<|endoftext|>This work proposes a multi scale fusion self attention module to help extract phase level at different scales. The authors conduct experiments on the relation extraction and GLUE tasks to demonstrate its effectiveness. I think the novelty is limited. Using convolution operation with different kernel sizes to extract information of different scales is quite general, e.g., googlenet [1]. As claimed in the paper, the dynamic sparse module is proposed to realize the end that if a word chooses to pay attention to a phrase, it should reduce the attention to each word in the phrase.
Reject; rating score: 3; rating score: 3; rating score: 3; rating score: 3; While I value the high level goal of the paper and think it is important to explore and establish the necessity of sequential reasoning for system 2 tasks, I feel that in the current form of the paper it doesn’t explore it rigorously or extensively enough to corroborate its claims in a solid manner:* **Too narrow task**: The focus on very specific addition problems in a quite particular non intuitive input output format doesn’t support the generality of the claims about the necessity  of guiding models through intermediate supervision to make them capable of tackling algorithmic tasks. * **Motivation section**: I feel that the motivation section in its current form is not compelling and doesn’t answer key questions about the choice of tasks and the design of the experiments discussed above. I think it could be really interesting to show the necessity of intermediate supervision but I think that will be better demonstrated by finding a case where even if you add more layers, heads, or parameters to the model, it still fundamentally doesn’t manage to solve an algorithmic task, unless the intermediate supervision is given.<|endoftext|>Humans are clearly provided with such supervision through the course of formal eduction (e.g.by being a given a demonstration for how to solve a multi digit arithmetic problem), so this seems like a promising direction to pursue. All experiments are performed on the very simple and limited domain of binary addition. This seems like a good testbed for trying out some initial ideas and getting a handle on the problem, but it would be good to show that the method is also useful in richer domains, e.g.more complicated mathematical reasoning problems such as those in the dataset from [1] (notably, problems requiring a number of intermediate steps seemed to be particularly challenging for the models tested in that paper, so it is plausible to think that the presently proposed method might be more useful there). With experiments only performed on such a simple domain, it is very difficult to draw any general conclusions from these results. The first major difference is that, in the  weak supervision  condition, the model is expected to produce an answer in a single forward pass, whereas in the  strong supervision  condition, the model produces intermediate results which are then fed back into the model as input.<|endoftext|>Having atomic operations is just a necessary but not sufficient condition for building more complicated algorithms from them. Decomposing novel problems to the known atomic operations is a very hard, unsolved problem. However, showing that it works in a particular case does not prove the general statement. The claim of section 3, that “no matter how many atomic operations separate outputs from inputs…” is true only if we ignore the additional complexity of decomposing the problem and combining the intermediate results, or if this has a fixed complexity, which may or may not be the case. This makes it unclear whether the performance gain comes from the additional computation steps or additional supervision. Of course, there are other ways to achieve the same result, but the one described in the paper is incorrect and can result in trainability issues.<|endoftext|>Strengths * I found this work interesting and it is important that researchers explore the limits of what can be learned with current deep learning methods. Weaknesses * In its current state, the work is too shallow for a conference paper. Intermediate computations seem to help for binary addition, what about other computations? Is there any task where the model would fail even when having access to intermediate steps? Why not trying with more architectures?
Accept (Poster); rating score: 8; rating score: 6; rating score: 5; This paper proposes a method for unsupervised domain adaptation under conditional and label shift, one of the most challening versions of DA. At the core of this method is an optimal transport problem that seeks a mapping between source and target domains *and* a class proportion vector that minimize a total transportation cost between the domains. Based on this formulation, and a set of 4 sensible assumptions, the paper provides theoretical results in the form of unicity of the solution and classic generalization bound on the target domain risk. It would be useful to have it here (even if in the appendix). is there a way to easily verify which one leads to the 4 Assumptions being satisfied? If so, would be nice to empahsize that this is the main difference between this paper and Rakotomamonjy et al.(2020).Typos:* It seems that Asssumption 4 should read $p_T(Z| Y i)$ (not k). This paper makes a solid contribution towards the challening problem of unsupervised domain adaptation under label and conditional shift.<|endoftext|>In this paper, the authors unsupervised domain adaptation (UDA) where both the label conditional and marginal distributions are different between the source and target domains (GeTarS). The authors propose an approach, OSTAR, to align pretrained representations under GeTarS. (2)	The paper gives both the theoretical and empirical analyses for the proposed OSTAR. It is unclear how to guarantee the target discriminativity is preserved when learning $g$. (3)	Regarding eq (OT), please also give reference for “least action principle measured by monge transport cost”. (4)	According to eq (9), the constraints in eq (OT) should be $p_N^\phi(Z)   p_S(Z)$? (5)	I understand that each of assumption 1 4 is less restrictive than the corresponding one in the related work. However, proposition 1 requires $Z$ satisfying all the four assumptions. (9)	For table 1, it is better to show the average results for each dataset. Overall, the paper is well written and makes some contributions.<|endoftext|>The paper proposes an approach for Generalized Target Shift (GeTarS) where both conditional and label shift are present in the target domain. The paper is well motivated from the Generalized Target Shift setup and uses optimal transport to map the source distribution to the target. 2.The proposed approach is supported by theoretical analysis that minimizes the upper bound of the target risk. The main argument is the use of optimal transport for joint alignment and classification, and it is not clear why it requires information maximization if the joint alignment and classification are successful. But in the main empirical results in Tab. It will be more informative and convincing to also include the results of CAL alone to help fully understand the impact of optimal transport. The idea of using optimal transport to address the problem of generalized target shift is interesting, but the main argument of this paper, optimal transport and its theoretical analysis, is not sufficiently evaluated and is confounded by the additional component information maximization.
Accept (Poster); rating score: 6; rating score: 6; rating score: 5; rating score: 5; This paper presents a generalizable shape embedding layer for 3D deep learning. The experimental results have shown that the proposed method can provide a boost on various representations in different applications. All the experiments have witnessed a boost in performance when incorporating the proposed shape embedding, which indicates the effectiveness of the proposed approach. ### Weakness  Though the performance can be boosted using the proposed method, the increase of the quantitative measurement seems to be a bit marginal. The idea of introducing an embedding layer to the 3D shape analysis just as the NLP community is interesting and novel. The experimental results are positive and have shown the effectiveness of the proposed method in a number of applications.<|endoftext|>This paper proposes a method for learning a latent spatial embedding of points in space to a feature space by pre training on a pretext task (e.g., reconstruction). The authors demonstrate that utilizing these embeddings improves results of state of the art learning methods on point clouds, meshes, and voxel grids. It also might be helpful to include some error bars across different random seeds. I think moving towards "universal" shape representations is an interesting direction for 3D deep learning, and this paper makes a nice step in that vein.<|endoftext|>The key idea is to use a learnable multi channel 3D grid to embed local shapes in the input 3D object, similar to word embedding in NLP; hence, the method can be pre trained and applied for various downstream tasks. The paper claims that the proposed method is generalizable (i.e., could be used in different 3D representations, backbones and downstream tasks) and computation efficient shape embedding layer for 3D deep learning. Strengths:(1) This work has a good motivation of trying to unify various 3D representations: meshes, point clouds, and voxels. So, it seems to me that it is too strong to claim that the method is generalizable. From Table 3, the improvement of the proposed method over DGCNN is quite marginal. (4) I am also concerned about the effectiveness of the design. Overall, I am lukewarm for this paper and slightly more on the negative side.<|endoftext|>This submissions introduces RASF (Representation Agnostic Shape Fields), an embedding layer that encodes local geometry and can be used for 3D deep learning with different input domains: point clouds, meshes or voxels. Weaknesses:  Implementation details are unclear sometimes. The baselines used to add RASF on top might be outdated. Specially, those architectures using transformers (eg.https://arxiv.org/abs/2012.09688), which can learn both local and global geometry. I would like to see these comparisons before recommending this paper to be accepted. What would be the expected performance of a transfer learning task? This could be an interesting direction for future work,This paper presents a conceptually simple approach that seems to boost performance for most baseline models for 3D deep learning.
Accept (Poster); rating score: 8; rating score: 6; rating score: 6; rating score: 6; The authors propose, theoretically analyze and empirically evaluate a sparse additive huber additive model (SpHAM), which is an additive model where each component function in the model is a kernel function that is fitted by minimizing a huber huber loss function with a functional sparsity inducing penalty term. The experiments nicely demonstrated the utility of the SpHAM approach for heavy tailed, stationary data and the adaptive SpHAM for non stationary data. Finally, the model and fitting routines are demonstrated empirically on 2 synthetic and 1 real world time series. The SpHAM model is not that novel, but the analysis of the model appeared novel, correct and of interest to those working with real world data (e.g.financial data) that is non i.i.d., non stationary and heavy tailed for which "practical" theoretical findings are not that common.<|endoftext|>The authors proposed a robust Huber additive model for non stationary time series prediction. The proposed time series model is both robust and interpretable. Are they tight bounds in general? I see in the experiments only a Gaussian kernel (which is a stationary kernel) is demonstrated. The literature review is insufficient, and some SOTA methods, such as LSTM, GPR, etc., are missing. The section of experimental results can be improved.<|endoftext|>This ms tackles a timer series problem where each observation is the sum of p hidden functions and a noise (termed as innovation in ms). Traditionally the noise is Gaussian, but this ms makes the noise into heavy tailed distribution such as t distribution. Based on this new loss function, this ms sets up the sparse Huber additive model and derive the bounds of the learned hidden functions, under stationary processes. In case of non stationary processes, the ms introduces different weights to each time points, which can be used to calculate a discrepancy that reflects the non stationarity. The experiments are not comprehensive.<|endoftext|>Assumption 1 on boundedness of $|Y^t|$ is very strong and may not hold even for simple distributions such as Gaussian. Also, are the results sharp? The authors claim in the abstract that `"... we propose an adaptive sparse Huber additive model for robustforecasting and inference (e.g., Granger causal discovery) …".
Reject; rating score: 1; rating score: 3; rating score: 6; The paper proposes an ML based binary similarity detection technique that uses a skip gram to encode the instruction sequences and then uses a transformer to encode the whole code fragment. The paper evaluated in cross compilation and obfuscation settings. + The topic is important, especially for binary code analysis and reverse engineering. The paper claims that they are the first to propose block level binary similarity. The authors did not compare with any one of them. The modeling is also not novel. It is a combination of skip gram and transformers. Although the paper compared with many alternate model choices, they failed to compare with any baselines. More interestingly, can it work on unseen obfuscation given they are using denoising skip gram? The paper works on an important topic. Thus, I am not convinced with the advancement reported by the proposed technique.<|endoftext|>By using a new reconstruction loss with the new architecture, the paper achieves better fine tuning results on downstream binary code similarity detection tasks. +  C. Weaknesses  The novelty of this paper is not very exciting. Also, only a single subtask is given (i.e., binary similarity check), which can hardly prove the generalization of the model architecture over the assembly code. This paper adopts this idea into transformers and masked the entire instruction for pretraining. But it never appears anywhere else and also in Figure 2. What do you mean by `using an arbitrary function ? Is that a trainable matrix? Many details are missing, e.g., your training hyperparameters for baseline MLM. This is important because pretraining a transformer using assembly code can easily run out of GPU memory. According to your maximum sequence length ( which is 512 given in Sec.4), I don’t think this is enough for a moderate size assembly code. What is IDF? You should at least explain how TFIDF works. Regarding your experiment results, I don’t know why O2 O3 matching is easier than O0 O0 matching. The paper has some novelty but needs a lot of improvements.<|endoftext|>This paper proposes an enhanced transformer for dealing with the generation of assembly code. The authors shows the efficacy of GenTAL against many other work proposed in literature, showing that their solution is able to better grasp all the possible different level of optimization applied by the compiler (O1 to O3). Pros:+ domain is very interesting, as assembly language is very difficult to deal with+ good experimental section that considers many models from the literature+ creating and learning from different compiler optimization technique is a good idea, since compilers might be very aggressive on edit they apply on the codeCons:  Obfuscation is lacking  Obscure for non BERT experts  Writing can be improvedThe domain is very interesting, since assembly is very difficult to tame. The issues of the paper are expressed as follows:**Obfuscation is lacking**This scenario is the most complex one, and it is treated poorly inside the paper. While the optimization levels are treated and described, this is entirely skipped by the authors. Also, is this obfuscation applied at assembly or at the source code level? The paper that is quoted seems to point on the second one, but the author should add these details to the paper.
Reject; rating score: 10; rating score: 3; rating score: 3; rating score: 5; rating score: 6; In this paper the authors extend the Fourier features framework, which has recently had great success in 3D scene rendering, to non Euclidean spaces, including S2, SO(3) and S2xS2. This is a nice paper! The examples are compelling and give good intuition for why it helps. Overall an impressive piece of work, I expect others at ICLR will learn from it as well. A good theoretical approach with results to back it up.<|endoftext|>6.The authors did not mention anything regarding how to choose the scale of the basis, although they mentioned it as a limitation. The authors are not using kernel basis2. Proposition 3 is well known result as well. 5.The rendering application is section 5.3 is interesting and it is nice to visualize the results by replacing Euclidean encoding with spherical harmonics in NeRF.<|endoftext|>The paper generalizes the commonly used sinusoidal position encoding scheme (such as that in NerF input encoding) to inputs naturally residing in non Euclidean manifold. This is achieved by representing the input coordinates as  projections on  alternative sets of orthornormal bases instead of the trivial Euclidean coordinates. Their natural shifts on manifolds are rotation, and can be almost trivially obtained. Their Fourier series have been studied by previous work. In other words, these results are well known and not novel. For a more generic non Euclidean manifold it is not clear whether or how a `natural shift  can be defined/derived. Strength: 1.<|endoftext|>This paper generalizes the positional encoding with Fourier features to non Euclidean manifolds, which has the rotation invariance for feature extraction. 2.The link of the proposed model with the NTK is well described. Actually, there is no big evidence provided by the authors about how the theory and model design are connected with the NTK theory. The underlying method took degrees up to 10 in the paper. If its the first case, can the authors provide some justifications on it? The link with the NTK of the proposed method is not evident to readers and may not be the truth as the authors mentioned.<|endoftext|>Strengths:  For manifold data, it makes sense to use an encoding that adapts to the underlying manifold structure rather than the extrinsic Euclidean coordinates. The submission makes some contribution in this direction;  Performance improvement verified in the examples;  The paper discusses multiple application scenarios where the data can be encoded as spheres or their products. Weakness:  The title appears to suggest learning on general manifolds, but the actual discussion is focused on manifolds with a spherical structure. It is true that the Laplace Beltrami operator can be used to derive an orthonormal basis on Riemannian manifolds. Unfortunately, there is no further discussion for such cases beyond this general statement. There needs to be more clarification on the applicability of the method on arbitrary manifolds, as well as the computational cost.
Accept (Oral); rating score: 8; rating score: 6; rating score: 6; rating score: 5; The authors propose a class of generative models that includes missingness of features, and develop a discriminative learning algorithm that maximises the conditional (posterior) log likelihood of the training data approximately. The ab initio generative model class proposed by the authors for handling predictions with missing features is convincing. The first term is lower bounded by ELBO as in VAEs. The reason is, that this effective subset can be very small and cover a subset of simple models only. The experimental section first analyses the learning properties of the method in an ablation study. This seems to be sufficient for the considered tasks from the  UCI repository. The conceptual part, i.e.the model and the proposed learning approach are in my view concise and sufficiently novel.<|endoftext|>The author propose an interesting discriminative learning approach with generative modeling to solve the missing data modeling problem, by extending the traditional variational lower bound (ELBO), with a novel and stable upper bound that can be estimated without bias with Monte Carlo estimation.<|endoftext|>The issue it tries to solve, discriminative tasks with missing input features, has great impact for a wide range of practical machine learning problems in real life. As this objective is intractable, the paper builds a conditional evidence lower bound (CELBO) that can be unbiasedly approximated using Monte Carlo samples. **Weaknesses and Questions**1. 5.Size of the test datasets.<|endoftext|>The paper addresses the problem of predictive modelling with missing input features. The authors formulate the problem as a latent variable model, and in addition to the standard variational lower bound (ELBO) propose to use a variational upper bound based on CUBO (Dieng at al 2017) modified by an exponential divergence to solve the MC estimation of CUBO. This should be improved. Or is this not true? Or is it the $y$ that depends on $m$?