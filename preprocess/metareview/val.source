Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 6. <BRK>This paper studies the variance of stochastic gradient in SGD conditioned on the initialization point. It shows that the variance of stochastic gradient is a decreasing function of minibatch size for linear regression and deep linear network. 4.The experiments are not sufficient to support the claim. Can you provide connections between the main results and the future work? Clarifying this will be helpful to evaluate the impact of this paper.<BRK>This paper studied mini batch stochastic gradient descent (SGD) for linear regression and linear neural networks. The analysis show that the variance of the stochastic gradient estimator is a decreasing function of b (mini batch size). I think the main result of this paper is not interesting. There are some comments. It is prefer to validate the proposed theory on more difficult task.<BRK>The paper shows that the variance of the gradient has an inverse dependence on the batch size in linear networks, subject to the knowledge of the initial weights. I am happy to discuss this with the authors and other reviewers during the discussion period. It will be useful to know if their theorems help tighten the convergence rates of mini batch SGD in convex regression or improve generalization bounds for mini batch SGD. Without any such application, I believe the theorems are incomplete.<BRK>## Summary and contributionsThis paper tackles the problem of the impact of mini batch size on the variance of the gradients of SGD. This is not true, the result is only for two layers deep linear network. For linear models, the variance of the gradients conditionned on the initial point is decreasing with the batch size b. For a deep linear model, the result shows that the variance of gradient is a polynomial in 1/b with no constant term.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 7. <BRK>"Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding." Reasons for score: The techniques used in the paper are not novel, and the choices on how to apply multiple compression techniques need more justification. The presentation of the paper needs to be polished. Cons: 1.It is well known that compressing the model is easier during the fine tuning phase [1, 2]. I don’t think this should be a contribution to emphasize for the paper. 5.The results in the paper are okay, but compared to previous works in computer vision [3], it seems that the model size can be further compressed.<BRK>But I also agree with other reviewers that the result is not very surprising. As R4 mentioned, the proposed method depends on the a specific downstream task where the "small" "general" BERT can be further pruned. For a fair comparison to previous work, baselines that are applied to a specific fine tuning task need to be compared. This paper presents a new framework for creating small fine tuned pre trained language models. 3. a set of techniques to prune or approximate the transformer element. 2.The framework is very efficient by removing large components (e.g., layers, attention blocks, ffd layers) at first and small components (e.g., weight group) later. If the validation loss is used, the experiment results in Table 1 are not reliable. Can the model prune more weight at the same performance level?<BRK>The main strengths of this work are as follows:1) The techniques do not require training networks from scratch and can be applied directly during fine tuning. 4) This is a practical and useful approach that should be widely applicable along with many useful insights about optimizing transformer based systems. I appreciate that the experimental results are reported with averages across multiple runs! I don’t see any major weaknesses in the paper. Here are some areas that can be improved:1) The description of the pruning strategies was hard to follow and needed to be tightened up. 3) Also some level of ablation analysis on the strategies used will be helpful. Same goes for the set of pruning strategies. 4) What is the impact on the fine tuning time?<BRK>This paper presents a method for improving a fine turned Transformer in terms of a specific metric such as size, speed, or accuracy. Although the individual techniques employed to realize the whole pruning process are not particularly novel, the paper presents a well thought out approach to combine those and reports very promising experimental results. I think this is a nice contribution to the community, given that the computation cost is increasingly important in dealing with BERT like models. The entire process of pruning is a bit vague and hard to replicate. (Is Algorithm 1 the whole process?) I thought this should be W.Minor comments: p.5 less the  > less than the?
Reject. rating score: 2. rating score: 3. rating score: 3. rating score: 4. <BRK>Though this article shows how the machine learning model can be guided with prior knowledge in the real world application, it is hard to say that this paper is above the accept line as there are lots of rooms for improvement. And the proposed method, called  Monotonic Neural Network , was applied to the application of chiller plant optimization. And the fifth line(Although Some... ) is not a complete sentence. How many layers(k) are used and why?<BRK>The overarching aim of this paper has been well studied before; given one aims at incorporating physical constraints, one needs a solid understanding of the underlying physical system, hence how one adds such a constraint to the model requires deep investigations. Other than that, the paper does not offer a comprehensive explanation on what the novelty and actual contributions are, nor does it provide a solid experimental set up. Some sentences are not finished or finish with a full stop where it shouldn t. Please consider proof reading the manuscript carefully for correcting such typos.<BRK>Experiments show that (1) both the hard MNN and soft MM outperforms multilayer perceptron (MLP), and (2) hard MNN yields lower energy consumption than MLP. + Appendix A.1 does a great job of giving an introduction about cooling systems and chiller plants. Specifically, no experiments empirically compare performance in terms of different amounts of training data. The second experiment (Figure 5.2) is unclear in its purpose, comparison method, metric, and effectiveness. I appreciate the overview but also think there is room for improvement. What is the key difference in terms of the experiment setting compared with the previous experiment? What is PUE, and how to calculate it? PID is not explained throughout the paper. Why no results of soft MNN in this experiment? However, the current manuscript compares with MLP only.<BRK>The submission references [3], which enforces convexity (not monotonicity). However, no comparison between these ideas was tested, so it is unclear whether the difference from the old work is significant. This paper would be of interest to a small segment of engineers who are interested in optimizing chilling plants. Sill, Joseph, and Yaser S. Abu Mostafa. 146 155.2017.4.Gao, Jim."Machine learning applications for data center optimization."
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. rating score: 6. <BRK>This paper proposes a method to select informative latent variables for representation learning. Pro: The paper points out the problem that since the scale of the latent variable cannot be recovered, the scale cannot be used as a criteria for variable selection.<BRK>This paper considers the problem of disentangled representation learning with an existing normalizing flow based approach, called general incompressible flow networks (GIN). In the original approach, informative latent variables were separated from noise by considering the variances of learned latent variables and selecting ones with high variances. I am still of the opinion that the presented method should be put in a broader context (i.e., considering it in broader settings and/or for other methods) or be better analyzed theoretically. Additionally, the proposed approach of selecting informative latent variables can be considered more broadly in the context of other disentangled representation learning approaches.<BRK>## SummaryThe authors propose an alternative method for finding informative latent variables in a model called General Incompressible flow Networks (GIN). In general, a lot of the experiments seemed to be based on Sorrenson et al, but are not properly described in the paper.<BRK>This paper builds on top of the paper “Disentanglement by nonlinear ICA with General Incompressible Flow Networks (GIN)” (Sorrenson, 2020) and argues that that paper’s method of identifying informative latent variables was wrong and instead suggests that informative latent variables can be identified by thresholding their mutual information with the auxiliary variable of the conditional generative model. Without judging the content as reason, the exposition of the paper is overly difficult to understand. In particular, it would be important to mention that both data $\mathbf{x}$ and auxiliary $\mathbf{u}$ are observed. Moreover, this reviewer could not understand how eq.(5) in this paper, which uses the data processing quality with the joint of the latent, motivates using the mutual information of the individual latent variables to identify informative ones. The review score has consequently been updated from 4 to 6.<BRK>The claim seems to be that the same data could have a set of latent variables that would have high MI with u1, and another set that would have high MI with u2. In fact, in the synthetic data, your 8 noise variables are very idealistic.
Reject. rating score: 5. rating score: 6. rating score: 7. <BRK>The technical contribution is limited. On the positive side:  The paper proposes an interesting mechanism to train conditional generators from a single image. Some good image editing results are shown in the experiments. The generation of primitives also highly depends on the accuracy of semantic segmentation.<BRK>The authors proposed a novel conditional manipulation method based on a single image, which is new in this area. Additional explanation and experiments, such as ablation study, would make the paper convincing. Additionally, detailed descriptions how to obtain the primitives (edge and segmentation) for the input image would be required. 1.The editing effects of edge maps are not distinct from those of segmentation maps. I think that the qualitative results of edge modification are not sufficient to prove its effectiveness, compared to those of segmentation maps. Due to these concerns, I would keep my previous rating of “6.<BRK><Paper Summary>This work proposes a method to design conditional generative models based on a single image. In particular, while some recent models have enabled one to sample (unconditionally) images from a generative model learned from a single image (like SinGAN), this work explores a way of conditioning the generation on a primitive, which can be user specified. At the same time, clarity (of both writing and technical aspects) could be significantly improved. This is in contrast to other methods (DIP, SinGAN) that train completely on *a single image*. This distinction could be made clearer in the text. It is also not totally clear how these transformations are sampled (the $t(i,j)$ s).
Reject. rating score: 3. rating score: 4. rating score: 7. <BRK>The paper describes a  knowledge transfer technique based on  training a student network using annotation creating by  a teacher network. Do you use step 3 in the experiments? Most the the rest of the paper is devote do describe experiment details. Hence, there is not much novelty in the paper.<BRK>Also, note the comment on the breast lesion dataset only being a single dataset with different splits. Further, the experiments do not contribute any new insights about how to chose the best student network nor which transferral dataset to use even though the introduction refers to unsuitability of certain pretraining tasks for a different target task. Most importantly the paper seems to be lacking proper positioning within the space of knowledge distillation and student teacher training which leads to an unclear message about the novelty of the paper.<BRK>In this work the authors propose to transfer knowledge between teacher and student networks trained on separate datasets, and claim to overcome challenges in availability of data annotations for challenging semantic segmentation in medical imaging domain. One of the major concern in medical imaging domain is the black box nature of the DL algorithms used, authors should comment on how relying on this black box nature for knowledge transfer would effect the interpretability of these results.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. <BRK>Summary: The authors present a formalization of a simple biological network (the mushroom body of the fruit fly), that allows very efficient “biologically inspired” word embeddings. Although the results are sometimes inferior, they are overall comparable, and importantly achieved at significantly lower computational resources. The main contribution of this work is not this specific network formalization (which is nice), but rather demonstrating that formalizing biological networks can generate more efficient algorithms, that achieve results comparable to the complex algorithms used ubiquitously. I think it would also be useful to evaluate on more extrinsic tasks, although given the space constraints I understand this may be difficult. Finally, it would have been good to see more analysis of the embeddings, e.g.by doing error analysis in one or more of the evaluation methods. Recommendation:I vote for accepting this paper. 2.I think it would be helpful to make a comment that context in this paper is essentially Bag Of Words, as there is no positional information.<BRK>Otherwise, I am very likely to keep my weak rejection vote. Why do you have a mod here? What does dt mean? 2.Some related work and comparisons are missing. 3.The authors actually do not explain why this method works well in machine learning perspectives (similar to a biological neural system is not a very strong explanation for many people). Clarity:The paper is easy to understand, but it does not explain why the method works from the ML perspective. It would be better if the author can compare with other optimization methods such as gradient descent and EM algorithm for Kmeans clustering. The method is compared with GloVe most of the time. Second, GloVe is a dense method.<BRK>The authors connect the optimization problem to other recent learnings in this area such as sparse learnt projection   this perspective is appreciated. I would not have expected it to compete with BERT like models nor serve all their use cases. There is also a small qualitative demonstration of the embeddings (Fig.3) This tells us that the embeddings are meaningful, and their quality is comparable to Glove, if not the newer generation embeddings. I think this is a good outcome for the model they develop.
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 6. <BRK>This method can be implemented very fast and could be added to other compression methods for additional compression. This compression has a small relative variance. Weakness:  Except for the compression method, the optimization algorithms discussed in this paper is the standard parallel SGD. There are many advanced optimization algorithms in the literature, and it is more important to see its performance of this compression method on those advanced algorithms. The compression rate is large comparing to other data compression methods such as p quantization in the paper (Distributed Learning with Compressed Gradient Differences). Also, the comparison in A.4 may be unfair for the standard dithering. Therefore, for large values, the variance of natural compression is larger than standard ones (the same applies to Theorem 2), while small values favor natural compression. As mentioned above, the analysis is based on the relative variation, so it would be interesting to see the comparison in numerical experiments as well. Line 4 in the caption of Table 1. The second sentence is not completed.<BRK>This paper proposes a novel unbiased bidirectional compressor for vanilla SGD to reduce the communication overhead. My concerns are mainly about the the significance of the proposed method in practice and the comparison to the previous work:1. Dist EF SGD [1] and SignSGD [2] both achieve nearly 32x bidirectional compression, while natural compression only achieves roughly 8x in the experiments. Although this paper focuses on unbiased compressors and Dist EF SGD and SignSGD are biased compressors, the gap in the compression ratio is hard to ignore, which makes it hard to claim that this paper achieves SOTA in practice. Furthermore, since the experiments lacks comparison to other works (the only baseline with compression is standard dithering), it s hard to justify the importance of the proposed compressor. It will be better if the authors could show results on simpler models such as resnet20. For other compressors, I understand that the other unbiased compressors may not have bidirectional compression. However, SignSGD [2] achieves bidirectional 32x compression without error feedback (error feedback improves the convergence of SignSGD, but not neccessary in some cases). Although SignSGD is biased compressor, it satisfies the requirement "vanilla distributed SGD with bidirectional compression" mentioned in Section 4, which makes it a good baseline. However, SignSGD is not compared or cited in this paper. For the ImageNet experiments in the appendix, no comparison in training time is provided.<BRK>The paper proposed a new compression scheme called natural dithering, which uses powers of 2 as quantization levels in gradient compression. The variance of the scheme is studied and the compression scheme is tested in neural network training experiments. Pros:The proposed compression scheme is easy to implement in practice and the experiment results show it can reduce training time in practice. The study on different aspects of natural dithering is quite comprehensive, including the variance, comparison with standard dithering, and the limit regime when the number of bits goes to infinity (where standard dithering is better). Cons:My main concern is the novelty of this paper. Although discussions in the paper are quite comprehensive, the proposed compression scheme is just changing the uniform quantization levels to powers of 2. The idea is quite trivial and standard especially given the current machine representation of floating point numbers.<BRK>This paper introduces a new, simple yet theoretically and practically effective compression technique: natural compression. Theoretically, the compression technique increases the second moment of the compressed vector by not more than the factor of 9/8. Empirically, the communications savings are substantial, leading to 3 4 times improvement in overall running time. Overall, I think the paper is theoretically solid and empirically validated. The authors also give the theoretical and geometric interpretation why the natural compression technique can reduce communication without sacrificing too much accuracy. 2.It theoretically demonstrates that the savings in communication due to compression can outweigh the iteration slowdown, which leads to an overall speedup. This is not common in existing compression work. It also considers the compatibility of the proposed compression schemes to the current IEEE standard and current programmable network switches. This provides evidence on the practical impact of the proposed compression scheme. Cons: 1.The literature review seems outdated.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 8. <BRK>The local instance structure is learned by first gets patch level and graph level representations for each graph, then maximize the mutual information between both correlated patches and correlated graphs, which are decided by attribute masking strategy. The global semantic structure is maintained by leveraging RPCL to derive hierarchical prototypes of the representation and maximizing the mutual information between correlated graph representation and the searching path in the prototypes. Strengths:+  This paper presents a framework to jointly consider the local instance structure and global semantic structure of graphs. +  The experimental results are quite thorough with comparisons to several baseline methods. The performance of this model seems to heavily rely on the attribute masking strategy as all the operations are built upon the correlated graph pairing from the attribute masking strategy. Overall, the proposes a reasonable model for learning hierarchical graph representations. However, the novelty is limited since the proposed method seems like a simple combination of several existing techniques. As I mentioned earlier, I wonder how reliable the attribute masking strategy is. 2.It is not clear how to leverage prototypes in classification tasks? 3.In the Constraint for the global semantic structure part, the loss for a graph embedding includes both the representations for its correlated graph and its searching path consisting of several prototypes.<BRK>This paper proposed a method for self supervised graph level representation learning. The main idea is to enforce both the instance level smoothness embedding constraints, and a so called global, semantic grouping structures across all instance graphs in the training data set. To achieve this goal, the authors have adopted a global clustering framework to encourage the embedding of the graphs belonging to the same clusters to be close to each other, and by using a hierarchically organized set of prototypes. The proposed method is applied to pre train GNN on massive unlabeled graphs, which is then fine tuned to downstream learning tasks. However, it appears to me that the paper has combined the carefully devised ideas from too many existing work, each of which alone has shown great success in improving the learning performance. In more detail, the authors have used (1) masking strategy by Hu et al., 2019 to generate correlated graph pairs, (2) mutual information estimation technique InfoNCE to enforce the correlation between paired graphs, and (3) Rival Penalized Competitive Learning (RPCL) as the main building block for hierarchical prototype based learning. If not, then the gains are merely due to the effectiveness of these specially designed components from the literatures and not by the general idea of local and global structures. With regard to this concern, I would suggest the authors to clarify on what is the main theme of the work and demonstrate that the empirical performance gains are truly due to a novel, focused idea they propose rather than by combining some of the  existing algorithms which have shown great impact and performance gains in their respective context. In the current form, the novelrity of the work seems less significant by introducing so many components from other works.<BRK>The motivation and novelty of the proposed method are good. However, the validation is kind ofweak. I can understand that this papers follows the validation in Hu et al (2019), however, I feel thattwo tasks (one on chemical benchmark and one on biological benchmark) may not be sufficientto give a detailed idea of the improvement of the proposed GraphLoG over other baselines. However, I more would liketo see fluctuated parts in the proposed GraphLoG. One example may be: is there any differentchoice/option for hierarchical prototype?<BRK>Pros:  The paper proposed a novel self supervised learning method to embed graphs to vector space. Different from previous methods, the method proposed a global semantic learning strategy to encourage the embeddings to form a hierarchical clustering structure. Authors have provided extensive and convincing comparison results and numerical analysis to show the effectiveness of the method. The paper is well organized and clearly written. To the best of my knowledge, the proposed method is technically feasible. Cons:  The number of prototypes is determined by RPCL and can not be adjusted in training. Clustering algorithms are usually not very robust. Since the prototypes of GraphLoG is initialized by RPCL, is the performance of GraphLoG robust?
Reject. rating score: 3. rating score: 3. rating score: 4. rating score: 5. <BRK>The writing of the work is also clear and easy to follow. And it is not clear why the explanation path is not the concatenation of all the paths of the cascading trees. This will especially impact the overhead in the inference. W3.The evaluation needs to be enhanced. D7.In medical scenarios, we would expect the negative data samples are much more than positive data samples. Overfitting may not be a good idea for this skewed dataset.<BRK>It is unclear if this is the best way to learn cascading decision trees. The paper introduces these models, presents an induction algorithm to learn them from data, and includes an empirical evaluation on three UCI datasets as well as a propietary dataset. The authors suggest that the algorithm can be paired with any generic pruning method. This model form is a contribution in and of itself (i.e., regardless of the algorithm used to fit cascading decision trees from data).<BRK>The reviewer finds this idea very interesting and clearly elaborated in this paper. However, more theoretical and empirical justification is crucially necessary in order to make the claims in the submission convincing. Major issues: Despite the fact that the focus of cascading decision trees (CDTs) is a short explanatory path, my major concern here is that it is very hard to justify them as a proper statistical model. But in general we should not make such assumptions, and it would be better if we could have more discussions in the paper.<BRK>*Summary*This paper introduces the Cascading Decision Tree, a novel variant of decision trees with permits to extract short explanations for a class of interest. > The proposed approach is simple but reasonable. iv) the explanation depths of classic trees are in average very short, can authors comment this? This would enlarge the scope of the experiments and the value of the proposed approach. > I think that the proposed approach can be better inserted into the state of the art.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 6. rating score: 5. <BRK>I think this is not the authors  fault. (1)  Although contrastive learning is very popular now, I think the method is better termed as metric learning   its original name. 2.The paper obtains clear SOTA results and significantly promotes the development of weakly supervised segmentation.<BRK>Summary:This paper talks about a novel weakly supervised semantic segmentation (WSSS) approach which leverages a single pixel to segment contrastive learning formulation. Pros:  Motivation was well described. It is interesting to see that four different types of pixel to segment (where same segmentation entities were sometimes regarded as different categories) relationships were leveraged in a combined manner to eventually pull up the performance. Experiments are reasonably carried out both qualitatively and quantitatively.<BRK>In this paper, the authors proposed a metric learning based semi supervised semantic segmentation approach. ##########################################################################################*Strength: The formulation of pixel to segment based contrastive learning is intriguing. ######################################################################################*Reason for score:The proposed work is more of engineering, and it does not have any theoretical novelty; however, formulating semantic segmentation in a contrastive learning context is interesting. Furthermore, the authors demonstrate the effectiveness of the proposed method by showing improvements over the state of the art methods.<BRK>Summary:The paper proposes a unified framework for weakly supervised semantic segmentation that can take various types of weak labels as the input, e.g., points, scribbles, boxes, image tags. In experiments, results with various weak labels show SOTA performance on the PASCAL and DensePose datasets. Pros:The paper is well written and is easy to follow. The proposed unified framework using the idea of pixel to segment contrastive learning is interesting for the weakly supervised semantic segmentation task. It is not clear to fully understand the effectiveness of the introduced four types of relationships. Overall, the paper presents an interesting and effective framework.<BRK>The submission proposes a unified framework for weakly supervised semantic segmentation which is compatible with different types of annotations including image tag, bounding box, points and scribbles. While the proposed method obtained STOA or close to SOTA performance on VOC2012 and densepose dataset, the reviewer feels that the novelty of the proposed method is not significant enough.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 6. <BRK>The authors introduce a new algorithm AdaLead to solve the problem of efficient design of biological sequences and FLEXS, an open source simulation environment for sequence design. The experimental setting is crammed together with the results. This is an uncommon setting in Bayesian optimization where usually the budget is restricted for the evaluation of the latent black box function. The second contribution of the paper according to the authors is the novel algorithm AdaLead. This evolutionary algorithm is poorly explained. The experimental results are unsatisfactory without a proper BO comparison. Is there any general insight that became available from this study?<BRK>In this work. the authors addressed a critical issue on biological sequence design. The paper is clearly written. pros: 1.Addressing an important issue. Sequence design and optimization are critical for many real world problems, such as antibody optimization, etc. They have also considered many settings such as DNA, RNA, and protein as well as "swampland" where the optimization landscape is flat. 3.Offered an open sourced sandbox for model comparisons and proposed and evaluated a simple yet strong performing evolutionary algorithm that should be used as a baseline in further works in this space. Although this is a valid and reasonable framework, it is not fully in line with other algorithms  assumptions. 3.Related to 2, there is a lack of exploration to understand why AdaLead is outperforming. Could they give more details or some results on this?<BRK>In this work, the authors propose a greedy search approach for designing biological sequences in an active learning, batch setting. As promised by the authors, the proposed approach indeed appears rather straightforward to implement. Since the methodological contribution of this work is rather limited (again, a pretty standard evolutionary algorithm), it is concerning to me that the strongest empirical results only arise in the settings based on these choices. Especially in cases where many solutions are found, it would be useful to also characterize the diversity of the sequences themselves, for example, by using BLOSUM or some other sequence similarity measure. Minor comments The references are not consistently formatted. When introducing “optimization”, the authors rightly point out that, e.g., moderate binding may be preferable to stronger binding in some cases. I found the description of the RNA ground truth simulation unclear.<BRK>The authors implement an open source simulation environment FLEXS to emulate complex biological landscapes (TF binding landscapes) and to train and evaluate (RNA or protein) sequence design problems. They also propose a simple greedy search evolutionary algorithm, Adapt with the leader (ADALEAD) that is shown to perform better than its competitors and to optimize in challenging settings. Comments: >> An important component of the environment and for ADALEAD is phi^prime. Is this the case? If so, the iterative aspect is not mentioned clearly in the paper.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 6. <BRK>This paper proposes to extend the simple CNAPS few shot learning method to the transductive setting. The proposed method is able to achieve SOTA results on the transductive benchmarks. The proposed method lacks enough novelty. The proposed method is actually a transductive extension of CNAPS. The authors introduce two novel components: transductive task encoding and soft k means clustering. The experiments could be improved. As stated in the experiment section, some results are not well prepared before the submission, e.g.Overlapping excluded ImageNet results. Meta learning for semi supervised few shot classification<BRK>This paper proposes a transductive few shot learning method, Transductive CNAPS, by using the unlabeled examples. The proposed approach is a simple extension of the simple CNAPS method. It extends the simple CNAPS by using the unlabeled query instances to update the class centroids \mu_k and variance Q_k with the predicted class probabilities. The level of technical contribution and novelty is very incremental and low. The experiments are not very convincing. First, for both experiments on Meta Dataset and min/tiered ImageNet, the feature extractor is pretrained on some ImageNet subset. Second, the transductive few shot learning methods need to be compared with on the Meta Dataset in Table 1.<BRK>In order to improve few shot visual classification, the authors propose a transductive meta learning method using unlabelled examples. The authors have introduced a two step transductive encoder as well as soft k means clustering procedure on the existing simple CNAPS architecture. Pros:The paper is well written. While in transductive SNAPS the support task representation is used to compute support set embedding ($e_s$) and query set embedding ($e_q$). Both $e_s$ and $e_q$ are processed by a two step LSTM to generate the final transductive task embedding used for adaptation. However, when it is compared with other SNAPS architecture the performance is good. Final reviewThe authors have addressed some of my concerns so I am updating the rating. I still feel that the mode s novelty is limited and thus my highest rating will be 6.<BRK>Pros:1) The paper is well motivated, clearly written and neatly organized. Cons:1) The originality of the proposed method is incremental. Compared to simple CNAPS (Betni et al, CVPR 2020), the task encoder is extended to incorporate both a support set embedding and a query set embedding through Long Short Term memory (LSTM) network, and the classifier is extended to include the unlabeled examples in the query set as well.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 6. <BRK>Comments for rebuttal and revised paperThanks for providing a detailed response and an improved version of the paper. One thing that I am still concerned with is how come the updated ablation study is so different from the initial results. Originally, the differences between KGEDCg and KGEDCg GE and KGEDCg FKG were very minor (one of my questions above), but now the margins are as large as 7+ pts. Pros:+ The proposed method outperforms all the compared baselines on two dialog summarization datasets.<BRK>This paper proposes a new neural pipeline for dialogue summarization that jointly includes word by word decoding, an utterance graph, and a factual knowledge graph. The paper contains a well motivated introduction and perform a sound related work section (as far as my judge). Overall the paper is pretty well written. As there are many design choices, this requires multiple ablation studies to validate all of them. Yet, the authors only ablate the factual knowledge graph and graph encoder. Another interesting experiment would be to run the same architecture on a non dialogue dataset (by naively setting some of the edges). I mostly want to point out that there is a large spectrum of things that can be done to further demonstrate the validity of their model.<BRK>The paper proposes a novel framework, Knowledge Graph Enhanced Dual Copy network (KGEDC) for abstractive dialogue summarization. Conversational structure and factual knowledge are incorporated in this framework based on graph network to deal with long distance cross sentence dependencies and faithfulness respectively. Experimental results on two datasets show the performance gains of the proposed methods over several previous baselines. The motivation is clear and is in line with the model architecture. Besides, the model description is also well detailed. 3.Maybe the paper should provide more content on ablation and case study.<BRK>This paper proposes to improve dialogue summarization by encoding the text with a sequential encoder (for token level contextualization) and a graph encoder (for long distance and semantic contextualization). A dual copy mechanism is used while decoding in the hope that direct access to this factual knowledge will enhance the faithfulness of the generated summaries. The authors use a biLSTM to encoder the utterances. The results look good. I am not satisfied with the ablation study. One important ablation to run would be an experiment without both GE and FKG. I would consider updating the rating once the authors respond.
Accept (Oral). rating score: 8. rating score: 8. rating score: 7. rating score: 7. <BRK>##########################################################################Summary: The paper proposed a theoretically grounded O(N) approximation of the softmax attention. ##########################################################################Reasons for score:  The paper is very well written and should be accepted. This is an important landmark in the research about O(N) attention. The design of the random feature mapping is reasonable and theoretical analysis is convincing. Experiments show that Performer is better than the other O(N) attention methods and also other efficient attention methods. Thus, I feel the author may also want to compare with the linear attention method in ((Katharopoulos et al., 2020) "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention".<BRK>This is a solid paper that presents a computationally less expensive, unbiased, low variance estimator of the Transformer architecture. 2.In the text after eq.(18): inequality  > equality**Update after the author s response**: The authors have answered my questions during the rebuttal period and I am satisfied with the response. 2.The estimator (called FAVOR+) seems a more scalable replacement for regular attention. In eq.(18), a norm is missing in the last term: $\mathbf{z}^2$  > $\left\lVert\mathbf{z}\right\rVert^2$.<BRK>It can also be applied to efficiently model other kernalizable attention mechanisms beyond softmax, achieving better empirical results than regular Transformers on some datasets with such strong representation power. I think this paper is a solid step towards more efficient Transformers for practical long sequence data, and should be accepted. Below I raise two potential questions and look forward to the solutions in the future.<BRK>The main contribution of the paper lies in the proposed _positive random features_ that can approximate softmax with a strictly positive feature map without which the training is unstable. I find the theoretical results interesting and important but I would like more experimental evidence. The positive random features are also useful outside of the context of self attention for efficiently approximating softmax. My only reservation for a higher score is, as mentioned in the weaknesses section, the lack of comparison with simpler feature maps under equalized computation time.
Reject. rating score: 5. rating score: 5. rating score: 6. <BRK>My major concern is on whether the method indeed implements the optimal transport map. But it does not seem to have a relation to optimal transport, which is the claimed motivation of the work. Some theoretical results seem to be already well established, e.g., results in Theorem B.1 are all covered by Villani (2008) and Ambrosio et al.(2008).The authors may need to rephrase these results as an introduction to the background knowledge of this area, so that the novel theoretical contributions can be highlighted. EDIT: post rebuttal  I thank the authors for their patient and detailed replies regarding my concerns. I noticed that the authors have addressed the concerns to some extent in the updated paper (e.g., the remark in Conclusion). So I raised my score by 1 point. The authors present the Monge Ampere Eq.(2), which solves for the _optimal_ transport from the reference distribution to the data distribution. I agree that "An important advantage of the proposed approach is that it allows general energy functional in constructing the gradient flow from the reference distribution to the target distribution". But I think it does not have much to do with _optimal_ transport or the Monge Ampere equation.<BRK>Overall, the paper and the result are interesting but more rigor should be put on justifying the claims. More precisely, some samples of a target distribution are given and the goal is to pushforward some samples of an initial distribution to the target distribution. I rate the paper as marginally above, as I appreciate the bound on the Wasserstein gradient which is a first step toward analyzing the ETP (and is relevant from a statistical estimation point of view). The authors describe a statistical methodology to compute this Wasserstein gradient based on the samples of the target distribution and samples from the current distribution. They also prove a bound for the estimated Wasserstein gradient wrt the true Wasserstein gradient. I understand that these questions are difficult and might not be answered by the authors now, but they should at least be raised as limitations of the paper.<BRK>The velocity fields are solved by minimizing the f divergence between the particle density at iteration k and the target data density, which is shown to be in the form of gradient of density ratio. For example, [1] also analyze the particle evolution with the Vlasov process, and interpreting it as a gradient flow for minimizing the KL divergence. [2,3,4] are state of the art score based generative models that iteratively move particles based on the velocity field (i.e., score functions). Does this mean that the density ratio estimation problem does not suffer from the no overlapping support issue?
Accept (Spotlight). rating score: 7. rating score: 7. rating score: 6. <BRK>Summary:  This paper introduces influence estimation for GANs. The influence estimates approximate how helpful or harmful each training sample is with respect to some evaluation metric or loss function, such as Inception Score or FID. Strengths:   Paper is well written. Influence estimations adds an interpretability tool to GAN training. Proposed technique does a good job of estimating the true influence. Weaknesses:   No visual examples of the data points that were highly influential. No analysis into what kinds of characteristics might make a data point highly influential, either in the helpful or harmful sense. This is particularly important because removing instances from the training set inevitably reduces diversity, but this is not reflected in the current metrics. Including metrics such as Precision and Recall [1] or Density and Coverage [2] would allow us to see the trade off between fidelity and diversity. Computationally expensive, as model parameters need to be saved at every iteration (not a major issue for this paper, but worth noting)   Proposed method is only applied to very simple datasets and GAN models (likely due to the aforementioned compute issue)  Recommendation and Justification:  I really like the direction that this paper is headed in, but I think there is one thing holding it back. My greatest disappointment with this paper was that it does not include any visual examples of highly influential data points, nor any analysis or discussion about what kinds of characteristics helpful or harmful data points might have. These insights are half of the reason why interpretability methods such as influence estimation are useful, and without them it feels like I have read a story without a satisfying conclusion. Clarifying Questions:   In the paper it is stated that the generator and discriminator are simultaneously updated in one step for simplicity. However, the majority of GANs trained in practice use separate update steps for the generator and discriminator. Does this method also work for separate update steps? How consistent is the selection of harmful instances? Recently it has been shown that removing data points that lie in low density regions of the data manifold can result in improved GAN performance [3]. "Instance Selection for GANs." Edit after author response:The authors have sufficiently addressed my concerns with the additions that have been added in Appendix D. As such, I will increase my score from a 5 to a 7.<BRK>Summary: The paper presents an influence estimation method for GANs. It discusses why previous approaches on influence estimation cannot be easily extended to GANs. The authors evaluate whether an instance is harmful based on its influence on GAN evaluation metrics. They show that removing these harmful instances improves performance of GANs on MNIST with respect to three metrics: Inception Score, FID and Average Log Likelihood (ALL). The paper is well written, motivates the problem well and provides a detailed analysis of the proposed algorithm. It distinguishes its approach with other works on influence estimation such as [Hara et al.] and [Koh & Liang]. They verify that estimated and true influence on GAN evaluation metrics have statistically significant correlation, and show how removing harmful instances can improve evaluation scores. It seems that this approach eliminates modes of the distribution that the GAN is not able to cover well. 2.There are no qualitative results to demonstrate which instances are harmful in training GANs. Considering the large number of samples that need to be removed for improvements in evaluation scores, the authors can examine those samples, visualize them and try to find common characteristics among them. 3.The authors use Isolation Forests [Liu et al.2008] as the baseline for anomaly detection. It would be better to consider more recent approaches for anomaly detection such as those outlined in [B]. #################################################################Additional Comments: There are other metrics for evaluating performance of GAN models such as Precision and Recall [A]. While I think the current metrics considered in the paper are adequate, the authors can also mention other metrics. It partially addresses my concerns. However, the qualitative results do not always show improvements in image quality, and most of the low quality samples are still generated after cleansing. Overall, I am still concerned about practical applicability of the proposed approach. I keep my score of 7.<BRK>The paper proposes a technique to identify "harmful" data samples in the training dataset of a GAN. The research question is if it would be possible to find training samples that can be removed to improve the GAN training. I found the paper generally well written and interesting to read. Then the FID is computed with respect to a different dataset. The authors argue that this idea has been applied in other settings and that they are the first to bring this idea to GANs and adapt the overall technical idea to this new context. Do we really want to have GAN papers without any images? I am supportive of having some research in this direction and I believe other similar work can follow that builds on this initial idea. On the downside, the paper fails to establish that the proposed idea can actually contribute to creating a GAN that samples images of better visual quality. I would argue that this should be the main goal. It would be preferable if such tests would already be performed by the authors so that others can know if it is worth following this direction of research. All results are using GAN evaluation metrics: FID score, Inception, and ALL. From these metrics, I doubt that ALL is as meaningful as the authors make it out to be and I do not think the Inception score is that great either. FID has been quite useful to compare GANs. However, I am skeptical that improving the FID score directly is a meaningful endeavor. It s easily possible that an attempt to directly influence a metric such as FID score doesn t lead to better results for GANs. Similarly, an important GAN component, truncation, significantly improves quality while significantly worsening the FID. This is also an indication that the FID score is not a great metric that can be directly optimized for. Typically, the statistics of the training set and the statistics of generated GAN samples are compared.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>This paper presents a deep architecture to extract a wireframe model from a 3D point cloud. This is a problem of high interest, and the author claim that the approach they present is the first one to address this task, which is true to the best of my knowledge. If not, this is a real issue, since it would create a high cost entrance barrier for any following paper working on the same problem + it would makes comparison between the work and any other method impossible. While I realize it is not direct, I find it hard to believe that comparison with none of these works was possible. 4.All the examples shown are with very dense point clouds and very little noise, an analysis of the influence of these two parameters would be necessary. Also, the argument that using all pairs during training (as done during inference) would create an imbalance is not so clear since balanced BCE is used in the loss. Would a comparison be possible?<BRK>In the experiments, the network is evaluated with two datasets, a subset of the ABC dataset and a set of 3D models from Google 3D warehouse. The results demonstrate the outperformance of the proposed method quantitatively and qualitatively. *** Weaknesses ***As mentioned in the conclusion of the paper, the biggest limitation is that the proposed method can only predict straight lines as edges. Also, the experiments are conducted only on the synthetic data but not on any real scan data. *** Justification ***I think this work is worth to be published in ICLR in that it first proposed the problem of learning for wireframe prediction in 3D point clouds. I hope the authors discuss the following concurrent work in the revision:Wang et al., PIE NET: Parametric Inference of Point Cloud Edges, arXiv:2007.04883.<BRK>Pros:+ This paper presents an interesting idea on transforming an unordered pointcloud to a structured graph and use networks to learn topologies. + This paper raised a new problem and created their own synthesized dataset based on existing datasets for evaluation. Cons:My biggest concern is the limited application of this method. Deep networks requires tons of training data, and also seems most of the experimental results in this paper are almost perfect point cloud. Due to these limitations, I don t think this paper has enough contribution for applications.<BRK>By pruning the proposed lines, PC2WF generates the final wireframe represesntation. ### CommentsExtracting lines and wireframes from point clouds is a relatively new idea. The writing of the paper is clear and the method is easy to understand. This paper only tested on the synthetic ABC datasets, in which models have fairly dense point clouds and relatively low noise. It would more convencing if the authors can show whether the algorithm works on real world 3D scanning, e.g., redwood 3d scan. 2.It seems that EC NET is the only data driven method among all the baselines. If so, could you comment on why their performance is bad (AP^e_0.01) in the experiment section?
Reject. rating score: 4. rating score: 5. rating score: 5. <BRK>This paper studies generalization bounds for neural networks with the following kind of setup:(0) 1 hidden layer and sigmoid like activations. The main result of this paper is a generalization bound for this class of neural networks, which comes down to technical Lemma 2.3 bounding the Rademacher complexity of the class. It would be useful for the authors to compare their results further with previous work in this area.<BRK>This paper studies the statistical risk bounds for two layer neural networks with $L_1$ regularization. The authors consider two types of $L_1$ regularization: the $L_1$ regularization on output layer and the $L_1$ regularization on the input layer. The paper is clearly written and easy to follow. Furthermore, the results are a bit standard. I think this constraint may affect the approximation error established in Thm 2.1. This is not meaningful.<BRK>Pros:* The lower and upper bounds in the paper are almost matching. * The discussion of the assumptions and results are clear and informative. The paper is well written and well executed, but I am concerned that the contributions are few and that their impact is limited. * The paper applies existing results/analyses to a new setting (combining neural networks and L1 regularization). This setting does not pose any particular technical challenge.
Reject. rating score: 4. rating score: 5. rating score: 6. <BRK>This paper proposes WAFFLe for anonymized federated learning. The idea seems interesting, but I have a few concerns. The authors claimed that meta learning based approaches require sharing a small subset of data, but not all do. Missing privacy guarantees: While secure aggregation comes with a solid privacy guarantee, there is no theoretical guarantee that WAFFLe indeed can protect clients  data.<BRK> Summary The paper proposes a novel approach to federated learning which decomposes model parameters into two sub modules with task specific weight factors. I agree that the method looks reasonable but there need more insights into the necessity of model components. Only tiny networks are used. The authors adopt the IBP process for sparse selections of the factors per client, which mitigates interferences across local clients.<BRK>Data privacy and security are often using interchangeably in the paper, when they correspond to very different concepts. This terminology needs to be clarified, as all the benefits of the proposed method are with regard to data privacy, and not security.
Reject. rating score: 4. rating score: 5. rating score: 6. <BRK>.The paper is overall unclear and difficult to understand. Lack of ethical considerations for a system that could be used in law  enforcement. I was not familiar with the "word semantics representation" noun phrase. How is this obtained? Are all the conversation turns separated by the  [SEP] token?<BRK>The authors use the example of the PAN 12 dataset for sexual predators to use information about linguistics behaviour for the grooming phases. I think we would be better suited if we also understood in the base models, how much of the priors were learnt. Please also include a note about some of the ethical considerations when dealing with the PAN 12 dataset and how the data was created.<BRK>The paper asserts that the same methodology could be applied to different scenarios in the domain of chat conversations, but this is not confirmed by the experimental evaluation. The approach is tailored and evaluated on a specific application, namely online grooming detection. Performance is shown to be improved with respect to the state of the art.
Reject. rating score: 4. rating score: 5. rating score: 6. <BRK>Recommendation\The experiment setup in this paper deviates significantly from recent work of similar nature. Considering clustering performance in addition to discriminative power is a fair research question in my opinion. +The proposed idea is interesting and seems reasonable. These datasets are quite small and are not used as the primary performance benchmark for modern unsupervised image representation learning work.<BRK>The idea behind this paper is very similar to the above one. [Summary]Overall, I think this paper is a good trial of combining instance level contrastive loss and deep clustering philosophy into a single learning regime, which I think is a promising direction to explore. The paper has a poor literature review of previous works. In the related work, both instance level representation learning and deep clustering methods are not fully covered and compared.<BRK>Pros:1  The paper presents a good solution for an important problem in self supervised learning and contrastive learning. However, this is a part of the problem as class labels do not exist. The good performance is also evident in clustering experiments. Cons:1  The motivation part of the paper is not precise.
Reject. rating score: 3. rating score: 4. rating score: 7. rating score: 7. <BRK>In this paper the authors propose a method for training a classifier to be more effective at OOD (out of distribution) detection. I do think, however that this idea, in combination with some other OOD tricks could be an interesting paper. Finally the main novelty of the method proposed by the authors is to sort a collection of OOD examples and sort according to the OOD score of the current model and use the "qNth" to be presented as OOD examples during the next epoch during training.<BRK>Pro:+ The paper addresses a very important problem which is growing in importance. Could you help identify the challenge and the novelty in this extension? The paper still has some major concerns preventing the reviewer from recommending acceptance of the paper:   Similarity of the theoretical analysis to https://proceedings.neurips.cc/paper/2019/file/32e0bd1497aa43e02a42f47d9d6515ad Paper.pdf  and https://arxiv.org/pdf/1804.11285.pdf . Typically, OODs have been used for out of distribution datasets (SVNH for CIFAR10), new classes on which model was not trained (leave some classes out of CIFAR10), or transforms such as rotations or other transformations of inputs. It would help to make this obvious in the main text instead of having this reference in the appendix. While there are other methods that also use "exposure to outlier" in OOD detection, it is debatable such an approach would be useful in practice, particularly, if one also included adversarial examples. In summary, the idea of using hard examples is not new.<BRK>1.The paper presents a lot of theory but insufficient evidence. This is because the approach is dependent on auxiliary data which is available for the image datasets. 2.The presentation of the theory in Section 3 is not proper and makes it too simplistic. What is its connection to the algorithm? This type of limit on n is not very useful in practice as it is too loose. This is rather trivial. It is *much* harder in practice to get a U_x that is similar to Q_x. A more interesting case would be to show that even if U_x was not very similar to Q_x, then is the FPR within reasonable limit?<BRK>This paper provides theoretical analysis formalizing the intuition of mining hard outliers for improving the robustness of OOD detection. It also provides an ablation study on the effectiveness of adversarial training, which further proves its effectiveness. I think it s a good paper overall and should be accepted.
Reject. rating score: 3. rating score: 5. rating score: 5. rating score: 6. <BRK>The paper presents a modification to conditional batch normalization, wherein an extra affine layer is introduced between the standardization and the conditional affine layer. The paper shows several empirical studies, including one for architecture search   with the main paper using DARTS, and the appendix using GDAS. The results in this paper are encouraging, but I believe the paper needs to explain more clearly why SaBN is expected to work (given that it preserves the hypothesis class as well as the minimum of the training objective).<BRK>The authors propose a new variant of batch normalization. I would like the authors to clarify that the contribution is only in the formulation paragraph. It seems to me that the paper organization is poor. There are little descriptions, explanations, and discussions about the proposed method (Eqn 3)2. the contribution. Could you give more explanation on this?<BRK>Following this idea, the SaAuxBN and SaIN have also been introduced in this paper. The extensive experiments demonstrate the effectiveness of such methods in neural architecture search (NAS), image generation, adversarial training, and style transfer. (1) “Specifically, the number of independent affine layers in the SaBN equals to the total number of candidate operation paths of the connected previous layer.” Are you sure it doesn’t equals to the total number of candidate operations in next layer? Could you please provide a more detailed explanation ? Could you give some analysis about the different about your methods and the above case?<BRK>The experiments on various tasks (e.g.NAS, GAN, adversarial defense) demonstrate the effectiveness of proposed methods. The SaBN output multiple tensors, each of which is associated with the candidate operation in the previous layer. Could the authors give some analysis of the impact on the searched architecture when using SaBN? I think more results generated from the original methods should be given for the comparison. (4)I want to know the limitations or some failure results of the proposed method, so that we can have a more comprehensive understanding of the proposed normalizer.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 7. <BRK>The concept of non robust features and universal perturbations is not new, but it is interesting to study the two concepts together. Visualization of qualitative results is provided. Conclusion is not well supported by the experiment. It is hard to measure what is human aligned properties, even the concept is not well defined. How would the findings of this paper be beneficial to the models? Wouldn t universal perturbations be optimized to be invariant such that they contain less non robust signal (but more invariant/general signal), otherwise how would it be universal? 4.The paper defines the adversarial perturbations to be non robust features, however, the existence of non robustness features is still an open question. It is one interpretation, but other interpretation also works. For example, it can be the common vulnerability of CNN models, thus the authors need to rethink whether a paper based on this assumption is solid.<BRK> > Summary:The authors investigate universal perturbations for adversarial robustness. They find that while universal perturbations are based on non robust features, they are more human aligned and spatially invariant. They also show that they contain less predictivesignal than other non robust features. While I am not that familiar with interpretability literature, what experiments on generalization seem to show, most of the features learned by models are in fact not universal perturbations. So I am not sure whether findings of this paper would be interesting for broad ICLR community. > Cons:While I am not that familiar with interpretability literature, based on my knowledge, I am not so sure of the impact of the findings presented in this work. I feel these are all interesting, but I am not sure how surprising they are. Universal perturbations are just a small type of possible adversarial perturbations and it is interesting to investigate their properties.<BRK>Experiments are based on a universal version of projected gradient descent (PGD). The findings are that universal perturbations are more aligned with visual semantics and human perception that general adversarial attacks. Generalization seems to decrease with the size of the set used for generating a given universal perturbation, while semantics of the features improve. The subject of the paper is of interest in general and relevant for the ICLR community. The experiments chosen for the exploration are appropriate. The paper is clear and well written. Concerns:  While the present work advanced the state of understanding of non robust features, it seems we are still far from a full picture of the topic. Many of the novel ideas proposed in the paper are still at a hypothesis or reasonable explanation state (e.g., universal perturbations reflect the intersection of non robust features for the input set they were computed on). Questions / suggestions:  It is unclear why universal perturbations being bounded in norm implies that they must be leveraging non robust features. Fig.4: how is the perturbation shifted?<BRK>Prior works generally thought non robust features, which are vulnerable to small perturbations, are not semantically meaningful but are useful for generalization. This work challenges these traditional beliefs by pointing out that non robust features can also be human perception aligned and be less useful for generalization, if these non robust features are discovered via universal adversarial perturbations (rather than via image dependent perturbations). Extensive experiments are provided to justified these arguments. (2) The idea of utilizing universal adversarial perturbations to analyze non robust features is novel, and lead to many interesting findings of non robust features, e.g., non robust features can also be semantically meaningful. For example, Table 1 suggests that with a large base set, non robust features  generalization decrease. To rule out this possibility, the authors should also investigate the setting where the number of universal perturbations is equal to the number of training samples, regardless of the value of the size of base set K. The dataset preparation could be like this: first, we randomly sample K images, and generate a universal perturbation for them; second, we randomly sample one image from K  samples (which successfully fool networks), add the generated universal perturbation, and put it to the new training set; third, repeat the first & second step until we collect enough images in this new training set. will the conclusions here generalizable to the setting of non targeted attack? In general, the reviewer thinks it is an interesting paper, and is happy to raise the score if the authors successfully address the concerns above.
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 7. <BRK>This requires more elaboration to connect with the routing example in the introduction. It seems a reasonable situation is that the observations are corrupted but the performance feedback is not. The target of optimization. It is technically sound, but the lack of involved and novel technical contributions makes it more belong to an incremental work.<BRK>+ For stochastic linear bandits, typically the regret bound does not scale with the size (or number of vectors) in the decision set. Why is such a dependence on K necessary here? Strengths:+ The regret bounds are novel, and offer an improvement upon known.<BRK>Overall I do not see the presented results of enough significance to make the paper eligible for ICLR. The paper is overall well written and is clearly presented. For each setting, the paper presents a no regret algorithm that is robust against such corruption. Is it only in the concentration of good events?<BRK>What does this exactly mean? The policy is not an MDP, but I m guessing it is the best policy against a worst case MDP from that family of consistent MDPs? Furthermore, the fact that the paper covers online settings of increasing generality is of use in terms of techniques, and indeed the theoretical contributions are interesting in their own right.
Reject. rating score: 4. rating score: 5. rating score: 5. <BRK>The paper considers the problem of estimating instance independent label noise. The paper proposed an information theoretic approach for this task, the key idea behind which is to estimate if a particular dataset has maximum entropy with respect to the labels. The problem of learning with instance independent noise has received some attention from the community and could be of interest. The current experiments only show that the approach can find better transition matrices Q in terms of KL divergence. Since this is simple, synthetic noise model this is not very convincing. The experiments are also only done on CIFAR 10, whereas most of the related work considers at least a few other datasets. 2.I found the discussion and notation in Section 2.3 to be a bit convoluted.<BRK>This paper studies the problem of learning the noise model in a classification problem. The term "instance independent label noise" refers to the fact that the law of Y | Z doesn t depend on X. The goal is to learn the noise model, i.e.the transition kernel Z  > Y. I feel the writing clarity could be improved in places. The definition of DILN in the main text is confusing (does it include the noise model or not?) An informal definition in main text would be helpful. As far as I understood the theorem statement, it says that if the algorithm is given as input a  separable map  g then it is possible to learn the noise model, and the main difficulty to apply it is to construct an efficient separable map (which in practice, they attempt with decision trees + LID scores). (And ideally, under which we could hope to find such a map.) Overall, I think there may be some interesting ideas in this paper, but I did not find the results to be very strong support for the claims made (see other notes below regarding the experiments). I also think the paper could benefit from clearer writing. Other notes:* The beginning of the paper suggests that the key contribution is to learn the noise model without needing to "accurately" learn a classifier X  > Z. * I found the definition of f_{matrix} confusing.<BRK>I would suggest adding more explanations about the theorem and algorithm. However, the paper is unsatisfiable in terms of writing and theoretical contribution. The problem of learning models of instance independent label noise is, by itself, an important and practical problem. I hope that the authors can address (at least some of) the concerns/questions/suggestions stated above. In particular, it is shown that the proposed algorithm can be combined with prior ones to improve their efficiency/accuracy. I believe this requires further clarification, especially on why these conditions are mild, given the existing literature. For example, the algorithm relies on the existence of a separable random function $g$ on $\mathfrak U[\mathcal D]$. Besides, how strong is this condition? Is such a condition satisfied by the random function $g_{\text{LID}}$ used in the experiments? The theoretical results show that the estimator is consistent if (nearly) all parameters tend to infinity, which doesn t seem practical. 2.I am also concerned about the writing of the paper due to the following reasons.
Reject. rating score: 4. rating score: 6. rating score: 7. rating score: 7. <BRK>This paper uses experimental measurements and empirical curve fits of learning curves to study their interaction with training protocols such as transfer learning and data augmentation. They define the learning curve to be the test error as a function of training set size. I appreciate the authors  goal of relating the scaling behavior of learning curves with common choices in NN training. "Do better imagenet models transfer better?." c)  I worry that similar issues exist for other conclusions of the paper. For example, the conclusions about "Increasing the network depth" are very interesting, but I am not convinced that the paper has the experiments to justify them. "Rethinking imagenet pre training."<BRK>This paper advocates for studying the effect of design choices in deep learning via their effect on entire *learning curves* (test error vs num samples N), as opposed to their effect only for a fixed N. This is a valid and important message, and it is indeed an aspect that is often overlooked in certain domains. Comments/clarifications which do not affect the score:Why are the experiments done using the Ranger optimizer? In summary, the paper oversimplifies certain important aspects in both the setup and the experiments. Would any conclusions differ if we use standard optimizers (SGD/Adam)? There is no discussion of the model size issue in the present work. Certainly this does not hold for any arbitrary pre training dataset, but perhaps it holds for "natural" pre training datasets close to the ones tested here. (Only convnets are tested in this paper).<BRK>This also allows an estimate of "data reliance", in essence the slope of error wrt dataset size, computing how much error decrease is dependent on dataset size. The authors then perform an extensive experimental evaluation on varying sized subsets of CIFAR 100 and Places365, across multiple different neural architectures and varying choices of finetuning, pretraining, linear classifier (frozen feature training) varying architecture size and data augmentation. They fit learning curves on these different empirical configurations, estimating data reliance (along with extrapolating error), finding interesting conclusions such as finetuning outperforming linear classifiers even on small datasets, and larger architectures actually improving data reliance even in small data settings. Both the learning curve computations and empirical evaluations are interesting and I recommend accepting this paper. As it is, it is very difficult to follow the main takeaways from the different experiments, and even the insights given by fitting the learning curve (and computing data reliance.)<BRK>### SummaryThe authors conduct an investigation of learning curves on image classification models to understand tradeoffs between error and dataset size across different design choices. These learning curves help clarify the relationship between error and data reliance as a function of choices such as depth, width, pre training and fine tuning. ### Comments* The introduction and motivation for the work is clear and well written. * The learning curve is a nice way to understand tradeoffs in design choices for a given model. ### Recommendation / JustificationI vote to accept the paper. The authors do an excellent job of motivating the importance of learning curves and are systematic about their experimentation and analysis. I think it d help to put more detailed descriptions of some of the procedures e.g."Fine tuning the entire network is only helpful if the training set is large" is vague if you do not quantify helpful nor define an alternative (such as fine tuning just the final layer).
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 8. <BRK>Summary: The paper proposes NeoEA, an approach that further constrains KG embedding with ontology knowledge. The paper first tries to summarize the existing embedding based entity alignment methods, stating that most of the methods choose TransE as scoring functions. But their embedding features are not aligned well compared to the neural based or composition based loss function. The paper, therefore, solves this problem by developing a new NeoEA architecture which shows that adding a KG invariant ontology knowledge can minimize such difference. The experiment shows the new constraints can improve state of the art baselines. Strengths:+ The idea of using ontology constraint as an additional loss is new. The paper shows significant improvements when combining the newly designed loss with state of the art baselines. Weaknesses:  The paper doesn t have a related work section. Figure 1 is a little bit messy. Especially for figure 1d, it would be better to make the figure a little bit larger. Section 2 should be merged into the introduction section. Some of the words are confusing such as neural axioms. The neural ontology alignment (which is stated in the appendix figure) is much more clear than the current Conditional Neural Axioms. The theorems and axioms should be propositions or some hypothesis. The proof in the appendix is also not well defined. **Post Rebuttal:  I appreciate that the authors have conduct revisions on the current version. However, I think the current paper is probably still not strong enough for ICLR.<BRK>Overall Comments:Entity alignment plays an important role in improving the quality of cross lingual knowledge graphs. As one of the most important solutions, embedding based methods aim at learning a semantic space where the unique entity cross knowledge graphs can have the closest distance. Most of research focus on entity level granular, but discard the whole picture of embedding space of cross lingual KGs. Besides the aligned entity pairs as the labelled data, this paper extended the labelled data with the conditional neural and basic axioms, which are actually sets of randomly selected entities or entities with the same relation type. Clarity:The presentation and organization of this paper is very difficult to follow. Besides the grammar and type errors, there exists many concepts that not clear, which makes it difficult to understand the main idea of this work. The claimed challenges, that have not been solved well by previous works ,are not convincing enough. In the 3rd paragraph, authors argued that previous research shows very good performance, but has not made on the theoretical analysis. Taking the theorem 1 as example, it s more like a justification but not a theorem to show the connection between the proposed axiom and "ontology". Its structure will be changed along with the KG in hand. From this paper, I can not find the connection between relation type alignment loss and the ontology. It s an interesting work but not ready. Are they labelled manually? If so, it will have flexibility issue for dealing multiple KGs. 2.Please compare to a recent proposed method [1] which also optimizes the distance between a group of entities from cross lingual KGs.<BRK>  Overall  This paper proposes an entity alignment framework that leverages the dependencies between entities and relations (reminiscent of TransR [Lin Y, et al.AAAI  15]) to further refine the results of conventional embedding based alignment approach. Merits: The proposed framework is shown to be effective in improving the performance of baseline models. Weaknesses: (1) the methodological contribution is limited. (2) the theoretical explanation part is trivial and contributes little scientific knowledge. The bound in Eq.5 seems meaningless since the assumption (i.e., one relation and one neighbour) on which the bound basis is too idealistic to meet in practice. 2.The explanations about the behaviour of embedding based entity alignment (in both section 2.2 and section 3.1) are straight forward and trivial, thus contribute little knowledge. It would be better to highlight the most important part i.e., loss function, while avoid emphasising too much on the detailed definitions and examples. minor comments There are some typos and grammar mistakes, need to be proof read carefully (e.g., “e_x^2”  > “e_y^2” in the paragraph just above Eq.6; “take X for example” > “take X as an example”).<BRK>In the paper, the authors propose to minimize the discrepancy between pairs of (conditional) neural axioms to align the embedding spaces of different KGs. This method is justified by the authors  study of all kinds of OWL2 properties. The author also studied the influence of margin $\lambda$ on less constrained/long tail entities. The authors conducted experiments by adding the proposed model on top of the best models for entity alignment. The results are mixed, but the proposed model improves the SEA and RDGCN consistently. This paper provides a theoretic point of view of the entity alignment task, which was mostly studied in empirical methods. The idea to align the axioms by minimizing Wasserstein distance is well justified. 2.The method described in this paper can be in principle adapted to any previous and future EEA scoring functions. For KGs that are on very different domains, this method may include errors, as two heterogeneous KGs do not naturally fit in one unified space. The influence of overlap on this method is not well studied.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 7. <BRK>Summary: This paper focuses on model based black box optimization problems in the offline setting. The main aim of the paper is to provide a test bed for algorithms that try to solve this challenge. (v) Synthesize a suite of methods from the literature that are distinct from each other and of interest to the community as strong benchmarks. The real world settings for these problems don t map well to each other. (6) I suggest the authors think carefully about what the evaluation criteria are. Just because some previous papers chose this as the design task doesn t make it a good benchmark.<BRK> ##########################################################################Summary: This paper proposes a benchmark suite of offline model based optimization problems. This benchmark includes diverse and realistic tasks derived from real world problems in biology, material science, and robotics contains a wide variety of domains, and it covers both continuous and discrete, low and high dimensional design spaces. The authors provide a comprehensive evaluation of existing methods under identical assumptions and get several interesting takeaways fromthe results. They found there exists surprising efficacy of simple baselines such as naive gradient ascent, which suggests the need for careful tuning and standardization of methods in this area.<BRK>The community currently lacks a standardized benchmark to compare the performance of methods. This paper presents a new suite of offline model based optimization tasks and standardized evaluation procedures for the community. The evaluation criterion for the quality of a benchmark is the realism and diversity of the tasks, with special consideration for high dimensional design space and the objective function s sensitivity. The paper then evaluates several algorithms on the benchmark. The result is a diverse set, with some representing real world design optimization problems. However, there are a few areas in which the benchmark and evaluation should be improved. I do not recommend this paper for acceptance as there is insufficient support as to why these problems should be considered over others or what challenges these environments present for designing new algorithms. Optimizer benchmarking needs to account for hyperparameter tuning. Can the authors clarify what is intended to be learned from including these tasks in the benchmark? Furthermore, it is unclear how the uncertainty of the results are quantified.<BRK>I liked the paper overall. The motivation of the paper is clear, and given that offline ML based optimization is beginning to take traction, this is a good time to set up benchmarks and evaluation metrics. The variety of domains considered, with careful consideration of complexity, are good characteristics of the benchmark. In a practical application, we need to know the efficacy of an algorithm for a particular task before they are deployed on the real task. None of the tasks considered have constraints on the problem. This is especially challenging for model based methods in continuous design space. The more popular traditional methods such as genetic algorithms and mixed integer programming would be good to compare against. ML methods will only be adopted if they can beat existing established methods.
Reject. rating score: 3. rating score: 4. rating score: 5. <BRK>The authors hypothesize that satisfactory assignments modeled as MDPs will be more alike than they are to unsatisfactory assignments. Therefore this can potentially be used as part of some kind of future automated feedback system. Is there something I’m missing about why this is an appropriate solution to the problem identified by the authors? I think there’s certainly potential. I have a number of concerns in terms of weaknesses of the paper. I am not sure what the architecture for the agent and the classifier was exactly. I think this approach is very interesting, but I’m concerned about whether it’s an appropriate one for this problem domain. I think this work is interesting, but too immature for publication at this time.<BRK>It tackles a class of programs that otherwise would be very difficult to grade, because they require interactions. Combined with poor writing quality, I think it is not yet ready for publication. Extending this approach to solve some additional domains of interactive programs would solidify this paper as one of the significant contributions to MOOCs grading line of literature. .. "This would clear up the confusion. I think the other reviewer summarized it best, "interesting but immature". I hope to see this work develop and published in the future. – one would have solved the challenge of playing to grade. I think this is an unwarranted challenge. The authors consider changes to the perceptual input such as "random theme change can happen if the ball hits the wall", which seems highly limiting.<BRK>The authors identify a relatively novel problem to solve with deep reinforcement learning   automatically grading programs which require dynamic input from the user to judge for implementation correctness. Overall, considering the three types of comments above, I believe that the paper is an interesting piece of work which makes some rather preliminary progress on a new methodology for automatically grading programming assignments which require dynamic input. In doing so, they also introduce a new dataset and baseline benchmark which can be used to encourage further work in the area. The task of grading is also something that should not have many false negatives (grading as incorrect when it was correct) since that s not good for students.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>Reason for Score:I vote for accepting. I believe the contribution of the paper is significant, as it is the first to construct a valid posterior distribution for the parameters in the neural networks.<BRK>I would like if the authors point out this disconnect for the benefit of the readers, and have a discussion section.<BRK>After reading the rebuttal and comments of other reviewers, I am increasing my score. *****  Questions for the Authors  *****Please address the above weaknesses of the paper.<BRK>2.In the entire paper, I didn t find a definition for $g(x,\theta)$.
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 7. <BRK>The paper proposes to swap the typical routing mechanisms in capsules for a more standard attention mechanism. The attention mechanism is based on computing similarity scores using gaussians instead of dot products . The authors show that this leads to better downstream performance of more natural tasks while preserving robustness to viewpoint changes, one of the main strengths of capsules. My main concern is that the proposed solution is not a good fit for the general framework of capsules. One of the key ideas behind capsuels is that objects are made up of parts which are uniquely assigned to exactly one object. This assumption is broken in this paper (if I understood the paper correctly). The results indicate that this is not necessary for downstream performance (it actually helps) and the models still remain somewhat robust to view point changes on synthetic data (NORB). Detailed Comments and Questions  Where is the "self" in self attention? IIUC there is no attention between individual states of one layer, not even indirectly. There is only attention between some learnable "object" states (inducing points) to their individual parts. Why is it necessary to model the likelihood of a part state in this model, when all we care about is the similarity between an objects inducing vector and a part representation. Why not using just simple dot product attention to compute the similarities? The paper says it is used to "encode the second order interactions among points". What does that mean? The paper says they use stationary routing weights to specific locations, but I don t see how this approach differs from that? The routing weight in Hahn et al.basically computes dot products between inducing points (the rows in the learnable routing weight matrix) and the output of the part capsule which are used as similarity scores. The only difference here is that gaussians are used to model similarity, which, as I explained above is pretty much the same as a dot product. Something that s different is the use of multiple heads and using a softmax over parts instead of over objects, which brings me to my biggest concern. CapsNets make the assumption that each part belongs to a single object. That s also the reason why typically there needs to be some iterative procedure to compute probable assignments. Here, however, parts can be part of multiple objects, because the softmax is taken over the parts and there are multiple heads. Ultimately, the architecture is basically a stacked version of the set transformer with some potentially interesting deviations, and it should be presented as such with the necessary ablations.<BRK>The paper proposes to use the self attention to find the agreement among the capsules of consecutive layers of a capsule network instead of iterative routing procedure. To reduce the computational and memory complexity of self transformer capsules of each layer are considered as a set and follow set transformer and apply inducing mixture of Gaussian distributions to compute the agreement among capsules of layer L and L+1. The proposed method have been evaluated on multiple dataset including large scale datasets like ImageNet and improved the performance compared to the baselines (except for  ImageNet)pros:1  The idea is interesting and the paper is well written and easy to follow. 2  The proposed method seems to be scalable to the large datasets. cons(clarification)1  There are related papers that are missing in the comparison section for instance  Capsule routing via variational Bayes. 2  Why is the performance the proposed method on the ImageNet is even less than the baseline Resnet50. 3  It seems that increasing the number of attention head negatively affect the performance, what is your justification for that? The proposed method has a significant improvement compare to the baseline on the novel view point of SmallNorb while it is on par with other method in familiar viewpoint. Post RebuttalThanks for the author(s)  responses. The rebuttal addressed some of my questions. I have a couple of  suggestions : 1  Your proposed capsule network is not the first one that is applicable on large scale datasets like ImageNet, there are other capsule networks that are applicable on real world scenarios and also ImageNet dataset with improvements over the baseline   Dual Directed Capsule Network for Very Low Resolution Image Recognition (ICCV 2019)   Subspace Capsule Network (AAAI 2020)please refer to them and also give intuitions about why your proposed Trans Caps is not performing well on ImageNet. The intuition and analysis is valuable to the community. 2  To support the generalizability claim of Trans Caps, I highly recommend reporting results on Multi MNIST and also affNIST. Specially when you train the model on MNIST and test it on these two datasets.<BRK>The submission details a novel technique to learn the routing in capsule networks for image classification tasks. The main idea in this submission is to leverage a non iterative attention mechanism to learn this routing and thus decreases computational cost. Overall, the submission is well written and easy to follow. The discussion of related work is thorough and to my best knowledge appears to be complete. ## Pros:   * The paper clearly presents the proposed method, and it is easy to follow. * The results show significant improvements over the baselines, especially on the Tiny ImageNet dataset and the SmallNORB dataset in the case of novel poses. * The proposed learning algorithm does not require expensive iteration and thus allows for better scaling. * The good results across several datasets suggest that the proposed model can be an important contribution to the field. ## Cons:   * The authors discussion of related work mentions several papers that combine CapsNets with an attention mechanisms to address the issue of routing. * However, the authors only provide a comparison to Hahn et al.(2019).The authors state that other methods are memory intensive and require knowledge of the number of concurrent iterations as an additional hyperparameter, which is probably why they do not compare to these methods. * Furthermore, it would be interesting to compare to state of the art image classification methods that are not based on the capsules concept. Adding these additional experiments would help readers to have a better overview of the task and where the proposed approach stands in the broader context of classification approaches. * In other areas of deep learning and representation learning in particular, use of GMMs in the architecture, e.g., in latent space, (as opposed to usage as a probabilistic output model) can lead to code book collapse. I.e., it is hard to balance during pure back prop learning how to pick the correct gaussian to sample from and to shape it  (or in other words: should one update the parameters of a mixture component or penalize the selection of that component if the action leads to a high loss in the forward pass). It would be interesting to see more details on this aspect of the architecture and training. etc.Summary:Overall, there is much to like about this submission but some questions remain (see cons). I m looking forward to the authors  response.<BRK>Authors propose a novel Capsule connection which is inspired by set transformer and induced points. They introduce log likelihood based attention based on Gaussians centered at trainable fixed queries. They calculate the routing factors (attention) using the probability of a key projection of votes under trainable gaussians. Their choice of modifying the mean based on the input rather than replacing it is quite interesting. It resonates with the concept of momentum as well. Afterwards they linearly transform and add a skip connection + relu to get the output capsule parameters. In this work the activation probability of the capsules are not calculated alongside the pose matrices. For the sake of classification (which needs the activation probabilities) they have an extra fully connected layer + cross entropy. Their method surprisingly can generalize to new viewpoints, backed by experiments on smallNorb azimuth and elevation generalization much better than the CNN and previous Capsule Networks. Also by removing the iterative routing and replacing it with trainable parameters they are able to achieve competitive results on Cifar10 Cifar100 tiny imagenet and imageNet. Pros:The paper is very well written and easy to follow. They provide a convincing set of experiments on reasonable datasets. Their method is novel and intuitive. One baseline is just the attention used by set transformer. Essentially what is the effect of having a Gaussian (standard deviation). It would be more convincing to add attention based vision models (bello et al 2019) to the tables for cifar10 cifar100. I enjoyed reading your work and it has answered some of the questions we wanted to explore in Capsule Networks.
Reject. rating score: 4. rating score: 4. rating score: 6. rating score: 7. <BRK>The authors propose an approach to explain a given signal $\xi$ (i.e., some function of interest, such as a 2D image, or a probability distribution) by learning simple "rules" that can accurately reconstruct it. What is the actual distance measure $\Delta$ used?) Another major issue with this paper is that the author seem largely unaware of the closely related, and very well established,  theories of induction coming from algorithmic information theory (AIT), as developed by Solomonoff, Chaitin, Rissanen (via minimum description length), and others. The relevant literature is too vast to mention, but one starting point could be Chater and Vitanyi, Simplicity: a unifying principle in cognitive science?, TICS, 2003.<BRK>This paper has addressed a very ambitious goal about explainability and generalizability from “small data" by generalizing the information lattice defined by Shannon. The paper has tried to answer some well known challenging problems in machine learning such as explainability and generalizability from a very different perspective. Thus I can not evaluate the significance of the technical contents. Meanwhile, I recommend the authors to use simple and explicit enough formulations to show their framework so that we can know the tractability at the first glance, such as the convexity/nonconvexity, continuity/discontinuity, etc. However, it is not very attractive to the general audience?<BRK>This set serves as the defacto language of the summarizations that result. Moreover, one might refute this difficulty by requiring the user to specify the terms in which they would like to describe the data. In the opinion of this reviewer, the community has not adequately solved this problem. The authors clearly state that a signal is a function from data to the reals, but then use the same term to describe images. The authors appear to use both  X  and  signal  to describe input data. It makes the paper feel disconnected.<BRK>This paper proposes a novel learning framework called information lattice learning. It is formulated as an optimization problem that finds decomposed hierarchical representations that are efficient in explaining data using a two phased approach. One concern I have is the complexity and scalability of the proposed algorithm. Did authors only consider "small data" regime due to the scalability problem? It would be interesting to compare their work with existing unsupervised deep learning algorithms that attempt to find disentangled representations.
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. rating score: 3. <BRK>Both theoretical and empirical analysis are provided and results look convincing. This paper investigates the normalization effect extensively for zero shot learning, and provides many insightful thoughts for utilizing these tricks. The authors also evaluate the proposed class normalization with a simple implementation on benchmark datasets and show convincing results. 3.The empirical analysis in the paper are extensive and convincing. Have you tried to explain this failure especially from the perspective of the proposed normalization trick?<BRK>The author shows that normalization helps to improve the CZSL result. Positive:1: The normalization is important in the NN/CNN model, in the ZSL, there are two sources of information, and proper normalization is important on both source for the better result. The paper gives a theoretical justification of why normalization is important and how we can do the same for the performance gain. 2: The proposed normalization shows the significant performance gain on the standard dataset for the GZSL setup (provided evaluation and code is correct). It is nice to see that the non generative model shows a significant improvement using the simple method, and it is much faster to train. The generative model can handle this scenario since they can generate data from the unseen class.<BRK>Summary: This paper presents a theoretical justification for normalization in model training on how it affects model performance and training time. Rating Reason: This paper has included a well detailed analysis and mathematical formulations for normalization, initialization about model training. To address this problem, a new initialization scheme is introduced. The experiments for CZSL are performed in two datasets, CUB and SUN. For good model training apart from normalization tricks, it introduced an initialization scheme. It also extends ZSL in continual learning. I expect a  new proposed model from authors to make the paper more strong. Therefore it must be included in the comparison table. 6   I wonder by the training time you reported. Still, to have a clear view or fair comparison, you should have compared the timings with other initialization and default normalization and scaling tricks as well.<BRK>Simple method. ClarityThe organization of the paper is such that the reader has to refer to the appendix a lot. My biggest concern on clarity is on the “theoretical” results which are not rigorous and at times unsupported. W2.SoundnessI have a lot of concerns and questions here as I read through Sect. This is in part due to poor clarity. If it is difficult to apply these tricks, further explanation should be given (generally, also mention applicability of these tricks.) This is done to some degree in the continual setting. or why is it “surprising” that this is preferred in practice? 3.Suggestions for references for attribute normalization. Abstract: Are the authors the one to “generalize ZSL to a broader problem”?
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 7. <BRK>The loss function that integrates out of class samples is counter intuitive and seems to be chosen based on improved empirical evidence. Clarity:The paper reads well. Detailed Comments:The paper uses contrastive learning idea proposed in SimCLR (Chen et al.2020) to detect out of class samples and treat them in a different way than in class unlabeled samples during semi supervised learning. It also explores the idea of auxiliary batch normalization (from Xie et al.2020) in the open set SSL setting but the results of the ablation study suggest the level of improvement achieved by this normalization is negligible and the most of the improvement comes from more accurate detection of out of class samples through using the projection header function introduced in SimCLR paper. Class conditional likelihoods has been shown to be not very useful in detecting out of distribution samples in cross entropy loss learning. What aspect of contrastive learning makes it more useful for open set classification? It is simply computing the loss with respect to an incorrect label. If the purpose here is to capture shared characteristics of the samples then SimCLR trained with both labeled and unlabeled data already takes care of it.<BRK>This paper considers the problem of semi supervised learning, where the unlabeled data may include out of class samples. To address this task, the paper proposes a method consisting of three steps: (1) detecting out of class samples in the unlabeled set, (2) assigning soft labels to the detected out of class samples using class conditional likelihoods from labeled data, and (3) using auxiliary batch normalization layers  to help mitigate the class distribution mismatch problem. Quality: This paper is well written and well organized. I find this paper easy to follow. Clarify: The preliminaries section clearly describes the setting of semi supervised learning concerned in this paper and also clearly describes the contrastive representation learning. [a] Chen et al.Learning to learn in a semi supervised fashion. It would be great if the authors can talk about whether the semantics oriented similarity representation in [a] could be used (and how to use it) to help improve the performance of the proposed method in the setting concerned by the paper.<BRK>The authors proposed to address the task of semi supervised learning (SSL) by contrastive learning techniques, and the proposed techniques can be applied to handle open set unlabeled data (i.e., the label spaces between label and unlabeled data are partially disjoint). I found the paper clearly written and easy to follow. The proposed learning scheme is extended from SimCLR by Chen et al.More precisely, the authors perform unsupervised learning using D_l (w/o observing labels) and D_u using SimCLR techniques. In order to handle out of class samples in D_u, the authors present the idea of learning class wise prototypical representation based on the above contrastive features. The use of such soft labels allows the training of such unlabeled and out of class samples, which would be the major novelty of this work. Overall, I feel that most of the technical components come from existing works (e.g., SimCLR from Chen et al.2020, auxiliary batch norm from Xie et al.2020.).The use of soft label assignments has also been proposed by existing works, which makes the overall technical contributions to be marginal. The lack of verification on the soft label assignment would be the concern as well.<BRK>##########################################################################Summary:The paper proposes a new approach for open set semi supervised learning, where there are unlabeled data from classes not in the labeled data. Then the paper filters outlier samples by the similarity measurement and further utilizes outlier samples with soft labels. The learned encoder can encode the semantic information into the feature for both labeled and unlabeled data in an unsupervised way. The paper filters the out class samples by the learned similarity measure based on a threshold, which can better filter out out class samples. The paper employs different batch normalization layers for the in class and out class samples, which avoids the influence of the distribution shift. The paper is well written and the claims are clearly clarified.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 7. rating score: 6. <BRK>This paper explores the weight sharing schema in one shot width search and proposes a locally free weight sharing strategy (CafeNet). Strengths:1) The paper is well written and easy to follow. The motivation is clearly explained by an example and the problem formulation. Such a solution for the previous fixed weight sharing seems sound. I have the following concerns and suggestions:1) Missing some relevant papers. As shown in experiments, β can be less than 1 and the second term in the right side of Eq.(9) is also less than 1.<BRK>The motivation and intuition are reasonable, which is to design a more flexible weight sharing pattern for network width search. 2.The free channels are the neighborhood of the c th channel in this paper, but I think more channels on the right should be included in the zone. Compared with conventional fixed weight sharing pattern where the leftmost channels are assigned as the sub network, the proposed locally free pattern increases more flexibility while the search space also scales at O(n).<BRK>The authors introduce in this submission a locally free weight sharing strategy for selecting effective network width. The problem studied here is also important and could be of interest to a large audience. + Experiments are sufficient. Weakness:  The proposed approach seems to be a  compromise between completely free weight and fixed weight, right? The influence of the super network should be detailed. The writing can be enhanced.<BRK>Moreover, this paper further proposes FLOPs sensitive bins to reduces the size of the search space. Compared with the manually ﬁxed weight sharing pattern, this paper proposes a locally free weight sharing strategy (CafeNet), which allows more freedom in the channel assignment of a sub network. More explanations are required. 4.Is it possible to find a sub network with zero width ($c 0$) for a layer? In Figure 2(b), the performance of the proposed method goes worse with the decreasing of the $\lambda$. In appendix A.13, “… and the bin evolving speed α in Section 3.4” should be “… and the bin evolving speed α in Section 3.3”.
Reject. rating score: 5. rating score: 6. rating score: 6. <BRK>## Summary of the WorkThe work analyzes the failure modes of fine tuning and joint training for multi task learning in a computer vision setting, then proposes a meta learning algorithm dubbed "MeRLin" to address these failure modes using bi level optimization. ## Pros and Cons### Pros* Principled approach to analyzing the failure modes of fine tuning and joint training### Cons* Requiring data from the target domain during training negates many of the sought after benefits of transfer learning (e.g.re using pre trained models quickly without lengthy pre training, adapting to unforseen new tasks, etc.) ### Significance2/5The analysis and synthetic experiments in this work are, I believe, a significant enough contribution to the field irrespective of the proposed algorithm. If the paper itself contained only extended versions of the theoretical and synthetic empirical experiment portions, I think it would be significant enough. "* I recommend moving this portion to the Appendix to make more room for justifying the method and better in text coverage of empirical results.<BRK>### SummaryThe paper investigates failure cases for transfer learning (fine tuning and joint training), specifically in the context where training on the source data may highlight features that are irrelevant for the target data. The authors also derive theoretical results on constructed data distributions for which superiority of Merlin can be shown analytically. This is not a problem in itself, but it makes it less clear, what exactly the authors think that the main contribution of the paper is. E.g.the BERT paper seems to have tuned on dev and then reported numbers on test, but I did not find dev set numbers in a quick search. Maybe this should be mentioned?<BRK>The authors generate a toy dataset, in which the source specific and transferable features are thus clearly distinguishable, for the transfer learning tasks. In order to investigate how a model (including a feature extractor and a classifier) interacts with the source specific feature and transferable feature within a data, the authors generate a toy dataset and examine the effectiveness of fine tuning and joint training methods. Some of these works also aim to learn the feature, which can be generalized to the target domain. I understand the problem settings of domain adaptation/domain generation are a bit different from the setting used in the paper, while such discussion can further improve the completeness of this paper. Overall, the paper presents an insightful study and a reasonable new method.
Reject. rating score: 3. rating score: 6. rating score: 6. rating score: 8. <BRK>In general, although the manuscript presents some interesting ideas, it makes many assumptions without providing clear bases for these assumptions (e.g.compressing the weights of a pretrained network to sample new weights is posed as a realistic approximation of the infant visual brain) and lacks a theoretical foundation for the claims and experiments that are presented. This is because to the best of my knowledge, no neuroscientist is claiming that a deep neural network is a complete and accurate model of the (development of the) primate visual system. It is not clearly explained why it is necessary to claim that the learning in these models and the development of the brain has to be similar for them to be good models of vision. At this point their approach sounds like arbitrarily selecting a set of criteria to make the networks perform worse than fully trained networks, and then training them. For instance, why would an infant brain be made up of a DNN with connections whose weights are initialized with the method authors came up with? I found it rather odd to not find any information about, for example, the proposed weight initialization method in the paper. It is not clear to me what is presented in Figure 1 and why. Why are the authors showing how models from another paper trains? The improvements of the results could be a coincidence, since the results are heavily dependent on one experiment.<BRK>I am keeping my original assessment of this paper as being borderline. The significance of this result in my opinion largely depends on how well we can map those observations and methods to biological meaning and knowledge on how primate brains are trained (see the discussion point below). (2) "squirrels to jump from tree to tree within months of birth", "macaques to exhibit adult like visual representations after months"   hoe many synaptic updates happen during those months? What would be a model (function?maybe a constant function?) Uniform?Recommendation and justification My main concern is with the interpretation of the meaning of this work. I suspect, however, that such an ablation study will show that there are ways to achieve high% of BrainScore using models that are completely dissimilar to the brain.<BRK>Pros:The problem that the authors present is an interesting one and undoubtedly useful for many applications. Cons:The authors take the analogy between "a developing visual system" and "training a model" a bit too far. It is not articulated how their method (WC) overcomes the critiques they raise against Frankle et al.2019.Moreover, claiming that WC achieves decent performance with "zero" synaptic updates is not fair. This seems to be closer to restoring pre trained weights than to random initialization (like KN). For CT, the authors choose "critical" layers to update. Is there a rationale (or a statistical metric) that justifies choosing these specific layers? The authors allude to the possibility of using "local" learning rules on a subset of layers identified by CT. However, this is speculation from the point of view of the current manuscript.<BRK>Summarize what the paper claims to contribute. This is done by combining three approaches, including reduced training, initialization of weights using compact distributions that describe trained weights, and updating only a minority of layers. Clearly state your recommendation (accept or reject) with one or two key reasons for this choice. I recommend acceptance. The number of updates needed to learn realistic brain like representations is a fair criticism of current models, and this paper demonstrates that this number can be greatly reduced, with moderate reduction in Brain Score. So I’m not sure its obvious how this relates to the weight compression scheme. The target neurons are already fully specified in CORnet S.  	Pg. Section B.1: How many Gaussian components were used, or how many parameters total? Or if different for each layer, what was the maximum across all layers? Section B.3: I wasn’t clear on the numbers of parameters used in each approach.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 7. <BRK>Summary: The paper proposes a fast method for solving reinforcement learning problems with constraints. The paper uses the reductions approach proposed in Agarwal et al, to transform the policy constrained problem to a mixed policy scheme in which finding a feasible solution is equivalent to finding a distribution over policies. Now, with this reduction, the paper leverages the fact that feasible set is guaranteed to be a polytope (given by the convex hull of all feasible policies), and hence the paper solves the equivalent problem of finding a point that minimizes the distance to the convex polytope. The minimum norm point algorithm or Wolfe s method is proposed to solve the distance minimization problem. In addition to the time complexity of the algorithm, they also provide a memory complexity bound which in some sense is inherited from the minimum norm point algorithm.<BRK>The authors reduce the RL problem under convex constraints to a distance minimization problem and solve the distance minimization problem with a Frank Wolfe type method (with an RL solver as a sub routine). The authors further show that the algorithm converges (in terms of approximation error), and validate some of their theoretical findings with simulations. There is a lack of connection to the original Wolfe s algorithm (such as what corresponds to the objection function and to the linear minimization oracle?Why the linear property of the RL oracle is important? How to choose the policy set in the real world application? + Some comments on the meaning of equation (4) should be helpful for the readers to understand the main flow<BRK>This problem was first considered in (Miryoosefi et al., 2019) and the authors of this paper propose a new algorithm and claim an improvement over the sparsity in the mixed policy. p*c(\pi) + (1 p)*c(\pi’)\in\Omega. The main idea of the new algorithm is to solve the problem as convex minimization of the squared distance to the target set. Therefore, if we view the two results under the same optimality measure, these two complexity results are the same. The reason why I still think it is marginally above the threshold is that, compared to the existing approach in (Miryoosefi et al., 2019), the Frank Wolfe type algorithm is way more natural and robust.<BRK>This paper presents a reduction approach to tackle the optimization problem of constrained RL. They propose a Frank Wolfe type algorithm for the task, which avoids many shortcomings of previous methods, such as the memory complexity. The method is basically different from the that of Miryoosefi et al.(2019).The improvement is mainly due to the algorithm design. The theoretical improvement is solid. The paper is well written. comments:  The algorithm requires a policy set $\mathcal{U}$ and finds a mixed policy $\mu \in \Delta(\mathcal{U})$ to satisfy the constraints. How to get a policy set with a feasible solution?
Reject. rating score: 5. rating score: 6. rating score: 7. rating score: 7. <BRK>The paper considers SMC to construct variational approximations. SMC methodscan be improved with partial rejection control (PRC). Then it is not obviousthat one can obtain unbiased estimators of the normalizing constant, as inplain SMC. The writing is mostly clear. The authors do cite that paper, but I did not understand exactlywhy that paper does not completely solve the problem that the authors consider. At first glance it seems that the Kudlicka et al paper would apply "out of thebox" when replacing the prior by an arbitrary proposal in SMC, i.e.using aproposal "$q_t$" instead of "$f_t$" and the ratio "$g_t f_t/q_t$" for the weights. Thus it was not clear to me that there s a need for another paper showing thatSMC with PRC can still provide unbiased estimators of the marginal likelihood. If the authors made a convincing case that the extension to general proposalswithin SMC with PRC requires significant work, their contribution would be moreconvincing. It is just p(a,b)   p(a)p(b|a). The latex command \eqref seems to have been used instead of \ref, in various places.<BRK>### Review update following author discussionI ve read the author responses as well as some of the discussion with the other reviewers. Overall, this is valuable work and I ve considered raising my score, but I think a weak accept is appropriate, all things considered. I think a key strength compared to prior work is the empirical validation of the approach on a variational RNN. However, the significance of the paper in terms of the novelty of the ideas, both conceptual and technical are overstated in my opinion, hence the weak accept. in BRPF [Schmon et. al, 2019] as the "Bernoulli Race" (which the authors have cited). I would think that the current scheme is a special case of the "Bernoulli race" that uses a particular form of the acceptance probability parameterization similar to prior work (e.g.VRS [Grover et. The way it s written makes it appear as if there s a circular depdendence of the $c_t$ on $z_t$ and then the $z_t$ is again resampled, which changes the $c_t$. The experimental results compare favorably to prior works like FIVO and IWAE and demonstrate that using partial rejection control is beneficial in a variety of benchmarks. ### Additional Comments* The proof of unbiasedness (Prop 2) says "it is easy to show that Eq (15) is an unbiased estimator" and refers to prior work by [Naesseth et. More clarity here would be helpful (especially around the term $Q_{VSMC PRC}$) for assessing this claim, given that this is one of the main contributions claimed in the paper (besides also commenting on where the bias in prior work is coming from). You might want to introduce this around Eq (4), where the integral appears.<BRK>Positives:The combination of partial rejection control and dice enterprise for variational inference is new and interesting. However, using a dice enterprise step allows for a new unbiased bound which makes it possible to consider a lower bound on the log likelihood via variational ideas that can be optimized with standard techniques. Empirical experiments suggest that the method outperforms previous work. This seems to be not accounted for in the experiments. Choosing larger N 16 also has a better performance in the FIVO paper. Recommendation:I vote for acceptance of the paper. However, I think that the experimental section should be improved. Comments:Variational bounds can also be constructed by targeting a smoothing distribution (Lawson et al, 2019) and particle filters with complexity N^2 based on a marginal Fisher identity have been suggested (POYIADJIS et al, 2011) for parameter estimation that avoid estimator variances scaling quadratically in time. I was wondering if there is a connection between such filters and the method suggested here, particularly for K N?<BRK>The authors consider a completely general setting (the authors assume Eq.(1) but clearly there is nothing to assume here, this the standard Bayes rule). It is well known that the vanilla SMC sampler is a good candidate for ELBO because it provides an unbiased estimator of the likelihood. It is difficult for an expert in SMC algorithms to understand the algorithm as it is described (one must even guess the meaning of the notations). Also, the form of the rejection probability does not help to understand the action beyond the scene. The authors suggest (similar to Peters (2012)) to use a Monte Carlo estimator of these quantities. The results presented are encouraging and shows that the proposed approach outperforms IWAE and FIVO for a given calculation time. This result is clearly interesting and shows that partial rejection helps despite the additional difficulties linked with the intractability which requires an additional layer of complexity. I like the paper even if I have found it extremely unfriendly to read !
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>The authors proposed an improved deep learning based representation learning method that provides more efficient features for clustering analysis. (1) According to the comparison experiments on several widely used datasets,  the integration of a softmax formulated orthogonal constraint is able to provide more stable latent feature representation. (3) I was deeply impressed by the far above state of the art values of evaluation metric of this proposed representation learning method. Besides, adding some  real  visualization results existing in the original image space rather than the latent space could help to illustrate if the proposed method could mine visually meaningful concepts from the view of visual contents.<BRK>This paper proposes a clustering friendly representation learning method using instance discrimination and feature decorrelation. Instance discrimination loss and feature decorrelation loss are combined to optimize the network. The paper is well qritten and experimental results are good. I have some questions about this paper:1. There is no ablation analysis about the two loss terms in Eq.(6). What about the contributions of the two loss terms?<BRK>The other is instance discrimination. Besides the stated contributions, I thought there were a number of other positive aspects of this. A) I thought that the spectral clustering connection was nice and I am glad the authors included it. C) Finally, the evaluation clearly shows the benefit of their contributions in terms of performance. There are a number of questions I have with the work as is. This is particularly important to understand in unsupervised settings. Other works in the literature have explored this.
Reject. rating score: 4. rating score: 6. rating score: 6. <BRK>b) Among these generalization results, an unchanged term is the distance of representations between original samples and the ones after transformation. This is aligned with the conclusion in the literature. Weakness:a) My most concern lies in the validity of the assumptions, specifically A2, A5, and A6. Besides, it seems that the framework cannot incorporate the transformations in the adversarial attack, which however is very important and cannot be ignored. For any distribution, the " " (rather than "\approx") seems too strong. b) As stated in the i) in a), this paper may not incorporate the transformations in the scenario of adversarial attack. Besides, the definition of the invariance is only restricted in the scope of considered transformation; correspondingly, the robustness of the paper only lies in the interpolation rather than extrapolation [1]. It is more meaningful to consider the invariance and robustness for general out of distribution settings, such as [1] and [2]. Besides, how s the improvement margin if the batch normalization, default augmentation are implemented? d) The theoretical analysis barely have some novel insights, since most of them are simple derivations of existing results.<BRK>This work seeks to provide a general understanding of how we should train with augmented samples in order to learn robust and invariant models from both theoretical and empirical perspectives. One issue, though, is that the set of possible data shifts is known beforehand and the best performing RWA method must see all the augmentation functions. For example, training with texture and rotation augmentations but testing with the contrast augmentation. Clarity: It is not easy to follow, especially Section 3. There are in total 6 assumptions used in this work, but the authors decided to spend significant space on half of them and put the rest in the supplementary material without any explanation in the main text. Also, it might be better to give an informal illustration of the theoretical results before giving the main theorems so that it would be more likely to be appreciated by a larger community. On the other hand, there is a gap between the theoretical analysis and the empirical verification. Since the empirical analysis in this work in mainly driven by the theoretical work, some discussion regarding the use of squared l2 norm would make the work stronger, especially when "the ideal regularization" does not work well. Some minor issues: i) I think the symbols used in the last sentence in A1 would be difficult to understand for most of the community, therefore it might be better to explain it in word; ii) I think it should be "argmin" instead of "argmax" in Theorem 3.3.<BRK>For this purpose, they evaluate if  models trained with augmentation are generalizable and their associated regularization techniques. The paper tackles an important issue of training well with augmentation. 1.The theoretical analysis is strong and interesting. While a number of related work is discussed in the Related Work section, it has not been considered in the experiments section. It mentions constant references to the appendix in the theory, without being complete in itself. It is hard to separate the paper s proposed techniques from baseline models, for example, VWA and RA are well known methods in the literature. It misses standard deviation results as well.
Reject. rating score: 5. rating score: 5. rating score: 7. <BRK>The paper studies fair classification by using the notion of Exponential Renyi Mutual Information. A classification algorithm with low value of mutual information between the prediction and the sensitive attributes can be considered as fair. The paper establishes that the Exponential Renyi Mutual Information is a strong notion to ensure fairness: the authors show that this notion is stronger than many existing notions such as Lq fairness, etc. The authors propose an algorithm to train a fair classifier, with the mutual information being penalized in the objective function. Weakness:  I have some concerns with the mathematical notations in this paper. The mathematical definition in equation (2) does not seem to segregate the values of Z. Moreover, why is the expectation taken over Z when we already condition on Z \in \mathcal Z? The authors show that ERMI is stronger than existing notions, which is nice. However, it is not clear why a stronger notion is preferable for the penalized optimization of the form (11). One can think of penalizing the Shannon mutual information with higher penalty parameter lambda, and one may expect to see similar end results as problem (11)   especially if we plot the accuracy fairness tradeoff similar to Figure 1. I think this lemma should be put as discussion in the paper, and not a separate lemma. Why are the expectation in equation (15) taken with X? For example, I think there is a dependence of \hat Y_\theta on X which is not made explicit.<BRK>Summary: This paper introduces a new fairness notion that takes an exponential form of Renyi mutual information. The performances of the methods are then demonstrated on three datasets and compared with some prior algorithms. An interesting observation is made via Theorems 1/2/3: ERMI is an upper bound of other popular notions. I believe the tightness analysis is more important, as a loose upper bound plays a less role. 3.Experiments: Some baselines based on mutual information are missing, e.g., [A], [B]. Also in the 1st experiment (Fig.1), (Baharlouei et al 2020) is missing – I think it is needed to compare, since the notion estimation method may be different although ERMI is equivalent to Renyi correlation. Moreover, it is not clear which algorithm to use in all experiments between Algorithms 1 and 2. [B] J. Cho, G. Hwang and C. Suh, “A fair classifier using mutual information,” ISIT 2020. Clarity: Overall it is well written and easy to follow. Why italic in the last sentence in the 2nd paragraph on page 1? iii.A benchmark dataset, COMPAS, is missing. iv.Not clear why the proposed algorithms offer greater performances. v. Figure 2: Only RFI is compared, although many other baselines can readily be extentisible to the non binary classification. iv.Figure 4: Not clear as to what points the authors wish to make here.<BRK>The authors provide a unified view of the existing fairness violation notions and propose a new notion: Exponential Rényi Mutual Information (ERMI) between sensitive attributes and the label. ERMI is easy to compute and provides an upper bound on existing notions. Based on ERMI, the authors propose a framework, FERMI, which can be optimized with SGD with convergence guarantee. In experiments, results show that FERMI leads to a better tradeoff between performance and fairness even if fairness is not measured by ERMI. My major concern about the methodology is that it depends on the quality of the empirical estimate of the probability distributions P(s), P(y) and P(y,s). In the experiments, these distributions are relatively simple as S takes either binary values or are samples from Gaussian distribution. I wonder how the performance of the FERMI would be influenced by the quality of estimates of the three probability distributions. It would be interesting if the authors can show results on the estimation error of the three vs. the performance of FERMI. In section 3, the authors show that ERMI is an upperbound of several popular notions: Shannon MI, Renyi correlation, and Lq fairness violation. I have several suggestions and questions: (1) It would be better to specify whether some of them are known before this work. (2) In addition, is there any other transformation of Renyi Mutual Information that can achieve the same goal? I would appreciate it if the authors can clarify this point.
Reject. rating score: 5. rating score: 5. rating score: 7. rating score: 7. <BRK>SUMMARY#######The present paper proposes a study of the tradeoff between the number of neurons $k$ and the Lipschitz constant $L$ of a 2 layers neural network $f$ that fits a training dataset, i.e.$f(x_i)   y_i$ for all $(x_i, y_i)$ in the training set. To that end, authors consider "generic datasets" such that the marginal distributions of the input random vector $X \in \mathbb{R}^d$ and the random label $Y$ are the uniform distributions over the $\mathbb{R}^d$ sphere with radius 1 and the set $\{ 1, +1\}$ respectively. Authors essentially formulate two conjectures (in the following constants are omitted for the sake of simplicity):(1) If $f$ is a 2 layers NN with $k$ neurons fitting the data, then its Lipschitz constant $L$ satisfies (w.h.p.): $L \ge \sqrt{n/k}$. (2) If $n/d \le k \le n$, then there exists an activation function such that w.h.p. More generally, considering a potentially very large shallow network reminds the structure of a kernel function, with infinite dimensional feature map. The idea of considering one neuron/support vector per datapoint, as suggested by authors to ensure a $O(1)$ Lipschitz constant is also a key feature of these nonparametric method. Conjecture (2) is first proved in the case $k 1$, and $k n$. For an intermediate value of $k$, only $L \le n \log(d) / k$ is proved. As for conjecture (1), it is proved for the Lipschitz constant over $\mathbb{R}^d$ (which is by definition bigger than that over the sphere) in two particular cases: (a) $n d$, activation function   ReLU, and no biases, and (b) activation function   $t \mapsto t^p$ and no biases. Two synthetic experiments are finally provided. I feel that sticking with the latter would be less misleading, as very few is said about robustness, apart from that aspect. This 2 layers architecture seems very restrictive, all the more that the second layer is simply linear. b) about generic data, I had to assumed that authors were speaking about marginal distributions when referring to "with $x_i$ uniform", this should be better explicited. Again, recent understanding on the generalization capacity of neural networks rather consider that inputs are very structured, and actually concentrated on a lower dimensional manifold. The constructions could have simply been mentioned in a sketch of proof of Thm 1, the central result of this Section, that incidentally does not match the conjecture. It basically proves nothing as it applies to an upper bound of the Lipschitz constant. Although I agree it makes the analysis much easier (feasible), this fundamentally breaks with all that is thought to be responsible for the good NN learning capacity, making the subsequent study less convincing. this brings me to the next point, which is the comparison to kernel methods.<BRK>This submission studies the relationship between the hidden layer size of a two layer neural network and its robustness, that is measured by its Lipschitz constant here. This paper first makes a conjecture that any two layer neural network with k neurons and Lipschitz activation functions that perfectly fit the data must have its Lipschitz constant larger than \sqrt(n/k) (with n being the number of data points and k the number of hidden neurons). An implication of this conjecture is that overparameterization can help improve the network robustness (namely, by making \sqrt(n/k) with sufficiently large k). Furthermore, a weaker version of this conjecture was proved by replacing the Lipschitz constant with an upper bound on the spectral norm of the weight matrix. The conjecture was further proved for ReLU activation function and polynomial activation function in different data regimes. These theoretical findings were finally evidenced with numerical results. The paper is also sufficiently well written; at least the main technical ideas are easy to follow, but there are several grammatical errors, some of which are listed below along with several major comments. Link between this notion of neural network robustness and the neural network Lipschitz constant is not clear. ii) The probabilistic statements could be made more precise in the conjectures. What kind of data are generic data? iv) Both "two layers neural networks" and "two layered neural networks" are used in the paper. It seems more grammatically reasonable to use  two layer neural network.<BRK>**Summary**: In this article, the authors investigated the fundamental trade off between the size of a neural network and its robustness (measured by its Lipschitz constant), in the setting of a single hidden layer network with $k$ neurons and (approximately) Gaussian data, by proposing two conjectures, Conjecture 1 and 2, on the (lower and upper bound of the) network Lipschitz constant in perfectly fitting a given data set of size $n$ and data dimension $d$. Some weaker versions of the two proposed conjectures were proven, in Section 4 and 3, respectively. **Strong points**: This paper proposed a clear and promising mathematical conjecture to investigate the robustness in neural network models, and provided many examples and explanations on why such conjecture is reasonable. The paper is in general well written: the simple examples in Sec 3.1 and 3.2 make a clear sense of the proposed theory, and solid technical contribution is provided in e.g., the proof of Theorem 2. **Recommendation**: This is a good paper that made solid contributions to the theoretical understanding of the fundamental trade off between model size and robustness. I recommend it for publication at ICLR.<BRK>This paper concerns the Lipschitz constant of two layer neural networks that fit "generic data sets" exactly. A key contribution is the statement of two conjectures relating the number of hidden neurons in the network to the Lipschitz constant. Roughly, to have $O(1)$ Lipschitz constant, the authors conjecture the number of hidden neurons should be on the order of the input dimension. In a slogan, over parameterization is needed for robustness. Proofs are provided for various special cases, relying on results about real symmetric tensors and uniformly random points on the unit sphere. Overall, this is a very interesting paper. For such data sets, it somehow does not surprise me so much the fitted function should have a high Lipschitz constant... If possible, the authors could add intuition on why these data sets may have similar qualitative behavior to more structured data sets. Page 3, Conjecture 1: Do the authors expect the Lipschitz constant needs to be similarly high if $f$ is only required to approximately fit the data?
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. rating score: 4. <BRK>The paper also proposes Episodic Planning Networks (EPN), an RL method that replaces a weighted sum and multi layer peceptrons in memory networks with self attention. The paper formulates RTS as an extension of meta reinforcement learning framework, where in addition to optimizing over a distribution of tasks, the objective is to also optimize over a distribution over environments. I think the work proposed in the paper is an important step towards developing general agents that learn to adapt quickly. To that end, it would be beneficial for the authors to release their code to benchmark new and older methods.<BRK>The authors propose a non parametric memory based on the transformer/self attention architecture to learn over tasks that require planning from previously experienced tasks. They describe this style of learning as a form of meta reinforcement learning where individual episodes are a collection of tasks in the same environment and individual environments are sampled from a  meta  environment. This is very similar in spirit to the RL squared framework of Duan et al.2017.As is typical of meta learning environments, there is an outer loop and an inner loop.<BRK>### SummaryThe authors present the challenge of rapid task solving (RTS), where the goal is to solve a series of tasks as quickly as possible in a new, shared environment. This challenge requires both memory and planning capabilities from an agent, and the authors demonstrate that current SOTA modern deep RL agents with memory components still fail at this challenge. Are there other techniques in meta learning that could be applied here without an explicit architecture design of episodic memory or a planner? It demonstrates strong empirical results compared to previous SOTA in episodic model based control. The performance measure reported by the EPNs seems a bit suspect.<BRK>The paper is presenting a Transformer based architecture for episodic memory based sequential decision making. The authors are interested in solving RTS and perform experiments that are focused on the case of mapless navigation. So, beyond parametric variation of tasks, as presented here, a good improvement could be to evaluate this approach on non parametric variations of tasks as proposed in the manipulation framework of the Stanford meta world [4].
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 6. <BRK>Summary of the paper:The authors analyze the effect of sources of uncertainty on neural network performance. The authors use statistical measures such correlation between model predictions, change in performance with and without ensemble game models, and a state of the art method to characterize the functional behavior. 3.Experiments have been conducted on diverse domains (image and language) to demonstrate the effectiveness of the proposed method. Moreover, the different sources of non determinism may be influencing each other. Overall comments:The paper offers some interesting insights via experiments. Some aspects about the protocol metrics are intuitive as the authors explain, despite this, a theoretical analysis to back the experimental findings would have made the paper stronger. This is because it is hard to be convinced that the process of attribution can be established entirely based on statistical measures.<BRK>This paper discusses nondeterminism and instability in neural network optimization. The authors establish an experimental protocol to understand the effect of optimization nondeterminism on model diversity, and study the independent effect of different sources of nondeterminism. Cons:1.The accelerated model ensembling is from another work and using ensemble to reduce variability is quite intuitive. 2.More experiments need to be done on larger datasets to further demonstrate the findings of this work.<BRK>The paper investigates the effect of nondeterminism and stability in Neural Networks (NNs) for supervised learning tasks in a systematic manner. Is there anything more fundamental about the particular value of variability that all the different sources of nondeterminism concentrate around? causes similar levels of variability (based on standard deviation and correlation metrics), and 2) Changes in the optimization even in the order of 10^ 10 in a single weight can have same variability level as changing the random seed entirely. Although the work is independently interesting and opens up new questions for the field it also seems to be a great motivational section to develop Snapshot Ensembles which has already been published.<BRK>The authors establish an experimental strategy to analyze the different sources of nondeterminism. By modifying weights by a single bit, they experimentally demonstrate that an inherent instability in the neural network optimization procedure is the main reason. Why were 500 epochs used for the CIFAR experiments? I think the results (such as Table 1) are extremely interesting and should be known in the community, however, the current analysis of the optimization instability should be developed. Furthermore, the experiments could be improved as described elsewhere in this review.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 5. <BRK>## SummaryThis paper provides a novel algorithm to estimate the optimal value of $n$ for $n$ step temporal difference methods. ## Review### SummaryI am recommending a reject for this paper. However, the paper misses a discussion on the well understood bias variance tradeoff present with $n$ step methods. I do not find that the paper provides any clarity or novelty in the _understanding_ of $n$ step methods. This trade off results from balancing between bias and variance of the estimated returns. While adding shifting policies and considering the off policy setting does yield an interesting conversation on bias, the variance simply cannot be ignored.<BRK>This paper proposes an approach to adapting the n parameter in n step returns according to the off policyness of the sampled transition. "In summary, as the policy age grows, the off policy bias increases linearly ..." where do you show this? Overall, I enjoy the approach taken here, and I think it deserves a place in the landscape of deep RL research. However, I m not sure that the paper is of sufficiently quality to be accepted as is. The empirical results are interesting, but would benefit from being substantiated. There are a number of issues here which I m curious to hear your comments on.<BRK>The paper deals with an intriguing point in RL   how to correctly choose an adaptive $n$ for $n$ step bootstrap. From such a paper one might expect theoretical results on the tradeoff between bias and variance in the presence of off policyness. I believe the research question at hand can highly benefit from rigorous analysis, but other than the rudimentary Eq.(3) and the few equations that precede it there are no established principles according to which one should derive a formula for $n$. Instead, the argument that is given for choosing it regards the  age  of the policy. Based on stochastic approximation analysis, only asymptotic results on convergence exist or finite time analysis that doesn t assume monotonicity. In terms of experiments, it seems that the proposed algorithm works well.<BRK>### EvaluationAlthough the paper presents interesting observations and some promising experimental results, I recommend rejection. Besides, the paper is not well written, and experimental methods are a bit inappropriate. ### Paper SummaryUncorrected N step return is very effective and frequently used in off policy RL algorithms, such as R2D2 and Ape X. 3.The abstract says a critical issue of uncorrected N step return is that one needs to choose $N$. This made me a bit skeptical of the SAC implementation of the paper.
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 7. <BRK>By the way, it is too bad that the result is a bit weaker for the variance, which is precisely the quantity you care about for defining your rewards. I like that the empirical work was improved, including adding some comments about significance. However, the paper in its current form is not yet ready for publication for two main reasons. First, there are significant gaps in motivating and detailing the TDU technique. In sections 1 and 2, TDU is motivated as an exploration method deriving “an intrinsic reward from the agent’s uncertainty over the value function.”  In particular, it is heavily implied that TDU’s uncertainty estimate does not suffer the bias indicated by Lemma 1. ” This is a strong statement. The theoretical contribution seems like it could be interesting, but it is not fully clear. Is this a statement about all methods that try to estimate some form of value uncertainty?<BRK>This paper proposes a method for exploration in reinforcement learning by using the uncertainty over the value function as an intrinsic reward. It also offers an interesting theoretical analysis on the problem of estimating uncertainty over the value function. This is actually my main question/concern about the paper. I think it will improve the paper a lot if the authors add such environments to their analysis and discuss such problems. Based on the other reviews and authors  response I decrease my score by 1 point.<BRK>Summary:The paper proposes a novel heuristic approach to estimate the uncertainty of the value function of an agent. Since this heuristic for estimating the uncertainty can be applied not only to tabular RL, but also when using function approximation, it is an interesting approach. It introduces a new approach that is likely to be beneficial in a relevant number of problems. I recommend to accept the paper. It remains unclear, what is meant by "diverse and deep exploration". After reading Review1 I lower my Confidence. (Nov 29.)Taking into account the other reviews, the authors  responses to these reviews and the discussion, I now think the paper is not quite ready for publication and lower the score to 5.<BRK>The main difference with similar methods is that the value uncertainty is not used in a Thompson sampling scheme but it is instead use to provide exploration reward. Scientific quality:  The new method is generally well motivated. However, while the author use a Bayesian terminology (e.g.prior, posterior), I am not convinced that the proposed bootstrap method for uncertainty estimation has a clear Bayesian interpretation as a form of approximate inference. The experiment section contain an interesting set of experiments and compare with several relevant baselines. The paper is clear and very well written. However, I am not convinced by some of the claims. While the claim can definitely be true, I do not think it follows from their premises. I would like to see the authors to expand this argument.
Reject. rating score: 3. rating score: 5. rating score: 6. rating score: 6. rating score: 7. <BRK>This paper develops a stochastic MM type algorithm to minimize a finite sum. Essentially, the stochastic method draws one sample at each iteration, and find a majorization surrogate for the corresponding loss, and find the minimizer for the updated total loss. This step is computationally as expensive as the update step in a batched MM algorithm. 3.The convergence rate of the proposed method is not derived, which shouldn t be too difficult to derive.<BRK>This paper proposed MISSO, which is an extension of MISO to handle surrogate functions that are expressed as an expectation. MISSO just used the Monte Carlo samples from the distribution to construct objectives to minimize. It seems to me that MISSO is just a straigforward extension of MISO, also the empirical results seems to suggest the proposed MISSO has no advantage over Monte Carlo variants of other optimizers, such as MC SAG, MC ADAM, thus it is not clear to me what is the significant aspect of this work.<BRK>1.Summarize what the paper claims to do/contribute. The MISO is a majorization minimization algorithm, which shares a similar update style of the SAG method. However, different from SAG, whose convergence is not available for nonconvex optimization, and is even very tricky in convex case, MISO enjoys a global convergence guarantee due to its majorization property. The convergence of the proposed algorithm is also provided in this paper. 3.Provide supporting arguments for the reasons for the decision. (ii).(Strength) This paper provides a non asymptotic rate of convergence for the MISSO algorithm, which implies a non asymptotic rate for the MISO method, whose non asymptotic rate is not known before, which should be appreciated. Make it clear that these points are here to help, and not necessarily part of your decision assessment. Or, if such discussion exists in other literature, add a reference to that. Specifically, by the reviewer’s rough estimation, the dependence on n and L is O(n^3L^3), see my argument before, this dependence is not reasonable.<BRK>This manuscript contributes a stochastic optimization method for finite sums where the loss function is itself an intractable expectation. It builds upon stochastic majorization minimizations methods, in particular MISO, that it extends to use Monte Carlo approximation of the loss. However, I believe that these theoretical results are not enough to situate the contribution with regards to the wider landscape of optimization methods for machine learning. In this respect, the empirical study is crucial, however it is not completely convincing. **Additional comments after the discussion**The authors have thoroughly replied to all the comments from the various reviewers. After reading all the discussions (other reviews as well as replies from the authors), it appears to me that the practical relevance of this contribution is not completely clear.<BRK>This paper propose a doubly stochastic MM method based on Monte Carlo approximation of these stochastic surrogates for solving nonconvex and nonsmooth optimization problems. The proposed method iteratively selects a batch of functionsat random at each iteration and minimize the accumulated surrogate functions (which are expressed as an expectation). S2.The paper contains sufficient details of the choice of the surrogate function and all the compared methods in the experiments. I found the technical quality is very high.
Accept (Poster). rating score: 8. rating score: 8. rating score: 5. rating score: 4. <BRK>***Summary***I would firstly like to thank the authors for an interesting read. I enjoyed going through the submission very much. The authors propose to understand the qualitative effects of nonlinearities by studying the impact they have on the Fourier spectrum of deep neural networks. Therefore I maintain that this is still a clear accept. I think the balance of text to mathematics in the main submission was about right, reserving the appendix for a more in depth discussion. I like the use of the Fourier spectrum to show this and the analysis behind how the spectra of various nonlinearities affect overall network smoothness.. Will there not be issues with the Gibb’s phenomenon? This would not necessarily lead to blue shifting. I would have liked to have seen a deeper analysis of this effect.<BRK>The papers proposes an interesting analysis that links several aspects of architectural design in Deep NNs to the spectral analysis and observed roughness. Theoretically, the paper shows that there is a concrete link between architectural choices in the network design and the blueshift in the frequency domain. Moreover, abbreviations would be better used in a more uniform manner (e.g., SDFA, FDSA, SDSA). The starting point, which is not novel, actually, but relevant, is that specific types of non linearities introduce harmonic distortions, and the effect is potentially amplified when multiple non linearities are stacked.<BRK>Summary: The paper applies harmonic distortion analysis to understand the effect of nonlinearities in the spectral domain. This prediction could be related with the data: find the one nonlinearity and architectures that could best fit the data complexity. Strong points: The paper introduces an interesting measure "roughness" of deep neural network via harmonic distortion analysis. The blueshift fits people s intuition of the architectural choices.<BRK>Summary: This paper proposes a new approach for how to analyze the ruggedness of the surface of the neural network loss. Specifically, the paper proposes to apply harmonic distortion on the weight to output (w o) maps. The  paper shows that non linearities are responsible for blueshifting with deeper layers, that is for "transferring more energy" on the higher frequencies. The consequence is rougher surfaces, as well as higher frequencies for gradients, which can lead to exploding gradients in the deeper layers. There is an attempt to connect exploding gradients to blueshifting. Or some other phenomenon. This is how depth is  qualitatively  introduced as a variable. Is this indeed the intention? I have the feeling there are places where the analysis is imprecise, although it could be that I also misunderstood.
Reject. rating score: 2. rating score: 4. rating score: 4. rating score: 4. <BRK>`The paper argues that the existing way of using Translation Memory (TM) in neural machine translation (NMT) is sub optimal. The main concern of this paper is that, the contributions are quite limited. The authors claimed three contributions: n gram retrieval, universal encoder, and using the copy mechanism. E.g., In the contribution part in the intro, the second and the third items start with "does" and "apply". This paper seems a very hurry combination of some existing techniques. I basically learn nothing new from reading this submission. Further analysis of the proposed model would provide greater insight to the community. In this way, the adaptation ability of TMG NMT could be better proven.<BRK>** Summary **(1) The authors proposed a translation system with an external memory. (2) Specifically, in the encoder side, the M BERT model is leveraged to jointly encode the $x,tx,ty$. (3) in experiments, the $n$ in  n gram is set as? Compared to the related work, the novel part of this work is: (i) a new retrieval way, which is not quite clear and convincing to me. Similar idea also exists in [R3], which is missing from this paper. The differences with [R3] should be discussed. "Incorporating bert into neural machine translation."<BRK>This paper describes several improvements on using information from a Translation Memory (TM) in Neural Machine Translation (NMT). This is not clear. It essentially leverages the idea of reusing close matches to a source sentence in the training data. Despite the general relevant and interesting focus of this work, there are a number of issues discussed below, related mainly to modeling and to the experimental evaluation. I basically don’t buy the advantages put forward in the paper:  The cost of retrieval in a TM is dominated by the requirement to go over the entire memory for each source sentence, not by the computation of the score. The n gram matching would still incur that cost, unless some smart way to retrieve matching sentences (such as an inverted n gram index) is implemented. This should be clarified. 2) The encoding is straightforward but clever.<BRK>This paper presents a way to integrate a translation memory into a neural machine translation model. My major problem with this paper is that the model and the experiments are not adequately described. They claim that any sentence in the TM that matches any n gram in the sentence x. This would result in a huge set of matching sentences, where 1 gram that matches could be the work "the". This is a fundamental part of the model.
Reject. rating score: 3. rating score: 4. rating score: 4. <BRK>Summary:This paper proposes a new approach for multi task learning that estimates the individual task weights through gradient based meta optimization on a weighted accumulation of task specific model updates. Evaluations are performed in a multi task learning setup on tasks related to computer vision (Multi MNIST) and natural language understanding (tasks from GLUE and SuperGLUE). Pros:1) The paper is easy to follow. It is not very clear in the discussion how the alpha is different from task specific weights. 2) In general, for a multi task setup, I would expect to show the multi task learning with multiple auxiliary tasks (that’s the main motivation of this paper as well). However, the choice of the experimental setup is convincing, especially for the vision domain there is only one auxiliary task. Further, the experimental results suggest that the proposed algorithm performs more or less similar to previous methods. I would also suggest the authors to perform more experiments and ablations. In algorithm 1, if you replace deltas in line 10 with line 6, then there is no need to have separate alphas and task specific weights? 2) Please provide statistical significant scores for all the results.<BRK>Summary: This paper presents an algorithm for multitask learning that learns task weights via an EM like approach that alternates between updating the model parameters (using task weights) and updating the task weights (using current model parameters, based on the target task development set). The proposed algorithm has the beast mean performance, but results are within a standard deviation of the baselines. The proposed is quite ad hoc, and with little justification, it’s not clear why we should be doing any of the things the algorithm proposes. From a novelty perspective, I’m not convinced the proposed method is different enough from existing methods. The results from the experiments are not convincing to me. On MNIST, the results between all methods are fairly close together, and on the NLU tasks, there’s no clear best algorithm. For the “standard multitask baseline”, are the tasks balanced in size? This is mostly relevant for the NLU tasks, which have fairly different sizes. 2.On MNIST, given that the algorithm sets the weight of one task to 1.0 and the other to 0.0, why is this algorithm outperforming single task training?<BRK>This paper proposes a novel multi task learning method which adjusts task weights dynamically during training, by exploiting task specific updates of the model parameters between training epochs. Specifically, the proposed model takes the differences between the model’s parameters before and after the singletask update, after that the mixing factors of the model updates are found based on the differences to minimize the loss on the target task’s development data. The paper is well written and easy to follow, the authors summarize the related work in a clear manner. The proposed methodology is intuitive and well motivated, in the meantime, it is flexible and can be generalized to other variations in terms of models and tasks. 2)	Considering the lack of a theoretical justification, the experimental results are not convincing enough to justify the proposed method.
Reject. rating score: 4. rating score: 4. rating score: 7. rating score: 9. <BRK>## SummaryThis paper provides a spectral analysis of Lookahead dynamics. This paper’s main results state that lookahead dynamics can improve a given method’s convergence rate if the Jacobian’s operator has eigenvalues with non zero imaginary parts and an imaginary conditioning (ration between the largest and the smallest imaginary part) smaller than 3. Overall I think that the idea of this paper is exciting and is motivated by recent empirical observation. However, the theoretical results of this paper are relatively weak because of some restrictive assumptions. ### Cons The theory is only developed in a very restrictive setting (Bilinear with condition number smaller than 3 and general games with non real eigenvalues and imaginary condition number smaller than 3) where the optimization problem is easy to solve. (a small condition number corresponds to an easy optimization problem.) The experiments are relatively weak since they are only on well conditioned bilinear games, a 2D non linear problem, and do not explore the necessity of the assumptions of the theorems of the paper. ## Questions/comment:   It seems to me that the condition that the Jacobian has non real eigenvalue and $\frac{\mathcal I_\max}{\mathcal I_\min} < 3$ is an artifact from your technique proof because lookahead does converge in the context of minimization (only real eigenvalues). Though in the minimization case, it seems quite direct to see that look ahead slows down the dynamics $ 1   \alpha + \alpha \lambda > \lambda \, \forall 1>\lambda >0$ the interesting property of lookahead highlighted by this work is that it can positively affect the impact eigenvalues with a large imaginary part on the convergence. Since in games, it has been shown that eigenvalues with a large imaginary part may slow down the convergence of games [Mescheder 2017], lookahead seems to be a promising direction to improve the convergence rate of the gradient method. Overall, I think that this paper would benefit from a more precise analysis of the convergence rates. Then answer the following question: can we find some problems where lookahead provides a significant improvement in terms of convergence rate (against standard methods such as Extragradient or Gradient). The concept of acceleration usually refers to a significant improvement of the convergence rate (see for instance [Azizian et al.2020] for a discussion on acceleration on games.) In the case of Theorem 3 it would mean $\rho(f(X)) << \rho(X)^k$. Also, I am not sure about the relevance of the “local stabilization properties”, usually one may want to diverge from certain points where $\rho(\nabla_x F(x)) >1$ e.g., local maxima. Regarding your experiment, you could try to test if the conditions in your theorem are necessary:Can I find hyperparameters to make lookahead converging even when there are real eigenvalues or a condition number larger than 3? AISTATS (2020).<BRK>This paper studies the theoretical aspect of lookahead dynamics of smooth games, inspired by the recently introduced Lookahead optimizer, in the spirit of studying the game dynamics of multiple agents. The overall writing of the paper is not clear enough and should be organized better. * environments should also be omitted. Pros:   Possibly novel theoretical study of the game dynamics driven by the recently proposed lookahead optimizer. And the main text does not contain any pointers to the supplemental material for the list of mathematical notation. How is it different from $ \mathbb{R}^{m \times m} $? Or does it stand for a matrix group like $ \mathrm{GL}_m(\mathbb{R}) $? I also think the principal argument $ \mathrm{Arg} $ should be used instead of vaguely defining $ \mathrm{arg} $ which is multi valued in complex analysis. Some of the propositions and theorems in the paper are not mathematically precise or rigorous enough. E.g., proper choice of $ k $, small enough $\alpha $, etc. Without making the statements precise, I found some theoretical results in the paper vacuous and hard to interpret. The numerical experiments are not sufficient. Results of GAN optimization are expected as in other papers in this line of work, in order to demonstrate the full effectiveness of the proposed scheme. The figures, especially Figures 2 and 3, seem to be of quality not up to publication standard and unclear. Despite my unfamiliarity with this line of work, I think this paper needs to be improved before acceptance, and I suggest rejection for the current state of this paper.<BRK>### SummaryMotivated by the empirical success of lookahead in GAN training, the paper studies theoretically the convergence behavior of the algorithm in smooth games. In particular, the authors focus on bilinear games and local convergence around equilibrium points and derive sufficient conditions under which lookahead improves upon its base dynamics (by either stabilizing a non convergent algorithm or accelerating a convergent one). ### ProsThe lookahead optimizer studied in this paper is relatively new and worths more theoretical investigation. In this spirit, this work consists in, as far as I am aware, the first attempt to understand theoretically the performance of lookahead in game optimization. The results concerning the potential stabilization and acceleration achieved by the lookahead mechanics provide us some insight into the reasons for its empirical success. ### Cons#### On the significance of the results1. In more than half of the theorems, we require the (imaginary) condition number to be smaller than 3. Is it even reasonable to suppose that every eigenvalue of the matrix $J$ has non zero imaginary part given that we would be at the other opposite if we consider a minimization problem (every eigenvalue would be real)? The work would be much more complete if the authors could also discuss what may happen if these sufficient conditions are not verified. The fact that the dynamics avoids unstable equilibrium points of the dynamics itself seems to be of little interest for the problem that we are solving (and is a quite standard result). In effect, we are more interested in the characterization of the set of stable equilibrium points which may contain undesired solutions. This is clearly illustrated by the nonlinear game experiment in which the algorithm converges to a spurious attractor of the problem (to be explained in more detail below). This can be explicitly stated and I even feel it would be better to call them lemmas instead of theorems since it is not immediately clear what these two results imply. While this is easy to derive it would be helpful to clearly state this somewhere (such as a sentence saying that such $k$ will always exist in all the theorems.) Again, this may be straightforward but can help to understand the results. The authors describe why we only need to be interested in a bilinear game of the form (11). I feel that this point is much better explained in, for example, Zhang & Yu (2020). #### DiagonalizabilityI fail to understand the authors  arguments about the diagonalizability of the matrices in the proofs of Theorems 5\~8. Nonetheless, I feel this can be easily proved by using the fact that all real symmetric and skew symmetric matrices are diagonalizable and we do not even need the extra assumptions on $k$ of Theorems 6~8. #### Nonlinear game experimentIn my opinion, the nonlinear game example considered in the experiments is not very appropriate. For (13), the origin is not a saddle point and not even a min max solution (while it is a max min solution). However, the whole paper is motivated by the computation of a (local) nash equilibrium, then we would like to avoid this point. Notice how this example is presented in (Hsieh et al.2020): we want to escape from the origin and not converge to it. While the above points are not in conflict with what the authors want to demonstrate, this should be made clear to the readers to avoid confusion. #### A very minor pointEquation (25) and the following analysis are actually for the proximal method, not EG.<BRK>Summary: This paper investigates „lookahead dynamics of smooth games“. By this the authors mean discrete time dynamical systems generating from a given algorithm by adding a relaxation step in the updates. The main aim of the paper is to solve smooth games. Under sufficient convexity assumptions Nash equilibria for such games can be identified as solutions to a Variational Inequality with a monotone and operator. This is in particular the case for convex concave min max problems. The main conclusion of this paper is that a combination of relaxation and lookahead effects stabilizes the learning dynamics and can lead to acceleration over the base algorithm. Evaluation: This is a very strong paper with an extremely large number of interesting results. In my opinion it makes an extremely good contribution to the flourishing literature  on game dynamics. I only have some small technical remarks which can easily be fixed. .Define $F^{k}$ in eq.(7) .Check for consistency of notation: Sometimes $M_{m\times m}$ is used for the matrix space, then $\mathbb{R}^{m\times m}$. .Define $\rho$ in Theorem 3
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 7. rating score: 7. <BRK>This paper addresses the problem of answering queries about "foils,"i.e., why an alternative plan was not chosen by an agent actingoptimally in a deterministic MDP. The authors describe three broadclasses of responses to this query: (1) one of the actions in the foildoes not satisfy the preconditions, (2) the foil does not achieve thegoal, or (3) the foil has suboptimal cost. Another plus pointis that the authors show that their system is robust to uncertainty(via the confidence measures) and noise (via the probabilisticconcepts). A second major issue is the relatively low quality of the empiricalresults. Looking forward, I have one question for the authors. Given this, would there bea way to modify the current work to create feedback from the user thatallows the agent to improve its solution? Conference on Robot Learning.<BRK>Other minor points of improvement for the paper:o Mind the notation used for your Goal set (near the end of page 2, you are using a different syntax than the one introduced.o Defn 3 seems to have a random bracket at the end. High level concepts, particularly those tied to a symbolic description of the world dynamics, is an extremely compelling basis for explanation. In addition to the form of explanation primitives, the algorithms are intuitive, and the probabilistic inference seems to be sound. One of the most significant missed opportunities in this work is to focus on introducing new concepts. Assuming highly accurate binary classifiers for each concept is relatively extreme, and it s only one such overly strong assumption. Other very strong assumptions include:(a) The state is memoryless/Markovian: every concept can be determined by looking exclusively at the current frame. This makes it challenging to adopt the experiment s conclusion as written since it is also testing the quality of the saliency map in addition to the comparative nature of the explanations. I am leaning towards rejecting the paper due to the number of assumptions placed on the approach. The H1/H2 results are, in some sense, evident that the proposed explanations are preferred (19/20 for H1). While an important element (it would be very surprising if this weren t the case), they don t serve as a sufficient contribution in their own right.<BRK>The goal of the agent is the denounce the alternative plan by explaining the infeasibility or suboptimality of the plan to the user in concepts they understand. The explored direction is interesting and relevant as it seems to be a natural addition to the related problem of *generating* contrastive (a.k.a.counterfactual) explanations. I would suggest, however, that the paper more clearly distinguishes between the contrastive explanation generation literature (see, e.g., a survey [1]) with the type of explanation which is offered here, which is to identify the minimal set of preconditions (in an alternative concept space) that describes/explains the difference between two given instances (i.e., a model proposed fact and a human generated foil). This motivation is related to such (missing) related work as [2]. a comparison with optimal explanations is lacking (relatedly, statements such as "the searchable to identify *the* expected explanation" is misleading, as, without an infinite budget and exhaustive search, the algorithm can identify an approximate to the optimal explanation)  re: user study, I am not entirely convinced that the baselines are fair.<BRK>The other reviewers noted meaningful concerns, but I believe the authors have clearly addressed most of these points. I still believe this work is an "accept". The authors evaluate their approach on two domains with a user study. I agree with the authors: to my knowledge, this is the first work to provide explanations in terms of a learned symbolic model separate from the one used by the agent. (This limitation is not mentioned in the abstract.) It used manual translation of explanations to text. A comparison was made to saliency maps, but not to other explanations (such as causal explanations). Some Typos/etc: In general, another editing pass for articles and agreement of subject/verb plurality would help this paper. Line 2 of Introduction: "they ... with its" (change to "with their"). Paragraph 3 of Introduction: The " " after "questions of this form" is not matched.<BRK>#### SummaryThis paper introduces a new method for contrastive explanation of symbolic models on sequential decision making problems, i.e.explaining why a foil plan is not as good as the system proposed plan. To explain an invalid action, the method reports a missing precondition concept of the failing action; to explain a larger cost, it reports a set of representative concepts such that the foil actions under these concepts are guaranteed to give a larger cost than the proposed plan. The authors also introduce simple PGMs to evaluate the confidence of each explanation. The authors conducted user studies on the proposed explanation method and demonstrated its usefulness. In other words, CE is concept + image while BE is only image; BE is a strict subset of CE. Such a comparison does not seem very useful. It would be more attractive if the concept is more complex (e.g.you can t fall off a plane or touch an enemy as in Montezuma). is important for a user study and I suggest having it in the main paper or at least making it easier to notice. Does that mean all the output explanations are accurate? If so, it will be interesting to see how the search performs on more complex tasks.
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 5. <BRK>1.This paper did not distinguish between "random features model" and "neural tangent model", although they are closely related. It is empirically shown that, for a specific type of initialization, for less over parameterized neural networks, the gradient dynamics follows two phases: a phase that follows the random features model where all the neurons are "quenched", and another phase in which there are a few "activated" neurons. I feel that this paper is on the borderline. This paper studies the gradient dynamics of two layers neural networks.<BRK>The paper experimentally identifies a phenomenon in which the participation among the neurons in a two layer neural net trained with gradient descent (GD) changes with time. In particular:   The paper shows for certain artificial target functions with low dimensional structures, the neurons start out from a random feature behavior and then a few are selectively activated, distinct from the rest of the neurons which are “suppressed”. Lastly the paper argues that this behavior is specific to the choice of scaling, by showing that under the mean field scaling, the network displays much less sensitivity to the scale of the width. 3) The paper gives no explanation or heuristic to understand how the quenching/activation phenomenon arises. 4) It is unclear how to make the effective dynamics a useful tool. Is there an understanding on simple properties (e.g.how large) of these quantities? One can actually observe from Fig.1 that most neurons are still not quenched when the network departs from the random feature behavior.<BRK>This paper studies gradient descent dynamics of two layer neural networks with ReLU activation function. Please update reference or show a proof. Therefore, the paper targets an important knowledge gap. In fact, there is no conjecture of how the results may manifest in practical neural networks. The paper is purely experimental. Comments on the specific contributions:  The first contribution points out the existence two phases. In the first phase, the weights of the first layer do not change and the model behaves like a random feature model. This phenomenon seems to be limited to a few iterations where it is an almost trivial observation considering that the second layer weights are initialized at zero (which implies a vanishing gradient for first layer weights). The observed behavior aims to explain the implicit bias of neural networks. and consider undefined terminology of quenched neurons.<BRK>**SUMMARY** The authors perform an empirical study of the dynamics of gradient decent for single hidden layer NNs with ReLU activations on some synthetic datasets under L2 loss. They identify two distinct phases: an early phase matching RF models and a late phase where the neurons split into activated and quenched groups. Is this intentional? What happens if the b_ij are order 1? I also think the overall structure of the paper could be improved to support the main claim the paper is making.
Reject. rating score: 2. rating score: 3. rating score: 3. rating score: 5. <BRK>The background removal assumes that there is only one dominant object related to the label in an image. As the paper shows in Figure 3, this poses a serious problem when there are more than one object and the object of interest is not dominant.<BRK>Summary: Authors propose an augmentation technique for image classification. The augmented image is obtained by segmenting the salient object and masking the background. Concerning novelty, the authors themselves remark: "It is a good practice to train a model using images that include the labeled object only". There is no comparison to other methods, not even the ones cited in the paper.<BRK>The paper presents an image data set augmentation approach (4 variants+ original data set variant) in order to cater to the lack of sufficient data for learning purposes by increasing available image data sets through creation of  variations of existing images. In 4.THE EXPERIMENTS /4.1 USED MODELS:	  “When a human photo editor keeps one object in the image and removes all other objects, the image tends to be more accurate.” Not sure, this phrasing is”accurate”. Why not, you had the data.<BRK>The paper presents an object focused image based technique for CNN training, in which, for every image in the training set, another image is generated where the labeled object is kept untouched and everything in the background is rubbed out. Nevertheless, none of these techniques is used to compare to the OFI based method.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 5. <BRK>The authors propose a framework for learning a commentary or teacher modelthat provides helpful information during training. Additionally, it supportsits statements with a variety of interesting experiments. The authors then demonstrate that the commentary learning framework canbe used in a variety of ways:learning to provide example weights, learning a blending policy for data augmentation, and learning to provide an attention mask for image classification. This paper has many strengths.<BRK>This paper proposes a general framework for boosting CNNs performance on different tasks by using commentary  to learn meta information. The commentary module would be incorporated into standard networks and be iteratively optimized with the host via the proposed objective. This paper studies three kinds of commentary named weight curriculum commentary that learns individual weights for data samples, augmentation commentary that learns data augmentation strategy similar to mix up, and attention mask commentary that learns to focus on objects of images. All three commentaries have been examined via extensive experiments on small scale benchmarks and shown improvements.<BRK>Summary: This paper proposes a framework for incorporating and optimising meta information as additional parameters when optimising the validation loss for a specific network on a given task. Their method seems to need curriculum learning on top of example reweighting for full benefit, please comment. Ideas such as example reweighting, example blending (e.g.mixup), describing attention masks, or transforming input data (rotation, etc) are treated as special cases of a particular student teacher formulation.<BRK>[Summary] This paper introduces a gradient based meta learning method to learn weighting for each training sample, called "commentary" in the paper s description, as a form of auxiliary learning, updated on training loss to accelerate training speed, and improve generalisation. [Weakness] The paper has several weaknesses which I will outline below. The authors present two versions of "commentary", one is with a curriculum and the other is without a curriculum.
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 6. <BRK>Empirical results on WMT’16 English German demonstrates that the proposed approach reduces the of parameters without sacrificing performance. Further experiments on pre trained BERT models also demonstrate its efficiency. Overall, the paper is well motivated and provides a deep analysis of redundancy of the multi head attention. This may limit to the large scale of BERT pre training. From Figure 3 and Table 2, even reducing the $\hat{D_k}$ from 768 to 128, the total number of parameters of pretrained models (e.g.BERT base) only reduces from 108.3M to 96.6M. If we take a closer look, in case of $\hat{D_k}$ 768, the performance on the large tasks (e.g.MNLI) dropped significantly from 84.1 to 83.4 in terms of accuracy, and its improvement is from small tasks (e.g.RTE).It is better to have a deeper analysis and explanation. It is that suspicious that collaborative MHA takes 18% less time in practical since it requires many factors e.g., GPU kernel fusion. Additional information from Tab 2, these models have more than 101.4M (in case of D_k   384) which is almost the same as the original baseline. However, the performance on GLUE is dropped largely, thus it is hard to support papers  claims.<BRK>This paper presents an interesting collaborative MHA to enable heads to share projections, which can be easily applied to most existing transformer based models, including NMT and pre training models. With using the new collaborative MHA, the number of parameters and FLOPs can be decreased. The authors also propose a Tensor Decomposition based method to easily convert MHA to its collaborative version without retraining. The paper is well written and organized, the experiments are thorough. However, I have several concerns:1)	In the Table 1, it will be more convincing, if the run time on CPU and GPU can be provided. 3)	The proposed method seems to be not effective for pre trained models, e.g.when the number of parameters is decreased from 108.5M to 96.6M, this reduction of model size is not that big, while the average score decreases from 83.2 to 77.6.<BRK> Paper Summary:This paper proposes a new form of multi head attention. It can reduce parameters and FLOPs of Transformer models without performance loss on En De translation. A tensor decomposition based method is proposed for the conversion. More rigorous experiment is needed to justify this new model. Pros  the method is novel and well motivated  works for both original transformer and pre trained language modelsCons  improvements (in terms of evaluation metrics) is marginal to original MHA  more translation / generation tasks should be evaluated  does not measure empirical speed up at inference time Questions / Suggestions  In the abstract, the authors argued for over parameterization. In fact, over parameterized Transformers such as GPT 3 in fact achieves very strong performance.<BRK>The paper investigates the over parameterization of attention heads in Transformer’s multi head attention. This attention can be applied either instead of the standard attention during training, or as a drop in replacement for an already trained model. To use as a drop in replacement, the method requires to use tensor decomposition and subsequent model fine tuning. 2) the paper is overall clear and the method is explained well. Weaknesses(main) While the main contribution is a more efficient attention layer without a significant drop in performance, this claim is not supported empirically. Namely, 1) when used as a drop in replacement, the method is more complicated than a simple head pruning, but not more effective. 2) when used in training (see the MT experiments), the results are also not better than post hoc head pruning. This means that the authors are probably referring to some specific implementation, different from the original one provided by the Transformer’s authors. Therefore, I can not consider this as a contribution and think that this part is misleading for a reader. In the current state, I think it is ok :)2) On the comparison with head pruning and on the paper going beyond practical realm. I agree with your comments, but I do think you should make it very clear in the paper. In the current state, the paper tried to make practical contributions and, since they mostly do not hold (e.g., head pruning is simpler in practice), it s hard to appreciate the paper s value. For example, if you state explicitly that in practice pruning may be simpler, but your results say/illustrate something other than practical applications.
Reject. rating score: 5. rating score: 6. rating score: 7. rating score: 7. <BRK>However I did read the text twice with interest. The authors propose a task HALMA which involves a grid world maze that is partially observed, and includes visual panels that contain conceptual reasoning tasks which the agent must solve to find an optimal path. The aim of this environment is to test three proposed  levels of generalisation :  perceptual ,  conceptual  and  algorithmic . The authors propose several tests for generalisation:  semantic ,  affordance ,  analogy . The authors propose a dynamic manner of evaluating the agents by showing them problems that they appear to not yet have demonstrably understood. The authors conclude that existing agents are woefully inadequate at these generalisation tests and invite researchers to approach this novel task. I found the paper had an enjoyable positive style of discourse and built up the problem area quite nicely in the introduction, but was unfortunately very verbose and many important details were relegated to the appendix. For example the actual algorithm used to dynamically produce test set elements was relegated to Appendix D   however this seemed to be one of the central contributions to the paper. Possibly this paper has too much information for a conference submission and should fit better in a journal, otherwise it should be edited down to make room for a succinct description of the environment, and the evaluation approaches *in the main text*. I was curious to what effect many of the research findings depended on the arbitrary size of the training set; and would have been interested to see some generalisation metrics reported as a function of training set size (100 mazes seems very small). I would have also liked to see a human baseline and then to have had human normalised scores.<BRK>Overall, I think the proposed work provides a valuable benchmark for testing generalization ability of RL agents. I keep my rating unchanged (Weak Accept). This work proposes a benchmark for evaluating the task solving capabilities of agents on three levels: perceptual, conceptual, and algorithmic. The tasks are procedurally generated contextual 2D gridworld environments. I believe there is a lack of RL benchmarks on evaluating the agent’s understanding of object affordances, so this is a useful benchmark for the RL community. Clarity: The paper is well written and well motivated. The paper provides empirical evaluations of TD3 with various encoder/decoder architectures. However, there does not seem to be evaluations of model based/planning methods, despite the task requiring planning & reasoning. I think the comparison of model free vs. model based on this benchmark would be valuable. 2.For future work, I think it would be valuable to add continuous control to the benchmark tasks for more “humanlike abstraction learning”. For example, use locomotion actions instead of gridworld actions; or a robotic arm learning to play a logical puzzle game. 3.The related works section (Appendix H) can also add prior work on visual semantic navigation, which connects visual and semantic understanding with control:[1] VIsual Semantic Navigation using Scene Priors https://arxiv.org/pdf/1810.06543.pdf[2] Embodied Multimodal Multitask Learning https://arxiv.org/pdf/1902.01385.pdf<BRK>##########################################################################Summary:The paper introduces a new benchmark which measures agent reasoning abilities and their generalization of 3 kinds: perceptual, conceptual and algorithmic. The paper extensively motivates this benchmark and shows experiments with training RL agents on it. ##########################################################################Reasons for score:New benchmarks for measuring intelligence are important for driving the field. This one seems interesting. New benchmark which tries to provide a comprehensive measure of generalization, which is a very important topic. 2.Paper is nicely written and illustrated   pleasure to read! 3.Benchmark/code will be published upon acceptance, so the whole community will be able to profit from it. The history of artificial domains as AGI playgrounds has taught us a few bitter lessons. To give one very recent example, the winner of Abstraction and Reasoning Challenge (yeah, that one proposed by Francois Chollet to measure general intelligence https://arxiv.org/abs/1911.01547) wrote "Unfortunately, I don t feel like my solution itself brings us closer to AGI." The solution was some handcrafted search algorithm, no machine learning. Is there anything in the design of HALMA which we expect will protect it from similar problems? How do we know that our agents achieved human performance? ##########################################################################Questions during rebuttal period: I would be curious to know the authors  opinion on Cons above.<BRK>Summary Children generalize to new sights that combine known perceptual elements. Children generalize to new instances of known abstract concepts like order and number. Children know what actions they can take in new scenarios, because those actions have been available in similar contexts. Machines should be able to generalize in the same ways. This paper proposes a new environment, HAMLA, where machines are tested on theirability to generalize in all of these ways. Agents navigate through a maze to a goal location using only carefully designedsignals extracted from carefully constructed mazes. They must learn perception (MNIST digits, color, location), abstract concepts(number, order), affordances (move up/down/left/right as available), andefficient exploration strategies at once, like humans seem to be able to. There is no static test set or test environment. TD3 is used to train various agents based on different NN architecturesthat incorporate more or less structure. Strengths The motivation is ambitious, interesting, and relevant. Explicitly attacking multiple specific modes of generalization at once is an interesting direction. It may make it harder to compare performance across models, but clearer about how well a single model is actually performing. The writing is so dense and so many details are left to the 20 page appendix that it is hard to understand the approach or experiments at more than a very high level by reading just the main 8 pages of the paper. The main example here is the notation. It is also used frequently and most definitions are left for the appendix. 2) Novelty relative to some related work isn t clear. How does this compare to point goal nav? [1] I think the test procedure and available senses make it different, but it s still fundamentally navigating to a goal location. 3) Impact may be limited by difficulty. Tackling this whole problem may be too difficult right now. All approaches completely fail to solve the full problem including vision, as indicated by the bottom right of table 1, which is filled with 0s. If progress is limited to the symbolic setting then the problem is significantly less interesting. Preliminary Evaluation At the moment the paper is not very clear. The paper might be significant as either 1) a central reference for applying some cognitive science concepts to AI, 2) a benchmark that spurs new agent designs, or 3) inspiration for designing new evaluation metrics. My main uncertainty in this evaluation is because I haven t understood the paper in its full 30 pages of depth.
Accept (Spotlight). rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>There is a decent variety in the chosen set of benchmarks as well. Prior such work all required static analysis and planning of the network   and hence were of limited use.<BRK>Suggestions for improvement: The authors call out the relationship between rematerialization and tensor swapping only in the related work. This leaves little doubt as to the proposed method s effectivenessCons: The paper did not study rematerialization in at least the multi GPU, if not distributed parallel setting. Even if swapping is out of scope of the paper, I think some discussion on how DTR and swapping could be combined would make the paper even stronger.<BRK>Pros: 1.While goes under a limited setting, the theoretic analysis on the tensor operation and memory budget bound of the proposed method, as well as on the relationship between the proposed method and optimal static analysis method is novel and interesting.<BRK>The paper presents an online algorithm for dynamic tensor rematerialization. Theoretically, it shows the same asymptotic order on the memory budget and tensor operations as of the optimal static approach. Cover a pretty comprehensive study across theory, simulation and system implementation.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>General Issues to resolve:  Given the centrality of AST code representations to this paper, there should be some figure showing a sample code snippet alongside its AST in full. The authors show that the inclusion of this added structural AST information improves performance on the task, and also improves the cross language transfer learning abilities of the model. This reviewer believes that the paper is deserving of acceptance to ICLR 2021. The authors achieve state of the art results for the code summarization task on both the CodeSearchNet dataset (Python, Javascript, Ruby, Go) and the Java small dataset.<BRK>There is not much not to like about this paper as it has a simple idea to extend the transformer model. The paper proposed to not only take positional information of each token, but to add additional structural information about distances of tokens in the abstract syntax tree. This positional information gives an edge of this model on several code summarization datasets. The simplicity of the proposed model (assuming it is released by the authors) puts the work in the state of the art category in machine learning for code. The paper is also easy to follow and the contribution, while small, is clear and well explained. While it looks standard, the idea seems to deliver well in the evaluation results. In terms of writing, my main complaint is that the paper can better relate to existing works.<BRK>This paper wants to combine sequence (called Context) and AST (called Structure) representations of source code in a Transformer encoder model. This model is evaluated on the task of code summarization and compared against code2seq and GREAT models, and against different configurations of the proposed model, called Code Transformer. A separate evaluation with the model trained on all the languages together is also performed. This multi lingual model outperforms the mono lingual models. Please clarify this. The paper calls the proposed model as Code Transformer. I think the authors want to emphasize that they don t use program analysis information such as control and data flow.<BRK>#### SummaryThis paper presents a representation of source code based on the AST. On the code summarization task, the model improves on baselines, while training among many languages further improves results. The representations learned seem to share semantic similarities among languages. #### Overview(+) State of the art results for single/multiple language method naming task. This suggests that other models that could accept Semantic s ASTs (e.g.[a], [Alon 2019a], [Fernandes 2019]) could also act as language agnostic models, however, none of these are evaluated in the multilingual setting. (the "encoder output of the method name" is a bit vague, but suggests that)* Are all AST nodes input elements to the "Structure+Context" model?
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 5. <BRK>To take an extreme example: the one shot model can underestimate but cannot overestimate the rank of the most accurate architecture in the search space. I also think the authors  explanation of their isomorphic sampling procedure in the response to AnonReviewer2 addresses one of my questions. ## RecommendationI like the submission s high level goal of trying to do careful, controlled studies of different factors which can affect a NAS algorithm s ability to ability to rank different candidate architectures within a search space. This is expected since the training data are more and more concentrated on good architectures." However, due to some concerns about the paper s methodology and conclusions (discussed below), I do not feel comfortable recommending the paper for acceptance in its current form. ## Paper ContributionsThe submission conducts experiments to estimate how changes in training/evaluation setups will affect the ability of these two methods to rank different candidate architectures within a search space. But under this interpretation, I think a *negative* ranking diff would mean that the architecture is over estimated by the one shot model.<BRK># SummaryThe paper assesses two different approaches to speed up the evaluations of neural network architectures for neural architecture search (NAS). The first one is weight sharing, which trains a supernetwork that contains all possible architecture of the search space. The performance of single architectures can be then approximated by simply using the shared parameters of the supernetwork. In general I think the paper addresses an interesting problem in neural architecture search. However, compared to existing work, the new insights that the paper presents are rather limited. The paper is well written and easy to follow. Could be that the ranking of top performing architectures is low because the difference between architecture are almost the same? Unfortunately, the authors did not present any new results on other benchmarks (e.g Nasbench101 shot). I will therefore keep my score.<BRK>### Significance ###Scaling neural architecture search (NAS) to large scale tasks requires efficient ways of estimating architecture performance. However, in the current form, there are several issues that would need to be fixed. For the time being, I lean towards rejection but I would be willing to increase my score if these points would be addressed. ### Final Recommendation after Author Response ###I have read the author response and appreciate their feedback. Significance would be strengthened by releasing code for the conducted experiments. As the authors could not address some of the issues (error bars/statistical significance testing, other hyperparameters and other datasets/benchmarks) in the restricted time of the rebuttal period, I will keep my rating and recommend rejecting this submission for ICLR. However, I also encourage the authors to resubmit a revised version of the paper taking all feedback into account since I see clear potential.<BRK>Strengths:The authors conduct an extensive assessment of fast architecture evaluators on the NAS Bench 201 search space, and provide insights on how different configurations and strategies could influence the fitness of the evaluators. The overall novelty of this work may not be very strong. (2) The experiments are conducted on a single dataset CIFAR 10. I think it would be interesting to compare the performance of all the evaluators on the same task. This may provide further insights as well as concrete examples on the pros and cons of these two types of evaluators.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 3. <BRK>The only weakness of this work is its technical novelty. I think all the ingredients of the work were already available. I also think the paper is relevant enough to be cited:@inproceedings{li2019graph,  title {Graph Matching Networks for Learning the Similarity of Graph Structured Objects},  author {Li, Yujia and Gu, Chenjie and Dullien, Thomas and Vinyals, Oriol and Kohli, Pushmeet},  booktitle {International Conference on Machine Learning},  pages {3835 3845},  year {2019}}<BRK>While the introduction and definitions are aimed mostly at researchers who have some knowledge on the topic (and hence I could not verify the details of the results), I believe that the paper would be of interest to the ICLR community. The paper is well written.<BRK>This is stated on page 5, but considering that graph matching is still NP hard, the importance of Theorem 1 seems to be overstated throughout the rest of the paper. ICLR 2017[Update after author responses]: The authors have addressed most of my concerns and I ve updated the score accordingly.<BRK>The training of the GNN is unclear to me. However I still think the paper requires more work before publication. [After rebuttal]  Thanks for the clarification, the aim of the authors becomes clearer.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 6. <BRK>The paper discusses the problem of inducing complementary representations to improve classification and isolate relevant signals among possible spurious/artificial ones. This is certainly an interesting agenda, and the experiments seem to provide some empirical evidence that the proposed method seems to work reasonably well, yet I must say that from the theoretical side, I did not manage the identify the reasons for this. How good an approximation of the original goal can we expect it to be? There is unclarity about whether the test distribution is a subset of the training set or data issued from a test set (in which case certainly one cannot tune hyper parameters using them). Maybe a synthetic drawing would help to understand how are created/used the noisy data sets?<BRK>The authors then test this on a variation of the Colored MNIST task, showing gains over baseline approaches. Simple and intuitive is good  but requires (A) rigorous comparison to prior work to show why something like this has not been done before, and (B) rigorous experimental comparison to other approaches. Unfortunately this paper does not do enough here. Note in particular that while the authors discuss an "ensemble" of classifiers, there is not a major distinction between this and a single model where the diversity of that model s features is regularized.<BRK>This paper proposes to learn a collection of classifiers, each of which is incentivized to use distinct features. In particular, the authors train multiple models that minimize both their ERM losses and the total correlation (TC) of their final layer representations conditioned on the label. Methods are tested in two adaptation setups: choosing the single best performing model from the collection, or training a linear model on top of the final layer representations from each model. Overall, I think the idea of training a set of distinct models to separate robust and spurious features is interesting, and results under the "best" setup show that the proposed method has isolated a single model that can generalize to the test data, which is promising. I think this is important for understanding the model behavior.<BRK>This paper proposes a method for training an ensemble of classifiers for problems where spurious correlations may be present. However, the authors do not argue for its practical relevance. This work is novel in that it focuses on the utility of disentangled learning in supervised learning. Overall there are some ways in which the authors could strengthen the case for their method, and make the presentation clearer. *    My understanding is that InfoNCE provides a lower bound on mutual information. I understand $K$ to be the batch size. Are we to assume that the contrastive examples are sampled from the subset of the batch that shares the same value of $Y$ as $Y_i$? I do not believe this is addressed in the paper. *    The argument for conditioning in Section 3 is reasonable; however, it is not backed up by concrete results.
Reject. rating score: 5. rating score: 6. rating score: 7. <BRK>Many existing researches on distributed deep learning works in byzantine robustness in a centralized PS setting under the assumption of i.i.d.data distribution on workers, e.g.KRUM.This paper presents a simple resampling scheme that adapts the existing robust algorithms to heterogeneous datasets (referred as KRUM RS later in this review). Then it proposes the RS algorithm and proved its theoretical convergence guarantee the of KRUM algorithm over non iid data, and when the parameter server (PS) does not control the dataset distribution. The technical part is not self contained. For example, in Def. More clear explanation and definition should be present. This paper seems to over claim some of its results: for example, this paper claims that "We propose a simple new resampling step which can be used before any existing robust aggregation rule", but the theoretical analysis only applies to the rule from KRUM. How could one should the new resampling method work for any robust aggregation rule?<BRK>Paper summary:The paper studies Byzantine robustness in the context of distributed learning from heterogeneous datasets. This problem has been widely studied previously, but under the additional assumption that the data of the good workers is i.i.d.. The authors give examples of situations and poisoning attacks with which current defences designed for the i.i.d.situation can be overcome. It is also amendable to theoretical analysis, as shown in the paper. The experimental evaluation does show clear improvements in many cases when the resampling scheme is used. It seems like this is not just the usual assumption that the variance of the stochastic updates is bounded, but it s also a measure of how non i.i.d.the datasets are. For example Remark 1 could be followed by some examples of situations when mixing the gradients in advance will help or alternatively some comparison to other fields where such resampling approaches were found useful. Similarly, it would be interesting to see a discussion of how much and in what respect does the proof of Krum change once dependence between the vectors is introduced. ##########################################################################Minor points:  In Algorithm 1 the inputs passed on to Algorithm 2 are different that the inputs received in Algorithm 2. In Proposition 1 it is assumed that the input is a set of gradients. However, in the second bullet point $f$ gradients are assumed to be Byzantine. I think I understand what is meant, but it will be nice to clarify.<BRK>In this paper, the authors propose the resampling mechanism to improve the performance of existing robust statistics method for Byzantine robust distributed SGD. Both theoretical and empirical analysis are provided. Here is some detailed comments:1. If the averaged random subsample contains Byzantine gradients, then it is corrupted anyway and we don t care it s biased or not. Some people may think this work is incremental, but I think the simplicity of the proposed method is an advantage. 3.It will be better if the authors also provide the convergence analysis of resampling with CM. For Krum, note that larger n (number of workers) actually results in worse convergence in Theorem II, which questions whether we should use distributed training.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 6. <BRK>The work proposes Continual Few Shot learning   a setting to study tasks (1) with a small labeled dataset, and (2) retain knowledge acquired on a sequence of instances. The overall idea is interesting (few shot + sequential observations); however, it s not clear should one take home after reading this draft. I would take issue with the way the word "continual learning" is used. However, this is not quite what they motivate: "Consider a user in a fast changing environment who must learn from the many scenarios that are encountered."<BRK>c) I do notice that there are Pre. I agree that continual few shot learning is a very useful way of framing few shot learning problems, especially in the domain of general purpose robotics. However, the way it samples new classes and the datasets that the paper study are still in a very limited sense, which prevent the paper from getting direct applications. On the other hand, it is very similar to some other incremental class learning benchmarks and it is not clear what different kinds of conclusions we can draw (e.g.different models that may work better) by using this benchmark instead of using previous ones. The benchmark seems very similar to previous benchmarks (e.g.class incremental learning and CORe50) with the main difference being low data. Since this is low data, it doesn’t seem to have a huge memory cost.<BRK>**Minor Comments:**    In the introduction, you should mention meta dataset [2] along with the other few shot learning benchmarks listed as it is arguably the most challenging/meaningful one at the moment. **Pros:**     The idea of fusing the previously separate domains of few shot learning and continual learning in the form of a new benchmark is fantastic. The paper is well written and straightforward to understand. i.e.‘benchmark’ is a bit too general as there is no other mention of key parts of the benchmark including memory and computational complexity metrics. It would be more revealing if a greater number of configurations were explored, including support sets with random shot and random way.<BRK>By controlling the 1) number of support sets per task and 2) the rate at witch tasks change, the authors can span a wide variety of settings previously explored in the literature. + The authors monitor both memory and computation. This is a great first step to encourage other practitioners to use this dataset+ The results give interesting insights into popular few shot learning methods like MAML and ProtoNetCons   Table 1 is very hard to read. I understand this is hard due to space constraints, however a whole page is spent motivating it. Edit : After reading the appendix, it is mentioned that each model is trained for 250 epochs of 500 steps, where each step is done on a single continual learning task.
Reject. rating score: 6. rating score: 6. rating score: 6. <BRK>They propose a new algorithm that uses the previously used policies as a buffer and replays those policies to learn a new policy. In particular, it would be good to have a comparison of the variance of the three algorithms and show that the variance introduced by the IS procedure is indeed a meaningful problem for the final outcome (i.e., variance in the convergence value to the NashConv) and that the proposed method indeed reduces it? (5) Some suggestions on plots: I find Figure 1 to have particularly difficult to read color schemes. The same problem with Brown. In particular, one of the key claims of the paper is that it eliminates the variance introduced by the IS procedure, yet there are no experiments to substantiate this (more below). In figure 1, both the x and y axis seem like titles that were used internally by the authors. I do not understand what they mean. My current assessment of this paper is that without a major revision of the writing and execution, this paper is not in a state to be accepted. I think 0 implies that the algorithm is good. Having such a unified simple game will make this process easier on the reader. This also ensures that the implementation and the algorithm is itself correct. In particular, the quantity NashConv is not defined in this paper and relegated to a related work. What is the range of NashConv? These questions could be easily answered if a precise definition of NashConv was included in this paper.<BRK>In this paper, the authors adopt the idea from gaming theory to reinforcement learning and propose a new algorithm that uses the previous policy to update the current training without using importance sampling. Experiments show that the proposed algorithm cannot only work on the single player setting but also work on the multi agent (zero sum) problems. However, I have the following concerns about the algorithms: 1) To train the critic, how would the sample complexity be? 2) Since the original CFR method is solving the multi agent zero sum algorithm, it would be interesting why this extension could solve the single player problem? Also, it would make the contribution of this work more clear if the author can compare this exploration method with other exploration methods, such as $\epsilon$ greedy or UCB. If we make all $\mu$ be random policy, is this exploration similar to $\epsilon$ greedy to some extent? Considering all contributions and concerns mentioned above, I will suggest a borderline accept for this paper.<BRK>Review: This paper proposes a general model free RL method for no regret learning based on a repeated reconsideration of past behavior. This paper idea is origin from DCFR, DNCFR, single CFR. But those are model based algorithms. ARMAC is model free algorithms and it can be used in a more border environment. If the author compares this paper to the DREAM algorithm(Deep Regret minimization with Advantage baselines and Model free learning) to state ARMAC advantage, I will update my score.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 6. <BRK>In this paper, the authors aim to solve the problem of one class classification using self supervision. The method is compared against other self supervised one class classification methods on CIFAR 10/CIFAR 100 and SVHN datasetsThe novelty in the proposed work is limited as it specifically addresses the issue of geometric transformations for one class classification and it is a very specific case that is addressed. The techniques adopted are also fairly straightforward. These techniques are not particularly novel. The evaluation is limited as the paper argues that data augmentation would not be applicable as it would result in inconsistent supervision. However, no actual evaluation is provided for the same. It would be interesting to consider additional evaluation where SIM CLR is evaluated only with the additional one class that is of concern. I have considered the rebuttal provided. In view of this I raise my score from 3 to 4, but maintain my view that the paper is presently not good enough for acceptance.<BRK>Summary: This paper considers the deep one class classification problem. Since, humans are (presumably) able to exhibit rotation invariance during test time in 1 class classification, this is considered a flaw in existing methods. To rectify this flaw, the paper proposes to use an anomaly score which is the maximum over all possible rotation predictions. The results show that the proposed method outperforms prior approaches when exposed to novel rotations at test time. It can be seen as identifying a family of pathological examples where these methods fail. Niche issue: This paper is ultimately focusing on a niche issue: One class classification  > Specific rotation prediction family of one class methods  > Robustness of this specific family of methods to novel test time rotations. It’s not clear that this issue is of sufficiently general interest to be worth publication in ICLR. The “bug” that this paper identifies is not surprising, and would not be likely to bite anyone in practice. 4.Significance: The whole study is about fixing a glitch in some current high performing self supervision for 1class methods. Unless it’s clear that rotation prediction is the final word in 1 class learning, then it may not be a significant result.<BRK>I encourage the authors to build on their current results and extend their work with additional datasets and an analysis of geometric shifts also at training time. The paper critically remarks that existing state of the art self supervised methods [5, 6, 2], which learn one class models through auxiliary classification based on applying a set of geometric transformations, are sensitive to changes in viewpoint due to a one to one identification of auxiliary labels with certain transformations, thus assuming a fixed viewpoint for inlier data. This enables, even when there is a geometric change of viewpoint for some images at test time, to find the best matching in class transformation, thus implicitly arranging an image with the most characteristic in class viewpoint learned from the training data. The difference lies in taking the maximum over dot products/logit values at testing time? + The paper has a clear and easy to follow structure. **Cons**  The novelty of the presented work is rather low as the non robustness of self supervised methods to geometric changes is not surprising, given that these models have been trained on the premise that the training data has similar viewpoints and geometry. In my opinion, the experimental evaluation should also consider a geometric augmentation of the training data to evaluate the robustness. In ICLR, 2020. Such an analysis would be interesting and could be insightful for the community to assess the usefulness of self supervised methods based on geometric transformation in general.<BRK>This paper presents a one class classifier robust to geometrically transformed inputs (GROC). A conformity score is proposed that measures how strongly an input image agrees with one of the predefined in class transformations. The proposed method is well motivated and results are comprehensive. Results are quite promoting compared to existing works. Technical:   How the proposed method is different from SimCLR? This should be better clarified. A nice thing in Bergman and Hoshen s paper is that geometric transformation is generalized to the affine class, which enable it to be applied to non visual data, such as tabular data. In this paper, the application domain seems to be confined within images only. While the paper mentions a challenge "to optimize the encoder network so that its outputs follows" a Gaussian distribution, it is not clear how this challenges is resolved effectively. Experiments:Results and analysis look good, but I do have a few concerns as follows:  Why the performance of SimCLR is so low compared to other methods? It is closely related to this topic as it also applies geometric transformation for anomaly detection and compares with GT.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. rating score: 6. <BRK>**Summary**The authors study the lottery ticket hypothesis  for generative adversarial networks. They provide extensive empirical evidence using that ```winning  tickets exist in GANs. Further they show that iterative magnitude pruning and channel pruning successfully find such `winning  subnetworks. Finally, they show state of the art results on GAN compression through channel pruning. 2.The work is well motivated and novel. 3.The experiments are extensive and help prove the authors  claims. **Weaknesses and Clarifications**From the given qualitative results, there does seem to be a loss in the finer features (edges and textures) upon sparsification. However, the FID scores do not seem to reflect this.<BRK>Extensive experiments show that matching subnetworks can be found using unstructured magnitude pruning and channel pruning and they are transferrable to other tasks. This paper is well written and addresses an interesting problem in GANs compression. This paper first studies to verify if lottery ticket hypothesis works on GANs. Although the concept of LTH is not new, empirical verification on GANs is of value to this community. I am not familiar with the original work of lottery ticket hypothesis, but it is easy to understand the concept of it without reading the related works except for the following:1. 2.What is the difference between matching subnetworks and winning tickets?<BRK>In this paper, the ideas from lottery ticket hypothesis are evaluated in the case of SNGAN and CycleGAN. The proposed work is useful in terms of the study and analysis of a particular approach from model compression to that of GANs, it would be useful to understand if the results are meaningful specially for the much larger GAN models such as Progressive GANs, BigGAN for high resolution images. There are a huge number of GAN models proposed in literature and validating the ideas on all would be infeasible. It would however be useful to provide some more results on a few more GAN models, particularly addressing the first point. However, I am not fully convinced that the LTH on GANs has provided significant new insights.<BRK>"The biggest potential impact of our work is that it provides empirical evidence that lottery tickets exist in GANs"This reviewer s opinion is that this is not at all surprising. I feel strongly that the significance of this work will be diminished by this. I struggled to read much of this paper due to this, and I feel that the paper’s impact would be strongly diminished due to this. I will argue against acceptance, but I don’t think this is 100% a clear reject and depending on the opinions of the other reviewers I would not feel that accepting this paper was completely out of bounds. I found this figure confusing and out of place. “We use a benchmark pruning approach named Standard Pruning”Is there a reference for standard pruning, or is this novel?
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. rating score: 4. <BRK>Pros:1.We currently lack any substantial theory about attention modules and why they work. I would even consider it fairly realistic relative to a lot of the assumptions required for theoretical results in training dynamics research. Currently theory of attention is grounded in infinite width networks, an assumption this paper does not make. 3.The natural language experiments make a specific claim about the different dynamics for competing words of different topic purity, but only presents an example of two words as evidence. Have you checked the actual variance that would be associated with word embeddings late in training? 4.What is actually meant by a word being "paired" with another word in the natural language experiments?<BRK>(Summary)The paper investigates the dynamics of attention mechanism by configurating a controlled experiment on a simple topic classification task and training via gradient descent. Each random sentence in the training data is synthesized to include only one topic word among many. Then the authors try to find an intrinsic mechanism that triggers the attention model to discover the topic word and accelerates training via mutual promotion. This discovery sounds to be original and relevant contribution to the field.<BRK>The rebuttal appropriately addressed (1), although I am looking forward to the revision to see how this is discussed in the paper itself. Due to the extreme simplicity of the setting considered, as well as the number of assumptions made, it is unclear to me what to make of these results. I cannot really say anything about any improvements on the writing (2) without seeing the revision, but I am confident that the authors can address most of the issues pointed out by myself and other reviewers. Moreover, in realistic settings there will be non topic words which appear very frequently (stop words such as "the", "a" in English).<BRK>This paper studies the dynamics of attention in a task of simplified topic modeling, over the course of training for a specific model, where the context vector is the sum over words in a sentence of their embedding weighted by the exponential of the dot product their key embedding with a global query vector, normalized. The applicability of the theoretical result is close to zero, and a somewhat known property (e.g.in word2vec, Mikolov et al.2013).The experimental results include two parts. One on a tiny synthetic dataset that matches the simplified topic modeling problem and serves as illustration.
Reject. rating score: 2. rating score: 2. rating score: 4. rating score: 4. <BRK>This idea can be traced back to the original R CNN paper, which is not even referred and discussed. There are also many papers having a second stage to refine the detection predictions, e.g.Cascade R CNN, RefineDet, Revisiting RCNN, but none of them are discussed in this paper. The writing is terrible.<BRK>There could also be false positives in the prediction, does these boxes harm the training (e.g.existence of background box)? Also the expanding ratio of bounding boxes is an important parameter, but there is also no experiment on this parameter. What is the average end to end runtime on the whole dataset?<BRK>This paper proposes an approach, BBRefinement, to refine the bounding box predictions of an object detector. I am not seeing why this is an advantage to BBRefinement. The authors also claim that BBRefinement is more robust to missing labels.<BRK>BTW, The author should pay attention to the rules the next time. + In Abstract, the author of this paper provides his code which is non anonymous. This behavior violates the rules of the anonymous code mentioned in the Author Guide.
Reject. rating score: 6. rating score: 6. rating score: 7. rating score: 8. <BRK>The proposed condition can be satisfied via reparameterizing an unconstrained set of trainable parameters. The parameterization satisfies a condition which is shown to imply the existence of the unique solution of the fixed point. The paper introduces two proofs: the solution of the DEM can be shown to be equivalent to 1) the minimizer of a strongly convex potential and 2) the equilibrium of a contractive neural ODE. The results seem nontrivial (although I did not check the correctness). Weakness: the paper is more theoretical, and weaker on the empirical side. The new condition on the weights are less restrictive than the condition of Winston & Kolter, which means the model has better expressivity. However, the benefit of this extra expressivity is not discussed much and demonstrated empirically, which is why I lean towards weak rejection. The model used in the experiment section is also very small (80 hidden units for MNIST), which does not seem very realistic for testing adversarial attack. Also, I think the paper is quite borderline given its scope, and it depends on the wider adoptability of DEM. It will help me better understand its potential impact if the authors could try to motivate it better, explaining the need to further study the theoretical properties of DEM. Please provide some visualization of the perturbed and unperturbed data points. Also are all four variants run on the same machine? Typo: between (7) and (8), “on the other size*”  AFTER REBUTTAL  The authors have raised a few fair points in the rebuttal (especially point 1), so I ve adjusted my rating accordingly.<BRK>The paper provides a novel approach to guarantee Lipschitz for a special type of equilibrium network. (a) The use of equilibrium networks is not well motivated. Different from a feed forward network, the computation of an equilibrium network is not always well posed. It raises the question of why the equilibrium is adopted at the very beginning. While equilibrium networks generalize traditional networks, it is not well explained why people need to deal with the complicated constraints instead of using a traditional network. The toy problems in the experiments are relatively easier to solve using traditional networks. (b) The choice of equilibrium equation (1) is not very well explained. The fact that it covers a deep or residual network as a special case is not a good reason for generalization   we hope the generalization has additional properties that a special case does not have. It seems the paper only addresses the direct parameterization of the particular choice of equation (1) and can not be adopted if the equilibrium equation is changed.<BRK>> Summary: This paper studies a new and more general way of parameterizing the simplest equilibrium network of the form $\sigma(Wz+Ux+b)$, a form that has been tackled by works like (Winston & Kolter 2020)and (El Ghaoui et al.2019).The authors provide a computationally (relatively) efficient way of computing Lipschitz bounded equilibrium networks and a detailed analysis of how the network should be constructed, along with the proof of the existence and uniqueness of the fixed point (and with less restrictive conditions when compared to MON). The empirical results on adversarial robustness shows that the proposed approach is a bit more robust than prior layer based networks and other implicit networks, and validates most of the theoretical claims made by the authors. Some minor things:  i) I didn t realize that the text following Theorem 2 on page 4 is the proof to Theorem 1 & 2 (as there is a "but first we make some straightforward remarks" sentence). It was a bit surprising to me to see $z_a$ and $z_b$ as the theorem claimed "uniqueness". But in general, I think this paper offers an insightful extension on the prior work(s) in this direction, and is an important addition to the implicit depth neural network literature. The proofs are generally easy to follow (and pretty standard, to be honest). 3.The authors additionally study the relationship between LBEN and neural ODEs, which to my knowledge is new (in terms of formally connecting these two implicit models). For example, the non singularity of $I JW$, the monotonicity and non expansiveness of the proximal operators (a well known result), Appendix B, etc. Granted, the two papers also have slightly different focuses (e.g., MON talked more about the PR algorithm). I didn t find the proof to Theorem 1 s claim on "finite Lipschitz bound from $x$ to $y$" in the paper. I didn t find a formal "proof" to Theorem 1, and the proof of uniqueness is simply on page 4. 2.In the Neural ODE discussion, the paper claims that "for any well posed equilibrium network, there corresponds a contracting (strongly stable) neural ODE", which intuitively makes sense to me (as one can think of a deep equilibrium model as a backward Euler...?). But more strictly speaking, is this supposed to be a general statement for deep equilibrium neural networks, or just for networks of the form (1) in the paper (i.e., $\sigma (Wz+Uz+b)$)? 3.One concerning part is the empirical study of the approach. But the results do *not* seem to suggest so. I m curious to see how it compares with MON and other implicit models there.<BRK>3.Just below equation (7): I think there’s a typo in “On the other size, […]”. Central claims: 1. There exists an extension to the parametrization proposed in Monotone Operator Equilibrium Networks (MON) that allows for explicitly setting a Lipschitz bound. unconstrained optimization, which involves computing the equilibrium using tools from convex optimization. "Sorting out lipschitz function approximation." The properties that LBEN models are proven to possess (well posedness under less restrictive conditions than MON and more natural assumptions on the activation functions) are quite compelling. LBEN models also don’t have any extra computational overhad over MON models. Claims 1 and 2 are very well supported in theory. Weak points: I believe the only relative weakness of the paper is its experiments section. It would be nice if there were at least some experiments that varied the size of the network and showed a trend indicating that the results from the small scale experiments will (or will not) extend to larger scale experiments. 2.Need for more robustness benchmarks: It is impressive that the Lipschitz constraints achieved by LBEN appear to be tight. 3.Possibly limited applicability to more structured layers like convolutions: Although it can be counted as a strength that LBEN can be applied to convnets without much modification, the fact that its performance considerably trails that of MON raises questions about whether the methods presented here are ready to be extended to non fully connected architectures. Decision: I think this paper is well worthy of acceptance just based on the quality and richness of its theoretical development and analysis of LBEN. Other questions to authors: 1. Given how notoriously difficult it is to compute bounds on the Lipschitz constants of neural networks, I think this section requires more elaboration. Possible typos and minor glitches in writing: 1.
Reject. rating score: 4. rating score: 4. rating score: 6. rating score: 6. <BRK>To reduce the computational cost, the authors proposed the Random Coordinate LMC (RC LMC) algorithm that only updates one of its coordinates randomly at each iteration. Despite the fact that only one coordinate is updated, the authors prove that RC LMC still converges fast to the stationary distribution. Given some nice properties of the log density functions, the total cost of RC LMC could be smaller than LMC especially when the function is highly skewed in a high dimension space. This paper is well written and clearly presented. I also have some comments on the technical analysis of this paper as follows. It seems that the analyses in that paper and the current submission are closely related. The cost in Theorem 4.2 depends on the sampling probability for choosing the coordinate.<BRK>This paper generalizes the Langevin within Gibbs sampler to be able to put different frequencies over different coordinates. The idea is cute. The result, however, is not convincingly better than the vanilla Langevin algorithm. The convergence rate for the current method for strongly convex and Lipschitz smooth case scales as O(d^2/\epsilon^2) in Wasserstein 2 distance, where every step requires partial derivative in one coordinate. One possible use case, as the authors mentioned, might be that certain dimensions are more stiff than the others, calling for more careful exploration.<BRK>Post rebuttal update:I read the other reviewers  responses, and, although I am still positive about this paper, I agree with R2 and R4 that safely fixing the theoretical proofs would require a full revision. %%%%%%%%%%%%%%%%%%%%%%%%%The authors propose a variant of Unadjusted Langevin Algorithm by replacing the full gradient of the log density by the gradient of a single coordinate selected at random according to some chosen probability distribution phi. When the log density of the target distribution is gradient Lipschity and strongly convex, and the step size for updating a coordinate is inversly proportional to the probability of selecting it, the authors show approximate convergence in 2 Wasserstein distance of Random Coordinate LMC (RC LMC) to the target distribution.<BRK>$\textbf{Post rebuttal update}$I am still positive about the paper and believe that it deserves to be published. However, I agree with Reviewer 2 that some non trivial rewriting is necessary. The flaws are most likely not very hard to be repaired but it will require substantial additional work and the new version needs to be carefully checked. Another flaw in the proof of the theorem concerns line 8 of the proof. Page 2: the authors claim that "RC LMC [...] is cheaper than the classical LMC" This claim should be carefully reformulated. What the results of this paper imply, under the most favorable interpretation, is that the obtained result for RC LMC is better than the best known result for the LMC.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 8. <BRK>Although one point gradient estimates are possible, they have impractically large variances. While this manuscript has many strengths, there are several issues that need to be clarified before it can be accepted for publication. Cons:  The theoretical results can be cleaned to offer better guarantees and make them more interpretable. From the proofs it seems that the expectations are over the perturbation directions u, but these are not introduced in the assumptions. A discussion of this issue in the main body of the paper seems appropriate. After addressing these issues, I think this work would warrant acceptance to ICLR.<BRK>Note that it is possible to combine Gaussian sampling with constrained online optimization [5], and that feasible zeroth order optimization algorithms based on residual feedback have been developed [6]. The paper is technically sound and the developments are clear. See my concerns below and my questions to the authors. This discussion is missing. It is natural that the author(s) give the best picture of the algorithm they propose. [5] https://arxiv.org/abs/1607.03084[6] https://arxiv.org/abs/2006.05445__________Update after the discussions:I would like to thank the author(s) for all their comments.<BRK>2.Strong and weak points of the paper    1) Strong points: The paper s focus is on one point zeroth order gradient estimate, which is a more realistic setting in non stationary online optimization problems as compared to most popular two point estimator due to the queried function is time varying. The new estimate method is based on residual feedback from previous time s perturbed objective value, which can help improve the regret order when function variation is small. 4.Supporting arguments for my recommendation. Can the author/s make any comments on this?<BRK>Evaluation:I believe that the paper contains new interesting results on zero order methods with one point feedback, which are supported both theoretically and numerically. The variance of the corresponding proxy for the subgradient is estimated under more relaxed assumptions than existing in the literature. Also it would be nice to explain in more details, why their approach is impractical. 2.Numerical results support theoretical findings.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 6. <BRK>This paper defined a new problem called “variable shot adaptation for incremental meta learning”. 3  About the novelty. The idea of meta learning the base learning rates is not novel. Pros:1  The idea of using a scaling rule for the learning rate that scales with the number of shots is interesting.<BRK>In the experimental section they compare with standard meta learning methods in a variable shots task both offline and online. However, "s" does not appear inside the function The relation between the learning rate and the mini batch size is given by a function parameterized by two additional parameters, where the functional form is derived in theorem 1.<BRK>To the current status of the paper, I have a few concerns below. The key contribution is to learn meta learning rates besides the initial network parameters. The contributions of this paper are as follows:1. The proposed online and offline meta learning algorithms aim to tackle the few shot meta learning problems with variable amounts of data received in a stream.<BRK>## SummaryFollowing Finn et al.2019, this paper aims at solving incremental meta learning problems. A new setup is proposed as illustrated in Fig 1, which is motivated by learning a model that is capable of generalizing to a new task with decreasing number of shots. 2.Proposed the learning rate scaling method for variable shots. It is not clear when and why zero shot should work if no information about that task is revealed.
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 7. <BRK>The authors show observations that sharing all layers can not be optimal for lifelong learning. Hence, they adopt a layerwise transfer configuration vector which decides activated layer sharing at specific tasks. The problem is solved by EM algorithm based approach. Cons   The problem is task incremental which clearly gives task oracle during training and inference. Baselines are too weak. If the paper targets task incremental learning problems, the authors should compare their methods with recent works, rather than with 3 5 past years  works. I recommend to include further strong baselines like [2,3,4]. The strengths of the methods may not be impressive on modern deeper networks, like ResNet 50. Since it requires massive computation time. The paper didn t show the results/analysis of modern deep architectures. The model inevitably requires much capacity for not shared task specific layers. "Three scenarios for continual learning."<BRK>This paper introduces a lifelong learning algorithm across multiple tasks that automatically learns which layers need to be optimized using an EM learning strategy for each task. In the expectation step, the algorithm updates the next best configuration and in the M step, it optimizes the model parameters. Some details of the algorithm are not explained well. It could help a lot if the authors could provide the objective function they are trying to optimize. As examples: (1) The motivation behind Equation 3 is not clear at all. I have a difficult time to understand why this is the case. It makes sense to use a soft version of this (sum of probabilities) to have it more compatible with the rest of the algorithm. Even though, some aspect of the proposed algorithm is not clear, the algorithm works in practice and outperform multiple suggested baselines.<BRK>This paper studies the problem of lifelong learning of a sequence of tasks by selectively transferring knowledge of some layers across tasks. The authors present results in benchmark datasets for continual learning. As strengths of the paper I would remark:  	The paper is well motivated and demonstrates experimentally why simply transferring all layers does not lead to the best transfer configuration. The paper includes thorough results of the computational complexity of the proposed method, which could be a concern given the proposed approach of maintaining both shared and specific sets of parameters for each task. The weaknesses I observe in this paper are:  	I am concerned on the extensibility of the proposed method to a large number of tasks, which I would think would pose a challenge in the proposed setting similar to how it does in multitask learning. The experiments provide results for a limited number of tasks (up to 20). Questions for authors:   Please address the question regarding the effect of number of tasks on the proposed approach.<BRK>Summary: authors look into life long learning setting (when task arrive one after another) and try to understand which layers of the source model need to be reused (transfered) and which should be re learnt. Authors argue that this decision should be task specific, and come up with an algorithm (EM like) that for each task derives indices of the layers to reuse and also updates reusable and non reusable layers. Overall, this paper is really well executed. Easy to follow, enough of background and motivation is provided, and the algorithm for the most part is clear and intuitive. My main complaint is that it is somewhat thin on the experiments. is it  working by recalculating the counts from step (1) (so it is not a smooth function that is differentiable and updated via optimization)2) In Algo 1: while IsMoreTrainingDataAvailable is not clear to me. 3) Experiments: One baseline would be useful to just randomly select c_t for each task and see how it does. EM is pretty expensive4) Finally, I feel that related work should really go after the into
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 8. <BRK>This work proposes an hybrid framework where the dynamical system inferred in Neural ODE (NODE) is parameterised by two separated components: a component implementing known dynamics provided by a given ODE, and a free component parameterised by a neural network. The experiments are carried out on synthetic data generated from the Lorenz system, and on data representing a simplified fusion system and control experiment. The work is interesting and provides a convincing argument for the usability of NODE in settings different from the one proposed in the original paper. The idea of hybrid parameterisation of the dynamics is also interesting and proven to be useful in the experimental setup. However, to my opinion the contribution of this work is quite incremental. However, I feel that the work should provide a more thorough evaluation of the dynamics, and attempt the implementation of more complex systems.<BRK>The paper proposes a neural network architecture for modeling dynamical systems that incorporates prior domain knowledge of the system s dynamics. The paper has a very strong introduction and the authors provide a good (and quite lengthy) overview of the related work. In particular, the experimental design as well as the presentation and the discussion of the findings need to be improved. NDS, with respect to g(.)? NDS are of length T ,  but in the other u is of length T? What is the interpretation of this?<BRK>### SummaryThe paper combines gray box optimization with Neural ODE, for improved predictions of time series data from low dimensional physics systems. ### RecommendationThe jury is still out on how to best combine prior knowledge about a physical system with learnable components, so this is an important research direction; and combining system identification/gray box optimization with Neural ODE is a reasonable approach. I m still concerned that in its current form, the learnings we can extract from the paper are limited. This is not surprising, and a good sanity check to perform. Unfortunately, this is also the largest part of the experimental results, and I m not sure we can learn that much more beyond this. There should be a baseline for pure GBO (same model and setup as NDS, but turning off the residual, let s call it NDS0). In conclusion, I feel while this is an interesting topic, the actual learnings we can draw on the value of using model priors and GBO are limited.<BRK>This paper is well written and introduces a novel method to learn dynamical models, incorporating prior knowledge in the form of systems of ODE. The experiments explore three different applications of the NDS method introduced in the paper, to a simple synthetic and noiseless physical system, a simplified fusion system where the system dynamics are approximate, and to a modified Cartpole control problem. The experiments show promising results, and it seems likely that the machinery developed in this paper will find impactful applications in the natural sciences and in model based RL. All in all, it is an interesting contribution to the literature of physical predictions, and I recommend it for acceptance.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 8. <BRK>The authors develop a model for learning the observed responses of grid cells (GC) in the entorhinal cortex from the animal movement vectors. Their key assumption is that the GC activity vector rotates with the movement magnitude according to the Lie group formalism and the corresponding Lie group generator is also rotated by the change in movement orientation. I find the paper original, interesting and clearly written. However, given the current state of the field, I would like to see an analytical demonstration of this claim like in the work of Sorscher et al which the authors cite. Such analytical demonstration would provide a much needed insight into the operation of the system.<BRK>This paper proposes a simple recurrent model of how grid cells may perform path integration, which also recapitulates the well known finding that grid cells exhibit hexagonal firing patterns. Their model consists of two primary components where self position is represented by a population activity vector (which is rotated by a generator matrix of a Lie algebra whenever the agent moves in a given direction), and where self motion is represented by the rotation of this vector (whereby when the agent changes its direction, this generator matrix is itself rotated by another generator matrix). Weaknesses: 	It is unclear to me what scientific insight we get from this model and formalism over the prior task optimized approaches. So it is not clear that your work provides any further “explanation” as to how these nonlinear models attain such solutions purely through optimization on a task. Furthermore, I am not really sure how “emergent” the hexagonal grid patterns really are in this model. For these reasons, I recommend rejection.<BRK>Also, the modularity   one of the most interesting facets of the grid cells system   is rigged in advance, rather than an emergent property. So it leaves you wondering, what s the point here? For example, what is the advantage conferred by embedding 2D position into a higher dimensional space? the answer seems to be "that s what emerges from our matrix Lie algebra model." That would make the paper a lot more interesting in my view. The authors do a nice job investigating the influence of different terms. Also this sentence in the intro I find rather unsatisfying:  "It is worth noting that our work is mainly concerned with representation learning. Its not just about representation learning, but implementation and biophysical constraints are also important.<BRK>Summary: The authors propose a simple recurrent network as a model of spatial navigation in the MEC/Hippocampal network. Weak Points: As a non mathematician, it s unclear to me what the implications of the lie algebra presented in the beginning of section 2 are. My understanding is that this makes grid activities the product of two separable matrices (displacement, and rotation).
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 6. <BRK>This manuscript studies the problem of domain generalization and proposes a method, dubbed Rex, for this purpose. The main movitation of this work over the invariant risk minimization (IRM) [1] paradigm is that IRM is not robust to covariate shift, while the authors claim that Rex can deal with both covariate shift and concept shift together. My main concern is that it is not clear to me why Rex could deal with both covariate and concept shift together, as from the optimization formulation in Eq.(6), neither invariant predictor nor invariant representation is enforced. In particular, no theoretical analysis is given to justify this claim. This is quite strange, since the mixture distribution is only a convex combination, not affine combination. Furthermore, I also found some of the discussions in the related work section misleading:     "The first method for invariant prediction ... is IRM". The discussions about invariant representations on page 4 are not accurate. Only P_e(\phi) (but not P_e(\phi | Y)) is called invariant representations, and only this one can fail domain adaption if the marginal label distributions differ. The C ADA does not try to find invariant P_e(\phi | Y). See more discussions in [4]. Only the conditional mean matters. About the discussion on fairness of equalizing risk across groups. More detailed comments:    The reference of David et al., 2010 should be Ben David et al.2010 on page 2    I think the naming of invariant prediction on page 2 is not very accurate. However, the right most term has a wrong weight for a domain. On page 4, the citation of Pan et al.2010 is not accurate.<BRK>This paper reviews various previous work but does not provide a clear comparison from them. I have some comments and questions as follows:1. Contribution 1 and Table 1 state that REx is suitable for invariant prediction. In the experiments, the authors also take IRM as the competitor. So I think the main objective of REx is the invariant prediction, rather than covariate shift. However, the authors emphasize that REx can deal with covariate shift. Please explain more on how REx discovers the invariant prediction. 3.What is the expression (1) in Page 1? If (1) is the risk function of the OOD generalization problem, $\mathcal{F}$ is unseen and should be the same throughout this paper. At the end of Page 1, you say: "Our method minimax Risk Extrapolation (MM REx) is an extension of DRO where $\mathcal{F}$ instead contains affine combinations of training risks, see Figure 1." However, this test accuracy is not worst case performance. According to (1), the problem is to generalize to all four domains at test time and to find out the worst domain.<BRK>The authors propose Risk Extrapolation (REx), which is a novel approach for out of distribution generalization when the new test domain for which we do not even have the covariate matrix. Thorough empirical experiments show that REx significantly outperforms state of the art. The paper is clearly written and easy to understand. Based on the thorough literature review, this idea of this work is original. The results of this work are highly significant and of interest to the domain adaptation and transfer learning community. Cons:  Although I understand the page limit, most of the major parts of the paper (especially the theoretical aspects) can be found in the appendix. As a person who is not very familiar with the literature on distributional shift from multiple domains, I did appreciate having this thorough overview; however, the detailed discussions of the contributions of the paper might be overlooked if (when) located in the appendix. ############################################################Post Rebuttal:After reviewing the concerns raised by the other reviewers, and the responses provided by the authors, I have decided to adjust my scores. Moreover, I was disappointed that the authors did not use the extra one page to move some material from the appendix to the main text in order to elaborate on the proposed method.<BRK>Overview:The authors propose Risk Extrapolation (Rex) which is an invariance based approach to domain generalization. Positives:The paper has a well written and motivated introduction, and there s substantial added expository material in the supplement. The arguments are pretty well thought out, including some discussion of the differences between causal recovery, invariant prediction, and domain generalization. I also wonder if the variance version of the objective can be tied to DRO based approaches via the fact that DRO on a chi squared perturbation ball is equivalent to variance regularization of the risk. Negatives:This is possibly a comment that refers to a paper that s too recent, so not addressing this comment won t affect my rating of the paper, but it seems worthwhile from a scientific perspective to address how recent negative theory results (Rosenfeld, Ravikumar, Risteski 2020) about IRM (and REx) affect the intro framing. In particular, I d like to see the claim in the intro about how REx can extrapolate can be reconciled with the claims in the other paper that IRM and REx succeed under the same conditions as ERM. It would be nice to see a clearer justification for this behavior. Having looked at the supplement, there s substantial and good material there, and I would maybe suggest that the authors cut something like figure 3 and algorithm 1 to bring back some more intuition and motivation about REx, maybe some result from section E.Having looked at the Thm 1 result, wouldn t 3 interventions on every variable also recover the true beta with ERM? I do think this is a neat result though, and it might be useful to use this to give additional intuition about REx in the main text. DRO is usually expanded to distributionally robust optimization, not domain robust optimization.
Reject. rating score: 2. rating score: 2. rating score: 4. rating score: 4. <BRK> It is known that Assumption 3 equation (10) (bounded variance) with Assumption 2 (strong convexity) leads to a contradiction. Thus having these two assumptions together is strong. There are some recent works that overcome Assumption 3 by a different assumption known as the expected smoothness like in the following work: Gower,  R.  M.,  Richtarik,  P.,  and  Bach,  F.    Stochastic quasi gradient methods: Variance reduction via Jacobian sketching.arxiv:1805.02632, 2018. The authors may revise their strongly convex results part using this kind of assumption. In proposition 1 equation (13), it is not clear what the authors mean by the probability of a random variable. Page 5, concerning the quadratic example: this is a trivial case and the only case where one can hope the lower bound to match the upper bound. In fact, alpha   beta iff L mu and from Assumptions 1 & 2 we get that F is quadratic with mu L which implies H   mu I. Equation (20): for me this one of the main results of the paper.<BRK>1.The authors considered uniform upper bound of the stochastic gradients g_i. 3.“show good performance in some certain tasks” bad sentence. [2] Khirirat et al., 2020, A flexible framework for communication efficient machine learning: from HPC to IoT. Moreover, an even stronger argument can be made that the above assumption is in contrast with strong convexity. as one of the instances. 2.There is work [2] that proposes we propose a flexible framework which adapts the compression level to the true gradient at each iteration, maximizing the improvement in the objective function that is achieved per communicated bit. There is a count of bits communicating in each round for QSGD which depends on s, the quantization level. Moreover, you also mentioned that “quantization level is roughly exponential to the number of quantized bits”. Additionally, in Section 3, equations (3) and (4) are not your contributions. On the other hand, where are the derivations/proofs of Equations (18) and (20)? Those are the main results of this paper if I am not wrong. To do proper experiments by using compression techniques, the authors can check a very elaborative work and codebase by [Hang Xu et. “ In my opinion, QSGD is robust and stable, For 1 bit SGD (both Seide and Bernstein), the proposed solution is to use error feedback.<BRK>The theoretical analysis suggests adjusting the quantization level according to the gradient norm, convergence rate of the model, and the current iteration number. Theoretical results show that the dynamic bits leads to better error bound than the fixed bits. Overall, the paper is clearly written. But the improvement is not significant enough to warrant a publication at ICLR. Is this empirically supported? And, the final result looks wrong to me. For example, $p(\frac{g_i}{\|g\|_p}   \frac{l}{s} + \epsilon_0)$ should be $1/s$ and $s   s^2\epsilon_0$ can be much larger than 1. They are contradictory to each other. 4.As the proposed method uses dynamic number of bits, how do we compute its compression ratio? Since the compression overhead is nontrivial in practice and there is no report on training time, it is not clear if the proposed method is faster in terms of CPU wall clock time. 6.The baseline looks weak. And, is momentum used in the experiment? "Communication efficient distributed blockwise momentum SGD with error feedback."<BRK>Based on the theoretical analysis, the authors proposed a dynamic quantized SGD framework to optimize the quantization strategy to be analyzing the trade off between communication and model error. Strengths:Although the idea of changing the number of quantization bit during training has been proposed and studied before, but this paper looks at it from a new angle and by analyzing the convergence rate under different assumptions for the loss function, it computes the required number of bits to minimize the upper bound of the convergence error. To be more accurate, the distribution of error for example for $e>0$ can be written as $P(e) (1 se) \sum_l P(g_i/\|g\|_p   e+l/s)$, ...2. 1 assumes it is uniform. It seems that this assumption is not in line with the previous assumptions and practical cases. Compared to Adaptive and AdaQS, both these methods achieve higher accuracy with a lower compression ratio. The limited experiments and not appropriate comparison setups are major shortcomings of the paper. Minor:Theorem 1 uses assumption 3, not stated in the body.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 7. <BRK>Two conjectures were made to predict the behavior of the distributional generalization for a few models and data. Should D^n be replaced by S? "Too many" ideas. Later on it was mentioned that "S ∼ D^n" which does not make sense if D^n is a sample. 2.There are only two theoretical results in the paper with rigorous proofs. Unfortunately because there are only conjectures and no rigorous proofs, this part is still not clear to me.<BRK>This paper introduces a new notion of "distributional generalization" as a tool to quantify the difference between the outputs from training and testing data sets using a certain machine learning algorithm. The justifications of the conjectures are almost purely empirical (except in the 1 nearest neighbor (1 N N) classifier case).<BRK>The paper formally states that the distribution of train and test outcomes are similar in distribution with respect to tests which themselves can learned by that model class with the current number of samples. I recommend accepting this paper because the notion of distributional generalization that this paper introduces is both interesting and surprising.<BRK>This paper proposes an extended notion of generalization. The paper proposes three interesting conjectures that are related to distributional generalization. The paper also gives empirical evidence supporting their conjectures. I think this is a nice contribution. It raises some new questions on generalization, and points to some interesting properties that interpolating classifiers satisfy (at least empirically) that deserve to be studied in more detail.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 6. <BRK>This work restricts agent observations to a small viewport within the full image representing the environment’s state, and trains one agent to control the location of this viewport so as to reduce the reconstruction error of an internal model of the full image state. Pros  The work is carefully motivated and focused on the question of how to successfully control hard attention. Cons  The two environments considered are small and simple. This contrived arrangement injects a strong inductive bias that seems unlikely to generalize to environments that don’t follow these strict assumptions. It begins with the intuition of using attention to gather information where it is needed, and ends with the minimization of reconstruction error, which is essentially the starting point. If the agent learns enough about its environment during this first (model building) phase, it may learn more quickly than the full observation oracle during a second phase of training, where the external reward is provided. Such a demonstration of benefit from restricted observations would be an important contribution, if it held for more complex environments, and if the results included systematic hyperparameter tuning and multiple runs using different random seeds.<BRK>Specifically, the paper proposes to learn the “glimpse agent” (which controls the hard attention window) by task agnostic loss that seeks to maximize information gain by the glimpse to the learned world model. The mutual information objective the authors proposed to learn the glimpse agent is novel and intuitive. Negatives Experiments are not thorough at all. As a comparison, the experiments in SpatialNet, a work that inspired the authors, include a more diverse range of tasks including Atari tasks and 3D tasks (PhysShooter3D) and are generally more difficult. The existing experiment results are not convincing. While results from PhysEnv show clear advantage of the proposed approach, the gridworld task does not. For example, it’d be interesting to track the mutual informative objective, which the glimpse agent optimizes, over the course of training. It’s also helpful to see if the learned glimpse agent + DMM can indeed be used for learning different tasks (with same physics environment), as the authors claimed to be task agnostic. RecommendationOverall I find the experiment results to be unconvincing and therefore recommend a reject.<BRK>This work presents a method for learning a hard attention controller using an information maximization approach. This is because the proposed method uses an attention mechanism that s trained for reconstruction (via the infomax objective), while the other methods are trained either for an RL task (which may be only loosely correlated with reconstruction) or are heuristic. The authors validate this approach by showing that the resulting attention mechanism can be used for two simple downstream tasks. The resulting agent outperforms others trained using baseline attention mechanisms: a hard attention mechanism that is trained on task reward ("environment"; similar to Mnih et al 2014), as well as models that attend to random positions or to the agent s location. This work is well motivated, easy to read, and appears technically correct. These results would be much more compelling with stronger baselines (and model ablations). The paper only shows results on two simple 2D environments, and one of these evaluations has caveats that I feel significantly weaken the paper s case. The resulting attention policy is likely to indirectly leak task information to the agent, which makes it hard to compare to models trained without expert data on task reward alone. For example, PPO uses a policy entropy bonus, and the authors don t report tuning this hyperparameter, but it will presumably play a large role in the entropy level of the learned attention mechanism. At a more fundamental level, I m not convinced that the task agnostic strategy for information maximization proposed here is the correct one for all tasks. These results would be even stronger if shown on a perceptually harder task, such as one involving natural scenes, 3D content, or more realistic dynamics. If this approach works, it might allow the attention model to be trained on PhysEnv without requiring expert demonstrations.<BRK>Summary1.This paper proposes a new way to solve tasks under the limited view of the surroundings. 2.To solve the tasks, three components, 1) decide where to place the attention, 2) record the observation and make internal memory, 3) solve the task with the memory, are necessary3. 4.The proposed system of this paper is able to reconstruct the full state at every time step. The problem definitions and the proposed models are interesting and clear. 2.This paper investigates the unexplored research area, hard attention in reinforcement learning domains3. This paper has empirical contributions, but the technical contributions are not clarified. It is hard to follow that the necessity of regularization term and alpha (in total loss and entropy weighting), beta. 2.How much alpha (in total loss and entropy weighting) and beta influence the performance? I suggest that this paper should be modified carefully. What is the reason that l2 performs worse than full on the PhysEnv, while full and l2 shows similar performance on the gridworld? TyposEmperically  > EmpiricallyAfter RebuttalThank you for your detailed response, and I will keep my positive score.
Accept (Oral). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>This paper tackles the problem of making Ai/ML systems more trustworthy making the uncertainty associated with a model, in this case BNN, visible, more interpretable. Interpretability and knowing the limitations and uncertainties associated with a model are definitely very interesting research challenges.<BRK>This paper addresses the problem of explaining the uncertainty of a prediction made by a differentiable probabilistic model (as opposed to the prediction itself) through counterfactual explanations. Given that it performed the best in section 5.1 on the datasets used, the results would be interesting. While the introduced approach does require the non trivial complexity of training a DGM, conceptually the method is an elegant way of dealing with one of the big challenges for counterfactual explanations   staying on the data manifold.<BRK>VAEAC, failing to reduce uncertainty on error*". ## RecommendationThe effectiveness of CLUE, and indeed of every counterfactual explanation method cited that makes use of an auxiliary generative model of the data is bounded by the faithfulness of this DGM to model the density.<BRK>This paper introduces CLUE   a method to explain uncertainty estimates. One related method that isn t discussed is https://arxiv.org/abs/2002.10248. Further, there are a number of strong experiments in the paper   I particularly liked the human evaluation and found this convincing.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 6. rating score: 1. <BRK>Are all dimension parameters explicitly listed in the paper? **Reason to reject:**Model complexity analysis and inference time. Proceedings of the 2018 Conference on EMNLP.<BRK>Overall, the paper has a lot of promise and the work conducted looks solid. The section in the appendix on the topic did not shed more light to this process. The authors run experiments on standard temporal datasets and have appropriate ablation studies.<BRK>They use attention mechanisms to extract a query dependent subgraph. The writing of the paper could be improved. As an example, authors have mentioned they have provided the "first" explainable model based on attention mechanisms for temporal KGs.<BRK>7.What is the role of the \gamma hyperparameter in 4.2? The problem addressed in the paper is relevant for the community, and the angle proposed by the authors is interesting, i.e.tackling learning from temporal graphs with an eye to interpretability.<BRK>In Appendix I.1 it says "We are machine learning scientists from the University of Munich...", which violates the rule of anonymity. Please let me know if I should continue on reviewing this paper.
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. rating score: 5. rating score: 3. <BRK>I m a bit more in favor of acceptance of the paper now. Overall, this seems to be a very successful scale up of MCTS, resulting in a new state of the art result in molecular design. The algorithmic description is a bit thin right now. + The paper does a good job of making its contributions clear relative to the broader body of work in MCTS and molecular design. Any message sent to that node will be processed by the assigned worker. Is there more to it than this?<BRK>The authors present an approach to parallelize MCTS on a large number of processors. Impressively, this is achieved in very little wall time. All in all, I think this paper could in my opinion be accepted at ICLR if the authors commit to running the harder guacamol benchmarks for the camera ready version.<BRK>Byincluding a statistics history table in the messages, the TDS DF UCT algorithmhas been shown to scale successfully with up to 100 workers by reducingcommunication congestion. In the remainder of the paper, the authors evaluate MP MCTS in a moleculesynthesis domain, one which has recently drawn significant attention from the MLcommunity. + The paper is well written, organized and easy to follow. I found that the paper s reasons for MP MCTS s gains over TDS DF UCT    somewhat unclear. I also didn t understand what caused    TDS DF UCT to build wider/shallower trees. A slightly more detailed    discussion of this would be welcome and strengthen the paper. While there are some methodological questions and some claims that may needtempering, I think the results are sufficiently interesting for me to recommendACCEPTANCE.<BRK>In this work the authors apply a distributed parallel Monte Carlo Tree Search (MCTS) algorithm to the Molecular Design problem. The goal of this paper is to speed up the computation needed by an MCTS algorithm using parallelization over multiple machines. Although I m not an expert in Molecular Design, it seems to me that this work should also be of interest to people in Computational Chemistry. Overall, I tend to be positive about the paper, given it s claimed significant contribution in the application to the Molecular Design problem. I suggest using "Distributed Parallel".<BRK>It seems more appropriate to show that the scalability can be general to any MCTS application. The paper is light on results and their discussion. Adding long chains of carbon to improve the score is a notorious problem for these design methods, and it does seem to be an issue for the designed molecules presented in the appendix. It is also unclear if the scoring method for logP used in this study is the same as the one used by the other methods mentioned. I think the paper makes some interesting engineering contributions, but the focus on chemical design is somewhat arbitrary, and the authors don t investigate the domain application with enough depth.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 8. <BRK>In this work, the authors propose m coherence as an replacement of sign stiffness / cosine stiffness / gradient confusion to measure the coherence of gradients in SGD training. The experiments show that for both real label and random label datasets,  the initial coherence is low, then increases as the training goes on, finally decreases as more and more data gets fitted. # Interesting empirical results. I think the empirically similarity of gradient coherence for SGD learning real label and random label datasets is interesting. # Misleading interpretation. I think the writing of this paper is quite misleading. For example, in page 6 Real Labels paragraph, it claims for real labels, the initial coherence is very high? Personally I find it misleading to make a claim first but then correct the claim with another one which is totally contradictory to the formal. The statements are super vague and not supported by experiments/theory. I do not understand why these arguments can make sense. I strongly encourage the authors to backup their statements with evidences. # Experiments  The authors claim several benefits of the proposed m coherence compared with existing measurements like sign stiffness / cosine stiffness / gradient confusion. But it is not clear whether or not sign stiffness / cosine stiffness / gradient confusion agrees with m coherence empirically.<BRK>In this paper, the authors provide a new metric named m coherence to measure the gradient coherences, i.e., the alignment of per sample gradients. The empirical results based on m coherence are mostly intuitive. There is also some surprising results. For example, the experiments on random labels show that the gradient coherence does not go to the expected 1. The empirical results seem reasonable. For me, it is more interesting to provide a completely theoretical demonstration via recent advances on theory of over parameterized neural network rather than explain these results via merely looking at an empirical metric. (3) One motivation of the new metric is the computational cost. Therefore, it would be better to demonstrate such an advantage over other metrices in the experiments. In addition, I am wondering whether the proposed metric is looser than other metrics (of course with higher computational cost), e.g., those in Fort et al., 2020 and Sankararamen et al., 2019.<BRK>The experiments presented are pretty interesting and the paper does a good job of interpreting the results of the experiments. 2.In general, although section 6 adds value to the paper, it does not seem to be the result of a principled analysis and seems more like a commentary on the plots. ##  Overall score:I am giving this paper an overall score of 6. There is a new metric defined, and some general theory provided w.r.t this metric and experiments that study the evolution of the proposed metric. 2.The analysis is vague and hand wavy. Studying how the coherence changes w.r.t the complexity of the model.<BRK>This empirical finding could be an important stepping stone for theoreticians in understanding optimization and generalization for over parameterized models. My only criticism of the work would be that the authors should try to provide a more concrete comparison with other works on a random subset of CIFAR10 to determine whether gradient coherence phenomena are robust across all these methods. Strengths2.1. m coherence is a very natural metric for understanding gradient coherence in that it is scale invariant, easy to compute, and is easy to analyze mathematically. Is there indeed a higher m coherence for such subsets of ImageNet during training or does m coherence actually increase on these subsets as well?
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 6. <BRK>This paper identifies an important problem: policy optimization in undiscounted continuing tasks. I feel the main contribution of this paper is Theorem 1, an average reward version of the monotone improvement theorem. My concern is about the algorithm (13) that is almost the same as TRPO, with a bit straightforward extension on the estimation for the advantage function. This does not make very much senses to do this approximation. Those two assumes the P is known but the average reward performance difference lemma has been explicitly derived in those two. Thanks the authors  response.<BRK>***Summary***The paper proposes an extension of the performance improvement bound, introduced for the first time by Kakade & Langford (2002), to the case of average reward performance index instead of discounted return. The authors clarify that this matrix is guaranteed to exist for regular chains. The two algorithms are designed to optimize different objective functions: the average reward and the discounted return, respectively. ***Overall***I think that the paper can be considered incremental compared to TRPO and, more in general, to the papers that study the performance improvement bounds in the discounted setting. Moreover, I have some concerns about the interpretation of the experimental results. Therefore, my current evaluation is borderline.<BRK>In the case of policy optimization, the paper shows the monotonic improvement bound from Schulman et al.(2015) becomes vacuous as gamma approaches one. The rest of the paper presents theory to support a new, non vacuous bound, which leads to a new algorithm analogous to TRPO (A TRPO). The data seems to vary between the plot markers. The paper is also well motivated. The goal would be to help readers understand the contribution by comparing and contrasting it with what others have done. Pending a satisfactory response to these issues, which I describe below, this paper could be ready for publication. I suggest the authors remove this citation and reproduce the results needed for their arguments in the appendix. The A CPO results have been tucked away in the appendix. I take issue with this approach for several reasons:1. It is not clear that it has a meaningful connection to the average reward criterion.<BRK>This paper proposes a practically feasible algorithm for the average reward setting in RL. There are few major and minor concerns I have about the contribution, mostly from an algorithmic point of view, which are described in details below. Overall comments :   This work provides monotonic performance improvement guarantees, by deriving a lower bound policy improvement criterion for the average reward setting. Additionally, the authors propose a constrained optimization variant of their algorithm, building from the CPO algorithm (Achiam et al) and shows that the average reward criterion can also be used for policy optimization under cost constraints. The paper is well written and easy to follow, with the key theoretical contributions clearly defined. The key result is shown in theorem 1, which bounds the performance difference based on the average reward criterion, but reduces to the divergence between policies only. Overall, I think the algorithmic implementation of this is not clearly explained. Under this modification, the authors compare to TRPO and CPO with different discount factors in the continuing environment, and shows the significance of optimizing the average reward lower bound objective. However, as it is, I would recommend marginal acceptance, and open to discussions with other reviewers and authors to clearly understand the significance of the work.
Reject. rating score: 4. rating score: 4. rating score: 5. <BRK>##########################################################################**Summary**:This paper proposes to apply the soft attention mechanism in a CNN network to boost the learning speed of an RL agent in environments such as DeepMind Control Suite. ##########################################################################**Weaknesses**:The motivation in the introduction section does not seem to be strong. How does the proposed architecture in the paper compare to the one in [2]? This paper also shows some visualization about the attended regions on the observations in Figure 5. 2019.[7] Pinto, Lerrel, et al."Asymmetric actor critic for image based robot learning." How does the attention based policy compare to this line of works?<BRK>The paper proposes an alternative encoder architecture for an image based deep RL agent that leverages attention. Significance:The overall novelty and significance of the paper is low. While I appreciate the authors’ drive to research alternative network architectures in deep RL, I’m unfortunately unable to find any significant insight either from a theoretical (not studied) or empirical (the experimental setup is questionable) perspectives. Moreover, the empirical evidence provides inconclusive support for the proposed method. Weak baselines.<BRK>Also, related work section can be improved by covering more past works: there are a lot of works on visual attention/saliency, and there have been some recent works attempting to use attention for deep RL agents (not just the ones that I mentioned above). The authors present empirical results of their algorithm and compares with SOTA baselines in deep mind control suite. This paper is fairly well written and easy to follow. arXiv preprint arXiv:1811.04407##########################################Overall, I think this is a nice work, but given the concerns I have above and marginal/incremental advances in empirical results, I think this paper doesn t yet meet the threshold of ICLR.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 8. <BRK>3.The authors present a new model, CONFET, which is well designed so that the model is flexible and its learning is efficient. The proposed model, CONFET,  is for the probability density estimation of sets and may not be for point process intensity estimation. Reasons for score: An attempt to unify point process models with normalizing flows is interesting. Overall, the paper is well written. But, several points are misleading to me; this is important in the determination of the placement of this work in the research field. Another option is to place this work as the method of determining point locations given the number of samples $n$.<BRK>Specifically, the paper is interested in applying normalizing flow methods to these point processes. The specific method proposed by the paper relies on continuous normalizing flows. In the case of point processes, $f$ needs to be _equivariant_, meaning that permuted inputs to $f$ must have the same output up to the same permutation. The authors use neural networks with equivariant layers, and propose three methods for computing the trace: a MC estimate using Hutchinson s estimator, fixing $f$ so the trace is 0, and exact calculation. My main criticism is that this paper describes an incremental improvement. It is not inherently a problem to me that the proposed method only makes incremental changes from existing methods; if experiments are convincing of the proposed method s performance, it would be a reason to accept. Additionally, the experiments would be more convincing if there was a more extensive set of real world datasets. In conclusion, I think the proposed idea is interesting and could be useful.<BRK>The paper introduces a tractable likelihood model for point processes named Confet. They introduce some variations (stochastic/fixed/exact) with different approaches to compute the trace of the Jacobian. Strengths:The paper is well written and introduces the subject really well. The variations of the methods (stochastic/fixed/exact) is a nice addition in the context of point process modelling. The paper is correct that Eq.(8) is more expressive than the transformation in Kohler et al, but the reason for this is that Kohler et al.enforced an additional equivariance constraint on physical geometry. Further, there seem to be connections with graph normalizing flows that are not explored, as point clouds can be interpreted as graphs with edges between all points.<BRK>A point process gives a probability to a set of points, not assuming that the points are independent of one another. The authors describe the CONFET method, which uses continuous normalizing flows to map from a uniform point process with a learned transformation. *PositivesThe paper is generally well written and motivated well. The exact computation of the trace in this setting is a key aspect to the tractability of this work, and is a good contribution. Figure 5 might be better off in a different colour scheme than green/red for those with colourblindness
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 6. <BRK>The authors suggest that residual networks can in principle implement such iterative algorithms and experimentally show that networks trained in practice do not naturally learn them. The authors define numerical indices to formalize the criteria for "iterative ness" that they are looking for, which are useful for comparisons. It is then hypothesized that such behavior might be a good inductive bias for neural networks, but it is not discussed why this might be expected. Doesn t that make the question investigated (whether ResNets learn iterative convergent behavior) in this paper somewhat redundant? ## After Author ResponseI think the changes to the paper have improved it. Doesn t this go against the conclusions of this paper?<BRK>These are not discussed in the paper (nor are they the main focus, I guess), but this doesn t mean that ResNets do not converge in general. Post rebuttal thoughts:I would like to thank the authors for their detailed response and the revisions made to the paper. The verification of hypothesis 2 is especially hasty. 5.There is a clear missing gap in the related work that I think the authors should pay attention to. Again, I think it is interesting to investigate the relationship between ResNets and iterative computations.<BRK>ICLR 2020. Figure 1 is difficult to understand. Are you plotting activities against each other? This paper suggests that if you change the model nonlinearities to globally contractive ones like tanh or sigmoid (or use their algorithm) you ll control this problem. But gated RNNs are standard for recurrent vision models, and these do not have such a constraint. Are you comparing ResNets to RNNs or iterative algorithms (as are alluded to in the intro)?<BRK>Given this paper s focus on iterations, convergence, and divergence, this body of work seems relevant. Is there any explanation for this? The recurrence index measure also does not seem to add much value. Pros:The problem that the authors tackle is undoubtedly interesting and useful. This is an interesting suggestion. Cons:The strong conclusion that ResNets do not benefit from recurrence regularization is premature, given the current set of experiments presented in this manuscript. As the authors themselves point out, "iterative computation" is an inductive bias.
Reject. rating score: 3. rating score: 5. rating score: 5. rating score: 5. <BRK>This paper proposes to augment graph neural networks with awareness of global graph information based on memory neural networks. However, the proposed models (MemGAT and MemGCN) are mainly compared to some representative graph neural network architectures, like GCN and GAT, which is insufficient to demonstrate the effectiveness of the new graph neural method. As a general graph neural network model, the evaluation experiments on only the node classification task can hardly comprehensively justify the rationality of the proposed approach.<BRK> Summary In this paper, the authors study how to enhance the expressive power of GNNs by memory augmentation. Empirical results on public benchmark datasets suggest the effectiveness of the proposed method. 2.The core idea is well presented with logical reasoning based on theoretical and concrete evidences. 3.Empirical results suggest the proposed technique is promising in node classification tasks. I keep my rating as it is.<BRK>However, the proposed method requires model specific modifications and cannot be applicable to other tasks on graphs, e.g., link prediction. Is this a typo? Clarity.Overall, the paper is well written. The paper also studies the limitations of local GNNs (not specifically LUMP) but the resulting model is similar to memory augmented GNNs and it has skip connections and augmented by convolution in the latent node space.<BRK>This paper is basically well written and easy to follow. The experimental results demonstrate the efficacy of the proposed method. However, I have the following concerns:1. However, the example only works in the case of two graphs. It would be more convincing to use original adjacency matrix in the proposed method, or add (Klicpera et al., 2019) as a baseline.
Reject. rating score: 4. rating score: 4. rating score: 6. <BRK>It might not be qualified as a regular paper for ICLR. So we think the technical contribution of this paper is limited. Furthermore, some parts of this paper are not clearly explained. The authors proposed to use BERT as the main model.<BRK>The manuscript focuses on a trending topic of applying a Bidirectional Encoder Representations from Transformers (BERT) based prediction model to a new domain. In conclusion, the study is valuable but needs further work. However, I am not convinced that these experiments are sufficient to support accepting the manuscript for the following five main reasons:First, the compared models and methods are somewhat old and elementary (e.g., word2vec and TFxIDF), thereby failing to capture the current advances. Third, the overall experimental setting does not seem to be adequately captured and justified, and I am unable to find a description of the performed statistical significance testing.<BRK>This paper studied the problem of classifying skills into competency groups. The authors proposed SkillBERT, a BERT based model, to extract the embeddings of skills and use that for the classification task. Overall comments:This is a good paper and I personally support it to be accepted. This is not for me to decide, maybe Meta reviewer could share some opinions about whether this paper is suitable to ICLR.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 6. <BRK>Assuming the perturbations are in form of a collection of several rectangles, the model parameterizes the physical modifications. By simply ignoring the closed loop of viewpoint sequence and frames, the model directly creating adversarial frames with compositing methods. With the improvement above, the iteration speed of the model is greatly improved. Strengths:+ This paper proposes a highly scalable framework for designing physically realizable adversarial examples against end to end autonomous driving architectures, which makes the much stronger attack results. + With a small number of initial calibration running, the search for optimal parameters can be carried out with end to end gradient based optimization, instead of relatively slower Bayesian Optimization. The authors should clarify how much such approximation affects performance. Please try to improve the presentationI hope to hear the authors response regarding the weakness listed above during the rebuttal period.<BRK>This paper presents an approach to design physical adversaries to attack end to end autonomous driving systems. The proposed approach maps adversarial patterns onto video frames recorded from real world to generate adversarial examples for deviating the control of a vehicle. The problem considered by this work is significant as physical adversarial attacks pose serious threats to the safety of autonomous driving. The paper is well written and most of the technical details are clearly articulated. One main concern I have about this work is that its motivation does not seem strong enough. The authors may need to more clearly argue why simulated images are less advantageous in this case. Some other comments:1) It’s a little bit difficult to understand why a few painted boxes on the road could fool an autonomous system. It would be helpful if the authors could provide some explanations of how these physical modifications affect the vehicle s controller.<BRK>To summarize, the authors propose a road painting attack with rectangles to deceive a controller network such that the car will deviate from the correct trajectory. The simulation is done on CARLA. **The threat model**Painting roads with rectangles is very interesting. The closest one I saw is patching stop signs with rectangle markers [Eykholt 2018] as cited in the paper. With such a small space, I would expect the space allowed for changing the controller network output is also small. Searching with BayesOpt or GradOpt may not help much; there could be a wide range of parameters that can cause reasonable deviations already. In traditional adversarial attacks, the perturbations are small enough to be ignored by humans but will cause a deep network to fail; the current setup is not the case; therefore it can be easily defended by humans. BO**  BO is a black box optimizer that has no access to the inner structure of the controller. GradOpt in this paper is a white box model and it is unfair to compare BO with GradOpt. It will be very interesting to see the evaluation metrics in this case just for ablation purposes. The submission lacks transferability experiments to study those scenarios.<BRK>4.Related to 3, I m also curious about the design choice of using the controller deviation as the optimization objective function while using the trajectory deviation to measure the effectiveness of the adversarial attack. Those two may not necessarily correlate (An example will be if the car is supposed to go straight, while zero steering and both a left and a following correcting right steering will keep the car straight thus resulting in very similar trajectory, the controls are more different.On the other hand, a single left or right steering will result in large trajectory error as it accumulates.) The proposed method relies on an approximation of the image frames of the trajectories by adding random noise to the controller outputs and use those trajectories for learning. The paper states that "Given the fact that actual control is closed loop, it is not evident that this simple approach would work; however, our experiments below show that it is remarkably effective, despite its simplicity."
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 6. <BRK>Summary: The authors propose a connection between abstention and robustness to adversarial examples. A good effort is made to address the problem from several relevant angles. Cons: The paper is very difficult to read. Perhaps there are further details in the referenced papers, but it is not even clear to me how many classes are in the set. The clarity of the paper is severely lacking.<BRK>The paper proves a separation between the power of models with and without abstain. The adversary picks the features, and it is up to the model to correctly classify it or not. Pros: Not many previous works have studied the role of abstention in adversarial attacks (aka adversarial examples / evasion attacks).<BRK>This paper studies the power of abstention in robust classification. The paper first shows a negative result on the possibility of robust classification. If not, you should explain why.<BRK>It aims to tackle a pretty fundamental problem, and provides some clear and simple results in relation to a simple algorithm. For the statements I checked, the paper was technically sound. Moreover, there are clearly some readability issues, based upon the reactions of the other reviewers. It seems that these automatically provide some notion of abstention when confidence is not high enough. I found it surprising that none of these were mentioned.
Reject. rating score: 5. rating score: 6. rating score: 7. rating score: 7. <BRK>Summary:The paper investigates the role of Batch Normalization (BN) in the generalization of deep networks, and its impact in the trade off between clean and robust accuracy in adversarially trained networks. The authors demonstrate that the rescaling operations in BN when considered in conjunction with the ReLU activation, serve as a feature masking operation. Based on these observations, the authors propose RobMask, which uses a linear combination of such rescaling operations to achieve improved generalization in deep networks. Pros: 1) Interesting observation, and highlights the needs to re examine techniques from the standard training paradigm that are directly used in adversarial training of deep networks 2) Easy to integrate with any network architecture that already uses Batch Normalization3) Demonstrates enhanced standard performance over different network architectures and datasets4) Achieves improved trade off between clean and robust accuracy for adversarially trained networks on smaller constraint sets (L infinity eps   2/255 to 6/255)Cons: 1) The paper lacks novelty from the standpoint that very similar observations were made by Xie et al.[1].2) Further, the primary results are demonstrated for the case of k 2, using a linear combination of Batch Normalization parameters obtained for normal and adversarial training, which represents only a minor change from the algorithm proposed in [1]. Could the authors provide an intuitive or theoretical explanation for the same? Thus the performance comparison made in Table 3 at the 20th epoch for models trained using normal training and RobMask is unfair, since RobMask uses 7 step adversarial training. 6) To quote from the last para of Section4: “Also, the proposed RobMask method is more general than Advprop, and AdvProp can be considered as one special case of RobMask when we set the linear layer rank k to 2 and freeze p   1 in the whole training process.” Thus, could the authors provide additional results for k 3 or k 4? Could the authors clarify the exact BN parameters used in final evaluation on clean and adversarial samples? 8) When k is set to a value larger than 2, it is not immediately clear what the different $w_k$’s would represent, since the 2 dimensional $u_i$’s already encode information about different $\ell_\infty$ constraints. Further, it would be beneficial to the reader to include an example with k 3, along similar lines to what is presented in Section 4. Adversarial examples improve image recognition. Expecting the authors to address the following during rebuttal period:    Please address and clarify the cons as listed above. If it represents iterations, or epochs? Given that the parameters of the first BN layer are presented in Figure1, since the convolutional layer that occurs before this BN layer is frozen, it is expected that Plots 1(a) and 1(b) are identical for the two cases, and does not offer additional insight. Could the authors comment on the behaviour of BN parameters in deeper layers of the network when the same fine tuning experiment is performed? In Algorithm 1, could the authors clarify if the number p is sampled uniformly from the [0,1] range? As currently presented, it appears as though the BN parameters corresponding to standard training are used in this step. Could the authors show an ablation experiment where the BN parameters corresponding to RobMask are used for attack generation to understand this better? If so, could the authors clarify why the accuracy for eps 0 in Table4 differs from that shown in Table3 (5 6% difference)? Also, it is not completely clear from the algorithm if BN parameters of all layers in the network are updated, or if it is restricted to some specific layer. ########################## Update after rebuttal ##########################I thank the authors for their detailed response; several concerns have been addressed in the rebuttal. Thus, I have not further increased the score. [3] Rice et al., Overfitting in adversarially robust deep learning, ICML 2020, https://arxiv.org/abs/2002.11569<BRK>## Overview The paper focuses on the generalization issue with adversarial training that various work has recently demonstrated. The authors single out the rescaling operator in BN to significantly impact the clean and robustness trade off in CNNs. ## ContributionsThe contributions of the paper are as follows:1. Showing the effect of BN (and, more specifically, the scale parameter of BN together with ReLU) as adversarial masking. a.Authors show that *adversarial fine tuning* of only the BN parameters of a *vanilla trained* network provides some adversarial robustness, although at the trade off losing test accuracy. 2.Showing that interpolating between the BN parameters in Contribution 1 provides a smooth trade off between generalizability and robustness. 3.Devising an approach for utilizing different perturbation strengths for model training. The authors build on their Contribution 2 and propose $k$ basic (or better to say principle) rescaling parameters, the linear combination of which leads to a rescaling parameter. 4.Providing a short yet informative, ablation study to show the effectiveness of Contribution 5. Contribution 3 turns AdvProp into a particular case of RobMask. In fact, Xie et al.(2020) mention in their paper that "a more general usage of multiple BNs will be further explored in future works," which seems to be the inspiration behind this paper. The main limiting factor for the impact of this paper is the experiments. Given that the paper can be considered an extension/improvement over AdvProp, it is desirable to have similar largescale experiments in Xie et al.(2020) on ImageNet and its variations. 2.Regarding the practicality of the approach, I am missing a computational analysis of the approach to compare it against BN and AdvProp, e.g., it would be great if the authors provided a head to head comparison of training curves. 3.How many times did you run each experiment? What are the standard deviations in Table 3 (and other tables)? ## Questions and comments for the authors1. 2.On the bottom of page 7, you wrote: "It is because both AdvProp and Adversarial training models are trained with adversarial examples generated with $\epsilon  8/255$, while our methods use a random perturbation where $\epsilon_{max} 8/255$." The term "random perturbation" is misleading here, as I believe you are also using PGD attack, but the adversarial perturbation s strength is randomized. I suggest that the authors remove the figure and use the space to address the raised concerns. ## Evaluation logicI find the paper an interesting extension of the CVPR2020 paper by Xie et al.However, the paper s experimental section does not provide enough information to the reader to see the concrete benefit of the proposed method in training a large scale CNNs. I think the paper could significantly benefit from a more extensive experimental setting. As a result of the authors  responses, I increase my score to 6.<BRK>The paper observes that the rescaling operation in the batch normalization layer and the ReLU activation learn to select different features for standard and adversarial trainings. The authors call this effect "Adversarial Masking." Based on this observation, the authors then propose Rob Mask that achieves good standard and adversarial accuracies at the same time. The trade off between standard and adversarial accuracies are an important problem in adversarial machine learning. The findings in this paper are really interesting and deserve further investigation from the research community. My biggest concern is that, since it is an empirical paper without rigorous guarantees, I would like to see the experiments on more datasets. The experiments in this paper are only on CIFAR 10 and CIFAR 100, which are similar datasets. Does the finding still hold in other datasets like ImageNet? Also, there are many models that do not use Batch Normalization, but the standard accuracies also drop when doing adversarial training. The proposed hypothesis cannot explain that. One ablation test that I would like to see is whether fine tunning a single convolutional layer will have similar effect as fine tunning the BN layer done in this paper.<BRK>Summary:This paper follows the direction of previous work AdvProp (Xie et al.(2020)) and aims to use adversarial training as a regularizer to improve the network generalization on clean data. The authors analyze that the different rescaling operation in the batch normalization layer along with ReLU acts as feature masking/selection layer, which can control the trade off between adversarial robustness and clean data performance. Unlike AdvProp, which uses different batch normalization layer for clean images and adversarial images at a specific perturbation strength, here the authors propose a technique called RobMask that adapts the rescaling parameters of batch normalization based on the perturbation strength during training. The authors show that such adapting technique is more effective than using different batch normalization layers (as in AdvProp) for each perturbation strength and thus improves the clean data performance on CIFAR10/100. + Discussed its differences to previous works. Weaknesses: 	The authors claim about well balanced robustness trade off using their method and also claim that their major objective is only to improve network generalization on clean data. There is a little ambiguity regarding the major contribution of this paper. Isn’t the hypothesis that is stated as “new” in this work already discussed in AdvProp i.e.using different batch normalization for clean and adversarial images improves network generalization, which in turn draw the conclusion that rescaling operation of batch norm could control the robustness and generalization trade off. The two learned adversarial maskings discussed in section 3.2, it is not clear how they are generated. Results demonstrate that the proposed approach improves generalization but the performance gain is minimal (only 1% 2%) and not so significant compared to the baselines. Minor point: 	I understand that the major objective of this work is to improve performance on clean images but not the adversarial robustness. The results demonstrate higher robustness against PGD based adversarial attacks with perturbation strength lower than 8/255 is interesting but not of practical importance since the method requires perturbation strength as an additional input and very specific to PGD based attack. Although the performance gains on network generalization are minimal compared to the baselines, this work cleverly addressed the limitations of previous work and extend it with simple modifications. However, I suggest the authors to also consider the evaluations carried out in AdvProp (Xie et al.(2020)) to improve the significance of their work.
Reject. rating score: 3. rating score: 3. rating score: 4. rating score: 5. <BRK>SummaryThe paper suggests an improvement over double Q learning by applying the control variates technique to the target Q, in the form of $(q1   \beta (q2   E(q2))$ (eqn (8)). To minimize the variance, it suggests minimizing the correlation between $q1$ and $q2$. In addition, it applies the TD3 trick. The resulting algorithm, D2Q, outperforms DDPG and competes with TD3. If they are independent, what’s the point of including q2? It’s unclear how it’s calculated.<BRK>The paper proposes a method that modifies double Q learning by eliminating a linearly correlated part of one Q. I am not familiar with the proof of Double Q learning and TD3, and thus find the proof of this paper hard to read as it omits the majority of proof by claiming it is similar to the proof of the aforementioned two algorithms. I am also curious how the de correlation term helps to improve the convergence in the analysis as it is the main contribution of this paper.<BRK>All experimental results are about reward vs. iteration curves, which are not convincing or insightful enough. Overall, the ideas of this work are interesting and bring some insights for tackling the overestimation issue of Q learning. In particular, in the current version, the formal definition of the correlation term (Sec 3.2) and the description of the full algorithm itself (Sec 3.3) appear after the convergence analysis of the algorithm (Sec 3.1), which looks weird. The theoretical analysis of convergence seems hand waving and confuses me.<BRK>Pros:1.This paper introduces a new variant of Double Q learning to reduce the overestimation risk and reduce variance. Moreover, in the convergence analysis, the meaning of many notations are not explained at all (e.g.$\alpha_t$). Cons:1.This paper does not provides enough insightful intuition and theoretical guarantees on the design of the new algorithm.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>4.The paper is clearly structured and well written. 3.Are the results significant? ##########################################################################Questions during rebuttal period:  Please address and clarify the cons above  #########################################################################<BRK>The authors  might want to rewrite that sentence to make it clear. Summary:This paper proposes a simple scheme for training with multiple augmentations of training data in one iteration and reweighting the instances by their relative loss. In the appendix the lambda is given as 1. I encourage the authors to point to these results in the main body.<BRK>I believe more discussion and experiments are required to present the difference of you method. The authors of the paper presents their main objective to provide generalization while they have not provided comprehensive experiments or discussion to address this concern.
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 6. <BRK>################################ Summary:This paper presents a model extraction attack (MEA) for BERT based models that are hosted behind an API. Using the model obtained in this step, the work aims to subsequently demonstrate attribute inference attacks (AIA) to expose sensitive information of the underlying data used during fine tuning and adversarial example transfer (AET) that can be used to attack the hosted model. It s unclear how common the case of fine tuned BERT models behind APIs are from this paper. The motivation of adversarial attacks against a pay per query API are unclear. It s clearly undesirable with respect to creating robust models, but as presented it s unclear why this is problematic. Knowledge of black box model.<BRK>Overall Recommendation:While this is a very practically important setting, I m not entirely convinced the proposed attacks work. The authors then study two attacks on the copy model   private attribute identification of sentences in the API s training data & adversarial example transfer from the white box copy model to the black box API. In 3.2 and the Abstract / Intro I would remove the claim that "architecture, hyperparameter is not known", since both the victim / attacker are finetuning BERT. The paper s story will be stronger if a corpus like Wikipedia is used for the query distribution, with the same set of downstream datasets. I still encourage you to run this baseline in the next version of the paper, instead of only doing black box attacks on extracted models. You can make transfer rate 100% by retrieving examples from the target adversarial class.<BRK>The pipeline can be summarized as the followings:1. 2.Conduct model inversion attack to the stolen model in step 1 to expose sensitive information of the training data. 3.Create adversarial samples for the stolen model in step 1 and use them to attack the original API. Some of the assumptions of the experiment settings are too strong and far from the real situation, but the idea of using this pipeline to conduct model inversion and adversarial transfer attack is very interesting. ### ProsThe pipeline proposed by the authors is very insightful. However in real practice we are not able to know which pre trained BERT parameter set is to be used for fine tuning, nevertheless to get the pre trained model. Besides, the stealing method is just a conventional distillation.<BRK>The paper presents a model extraction attack, where the adversary can steal a BERT  based API (i.e.the victim model), without knowing the victim model’s architecture, parameters or the training data distribution. The model extraction attack, where the adversary queries the target model with the goal to steal it and turn it into a white box model. They demonstrated using simulated experiments that how the extracted model can be exploited to develop effective attribute inference attack to expose sensitive information of the training data. They claimed that the extracted model can lead to highly transferable adversarial attacks against the original model (victim model). The model extraction step of the proposed method is the main concern for me. Conclusions maid by simulated experiments on model extraction attack might not hold for a real experiment. Some explanations with real scenarios would make the claim more realistic. ”: provide references or evidence to support the statement. Re: “The intuition lies in the fact that the similarity of our extracted model and the victim model allows for direct transfer of adversarial examples obtained via gradient based attacks.”  — BERT part is same for both victim and extracted model but rest is still unknown and how the complexity of the similarity measurement increases for a real scenario?
Reject. rating score: 4. rating score: 4. rating score: 4. <BRK>GradNorm is first formulated as a Stackelberg game, where the leader aims at normalizing the gradient of different tasks and the follower aims at optimizing the collective weighted loss objective. To further account for the different gradient directions, a learnable rotation and translation are applied to the representation of each task, such that the transformed representation match that of the single task learning. The other contribution of the paper is a learnable task specific rotation that aligns the task gradients with single task learning. The experiment on the second dataset seems to be very preliminary, which might not be sufficient to justify the proposed method empirically. After author s response I am not fully convinced by the explanation of the motivation behind rotation matrix, in particular why it is aligning with the single task learning, which is counter intuitive.<BRK>E.g., could it be better to have a separate rotation matrix for each class? The paper introduces a new way of thinking about this kind of method, i.e., through the lens of Stackelberg games, which could be useful in reasoning about the convergence of such methods. These issues don’t make the work unclear, but they are a bit distracting. Stackelberg games are an interesting framework for thinking about methods like GradNorm and Rotograd that adaptively guide MTL training. The main drawback of the paper is that it is not clear that direction homogenization could lead to practical improvements for multi task learning. More broadly, since the point does not only apply to Rotograd, this ablation could also be done on Gradnorm and other methods. Another main takeaway from the theory is that the rotation matrices and translation vectors should be updated with gradient descent, instead of simply replacing them each step.<BRK>Specially it introduces a rotation matrix to rotate the hidden representation from the last shared layer. The authors put the proposed method in the context of game theory to show stability and convergence of the training, which might be of merit. The writing of the paper doesn’t meet the publication standard, needing major work to improve. There are many typos and awkward sentences, hindering understanding of their work. So, what is the shape of this gradient matrix? There is lack of adequate explanation of the motivation behind the objective in Eq.(6).By reading the paper, I have no idea about the two oracle functions, and why they are defined in the way shown in Eq.(8).Eq.(3) is inaccurate, not aligning with that proposed in the GradNorm paper for the computation of L_{grad}^k. Why operating on z instead of the gradient in Gradnorm can resolve the discordant gradient issue among tasks is not properly justified.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>This is a good paper, in my opinion. I have some suggestion to improve the quality of the work. See below. I suggest to improve Section 3, open a bit the range of references,  considering general and relevant works such as C. Grigo et al.A physics aware, probabilistic machine learning framework for coarse graining high dimensional systems in the Small Data regimearXiv:1902.03968, 2019. what is your inference goal? please clearer state these points.<BRK>In addition, the latent representation of previously unseen initial conditions can be inferred (with decent error bars), allowing to predict the future of unseen initial trajectories. Overall, the idea is interesting and supported by correct mathematical derivations and experimental proofs of concept. However, I have some questions and remarks that I would like to be addressed. Also I have a trivial question, that however I think needs to be discussed in the paper. My guess is that by construction, the generative model has a 100% error (as good as random, I mean), because it does not track the future of individual particles, but rather predicts the future of the large scale, slowly evolving coarse grained variables (X, governed by the latent z s). Also, after reading Appendix , Figure 9 and 11 could be compressed and included in the main text, or at least referred to explicitly.<BRK>Quality, clarity, originality and significance:Pro: The paper is very well written, clear, and the generative approach is novel. Adding domain knowledge is relevant and significant when dealing with real world applications. Cons: Below are a few comments and questions to clarify some of the aspects and choices made. Does it necessarily need to be a stationary process as in the experiments, or can it be a trend, or a periodic signal? Would it be possible that the latent space X is sufficient, and if not, would the authors have examples when this is not the case? What is an explanation for this?<BRK>This paper presents a generative state space model using two layers of latent variables. The latent variables in the first layer aim to capture long term dynamics and ensures the stability. The authors have shown some promising results in modeling particle dynamics. Since the authors claim that the use of X can reduce the complexity/the search space of the learning model, it would be great if the authors can show how the performance change given different number of training samples. Another issue is about the effectiveness of this model in simulating real world physical phenomena.<BRK>However, the authors have addressed most of my major concerns. Strong points:The paper presents an interesting and motivating case for Bayesian inference in probabilistic generative models: a problem that has inherent uncertainty along with the ability to incorporate domain knowledge that can reduce the inference complexity. In general, the paper is well written (apart from some higher level structural issues discussed below) and the notation is clear and unambiguous. The addition of Appendix H, in my opinion, considerably strengthens the paper s story and case for acceptance.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>I d argue this is the implicit assumption, since the language is centred around a view of discrete flows as permutations, which are by definition bijections. My point here is that the distinction made by this paper is useful, but not mutually exclusive with the discussion in Papamakarios et al.The analysis of gradient bias is good, and works to dispel the result I had taken from Hoogeboom et al 2019 regarding the limitations of deep IDFs.<BRK>Overall, this paper is well written and well motivated. The authors theoretically analyzed the flexibility of normalizing flows for discrete random variables, showing that normalizing flows for discrete random are capable of mapping from a distribution with dependencies across all dimensions to a distribution which is fully factorized. Specifically, the contributions of this paper are:1. 3.Based on the previous analysis, the authors proposed an improved architecture for IDF, named IDF++.<BRK>The authors claim is mainly three following points:1. In this paper, authors claim that gradient bias due to the STE is less of a problem, and it highly depends on architectural choice of coupling layer.<BRK>The first theoretical result in this paper (Lemma 1) seems to be a still interesting but an almost obvious result that if you allow for infinite number of discrete values you can factorize any distribution (i.e., by just putting all possible non zero configurations along one dimension). **Update after author response**I thank the reviewers for their response. The results seem incremental. Again, while it is interesting and useful to discuss this, a more nuanced discussion is likely needed.
Reject. rating score: 4. rating score: 6. rating score: 8. <BRK>This paper proposes a temporal adaptive module for video recognition. The experiments conducted on several datasets demonstrate the effectiveness of the proposed method. Paper Weakness(1)	Figure 1 is confusing. Is there any insight for this design? However, only the ResNet 50 backbone is verified in the experiment. More experiments with other backbones such as VGG and Inception should be added. (5) In Table 3, the proposed method is not compared to the state of the art X3D method [*] on the Kinetics 400 dataset. The experiments are not thorough enough to demonstrate that the proposed module can be plugged into different 2D CNNs with good performance.<BRK>This paper presents a new temporal adaptive module (TAM) to generate video specific temporal kernels based on its own feature maps. The visualization of the statistics of kernel weights shows that the shapes and scales of distribution are more diverse and data adaptive. However, the paper can be improved further. 1.Some SOTA performances are ignored selectively, such as some performances of Slowfast[1]. Slowfast networks for video recognition. In ICCV, pp. 6201–6210, 2019Maybe the proposed method does not achieve the best performance, but it is necessary to compare with the SOTA methods completely. This may limit the final performance of the network. It is not straightforward to figure out how the attention weights or kernel weights are learned. Arrows in Fig.1 are also confusing. Some indicate names, while some indicate feature flows.<BRK>In addition, these feature maps are a combination of local and global features and as an exemplar, the author presents an architecture   TANet by incorporating their temporal operator. Together with a variety of experiments on standard benchmark for Video Recognition: Kinetics   400 and Something Something, the author showcases that for the task of Video Recognition compared to existing temporal operators, TAM s performance is fairly consistent and better and archives State of the art with similar complexity in their exemplar architecture   TANet. A seemingly reasonable approach has been proposed in this manuscript for the task of Video Recognition. Compared to the existing baseline and recent approaches, the proposed architecture   TANet achieves SOTA results. In addition, the exemplar showcased by the authors   TANet has been created by incorporating TAM in the existing 2 Dimensional CNNs to capture vast information which proves that the proposed module/operator is flexible enough and can be adapted to different frameworks/architectures for better performance. This new operator and exemplar TANet shows improved performance in nearly all cases on the datasets   The experimental evaluations demonstrate the effectiveness of the proposed architecture and showcase its practical value. Thus I do not have any major weakness issues after reading the manuscript several times. I found this paper pretty solid and have not able to found concerns relating to the proposed work. As mentioned earlier, I like the simplicity and wide applicability of the proposed module, and the architecture and setup details are provided in such a manner that it is very easy to convert into code in some timeframe.
Reject. rating score: 4. rating score: 4. rating score: 7. rating score: 7. <BRK>The paper provides a convergence analysis for GCN training and proposes two variance reduction algorithms to ensure convergence. The authors first draw a connection between sampling based GCN training and compositional optimization, and they divide the error of gradient into a bias term and a variance term. Based on the variance and bias decoupling, the authors then establish a convergence result for biased SGD (Theorem 1). The analysis itself is trivial and is not new. Although the paper is the first to apply the algorithm to GCN training (as far as I know), it is important to have correct references to clarify the contribution. This makes me suspect that the good results reported in the paper may be due to some parameter tuning instead of the algorithm itself.<BRK>This paper studies the convergence of stochastic training methods for graph neural networks. It provides theoretical convergence analysis for SPIDER used on GNNs, showing that the proposed method has a better convergence rate compared with the traditional gradient descent method. At last, this paper conducts experiments to verify the proposed algorithm. 1.Overall, this is a new application of SPIDER on GNNs. However, some assumptions are too strong, such as assumption 3. 2.Some parts are not very clear. In eq.(3), this paper claims that the bias term is mainly caused by node embedding approximation in the forward pass while the variance term is caused by gradient varinace in the backward pass. The node embedding approximation affects both the forward and backward pass. 3.For figure 3, the exact sampling method uses all neighbors. Therefore, it is actually the full gradient desenct method. In detail, the theorems study the convergence rate of (variance reduced) SGD. Thus, the experimental results cannot support the claim of theories.<BRK>Node sampling is a crucial point in making GCNs efficient. This paper finds that the convergence speed is related to not only the function approximation error but also the layer gradient error. 2.The idea of doubly variance reduction is reasonable. Cons:The biggest weakness of the proposed method is that it requires to compute snapshot features and gradients over all nodes (Line 5, Alg.1 & Line 5 Alg. The authors have provided the related analyses in the appendix. It will be better if the experiments without full batch shapshot are added in Table 1, as such we can check how it influences the final performance and if certain approximation will work well.<BRK>The paper draws the idea from VRGCN that integrates the historical latent representations of nodes computed with full Laplacian to approximate the that computed with sampled sparse Laplacian. The variance reduction is implemented on both node embedding approximation, as well as layer wise gradient computation in back propagation. ##########################################################################Reasons for score:Overall, I vote for accepting. The proposed variance reduction techniques can successfully accelerate convergence of any sampling method, according to the experiments, while also enjoys theoretical guarantee. The authors introduced a doubly variance reduction which can effectively reduce the node approximation variance the layer wise gradient variance of the existing sampling based GCN methods and accelerate convergence. 2.This paper also provides thorough theoretical analysis and convergence guarantee of the proposed algorithms. The quantitative results clearly demonstrate the effectiveness of the proposed algorithms. To better illustrate the idea the variance reduction, the authors could compare the proposed algorithms with a vanilla full batch GCN (instead of the mini batch training version Exact used in this paper, and it could be evaluated on smaller datasets like Cora). 4.The paper is well written in general.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 7. <BRK>*Summary:This paper investigates stochastic methods for finding an approximate stationary point for a non convex function that can be written as a finite sum. *Significance: The authors have missed a very important aspect of stochastic optimization. Basically, when m 1, the method in the paper is equivalent to GD and obtains the same optimal $O(1/\sqrt{T})$ rate.<BRK>However, I have some concerns:1. Therefore, I would expect comparisons against other methods as well, which is of interest to ICLR community in my opinion. Proposed contributions of the paper are stated clearly although I do not totally agree with certain claims. Could you please explain how this is obtained?<BRK>3) Is there any example that is independent of algorithms such that this assumption can be twisted to satisfy? However, after reading up to the analysis as well as the assumptions stated in the paper, it appears that Assumption 3 seems to be artificially enforced to achieve such a fast rate. So, I do not see the experiment reflects much the efficiency of the new algorithm if the test is only done with NN training.<BRK>My main qualm with the paper is that the modified algorithms appear to require O(m) memory in order to implement the windowed adagrad. The key new assumption is a “consistency condition” that bounds how big individual example gradients can be when the overall gradient is small. Overall I liked this work, the result seems interesting, and potentially will inspire future work. I liked the consistency condition here, but I think the paper would be very much served by a more in depth discussion of what this condition is saying.
Reject. rating score: 3. rating score: 4. rating score: 5. <BRK>This paper proposes to better represent relationships between different variables within time series and use the result to perform anomaly detection. The experiments show reasonable results. However, the algorithm needs to be presented in a cohesive way to be able to understand what is actually being done. Various steps are presented, but they are not tied together in a way that a reader can implement the algorithm. Note that, with OCSVM, time series or sequence oriented kernels can be used to enable it to be used with time series.<BRK>The authors present an architecture to detect anomalies in multi variate time series. To represent this complex input, they rely on a multi correlation attention. Their approach compute correlation while taking into account periodicity of the signal through FFT. The authors have to clarified their architecture and the computation that are made before proposing proofs. For me, the architecture is not clear. f1  > f4 are not formalized, the link between function f anf FFT should be clarified. Do the authors carry out some experiments in that direction?<BRK>The paper presents GenAD, a DNN architecture for multivariate time series anomaly detection. The key differences in comparison to existing approaches are two attention mechanisms tailored to capture patterns within time series and correlations between time series. Experimental results on two types of datasets show the effectiveness of the proposed architecture. Math formulas have issues  Evaluation is weak   essentially one of the two datasets show improvement   in the other dataset the difference is marginally betterDetails:  The novelty is low. However, this is something that all other methods can take advantage of. Writting is not clear, terms are mentioned (ripple effect) before introduced etc. Previous methods can take advantage of current attention mechanisms.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 6. <BRK>Summary: The paper presents a new model for the task of language modeling especially suited for longer sequences. The idea is that by bidirectionally attending simultaneously to the previous input segment and to a memory module, the decoder should be able to improve its generation capabilities. To evaluate performance of the Memformer model, the authors present results on the Wikitext 103 dataset. Are the perplexity results also computed over BPE tokens? Introduction section is not well written.<BRK>Summary:This paper proposes an encoder decoder memory augmented language model, called Memformer. Similar to recurrent neural networks, the memory part is updated as a new input token comes in. Reasons for score:Contrary to the paper s argument, I am not sure that Memformer improves computational complexity or performance on long range sequence modeling. I suggest following experimental settings in the Longformer paper [1]. Comparison of Transformer XL and Compressive Transformer with Memformer by reducing memory size (attention length) is not fair because they are designed to capture long term context.<BRK>The paper is clearly written and explains the concept well. Lack of experimentation: The authors compared other two transformer networks on just one dataset and presented their results with a hypothesis. The claims that the architecture can be scaled to any task is not justified with any empirical evidence. 2.In continuation with previous point, the idea usage and applicability is not justified. It is left for future work by the authors which doesnot go well with the hypothesis.<BRK>This paper proposes a new style transformer with external memory, which is updated and used through an attention mechanism. I think that section 2.1 requires an additional description: (e.g., in (a), what is the meaning of W_rE_{x_j}?, in (c), how W_k E_{x_j} can be the global content bias?) 2.This Memformer requires recurrent processing segment by segment, which can be another limitation on training or processing (can require more time than naive Transformer, which can process parallelly). 2.They also propose a new training algorithm, MRBP to update the memory with the local back propagation of loss, which makes sense and they showed empirical results about how much this algorithm reduces memory overhead efficiently.
Reject. rating score: 3. rating score: 5. rating score: 6. rating score: 7. <BRK>SummaryThis paper proposed an attention module in which each element of features is scaled based on the sign of its value. The authors argue that the combination of this attention module and ReLU can be considered as an activation function. The experimental results show the proposed method outperforms conventional activation functions in few shot settings of transfer/meta learning. Decision reasonSince the learning rates of the baseline methods seem to be not optimized, the experimental results are not convincing. I recommend resubmitting the paper after the tuning of baseline methods. Their learning rates are also different from these of other parameters? * How is the learning rate determined for the experiments in Section 4.4 and 4.5? Additional Feedback* Although it is interesting to formulate activation functions with an element wise attention mechanism, the merit of this new perspective is not clear for me.<BRK>Pros:This paper is well written and is easy to follow. It focuses on the basic component in neural network, i.e.neural activation function. Extensive experiments have been conducted and the corresponding experimental results are provided. Cons:While, in my view, the novelty of this paper is limited and needs more improvements. What s the difference between this paper and the proposed one? It seems the difference is limited, and they belong to the same family.<BRK>[Overview]In this paper, the authors proposed a new activation function called AReLU which introduces an attention mechanism to the original ReLU function. I would like to hear more from the authors in the rebuttal. The experimental results showed that AReLU can achieve much better performance with small learning rates while comparable performance with fairly large learning rates. This inspires another set of transfer learning experiments that demonstrate the effectiveness of AReLU. Also, for a lower learning rate, the convergence speed is much faster than other activation functions. This property facilitates the transfer learning scenarios as shown in Sec.4.4.[Weakness]1. Also, it is not clear about the final performance on CIFAR 100. [Summary]Overall I think this paper is well written and with a fluent flow to follow.<BRK>Description:This work presents a novel learned activation function called Attention based Rectified Linear Unit (AReLU). More effective training with a small learning rate. Impressive results for transfer learning and meta learning   Grad CAM on CIFAR100 show semantically more meaningful activations Weaknesses:  Results are reported on CIFAR100 and MNIST datasets. Implementation details are not provided. The paper is clearly written in most parts, It would be interesting to see other comments and discussions on this paper. Post Rebuttal update:I would like to thank the authors for providing relevant details and a thorough rebuttal to all the issues raised by the fellow reviewers.
Reject. rating score: 2. rating score: 5. rating score: 7. <BRK>This paper proposes to learn patient specific representation using patient physiological signals. In the supervised part, the classifier is generated from patient specific parameters by meta learning. Also, we encourage the paper to discuss why using the Euclidean distance not the cosine similarity as mentioned in Sec.3.1.In Experiment 5.4, the paper shows two examples of similar patients, where the performance is hard to evaluate. The authors are encouraged to analyze the computational complexity of the proposed method. It is unclear how to use the dataset for evaluation. The motivation of the paper could be improved. Also, some related works on contrastive learning are missing, for example [17 18]. In Experiment 5.2,  the detail of meta learning network, hypernetwork, is not mentioned in the paper.<BRK>ICLR PAPER PCPs: Patient Cardiac Prototypes##########################################################################Summary: The paper proposes unsupervised neural network models to learn patient specific representations for ECG applications. The architecture implements ideas from prototype networks and contrastive learning in order to discover similar patients across datasets and compress datasets based on the representations only. 3.In terms of tasks, the paper presents the patient similarity and the dataset distillation as the most important applications of this method. The hypernetwork module is not motivated sufficiently.<BRK>Some general comments. It would be helpful to know why authors used cosine similarity score as opposed to other metric. The motivation can be further emphasized. The English is at time very assertive and strong use of statement, where instead the patten of distributions are not confirming but rather corroborating since the experiments are not systemic but rather authors show few distributions that seem to demonstrate what they want to highlight. Code should be made available for this submission.
Reject. rating score: 3. rating score: 6. rating score: 7. rating score: 7. <BRK>#### DESCRIPTIONThis paper considers the representational ability of normalizing flows in terms of their overall size (depth, no.of parameters etc) and how they choose a partition for coupling layer transformations. None of this is clear. It would be remiss of me not to mention the fact that the paper has altered the margins of the ICLR template to increase the amount of material. As in Huang et al (2020), are the authors now doing variational inference in an extended space? Why is it worthwhile to have a result about how many affine coupling layers with a fixed partition are needed to compensate for the removal of a 1 x 1 convolution? Like Theorem 1, I m also not sure of the meaning/point of Theorem 2. Sections 4 & 5 & 6: I m finding it very difficult to parse what s going on here. The experimental results are toy, and I m not entirely sure what they re trying to show.<BRK>**Summary**This paper studies theoretical properties of flow models, mainly focussing on affine coupling layers. **Pros*** The paper presents interesting fundamental questions on the representational capacity of normalizing flows, which can help practitioners in their design choices of normalizing flows. The cons currently outweigh the pros for me, leading to the rating of 5, but I do think this paper could be a valuable contribution to the normalizing flow community. Therefore, I hope the exposition can be made a little clearer during the revision period so I can raise my score. However, the proof sketches and the relation between theorems can be made much clearer. * The authors empirically validate some of their theoretical results. **Cons*** There is quite some room for improvement on the side of the empirical results. First, empirical results are obtained by optimizing with an objective that is not maximum likelihood but a regression objective. * Please include masked autoregressive flows in related work on generative normalizing flows [2]. In higher dimensional cases such as image datasets it is unclear if this still holds, as dimensionality can potentially play an important role here. It would also be helpful for instance to relate theorem 1 and theorem 4 and their relationship.<BRK>The normalizing flows (NF) are among popular generative models, as they have the capability of converting a simple base distribution to complex distributions by successively applying change of variable formula. However, we lack a theoretical understanding of how much deep is enough to guarantee best results. It specifically studies the affine coupling layers and provides an upper bound and a lower bound on the depth of the layers required to achieve a good performance. Although this paper does not answers all the issues of the NFs, I think providing some interesting theorems on even simple questions, such as the effect of depth, could shed light on other questions in this field for other researchers. I think the authors should double check it to make it coherent with the other papers. 2  Although this is a theoretical paper, the number of experiments are limited. I believe that adding extra experiments can better reflect the value of the paper.<BRK>The paper gives very thorough mathematical representation for two challenges related to normalization flows, namely model’s large depth and conditioning which relates to the smallest singular value of the forward map. All variables and concepts are explained and presentation is clear. The cons of the paper are that it positions itself for previous research quite loosely. The paper would be a good handbook on the mathematics of the subject, but it is unclear at which level the topic has been addressed in the previous research, although the related work is presented in a short section.
Reject. rating score: 6. rating score: 6. rating score: 6. rating score: 6. <BRK>**Summary:**The paper proposes a new approach for generating black box adversarial attacks based on slowly varying contextual bandits. The proposed approach achieved query efficiency and success rate which are not too far away from the current state of the art. Better runtime than other Bayesian optimization methods (but it’s unclear to me how effective the Bayesian optimization approach is in the first place, see Cons below). The paper is clearly written. Thus, I would be very interested to see an ablation study for the success rate and query efficiency of (1) CorrAttack Flip vs (2) CorrAttack Flip with a **random selection of actions**. In other words, does the proposed Bayesian optimization framework really help, and if so, what is the margin? **Minor suggestions**  It would be good to include at least a short discussion why you selected the expected improvement acquisition function and not other alternatives. Page 4: “where d is the image pixels”  > “where d is the **number** of image pixels”  Implementation of the time varying property: for completeness, it would be also good to provide an ablation study that would justify the choice out of the two alternatives you mentioned.<BRK>This paper proposes a new black box attack that assumes access to loss oracle of the target model. It also provides a novel formulation of how the action space of perturbation should be searched by optimizing a surrogate reward function over a low dimensional space comprising image block location and a PCA based feature. The proposition combines several aspects of different attacks from the literature in one framework: sign flips, Bayesian optimization, hierarchical blocking, and exploitation of temporal  and spatial correlation in natural images. Table 1: NAttack achieves 100% on VGG16 and DenseNet. It is not highlighted in bold. Most of the discussion is around BayesOpt. Page 2, Sec 2, Paragraph 2: Reference of AutoZOOM is missing. Page 2, Sec 2: The context in which NES and CMA ES are presented gives the impression that they are adversarial attacks, please rephrase to indicate they are gradient free optimization techniques. Eq.9 : the notation is not clear. Is $e_{ijk}$ of the same dimensionality as $\nabla_{x_t}l(x_t,y)$?. It seems that most of the computational gains that CorrAttack has over BayeOpt and BayesAttack is due to the use of **scalable** GP regression (GPytorch)   not to mention the low dimensionality of its GP. In particular, is PCA performed for each block (pixel level PCA) or over all the pixels of the image? Just as the authors compared the Bayesian aspect of the attack to Bayesian attacks (BayesOpt, Bayes Attack), it would have been great if the authors could contrast the "flip" aspect of the attack with flip based attacks e.g., SignHunter & SIMBA  that the authors mention in Sec 2.<BRK>The paper proposes to use time varying contextual bandits in order to improve the query efficiency of score based adversarial black box attacks.The effectiveness is demonstrated on various classifiers for ImageNet and compared against several baseline methods. The improvements over the baseline methods seem significant, although I have one question: in Algorithm 1, what is the actual value for m used in the experiments? This is an important detail because it determines the number of queries that CorrAttack needs to perform upfront. Is it correct that, depending the actual value of m, some of the baseline methods may actually be more efficient than CorrAttack if the attacker only wants to compute a single adversarial example? Did you use the same hyper parameters as in the original work? : last paragraph before Section 4.1: the specific context of images ia introduced a bit abruptly; it will be good to make more clear which parts of the paper apply to classifiers in general, and which ones only to the image domain. : “we also find that the”  > “that” should be omitted p.5: what is “EI”? p.8: “find the action the large award”  > something’s missing in this sentencep.8: last paragraph: what is meant by the “embedding from the transfer based attack”? The very last sentence about adversarial training doesn’t make much sense to me; in particular, defending against white box attacks should also work against CorrAttack; on the other hand, the objective of adversarial training usually isn’t to defend just against one specific attack, but to make the model robust against worst case examples within e.g.an l p ball, regardless of what attack algorithm may be used to construct such examples.<BRK>This work considers an important problem of generating adversarial examples to attack a black box model. The paper proposes a new approach to consider an adversarial example as a result of a sequence of pixel changes from a benign instance. Therefore, the adversarial generation problem can be considered as a bandit problem, and thus we can leverage Bayesian optimization to search for an instance that maximize the changes on the loss function through a sequence of pixel changes. The only concern is that I m not so sure why the speed of CorrAttack is much faster than BayesOpt and Bayes Attack. It seems that the main reason is because the work defines a block structure (as in Sec 4.1), so that the number of blocks is much smaller than the number of raw pixels. Such a hierarchical structure can naturally lead to a hierarchical search procedure as discussed in Sec 4.3. Such an idea doesn t seem to be unique to the bandit setup, is a similar idea also applicable to previous work such as BayesOpt and Bayes Attack? I hope such an issue can be discussed more in the revision.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 5. <BRK>This paper analyzes the current limitations of existing magnitude based pruning methods. First, the paper focuses on the similarities between three methods and then focuses on the redundancy in large networks. The paper also analyzes the weight distribution for a well trained network and propose CDWA as a way to prove this distribution. My main concern with this paper is the contribution to the field. I guess I am not very surprised the first two are similar in the structure and slightly different in performance as that is expected. 2.In terms of redundancy, I am not sure I understand the point there. I think that should be reconsidered as L2 is the magnitude and there are many works using other criteria referred as importance (for instance gradient *magnitude). All in all, not sure the real contribution to the community.<BRK>### About weight distribution assumptionThe authors assume that all well trained filters are i.i.d and follow a Gaussian alike distribution, plus the co variance matrix is a diagonal matrix. In other words, these weights in filters should be highly correlated to extract features (though may not be linearly correlated), which seems to contradicte the assumption. ### About contributions of the paperAs I understand, the contributions of the paper are mainly two folded: 1) Weight distribution assumption and its correponding conclusions. 2) The paper reveals the similarity of some previous pruning algorithms and finds they may not be good criteria. ### Updates after rebuttalAbout the contributions of the paper, it does provide some interesting points of view. The concerns are as follows: First of all, why the authors claim it as a diagonal matrix when actually observing it as a block diagonal matrix? Specifically, the assumption in the first version of the paper is a paradox to some extent, and the revised version casually modifies the assumption without too much verfication. I have to decrease my rating to 4.<BRK>## Summary The paper discusses various baseline scoring mechanisms used for filter pruning that are norm based and finds that none of the scoring mechanisms are particularly effective at pruning filters from CNNs. * I didn t carefully check all the math but it seems to be reasonable. These conclusions are based on a theoretical (and experimental) analysis of the various scoring mechanisms under the assumption that trained filter weights follow a Gaussian like distribution, which reveals that in this case the scoring mechanisms are insufficient to reliably discern the importance of filters since the resulting scores are very similar. Or are these methods already a stepping stone in the right direction towards building more meaningful pruning criteria? Having the baseline methods alone is not sufficient in order for the community to build on top of this work in a meaningful way. * The CWDA assumption is verified using a large set of experiments and the authors also discuss the limitations of the assumptions. This is really encouraging to see and I appreciate the rigor. ## Weaknesses* The lack of additional filter pruning methods as stated above. * The authors discuss the limitations when CWDA does not hold. * Is there a difference between $F_{i,j}$ and $F_{ij}$?<BRK>The key ingredient is the CWDA assumption that filters in a particular convolutional layer are iid and approximately follow a Gaussian distribution, which is shown based on extensive statistical hypothesis testing. Based on this assumption, they prove that these pruning criteria are roughly the same. **Pros:**I like the nature of the question asked in this paper. The paper is backed with extensive results testing their CWDA assumption and proving how the criterion are overlapping in their functionality. Also, at some other places like in the results of different pruning criteria, the experimental methodology and presentation is rather loose. I would have also liked to see at least the empirical results for global pruning because there even some out of these norm criteria might be better than others. And, I understand that your main point is showing that many norm based methods are ranking filters in more or less the same way. If you are pruning to small levels, then it does not matter as much of course (also known from results of other papers). The testing should be done precisely in the regime of high pruning ratios (> 90%, 95%), so that you actually see the difference.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 5. <BRK>### SummaryThis paper proposes to apply uncertainty based measures to guide the collection of training samples for reading comprehension. But at this point in the paper it’s unclear what the vector space represents (tokens in the input?). Finally, the results seem inconclusive to me, and it is unclear whether statistical variations are dominating effects in SQuAD and NewsQA.<BRK>I like the approach of ensuring that the model weights do not deviate too much as this also addresses the challenges faced in active learning when we end up sampling outliers. The paper is easy to follow. Cons:  All evaluations are conducted using a Bert base model. BERT based models produce highly confident predictions and to visualize the distribution from your empirical investigation would help bridge the divide between the equations and the empirical results (how they actually turn out in practice).<BRK>The paper is tackling the problem of labeling cost in Machine Reading comprehension. I find this proposition pretty interesting but trivial compare to uncertainty based active learning literature applied to text [2, 3].<BRK>* The algorithm requests labels for the most “informative” (or uncertain) instances with respect to the current model. Having to collect 25% less data is not so desirable, especially if there is suspicion that the active learning algorithm might introduce unwanted biases.
Reject. rating score: 3. rating score: 3. rating score: 5. rating score: 6. <BRK>SUMMARY This paper introduces GP NC, a new methodology that allows Gaussian processes to stay far from certain data points (referred to as negative datapairs). These negative datapairs complement the standard training points to be fitted. ###################################################PROS1) I am not aware of previous works in the GP literature dealing with this scenario, i.e.data pairs to be avoided during training. However, this type of problem is not addressed in the experimental section. I would expect to see how the proposed method compares to the previous approaches that have modelled the navigation problem with GPs. See e.g.:* caption of Fig.1 "is been given". * The results of GP NC (the proposed approach) and GP are very similar.<BRK>This is done by iteratively training a standard GP and maximising the KL between the GP and blobs of the negative data pairs. The problem tackled in the paper is of relevance in fields such as spatial statistics and robotics, with possible applications also in more general ML tasks. Concerns 1. Rather than artificially changing standard test data sets to fit your task setting, it would have been more interesting to see actual problems, where the model class would have been beneficial. As a practical suggestion: Background material could be presented more concisely, the methods section refined to be more concise, and the additional space used for improving experiments and discussion. 5.From how Alg.<BRK>I especially like the scalability aspect, since it would allow the use of a otherwise prohibitively large amount of  negative  examples. The problem of GPR with both  positive  and  negative  data points (and in the context of robot navigation) has been investigated before [1], where a non stationary kernel function is proposed. This makes it hard to assess how well the proposed method works for real problems of the kind the paper intends to address. If $\theta$ is changed, then the first distribution in the KL divergence is no longer the GP model learned from the positive data points. I suggest that this section is made clearer and more precise. > "in both the positive and negative data pair set."<BRK>The proposed method is called the Gaussian process with negative constraints (GP NC). This paper is also technically sound and to the best of my knowledge is novel and relevant to the community. In figure two you have mentioned you sampled the inducing inputs randomly from the whole range of training inputs. In the error calculation, standardized mean squared error (SMSE) could be a better choice than RMSR since the former incorporates variance information as well. In the experiment section, all negative datapairs are synthetically made. The paper is indeed addressing a very practical issue in the ML community. After reading other reviewers  comments and concerns I decreased my score.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 6. <BRK>The paper studies poisoning attacks against online reinforcement learning agents. The attack can be completely black box, meaning that the proposed method allows an attack setting where the attacker has no knowledge of the RL algorithm used by the victim agent or the environment. In this paper, the attacker needs to imitate the victim learner, and the accuracy of imitation result will depend on how many training data are available. The experimental part of the paper is also strong.<BRK>Especially, how is the reward accessible to the attacker? Pros:1.It is an important question to investigate how policy based RL algorithms can be poisoned by adversarial attacks.<BRK>At a high level, the approach this paper takes can be summarized as follows: It defines the optimal poisoning attack problem in an unknown environment as a **sequential decision making problem**, which is well motivated and clear. Is it estimating the value of the optimal policy, the learner s current behavior policy, or the learner s current $\hat\pi_k$ which according to the paper is different from the behavior policy? All rewards are zero except for when the agent successfully arrives at the right most state, which takes H right actions consecutively.<BRK>.The first sentence in Challenge II is also not clear: why is Markovian property important in  are no longer i.i.d.due to the Markovian property ? Overall, I enjoyed reading the paper, and its contributions seem novel and important for the line of work on poisoning attacks in RL. doesn t seem to be precise (since dynamics can be obtained from Atari simulator...).
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>This work describes a set of benchmarks for offline policy evaluation and selection. The work builds upon existing benchmark tasks for offline RL. The work as presented makes little attempt to connect with representation learning which is the focus of the ICLR conference.<BRK>it would be great if the authors could give some statistics to measure the property of the datasets, such as a measure of the missing support of logging policy, how difference are the logging policy and target policy? This will definitely give some further directions like which method perform well in which setting. Will is possible to also add some datasets with propensity included? However, it is missing in this paper and it would be excited to see how it works in the D4RL tasks.<BRK>## SummaryThe paper proposes an off policy evaluation dataset (DOPE) for various control tasks. This is going to allow researchers to do a more thorough analysis of offline reinforcement learning algorithms. It is clear what the "features" of the proposed benchmark are, and what problems the benchmark addresses that the other benchmarks didn t address before. The authors include various baselines for off policy evaluation and evaluate them with additional metrics to MSE, namely regret@k and rank correlation.<BRK>This article proposes a benchmark of off policy evaluation, which provides different metrics for policy ranking, evaluation and selection. And the author proposed absolute error and rank correlation in the benchmark to evaluate the strategy. Verify the effectiveness of different offline evaluation methods. The paper’s key strengths:1.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 8. <BRK>I m afraid the current version of the paper is quite off this mark. Putting formal soundness aside, my present understanding of the idea of the paper is the following: The authors observe that many basic computations on graphs can be parallelized into a few computations of small width and depth. Possibly expand the experimental section.<BRK>The authors show that it works well in practice on several synthetic datasets, and show many theoretical results. Unless I am mistaken the  GNN+ architecture requires different features inputs for all copies of the GNN, this should be added into the parameter counting that is done.<BRK>This paper proposes a new building block for GNNs, called GNN$^+$. + The paper is clearly written for most parts and the key intuitions behind the theoretical results are explained in sufficient depth. The paper lacks evidence if this model outperforms GNNs on real world graph datasets. The central premise of the paper seems to be that the GNN$^+$ models require less depth (and total parameters) compared to standard GNN architectures. This is not clear from the experiments.<BRK>The main question this paper tackles is: can one develop sample efficient architectures for graph problems while retaining the simplicity of the message passing framework? This paper introduces a theoretically principled architecture   GCN+   which is attempts to make GCNs more efficient by using ideas from the subfields of sketching approximations and parallel computing. ############Pros+ An interesting paper with novel contribution combining ideas from parallel computing and sketching approximations.
Accept (Spotlight). rating score: 7. rating score: 7. rating score: 7. rating score: 7. <BRK>This paper provides an interesting empirical study of self supervised image embeddings as an alternative to pre trained ImageNet classification models, for the purpose of evaluating the quality and diversity of GAN generative image models. I would also find it insightful to better understand *why* self supervision works better for evaluating representations?<BRK>In general, I think the paper can benefit from more analysis around the choice for the right self supervised network and trade offs. ## SummaryThe papers looks at the problem of evaluating GAN samples. In all experiments the authors show that the self supervised trained model produces a GAN ranking that is closer to the truth.<BRK>The authors use 5 datasets and their respective representations from 5 models, 3 supervised and 2 self supervised, to show that representations from self supervised models lead to better GAN evaluations. A ranking of the GAN models shows inconsistencies between supervised and self supervised based representations. The experiments are extensive and support the claim of the authors. Testing for invariances of representations is an interesting idea and the results support the use of embeddings from self supervised models. It would be interesting to see how such representations, e.g.from the autoencoder used to show the invariances described in section A.1, behave compared to the proposed self supervised representations.<BRK>Experiments are conducted of multiple large GANs and datasets. Novelty: I am not aware of previous works investigating self supervised features for GAN evaluation, but note that [1] evaluated self supervised features as a perceptual loss which is highly related. Overall: the investigation of better ways of evaluating GANs is important.
Reject. rating score: 4. rating score: 5. rating score: 7. <BRK>SummaryThis paper attempts to address the fundamental barriers of learned optimization. The proposed model is very simple in theory but the implementation seems to still require quite a bit of “hand engineering” in terms of selecting features etc. Concerns:  My main concern with the paper is that some claims are over blown. Statements like, “We see this final accomplishment as being analogous to the first time a compiler is complete enough that it can be used to compile itself.” and “we believe learned algorithms will transform how we train models" are too strong given the current evidence of the performance of the learned optimizers. I would suggest the authors tone down these claims. For example, on CIFAR 10 in Figure 5 the learned optimizer fails even for batch sizes in the training regime. For example, the “careful tuning of learning rate schedules and momentum timescales” is traded instead for the selection of design of a sufficient range of tasks on which to train the optimizer. This seems to be a far more difficult task than just tuning a few hyperparameters. The contribution of the paper in terms of new insight or knowledge is not clear. The “large scale” training on a wide range of tasks and many unrolled steps is interesting but I’m not sure what new insights can be inferred from this? Furthermore, the hierarchical optimizer seems to be a small improvement of the mode proposed in (Wichrowska et al., 2017) with some additional input information (like validation loss).<BRK>The authors propose to use a few thousand tasks as developed in Metz et. The task is a challenging one and the authors propose a hierarchical optimizer similar to Wichrowska et. al.I wish the authors invested more thought into the architecture of the optimizer beyond assimilating features of previous work. My main concern with the paper is lack of generalization capability of the proposed approach. The authors could have done a more thorough job evaluating the architecture by performing ablation studies, i.e., different permutation of inputs to the LSTM and feed forward modules and understanding its effect. I am also interested in understanding the trade off involved in using ES over unrolled optimization. There should be some experiment discussing this trade off as it seems to be a critical improvement over prior approaches. The sub optimal performance on imagenet is concerning.The authors should also provide guidance on task selection for optimizer training.<BRK>The goal of a learned optimizer is to replace a human designed optimizer with a parametric optimizer. However, prior learned optimizers were ineffective at generalizing to a diverse set of tasks. This paper investigates how to learn a useful optimizer by increasing computational scale, building a large, diverse training dataset, and designing the learned optimizer s architecture. Strengths: + This paper thoroughly examines the challenges of how to train a learned optimizer. Weaknesses:+ Training the learned optimizer is fairly complex and computationally expensive, which will prevent broader adoption and makes the paper difficult to reproduce. Questions:1) What conditions are necessary for a learned optimizer to replace a standard optimizer like Adam? 2) Could a learned optimizer work with noisy, quantized gradients? e.g.deploying a learned optimizer in a federated learning environment
Reject. rating score: 2. rating score: 3. rating score: 4. <BRK>This provides a more efficient alternative to the traiditional approach, which is based on the astrophysicists workforce. REASONS FOR SCOREIn my humble opinion, this work does not present a machine learning contribution of interest for the ICLR community. It applies standard and well known approaches to an specific remote sensing problem. Moreover, I would recommend the authors to compare their approach to some others methodologies used in the field. Right now, the experimental section only analyzes the results of the proposed approach, with no baselines.<BRK>The results show that the methods perform quite well in terms of RMSE and explained variance. It does not deal with representation learning, and the methods are too basic for a top tier machine learning conference in general. In addition, the experimental evaluation, which is the main body of the paper, is quite weak and will need significant improvements. KDD might be a better fit or an astronomical journal such as PASP. This is not a problem per se, however, as it is not clear that a multi output model would be better suited for the task considered here.<BRK>For what it s worth, I would have been interested to see a description of the CASA routine for calibration to understand the difference between this and the ML algorithms.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 6. rating score: 6. <BRK>The performance over existing methods across multiple datasets is significant. + The capability of cross domain video synthesis could be beneficial, particularly when high quality video datasets are difficult to collect. + The appendix and the supplementary material provide extensive details about the experimental settings and results that would help reproduce the results. **Weakness**  When discussing the difference over [Tulyakov et al.2018], the paper states “…applies h_t as the motion code for the frame to be generated, while the content code is fixed for all frames. It seems to me that the proposed method can only handle 1) “subtle” motion, such as facial expressions and 2) short video sequences (e.g., 16 frames). In sum, this is a paper with an interesting idea and extensive experiments.<BRK>Overall, the authors have adequately addressed my questions and concerns. ______Pros:  Novel objective formulation that controls for motion diversity, disentanglement and content matching. High resolution video generation at 1024x1024. My recommendation is to **accept** this work for publication to ICLR 2021. This should be investigated in the paper so as to ascertain the limits of this approach. These are all uni modal datasets.<BRK>* Equation 4 (and the last paragraph of Section 3.1) shows that the motion generation is evaluated with the loss using the image generated from the image generator. Reasons to accept the paper:* The decoupling idea is new and interesting from an interested outsider s perspective. * Evaluation is pretty comprehensive and the results are decent. I have a few clarification questions though.<BRK>SummaryThis paper proposes a method to disentangle content and motion from videos for high resolution video synthesis. In experiments, video generation by the proposed method is performed on UCF 101, FaceForensics, and Sky time Lapse datasets. Overall,  about this paper, I am leaning on the positive side. The proposed method was tested on the various datasets and showed experiments about cross domain video synthesis. It is better to conduct a comparative experiment with the same metric and the same compared methods for all datasets. There are qualitative results in section4.2 about cross domain video generation. Are there results that have been verified quantitatively?
Reject. rating score: 3. rating score: 3. rating score: 4. rating score: 5. <BRK>Summarythis paper propose a constraint in GAN training, to improve generated samples fidelity and stabilize training. It would be also interesting to explore this in combination with proposed normalization5  what is the impact of the proposed normalization on speech recognition performance? 6  The evaluation on image generation is also required for the proposed normalization approach.<BRK>This is intended to prevent instability during training. * I would recommend including the steps of the overall learning algorithm for those who want to better understand the concrete steps involved in implementing the approach and how the DDFN objective is integrating into the standard GAN learning algorithm. The technique is applied to audio spectrogram generation. However, I do feel like the paper was quite low on clarity.<BRK>This paper proposes a conditioning method to train the GANs. The conditioning trick is based on the DFN (departure from normality) metric computed in the spectrogram domain of Schur decomposition to ensure the correlation between real and generated samples. Cons:  The major concern about this paper is the lack of thorough experimentation to prove the usefulness of the proposed method. The paper is not easy to follow. So are the descriptions of the figures. Therefore, I recommend the paper is not ready to be published in its current form.<BRK>The paper proposes a novel conditioning based regularization of GAN training. The experimental results also support the authors  argument about the improved stability and results in terms of objective metrics. One thing I wish the paper could improve is about the reason as to why the proposed method is superior to the existing regularization techniques, which is somewhat buried in the mathematical details.
Reject. rating score: 2. rating score: 5. rating score: 5. rating score: 6. <BRK>I spot a couple of key issues with the proposed method:1) Misinterpretation of the previous study by Shen et al.(CVPR 2020): As presented in Eq (8) of that CVPR paper, the dataset where the influence loss is applied is NOT necessarily the old training set; It  can be the new training dataset. Also, their results show that using new training set for the influence loss is slightly better than using old training set. So Eq (3) of this submission is not complete and somewhat misleading. 2) Shen et al.(CVPR 2020) only use the classifier but not the feature model.<BRK>This paper addresses an interesting problem in retrieval system   compatible features learning. Given the old feature extractor and a new dataset, the objective is to learn a new feature extractor, so that the features extracted by two (old and new) feature extractors are comparable to each other. The construction of the pseudo old classifier is well motivated and reasonable. Second, the formulation of the loss function (L) is not given. Third, there is no experimental comparison with other works. So it is hard to see the contribution of this paper.<BRK>This paper deals with an interesting problem of feature compatible learning that the features produced by new model should be compatible with old features. 5.More experimental results, e.g.visualization, should be given.<BRK>The new constraints avoid using old training data and the old model’s parameter when learning a new model. The constraints added to the feature compatible learning are sounded and may have practical value. 2.The writing of this paper is satisfactory. 2.The gain of the RWN classifier is not significant. The RWN should be compared with those simple baselines. 3.There are several minor typos.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 5. <BRK>The paper presents FEDBE, a novel method for federated learning. The global model is constructed by Bayesian model ensemble. Knowledge distillation is used to construct the model that is passed to the clients for the next round. FEDBE is simple and shows strong empirical performance. The paper is well written. Clearly state your recommendation (accept or reject) with one or two key reasons for this choice. This type of work is definitely needed to enable more powerful models that preserves the privacy of the clients. Questions:  What is the effect of number of epochs between model collections (25 steps vs. 50 or more)? Is this what is shown in Figure 10? What is the effect of the step size decaying on SWA?<BRK>  Summary  The paper proposes a new approach to model aggregation at each round performed on the server using a combination of Bayesian ensembling, model distillation, and stochastic weight averaging. Overall evaluation  I find the paper well written and the idea of using ensembles and distillation for model aggregation in FL settings very interesting. Although the overall idea is identical to FedDF by Lin et al.(2020) who proposed to use ensembling and distillation and the authors cite and compare with that work, I think forming the ensemble distribution, sampling from it, as well as using SWA for distillation are all novel components; the contribution of each introduced component is empirically analyzed in ablation studies. In some practical settings, FedBE could be a great replacement for FedAvg, but in other settings, it might be too compute intensive. Can the authors explain this phenomenon? Also, which metric was used exactly to tune the number of local epochs for FedAvg?<BRK>The labeled dataset is used to train a global model. Pros:(1) The paper is well written and easy to follow. (2) The studied problem is important and the proposed idea is interesting. I have the following concerns, mainly about the experiments. Can the authors explain it? (4) The size of unlabeled public data is large in the experiments. Can the authors show the performance of FedBE with a smaller public dataset? (5) The authors can add a figure to show the test accuracy of different approaches versus the number of communication rounds. Which version is used in the experiments? Since the paper focuses on the Gaussian distribution, the authors can present an example based on Gaussian distribution instead. Post rebuttal Thanks for the authors  response. The authors have addressed some of my concerns.<BRK>For this purpose, this paper proposes FedBE, which combines the idea of Bayesian model ensembling and distillation based on stochastic weight averaging. A set of unlabelled data on the server is required for distillation. The paper is well written and easy to follow. For baseline method like FedAvg, these 10K images are useless since it does not need unlabeled data for model average on the server. The different number of total training instances between FedBE (40K labeled + 10K unlabeled) and FedAvg (40K labeled) makes the comparison unfair. FedBE replaces the original model average with ensemble prediction & training, which is very slow. And these computations on the server seem cannot be parallelized with the computation on the client.
Reject. rating score: 2. rating score: 2. rating score: 2. rating score: 3. <BRK>The paper is very poorly written. The exact relation to the Guan et al.2020 paper, which appears to be important, isn t properly described. Some examples of lack of clarity and poor writing:Introduction: * "commonsense knowledge graph"; not clear what "commonsense means here.<BRK>The authors validate their methods on rhetorical text generation. pros:1.Rhetoric is a stylistic aspect of literature. It is interesting to consider this attribute in natural language generation. This paper aims to introduce knowledge to guide text generation and improve its logicality, but it only evaluates the proposed framework on rhetorical text generation.<BRK>5.What do authors make sure the evaluation of artistic aesthetics is consistent and meaningful? Pros:+ The idea of using a knowledge graph to generate rhetorical text is interesting. Cons:  Overall, the paper is poorly written. Which transformer model is used as the language model?<BRK>Experiments should also include comparisons with other knowledge enhanced generation methods. The framework is a simple combination of existing methods (pre trained language models like Bert and GPT2), and the construction of the knowledge graph is not so novel. 5.As this paper proposed a new dataset, authors should make it possible to be followed and reproduced.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 5. <BRK>The method combines two existing techniques in fairly obvious way and despite good empirical comparisons has also issues in evaluation since more recent comparison methods are missing. Detailed feedback: The related work and importance of the application are well covered, and the technical solution is sound. The conceptual novelty is, however, fairly limited; several sparse CCA variants have been proposed in the past and switching to proper sparsity ($l_0$ vs more common $l_1$) is a natural thing to do. In summary, the paper does not make fundamental conceptual or technical contributions. They do show that the method works well in comparison against reasonably chosen competing methods, but do not clearly indicate qualitative change in CCA applications.<BRK>This paper combines an approximate $L_0$ regularization on the canonical vectors with CCA to encourage the CCA for getting sparse vectors. The paper seems to be a combination of deep CCA (Andrew et al.2013) and Louizos et al.2017.In particular, the $L_0$ regularization approximation is very similar to that proposed in Louizos et al.2017.It would be great if the authors could be more clear on illustrating the differences (if any). Therefore, the novelty of this paper is unclear. In addition, the major benefit of using neural networks as embedding function is the ability to capture non linear relationships.<BRK>This paper presents a new deep CCA method to learn non linear relationships between two modalities. It trains two neural networks each for a modality to maximize the total correlations of their output representations. Experiments on one synthetic and two real datasets demonstrate the superiority of the proposed method. 2.Minor comments (notation inconsistencies/abuse, typos, etc.):The sentence "For example, in biology ... and engineering (Chen et al., 2017)" is not complete (sentence fragment). In Section 2.2, first paragraph, last sentence "The total correlation in Eq.4 can be expressed using the trace of ..."  "total correlation" should be "total squared correlation".<BRK>1.Paper summary:This paper proposes a DL method for learning sparse non linear transformations that maximize correlations between two views. In particular, each view is passed through a separate network. The two networks are jointly trained by maximising the correlation between their outputs. 2.Strong points of the paper:Stochastic Gating gives way to an objective function that can be optimized through Stochastic Gradient Descent. Similarly to DCCA, the method suffers from the two issues. However, the loss function is not decomposable into batches. ICML (2) 2013: 316 324
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 8. <BRK>Summary:The author proposed a set of tools to analyze the properties of the learned neural network based optimizers. The author takes an RNN based optimizer as an example and analyzes it in the empirical section. The author investigates several properties including (1) momentum using linearization (2) gradient clipping using the function visualizer (3) learning rate schedule and (4) learning rate adaptation. What advantages does this behavior provide? Review:Clarity: In general, some parts of the paper is clearly written and motivated such as the problems of the current learned optimizer. However, the paper quality can be improved if some sections can be more clearer. Novelty: The tool seems to be novel but the idea of linearization is not new, which has been used in other analyses of the dynamical systems as mentioned by the author. How these properties are combined is not analyzed. 3.The author only analyzes one NN based optimizer. are there any other forms of the leaned optimizers? What advantages of those optimizers compared to the one used in the paper? 4.As mentioned in the clarity, better use mathematical equations to explain certain terms rather than words. 5.In section 3.2, I am a bit confused about the difference between the fixed point and convergence point? Any examples of scenarios that it is a fixed point but not a convergence point?<BRK># SummaryThis paper studies learned optimizers and tries to discover what they have learned. The authors argue that the dynamics of the learned optimizers are responsible for those behaviors. # Strength  This seems to be the first paper on re discovering what the neural optimizers have learned from dynamical system angle. Overall, the paper is easy to follow. Even in the original neural optimizer paper by Andrychowicz et al.they considered CIFAR (subset). If we only run the optimizers for several hundreds of steps, I think it s totally affordable. But it is not clear to me which behaviors shown in Section 4 are considered novel? But in Section 4.4, paragraph 4, the authors say ".... similar to the changes observed as the RMSProp state varies," which might suggest this is not novel to human designed optimizers with a memory mechanism. Even this is novel, this is only observed on three toy datasets.<BRK>This paper aims to shed the light on the work (mechanisms) of the learned optimizers. 2.The authors applied the methods from dynamical systems to the analysis of learned optimizers. While the choice of such simple tasks is well motivated by the fact that optimizers training is very computationally intensive the choice of training method from [1] leads to significant limitations of obtained results from my point of view. As authors from [1] noted the considered method has problems with generalization (for example, if the activation function of the inner task is changed, the learned optimizer is no longer able to generalize and train NN with the different activation function). Overall, this paper is well written, but there are some sections of the paper that are difficult to follow. I would recommend adding into the paper a brief description of the method with its limitations and all necessary definitions. Some other questions and issues:1. In Advances in neural information processing systems, pp. 3981–3989, 2016.<BRK>I very much enjoyed reading it. As with any black box system like a learned optimizer, there is naturally a lot of interest in what, actually, the optimizer itself is learning, and why it has learned in the way it has. I found the experiments to be comprehensive and the figures to be adequately described. (For the record I am not saying I disagree with the results, only that if more detail can be provided, it would be helpful.) The experiments are only performed with one form of learned optimizer, using an RNN with 256 GRUs. What happens if different learned optimizer architectures are used? Adding some extra information about this question to Section 4 could improve the paper. Overall, I think this is a good paper and should definitely be accepted. You might consider switching Figure 2 and Figure 1; the background on learned optimizers meshes well with Figure 2 and so it may be more streamlined to introduce it there. However, your call. This also applies to Figures 4 and 5.
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 6. <BRK>The paper studies how to solve a class of group sparsity regularized minimization problems. In particular, a half space stochastic projected gradient (HSPG) method is proposed, which is based on the Prox SG and a new half space step that promotes group sparsity. Convergence analysis is provided, together with the theoretical discussion that HSPG has looser requirements to identify the sparsity patter than Prox SG. Numerical experiments on the DCNNs based image classification shows the proposed method achieves the state of the art performance in terms of accuracy. The work looks interesting with wide applications, especially in deep neural networks. 2.Practical guidance on the selection of the parameters $\lambda$ and $\varepsilon$ could be provided. 3.In the numerical experiments, comparison of computational complexity and running time for the listed methods is not provided.<BRK>This leads to an estimator with enhanced group sparsity without the sacrifice of accuracy. My major concern is how to use such a group sparsity result? We end up with a more group sparse estimator, which is good. But I feel like that is not the end of the story. In a deep neural network, the group sparsity seems not the major point people care about. In all, my understanding is that the group sparsity could be an intermediate result that can be further analyzed and used to improve the design of the model, instead of being the final goal itself. If we start with different initialization (probably just slightly different), then will we end up with the same final estimator or at least the same group sparsity results? After obtaining the final estimator with group sparsity, can we refit the model on the active group index only? Will this improve the performance?<BRK>This paper proposed a new algorithm for the group sparsity regularization problem. The new technique requires an initialization that is closed to some truly sparse local minimum, which is achieved by running proximal gradient descent first. The authors also provide convergence analysis and numerical evidence for the newly proposed algorithm. It is not clear to me, in Theorem 1, how are the parameters depend on the confidence \tau? I am confused as it seems no parameter is explicitly dependent on \tau, so the convergence in Theorem is almost surely one? I don’t understand why (1+\theta) can be omitted. No rate of convergence for either the initialization phase or the half space projection phase.<BRK>Theoretical analysis tries to show the sparsity identification guarantees. In experiments, the effectiveness of HSPG was verified on the classification tasks. [Strength]The idea behind the proposed method seems to be reasonable and interesting. [Weakness]A major concern is the correctness of the statements. In the equation (97) in the proof, the equation $E_B[e(x)]   0$ is used essentially, and it is also stated in page 6. However, I think it does not hold because the proximal operator associated with sparse regularization is nonlinear. It may be probably difficult to fix this issue. It is known that RDA has the superior ability to find a manifold structure of solutions as shown in the following paper. Manifold identification in dual averaging for regularized stochastic online learning.
Reject. rating score: 2. rating score: 2. rating score: 3. rating score: 5. <BRK>Summary:The paper proposes a variation of the conditioned Transformer based language model. The authors use POS labels (for English and Chinese) and participial construction labels (only for Chinese) to control the output of the decoder and show the results are better than an unconditioned generation with GPT 2 in terms of several metrics. Cons and questions:  The paper describes a generic conditioned language model approach, I can see no novelty of methods or results here. No definition of "structures" given. Instead, the authors use fuzzy formulations like "multiple types of multi granularity structure information" or "the auxiliary model can be any credible model or tool that can extract soundable structure information from the template." No clear description of the resulting architecture is provided ("We only modified the input representation and few parameters of transformer").<BRK>SUMMARYThis paper presents a text generation model conditioned on desired structures. The proposed method is essentially a translation model from structure information (represented with multiple sequences of tokens) to a text. This study converts a text into structure information such as part of speech (POS) and participial construction (PC). Then, this paper proposes Structure Aware Transformer (SAT), which is essentially the same as the Transformer architecture. This paper reports that giving structure information improved the performance in PPL and BLEU compared with GPT 2.<BRK>Review of Paper: Structure Controllable Text GenerationSummary:This paper proposes a structure aware Transformer (SAT) by incorporating multiple types of multi granularity structure information to control the text generation. Weakness:•	The paper does not clearly define the input content of this task. It is unfair to compare this model with GPT 2, because it’s not a standard language modeling task. •	The baseline is not solid and suitable for the datasets they use. Reference:[Li, P.] Li, P., Zhang, H., Liu, X., & Shi, S. (2020). arXiv preprint arXiv:2004.08022.<BRK>Other information, such as syntax, also should be considered. 5.Some related references are missing. The title of paper is "Structure Controllable Text Generation", but the proposed method is just to infuse structure information as features. Therefore, the proposed method is more like "structure infused" rather than "structure controllable". Zhang X, Yang Y, Yuan S, et al.Syntax infused variational autoencoder for text generation[J]. arXiv preprint arXiv:1906.02181, 2019. 2.Casas N, Fonollosa J A R, Costa jussà M R. Syntax driven Iterative Expansion Language Models for Controllable Text Generation[J]. arXiv preprint arXiv:2004.02211, 2020. 3.Wu S, Zhou M, Zhang D. Improved Neural Machine Translation with Source Syntax[C]//IJCAI. 2017: 4179 4185.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 6. <BRK> Paper summary The authors propose Adversarial Feature Augmentation (ALFA), which augment features at hidden layers by adding adversarial perturbations. Review summary Although the motivation of this study is clear, the proposed method is not appropriately designed along with the motivation. Details Strength  The motivation is clear and seems reasonable. Training with adversarial perturbations is known to be effective but computationally expensive. The proposed method consistently improves the performance of baselines over several datasets and network architectures. Since the adversarial perturbation is computed for every layer, its computational complexity should be almost same with that of standard adversarial training. Since the norm of \delta is upper bounded by a certain constant \epsilon, the effect of the adversarial perturbation can be reduced just by increasing the scale of features. The optimization of \eta in L ALFA is not reasonable. An idea of adversarially augmented features has been already presented in [R1].<BRK>Adversarial examples improve image recognition. Major Concerns:The method proposes crafting adversarial perturbation at different layers of the network. The method which attempts to cause perturbations in the feature space also ensures that perturbed image is similar to original image(imperceptibility constraint) [3]. But in this paper adversarial training has been shown to increase standard accuracy even without using any adaptive parameters (like batch normalization layers used in AdvProp [2]). Typically adversarial training  increases its robustness [5], which is not observed with the proposed method. Could the authors clarify this? (This is important as the magnitude of improvements are not very large over the standard accuracy in case of ImageNet and iseven smaller when compared with Torchvision models). So it is unclear if the proposed method will continue to be cost efficient in comparison to pixel based methods. Overall, this seems a very complex method without any theoretical grounding to increase the generalization performance.<BRK>Instead of augmentation the pixels space, which is expensive and potentially harder, they augment the intermediate feature representation. In what experiments did ALFA L beat ALFA on the last block? I am positively inclined towards this paper and hope the authors can address by concerns in the rebuttal. The more important ALFA hyperparameters that would most benefit from automatic tuning are not sufficiently treated. Although I do like the objective of this paper and some of the approaches, I think it might need to be revised and resubmitted, incorporating the extensive discussion presented by all the reviewers.<BRK>The authors also propose a learnable version (LALFA), to automatically learn the location and strength of the perturbations. Overall, I think this is an interesting paper that can be considered for publication at ICLR. The following elaborates it further:Strengths:* Even though the adversarial feature augmentations at the feature level are not unprecedented, but the learnable tuning of the location/strength is an interesting approach that can potentially avoid expensive hyperparameter optimization. * The performance and the offered advantages seem quite sensitive to the choice of hyperparameters in ALFA (Tables 4 and 5). * An ablation study on the L1 regularization could have been useful. * Defining both F(x;\theta) and f(x;\theta) is referring to the neural network, and its output seems unnecessary and confusing; besides, F(x;\theta) is never referenced before. * f(x+\delta,\theta)  > f(x+\delta;\theta)* Adding a row in table 7, representing the results from ALFA, will make the direct comparison between ALFA and L ALFA easier.
Reject. rating score: 3. rating score: 5. rating score: 7. rating score: 7. <BRK>Most importantly, I think the scope of this paper does not match with that of the ICLR conference, as there is no representation learning aspect in this work. This test is developed for the online setting where we have a stream of data coming in and we want to terminate the trial (here, A/B testing) as soon as possible in order to limit the possible adverse effects that the trial might have. Pros:  The paper is well written and easy to read / understand. Summary:This paper provides a statistical test that checks whether a subgroup would benefit from a certain intervention, even if on average the entire population does not benefit from that intervention.<BRK>In this paper, the authors propose SUBTLE, which performs sequential A/B test with heterogeneous treatment effect. Compared to prior work, SUBTLE does not require specification of the parametric from of the treatment effect on some covariates. I believe the paper is original and could be useful for practitioners. The paper could be improved in clarity. Is the probability/expectation on X taken with respect to the distribution of an individual observation X_i, a subgroup of observations \mathcal{X}_0, or the distribution of all the observations \mathcal{X}? The authors did this on the real (private) dataset on page 8, but it would be good to do the same on a dataset where it is publicly available and the ground truth is known.<BRK>The authors formalize the problem into a clean hypothesis testing problem (9) that tests if the value gap between the optimal policy and the all control policy is 0, propose the algorithm SUBTLE, and prove that it is able to control type I error at any time. This paper focuses on an important real world problem that many ICLR readers care about, and is easy to follow in general. I like the idea of "testing the existence of a beneficial subgroup", which should be more useful in practice than "testing whether there s difference between treated and untreated" as SST does. The proposed algorithm (SUBTLE) adapts mSPRT to address the current problem, which is easy to understand and achieves provable type I error guarantees. The paper is clearly written and easy to follow. Is there any good explanation for this? Is it because the treatment effect structure in model V makes random forest a more favorable method?<BRK>This paper is to propose a sequential test for subgroup treatment effects based on value difference, named SUBTLE,to address these two problems including  a fixed horizon framework and  identifying a subgroup with a beneficial treatment effect。 Although there are several interesting results, this paper is full of many typos and small errors. 1.Please not use (1) (?) to itemize different things, since you also use them for equations. 2.The estimator in (10) still suffers from the instability of the estimated propensity score.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 4. <BRK>Could the authors clarify the experimental setup, especially the number of samples employed in the meta training and meta testing phases? *EVALUATION*The idea of focusing the exploration on identifying the current task, instead of collecting good samples for policy improvements, is quite interesting, albeit not completely novel. However, I believe that a partial overhaul of the paper structure, along with a sharper experimental analysis, would make for a nice contribution to the meta RL community.<BRK>The core of the algorithm is to use an exploration bonus for the exploration policy that rewards finding novel trajectories. Pro:   Very relevant subject. Suggestions:  You write that "efficient exploration of meta RL needs to consider both training and adaptation phases simultaneously". This should be explained in the paper. I think the authors are not doing themselves a favour by setting up the presentation of their methods like this.<BRK>Post rebuttal feedback I thank the author for their reply and I encourage them to make the suggested improvements to the paper. The empirical results are significantly better than the considered baselines. It looks to me that the major contribution discussed by the authors is the use of intrinsically motivated exploration in meta training and adaptation, while the use of task inference has been already proposed in other works and the use of different policy networks for exploitation and exploration is trivial.<BRK>Towards this goal, the paper proposes to use an intrinsic reward (two versions) and to learn a separate exploration policy to collect data for faster task inference. The proposed technique does not utilise dense rewards during meta training. "What s interesting?" However, I will discuss my concerns regarding the experiments in the subsequent sections. Another main concern is regarding the novelty of the proposed approach since the major off policy meta RL component appears similar to PEARL.
Accept (Poster). rating score: 9. rating score: 6. rating score: 6. rating score: 4. <BRK>For a two layer convolutional neural network with ReLU activation, with weight decay regularization, that is non convex, this paper establishes strong convex duality, where the dual optimization is quite tractable and interpretable. It also shows empirical results for MNIST denoising and MRI reconstruction that support the claims. Strong points:    This paper is very well written and well organized. In particular, the filters visualized in Fig.4 are insightful to what a neural network learns. The clustering interpretation is also neat. I am wondering about the complexity of the dual optimization problem. Also, about the required number of sign patterns, can the authors provide intuitions for the datasets used in the paper, say for MNIST?<BRK>The authors propose a convex formulation for training a 2 layer neural network for reconstruction which should make training easier. It would probably be nice to make it clear that this is mostly of theoretical interest. * Please make all images in figure 5 equally large. * Is there a good reason results are not reported on the full FastMRI dataset? Showing me a few hundred filters might give a tiny bit of insight, but I wouldn’t say it lets me interpret the reconstruction properly. * Has the number of sign patterns been ablated?<BRK>This paper describes the interpretation of image restoration by fully convolutional neural network (FCNN) through the dual problem of neural network learning. Since the activation function is non linear, it is generally difficult to interpret the main problem. As a result, when a finite number of sufficiently large filters is prepared, this becomes a convex dual problem of the primal problem. The conclusions about the interpretation of FCNN were that the l2 penalty of the weight parameters was group lasso in the dual parameter and sparse regularization of the path, and that denoising was patch based clustering and its linear filtering. The interpretation of neural image restoration through the dual problem is very interesting and I think it is well written as a paper. However, there are some recent studies that are considered to be very closely related to this paper, and the novelty of the conclusion itself of patch based image processing seems to be weak. "Convolutional neural networks analyzed via convolutional sparse coding." **short summary** In the above paper, the CNN filters are interpreted as a patch dictionary, and the sparse feature map is interpreted as a coefficient. Image reconstruction by CNN is interpreted as patch based sparse coding.<BRK>This paper proposes the dual formulation of a two layer neural network, which makes the loss for training convex. The convexity guarantees the global optimality of training step compared to training on the primal loss function. Then the paper gives explanation and experiments regarding the dual nn. However, I m not sure what is the novelty of the work. I think if there s any dual computation in Sec 4.3 for nn with any number of layers, it could be more interesting. However, to use Slater s condition, is the primal problem convex? I cannot find the proof. This is really a theoretical claim, otherwise please say "small duality gap observed empirically". Regarding convex NN, this paper is also interesting.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. <BRK>## OPAL: Offline Primitive Discovery for Accelerating Offline Reinforcement Learning### SummaryThe authors present OPAL, an offline reinforcement learning approach that distills useful common behaviors from offline transition data. Their experimental results show that OPAL compares favorably against other SOTA methods in offline, online and few shot reinforcement learning settings. Overall, the paper is clearly written, and the approach is potentially practical for real world applications, due to its good performance on both offline and few shot adaptation settings. The property of OPAL is mathematically analyzed. OPAL can potentially benefit from the advancement of offline RL algorithms by simply replacing CQL. Can be adapted to a new task through few shot imitation learning.<BRK>Summary To best leverage diverse datasets of logged experience, the authorspropose to extract a space of primitive behaviors in a continuous space,and to use these for downstream learning. The authors claim that this approach to offline RLavoids known distribution shift and allows for temporal abstraction. How were the baselines chosen. This is an    interesting combination, and deserves to be investigated. The authors have addressed many of my concerns with respect to motivation, theoretical analysis and empirical evidence. My decision would be to "accept the paper if there is room". As you show, it can be applied to    online RL. You use    standard offline RL algorithms, yet claim that the temporal    abstraction of skill discovery is crucial to the results.<BRK>In the RL setting, this paper tackles the case where an agent may have access to large amounts of offline experience data. The objective of the work is to find an effective way to leverage this data for finding temporally extended primitive behaviors. The paper tackles an important question in reinforcement learning: learning temporally extended primitive behaviors from off policy data is a very relevant question. However, I found the motivation for the approach quite vague as well as different elements that require clarification (see below) and because of this, I can t recommend acceptance.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 5. <BRK>**Update after rebuttal** : I want to thank the authors for their rebuttal. Specifically:A) In general, a lot of ablations are missing. The effect of the number of heads, the dimension parameter $d_k$ and the very important region parameter $L$ are not ablated. The term "global axial attention" is mentioned in the text, but the text does not say that global axial attention is the "default case". Similar to this paper that restricts softmax only on the keys , the softmax in GloRe  is also removed for similar reasons.<BRK>Summary:This paper proposes global self attention networks for image recognition. Experimental evaluations are conducted on ImageNet and Cifar100, and the results are promising. I think this paper is moderate. The author should not ignore the channel values as they are large in the top layers. Thus I am not convinced by the claim of the paper.<BRK># Post rebuttal updateI would like to thank the authors for the detailed feedback. While the paper delivers state of the art performance, the novelty of the paper is weak. The method outperforms all compared methods, albeit by a small margin. The paper is easy to follow, with detailed coverage of related works. The main novelty of the paper then seems to be that the softmax is not used at certain locations and that the two different types of attention are combined together. Furthermore, while the focus is on the combination of the two types of attention, which does seem to help in the ablation study, I wonder whether the removal of the softmax had a larger effect. The removal of the sigmoid seems to have had a large effect. Could the authors clarify this?<BRK>There have been multiple attempts to use self attention in computer vision backbones for image classification and object detection. An experiment with the same number of parameters with/without content part is missing. CIFAR 100 experimental setup is uncommon and very surprising, as the authors use ImageNet pretrained models and finetune on CIFAR 100 with large input images, in contrast to training from scratch on CIFAR 100. I suggest that these experiments are updated, otherwise it is extremely difficult to reproduce the paper with a limited computational budget. Disregarding this, the paper has valuable findings but needs more rigorous experimental evalution, as I suggested in my review.
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 7. <BRK>Summary:This submission proposes a joint training of self supervised training and supervised training for neural machine translation (NMT), especially for the rich resource language datasets. * Since the authors compared with other pre train methods, MASS, BART, therefore, the experimental results are expected to make a clear comparison. Overall comments:The method proposed in this submission is interesting, the crossover operation conducted on the monolingual dataset is similar to the previous works like MASS and BART, but different in the bilingual sentences mixed version. It seems to be not clear. It seems more explanations need to be shown. Therefore, it is hard to think of the reason or the motivation for such mixing, especially in a weighted version as equation (7) and (6). More motivation for this modeling is expected.<BRK>This paper presents a method, inspired from genetics algorithms, to trained jointly self supervised and supervised NMT. On En Fr, it is well known that the differences between these different BLEU may be within 5 BLEU points (!). sacreBLEU must be reported instead of, or in addition to, multi bleu scores. Moreover, the work compared in Table 2 used different training data and different pre processing, the difference in BLEU may then only come from these data differences and not from the frameworks. I actually expect this method to be worse than Zhu et al.(2020) considering that their sacreBLEU scores is less than 2 BLEU points below the multi bleu scores reported in this paper.<BRK>This paper proposes a joint training strategy that combines supervised learning on parallel data and self supervised learning on monolingual data for NMT. The monolingual sentences are corrupted with word order shuffling and masking. Main results (table 2) should compare with BT and noised BT (Edunov et al.2018) using the same amount of monolingual data as well. Would this potentially hampers self attention in the encoder or the enc dec attention? This also leads to the question, what is the best fusing scheme (i.e.input (x, y), (x , y )) for the proposed method?<BRK>summary: This paper introduces a new approach to semi supervised training of neural machine translation. additional questions to the authors and minor points:   some implementation details were unclear to me. There is no strong improvement in the face of "drop word" noise, and if this noise were of practical relevance, the much simpler strategy of applying word dropout at training time is likely to remedy it. related to the above question (where does improvement come from?), back translation. If back translation experiments are seen as separate from the self supervised learning experiments, it is misleading to imply that self supervised learning achieves a new state of the art.
Reject. rating score: 3. rating score: 4. rating score: 7. rating score: 7. <BRK>Also given that the variance is pretty high here, comparing performance over just 3 random seeds is not statistically conclusive. Pros: A simple architecture modification that might be beneficial to impose a stronger inductive bias on temporal dynamics. Clarity:The paper in general is clearly written and well organized.<BRK>Overall, I believe that the paper proposes a simple and interesting method, but I am not convinced that it would lead to better results consistently. This paper presents a new method for aggregating temporal information in reinforcement learning policies. I do not believe this is true. Recurrent architectures are commonly used in RL algorithms (for example RSSM in Dreamer) and are widely available in open source implementations (for example https://github.com/openai/baselines/blob/master/baselines/common/models.py).<BRK>Overall      This paper presents and effective idea, however, there are some additional experiments (using an RNN to combine latent vectors) that I think would strength the paper considerably## Post RebuttalI thank the authors for their response. I disagree with R1 that it is a stronger baseline as FLARE outperforms it in all tasks and stack SAC similar or better three (arguably four) of five tasks.<BRK>Why so?########################Reason for score:The approach is well motivated, and the paper is clearly written. Typically, RL algorithms employ a frame stacking heuristic to incorporate temporal information (early fusion). ########################Pros:  The presented approach provides an effective alternative to the frame stacking heuristic for incorporating temporal information in RL with pixel based state representations. It is unclear why Flare works well on some environments and not so on others.
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 8. <BRK>This paper studies the relationship of correlation of ranking of networks sampled from SuperNet and that of stand alone networks under various settings. They also study the how masking some operations in the search space and different ways of training effect the ranking correlation. Pros:The paper has a lot of experiments to substantiate the claims. Figure 3 where every operation is systematically masked, provides more insights about which operations are effective and how NAS behaves if one of the operation is masked. Cons:Several other papers have already published similar findings. Overall the paper is very incremental. How is the SuperNet trained? How is Figure 2 different from that? Figure 4 seems to be inspired by that. While it is interesting, this might be a slight extension to the work done by Yu et al [1]4. RobustDarts also suggest some recommendations such as L2 regularization, drop path etc although in the context of DARTS. So while Figure 6 demonstrates this empirically, it is not a new finding. Overall, the empirical results in the paper are very useful for the NAS community. But the work is still very incremental. This might be better received as a workshop paper instead.<BRK>This paper introduces an empirical study on the ranking correlation in the singe path setup. In general, I appreciate the authors  effort in bringing more infrastructure to the community of NAS. As a recently emerged community, we do need works like this one, as well as previous ones such as NAS Bench 101 and NAS Bench 201, to make the evaluation protocol more scientific. Colleagues who would like to invest their time and resources in exploring and manifesting this search space to uncover more phenomena are thus worth respect. However, this respectable responsibility also comes with a higher standard to evaluate works attempting to fulfill it. My major concern with this work is that the manuscript is not organized well. Although authors provide substantial details on their empirical study, they did not form a coherent logic flow to present these empirical findings, which makes this work more like a technical report than an academic paper. Readers may find these phenomena interesting but may not get interesting insights after reading this paper. Hence the technical contribution, especially on novelty, seems quite limited, even if there may be some intriguing points in the authors  discovery. I would recommend the authors to pick some phenomena e.g., masking Zero, masking Skip, etc., as examples to provide more analysis, so as to demonstrate to colleagues in our community that these findings can indeed lead to interesting research topics. For example, the authors did not give adequate credits to colleagues who pioneered in using Kendall Tau to evaluate NAS training. As far as I know, Sciuto et al., 2019 was one of the earliest works. In the third paragraph, when reviewing recent progress, the authors did not distinguish the ranking correlation between NAS searching and retraining from the correlation between NAS searching results and stand alone training. The former one was discussed and addressed in Hu et al., 2020. As the community has not fully realized the subtle but crucial difference between these two correlations, I believe a better framing of this work can be more helpful to other colleagues, especially those new comers.<BRK>In this paper, the authors proposed several findings on the single path training strategy. The experiments are conducted on NASBench201. However, the aforementioned weight sharing xxx , there are a number of efficient multi objective (Pareto front) NAS methods. Summarize the main findings in the introduction section. describing the ranking correlation of the average prediction accuracy depending on N , what is the average prediction accuracy for two lists? If removing other operations other than zero, will the $\tau$ be lower? In my opinion, removing any operation leads to smaller search space, and they all have a higher ranking. 4.2, 4.2 examine some of the training strategies proposed by previous works. Add the venues for all the papers. This paper tries to explore the single path training strategy by studying the search space, the supernet, the linear transformer, the strict uniform sampling, the topology sharing, the LR warmup, the regularization, the clipping. The authors have done lots of experiments to clarify the important reasons for the ranking. However, most of the findings are not new to me. They have been discussed more or less by previous works and discovered by my own experiments. The paper is mostly clear, some paragraphs and references need to be polished, more related works should be added.<BRK>+ This paper studies the single path one shot super network predictions and ranking correlation throughout an entire search space, as all stand alone model results are known in advance. This is a crucial step in NAS. It makes nearly all previous NAS methods not better the random architecture selection (suggested by two ICLR 2020 papers and many ICLR 2021 submissions). Could the authors also provide a comparison using these two metrics? ( )EagleEye: Fast Sub net Evaluation for Efficient Neural Network Pruning  I think NAS Bench 201 is not enough. As we know, CIFAR 10 is sometimes considered a toy benchmark, and the sole result on CIFAR 10 is not convincing. Could the authors provide more results in addition to CIFAR 10? ( )  Could the authors provide more details in Figure 3. Figure 3 shows that the lines on the top mean the operation is used more frequently. But I am not sure what the value of the y axis means. ( )+ The following observation is believed to be crucial in NAS: "The baseline for small networks (top left, red) has the same averaged prediction accuracy for the top 10 as for the top 500 networks". Interestingly, \tau may improve within the predictions for the top N architectures." Specifically, it is fascinating to see that "Although the additional transformers seem to stabilize training, as seen by the lower standard deviation, they also worsen the τa problem." A more reasonable metric may be desirable. (+ )+ The following observation is important: "medium sized super networks require additional care." As shown by Figure 4, the averaged predicted accuracy of top N networks in several subsets is lower than that of a random subset of networks. This is consistent with previous work like DNA, which shows a large search space may be harmful to the architecture rating. Even if a medium sized supernet has a bad architecture rating, the ranking correlation should be worse in a large sized supernet. However, this is not relevant, as only the correct ranking matters". (+)+ It is interesting and convincing that many tricks such as learning rate warm up, gradient clipping, and regularization do not work to improve the ranking correlation. We are pleased that the authors provide so many experiments to point out some misleading approaches in NAS. I think this paper is very important in the context of AutoML. It would be good to see some analysis of large sized search space. ( )Overall, this paper provides a timely analysis of the current NAS s ineffectiveness caused by the inaccurate architecture rating problem. As there are many NAS papers published every year and their ineffectiveness may still be not widely recognized by the reviewers and the public, I recommend a strong acceptance for this paper to promote the analysis of the NAS s architecture rating problem. I have read other reviewers  reviews and the response from the authors. But their effectivenesses are unclear due to the lack of ranking correlation analysis. Many findings are new (at least they are not in published papers). I agree with R4 that the authors did not form a coherent logic flow to present these empirical findings, and the paper was similar to a technique report. I believe these observations are important and deserve publication. Undoubtedly, I also believe the comments from other reviewers can benefit the improvement of your paper.
Accept (Poster). rating score: 7. rating score: 6. rating score: 4. <BRK>In the second step, the proposed method generates counterfactual conversation conditioned on the dialog history and goal generated in the previous step. The authors show that evaluating current state of the art DST model on MultiWOZ datasets with the generated counterfactuals results in significant performance drop. Additionally, human evaluation shows that the generated conversations perfectly reflect the underlying user goal. The paper is trying to tackle an important problem of evaluating robustness of a DST model when most of the available datasets has similar distribution in train and test splits. It is not clear what is the performance of utterance generation model. This model is somewhat different from other language model since it is conditioned on belief as well. Also, seems like the proposed approach can generate counterfactual conversation only for one turn which seems limited for the evaluation. I would like to hear from authors on:  In section 5.4, it is not clear if slot combination dictionary was also split such that slot combination in train and test data doesn’t overlap?<BRK>I just wanted to clarify that I had meant that the counterfactual generation method itself may have limitations (not the high level DST task, which I agree has broad uses). However, I hope the authors may be able to clarify some of my questions (I’ve listed a few in the limitations section of my review). Limitations:  The proposed methods are using a lot of pre existing components and limited in scope to this one domain. While the created counterfactuals are useful as a challenge dataset for DST systems, the overall approach in this paper may not be more broadly impactful outside of this task. Questions:  In section 5.4: Any thoughts on why data augmentation is more effective for the Trippy model than the other two models? I recommend making it bigger.<BRK><Summary>This paper addresses the problem of evaluating dialogue state trackers (DST)’s generalization ability to novel and realistic dialogue scenarios that do not exist in the dataset. It proposes a model independent approach to evaluate DST systems with the idea of counterfactual conversation generation. The proposed approach is integrated with three recent DST models and evaluated on MultiWoZ dataset. The idea of counterfactual goal/conversation generation can be useful for the evaluation of DST systems, which is often quite challenging in practice. Thus, this work may be better fit to a venue of NLP. <Conclusion>My initial decision is ‘reject’ mainly due to lack of technical novelty.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 4. <BRK>The paper studies inductive biases that are encoded in three main seq2seq architectures: LSTMs, CNNs, Transformers. Could you comment on why this is the case? I also appreciate the attention to hyperparameter tuning and investigation of their effects in experiments.<BRK>The Topic of this paper, to investigate the inductive biases of different neural network architectures, is super interesting. I would really appreciate if the authors provide a clear distinction between different sources of the inductive biases and their interactions in their experiments. I d also like to appreciate the authors  efforts to address reviewers concerns in the rebuttal.<BRK>This work proposes to use description length (DL) to measure the seq2seq model s inductive bias. I do believe that this framework can be utilized by future work in this direction to get a better understanding of seq2seq models. The authors concentrate on a training set of a few samples, which is far away from the usual large data setting for LMs.<BRK>The paper introduces a series of new datasets and task and investigates the inductive bias of seq2seq models. LSTMs do not have the capacity to perform multiplication. As mentioned in my review I believe this approach is interesting. Who will benefit from this analysis?
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 6. <BRK>Perhaps this originates from my not knowing what the authors mean by “filter transform” and “steerable CNN”. Is that meant to refer to the work of Weiler and Cesa (2019)? I found these difficult to parse. The mathematical level of the paper is pretty heavy for the uninitiated. It may be advisable for the authors to include a glossary of terms, if not short descriptions, in an appendix. This is for two main reasons.<BRK>The precise way in which a basis filter is to be rotated and flipped to obtain the steerable filter basis for the 2D case had not been worked out before, to my knowledge, and this is one of the contributions of this paper. Having worked this out, the paper proposes to use this as a way of implementing the filter expansion step, starting from a basis filter and rotating/flipping it to obtain an expanded filter bank. Finally, the method is validated by training networks on benchmark tasks and shown to perform similarly to or better than the steerable CNN implementation of Weiler & Cesa, which is the best existing implementation. The paper briefly mentions that filters cannot be rotated exactly on a discrete grid, but I didn t figure out how the authors propose to deal with this issue. The paper itself is fairly well written and technically correct as far as I can tell, but may be challenging to read for those who are not yet knowledgeable about steerable CNNs.<BRK>This paper studies the connection between steerable CNN and filter transformation. The authors show theoretically that filter transformation can be used to realize steerable filters over different group representations. The authors also empirically show that the filter transformation based steerable CNN performs on par with the implementation based on harmonic bases. Also, the authors do not clearly show why the contribution of this work is significant. Therefore, the results derived from a single convolution operation may not be sufficient.<BRK>This paper builds the connection between various steerable CNN structures based on group representation theory and filter transformations. The main theoretical results in the paper on the considered discrete group transformation are interesting, and can potentially lead to other development in the community. The numerical experiments seem to be limited (though this phenomenon seems to be the issue for most of the papers in this area...)** Pros1. Well written and technically correct. However, I do recommend the authors to present the "equivariant loss" when using their proposed FILTRA, considering that discretization and interpolation might cause a problem in their setting unlike other means of steerable CNNs such as RotDCF and Harmonic Net
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 5. <BRK>Summary:This paper tries to disentangle the role of planning in model based reinforcement learning with a number of different ablations and modifications to MuZero. * BFS is a too weak planner to compare with. The results provide some insights that other researchers in the MBRL community can leverage for their future work.<BRK>The paper investigates how and why planning might be beneficial in model based reinforcement learning settings. I will therefore (happily) raise my score. Having that said, the paper itself is well written and provides important insights to the community. With more runs for each of the experiments, I would recommend a clear accept.<BRK>summary:This paper analyzes the role of planning in the model based reinforcement learning agent, based on evaluating MuZero on eight tasks (i.e.Ms.Pacman, Hero, Minipacman, Sokoban, 9x9Go, Acrobot, Cheetah, and Humanoid), which have discrete action spaces. Also, a too small or too large search budget can be harmful to the performance of the MBRL agent. pros:  The paper is well written and well organized. Hypotheses and the experiments are well designed and seem thorough. cons:  The analyses are limited to MuZero that deals with discrete action space and to the deterministic environments.<BRK>I would like to raise my score to 5. My second concern is addressed in the updated version of the paper, with additional experiments. Thanks for the author s detailed response. When we test planning algorithms, should we give the agent a fixed model and a fixed representation or fixed algorithms learning the model and the representation?
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>This paper proposes that in human AI collaboration using deep neural nets, the AI agents we train should separate learning the _rules_ of the environment from the _conventions_ used to coordinate with humans in that environment. Why does the partner module operate “on top of” the task module? Other than that the paper is high quality. Questions for the authors:1.<BRK>Addressing this challenge is important. This work proposes to achieve this by combining two separate learned representations. One for partner specific behaviors. The other for the task itself. If so, how? Clarification: In what sense are Train and Test different tasks?<BRK>This paper makes the observation that when performing cooperative tasks with partners, there are two components to learn: how to perform the task, and how to coordinate with the partner according to conventions.<BRK>The problem, is very interesting and the authors propose a nice formulation to study these questions. The paper proposes an interesting model to study multi agent interactions, in uncertain environments. What information about the partners are known to the agent ? The setting in the paper concerns a learning situation where some or all of the components of the MDP is unknown.
Reject. rating score: 2. rating score: 6. rating score: 6. rating score: 6. <BRK>A 2D CNN is used to mix previous N layer s attention maps. I think the key selling point and hypothesis behind this paper (using prev N layer attention) is not well supported. The experiments on GLUE are only comparing against BERT (the least the authors could do is to compare side by side with at least a few other models). This is somewhat a pretty incremental extension of the Synthesizer Transformer model.<BRK>The idea of treating multi headed attention maps as multi channel images is interesting, and the proposed convolution based attention prediction module is a natural choice under such settings. Specifically, the authors propose to introduce a convolution based attention map prediction module, so the dependencies of attention maps across different layers can be captured. This shows the effectiveness of the proposed approach.<BRK>This paper proposes a novel approach to improve self attention through by bridging the attention maps from different layers via a chain of convolution based prediction modules. 3.Is the source codes available to reproduce the work? 3.This paper provides comprehensive experiments, including both NLP and image classification results, to show the effectiveness of the proposed framework.<BRK>Therefore, they propose a method that yields the attention maps based on the lower layer s attention maps. The experimental results on several different datasets from different domains show that the proposed method consistently improves the performance. The following are the questions and concerns of this paper. This is the largest mystery for me about this method. If I did not miss something, there are no clear explanations about it. 2, the source of the effectiveness:This additional module seems to also work as a sort of skip or short cut connections between layers.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. <BRK>The paper attempts to propose a neural network based algorithm on tabular data, or non distributed representation to address the unique challenges in tabular data, which are non existing in the distributed data (images, language, etc). What is the parameter complexity for DNNF? 4) In table 3, some of the experimental results are better than XGBOOST and some are worse.<BRK>The authors propose a end to end deep learning model called Net DNF to handle tabular data. the last layer is a linear transformation of the embedding with a sigmoid activation eq(4).<BRK>This paper proposes a neural architecture that emulates the characteristics of decision tree variants, in the hope of mirroring their successes on tabular data. I find these components and their coherent combination into the Net DNF structure to be novel. The reference below is a representative publication. Pattern Recognition (2020)The exposition of the paper is lucid throughout. Why don t the authors provide empirical results to show this?
Accept (Poster). rating score: 7. rating score: 7. rating score: 5. <BRK>The paper is very well written and clear. I think that the proposed way of characterizing function complexities is useful for providing new insights in learning theory. However, the authors may find it useful to address the following points: 1. Some comments would be useful. In my understanding, both x and x  will be samples in the data set in the empirical estimate of NF, as well as that of MNF. I guess that would be the most meaningful result from this paper.<BRK>Summary:This paper presents two main theoretical results as its main contributions. The authors comment that this notion is (1) relevant and well motivated (2) enables the connection between generalization and local interpretability. This bound relates the model generalization, the training accuracy, local explainability, and the complexity of the explanations. I d ask the authors to at least reconsider this strong assertion that investigation into this direction is not useful.<BRK>This paper tries to make a novel connection between local explainability and learning theory, and proposes two theorems regarding bounds related to performance generalization and explanation generalization, respectively. The paper presents two sets of empirical results to illustrate the the usefulness of our bounds.
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. rating score: 6. <BRK>This paper proposes a novel variant of LSTM by analyzing its behavior againstscale free distributions generally found in natural languages. In addition to Dyck experiments, some other languages, such as a generation from PCFG or someprogramming languages might be also interesting for experimentation. My only question is the setup of the proposed LSTM: in Section 3.1.1, theauthors say that the first layer of LSTM has a fixed timescale, and only thesecond layer has an inverse Gamma bias parameters. The third layer does nothave inverse Gamma distribution and simply optimized.<BRK>Gated recurrent neural networks such as LSTMs excel in modelling natural language, however, the forgetting mechanism of LSTMs  is ruled by the exponential decay. And the expected value of exponential decay functions exp( t/T) can approximate the power law when T is sampled from the Inverse Gamma distribution. Then, the authors propose a multiscale LSTM model that exploits this property. The multiscale LSTM captures the right inductive bias to perform better in modelling less frequent words which might be useful to keep in the memory for longer time. The paper is well written and both the motivation and explanation of the approach are clear. What is the motivation of using smaller hidden size for the topmost layer for both the baseline and the multiscale LSTM?<BRK>The paper investigates how well LSTM language models learn the known statistical temporal dependency distributions of languages, which follows a power law. The authors deduct the expected distribution of LSTM unit timescales and present a method for manually controlling them by setting the forget gate bias using an inverse Gamma distribution. They show that while the timescales of a standard LSTM LM follow the expected distribution, manually controlling it instead of learning it still increases LM performance, especially for infrequent words. The findings are also validated on a formal language, where the time scales are explicitly known. Thus the authors "improvement" is actually significantly below the original paper from almost three years ago,which is a long time in this field, current models are now below half of the reported perplexity. Training models on DYCK 2 can be highly seed dependant.<BRK>They define a notion of timescale of each LSTM unit and analitycally show that LSTM memory exhibits exponential decay, while natural language tends (based on prior work) to decay following the power law. Or refactor Table 2 such that it has same freq based columns. Is it possible to estimate/learn the IGD alpha parameter from the data itself of the task you work on? 8.How important is the model layers tuning you do? E.g.only specific layers have fixed forget bias, but others not, **why is that? Overall this is decent work which will be useful for future research in studying representational power of models we are using to learn complicated dependencies of natural language. Eq.3: why can we simply average forget gates?
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>This paper proposed a general deep learning method with temporal kernel for continuous time series modeling. The decomposition of the neural and temporal kernel brings together the kernel methods and deep learning, which delivers a general and fundamental solution to time series, especially the irregularly sampled ones or those with missing values. In brief, this work may demonstrate a promising way of handling such problems and inspires and encourages other research in this direction. Though I did not check all proofs and the supplementary, the descriptions and arguments in the paper are properly delivered. The proposed model consistently outperforms RNN, TCN, and attention baselines on a variety of datasets. The settings of the case 2/3 are reasonable. Besides, it is interesting to see that the speed is still comparable to the baselines. I did not quite buy the claim that the proposed method is not compared with recurrent neural ODE type models and point process models because of its more generalization and flexibility. Minor typos:In page 3: infitnitesimal  > infinitesimal; covaraince  > covarianceThe font size in figures may be increased for better readability.<BRK>The idea is to borrow strength from the newly introduced time dimension to improve prediction performance. One of the key idea is to treat each hidden layer in NN as a Gaussian process, which is represented  as “neural network kernel”. For some reason it is difficult to use the continuous time system to compute the temporal kernel directly in the time domain. The ms proposes to convert the functions to the frequency domain, which leads to a nice property such that we can compute a temporal kernel (Eq.(2)) in frequency/spectral domain. Regarding the computation of the kernels, NN kernel can be computed by extracting the features of the hidden layer. The temporal kernel can be computed by sampling the spectral distribution, which is called the random feature representation (Eq.(7)).However, it is not clear how to specify the spectral distribution. In all examples, Normal distribution is used, which is OK but may not be able to capture the complexity of ODE. The ms applies the proposed method to real dataset and achieve better performance than baseline methods in prediction tasks involving irregular time points setup. In general I feel this is a nice paper. However, due to my limited knowledge in signal processing, I am not able to dig into the mathematical details and make strong recommendations (especially Claim 2). What is the purpose of Claim 1? There are lots of approximations in other parts of the model. 2.Why f(t) needs to be ODE ?<BRK>This article proposes a methodology to *adapt* NNs to continuous time data though the use of a (temporal) reproducing kernel. I enjoyed reading the paper, the message is clear, illustrative and the connection with other existing works is to the point. Although I am unfortunately unable to assess the theoretical novelty of the paper (I am unaware of the details of the state of the art in the subject) the contribution of the paper relates to the study of a kernel, given by an ODE, attached to the input of the NN. This kernel is also represented using Fourier feature expansions. Though the paper heavily relies on well known concepts  (standard NN, GPs, Fourier features), I see that is has a contribution. I suggest the following amendments: for some readers, the general proposed architecture might be confusing. How does the kernel turn the continuous time data into NN ready? much useful material is relegated to the appendix, if key results, scope and more are only in the appendix, they might not receive the deserved attention. please better clarify how different your work is from the existing literature: NTK, deep kernel learning, neural ODEs, etc<BRK>##### Post rebuttal updateI ve read the rebuttal and updated my score. This paper proposes a deep learning model for incorporating temporal information by composing the NN GP kernel and a temporal stationary kernel through a product. The temporal stationary kernel is represented using its spectral density, which is parameterized by an invertible neural network model. ##### Originality & SignificanceThe modeling approach taken in this paper is original to my best knowledge. ##### ClarityThe clarity is low. Do you use a GP predictive mean conditioned on the training points? The temporal kernel is defined through a random feature representation, do you take advantage of it for fast computation? * or you just take a weight space approach and compose the features (take pairwise product of the features of k_T and \Sigma to form the new features)? * Is NN GP or NTK kernels used to compute the kernels? I will be happy to raise the score if these questions are properly addressed. ##### Strengths* The modeling approach is novel. * The proposed method consistently outperforms other baselines in handling irregular continuous time data. * Although the performance is shown to outperform other NN based approaches in experiments, there might be scalability issues to apply the approach to larger scale problems with long sequences (assume a non weight space approach).
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. rating score: 3. <BRK>This paper shows that transformer models can be used to accurately learn advanced mathematical computations from millions of examples. The problems are drawn from the fields of differential equations and control theory.<BRK>Doing was technically complex. What seems to characterize the equations that the model makes mistakes on? Significance This paper demonstrates that neural networks are surpisingly good at the task of predicting certain properties of differential systems, such as their stability. Motivation/Introduction I agree with you that it is very impressive that neural networks can nearly solve these tasks. Are there engineering applications, for example,  where the classifier could be used to quickly screen proposed systems? Predicting Control feedback matrices I found this section very cool and encouraging for future work!<BRK>This paper investigates the use of deep learning models, and specifically transformers, to learn mathematical properties of differential systems. 2.How this work compares to previous ones such as (Lample and Charton 2020) that study learning symbolic mathematics using deep learning? I believe it can be improved by making the discussion more rigorous:1.<BRK>This paper empirically demonstrated the effectiveness of neural networks for learning to predict different mathematical properties of dynamical systems, which achieve high accuracy on synthetic datasets generated by the authors. Pros:+ the paper is clearly written and easy to follow+ the dataset seems to be carefully generated and is large. As a well known fact that neural network has universal approximation ability, it is not surprising that it can learn to predict the mathematical properties given enough data.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 7. <BRK>This paper proposed a streaming EM algorithm to learn latent representations and cluster in the latent space simultaneously, through a deep GMM model. The EM algorithm was unfolded into network layers as the forward pass, and the global parameters (GMM, encoder, decoder) was optimized through the backward propagation. Experimental results were provided to evaluate the proposed algorithm. I think some explanation is needed here to motivate the (streaming) EM algorithm. It would be nice to state that explicitly in Section 3, to reflect the streaming nature of the algorithm. 5) In Figure 3 we see that more EM layers actually hurt the performance. It is not very clear to me why using more EM layers (T> 7) hurts the performance, and why T 3 seems to be the best. 7) There are quite a few deep clustering algorithms proposed in recent years. It would be nice if the authors can compare with them as well.<BRK>The most common approach for deep clustering is to train a deep auto encoder (DAE) that is used to find latent features which are utilised for clustering with a classic clustering approach such as the K means. The K means is not differentiable and therefore the clustering was not part of the network. This caused alternative update of the parameters. The authors present an iterative EM algorithm which is part of the training procedure. High clustering results are presented on 4 different datasets. Interesting to see the performance of the proposed data also on this measurement. All datasets in the experiment are images. There is a repetition (almost identically paragraphs) between the introduction and the related work section. : Recursive parameter estimation using incomplete data.<BRK>This paper consists of two components. One is to unroll the EM algorithm for fitting a mixture of Gaussian model into a differentiable multi layer network. The other is to insert this unrolled network between the encoder and decoder in the latent space. I find the proposed method sound, useful, and efficient. My main concern is that this paper does not break new ground conceptually. Both the idea of unrolling an iterative algorithm and the idea of using auto encoder for deep clustering are well known. A top down model may also enable us to determine the number of clusters in a principled way. One may interpret the residual network and transformer network etc as unrolled iterative updating algorithms.<BRK>This paper proposes a deep clustering method called EDGaM. Clustering algorithms often suffer from cluster collapse or sample specific details overestimation. To balance both challenges, the authors propose a differentiable GMM network in the latent space between encoder and decoder. The network is designed with the scheme of skip connection and several loss terms are applied. Pros:+ The idea of streamlining EM into a differentiable network to learn its parameters by backpropagation is interesting. There seems to lack an ablation study to individually illustrate the function of each loss term in Eq.(9).The authors mentioned three types of deep clustering approaches in related work. I wonder if the results can be better or comparable with the other two types? After reading the rebuttal, I think the authors have well addressed my concern, thus this paper is good to be accepted for ICLR and I raise my rating from 6 to 7.
Reject. rating score: 5. rating score: 5. rating score: 6. <BRK>2.This approach shows improvements in the semi supervised setting and supervised setting compared to other self supervised training baselines and also against hand crafted feature generation techniques. Cons: 1.The paper applies the ideas of data augmentation, random sampling and self supervised learning in the event sequence prediction domain. It will be good to add this. 5.Why is the accuracy negative in Table2, is it the relative improvement with respect to the supervised setting? Self supervised doing better than supervised is not so intuitive and could you discuss why the supervised performance in general is low for these tasks ? 7.The paper has a lot of information as a laundry list that makes it harder to find the motivation and intuition behind different design choices.<BRK>The authors argue that self supervised learning approaches have been focused on NLP or Computer vision, rather than sequential user behaviors datasets (mostly tabular). In order to address this problem, this paper proposed a random slices subsequence generation strategy (CoLES) for contrastive learning in event sequences. The proposed CoLES algorithm is a data augmentation strategy that randomly selects subsequences from the full event sequence from each user. It was not clearly stated how CoLES was used to solve the classification (1, 3, 4) and regression (2) problems. However, considering the contents of chapter 3.4 3.5, I was able to infer that K subsequences are created with CoLES as well as several negative samples. For example, training a model by using additional augmented data generated by CTGAN [1], or tabular learning framework by masking some entities [2] can be strong baselines for wider experiment scope.<BRK>The method trains in a self supervised manner by randomly slicing event sequences to generate sub sequences. Strong points:   The paper is well written and the related work provides a good context of recent advances. The main reason is that the paper is well written and the evaluation is solid. The main reason for rejection would be not enough technical novelty and limited improvements. Comments & questions:  The paper assumes “periodicity and repeatability” in the data and bases the theoretical analysis on this assumption. Would the method perform better with a better encoder design? This is important for comparing results with supervised approaches. The paper uses a lot of phrases like “significantly outperforms”, “significant increase in performance”.
Reject. rating score: 4. rating score: 4. rating score: 5. <BRK>Authors firstly explore the current hot generative models:  VAEs and GANs, and experimentally find that both VAEs and GANs fail to learn a model which generalizes  well to novel domain. Specially,  the paper firstly augments the training sample to get the  input pair, and extracts the latent feature by the sharing Encoder. A weighted sum of both features is conducted to form the mixed feature, which further is taken as input for the decoder to synthesize the output sample. Differently   the paper performs the reconstruction loss between the output and the mixed input which sum the input pair with the same from to the one of the latent space. +This paper is well written, and easy to follow. #######################################Cons: The idea is little simple, and lack of enough novel. To my best knowledge, the data augmentation is able to increase the generalizability of model, and reduce the domain gap between the train and the test. In this paper, authors indicate AEs has good generalizability, which is interesting, but the proposed method is simple. Figure 1 is not convincing. In this paper, Omniglot train and test are used to evaluate the generalizability. Please collect me if I am wrong.<BRK>PROS:1.The work is well motivated. 2.The authors made interesting discovery on the generalization of AE. The assumption/settings about the method is not clear/correct, some statements are too strong, the implementation is not clear, the settings for the experiments are omitted. Detailed concerns are listed as follows. 4.I am curious about the range of the value for latent code learned by AE, also what s the difference of the latent space between AE and AugIntAE? In section 5, the authors used a shallow network for illustration, which makes it difficult for the readers to evaluate the reliability of the experiments, it is easier if an existing model is employed, i.e.WGAN GP.6.The training process for the method is very cryptic, since two datasets are mentioned in this work (one in large scale and another with few shot), which one is the case used in this work, trained jointly on these two datasets, pretrained on the large scale one and transfer to the few shot case, or only trained on the large scale dataset? e.g.what is the label for the augmentation obtained from 3 and 4?<BRK>The general idea of the paper is interesting: when using an AE one can use the constraint that "interpolated images" should also correspond to "interpolated latent codes". While the idea is interesting, the experimental results are not really that compelling. The idea is quite simple and the proposed setting to use synthesized and augmented images for the above mentioned interpolation constraint seems interesting to explore. While qualitatively (and potentially hand picked) examples seem to show that the proposed approach is working well, the experiments are not sufficient to convince me as a reviewer about the power of the approach. To me some experiments around this essential component the paper is incomplete and should not be accepted. Finally, somewhat linked to the previous comment, the paper does not really show failure modes (with the somewhat too obvious failure mode given in fig 3 right) to understand the limitations of the proposed methodSo overall the proposed method is interesting and simple (which is positive)   but the experimental results are not convincing and complete enough to justify acceptance at ICLR. Update after the rebuttal:Thanks for the responses. The paper seems not to be ready for publication at ICLR
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 8. <BRK>The paper suggests a new approach for unconditional generation of complex scenes. The approach performs generation in two steps: first a semantic map is generated from noise using a conventional generator architecture, then the semantic image is turner into an RGB image by SPADE translator. The paper has several strengths. Furthermore, the paper is well written and has detailed related work review (though some important papers are missed). The idea of two stage generation of complex images with GANs has been proposed before in a well known paper [Wang & Gupta, Generative Image Modeling Using Style and Structure Adversarial Networks, ECCV16] . 2) Deficient comparisons with 1 step GANs. The authors for some reason chose Progressive GANs and BigGAN as the reference 1 stage GANs. Given the use of SPADE (i.e.style based generator) in the authors  architecture, the comparison to StyleGAN would be all the more natural. I am therefore not convinced that the proposed idea is actually working. 3) [Minor] There is a published CVPR workshop paper with a very similar idea https://openaccess.thecvf.com/content_CVPRW_2020/html/w23/Volokitin_Decomposing_Image_Generation_Into_Layout_Prediction_and_Conditional_Synthesis_CVPRW_2020_paper.html . However, it does undermine the novelty. 4) The results are interesting, but they are not terrific.<BRK>1.Summary.This paper considers a two stage procedure of generation complex scenes (cityscapes or living rooms, i.e.without a central object): Firstly, a noise vector is mapped to a discrete semantic map (Gumbel softmax and straight through estimator are applied). Secondly, the obtained segmentation map is translated to an RGB image using the SPADE architecture. Both networks are pretrained separately and then finetuned in an end to end manner. 1.Decision.What I really like about this work is the attempt to dissect the scene generation into several steps. What is more, this work tackles the problem of the generation of discrete semantic labels and has success. However, the chosen intermediate representation is, to my mind, the main concern of the presented approach as ground true semantic labels are not so easy to crowdsource. One more thing, I would recommend choosing a more recent and appealing baseline like StyleGAN instead of ProGAN. 1.Questions    1. Is the sampling step with the Gumbel softmax trick crucial for the generation of semantic maps? Why the combination of simple softmax scores and straight through estimator does not suffice?<BRK>In this paper, the authors propose a new paradigm for unconditional image synthesis with semantic layouts as the bottleneck. The presented approach is straightforward: we can first sample a semantic layout from a latent variable, and then perform image synthesis from this semantic layout. The proposed method is able to synthesize images that look more realistic than unsupervised image synthesis methods such as BigGAN. The approach works well on the datasets with semantic layouts, including Cityscapes and ADE20K datasets. The experiments are comprehensive and convincing. Weaknesses:   Apparently, there is a limitation of the proposed approach: it can only work on datasets with semantic layouts. Why the perceptual evaluation of BigGAN in Table 3 for Cityscapes 5K is not available?<BRK>This work proposes a novel approach for unconditioned image synthesis of complex scenes by intelligently coupling two major tasks; unconditional label generation and label conditioned image synthesis. The former is based on the ProGAN with some modifications in losses to deal with discrete semantic labels, and the latter leverages the existing method (Park et al.2019) based on SPADE residual blocks. Experimental results demonstrate superior performance on the complex scene synthesis. Additionally, the latter part for segmentation to image synthesis task also outperforms the existing method (Park et al.2019) thanks to joint end to end training with the former using ProGAN. * Pros1) Decomposing the complex scene synthesis into two sub tasks (segmentation map generation and segmentation to image synthesis) looks novel, also validating outstanding performance over SOTA. 2) Segmentation to image synthesis is also boosted, thanks to joint end to end training with the unconditional segmentation map synthesis network. * Cons1) Though this paper is well written, some parts need more details. In Section 3.1, it would be nice to explain how to generate semantic segmentation maps progressively by referring to Figure 1. Why did you not use L_D_SPD and L_G_SPD in (2) for training the whole network with a pair of real RGB images and real segmentation maps.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK>This paper examines the setting of partially observable stochastic games where agents have the possibility of communication. The approach taken is what would be termed “direct revelation” in mechanism design: agents are supposed to reveal their full internal state / history to each other at each step. 8) In the experiments, there is improvement in both cooperative and non cooperative settings from SP IC3Net to TSP IC3Net. I think this needs to be investigated and discussed further. I like the motivation behind this paper, and agree that the idea of reasoning about the incentives to communicate and using techniques like peer prediction to encourage this is an interesting and important approach. Perhaps the TSP version uses the channel in a fundamentally different way? Perhaps there is a natural way to do an ablation for this? J is related to information asymmetry. In general it should be F(p||s) as the score depends on the whole distribution, not just p_s.<BRK>is there a reference for this or can you provide an explanation? The truthfulness is promoted through prediction rewards, which capture how well each agent can model other agents  policies via observing their messages. This paper provides theoretical justification of the proposed mechanism and introduces ideas from mechanism design literature in multi agent reinforcement learning under a non cooperative setup (not sharing rewards). The part that I find significant contribution is more on the theoretical justification than the mechanism itself (see related work section comments). **5   Numerical Experiment**Can you present the results also in terms of Avg. I think this needs further discussion on the main text. Also, as I mention later, this work is similar to centralized learning/decentralized execution work and as such, it should be presented in the abstract as well. **2   Related Work**Missing literature on centralized learning / decentralized execution (CLDE) which is related to the current work in the sense that self play with weight sharing is the same as CLDE [3]World models seem irrelevant as they are discussed with respect to their exploratory power or as applications of RNNs   why exploration is part of the discussion here?<BRK>They proposed a truthful self play method and add imaginary rewards into self play method by using the idea of reverse game theory. "Fictitious self play in extensive form games." I have read this paper for several times, however because of the poor writing, it s very hard to follow the key idea of this paper. Typically, these methods have good convergence on partially observed non cooperative games. If the authors can compare the Johannes s method on the experiment, it will be better. (2) The written skill about this paper needs to improve. For example, in section 2, the author talks about the related work on self play on the second paragraph. After that, the author talk about evolutionary learning and supervised learning. (3) The experiment results are not very clear. I only find the definition of pre state in section 3.1. (3) page 2: "interacts each other"  > "interacts with each other"(4) remove ", and" in Defintion 3.1 after "an observation probability"(5) page 3: $r_{ti}$ is reward, $\pi_{i}$ is a stochastic policy.<BRK>It outperforms self play on various communication domains and uses a number of interesting concepts. Edit  Other more confident reviewers have pointed out some concerns. Does not add a bias to the PG objective. Utilizing an imaginary reward is a very interesting approach   maybe there are domains that this is useful? Clearly good results on the domains tested. ## ConcernsHow is beta chosen in the experiments? ## Other ThingsPage 1: “P*e*rtially observable…”Page 2: I prefer “identity matrix” over “unit matrix”Page 3: “...but interacts each other”   ? clarify if theta and phi are scalars or vectors. Wording implies they are scalars. Page 4: CRA could do with a small diagram showing the interactions
Reject. rating score: 5. rating score: 6. rating score: 6. <BRK>While the use of an hierarchical flow based model to obtain samples of finer scales seems interesting, I have to admit that I did not understand much of the setup the authors propose. The methodology section is too dense and many of their claims are not really motivated at all. I do not see this paper accepted the way it is, but it might be true that a person more expert in flow based models and inference than me thinks completely the opposite.<BRK>The presented model is defined on multiple scales with information on finer scales being conditioned on coarser scales. The authors suggest to train the model from a coarser to finer scale in a multi stage process. This in turn results in good performance on a number of data sets. It is well known that this kind of inversion, although analytically correct, is computationally infeasible in high dimensional data. The methodology is sound and the experiments are convincing. In its current state the paper does not explain the method well enough in order to get a full understanding of the approach. This kind of inductive bias is simple to interpret and it appears to be a promising aid in model training. # Recommendations  Figure 1 could be improved to visualise the overall approach better. It would be worth visualising the sampling process for estimating Jeffreys divergence. Notation should be improved and made more consistent. Their results suggest that the multi scale model trained in multiple stages using Jeffreys divergence indeed performs best.<BRK>The authors propose a multi scale method to design invertible network and apply it to bayesian inverse problems. With that said, there are some steps that are lacking. I’d also be fine with a “if it exists” style statement. * All figures should be in vector format* The statement “ q(x(s1, s2))   q(x(s1, 1 − s2))” does not afaik imply more than one mode, consider e.g.a mode centered at s2 0.5. * Showing the “worst of 3” seems rather dishonest.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 5. <BRK>The paper studies (1) the relationship between the flatness of minima and their generalization properties, and (2) the connection between two measures of flatness, known as local entropy and local energy. Through a series of experiments, the authors show that the two measures are highly correlated and correlate well with generalization. They also empirically show that Entropy SGD and Replicated SGD, when used to explicitly optimize the local entropy, are able to flatter and better minima (in terms of lower generalization errors). I find the results interesting and the findings influential. Please explain this “All these algorithms require a differentiable objective” and the use of a new loss function.<BRK>This paper studies local entropy measures for characterizing flat regions in the energy landscape of deep networks. The paper discusses, at length, two previously proposed algorithms named Entropy SGD and Replicated SGD and demonstrates, using (i) controlled experiments where Belief Propagation (BP) can be used to estimate the local entropy integral precisely, and (ii) empirical results on deep networks that flatter minima generalize better. The paper revisits the line of work that initiated this debate and shows that flatness, as measured by local entropy instead, indeed correlates with good generalization. Entropy SGD/Parle were also shown to work well for state of the art deep networks then. The authors should state clearly what the concrete contributions of this manuscript are.<BRK>I have increased my score accordingly. Original Review:This paper presents an empirical evaluation of whether flatness correlates with generalization using a few different definitions of flatness   local energy and local entropy. The authors study two training procedures   entropy sgd and replicated sgd, and show that deep networks trained using these procedures are able to locate flatter solutions that also generalize better. Moreover, in the experiments it is not clear whether the solutions that are reached by the training procedures actually correspond to local or global minima of the cross entropy loss function (There is also the issue that minimizers of the cross entropy loss function occur at infinity). Exploring generalization in deep learning.<BRK>Authors essentially study the generalization properties of networks trained via two different algorithms, Entropy SGD (eSGD) and Replicated SGD (rSGD)  against networks trained via SGD. Both eSGD and rSGD are algorithms that have been previously designed using the notion of entropy guided modified loss function. With this the authors aim to find a correlation between networks with close to zero entropy and networks with lower local energy. The first experiment uses a simple two layer neural network with fixed final layer weights and studies both local entropy and local energy as a function of weight perturbation. Third set of experiments are the main contribution of this paper   testing the correlation of local energy and local entropy   this is done on ResNet 18 trained on CIFAR10 dataset.
Reject. rating score: 3. rating score: 5. rating score: 5. <BRK>But in the experiments, this is not demonstrated. ##########################################################################Summary:The paper studies policy optimization in multidimensional action spaces. The authors develop methods based on these two factorization techniques for discrete versions of PPO and SAC (called FPPO and FSAC) and evaluate them on Gym Platform, Google Football, and discretized MuJoCo tasks. Is it the improvements of the discrete versions over the continuous versions of the methods that are interesting (in which case, the results for the continuous versions on the MuJoCo tasks should be reported)? ##########################################################################Pros:  Factorizing action spaces is an interesting approach for scaling to high dimensional action spaces. So studying them more extensively is useful. ##########################################################################Cons:1) The paper generally does not position itself appropriately with respect to the literature. Discussion of such a baseline would be very useful in this paper which somewhat serves to summarize deep RL in factored action spaces. This work claims to develop techniques for dealing with hybrid action spaces.<BRK>### SummaryThe paper presents a study of factored action spaces for RL problems, and two basic forms of this,i.e., independent factorization (IF) and autoregressive factorization (AF). Update:  After reading the other reviews and the responses, I have changed my score to 5: marginally below acceptance, due to the framing and related work issues, as discussed by R2 and R4. Overall, the experiments show some of the possible potential of factored action spaces with PPO and SAC. ### Strengths  I believe that factored action space are an under explored area of research, and   understanding the benefits and limitations of these factorizatios is an important area,  as well as understanding how popular algorithms such as PPO and SCA can be adapted to use these factorizations. The experiments show some of the potential of factored action spaces, both IF and AF. When is it harmful to use a factored approach? It is difficult to fully understand the "take home message"  of each of the experiments. It would be good to understanding more about the relationship with possible continuous action derivations### RecommendationI am on the fence regarding this paper. It is stated that there is no inter correlation between action components for the MuJoCo benefits.<BRK>They address this issue by studying existing approaches of splitting the action into a finite number of sub actions and then sampling each sub action independently or auto regressively. In particular, I would expect authors to have at least  PPO and SAC baseline in their work. The number of discrete sub actions(“m”) for the policy is treated as a hyper parameter. How does one decide on the order of the autoregressive actions? This paper also talks about auto regressive actions.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>The proposed method works very well for irregularly sampled MNIST, and fairly well for the PhysioNet2012 data set for ICU mortality. # CommentsThe paper references related work, makes a meaningful contribution, and I think the empirical methodology is sound. As negatives, there are a few small issues with the paper, and it is hard to follow. It s good that you comment on this in Appendix H, and it would be better if you pointed the reader to it. ## Central limit theoremThe CLT argument referenced in section 4.6, only works if all the elements of M are equal. It is correct, but it is not the same object in section 4.6. You could also argue that the elements of M are close to independent with mean zero. This is understandable, because it uses mathematical tools unusual for an ML paper, but you can still make it easier for the reader.<BRK>This paper is way out of my comfort zone so I can only provide very general high level comments. First, the motivation for the paper seems very clear. For deep learning to work, we often have to discretize, and have to do so at lower resolution, which induces errors we typically ignore. Second, the architecture is well described and illustrated. I didn t check the math in all details, but the three experiments with superpixel resolution, generalization to new resultions and and irregular time series appear convincing.<BRK>It proposes to incorporate the numerical uncertainty related to the disretisation of the domain using an approach inspired by probabilistic numerics. It shares some ideas with the argument of Neal [1996] of an infinite limit of NNs being equivalent to GPs but it would be good to either give a more thorough discussion of this argument or provide links to related results in the literature. ##################################################################REASON FOR SCORE:I like the motivation for this work and the paper was generally a pleasure to read. Naturally, many technical challenges arise when trying to approach this problem from a probabilistic perspective; they are discussed in a fair bit of detail and generally seem reasonable. This is presented clearly and in a fair bit of detail. The appendix was very informative as well. The paper doesn t seem to make full use of the setup (see questions below). It appears from the left figure that the mean is very smooth in all layers. Also, given that your approach seems to outperform existing (deterministic?) The CLT argument in Appendix I is not very clear to me.<BRK>This work is most useful in the setting of irregularly sampled data. The problem addressed is very important in many domains. 2.The proposed approach is novel for learning from irregularly sampled data. 3.Empirical results show that proposed model performs well. Given the time complexity of Gaussian processes, could the authors comment on the run time of the proposed method as compared to standard CNN? It would be interesting to also compare the run time of both these methods. This makes these two approach more similar in what they are trying to achieve and I would expect the authors to compare the performance of these methods. Could the authors clarify this? I would be happy to increase the score once my concerns are addressed.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 4. rating score: 4. <BRK>However, the main theorem and its proof is not clear. I think it was good that this paper compared the proposed method with not only GNNs but also MLP and DeepWalk to show that the task needs both node features and graph topologies. [12 2] The new dataset, DBMovie, is interesting.<BRK>In this paper, the authors propose to sample nodes of a given graph multiple times to form a set of K sub graphs. In the experiments, it is interesting to compare with GraphSage by sampling nodes  neighborhoods multiple times. Especially, its performance in Table 2 is close to the proposed method.<BRK>I think the diverse sampling (DS) method is interesting and well motivated so that the sampled subgraphs are diverse, each node appears at least once and high degree nodes have a higher probability to appear in each sample.<BRK>However, I did not find theoretical support or scenario support for the proposed diverse sampling method. The problem that the authors focus on is interesting.<BRK>The experiment results on 4 different datasets justified the effectiveness of the proposed method. + Enhancing the diversity of neighborhood sampling is an interesting idea to investigate in GNNsWeaknesses:  It is not clear whether this idea can generalize to other GNNs, e.g., ResGCN, IncepGCN, etc.
Reject. rating score: 3. rating score: 6. rating score: 6. rating score: 6. <BRK>This kernel is parameterized by neural networks and is optimized through an objective maximizing the proposal entropy. The proposed method is tested on synthetic datasets, Bayesian logistic regression and a deep energy based model. As far as I understand, the proposed method appears to be technically sound. However, I have the following concerns about the paper. The connection and the difference to previous work are not very clear. If I understand it correctly, the proposed method seems a combination of L2HMC and [Titsias & Dellaportas, 2019] with some slight modification (a flow model) since the naïve combination did not work well (as stated in Section 4). The proposed method seems to use more neural networks (e.g.an additional network R to handle gradient) than previous neural network MCMC. It is not clear to me how to interpret the empirical results in Section 5.2.<BRK>The paper argues that a better objective to train neural MCMC kernels is to maximize the proposal entropy (Titsias & Dellaportas, 2019) and demonstrate a method on doing so. The novelty is not of the training objective but a neural instantiation with improved sampling efficiency. 2.The neural instantiation consists a few clever tricks to make the network tractable and explore the space well. It s very long and maybe adding some subsection would help. One need to additionally ensure that the kernel is irreducible and aperiodic. I think there should be some results on comparing the proposed method against well established HMC on a few Bayesian inference problem in terms of the posterior. ": I think vanilla HMC or MALA can do so by some online otpimization for step size as well (e.g.dual averaging as it s done in NUTS).<BRK>Summary:The author proposes a novel MCMC sampler parametrized by the neural networks. In particular, the neural network is chosen to be a flow based model that allows the exact evaluation of the proposal probability. What about the convergence speed of the sampler compared to others that use sample quality as the training objective? I also want to know the convergence speed of the EBM training and the quality of the generated images compared to MALA. These are the standard evaluation metric for EBM. This ensures the similarity between the proposed method and HMC. Empirically, the author evaluates the proposed sampler in some toy datasets, logistic regression, and deep energy based models. Review:Clarity: The paper is clearly written and easy to follow. However, the novelty lies in the usage of the flow model to allows tractability of proposal density. Although the author demonstrates the better ESS can be obtained using the proposed method, I still prefer more analysis to understand the properties of this sampler. What are the differences in terms of performances?<BRK>The authors describe an approach for adaptive MCMC which uses a proposal distribution parameterized by a neural network and which optimizes the entropy of the resulting proposal. The relation between HMC and their approach which makes use of intermediate steps x+R could also have been more thoroughly explained and given more intuition. I assume they perform their adaptation steps during the sample process as is the case of the Titsias and Dellaportas work, however this could use some clarification. Overall, I think that this and other confusions cited above could have been done away with by including a clearer outline/overview of the algorithm as a whole. Finally, overall the results seem to be quite a bit better than competing methods (although I m not an expert in this area). It s possible I missed this, but do not think it was discussed.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 6. <BRK>The advantage of the proposed method is verified in the experiments. The wider the applicability of the proposed method is, more valuable the proposed method would be. But there is a misunderstanding. MANGA as well as PEARL are online methods. It can encode the observation data in online manner. I think it is not evident whether IMPORT performs better than MANGA or PERAL. I would like to encourage the authors to perform more convincing experiment and make the claim of the paper consistent with the experimental findings.<BRK>Alternatively, it would be good to use prior approaches for TS, such as PEARL [1]. Generally, this paper presents a clear and coherent narrative. The motivation for the approach is clear (better leveraging task descriptors compared to prior approaches to more easily learn informed policies). [3] is also relevant to the exploration problem. Furthermore, the approach appears to be technically sound. In particular, I believe that the benchmarks can still be more carefully chosen to better evaluate IMPORT s ability to perform sophisticated exploration. It appears that all of the experiments use the same value of $\beta$? Finally, it would be nice if the experiments substantiate the claim that IMPORT outperforms TI by avoiding “reconstructing features in the task descriptor that are irrelevant for learning.” However, it’s not clear to me that this is occurring in the experiments.<BRK>This paper presents a method that leverage task descriptors for multi task learning. In this way, the RNN policy is trained as if the task description is available. The proposed method seems novel and the experimental results show its benefits. Especially, “online adaptation” described in the introduction is not clear. Both informed and RNN policies are further trained on test tasks? If so, I recommend to add the learning curve on the test tasks to show the performance of adaptation.<BRK>The paper could be stronger if the method was framed more generally and it was shown that it could be useful on a broader range of domains that require adaptation or exploration/exploitation strategies. The paper is clearly written and Fig 1 is very helpful to understanding the details of the architecture. The experiments are clearly explained. The main question as a reviewer is whether the paper has significance to the community.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 7. <BRK>They only show the effect of biased perturbation using a single number in Table 2, which is not convincing. This paper investigates adversarial feature augmentation for improving the generalizability of graph neural networks. The experiments provide new, non trivial insights, such as the effect of the number of network layers on the effectiveness of augmentation. The paper is well written and easy to read. Even though the method is completely adopted from prior work and there are no novelties, the authors claim they propose a new solution for graph data augmentation. 2.Even though the performance improvement is consistent across tasks, it is not significant. To be clear, this is not a weakness of this paper. It is similarly insufficient that augmentation improves the results on a discrete graph but doesn t improve when noise is added. Moreover, the authors do not elaborate "how" the data distribution affects the augmentation. In sum, although the paper provides new and valuable findings, the experiment analysis and conclusions that are made are not strong enough for a purely empirical study. Particularly, I still believe the novelty of this method is extremely limited, as it directly applies an existing method on graph node embeddings. Although the authors show extensive comparison on various datasets, their comparison does not particularly show the contribution of biased perturbation and unbounded attack.<BRK>The proposed technique consists on adding adversarial perturbations to the nodes’ features solving the standard min max problem for adversarial training. The authors show that their method can easily be added to standard GNN models like GCN, GAT, GraphSAGE and DeeperGCN with minimal changes, and that it improves these models on different tasks. Pros* The authors evaluate their method on the Open Graph Benchmark (OGB), which is a standard benchmark that facilitates comparison with other methods. * Clear explanation of the proposed method with a concise pytorch implementation as an example. * Good empirical results with different models on different OGB tasks. Cons* My main concern with this paper is that the proposed augmentation technique is not compared against any other graph augmentation techniques. What is the justification for using this one over other techniques? Can FLAG work with other data augmentation techniques? *The motivation for the biased perturbation explained in the “Biased perturbation for node classification” paragraph in Section 3 is not very clear. Also, there are a couple of contemporary papers on graph data augmentation [2, 3] that the authors could reference too. Justification for scoreThe paper is well written and the method interesting and well explained, but it lacks comparisons with other graph data augmentation approaches.<BRK>This work applies FreeLB adversarial training [1] to graph neural networks. Strengths:There are various best practices which are commonly applied in common task framework competitionssuch as data augmentation, adversarial training, and ensembles that improve results by fractions or a small number of percentage points. This work falls under such methods, and proposes to perform FreeLB adversarial training by computing the loss with respect to perturbations. The method is general and may be applied to any GNN for a very small improvement. Weaknesses:The algorithm (1)[1] is not novel and its application to GNNs is an incremental contribution. DeeperGCN+FLAG and GAT+FLAG are currently ranked 5th and 6th on the OGB leaderboard for node property prediction. DeeperGCN+FLAG is currently ranked 2nd on the OGB leaderboard for graph property prediction. Due to the many relative improvements of FLAG,rather than absolute rankings, on the OGB leaderboards, I think that the overall contribution is above the acceptance threshold.<BRK>This paper presented an adversarial augmentation technique for graph neural networks. Particularly, it proposed to inject perturbations to the embedding space of the feature space with the "free" strategy. The results on three datasets show the effect of the proposed method. I really like the analysis of why this method works on graph data, which shows that the data distribution shift is important. The ablation studies are also strong, especially on the bias of perturbations, and going deep, etc. There are a few concerns about this paper. Similar techniques have been applied to other domains, such as NLP and V+L [1,2]. it is better for the author to provide more insights on how these approaches used in FLAG.
Accept (Oral). rating score: 8. rating score: 8. rating score: 7. rating score: 7. <BRK>The authors consider "no press Diplomacy", a complex game played by humans which involves (limited) cooperation and competition. The method described is novel, but also a fairly straightforward extension of prior work to this domain, incorporating a single ply search on top of an imitation learned policy. Comments on the paper:* In figure 3, it might be clearer to plot the two graphs on the same scale.<BRK>This paper proposes a combination of imitation learning and search applied to the multiplayer, simultaneous move game of no press Diplomacy. Are those scores good? To the best of my knowledge, I agree with the author s claim that the imitation/search agent from this paper is the first to demonstrate human level performance in the game of Diplomacy. How many iterations are used?<BRK>The paper is well written; related work is addressed well, and the methods and findings are for the most part very clear. I recommend accepting this paper. Adding this comparison would be valuable, and would help substantiate the strong claim that SearchBot ‘greatly exceeds the performance of past no press Diplomacy bots’.<BRK>In this paper, the authors apply an interesting twist on 1 ply search to the problem of playing no press Diplomacy. The paper is clearly organized and well written; I thoroughly enjoyed reading it. The empirical evaluations are careful.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 4. <BRK>Since an explicit result is needed, they concentrate on one layer NNs. They use their explicit formula for the covariance matrix to compute the likelihood of the data, given $\sigma_w^2$ and $\sigma_b^2$. Besides, all the computation of the covariance should be put in appendix. At least, the authors should propose an application in deeper NNs, even by making strong approximations. Edit:### RebuttalI did read the authors  rebuttal, and the main issue, i.e.the significance, has not been addressed. I cannot take into account the new experiments, since they are not in the paper. Anyway, an experiment with a 2 layer network would be a significant modification of the present paper, which would be not acceptable during the rebuttal phase.<BRK>### UPDATE:Per my response in the thread, I appreciate the authors  replies and updates, but I am keeping my score because  Even with the new 2 layer results, I find this experimental setting still too limited;  Even within the conducted experimental setting, the benefit of likelihood guided initialization over He init is not robust (notably on small dataset sizes, which is contrary to the expectation that a good prior will be more beneficial in such cases). To be clear, I’m afraid this would still not be sufficient to make a convincing case due to the limitations of the considered grid search discussed above. The paper also provides an alternative derivation of the ReLU kernel. I believe the proposed idea is promising. This would not have been an issue for me if the question was answered.<BRK>This submission shows this connection is useful for initialisation, as the marginal likelihood for GP regression can be computed in closed form and for small dataset, optimising this objective is not costly. Weaknesses:3. novelty: The elements of the proposed approach are not novel. The covariance function for ReLU networks is not new and I don’t see the new insights from the new derivation compared to the work of Lee et al (for example: does this make things simpler when moving to deeper networks or a different activation function). It is therefore very hard to justify the ‘near optimal’ performance claimed early in the paper. 5. clarity: Whilst the exposition of the covariance function and single hidden layer nets is great, I found the presentation for the experiments less organised. Overall: I think the proposed method is promising and the topic is of importance. I think including the "on going research" points as the authors brought up  and improving the clarity will greatly strengthen the submission.<BRK>However, I have several concerns for accepting this paper. Unfortunately, this is not shown in the paper, and needs to be properly addressed. This is not discussed. The key for the success of the idea is: make sure the computational cost is commensurate to the improvements in terms of “performance metrics” with respect to existing (and much cheaper) initialisation methods. 2) The main technical contribution of this work is an alternative derivation of an appropriate covariance/kernel function that is equivalent to ReLU activation functions. The authors should focus on the key advantage of this alternative derivation. As such, the authors should work hard in convincing the reader and the practitioner that there is really a competitive advantage in using the proposed approach. If the method was applicable to larger networks and larger datasets, its cost would be prohibitive.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 5. <BRK>Paper summary:This paper studies the  problem of  model agnostic federated learning in the knowledge transfer framework. The main theoretical part (Theorem 2 and Theorem 3) comes directly from PATE. Noting most part of the paper focuses on differential private FedKT, I think the authors should provide at least one comparison with other privacy preserving federated learning algorithms [1,2]. But I have doubts about the claim that FedKT s communication cost is lower than FedAvg. Overall this paper considers a very interesting topic but lacks some important experimental comparisons and method is with limited novelty.<BRK>This submission proposes a new federated learning framework based on knowledge transfer. There is no clear advantage in the proposed method over these existing methods from the reviewer’s point of view. The reviewer would like the authors to explain and discuss the technical contributions of the submission and compare the proposed framework to these similar existing methods based on knowledge transfer.<BRK>The paper in general is hard to read. The paper proposes a novel algorithm for federated learning that reduces the number of communication rounds to one. The paper additionally provides a differentially private version of the proposed algorithm and proves its privacy guarantees; and performs an experimental comparison of the proposed method. This applies to the other algorithm s components as well.<BRK> Post rebuttal I thank the authors for their responses and additional experiments. I understand that the focus of the paper is the cross silo setting, however one of the key questions of an empirical study is to identify the limitations of the proposed approach. For future revisions, I recommend an empirical exploration that helps the reader to understand the limitations of the proposed method. This paper explores the idea of knowledge transfer applied in the federated learning setting. I also recommend a quantitative comparison of the communication costs by plotting accuracies of FedKT and baselines against the number of bytes exchanged between clients and the server.
Accept (Poster). rating score: 7. rating score: 7. rating score: 5. rating score: 5. <BRK>The idea is to train a reward function that explains both the past trajectory and the futur trajectory from that state, assuming that the very goal was that state (hence, the assumption of larger rewards in the past than in the future induced by the gradient). The results are encouraging and support the method. Therefore, outside simple imitation, it could even apply to self imitation or credit assigment in direct RL settings. However, I would ask for some clarifications:  What idea that could help the reader to understanding equation (2) (without going through the appendix of the cited paper)? (so the simple difference in the last line of the algorithm would nicely appear?) (I guess the states, but this is not explicitly told)Also, I would have expected a discussion regarding the environmental limitation of this approach, for example, when a state can be completely misleading, or at least not providing any information (for example, the initial state of an environment).<BRK>The paper considers an approach for reward learning from a single frame which was developed for tabular environments and explicit dynamic programming, and extends it to more complex environments through deep RL. There are some minor degeneracy issues stemming from the extension (in particular in gridworld environment) which the authors can mostly solve. While the paper does not have strong methodological novelty, it is well written, the approach is sensible and combines well with state of the art deep RL, and the results are certainly interesting. Is it a policy with its own parameters? Did you observe any such issues? [1] Reward Learning by Simulating the Past (RLSP)<BRK>I found the setting of the problem to be interesting but not clearly motivated or explained. For this reason, I am unable to recommend acceptance at this stage, due to major clarity issues. Do we only have access to terminal states from execution traces of an expert/human policy? I assume additional interactions with the environment are allowed? For example, let us take the expression in section 2.2:  What are the contents in $\ldots$? Is it actually a typo? Then, one can use the goal classifier as a reward for planning. If the authors can better motivate the setting and improve writing clarity and notations during the rebuttal revision, I am happy to revise my score.<BRK>This paper introduces an algorithm, called deep reward learning by simulating the past (deep RLSP), that seeks to infer a reward function by looking at states in demonstration data. I find the idea of the paper very interesting and the results showing meaningful behavior emerge from a single demonstration are quite nice. Worryingly, appendix D states that learning a dynamics model was attempted by the authors but failed to yield good results. I think the choice of evaluation environments is a little odd and simplistic. I think environments more aligned with the eventual application areas for a method such as Deep RLSP would make the paper much more compelling. From the empirical results, it is not clear that Deep RLSP works substantially better than the simple average features baseline.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>For OMWU in bilinear games over the simplex, they show that when the equilibrium is unique, linear last iterate convergence is achievable with a constant learning rate. In the case of projected OGDA algorithm, they introduce a sufficient condition under which it convergence fast with a constant learning learning rate. The paper does a good job at explaining technical improvements over prior results in the area and particularly the works by Daskalakis and Panageas and Hsieh et al.The experimental section could be slightly improved. It would be very interesting to see numerical estimation of the base of these exponents and see how close they match their theoretical bounds. Do experiments support fast convergence in this case? Overall, this is a nice paper and I recommend acceptance. [2]  Bailey, Piliouras.<BRK>For OGDA in constrained setting, the authors show linear convergence under some error bound conditions which the authors name as saddle point restricted secant inequality (SPRCI). after discussion with authors  During the discussion phase, the authors addressed my concerns and improved their results. "A forward backward splitting method for monotone inclusions without cocoercivity." Then the authors make $\alpha^{T_0}$ to the constant. OGDA:  I think the main reference the authors are missing is FORB by [1]. Variational analysis. It is worth noting that the rate referred in this paper due to Golowich et al., is on the last iterate, therefore not comparable to this paper. I think the authors need to add the related work on metric subregularity (MS) and compare their results with the algorithms utilizing metric subregularity for linear convergence [3, 4, 5]. Moreover, I think it is necessary to see the relation of SPRSI with metric subregularity. Then, what is the advantage of SPRSI and the new constrained OGDA compared to FORB?<BRK>For OMWU, it shows that if the equilibrium is unique and the objective is x^Ty, then a constant linear stepsize results in a linear convergence for the last iterate. For OGDA, it shows that, with constant stepsize, the average duality gap converges with slow rate. Moreover, it shows that under an extra condition, the last iterate converges linearly as well. The convergence is interesting and novel in the sense that it shows explicit linear rate with constant stepsize. Considering it has not beed discussed in other papers, it would be better to provide a more general nonlinear objective class (besides bilinear games on polytopes) that satisfies this condition. Moreover, the role of constraint set in this paper is not clear to me. Is the constraint sets X and Y essential to the problem? ***During rebuttal: I thank the toy examples provided by the authors.<BRK>In this paper, the authors consider the convergence of optimistic gradient for constrained saddle point optimization. However, in Theorem 3 the result is not so explicit: C_4 could be made explicit since it is not so complicated as I see from the appendix. The paper offers a strict improvement over known results of the literature. 2] to "introduced SP RSI +  near usual stepsize" and still manage to show linear convergence. * Since the authors base their analysis on a template inequality (Lemma 1); what would happen for other single call variants such as the reflected gradient (see Chambolle and Pock " A first order primal dual algorithm for convex problems withapplications to imaging") or "optimistic" (Daskalakis et al."Training GANs with optimism")? * (OGDA) and (OMWU) are basically the same algorithm with two different metrics (as stated in Sec.3), it can thus be troubling to see them opposed in the first two sections without this precision. * I am not sure that the 1/sqrt(T) average duality gap rate should be put forward in the intro since it may blur the whole message. * I find the notation "dist" for "\|x Pi(x)\|^2" troubling due to the square.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 6. <BRK>Experiments on a simulated dataset with a flying drone in a subway and living room environments demonstrate good SLAM performance (that approach traditional methods): bird s eye view projections of the 6 DoF poses and the emitted maps closely match the ground truth poses and the occupancy grid. This paper describes a Deep Variational Bayes Filter (DBVF) for Deep Learning based SLAM in 3D environments. al.19] to formulate a DVBF with realistic RGBD data streams. The authors mention that the computational times for this method is still far from conventional SLAM techniques   an actual quantification of the time taken during inference would be useful.<BRK>This paper presents a novel learning based visual inertial odometry algorithm. The world is modeled as an occupancy grid with color. However, compared with the SOTA methods, the proposed method is slower and at the same time has higher localization errors. The paper also reports two outlier data points in the experiments where the learning based dynamics model leads to significantly higher localization errors. The presentation of the paper is great. Novel formulation and learning based dynamics model3. The proposed method is behind the state of the art in both speed and localization accuracy2.<BRK>Strengths  The proposed approach is a very elegant and principled formulation of the dense SLAM problem. This isn’t a fair representation of existing works and doesn’t really do much to position the authors’ novel contribution with respect to the existing literature. The proposed method requires RGB D as input which is obtained from SGM in the paper. The experiments and comparisons are also rather limited. How does the proposed approach perform when trained on Blackbird and evaluated on EuRoC, for example?<BRK>## SummaryThe paper proposes a framework built on DVBF LM, extended to dense 3D mapping. Part of this is also that non standard terms and notation are used. Results seem promising (but comparisons to some other related approaches might be missing) ## Weaknesses   I found the methodology extremely difficult to follow. In particular, the very related DVBF LM which method builds upon.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 5. <BRK># SummaryThe papers studies the problem of robust machine learning, where the labels of the a fraction of samples are arbitrarily corrupted. The paper proposes an algorithm to tackle this problem and evaluates it on a standard datasets. # PositivesThe paper studies an important problem prevalent in modern machine learning, and proposes two algorithms to solve these problems. The experiments suggest that the proposed algorithm is better than the baselines. # Negatives The paper does not cite highly relevant papers, overclaims its results, and the theoretical results in this paper are immediate. Moreover, the paper is not well written. Prior work ([1,2]) has studied this problem in a much greater generality, which are not discussed in this work.<BRK>1.The related work section misses MANY related results on corrupted data and robust mean estimation. For example, "the algorithms themselves are NP hard" is not the correct statement   NP hard describes the hardness of a problem, not an algorithm. 4.Collaborative learning methods seem to have no solid theoretical understanding and it is unclear why the proposed algorithm build on top of it. Theorem 3 is for robustness guarantee with corruption only in the supervision, and existing results have shown O(\epsilon) guarantee (for linear regression and its variants).<BRK>In this paper, the authors studied the problem of training neural networks under data poisoning, i.e., when a small fraction of the training data is corrupted by the adversary. Their first algorithm, which removes the datapoints whose gradient norm is large when computing the average gradient, applies to the general supervision setting. Overall, I think the theoretical result in the paper is incomplete, and the experimental evaluation is insufficient. However, the eps result relies on the fact that the gradient of good data has bounded norm, and I believe in that setting Diakonikolas 19 also achieves eps error.<BRK>cons 1.The authors propose a method that only keeping the data with a small gradient norm in the training process to resist label noise. However, they do not verify that such a design is motivated by their theoretical results. Some important proofs for their key results are missing, e.g., Theorem 2 and Theorem 3, making this paper not self contain. This paper proposes a robust algorithm for noisy label learning. The empirical studies on several datasets show the robustness of the proposed algorithm over different kinds of label noise.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 7. <BRK>The authors develop a Gaussian process vine copula model, very much in the flavor of the modeling approach of Lopez Paz et al.(2013).The improvement to the earlier work seems to be a framework for flexible copula modeling including a copula mixture model, approximate inference, model selection, and calculation of mutual information. The paper on its own is a modest improvement on existing work and is both an engineering accomplishment and has the potential for a useful model. The paper is exceptionally well written and clear. I found it a breeze to read and I credit the authors for that. However, both the validation and the motivation for the model (namely characterizing the probabilistic relationships for neural and behavioral variables) seems particularly thin and could be substantially improved. This is by no means obvious form Fig2b,c. This modeling framework is an opportunity to determine virtually any expectation over the entire distribution and it is entirely possible that MI is neither all that interesting, nor does it play to the strengths of the model. They then describe changes  to the pairwise distributions of variables from the copula model but we didn t need the copula model to estimate. ## Minor points: The authors state (page 7, last paragraph) That the "stimulus related changes in the joint variability of the two neuronal signals are commonly described as _noise correlations_."<BRK>  This is an interesting neuroscience application where Copula estimation has shown to be effective. Having said that, I believe that the technical novelty is minimal. To the authors  credit, they did not claim a huge theoretical edge. They honestly reported the findings of the paper. The paper is well written. Ideas flow very neatly. Therefore, given the work by Hernandez Lobato et al.(2013), as well as the works which have eventually built on top of it, novelty of the proposed method is minimal. p3: "Since none of the aforementioned families alone could describe such conditional dependency, we combined multiple copulas into a linear mixture model": Is it possible to elaborate a little bit more on whether this is actually the best technical choice here? For instance are there any side effects (e.g.computational) resulting from using this linear mixture?<BRK>They demonstrate the efficacy of their method using information theoretic metrics on a synthetic dataset and a real world joint neural behavioral dataset from a neuroscience experiment. The paper was quite thorough in the motivation, development and empirical analysis of the proposed technique. The use of Copulas, that are commonly employed in the Finance community, but are rare in statistical neuroscience, should interest the more theoretically inclined reader. Given the accelerating trend of collecting long term joint neural and behavior in experimental neuroscience, the authors make an interesting and timely contribution to the statistical neuroscience literature. I found that the motivations of the paper were difficult to extract from the Introduction, as there was substantial use of jargon and the intuition behind the technical results were not accessible. To encourage the adoption of their methods in the neuroscience community, the authors should consider improving the readability of their manuscript by making it more friendly to the non statistician reader.<BRK>The experimental data shows that when the observed variables are highly correlated that the proposed approach improves estimation of entropy over competing benchmark approaches (MINE and KSG) when the variables are highly correlated. Synthetic results show a good improvement over competing methods, albeit in a limited setup (highly correlated variables)Mixtures of copulas seems an effective way to produce model complexity, and by linking it to a GP can make sure that it s smooth over the conditioned variable x. Why was this choice made? Please describe more of the scientific setup. What is the scientific question on this neuroscience application? Update after author response:Most of my methodological concerns have been address (except for the ablation studies). The scientific application here is not super well motivated. That s vague; it would be nice to at least clearly discuss how this could be used to facilitate or enhance these scientific experiments.
Reject. rating score: 3. rating score: 3. rating score: 4. rating score: 6. rating score: 6. <BRK>The authors their relate this to the bias variance tradeoff observed in n step temporal difference methods. The authors evaluate their method on Sokoban and the Google Football League environment. The results show that the authors  method leads to marginal improvements on these domains. I do not think what the authors are doing is very novel as MCTS combined with rollouts was already used in AlphaGo. Furthermore, I believe the small difference in results can be made up by using only MCTS with a different exploration parameter (i.e.like the one that was used in the AlphaGo paper). I would like to know what benefits this method brings that cannot be obtained from combining MCTS with rollouts as in AlphaGo or from a hyperaparameter search with MCTS. Is there an anaylsis of the bias variance tradeoff of this method?<BRK>summary:This paper introduces Shoot Tree Search (STS), a planning algorithm that performs a multi step expansion in Monte Carlo Tree Search. In some situations, optimistic initialization of the value network may be helpful to encourage exploration of the uncertain state regions. In contrast, the proposed STS adds multiple nodes to the search tree at each simulation, where each node corresponds to the state and action that are encountered during rollout. In the experiments on Sokoban and Google research football domains, STS outperforms baselines that include Random shooting, Banding shooting, and MCTS. The main reason for only expanding one node per simulation in standard MCTS is memory efficiency: if we fully expand the rollout trajectory and retain its information to the search tree, we may get slightly more accurate value estimates. However, to show the effectiveness of  multi step  expansion compared to  single step  expansion, I think that more thorough ablation experiments should have been conducted. For example, we can consider the setting where both STS and MCTS perform leaf node evaluation (i.e.UPDATE in Algorithm 5) by executing rollout policy rather than by using value function approximator. To relieve too much bias in the current MCTS s leaf node evaluation, mixing MC return of rollout policy and the output of the value network could also have been considered, as in AlphaGo (Silver et al.2016).It would be great to see if STS still has advantages over MCTS in various leaf node evaluation situations. There are some unclear or questionable parts. Instead, more discussions regarding the proposed method should have been placed in the main text. If this describes the MCTS used in the experiments, I would say this is wrong. In Algorithm 6: In UPDATE, $N(s,a)$ and $quality$ are increased by $c$ times more, which means that the longer rollout length, the more weight is given. Is my understanding correct? For the Sokoban experiments, the pre trained value function would significantly affect the performance of MCTS and STS, but I could not find the way how the value function was pre trained.<BRK>Summary:This paper proposes a new algorithm named ‘Shoot Tree Search (STS)’ to perform planning in large state spaces. The authors construct STS by redesigning the expansion phase of MCTS using multi step expansion. The authors provide pseudocode of the STS and compare the performance of STS and MCTS empirically in various domains, such as Sokoban, Google Research Football (GRF). Comments:Firstly, there is no intuitive explanation of why, what and how. Even after reading the paper, I do not agree that STS is good, because there is no intuition as to why it is better than naïve MCTS. More detail, I have a question   The main difference between STS and MCTS seems to be using multi step expansion or 1 step expansion. So I think that this paper needs at least discussion on an intuitive level about the advantage of STS. There are no reference for random shooting and bandit shooting. The authors should provide more explanation about them with references.<BRK>Summary: The paper presents "Shoot Tree Search", an approach that can basically be summarised as a variant of MCTS that expands (adds to the search tree) a longer sequence of up to H nodes to the tree per iteration, as opposed to the standard approach of expanding a single node per iteration. The experiments demonstrate improved performance in comparison to a "standard" MCTS and a variety of simpler rollout based planning approaches, in challenging planning domains such as Sokoban and Google Research Football. Some more explicit discussion about why the difference between using a trained value functions vs. heuristics / terminal results matters so much that it makes this substantially different from prior work would also help (I understand that it is because in prior work the only advantage of storing all those extra nodes was really just that it could retain slightly more information from backpropgations in those nodes, whereas in your case it changes which state is the state that gets evaluated by a trained value function, but this should be more explicit in the paper). Strong Points 1) Well written, mostly easy to read and understand. 2) Simple but interesting idea. If the paper gets accepted, I d still recommend looking at some of them again and clarifying more. I do still like that the paper performs a thorough evaluation of this idea, which I am not aware of appearing in previous literature, and the setting with DNNs for value / policy function approximations is also different from aforementioned papers which may lead to different trade offs. 3.I m not sure that I can fully understand the experiment setup, in particular looking at Table 1. But this is not really clear to me. That said, there is also enough to like about the paper, and I can easily envision that most of the points of confusion could be relatively straightforward to clear up in a revision. Most of the remarks I had were at least partially addressed.<BRK>**Summary**This paper presents a new planning algorithm, called Shoot Tree Search, to control the trade off between depth and breath of the search. STS modifies the expansion phase of tree search by choosing multiple actions (e.g.$\gt$ 1) instead of one level expansion. **Reasons for score**Overall, I liked the paper and the simplicity of the idea. However, my major concern is the comparison with MCTS. I am not convinced that STS would outperform vanilla MCTS when the number of simulations is in order of thousands (e.g.the number of simulations in AlphaGo paper is around 1600). **Strengths**+ The idea is simple and seems to outperform vanilla MCTS implementation in the environments with large action space. **Weaknesses**+ The comparison with the related work is not thorough which makes it hard to come into a decisive conclusion about the performance of the proposed method. + In general, the benefit of MCTS algorithm (like AlphaGo which performs around 1600 simulations) presents itself when the number of simulations are large.
Accept (Spotlight). rating score: 8. rating score: 7. rating score: 7. rating score: 7. <BRK>learning in an unsupervised way such a model of a dataset, as to be able to perform meta learning (here: few shot classification) later. 2.Implementing a model in this framework based on a VAE. (which is not emphasized in the paper). Question/proposal:In Sec.3.2. authors write "assuming that the modalities in prior distribution represent class concepts of any datasets". I find the model principled and new. Sec.3.2.:... inferring isotropic Gaussian distribution, to encode ...<BRK>This paper proposes a method for unsupervised meta learning based on using a variational autoencoder (VAE). For the supervised evaluation phase, in order to adapt the learned prior to the few shot dataset setting, semi supervised EM is run using both support and query sets to adapt the mixture of Gaussian distribution to the evaluation dataset. Then, the query set predictions are obtained using the learned prior and posterior from the VAE model.<BRK>The paper goal is to learn unsupervised feature representations that can be transferred between few shot classification tasks. Yang et al., Deep Clustering by Gaussian Mixture Variational Autoencoders with Graph Embedding, 10.1109/ICCV.2019.00654It seems from the presentation that the main difference is the application to the meta learning approach. The idea is to use a GMM and use an Expectation Maximization (EM) approach to learn the mixture. For the meta test, the model is tuned using EM in a semi supervised fashion.<BRK>"Optimization as a model for few shot learning." ##### Strengths: The semi supervised meta learning (unsupervised meta training + supervised meta testing) setting is interesting and worthy of study as an analogue of unsupervised learning. At "The difference of our model from original VAE is that we utilize a set level variational posterior $q_\phi(\mathbf{z}_j |\mathbf{x}_j , D_i)$, for inferring isotropic Gaussian distribution, to encode characteristics of a given dataset $D_i$. "...we set the prior distribution as a mixture of Gaussians (GMM), where $y$ is a discrete random variable indicating the component of a latent variable $\mathbf{z}$". In ICLR, 2019.
Reject. rating score: 3. rating score: 5. rating score: 6. rating score: 7. <BRK>Although the paper is covering an interesting topic, much of what s in the paper can be found in other works, and there s not a lot of novelty to the insights, nor a large breadth of experiments to justify it as a survey paper. The enforcing locality part is essentially why the pruning strength annealing schemes exist, this insight is not new and can be found in Zhu&Gupta, or in Bayesian settings like in the Molchanov paper, they suggest annealing from a Bayesian statistical perspective  The fact that this leads to multiple stages of pruning to be a good idea, is also known in the literature this paper cites. What is the reason for this? It would have been great to see a lot more insight/experiments on this topic. This can be proven/shown somehow. I do think the paper is well written; and I encourage the authors to look further into this topic and come up with more novel insights/results and methods to improve pruningOther things/questions/suggestions:  In formulations (1), (2) and (5), (6). Under  other considerations  for example, if the weights theta are large, the gradients likely follow suit. Thus the absolute magnitude of the weights might not matter, as it s the relative size of this to the gradient terms that should be considered. Constraining the step size in section 3.2. For OBD, as long as you recalculate the Gauss Newton matrix, I don t see why this method is different when not doing fine tuning. The result cited in appendix A is a very well know result. How could this link explain the OBD performance on the VGG network? Do pruning criteria better at preserving the loss lead to better fine tuned networks? <  this sentence doesn t flow nicely.<BRK>It then compares these three together with Magnitude Pruning (MP), and show that these among four criteria: 1. for the first three, using iterative pruning to enforce locality of the gradient calculation is important, 2. the best method for training loss before fine tuning does not necessarily lead to best validation accuracy after fine tuning. The paper s empirical investigation is valuable and appreciated. It is also useful to know that using iterative pruning can improve these gradient approximation based methods because of locality. 1.My primary concern is the experiments does not seem to lead to a useful guidance for future practice. The paper also didn t conclude which of the four criteria is in general best and recommended. This is also possibly due to that the experiments are not run extensively on different datasets and architectures. (after fine tuning) are not necessarily correlated, but did not give explanation on why this could be the case through experiments, or give useful suggestions to achieve a good valid acc. But due to the limited experimental scale on ImageNet (added in rebuttal, and in my understanding, it only verifies one of the multiple observations mentioned in the paper), I m still leaning on rejection. I updated my score from 4 to 5.<BRK>In the experiments, the authors seek to answer the questions: 1) how well do each criterion preserve the loss; 2) how does the locality assumption affect the final performance; and 3) how does the loss relate to the final performance? Also, the loss after pruning seems not strongly correlated with the performance after fine tuning. The authors did a great job of unifying the analysis of several pruning algorithms. More importantly, revisiting the loss modeling of network pruning is interesting, and it might invoke further research efforts in better understanding the pruning techniques developed in the past and also inspire researchers in designing improved pruning algorithms. However, I still have the following questions:  The authors show that the loss after pruning does not correlate strongly with the accuracy after fine tuning. I believe a large change in loss means that the pruning results are very close to random, so the comparisons in this regime may not be meaningful. For testing the locality assumption, you introduce an L2 penalty on the changes. Why not using some other techniques, such as backtracking line search for determining the pruning ratio at each iteration? In equation (5), why do we need to take the absolute value? I think preserving the loss is only meaningful when the network is converged. Do you have any explanation for this? I believe the studied topic in this paper is important and impactful for the pruning community. In the meantime, it would be great if the author can propose some hypotheses on this phenomenon. I kept my score unchanged. This paper is not proposing a practical algorithm but a revisiting, so I don t think the computational cost is a bottleneck in preventing you from using more advanced methods to get more robust conclusions.<BRK>They study a range of different approximations and modifications that can help improve the quality of the approximation (taking local steps, avoid large changes in weight magnitude, avoiding assumptions about convergence). The authors conduct a thorough empirical investigation that yields practical observations for the design of future pruning techniques. Pros:The paper is well written and well organized and the empirical investigations are well done. The observations made by the authors are interesting and practically useful for the development of future pruning techniques. 2.The quality of the local loss approximation can be improved by taking a series of smaller pruning steps. 3.That loss preservation does not necessarily translate into accuracy preservation. Cons:It would be nice to see experiments in domains other than computer vision. For example, language modeling with RNNs or Transformers. Results at a wider range of sparsity levels for ImageNet would also have been useful, as it seems possible that these techniques could perform differently for high sparsity (>90%) than they do for moderate sparsity (e.g., the 70% sparsity reported in Figure 4). Their theoretical/empirical results appear to corroborate your conclusions that loss preservation is not necessarily the best metric to optimize for when you care about accuracy preservation.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 6. <BRK>However, this step of fine tuning BERT is known to be quite unstable, and depends on a large set of factors, especially the initialization. Since the final performance on these downstream tasks can vary notably, different approaches has been proposed to circumvent this, but still the most common solution consists simply on choosing the best performing model, from a few random initialisation, using the validation set. * The caption for all figures 14 to 17 is wrong, as it should read fine tuning. These are the ones I could find, but it is not an exhaustive list. Besides, the paper is quite well written, and presents in a clear manner the problems with the models, some intuition about the cause of those issues, and then, the solutions to overcome them. Its improved performance is ensured thanks to the large set of benchmarks, on various datasets, the authors have compiled on the current manuscript.<BRK>This paper proposes a few tricks to improve the stability of BERT fine turning, which include a standard Adam optimizer (with bias correction), the top BERT layers re initiation and longer training. The paper is well written and provides an insightful analysis. At last, does these approaches help large tasks, such as MNLI/QQP? It will be great to have a few settings: experiments on small or large tasks.<BRK>The paper focuses on instability issues in BERT finetuning on small datasets. This paper should be cited here. Using all pretrained layers for finetuning   Reinitializing the last few layers before finetuning. Overall, I like the paper; the observation about reinitializing top layers of BERT was interesting and counter intuitive to me; and I think this will be the most important contribution of the paper.<BRK>### SummaryThis paper investigates fine tuning BERT for few sample datasets. Notably, the authors find debiasing omission in BERT adam. Besides, they also find re initializing top layers can speed up learning and achieve better performance. These two findings are interesting. Another finding fine tuning BERT for Longer is incremental to some extend. ### Strengths * The two findings mentioned above are notable. * I suggest the authors can investigate debiased adam and re init on the datasets with enough samples, like MNLI or QNLI. * Lack of explaining the meaning of Int.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 6. <BRK>However, the changes appear to be so substantial that the paper is now essentially a different paper which would require a new review process. The model is trained for masked language modeling (MLM) and evaluated via MLM on held out data and its ability to induce constituency and dependency trees. This paper has an interesting idea at its core: stemming from the success of models like ordered neurons, can we define a neural architecture that uses both constituency and dependency structure? The StructFormer reaches higher parsing accuracies than trivial baselines, but not as high as related recent work. Some results that I think would be more comparable are not included in the results tables. In addition to my confusion about the uncertainties above, I found it difficult to follow the description of the StructFormer due to its density.<BRK>Summary: this paper introduces a new deep learning architecture for unsupervised parsing. The model is trained end to end on masked language modeling (MLM). I list the issues below; I am open to revising my score more positively if the authors are able to provide a compelling response. If I m correct about this, it constitutes clear grounds for rejection of the paper. * The dependency results in Figure 1b are also confusing. Note also that Klein & Manning 2004 include results that don t use gold POS tags but rather word classes using a simple distributional clustering technique, and DMV takes only a fairly small performance hit from this, so it is not true that none of the * models can be directly compared with StructFormer. I would recommend a careful and comprehensive rewrite of this section.<BRK>This paper proposes a neural network optimized by MLM loss that has inductive bias to be useful for unsupervised constituency and dependency parsing. Following previous work, the distances yield constituency parse while the heights, in conjunction with the constituency parse yield the dependency tree. The attention heads of the model focus on different dependency based masks for training. The experimental results show that the method is effective to train a model in an unsupervised manner using just the raw data to predict dependency and constituency parses and performs better than the relevant baselines. Moreover, the ablation study shows that the different kinds of proposed dependency masks are important for obtaining good performance. However, attention heads 1,2, and 5 do look interesting. Further analysis of attention heads would make the paper stronger. One area where the paper suffers the most is readability. The math is broken at several spots and I am not sure if I understand the training objective (section 4.2) correctly.<BRK>The heights and distances are learned by a masking scheme similar to PRPN, but the model is a tranformer MLM rather than a recurrent memory network. The proposed model achieves nontrivial parsing performance. The new masking scheme that explicitly enforces MLM to be sensitive to dependency structure is interesting. The experimental results are interesting. The model outperforms ON LSTM in constituency parsing and is competitive with classical NLP methods that use gold POS tags in dependency parsing. The paper is a bit sloppy in writing. In fact, I still have difficulty understanding how the MLM is trained, as there is no mention of loss or objective (Section 5.1 is not about MLM: it is about data). While this is probably due to the inherent difficulty of dependency induction, it is a bit underwhelming.
Reject. rating score: 2. rating score: 5. rating score: 6. rating score: 7. <BRK>Taken together, this suggests that the link between adversarial robustness and calibration is mainly a link between OOD samples and calibration: generating OOD samples with input smoothing in MixUp works very well compared to the proposed approach, as does adversarial training in deep ensembles (both of which was shown in prior work). Another aspect of the presented work is the exploration of constructing an ensemble of neural nets trained with label smoothing. A comparison to this approach would also be interesting. ####post rebuttal####The contribution of this paper is marginal only. Also in terms of label smoothing the contribution is marginal: in their rebuttal the authors show that MixUp training   a different implementation of label smoothing combined with input smoothing   has a better performance than their method (ECE of 1.8 MixUp vs 2.3 their method for CIFAR 100); they do show that further post processing improves their method, but this is likely true for MixUp too (results not shown).<BRK>How does it relate to the argument that "the model is sensitive to small perturbations are more likely to have poorly calibrated predictions"? 4.I am familiar with the area of adversarial robustness and out of distribution detection, but not very confident about uncertainty estimates. I also did not find any specific introduction or helpful information in Sec 2. It seems to me that this area is not very popular and you can only compare your method with a few of other works, most of which are not even proposed for calibration in the first place (Label Smoothing/Mixup/Temperature Scaling).<BRK>Summary:The paper studied the relationship between adversarial robustness and calibration, then use the findings to improve label smoothing method. An adaptive label smoothing method (AR AdaLS) is proposed to improve the calibration performance. Combining AR AdaLS and deep ensemble can further improve the performance of deep ensemble method. 2.AR AdaLS improves the performance of label smoothing. AR AdaLS can also be used to improve deep ensemble. 3.Improving calibration of deep neural networks is an important task. 2.AR AdaLS does not perform better than deep ensemble, which is the state of the art (claimed by the author). Why is that? This paper reveals some interesting connections between the two things.<BRK>Summary: This paper proposes a new method (AR AdaLS) for label smoothing to improve deep network calibration. In particular, the authors draw a connection between lack of calibration (overconfidence) and examples which are prone to adversarial attacks. They show that by generating smoothed targets based on the adversarial robustness of an example, they can further improve model calibration beyond traditional label smoothing. Method and results are clear and seem to be well situated in the literature. The improvement of AR AdaLS relative to vanilla ensemble (and ensemble vanilla relative to AR AdaLS of ensemble) is meaningfully different for ImageNet.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 5. <BRK>DADS is (almost) a lower bound on I(z ; s  | s) and DIAYN lower bounds I(z; s)+H(a|s, z). This allows for using the exact empowerment (of the approximate dynamics) as the reward function for a reinforcement learning agent. This work tackles the generally intractable problem of calculating channel capacity by learning a dynamics model with a highly constrained latent space (linear with Gaussian noise) that permits an efficient solution. If there were some performance metric they all aim to achieve then this might be fine, but the only comparisons are to the quality of empowerment estimation (which they aren t estimating).<BRK>The paper proposes an new algorithm to simultaneously estimate and maximise empowerment for achieving unsupervised stabilization. Overall I would recommend to polish especially the method and the experimental section before accepting it for publication. 2.I think the claims of the paper are a bit bold. The method requires that the system dynamics can be formulated as a linear Gaussian channel, which requires a control affine system.<BRK>First of all, the paper is well organized, provides a very clear description of the method. UnclearThey estimate empowerment for a state s_t, which now is used to train a policy. Can it be used for other systems that use a VLB?<BRK>The proposed factorization, which is a key component for empowerment estimation, might be too restrictive and not scalable. The overall clarity of the paper could be significantly improved. RecommendationThe reviewer votes for rejection.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 6. <BRK>My score for this paper is based on these points: Motivation: I do not see the motivation for introducing these metrics and why that explains PER in the first place. If the focus of the paper is on understanding PER, then the paper does not do a good job of it. Why is the update on the Q value assumed to be tabular if the experiments are with a deep network on Atari? So, it is unclear why the method works. The method is generally close to PER, and maybe a little better, but no comparison is made on a more efficient method such as Rainbow, and there are only 9 Atari games, which is too little. So, that is not super convincing yet.<BRK>I think the work is very interesting and addresses a central issue, and I hope that the above comments can be useful to improve the paper. Overall, I think it is necessary to think more carefully about the connection between PER and quantifying the value of an experience (e.g.why EVB?how to reconcile moderate empirical evidence of the new bounds?). The authors demonstrate that the proposed tighter bound on the EVB *could* yield improvements in the soft RL setting. Pros of this work:  the work tries to tackle an important and still not fully answered question, which is why prioritized replay works well in the DQN setting but not in the soft RL settings, making very nice connections to the existing empirical literature on the topic, and a suggestion of how to improve PER  the paper is very clearly written and the motivation of addressing PER from a rigorous perspective is clear. PER?Is there a fundamental reason why PER could not be as effective for soft RL in general that cannot be explained by better myopic estimates of the value of each transition? This seems to be at odds with the claims that VER is a significant improvement of PER in soft RL.<BRK>This work aimed to understand the prioritized experience replay, a widely used technique to improve learning efficiently for RL agents. The authors proposed three different value metrics to quantify the experience, and showed that they are upper bounded by the TD error (up to a constant). The extension to soft Q learning was also presented. They also demonstrated that a new variant based on the upper bound achieved better performance on a subset of Atari games. The authors tried to achieve a deep understanding of the prioritized experience replay, which I believe to be an important task. However, after reading through the paper, I am afraid that the question why prioritized experience replay works so well in practice is not well addressed. However, in the derivation of Theorem 3.1, why is "s" used? I guess it may come from Theorem 4.1, but need more clarification.<BRK>The set up and motivation of the paper is relatively clear and well explained. One reason I like the paper is that the process is quite straightforward: 1) Strive to better understand a commonly used algorithm, 2) Derive theory with reasonably good intuition behind it, 3) show that it empirically holds true on simple environments, and 4) the better understood result also yields modest improvements on a test suite. I think an extra paragraph about why we should use surprise as the correct metric for prioritization would be helpful. Additionally, there are a number of small issues in the writing, notably towards the end. In addition, while limited resources might make more empirical investigations challenging, it s also worth understanding how other commonly used algorithmic mechanisms in deep RL interact with prioritization, such as stepping environments in batches, or the preprocessing done to observations, or things like reward or advantage clipping.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 7. <BRK>Overview:Summary:This paper tries to answer the following two questions: i) why training unstructured sparse networks from random initiation perform poorly? The authors show the following findings:1. Sparse NNs have poor gradient flow at initialization. 2.Sparse NNs have poor gradient flow during training. They show that DST based methods achieving the best generalization have improved gradient flow. 3.They find the LTs do not improve gradient flow, rather their success lies in re learning the pruning solution they are derived from. 2.The paper is well written and easy to understand.<BRK>The main contribution of this work is to study gradient flow both at initialisation and during training, and to propose an extension of known initialisation methods that works for sparse networks. #########################################################################Additional comments:I found this paper well written in most parts (just a minor comment on the terminology used in one sub section). 2) The results on gradient flow during training are only superficially commented, although the authors hint at additional ideas based on second order approximations of the loss, i.e.considering the Hessian and its eigen spectrum. Overall, the take home message from Fig.3 confirms the known behaviour of DST methods such as RigL. However, I have problems with the following. That said, it is expected — by construction — to find that LT final networks are close to the IMP solution.<BRK>Main comments:Pros: Three messages that the authors try to convey are important and interesting to the audiences in pruning. It is an observational paper which provides insights on 1) what is a good initialization of the for sparse NN training 2) why DST can achieve good generalization, and 3) is LTH really different from pruning. Cons: The presentation of the paper needs work. Hypotheses:I appreciate identifying the problem of naive initialization of sparse NN and connecting it with gradient flow. I am not fully convinced by the third hypothesis.<BRK>Summary:The paper is clear and very well written. Moreover, it unveils the relation between the performance of sparse neural networks and gradient flow. Based on this relation, it explains also why the dynamic sparse training approach has higher potential of improving sparse neural networks in the future, while lottery tickets are limited by the performance of the pruning solutions from which they are derived. The last but not the least, the paper introduces a simple and practical method specially designed to initialise sparse networks weights. •	The extensive set of experiments is well designed, very informative, and support the paper claims. I believe that extending them also to other types of networks would improve the overall quality of the paper. 3) The relation between Hessian, gradient flow, and sparse training raised my curiosity, but I agree with the authors that this investigation can be let for future work.
Reject. rating score: 6. rating score: 6. rating score: 6. <BRK>Real scene are very different due to both high level (more clutter, occlusion, number of objects) and low level (shading, less predicable texture) differences from the examples used in the synthetic experiments and will likely require different constraints to solve. Other researchers can still learn from the design choices made even if it turns out that those constraints don t work well in real scenes. "How do you turn single object 3D reconstruction models into priors, for scene level inference?"<BRK>The authors proposed a method for 3D scene inference, which jointly does object instance detection, instance segmentation, object localization, and 3D shape and texture inference. The authors designed an autoencoder like network such that it can be trained in a self supervised way from RGBD images. Did it model the relationships between objects? The authors did not test their method on real data.<BRK>Summary:This paper introduces a new model for obtaining object level 3D scene representations from images. The proposed method models 3D scenes by parsing objects one by one into structured representations of shape, texture, and poses. Is it possible to render new views given the parsed scene representation?
Reject. rating score: 4. rating score: 5. rating score: 8. <BRK>Summary:This paper proposes a new method (SEALS) to accelerate the active learning and active search with the skewness of the cardinality of rare class compared to the large scale datasets. The authors conduct very detailed experiments on the tasks of active learning and active search over three large scale data sets to validate the efficiency and effectiveness of SEALS. I like the idea to incrementally enlarge the candidate pool for labelling by the nearest neighbours. 2.The experiment is rigorous and the authors also consider and provide results on NLP domain in the supplementary. 4.The Introduction section only discusses the motivation of active learning for rare concepts. Is there any more technique challenge, or just conduct one more task only? As a researcher whose research areas include this, I would like to point out that this claim is wrong.<BRK>This makes the paper lack of technical depth. The paper is well written and well structed thus easy for understanding. The extensive experiments are sufficient to show the advantages of the proposed algorithm for active learning and active search. More details and clarifications should be provided to make the approach in a technical way. So, there might be some unseen factors that would have more or less impact on the evaluation results. This point should be clarified by the authors.<BRK>This paper proposes an active learning and active search approach that targets samples for rare classes in very large unlabeled datasets with highly imbalanced class distributions. This is a common scenario in real world applications, where these rare situations can be critical to accurately categorize   ie endangered species. Pros:This is a well motivated task, there is a clear need for this type of method as access to unlabeled data increasesTheir method scales effectively to very large sets of unlabeled data, eg matching baseline performance with only 0.1% of the unlabeled data sampled on their proprietary 10 billion image dataset. The authors could do a better job analyzing the impact of their chosen embedding function on the efficacy of their method.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 7. <BRK>The authors propose to use sampling of the binary variables z to compute gradients in Eqs. Was the server communicating with all clients or just a subset? However, the connection the authors make between federated averaging and EM is not as insightful as I anticipated. 18 19.The approach is indeed unbiased, but I m worried that the increase in variance might be substantial in some practical cases.<BRK>In the end, the reader is left with two interesting ideas but it is not clear what the take home message is. The paper revolves around two disjoint stories: *1) Federated learning can be cast in the EM framework*; and *2) FedSparse can solve federated learning with sparse priors*. Unfortunately, neither of these two stories are told in a convincing manner.<BRK>  The paper tries to use EM to explain the optimization procedure of the training method in a federated setting, and then treats the local model as a hidden variable of EM. Pros:It is a novel idea to treat local mode’s parameter \phi as a hidden variable in an EM framework. From a communication efficiency perspective, the proposed method should be also compared to FedDrop. Moreover, reduced communication cost in FedSparse is not very high.<BRK>But the major contributions are not clearly stated. I think it is better move the related work to an early section along with the emphasis on contributions. Originality and significance:I believe the connection between FedAvg and EM and the related understandings open up new avenues for designing better federated learning algorithms. Using an arbitrary threshold may not be the best given that the heterogeneous and non I.I.D. Perhaps the authors can look into the effects of this as a future direction.
Reject. rating score: 3. rating score: 5. rating score: 6. rating score: 7. <BRK>The paper proposed a method to address the distribution mismatch problem in off policy learning. Although I have many questions about the paper, the following two are most important currently and are hoped to be addressed first. It seems not clear to me whether this paper considers the tabular case or the function approximation case. It wrote in section 5 that "our goal is to improve performance of TD learning with function approximation", but its preliminaries (Section 2), its example (Figure 1), and its main theoretical result (Theorem 1) are all for the tabular case. Empirically, all the experiments are designed to solve continuous control problems, where both state and action are continuous. In fact, because both state and action spaces are continuous, all elements in the replay buffers are different from others and this ratio makes no sense.<BRK>The paper proposes a generally applicable modification to experience sampling in the context of actor critic algorithms using a Q function as a critic. Comparing to a generic actor critic algorithm, the changes include the keeping of two replay buffers ("fast" and "slow") and inclusion of an additional re weighting function w which in turn is used in the update of the Q function. The paper includes a thorough performance comparison on MuJoCo and DM Control Suite. The results are good, but the authors seem to use a weak implementation of SAC. The current statement says that the mapping is not a gamma contraction, but one can imagine, that relaxing gamma would still lead to a contractive mapping.<BRK>This paper is on an experience replay approach, as applied to deep RL methods, that uses a density ratio between on policy and off policy experiences as the prioritization weights. However, Figure 2 (b) shows that the performance is not sensitive to the size of the fast (on policy) buffer. The reviewer is curious about the advantage of the developed approach that uses two buffers over what s described above.<BRK>When the samplingdistribution is not the on policy distribution the RL agent may learna biased estimate. Also, how robust is performance to degradations in this accuracy? This work replaces thiscorrection, in the off policy setting, with a learned ratio. The fundamental idea is broadly applicable, althoughsome of the related work touches on related ideas this particularapplication of the idea appears novel and potentially impactful. The reported accuracy is low enough, with empiricalperformance strong enough, that I suspect it *is* quite robust. Prioritized experience replay is an importancesampling technique. Overall, I don t want to attribute this to an actual misunderstanding  by the authors, I think section 4 clearly shows thedepth of their understanding.
Reject. rating score: 5. rating score: 6. rating score: 7. <BRK>The paper introduces a new Markov chain Monte Carlo (MCMC) algorithm to obtain and track the posterior distribution over unknown parameters in a non linear system. Its discovery seems to be the result of the intersection between fields: system identification and Bayesian sampling techniques, leading to new bridges. In fact, novelty and interdisciplinarity are the strong points of this paper, but mostly everything else is compromised due to a severe lack of clarity. A simple, intuitive, and correct notation for the data is needed. * Could the authors use clearer measures of improvement than percentages of improvement?<BRK>APS or MAPS? ### StrengthsThe paper is rather well written and interesting. **The authors added an experiment to show that the method is more general than it appeared in the first version, giving more strength to the paper. please discuss how ARMCMC differs from RJMCMC (Reversible Jump MCMC), where we could jump between model 1 (no contact) and model 2 (contact) back and forth. **  also, how does the proposed algorithm relate to the following ones? Or was it synthetic data? There is a mention of results without EM at the end of section 4.1, but are there any WITH EM, and if not why is it so? In Table 1, why use different metrics for the parameters and for $F_e$?<BRK>**Quality**Overall, the technical content appears correct. The authors present results that show their proposed ARMCMC method outperforms RLS and conventional MCMC for a Hunt Crossley problem. The results also show the robustness of the proposed algorithm to adapt to abrupt changes in the model parameters. **Clarity**Overall, the paper is well written. •	The proposed algorithm is compared with other algorithms. •	It is not clear how some of the parameters are chosen, e.g., threshold (\zeta_{th}).
Reject. rating score: 3. rating score: 4. rating score: 5. <BRK>When the learning rate is above a certain threshold, the authors observed this catapult effect along with a decrease in the curvature of the landscape. Several experiments that demonstrate the authors claims were presented in the paper. These results can be further used to propose algorithms designed to converge to flatter solutions. The model studied in Theorem 1 is very simple. The last paper was not cited in the submitted manuscript. 2.In the definition of $\Theta$ in Page 4: I think it should by $\sum_{\alpha   1}^m$ instead of $\sum_{\mu 1}^p$.<BRK>I think the paper would be more complete if the experiments would verify it in a more robust way. The paper is further evidence in that direction. Overall, the paper has very interesting bits and pieces but it fails to come together as a coherent whole to provide a consistent story.<BRK>I believe this to be an extremely relevant problem since large learning rates are widely adopted in practice due to the their positive impact on the model s generalization, even though we don t understand the reason behind this. I agree that the simplicity of the results/model is valuable, but additional theoretical results (even extensions to Theorem 1, with more involved but stronger claims) would greatly improve the paper and make its contributions closer to what is expected of a ICLR submission. While the theoretical result does provide a precise characterization of the phenomena, it is restricted to a very constrained setting (2 layer linear net trained on one data point), making it hard to evaluate how the result translates to more complex settings (although the authors do a reasonable job at exploring this via experimental analysis).
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 6. <BRK>Summary: The paper discusses a new calibration mechanism for regression models which produce better model prediction and uncertainty estimates. Also it will be easier if the various mathematical symbols are cleanly defined before use in the main paper. Also the experimental results can be reproduced as the code has been included as part of the submission. But I do have certain concerns on the results in Section 3 and the overall presentation.<BRK>On the light of these observations, they propose quantile regression (QR) which does not require an additional dataset. The discussions on the shortcomings of isotonoc regression are valuable. This confusion should be clarified. I am surprised that the datasets picked in Table 1 have no overlap with the ones in Kuleshov et al., 2018. In discussion section, the authors mention that Isotonic Regression suffers in case of smaller calibration datasets. Kuleshov et al., 2018 state that K fold cross validation as an alternative approach to using a separate calibration set. I wonder if such approach would be helpful for small datasets and such addition to discussion would be helpful. I increased my score based on their response.<BRK>Additionally I would have liked to see how accuracy and calibration trade off based on the penalty parameter. Machine learning models are not usually calibrated after standard training, so the authors consider a regularization approach to improve the calibration of the model during training. Overall I found the paper interesting and the main idea principled with decent experimental results. The authors improve using isotonic regression for calibration when the probabilistic regression model outputs Gaussian distributions.<BRK>This paper presents a novel method to produce probabilistic models whose predictive uncertainty is quantile calibrated, without requiring the use of a calibration dataset separate from the training dataset, and with sometimes improved calibration error. The advantage of not requiring a calibration set is definitively important, especially in the case of small datasets of course. The experiments are sound. The exposition has a good structure, especially sec3 is useful in pointing out weaknesses of isotonic calibration. is it the same as $\Phi$ in Algo 1 (undefined there as well)? table 1 and 2: The text does not explain what the case without $\lambda$ stands for   this seems important to analyse the results however.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 5. <BRK>The paper proposes combining the MixUp data augmentation method with teacher student distillation to improve the fine tuned performance of BERT on benchmark NLP tasks (GLUE). The problem is important, well motivated and of interest to a broad base of NLP researchers and practitioners. The paper is clear, and generally well written, although the idea itself is not surprisingly novel (somewhat of a low hanging fruit), from the experimental results, the method improves upon baselines, and so the real world impact could be high, especially given its simple implementation. * Optimal hyper parameters $\alpha_\text{SM}$ and $\alpha_\text{TMKD}$ for "MixKD" on the GLUE dev test are not reported in the main text* "MixKD" includes a back translation (BT) loss term as well as a student loss on mixup samples (SM).<BRK>The authors provide good motivation for the problem of distilling large scale language models and conduct extensive experiments across multiple GLUE benchmarks, including ablation and hyperparameter sensitivity studies, and the results are quite convincing. Since Mixup is central to this distillation framework, it would be valuable to briefly describe the method in the body of the paper, before describing how it was adapted to the domain of language.<BRK>Some of my concerns:  The improvements in the accuracies reported seem marginal and I think there are indication of the significancy of the results in the paper (e.g., mean and variance over several trials?). Two different architectures are used for student models (6 layer BERT and 3 layer BERT). I think the type of data augmentation applied is a special case of Manifold Mixup, so it would be nice to cite this paper as well:  https://arxiv.org/abs/1806.05236<BRK>This paper applies mixup (Zhang et al., 2018) to augment training data to improve knowledge distillation in NLP tasks . To apply mixup to textual data, this paper applies mixup to the word/token embeddings instead of the tokens themselves. Some theoretical analysis has been done, and the experimental results show improved metrics over baseline methods such as DistillBERT. *Weak points*  The proposed method is a direct application of mixup (Zhang et al., 2018). It is true that for benchmarks like GLUE, the datasets are quite small so that data augmentation is important for knowledge distillation.
Accept (Poster). rating score: 9. rating score: 7. rating score: 6. rating score: 6. <BRK>Recently, there are a large number of deep learning theory papers related to the property of neural tangent kernel. This paper shows that for ReLU, the kernels derived from deep fully connected networks have the same approx. properties as their shallow two layer counterpart. This highlights the limitation of the kernel framework for understanding the benefits of deep networks from such perspective. This paper is very well written. I think this paper should be clearly accepted. Here are some minor comments about references. Learning and generalization in overparameterized neural networks, going beyond two layers.<BRK>The paper shows that the kernel derived from deep fully connected networks on the sphere have the same approximation properties as their two layer counterpart for ReLU activations. Overall, I vote for accepting. The paper is overall well written and has a solid contribution. It addresses the limitation of the kernel framework, which is very interesting and phenomenal. The experiments are clear to illustrate the theory. Can the analyses be generalized to other network architectures? The proof only analyzes the kernels on the sphere. Some related literature are missing.<BRK>This analysis can be used to recover some previous results. Pros:1.Analyzing the role of depth in deep neural networks is a very important problem, and people observed empirically depth separation phenomena, i.e., deeper networks have more expressive power. The authors analyzed this problem and showed that NTK constrained to the unit sphere does not have the depth separation phenomenon, which makes this paper interesting. 2.The authors did experiments in a nice way to validate the theoretical results. 2.The experiments are only done on synthetic datasets/MNIST/Fashion MNIST with RF/NTK. The main reason is that the problem analyzed in this paper is important, and the results in this paper are interesting and kind of surprising.<BRK>In particular, the approximation power is analyzed through the eigenvalue decay of the corresponding kernels   shallow and deep networks share the same order of decay, expect the difference in parity and constants. This paper, provides theoretical justifications of the insufficiency of the kernel view point. The organization of the paper is clear and it is relatively easy to read through, although some background on kernels and overparameterized neural networks are needed. Theoretically results appear to be correct and sound. Weakness  The data distribution is a bit restricted. Meanwhile, the network width in ReLU random features experiment is smaller than the sample size, which is not the standard overparameterization discussed in lazy training regime.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 6. <BRK>*Strengths*: The method is surprisingly simple and empirically quite effective. *Weaknesses*: the paper does not do a convincing job of arguing that the reasons for the faster convergence comes from better modeling of rare words I’m still not entirely sure why this works so well. “It means that to reach the same performance, TNF can save 60% of pre training time.<BRK>It is argued that the inadequate training of rare words slows down the pre training. The authors then proposed to keep a moving average of the contextual embeddings for the rare words and use it to augment the input embeddings of the rare words. It is argued that the proposed approach helps with rare words problem.<BRK>The paper proposes an external memory architecture. The experimental results on GLUE are quite surprising. I will vote for acceptance if the authors could answer these critical questions I raise above strongly. This is a crucial weakness. It is not surprising that taking notes for rare words could achieve lower loss/perplexity because the note dictionary gives the extra memory capacity [1].<BRK>It works by identifying rare words in the pre training and adding a “note taking” component to the masked language model which augments these words with an extra “note” embedding at the input layer. The notes are dropped in fine tuning. I am also sure the authors could come up with better ablations than these as well. It is laudable that the authors give experiments in the appendix to give a sense of hyperparameter sensitivity.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 4. <BRK>3.It would be easier to read the table 2 with more detailed caption. The current form of the paper is not ready to be published. Summary:This paper presents a model combining VQ VAE and GPT like autoregressive model. The experiments on Moving MNIST, the BAIR Robot datasets, and ViZDoom show that the model can produce high quality and coherent action conditioned samples. Pros:+ The model generates realistic and high quality video frames. 2.Qualitative results are shown on all datasets but without any comparisons with other models. Since this paper has limited quantitative comparisons and no qualitative comparisons, it is difficult to judge the performance of the model. 3.I believe [1] is very related to the proposed model. In Figure 6 caption, authors claim that  (Bottom) shows samples conditioned on a single frame and action sequence. It requires more explanations. The references of the models in Table 1 are missing.<BRK>Pros:  Simple, principled setup  Architectural novelties for videos (3D CNN, transformer prior) Cons:  I feel that the development is slightly incremental, compared with the original VQ VAE work. Codebook collapse: it would be nice to have some more analysis of this component of the model. Other tasks: This work looks at the task of video generation. Overall: The work is interesting, but does not seem to have sufficient novelty other than having a different architecture design than used in the original VQ VAE work. That being said, there s a lot to learn for practitioners if the authors were to put up a detailed write up on architectures and experiments.<BRK>I believe the authors should take it more seriously on measuring the tradeoffs, especially when the tradeoffs are the main contributions. Requires a clear justification of the main contribution and tradeoffs, as well as a clear comparison with prior arts other than quality metrics (i.e., FVD) alone. It will be problematic for future papers referring to the proposed method. I would recommend the authors to make it more specific to the main feature or novelty of the proposed method. 5.(Optional) Compare with a classical video generative model by modifying the architecture of CNNs and LSTM with VQ VAE and GPT. I believe such a descriptive comparison and analysis are pretty common and should be presented in this paper. I understand this paper is not generation quality oriented. This is a very subjective statement but still needs to be carefully justified and compared against prior arts in the paper. The authors should make it specific.<BRK>Summary: Authors propose to model video by combining a VQ VAE encoder decoder model and a GPT model for the prior. The claim is a new method to model complex video efficiently. There is no experiments and/or benchmarks validating this claim anywhere in the paper. There is work on efficiency in the video generation field that is neither cited nor benchmarked against. It is hard to believe that the authors can manage to find and cite every relevant (un)published paper by google and deepmind authors yet they fail to find work published by other groups in this field. " The complexity of the problem also demands more compute resources which can be considered as one important reason for the **slow progress** in generative modeling of videos." The ablation studies provide for the most interesting insights with regard to this work. I am happy to update my review and score if these issues are addressed. But this work in it s current form is not publishable at any conference.
Reject. rating score: 4. rating score: 4. rating score: 5. <BRK>This paper explores the application of spectral clustering methods to assess modular organization in the emergent architecture of deep networks. The paper is comprehensible, though the general structure and the writing could be improved to improve readability. For example, it would be nice to see if clustering coefficient and average path length (as defined in [1]) can provide useful information also for the analyses proposed by the authors. Note that in [3] the authors also investigate “lesion tests” by means of interventional techniques, which would make that approach very interesting as a further benchmark. If this value is not theoretically motivated, further analyses should show that the results are robust to variations in this value. The authors consider “modularity  as an organizing principle to achieve mechanistic transparency”. Though I sympathize with this statement, I guess there are several cases where modular systems (or in general systems with localized representations) can develop complex emergent dynamics that still prevent interpretability.<BRK>Despite these positive notes, I have reservations about the presentation and results of the paper. In figure 1, for example, the authors show slightly higher contrast in their identified clusters than random clusters. The results are limited to black and white images (MNIST and fashion MNIST), and not all examples look great. (2) The results crucially rely on a second paper which was concurrently submitted and can t be reviewed because it is anonymized. The results shown in this paper are thin and qualitative (see point 1), so in my view these two paper should be combined into a single paper which overall might tell a more comprehensive and compelling story. (3) The paper does not generate testable predictions or practical insights that could be used by used by practitioners.<BRK>The manuscript introduces an approach, based on importance and coherence, for evaluation whether a partitioning of a network exhibits modular characteristics. is there an indication on the effect of this parameter? While in some cases some patterns are indeed visible, in other cases it is hard to make sense of what is being presented. Otherwise it is hard to assess properly the origin of observations made on the results of the experiments. However, from the observations made on the experiments I do not see the added value that the proposed method could bring to interpretability/explainability of the analyzed models (networks). However, from the content of the manuscript it is not clear how having a modular network/representation does contribute with the two listed aspects.
Accept (Poster). rating score: 7. rating score: 7. rating score: 5. rating score: 5. <BRK>Improving the performance of transformer is an active area of research and has numerous applications. The authors first show that a single layers  ordering is not better than all others and further demonstrate that per instance layer ordering with parameter sharing consistently improve overall performance (which is quite surprising). #### DecisionI tend to accept this paper as the method is novel and the results are good. (for instance IOT (N 6) on IWSLT14) I am curious if it is balanced and if the decisions made by the classifier are consistent with the best performing order in Table 1 when each transformer order is trained separately.<BRK>During training the model uses a soft Gumbel noised output of the classifier to combine the outputs from stacks with differently ordered sub layers. The proposed approach results in significant gains over a standard transformer on all the reported tasks. A more careful comparison on all the reported tasks to counter against the regularization hypothesis would significantly strengthen the claims in the paper. Other questions for authors:1. 3.Did the authors try using a single predictor for both the encoder and decoder (i.e.a single classifier predicting $M\times N$ classes)? 5.Other relevant work on instance level adaptation in NLP: [1,2,3]Recommendation: I think there are several interesting ideas in the paper, and would recommend (weak) acceptance. There is still some doubt as to whether the quality gains are arising from a regularization effect or the instance based layer re ordering.<BRK>I think the general idea behind this paper is very exciting: architectures mutating in an instance based way. However the proposed architecture is essentially training an ensemble of three, or n_reorder, weight shared models and then at inference time, using a hard threshold. Suggestions:* Compare training flops and eval performance to the approach of training a 3x larger model that is then pruned or distilled.<BRK>It makes a hypothesis that different layer order has an impact on the performance of the model, and the hypothesis is verified by experiments. The IOT structure proposed in this paper has been evaluated on several datasets of machine translation, abstract summarization and code generation. My concerns are as the following,1. Training time: it seems that this paper only reports the time of inference. Therefore, I expect to see comparison of IOT with Transformer. 2.Internal order: since the mentioned internal structure order of Transformer will affect the results, meanwhile, according to the previous work, the order of LayerNorm may also affect the performance of the model. I expect to see some obvious enough evidence on different structures in sequence level or batch level from such training.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 7. <BRK>The paper describes a few implementation tricks to improve the performance of word2vec and fastText algorithms on CPUs. Cons:The novelty as well as the importance of this work is not clear. The algorithmic implementation variants tried on the other hand such as ignoring subwords or batching for skip grams are specific to fastText, but are fairly straight forward. The only novel contributions to the algorithm that result in improvements appear to be dynamic hidden layer update and byte pair encoding. Additionally, the paper fails to motivate why this line of optimization is important while there appear to be other straight forward ways to make the training significantly faster (distributed training, clean GPU implementation that is open source, etc).<BRK>This paper applies several algorithmic and code optimization techniques to reduce the training time for word2vec and fastText. *Strong points*  The 3 20x speedup is impressive. Many of the optimization techniques being used can also be found in previous works. Therefore the novelty of this paper might be slightly below the standard of ICLR. + The main text frequently refers to the line# of the source code, which hurts the readability of this paper. Ideally, provide a table to summarize the datasets being used in this paper.<BRK>This paper proposes the approaches that reduce the training times of word2vec and fastText. Basically, I like the motivation and results of the paper; it is quite an important research problem to reduce training times of popular word embedding approaches. Besices, this paper is well structured and easy to follow; contributions are clear and proposed approaches are well described. However, the approaches proposed in the paper are somewhat technically shallow. This paper seems to be an engineering or industrial paper rather than a research paper; ICLR is not the best venue for the paper. To improve the paper, it is good to clearly describe the highlight of the proposed approach and explicitly show its theoretical property.<BRK>The current work introduces a new approach that speeds up the training process of word embedding models such as word2vec and fasttext on specific hardware. I guess WordSim 353 is a must for a semantic similarity evaluation and its multilingual extension. From what I saw in the appendix, MUSE is an average of many datasets, which includes WS353. In addition, a discussion or application of the author s improvements on GloVe seems also reasonable. In fact, columns showing almost no difference between models.
Reject. rating score: 4. rating score: 5. rating score: 6. <BRK>The authors claim that the model can be to adaptive to domain shift, but there is no explanation why the model can do this? Although the objective is simple, in the current state, the model is still not clear enough to follow. Update after rebuttal Thanks for the feedback from the authors. Unfortunately, although some parts are clarified, the main issues still exist. Overall, the novelty is limited and the motivation is still not clear enough.<BRK>The authors claimed that the new form leads to state of the art performance in several numerical experiments. The motivation for the work is sensible. How can this new loss function be applied other than the dropout networks? However, Kendall & Gal (2017), which the author cited in the manuscript, addressed this issue and provided a better uncertainty quantification method. The paper s writing makes it very hard to understand the results. The current presentation is confusing and it s hard to recognize which is better. ##########################################################################I vote for rejection.<BRK>On real datasets, the proposed method is compared to many uncertainty  estimation methods and shows competitive performance. I think further discussion on this issue would be  welcome. The paper is in general correctly structured but leaves some important details  to the appendix. For a fair comparison with ensemble methods, the total number of parameters  and operations should be taken into account. Overall, I maintain my positive outlook on this work. Although theoretical justification could be improved, I think the experiments do signal that there is something interesting and valuable in this simple approach for characterizing uncertainty.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 7. <BRK>The authors argue that there are some safe "spots" in the data space that are less prone to adversarial attacks. The application is important and the results look promising. However, I have the following concern:  The authors propose a new threat model where the adversary may have access to the labeled data. How can we find a safe spot when the label is unknown. Some of the findings are not quite surprising.<BRK>Usually, the networks are trained using adversarial examples to improve robustness (adversarial training). + The proposed approach is interesting and different from existing works. Weaknesses: 	The major concern lies in the evaluation of the proposed technique. Here, the authors find safe spots and also propose safe spot aware adversarial training but evaluate on PGD based adversarial attack in a standard  manner. Final thoughts:The proposed method is clearly motivated. Therefore, I marginally accept this paper. However, I also agree with the other critical points raised by other reviewers (particularly Reviewer 4) that are of major concern.<BRK>But I don’t think their results could support this conjecture. Overall, the writing is clear and the idea is interesting. Could the authors compare their method to them? However, I have the following concerns: 1. Although they have shown that the attacks could not successfully find adversarial examples for the safe spots, it might be due to the “gradient masking” issues. Some of my concerns have been addressed and I have raised my score. In such cases, the proposed defense framework may not work. Could the authors describe it in detail?<BRK>The paper proposes a new method for making adversarial attacks more difficult. They also extend the idea to out of distribution (OOD) detection, though the main contribution seems to be in mitigating adversarial attacks during testing. Section 3.6 "OUT OF DISTRIBUTION DETECTION" is not convincing:Apparently the proposed method is not only hard to train, but also has three important hyper parameters $\gamma$, $\lambda$, and $\mu$, which need to be carefully tuned. Therefore, even though the authors report improvements over previous methods in Section 4, I am not convinced that this is a practical approach to OOD. In Section 4.1, I am not sure what the authors mean with "our methods can find safe spots on over 85% of thetest set images".
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 6. <BRK>Here, I think it would make sense for the authors to state what their assumptions are about the data generating process, separately from the parameterization of their model. The authors argue that this is a weaker condition than requiring that x satisfy unconfoundedness. # FeedbackIdentification of causal effects in the presence of proxy variables for unobserved confounders is an important but subtle problem. However, this means that the bar for making contributions in this area needs to be high.<BRK>They show that the confounder is identifiable up to an affine transformation. Despite some clarifications from the authors I still vote for rejection as I believe the paper requires a major revision. The main contributions of this work are presented in Sections 5.1 and 5.2, where they derive identifiability of the treatment effect via identifiability of representation. Also, this reference should be added in the related work section.<BRK>Pros: The paper addresses an important problem The paper provides a theoretical analysis of the identifiability of representation and treatment effect. In the works that do not consider unobserved confounders, I believe the covariates $x$ in the treatment effect $\mu_t(x)$ are the observed confounders and assumed to satisfy the ignorability condition. That would be a very unreasonable assumption. I think the ignorabiltiy assumption made before Eq.(2) is not correct. Overall, I vote for reject. I think the paper made some unreasonable assumptions. Why only compare with CEVAE on the synthetic datasets but not other methods?<BRK>It would be useful if these conditions were listed clearly and centrally in the paper, without assuming familiarity with this previous work. Cons:  The difference between CEVAE and CFVAE should be clarified. al., where it looks like they state that there can be a direct edge from X to t. Could the authors comment on this? My main concern with the paper is an unclear comparison to Louizos et.
Reject. rating score: 3. rating score: 4. rating score: 4. <BRK>The paper proposes a new regularization for the dictionary in the learned convolutional sparse coding model of Sreter & Giryes  18. The paper tells a story that the dictionary need to be incoherent, that some prior work enforces this by using large stride convolution, and that the proposed method does not need to use large stride. So how do you enforce those high frequency filters to be incoherent? It is stated as a second main contribution of the paper that the thresholding in the network can be made to be dependent on the noise level in the image, so that the network is capable of performing blind denoising. However, the explanation for how it works (Sec.2.3) is very sketchy. Why is the adaptive threshold only used in the last layer? Also, the threshold is set to be a multiple of noise variance, but how do you estimate the noise variance in practice? Overall, the presentation of the paper needs major improvement to make the motivation as well as technical details clear. In addition, the technical contributions appear quite minor and are not fully justified: it is unclear how fixed low pass filter compares with a bias term, and it is unclear how effective the adaptive threshold is in practice.<BRK>This paper proposed a convolutional dictionary learning (CDL) work for image denoising. Compared with existing CDLnet, the proposed method used stride convolutions with pre defined low pass channels  to improve the performance and reduce complexity. The idea of constraining low pass atoms in learned dictionary is interesting, which limits the space of learned atoms to have the desired behavior. 2.The proposed method parameterize the soft thresholding operator in LISTA such that the thresholding is directly linked with the estimated image noise level. The contribution of this paper is incremental. The proposed method is heavily based on existing CDLNet with some modifications. Although such modifications improved results a little bit, the motivation is not very clear and the implementation is empirical, e.g., Eq.(7).2.The proposed method did not substantially improve the results. It improves the baseline CSCNet a little bit, but still not good as the state of the art methods such as DnCNN. In addition, all the experiments are using synthetic data with ideal Gaussian noise. Actually image noise is more complex and is a mixture of Poisson and Gaussian. The results on real image noise might be even worse. "Benchmarking denoising algorithms with real photographs."<BRK>## SummaryThe paper proposes a denoising method with a neural network inspired from convolutional dictionary learning. In the proposed method, one atom of the dictionary is constrained to be a low frequency filters and all other atoms are to be high frequency atoms. The authors also propose to make the threshold depends on the noise level to better adapt to different noise level and to use strided convolution to reduce the computational cost of the method. 4.What is the training time for the proposed model? *For extra citations, see bibtex at the end. This is not discussed, but I guess this is similar to the training time of other models. p.1: `a linear combination of a collection of vectors`: this is simply linear representation. The sparse linear representation also promotes the usage of only a few atoms. Apart from this point, the novelty of this work is not very significant. p.3: `interpretabile`  > `interpretable`  p.4: Note that there is a third option for training LISTA networks which is to use loss in Eq.(5) as a training loss, as it is done for instance in `[Ablin2019]`. In particular, how does FCDLNet compare with FCDLNet with `stride` $\in\\{1, 3, 4\\}$. As the gain is small between CDLNet and FCDLNet, it would be interesting to see the confidence here. It would also be interesting to study the impact of the noise level estimator. This would be interesting to add such experiments. Could the authors comment on why the learned network is more interpretable than a classical network? This makes the model somewhat incomparable with other approaches as $d^j$ won t have the unit norm property. 3.When training the network, is the true noise level given as an input of the model or is it also estimated using the wavelet based estimator?
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 5. <BRK>Some statements to motivate this work are not well explained and supported. However, the differences from previous studies are still not significant to me. The experimental studies are not convincing to me in order to show the effectiveness of the proposed PGC. Ablation studies on this readout layer should be provided in order to show that the improvement is indeed from the PGC.<BRK>2.The relationship between linearly stacking k GraphConvs and using a single k PGC layer is cleanly explained and motivates the work well. Recommendation: Overall, I am slightly leaning to reject. What is the contribution of using this stronger aggregation scheme instead of a more standard weighted sum?<BRK>2.The whole paper is described clearly and easy to follow. The generalization of PGCN on large scale graph remains a serious concern.<BRK>It is unclear why these tasks were not considered. Moreover, methods of  (Kipf and Welling, 2016), (Luan et al.2019) and (Defferrard et al., 2016), that are most relevant to this work are not considered for experimental comparisons. Paper presents a novel framework.
Reject. rating score: 6. rating score: 6. rating score: 6. rating score: 6. <BRK>Summary:This work proposes that many common issues with GAN methods are based on the weighting of the samples given to the generator’s objective function. The authors argue that this modified objective is to blame for a number of common issues with GAN methods   most notably the mode dropping issue. The authors present theory that backs up these claims and they propose a new generator training objective which re weights the gradients of the generator objective to have the same average magnitude as NS GAN but have the same relative magnitudes of the original GAN objective. The difference between the presented baseline and the proposed method is smaller than the difference in performance between the presented baseline and previously published methods with the baseline objective. For these reasons, I am advocating against acceptance of this work but making it clear that I think this paper is borderline. This change may seem insignificant on the surface and the authors here clearly show that it has an impact   one large enough to consider if it should be used at all. If the experimental setup was more in line with prior work and the same trend in results held, then I would be more likely to recommend acceptance of this paper. This model uses the NS GAN objective (to my knowledge) so, I am confused as to why the authors did not simply replicate their setup   especially since the standard CNN model proposed in the original spectral norm paper can be trained with reasonable compute and the authors have released code. If I am misunderstanding something about the experiments, please do let me know, but I am confused by this choice. There have been a number of proposed metrics like this such as IS and classifier score (https://arxiv.org/abs/1905.10887). Pre trained models from prior GAN papers could be used to obtain these scores. Since this is not shown, then I am uncertain of the significance of the observations made in this work.<BRK>The authors present a simple estimator which approximates the gradient of the "true" generator loss in GANs using the gradients from the more commonly used non saturating generator loss. They present results on toy data and small image datasets like CIFAR to show that the method leads to stabler training and does not suffer from mode collapse as badly. Had this paper appeared at ICLR in, say, 2016 or 2017, I think it would have been a welcome addition to the literature on "tricks to improve GAN training", which was fairly small at that point. In addition, recent advances to VAE training (building on some of these GAN tricks like spectral normalization) have massively improved the quality of VAE samples, so the need for GANs continues to diminish. Nevertheless, it is not my job to judge whether a given paper is trendy or not   only to judge whether it is technically correct and up to the standards of publication. On that front, I think it is a decent paper   the method is quite simple, but the results on both mixtures of Gaussians and CIFAR and other image datasets do seem like an improvement. As it is, I worry this paper will disappear in a flood of other similar papers without more rigorous comparisons   but then, that may be for the field to judge after it is published. They are unreadable unless zoomed in very far on a screen.<BRK>This paper reexamines the original (MM) and the non saturating (NS) GAN objective. While the scaling factor for the MM gradient is responsible for the well known vanishing gradient if the discriminator is optimal, the scaling factor of the NS gradient counteracts this saturation effect. However, on the other side, the NS scaling factor introduces a mode dropping effect and the inability of the learning dynamci to discover new modes. On MNIST, CIFAR10 and Cat 128x128 MM nsat was compared with NS, Hinge and LS GAN outperforming all of them based on the FID. Pros: The theoretical insight why NS GAN suffers from mode collapse is novel and interesting. The experiments are convincing and extensive. For completeness, it would be interesting to see the experiments in section K with the pair MM nsat/MM as well.<BRK>This paper proposes an explanation for mode collapse in the original GAN with the log  D objective for the generator (dubbed the non saturating GAN or NS GAN for short). It sheds light on the weaknesses of the log  D variant of GANs. The argument is not presented clearly, and sometimes observations with no obvious logical relationship to the claim are mentioned. At other times, it is unclear what the point is. 2.3, it is mentioned that "the minibatch used to update G will have more samples from O since they are generated more often". It is unclear how the number of samples from O in the minibatch could cause a difference between MM GAN and NS GAN   this observation is true for both MM GAN and NS GAN! In the next paragraph, the paper mentioned that the generator gradient "is only locally informative"   again this is true for all GANs. How is this relevant for the argument made in the paper? In the next paragraph, the paper mentioned "NS GAN struggles to discover new modes: g(x) ≈ 0 ⇒ 1 − D_p(x) ≈ 0"   the connection between the claim about the difficulty of discovering new modes and the equations should be explained more clearly. Also, the implication in the equations would not be true if r(x) ≈ 0. So technically the paper s claim on the discovery of new modes cannot be justified by "g(x) ≈ 0 ⇒ 1 − D_p(x) ≈ 0", because D_p(x) could take on any value at locations other than the data points. Though because this is a common mistake in the literature, I d be fine with the addition of a note that explains this caveat before presenting the theoretical argument (without insisting on a fundamental solution to this issue).
Reject. rating score: 3. rating score: 3. rating score: 5. <BRK>The major contributions of this paper are as follows. [Cons]* The paper is very hard to read. Below, I list the points that makes the paper hard to read. I could not understand what the "discrimination part of CNN" means when I first read the paper. Although the authors cited some related studies, such as [Wan+20], the importance of the problem need to be described in the paper. For example, the inputs and the outputs of $\mathrm{CN}$ and $LN$ are not defined. In addition to the readability, I also found some technical errors. The authors somehow ignored it.<BRK>The paper would also be significantly improved if more realistic datasets would be explored. This alone is a barrier to publication. Setting aside the grammatical errors, I believe there are some issues in the content of the paper that would need to be addressed as well in order to make this paper ready for publication. My primary issue with the paper is that it attempts to provide an  explainable alternative  to a CNN but this explainable model still relies on the features extracted from the convolutional section of a CNN.<BRK>This paper proposes an interesting framework for training interpretable CNNs, similar to distillation methods. While the motivation is easy to understand, notation is continuously introduced in the paper without clear description and definitions. This makes it very challenging to parse the technical correctness of the paper. 5.Way too many typos in the paper. Clarify what "discrimination part" of CNN means, very early on in the introduction and abstract. 6.Did the authors do a qualitative analysis of where exactly the approximate DCLM has lower accuracy compared to the CNN model and why?
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 5. rating score: 6. <BRK>This paper proposes a modification to the latent distribution of a flow model, replacing the commonly used full rank Normal with a low rank one which has the form N(0, AA^T). However, the invariance of KL under reparameterization is a well known result, and it immediately follows that doing MLE   minimizing KL in X space   minimizing KL in Z space. Other parts of the paper are more like combination of existing methods and known results (e.g., the invariance of KL under invertible transformation, spread divergence). * The variance of K is close to zero.<BRK>This paper proposes a new method to train flow models on data from low dimensional manifolds embedded in high dimensional ambient spaces. The basic idea is based on minimizing the KL divergence in the latent space, which is equivalent to maximizing expected log likelihood over the data distribution. However, my concern on the theoretical part remains. In addition, I agree with R1, R3, R4 on the scale of their experiments and scalability of the approach.<BRK>Pros:1. this work propose to learn a manifold prior, by doing so, it can be used to improve the generation and representation quality. Intrinsic dimension is applied in the method. Cons:even though I love the idea of this work, in the experiment section, the authors fail to compare their method with other flow based method with quantitative results.<BRK>This paper proposes a new method of training flow models, instead of minimizing KL divergence in the data space X, the paper proposes to minimize the KL divergence in the latent space Z. The experiments show that for simple synthetic data, the proposed approach is able to recover the true dimensionality of the data.<BRK>The notation is pinpoint, and the paper is overall an easy read. ### Concerns:One primary problem with the paper is the reinvention of the connection between MLE and KL divergence minimization in Z space. Masked autoregressive flow for density estimation. 2019### Updates after the rebuttalI found the approach clean and believe it has some merit among competing approaches. However, as detailed in discussions, I am skeptical of the scalability of the approach.
Accept (Poster). rating score: 6. rating score: 6. rating score: 5. rating score: 4. <BRK>Strengths:+ The paper is well written and technically sounds. Weaknesses:  This work wants to see the effectiveness of the use of intention as a communication scheme under MARL. However, in general, this is not the first to use intention. If so, is it really important? Where is the ablative study of using attention?<BRK>The method involves agents producing imagined future trajectories using learned environment dynamics, and then communicating some parts of these trajectories to other agents. My main concern is a lack of ablations, which makes it hard to tell which parts of the method are important. However, this is not all there is to communication, and the communication of agent intentions is an important topic to address. * A comparison to a case where the agents receive only their own messages to condition their next actions. I share the concerns of Reviewer 4 about the significance of the results   this is still not in the paper, and not present for the new experiments.<BRK>Additionally, the attention module in Section 4.2 is learned to weigh between current and future information dynamically. Weaknesses: While the motivation of this work is clear, the experimental results fail to support the claim. The results in Figure 3 show a small performance difference between IS (proposed approach) and baselines due to the large variance. Performing a statistical test, such as the t test, to verify whether the proposed approach achieves statistically significant results than baselines will be helpful. Adding an abbreviation analysis, such as the performance difference between with and without attention, will be helpful. 2.The action predictor (Equation (2)) predicts the peer agents  actions based on the agent s own observation. ICML 18I have read over the rebuttal and discussion and will keep my evaluation score as it was since the concerns about the weak performance result still remain.<BRK>[Summary]Paper proposed to generate the communication message in MARL with the predicted trajectories of all the agents (include the agent itself). in the MARL domain. [Post rebuttal]I have read through all the other reviews and the rebuttal. Would like to thank the authors for their efforts in improving this submission. However, the main issue on the lack of novelty remains and I also find R4 s concern on the significance of results is valid.
Reject. rating score: 4. rating score: 4. rating score: 6. rating score: 6. <BRK>The paper is very well written, straight to follow, theory is well laid out and it is interesting. However, despite a well outlined theory, I think that in the end the experimental evaluation demonstrates a major weakness of the proposed approach.<BRK>I would be willing to vote for acceptance if the experimental part is strongly improved. Justification of score:Despite my many points of concerns, the theory in this paper and the main idea are good. The authors should elaborate on this. iii) Conclusions of the experiments are not justified due to limitedhyperparameter search.<BRK>Summary:In this paper, the authors introduce the bidirectional self normalizing neural networks (BSNN) that preserve the norms in both forward and backward passes. Or it still has the property of fixing deviations in preceding layers? The proofs seem rigorous.<BRK>2.How does this impact generalization? I did not check the math. It would be good if the authors could provide some intuition for the main contribution: that an appropriate scaling and shifting of the nonlinearity is what is needed. Particularly in terms of run time and generalization.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 7. <BRK>The paper presents a novel pretext task for self supervised video representation learning (SSVRL). The authors design several surrogate tasks for tackling intentionally constructed constrained spatiotemporal jigsaw puzzles. The proposed method shows superior performances than state of the art SSVRL approaches on action recognition and video retrieval benchmarks. Good performances on two benchmarks. Lack insightful analysis of how the idea is inspired, why it works. It seems the intuition of the paper is to make the 3D jigsaw problem easier to solve and it will just work. But why the easier problem could help learn better representations? Actually, the carefully designed surrogate tasks are quite different from the constrained jigsaw problem. All these questions need more indepth clarification. In case the proposed method is not a principled method, but a carefully designed method. ## SummaryOverall, the paper presents yet another method to design the pretext task for SSVRL. But my major concern is it lacks enough insights for inspiring future research for this topic.<BRK>Positive results on two downstream tasks, action recognition and video retrieval, are shown. **Strengths**:+ The pretext task of solving spatiotemporal jigsaw puzzles in order to learn meaningful video representations is well motivated, as has also been demonstrated in the literature. + Constraining the number of permutation to make the problem tractable indeed seems necessary, and the proposed method of doing so, along with the proposed surrogate tasks, are indeed effective, as indicated by the positive results on downstream tasks. **Weaknesses**:  I found some of the major statements in this work to be over claimed:    " To our best knowledge, this is the first work on self supervised video representation learning that leverages spatiotemporal jigsaw understanding". As you mention in Sec.2,  Ahsan et al.(2019); Kim et al.(2019) both attempted to solve spatiotemporal jigsaw puzzles as a self supervised pretexts task for learning spatiotemporal representation. This paper claims that the particular constraints imposed on the permutations used in those papers in order to increase the tractability of the problem, are not "true" spatiotemporal permutations. I believe this work at most relaxes some of those constraints, albeit in creative ways, but is not solving a fundamentally different problem. Table 1, which demonstrates "state of the art performance" on action recognition, is incomplete. Some stronger, not included results of methods you did include in the table, and which, to the best of my knowledge, use only the RGB modality:   Pace           |  S3D G  |  87.1  |  52.6   SpeedNet  |  S3D G  |  81.1  |  48.8  for UCF101 (left) and HMDB51 (right). In general, I find new SSL work especially interesting if it (A) enables solving new tasks that were unfeasible before, and/or (B) pushes the boundary of SSL results on interesting/important tasks. **Post rebuttal**I d like to thank the authors for addressing my comments. What I m not entirely sure about is how much this method manages to push the boundary of SSL. Comparing methods with different backbones is indeed tricky, and my intention was definitely not to discourage SSL works from academia. But the burden of proof should be on the new method to perform as close to an apples to apples comparison (in terms of backbone) to existing methods as possible. In the end, there are many many potential pretext tasks for SSL of video representations, and I do feel that in order to be publishable at a top tier venue, they should either enable new tasks, or show clear superiority over existing methods. This should be moved to the main paper. If I could, I would be borderline on this paper. But since I can t, I ll give the authors the benefit of the doubt, and raise my rating to 6 (marginally above).<BRK>**Summary and contributions**:The authors propose an approach for solving a constraint version of the 3D (space+time) jigsaw puzzle over a video, using four easier surrogate tasks. All of them are learned together, using a joint learning objective. The authors show the value of the newly learned representations in the self supervised scenario on two downstream tasks (video action recognition and video retrieval), achieving state of the art results compared with new methods. **Strengths**:  The paper comes with a solution for expanding a classical self supervised 2D problem formulation to 3D, proposing a tractable approach by breaking it into four tasks and imposing constraints through them. A good amount of details on the method and on how the permutations and the surrogate tasks were chosen. The fact that even though the permutations are heavily constrained, they proved to be useful. The newly learned representations, that embed the temporal and spatial continuity aspects, achieve state of the art results on two video tasks. **Weaknesses**:  The temporal aspect is not sufficiently highlighted in the experiments:  An ablation study to better distinguish between the spatial and spatiotemporal representations, with both quantitative and qualitative experiments would strengthen the submission. It would be interesting to see how much the quantity of temporal information reflects in the performance (ablation on the number of used frames)**Quality**:The idea is simple but complex enough to generate valuable representations, having the temporal aspect integrated. The paper is technically sound. **Novelty**:The overall idea is not novel, but the way it is implemented and the proposed constraints are novel. **Significance of this work**:This work is relevant for the field, it incrementally advances the current integration of the temporal and spatial aspects in video. **Typos**:The 3rd vertical line in Table 3 should be shifted with 1 column.<BRK>#### SummaryIn this paper, the authors extend the self supervised 2D jigsaw puzzle solving idea to 3D for self supervised video representation learning. Second, since the constrained 3D jigsaw is still intractable, they propose four surrogate tasks of the 3D jigsaw: 1) LLCD (detecting largest continuous cuboid), 2) CSPC (3D permutation pattern classification), 3) CLSC (contrastive learning over permuted clips), 4) CCMR (measuring the global continuity of the permuted clips)They evaluate their method s efficacy on the public benchmarks by following linear/finetuning self supervised learning evaluation protocols. #### StrengthsI like the idea of solving a 3D jigsaw puzzle as a pretext spatio temporal learning task. Solving the jigsaw puzzle as a pretext task for representation learning is already explored and shown to be effective both in 2D spatial for images [Noroozi & Favaro, ECCV 2016] and 1D temporal dimension for videos [Xu et al., CVPR2019]. Nevertheless, due to the problem s intractability, there is no such prior work on solving a jigsaw puzzle for video representation learning. #### Weaknesses and suggestionsHowever, I have several concerns about the work. The authors show an example of grouped permutation: {12345678}  > {84567123}. It seems that the groups are {123}, {4567}, {8} from the original sequence. I suggest the authors provide more details on how they group the pieces. In contrast to the 2D jigsaw [Noroozi & Favaro, ECCV 2016] and 1D jigsaw [Xu et al., CVPR2019], the backbone encoder in this work takes the shuffled clips with artificial patterns (see the Fig.1(c).There are vertical and horizontal lines). It is unlikely to see these artificial patterns in the downstream tasks. There is a training testing mismatch. Finetuning might fix the problem, but it is not guaranteed. I want to listen to the authors  opinions on this issue. I list the missing analyses below. 1.Why the number of largest continuous cuboids are two? What happens if it is one, three, or four? It would be informative to show the performance when we remove this part. Also, I am not quite sure why FPN is used only for LCCD. What happens if we do not use FPN for LCCD? I will increase my rating if the majority of concerns are resolved.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 7. <BRK>To address the oversmoothing problem and reduce the computational cost of GNNs, this paper proposes to train deep GNNs with shallow subgraph samplers. Thus, it would be great to have experiments to bridge the gap between the theory and the practice. For example,  training a SGC (Wu et al., 2019) model with more than 20 layers to show the effectiveness of  Shadow GNN would be interesting. It would be great if there is a comparison of the inference time on the whole graph. In ICML.Post rebuttal Comments: Thank the authors for the response. * Experiments do not fully justify the superiority of the proposed method in training deep GNNs.<BRK>This paper proposed a simple technique to improve the performance of message passing neural network models. The experimental results show the competitive results of the proposed method while preventing the over smoothing issue. Below, I listed some of my concerns about the paper:  Theorem 3.3 seems incorrect. It would be interesting to see how the performance changes as one increase the depth of layers. Consistency between table1 and table 2: In table 1 2 hop sampler is used to show the best performance while in the ablation study 1 hop sampler is used.<BRK>The paper proposes a simple but interesting new graph sampling method for graph neural networks, called “deep GNN, shallow sampler”. In the experiments, the authors show the computation cost of each target node. 3.One of the main advantages of graph sampling is its efficiency. In general, I think this paper has concentrated contributions and it could be impactful in practical use, but I still have some concern.<BRK>It addresses the two main reasons that GNNs have not previously been extended to deep GNNs: expressivity and computational cost. Is there a performance cost tradeoff for the subgraph ensemble setting suggested by the paper?
Accept (Poster). rating score: 7. rating score: 6. rating score: 4. <BRK>## Paper SummaryThis paper aims to improve our understanding of BatchNorm s positive effects on training and generalization by identifying its constituent effects, and then replicating those effects with simpler techniques. Through a series of experiments using these simpler techniques, the authors show that perhaps the most important mechanism by which BatchNorm improves training, and especially generalization, is by preventing excessive growth of the activations of the layer before the output layer. ## Review SummaryDespite the weakness above, I think the results are sufficiently interesting to be published and evaluated by the wider community.<BRK>It proposes the point that the BN s effect is connected with the regularizing against explosive growth in the final layer. To motivate this point, it takes a single layer case and shows the BN approximately penalizes on the norm of the feature embedding thereon. The studied direction is very important and would bring some new understanding to the ICLR community. Motivated by the simple case and connections with the previous techniques, like dropout, fixup, standardizing are really good points. The reviewer can see clear differences and contributions to the literature. So the results and conclusions tend to be general. Overall the paper is easy to follow. (The reviewer guesses the experiments of Figure 3 is not conducted on ImageNet.) To see if the proposed regularization can help training even without proper initialization, like the last layer BN experiments did in Sec.4.4.<BRK>The proclaimed two possible effects are1. standardizing the intermediate activations2. regularizing against explosive growth in the final layer. This leads me to think that the results from directly penalizing the effects of batchnorm are unconvincing. 2.One of the benefit of batchnorm is that it allows for a significantly larger learning rate. Although the paper is interesting, I do not find the results to be very convincing (please see "questions" section for clarification)** Pros1 Well written and well organized.
Reject. rating score: 1. rating score: 2. rating score: 2. rating score: 3. <BRK>As described below, I think there are multiple serious flaws in the evaluation and the defense likely won’t work. Issues with the paper:* Experiment section is lacking rigor which is necessary to properly evaluate defense against adversarial examples. Nevertheless authors do not address why the proposed defense (which is also input transformation) is not broken. * Use https://arxiv.org/abs/1902.06705 as a guide on how to properly evaluate models for adversarial robustness. And redo evaluation following this guide.<BRK>There is little theoretical reasoning for why this is a sensible defense. As has been argued again and again in the literature and in community guidelines such as [1, 2], the oblivious threat model is trivial and yields absolutely no insights into the effectiveness of a defense (e.g.you can just manipulate the backpropagated gradient in random ways to prevent any gradient based attack from finding adversarial perturbations). The paper also fails to point out that Pang et al.2020, one of the methods they combine their method with, has been shown to be ineffective [2]. Fixing this problem will require developing adaptive attacks that are effective against the proposed defense.<BRK>Even in the experiments section, only oblivious attacks which are unaware of the defense mechanism are considered. Apart from this, it must be assumed that all other details related to the defense are known to the attacker. This is because an attacker is assumed to be infinitely thorough, without any computational restrictions, towards finding an adversarial perturbation within the threat model if it exists. While Carlini & Wagner define the Zero Knowledge threat model in their earlier paper [2], they clarify in their subsequent work [1] that there is no justification for such a threat model, and it was defined only to highlight that some defenses were not even robust against such a weak threat model. All the baselines and methods that the proposed defense is combined with, are broken in prior work.<BRK>The author proposed a input transformation method for countering adversarial attacks. However, I have some strong concerns regarding the evaluation method of this paper:  The proposed method is based on the principle of obfuscated gradient, which has shown to be vulnerable against adaptive attacks [1]. As pointed out in [1], defenses based on obfuscated gradient may still suffer from gradient free attacks. have already been shown that are not effective [1]. I m not sure how similar is [4] to the proposed method but the author should discuss the difference in the related work. "Black box Adversarial Attacks with Limited Queries and Information."
Reject. rating score: 3. rating score: 4. rating score: 6. rating score: 6. <BRK>Or do the authors mean zero shot in the sense that the safe agent is trained on a different set of environments and the online version means they re trained on the same environment? The proposed algorithm is competitive across the experiments presented in the paper.<BRK>Otherwise, it is difficult for readers to understand the difficulties and the usefulness of these tasks. The action distribution P_Z_\psi would be concentrated on the zero action. This might explain why the paper observes that the hyperparameter \beta is difficult to tune.<BRK>My responses to the authors  comments are in the respective threads. Results look promising, and the paper does a great job at presenting the work.<BRK>The paper aims to reduce the unwanted side effects of the actions of a reward maximizing reinforcement learning (RL) agent. The work s proposed solution trains an agent who focuses on the total reward and another agent who minimizes the total side effects. In my opinion, Section 3 would read better if the authors append a preliminaries section wherein they establish the notations and the descriptions of the task agent and the virtual safe agent. * The empirical results suggest that the proposed method is effective.
Reject. rating score: 2. rating score: 2. rating score: 2. rating score: 3. rating score: 4. <BRK>The link between Eq.(4) and their algorithm is not clear. Many parts of the paper are poorly described.<BRK>This is unfortunate as there are opportunities to shorten less relevant parts in favor of a derivaiton of the equations. The presentation needs improvement.<BRK>In short, the paper is not properly written nor well organized; is hard to read with vague contributions and vague positioning with respect to the state of the art. Experiments are not convincing: Toy experiment and minimum experiments in MNIST without a clear comparison to existing neuron pruning algorithms.<BRK>If this is a concurrent submission to ICLR, the author should still cite it anonymously. The experiments are not convincing enough.<BRK>It would be better to zoom in the relevant portion of the plot. This is a significant omission.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 4. <BRK>##########################################################################Summary:The paper develops a method to select a radar return region to be sampled at a higher rate based on a previous camera image and radar recording. While it is generally a great idea to guide the selection of radar regions to be sampled at a higher rate the paper is very application focused and lacks novelty in its method. The result that camera and radar data combined will outperform camera data only is expected.<BRK>Summary:In the paper, the author(s) propose 1) a method to dynamically adjust the sampling rate on radar data using 2D object detections (algo1) and previous image and radar data (algo2); 2) an end to end transformer based 2D object detection model using both radar and image data. It is somewhat hack y to me and hurts the novelty of this paper. 3.Need more analysis on the latency. Would this affect the performance? I think it would be good if you can also test the detection performance using image + CS radar. 5.Could you provide the motivation of using the transformer rather than faster RCNN on detection? 1.Cons 1: The rebuttal does not provide sufficient explanation on how the magic numbers are chosen (though it is somewhat explained in the rebuttal, it looks more like a design and lacks experiments to back it up: why these numbers but not other numbers?)<BRK>The authors claim that the purpose of detecting the "important regions" in the radar domain is to implement compressed sensing during radar data acquisition. The innovation is limited for both Algorithm 1 and 2. The experiments are not adequate to illustrate the performance. It s not clear that the dataset used in this paper. Overall, I think the paper is not good enough to be accepted by ICLR.<BRK>The paper proposes two different algorithms to solve this problem. It is not clear when one should be preferred over the other. It seems that the second algorithm is an improvement over the first. The second includes the radar data and improves on the first one. There are also a lot of ad hoc choices that are not justified. While the paper might have some nuggets of interesting ideas, these are buried in the detail of all these design choices of the authors.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 4. <BRK>This paper defines a new scoring function for efficiently selecting between different network architectures, without requiring the expensive training of the models. The claims that “GT NAS is universal to different architectures” and that it is “universal to different initialization approaches” seem way too strong, given the limited empirical evidence. It seems the central argument of the paper is that there is a strong positive correlation between the proposed metric and the final test accuracy of the model.<BRK>This paper shared some interesting observations about gradient values and correlations and utilized these findings in the context of neural architecture search to improve the search efficiency. The proposed approach is a kind training free NAS method, which is an interesting direction in NAS. Cons:  The presentation should be improved. The proposed method needs to sample s models and train k models. Coould the authors comment on that?<BRK>This paper pointed out that both gradient correlations and gradient values have strong impacts on model training, which is an insightful finding. Based on this finding, they explore a simple yet effective network architecture search (NAS) approach. The new approach replaces the expensive “train then test” evaluation paradigm with a new lightweight function according to the gradient based kernel at initialization. Concerns:The key concern about this paper is the lack of rigorous experimentation to study the usefulness of the proposed method. However, I did not see such an experiment in this paper.<BRK>This paper consists of two parts, where it first studies the difficulty of neural network training, then proposes an efficient criterion for neural architecture search (NAS). The results (both absolute and relative to baseline ones) need to be much improved. cons: The first part of the paper is intended to motivate the value of the gradient correlation, but it does not do a great job IMO.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 6. <BRK>This paper concerns the problem of learning video representation from paired video text pairs. The proposed framework is weakly supervised as the text associated with videos comes from user provided YouTube titles or Instagram captions. The proposed method uses standard visual encoder and textual encoder and similarity measurement for the joint embedding space. i) Some of the claims are not well substantiated. For example, on page one bottom, the paper claims that existing works learn "high level visual text embedding" as opposed to "video representation" as in this work, which is not true considering papers such as Miech et al., 2020. Another notable claim in the paper is that "[...] Our work demonstrates that pre training a relatively small video text dataset is also possible to match the STOA performance [...]", which is somewhat over stated given that the pre training data is curated on a quite restricted domain (e.g., human actions) and sometimes containing manually curated data, and could intuitively benefit downstream tasks about the same domain (all tested datasets fall into this category) vs. instructional or more generic pre training data.<BRK>The paper proposes a weakly supervised method for learning spatiotemporal features by video and text pair discrimination, namely cross modal pair discrimination (CPD). But at the same time, the weakly supervision setup make it less significant compared to the original work. The right one to compare with CPD may be Ghadiyaram et al.CVPR 19.The author(s) may argue that this work used a lot more data than CPD. That is true, but they can train a baseline network, e.g.same backbone as CPD, with a cross entropy loss on Kinetics title clean and Instagram 300k using title or search query as label (similar to Ghadiyaram et al.CVPR 19).This is a fair and important baseline to understand the proposed method of CPD. *Some minor comments:  abstract: pre training a relatively small dataset  > pre training on a relatively small dataset  introduction:  So these expected these associated  > So these associated  missing i, j indices for f^u, f^t in Eq(1)?<BRK>2.It’s interesting to utilize text as weak supervision for video representation learning, and the experiment results also indicate effectiveness of the learned video representation. The novelty of this paper might be limited. Previous works have explored the possibility of utilizing text as weak supervision for video representation learning (MIL NCE), from the reviewer’s perspective, the main difference is that the different loss function is adopted. 2.Compared with methods that adopt other information (such as audio) as weak supervision, there is an inherent advantage of using text as supervision since pretrained text models such as BERT can be utilize as a guidance. So a meaningful comparison would be the comparison with TWS and MIL NCE, although the proposed method can achieve comparable performance with other methods with much less data, the author does not give analysis about what design in the proposed method that enables this. 3.The performance comparison is not convincing enough.<BRK>*Summary:*The paper proposes an approach to learn a video feature backbone in an unsupervised manner through the use of video titles (text modality) associated with user generated content from Youtube or Instagram. Contrary to previous works in this direction that require millions to hundreds of millions of paired clips, this work shows that good performance can be achieved by using much fewer (on the order of 100k) clips. Shows that videos and their titles can be used to learn video features from scratch in an unsupervised manner. Novelty of the method itself (not the task to which it is applied) seems fairly limited. An additional complexity is with respect to the memory bank, but I believe this is only used for improving the estimation of the denominator in Eq.3, right?2.Experimental details:(i) Some statistics about the datasets: Kinetics 210K and Instagram 300K would be nice to include. (v) Since a major emphasis of the paper is on the smaller size of the data that allows learning similar performing representations, it would be nice to include additional details such as time taken to complete entire training on the 8 GPUs mentioned and perhaps an estimated comparison to larger datasets. Would request authors to please tone this down.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 6. <BRK>The approximation also raises concerns regarding the convergence of the algorithm. Overall, I think the approach is promising, but some justification of the quality of the approximation is needed. ##############The authors propose a streaming approach to tensor factorization with Bayesian neural networks. The proposed approach combines a Bayesian neural network (BNN) whose output predicts the entries of the tensor and the streaming variational Bayes (SVB) for incremental posterior updates.<BRK>This paper proposes a probabilistic tensor factorization model for streaming data. The model uses:* neural networks to learn richer factors,* spike and slab prior on NN weights* and online moment matching for posterior inference. What is the trade off between performance accuracy and computational complexity of the proposed framework?<BRK>Although there has been some work on streaming or online tensor factorization, the (Bayesian) deep factorization has never been addressed in the streaming setting, which is also quite an important problem to study. ##########################Summary:This paper proposes a streaming probabilistic deep tensor factorization model, called SPIDER, to solve the tensor factorization problem under streaming setting with deep factorization parameterized by Bayesian neural networks. To encourage sparsity and prevent overfitting, a spike and slab prior is used for the weights of the neural network layers. Does the algorithm converge?<BRK>##################################################Summary:The paper proposes a Bayesian neural network model for tensor factorization, with particular focus on streaming data. The main computational challenge is how to handle the streaming data in the posterior inference steps. 1.There are many (usually first order) approximation steps in the inference procedure.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. rating score: 5. <BRK>The organization of the paper is confusing, some effort should be given at creating a clearer layout that makes the paper easier to read and follow the flow of ideas.<BRK>The authors presented a comprehensive study on the role of background in image classification. The paper is well written, and the figures and tables are clearly presented.<BRK>Does this imply that the model might be learning shape features as it is doing better than random? 3.Image classification models have higher accuracy tends to depend on the background image less. The paper provides a detailed quantitative analysis of the effects of using different backgrounds (both training and testing).<BRK>Am I misreading this table? ( ) The takeaways are mostly already recognised from the many works that have pointed out reliance of object recognition models on backgrounds. Could there be a way to detect such backgrounds?
Accept (Spotlight). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>SummaryThis paper proposes an interpretation method for graph neural networks (GNNs) by learning parameterized, differentiable edge masks. Strengths  The paper proposes a novel interpretation method for GNNs. The method is technically interesting and well motivated. The discussions for the QA and SRL models are thorough and interesting and provide useful insights. The paper is well written overall. It would be helpful to add more explanations/definitions so that the paper is more self contained.<BRK>The gates can be trained together with the model in a fully differentiable way. At the same time, the remaining edges could be used for interpreting model predictions. This paper introduces a simple yet viable method to deal with it. 2.The experiments are well designed including both qualitative analysis, quantitative results, and reasonable ablation to show the effectiveness of their method. I really like the synthetic experiment designed in the paper which distinguishes the explanation proposed in the paper from others to be faithful. However, from my point of view, the task introduced here is too simple to say that such kind of faithfulness could be generalized to the real setting.<BRK>The authors of the paper set out to design a method for explaining the behaviour of graph neural networks. They are one possible way. One weakness of the proposed approach compared to prior work on explaining GNNs seems to be that it can only mask edges and not features. Overall I think this is a well written and executed paper. The missing discussion of amortization is especially problematic, as it seems to be the decisive factor in the proposed method s performance.<BRK>SummaryThis paper presents a new interpreting algorithm for understanding graph neural network (GNN) models for natural language processing (NLP) tasks. The main idea is to remove redundant edges for each layer of GNN after training the model. Experiment with synthetic datasets shows that suggested algorithms can find the important edges for the task rather than others that find only part of the important edges. But it is hard to imagine the amortized and non amortized algorithms in the experiment sections. Overall, I admit that the suggested algorithm is a sound method to increase the interpretability of GNN models. I know that the goal of masking is interpretability, not performance.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. rating score: 4. <BRK>This paper studies a very interesting new problem of assessing unrolled models in a broader context using NAS methods. I would request to see comparison results from (1) adding dense connections only in the latter half layers; and (2) adding random connections, but with the same total connected percentage as the LISTA pattern. I think bringing NAS to learning based algorithm design is a very novel and interesting direction. The paper is also well written, and the logic is very clear to follow. Perhaps, that implies some general idea of how dense connectivity could be properly injected in a sparse regression task.<BRK>This paper studied the structure designing problem for a class of neural networks, i.e., the unrolling networks, or more specifically the LISTA model. The studied problem is interesting and important, and the conducted experiments reveals some insights of unrolling based deep networks, which provides some useful guidelines for further research in this direction. The paper is also well written and easy to follow. A drawback, concerning about the experiments, of the manuscript is that it lacks applications.<BRK>This paper conducts an empirical study on unrolling architecture design, by applying NAS to search the connectivity pattern based on LISTA, and compares the searched model to the original LISTA. Pros:+ The paper is clearly written and not hard to follow+ Experiments show that the searched architecture performs better than LISTA+ The experiments are comprehensive, including various experimental settings that can compare the unrolled architectures from different aspectsCons:  Technically, it is a direct application of NAS to LISTA, so the methodological contribution is not very strongThrough extensive experiments, this paper shows the effectiveness of NAS for searching for a better architecture based on unrolled algorithms, via a case study on LISTA. Unlike what I initially commented,  my score is actually between weak acceptance and weak rejection , now I am happy to rate this paper as a weak acceptance.<BRK>This idea is interesting. Weaknesses:  I think that the overall conclusions of the paper are not very significant. The authors only focus on a very simple problem : the lasso on synthetic data (with known dictionaries). I am not convinced by the results regarding transferability. What would be the results of the NAS in the setting with unknown dictionary, how would unrolled models perform in that setting ? I think it would be relevant to compare the unrolled models to other algorithms for solving the lasso (lars...).
Reject. rating score: 3. rating score: 5. rating score: 6. rating score: 6. <BRK> Update after rebuttal period The connection between the contrastive learning objective and discriminative learning is made via "resemblance". And the author claims the "resemblance" as a theoretical contribution, which the first reason I vote for a clear rejection. This issue has not been addressed by the authors. The content is not organized well and some ambiguous wordings should be avoided. This paper connected contrastive learning and supervised learning from the perspective of the energy based models. Strengths: The paper attempts to connect supervised and contrastive learning. The author claims that the proposed method is performed on adversarial modeling and generative modeling, while these two sections only appear in the Appendix. The main contribution of the paper by connecting supervised learning and contrastive learning is overclaimed.<BRK>In order for this paper to reach the standard of ICLR, I believe that the authors must revise the strength of their work and redesign their experiments as such. 2, Strengths and Weaknesses:The proposed method of training energy based models achieves good performance on various tasks (e.g.OOD) without SGLD which usually requires a lot of tricks to work well on high dimensional data. The sheer results look promising, and it seems that the method is succeeding in capturing some aspects of the data distribution. This is the weakness of the paper.<BRK>The paper proposes HDGE   a simple method to improve over JEM. The idea is to approximate the normalization constant $Z(\theta)$ with an empirical averaging of energy functions over a large memory bank. The benefit of using such an approximation is that this eliminates the need for running SGLD, thereby improving the stability of training. I would like the authors to have a discussion on the training stability of HDGE compared to JEM. So, it is important to perform multiple runs and report mean and standard deviations to understand the statistical significance of the results.<BRK>Summary  Paper proposes Hybrid Discriminative Generative training of Energy based models (HDGE) which combines supervised and generative modeling by using a contrastive approximation of the energy based loss  Approach shows this is better than baselines on various tasks like confidence calibration, OOD detection, robustness and classification accuracyClarity  Overall well written paper. The details are not presented.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 6. <BRK>The authors also propose a workflow for using the proposed tools. The approach follows an interesting direction towards explaining RL agents, but I am missing concrete design decisions and empirical evaluations for the proposed set of visualizations. I am aware that other explainable RL approaches based on counterfactuals or attentions might be easier to evaluate (and might not require end users for acceptance at a conference), but I still feel a deeper evaluation is necessary here. To this end, I am wondering it which situations the tool would be beneficial over other explainable RL approaches. Otherwise I find it hard to tell to what extent / in which situations / for which user group the tool is useful. While I appreciate that the authors made changes to their manuscript based on the reviewers  comments, I thus keep my recommendation for rejection for the current version of the paper.<BRK>It also contributes a set of views for visualising RL training data that could be used in general. The paper does include 3 extremely brief examples of how the tool might be used. However, it does not include any experiments to suggest that this tool would actually improve the debugging process for training RL in a real user study. The manuscript contains references to several relevant publications that can be compared to the current work. However, the paper is also missing references to many related and relevant works in the space of debugging reinforcement learning using visualisations, especially with an eye towards explainable reinforcement learning. See references below (also some in the submitted paper).<BRK>This paper describes a new interactive visualization tool for better debugging of RL algorithms. Various viewports under the two different views are described in details. This tool is definitely useful to many RL researchers because debugging RL algorithms is known to be more difficult than traditional ML algorithms. However, I feel that the paper has not yet sufficiently demonstrated the proposed tool’s full value. Is there any mechanism for the user to jointly operate several viewports (e.g., state, action, reward) together? This way, we will have a better idea on when the tool is most effective and how the different components contribute to the values created.<BRK>However, the paper is too much focused on the side of showing what can be visualized by the users interactively, and it is missing more technical information about it. Does the tool bring a library of algorithms and environments? Figures need to be more connected with text in the paragraphs, some are not mentioned and some are mentioned very far from its place. In temporal views it could be also possible to visualize value functions, advantage functions, returns, and also the cost functions computed for training the models (NNs). and thus design reward functions that are immune to this problem. Is it useful for doing experiments with real physical systems?
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 4. <BRK>My main concerns are in the theoretical results: in the paper the authors argues the following: standard MSGD has convergence of O(/1/sqrt(C) + B^2/C), while SNGM, with B sqrt(C), has convergence rate of O(1/C^(1/4)). It seems to me it is unfair comparison as one has O(1/C^(1/2)) rate and the other has O(1/C^(1/4)) rate.<BRK>###################################################################Minor Comments that did not Impact the Score:1. First, the theoretical contributions of the paper are only small extensions from existing works and not significant.<BRK>I quickly went over the proof and did not find obvious flaws. I like the style of this paper.<BRK>As such, none of the components are new. The theoretical results are not very surprising, and the techniques used are not new. The paper successfully fills this gap.
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. rating score: 6. <BRK>This paper proposes an analysis of the benefits when using counterfactually augmented data (CAD) for classification purposes. They focus on the problem of spurious correlations and how they affect out domain generalization. These qualitative insights as well as framing the problem in a causal model are the main contribution of the paper. Framing the problem, even if considering simplified models, is a critical step. I appreciate the dual causal model that is studied (causal and "anticausal"), not limiting the analysis to the most common interpretation. The paper will be more informative with a discussion about the noise model in the case of the noisy proxy (pg.4).Is this the most probable noise model? The paper presents a large series of experiments to support the proposed hypothesis.<BRK>This paper presents a perspective on counterfactually augmented data (CAD) and experiments providing evidence that CAD elicited from humans indeed bears resemblance to interventions on labels or variables that are causally related to the labels. Section 3 shows that noise in the causal variable increases a learned model’s dependence on a confound, whereas noise in a confound does not affect the model in the absence of noise on the causal variable. More weight given to the confound will hurt out of domain generalization by the model. ### StrengthsThe paper has a fairly clear logical flow and provides clear theoretical and empirical components which are connected well. Lots of relevant work is brought together and a good case is made for the importance of the questions addressed by this paper. The contribution is a tad narrow, but it makes progress on some tricky and difficult questions. But the experimental framework gives what seems to be pretty good evidence that other automated methods *don’t* implicate causal variables, so that’s nice. The paper seems sound, is well written, and addresses an important problem. It makes a particular claim about “language meaning” which implicitly views meaning as corresponding to the causal connection between input language and output labels.<BRK>There is some added discussion, which is helpful, but actually sketching out and conducting experiments, including intermediary steps moving from a full toy example to the naturalistic case, could be very helpful here. # SummaryThis paper studies the impact of counter factually augmented data on out of domain generalisation. (eq 3) Differentiate between the noise in the data generation (eq 3 terms) and the measurement noise mentioned right after. The paper studies an important question of how models may rely on causal and non causal features, and how this impacts out of domain generalisation. This is consistent with the sentiment case. 2.That said, I think the emphasis on counterfactually augmented data is not conducive to the arguments and experiments in the paper. 4.There are a lot of experiments and results, but the presentation and discussion of them are hard to follow. Some tables are mentioned in the main text but appear in the appendix and it s not clear that the reference is to an appendix table.<BRK>I believe the paper leads research in an important direction and so can be published despite these flaws. Summary:The paper investigates the issue of counterfactual data augmentation (CDA), i.e., the process by which humans augment a model s training data by making minimal edits necessary in order to flip the label. I think a good theoretical contribution requires some discussion along these lines. That result alone seems unsurprising. This paper makes two contributions. * First, the authors attempt to analyze CDA through the lens of causal inference, looking at a toy setting of Gaussian OLS. The takeaway is that noising non causal features should lead a model to rely more on causal features (and thus generalize better), whereas noising the causal features should lead the model to rely on spurious features and thus perform worse overall. The idea of analyzing CDA in terms of causal graphs is exciting, but its not fully fleshed out here. I don t fault the authors for beginning with a simple/toy setting, but I do think they owe the readers more discussion of the implications and limitations of this setting (see my questions below) in order for the theoretical results to be useful. I also found the tie between the empirical results and the theoretical ones to be tenuous the empirical results are unsurprising (they can be paraphrased as "adding noise to important features causes models to perform worse") and could probably be explained by a number of theoretical frameworks.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>This work suggests a variant of ensembling that is more compute efficient. Specifically, it involves forking an ensemble only in the late stage of training, and forming this ensemble via a "low dimentional" family. I weakly recommend acceptance, because the method appears promising for future work, and the experiments seem correct. The OOD uncertainty results could be expanded. Uncertainty estimation and robustness are some of the most relevant practical uses of ensemble methods, so it is especially important to evaluate ensembles in this context.<BRK>### SummaryThe authors propose late phase weights, a method of updating the weights near the end of training via a splitting and ensembling mechanism. * I think there should be more discussion on the choice of $T_0$. ### Recommendation / JustificationI vote to accept the paper. The idea is interesting, well motivated, and seems straightforward to incorporate into existing pipelines. However, the improvements seems modest in some settings (e.g.ImageNet) and for the best performance, it seems like we should still stick to Deep Ensembles. Would this help performance? This is different from when it is initially described.<BRK>Summary:The paper proposes a method to improve solutions found by SGD by ensembling subsets of weights in late phase. Pros:The paper is clearly written and easy to understand the proposed method is. While all experiments show that the proposed method improves the baseline somewhat, deep ensemble baselines remain strong. I find some discussion on the appendix but seem to be missing in the main text. I thank the authors for their hard work addressing issues raised by the reviewers.<BRK>To improve the generalization performance of SGD methods, this paper proposes to use an efficient ensemble like approach which computes an average of an ensemble of SGD weightswhen retrained from some late phase of SGD dynamics. It is written in Section 2.1 that in practice … sigma0>0 yields a set of models …this results in improved final generalization. ##The revisions made by the authors have addressed all my concerns. I find that this approach is quite sensitive the choice of the hyper parameters, such as the beginning of the late phase T0, and the noise perturbation sigma0.
Reject. rating score: 3. rating score: 5. rating score: 6. rating score: 7. <BRK>The potential insight I see is proposal neglect. 4.RecommendationI think in it s current form this paper has to be rejected. So what can I recommend the authors to do with the paper? 5.Questions/Suggestions  Provide quantitative analysis of the "proposal neglect" problem.<BRK>It seems that the sign of the determinant shouldn t matter. Table 3 might actually not be the correct way to assess the "proposal neglect" (or resolving proposal neglect). While I still like the overall idea of this paper and I believe this is an interesting direction of research, the additional results in the revised paper actually raise more questions and there are issues that need to be addressed in this current study.<BRK>I am a bit lukewarm about the paper in general. Positives  The paper is easy to follow and proposes a simple idea of improving the RPN classifiers so that any proposal with high IOU with the object is not pruned out early due to the RPN. Why does what is being proposed to solve this problem of proposal neglect the right way to do so?<BRK>The results reach state of the art on some cases; especially with very few shot domain. The topic is relevant, and not too widely studied.
Accept (Spotlight). rating score: 8. rating score: 7. rating score: 7. rating score: 6. <BRK>### SummaryThe generalization ability of networks with zero training error has been heavily studied. This paper extends beyond generalization to test sets to study the network s robustness to adversarial examples. The paper provides two theoretical contributions demonstrating that a very low training error can indicate poor robustness under reasonable conditions. They illustrate this with experiments using label noise, demonstrating that adversarially robust networks spurn overfitting on incorrectly labelled data. ### SignificanceThe generalization properties of neural networks and adversarial robustness are two very fast moving areas of machine learning.<BRK>The main contribution of this paper as I see it is in pointing out that label noise can negatively affect the adversarial robustness of interpolating predictors, even when the standard 0 1 error is small. The paper also argues that adversarial training techniques avoid memorizing noisy labeled examples and  rare examples which partly explains why adversarial training incurs higher standard 0 1 error.<BRK>The goal of the paper is to investigate both theoretically and empirically the reasons of vulnerability of overparameterized classifiers obtained by the so called “benign overfitting”. More precisely, two causes of adversarial vulnerability are underlined: label noise memorization and sub optimal representation learning. Namely, while for a good representation one may have “training error   test error   adversarial error   0”, for another representation it holds that “training error   test error   0” but “adversarial error > 0.1”.<BRK>Both the training and test error will be small (since kNN averages the labels of neighbours and the label noise is small), but the adversarial error is large (with a high probability, there will be a small region in each sphere that is predicted differently by kNN). The authors claim that one primary source of the lack of adversarial robustness is label noise. There are a few typos in the paper:	* Abstract: “partsub “optimal —> part sub optimal 	* Page 4: “Thus, smaller the value of” —> “Thus, the smaller the value of”. For example, all ResNet18, DenseNet121, and VGG19 trained on CIFAR10 have a large adversarial error even with zero label noise! This would correspond to about 2% of CIFAR10 training examples.
Reject. rating score: 5. rating score: 6. rating score: 7. rating score: 7. <BRK>Finding the optimal scheduling (assigning robots to tasks) under reasonable assignment constraints poses an NP hard problem. The paper casts this problem as a mean field inference over random probabilistic graphical models (PGMs) and proposes a two step hierarchical inference method that employs Q function estimation for a graph neural network (GNN) representation of MRRC. 3.The figures in the paper are very illustrative. My major concerns regarding the paper s clarity and the implications of its theoretical results still hold. 4.The authors have made the code available. It seems that understanding the paper heavily relies on another paper (Dai et al.2016).2.It is hard to understand the theoretical contribution of the paper, e.g., how the local optimality condition in Theorem 1 translates into the overall performance. Also, it is unclear to me whether the order transferability is guaranteed or not. 4.The proof of Theorem 2 resembles that of the near optimality result for a greedy algorithm that aims to maximize a submodular function under a cardinality constraint. 2.The order transferability is only tested for the deterministic environment with a linear reward.<BRK>This paper studies a class of problems called the Multi Robot Reward Collection problems. These are scheduling problems that are combinatorial in nature and hard to solve efficiently (in polynomial time). Traditionally, such hard problems have been solved with approximation algorithms or heuristics. On the other hand, heuristics are easy to develop however the solution has no performance guarantees. The presented method takes inspiration from structure2vec by Dai et al.(2017), which uses GNNs to compute solutions to several well known NP hard problems including the vertex cover and traveling salesperson problems (TSP). This paper presents a method to embed graph environments to select actions. Overall, I think the method will be interesting to the community and the paper is written well except for the points listed below:     The $(1   1/e)$ optimality bound: The approximation factor presented in Theorem 2 is obtained by the greedy maximization over the Q values. This is computed using the bound on diminishing returns with greedy selection, as presented previously by Nemhauser et al.(1978).Although this result can be used to claim approximation of the optimal Q values, it is not clear to me how it would translate to performance comparison against the optimal algorithm’s solution. Can multiple robots occupy the same node of a graph? Is there a notion of traffic in the flow of the graph? Learning combinatorial optimization algorithms over graphs.<BRK>This paper considers multi robot, multi task scheduling problems. By introducing the notion of random PGM, authors develop a mean field inference method and further prove that a modification of a GNN embedding is sufficient to embed a random graph. Experimental results show the near optimality of the proposed method. By doing so, the MRRC problem can be solved using random structure2vec with a nearly optimal solution. Overall, this paper is well written. The idea of treating the multi robot multi task problem as a sequential decision making problem with directed bipartite graphs is novel. I only have a minor concern regarding the stochastic task completion time. In real cases, it is hard to calculate the exact task completion distribution when tasks are more complex. There is no discussion about this in the paper.<BRK>The paper presents how to embed a random graph using GNN and shows how  to use this to solve NP hard scheduling problems for multiple robots. Each state of a Multi Robot Reward Collection problem is represented as a random probabilistic graphical model. Then random structure2vec is  used to design a reinforcement learning method that learns near optimal NP hard multi robot scheduling problems with time dependent rewards. The work makes a significant contribution to solve a hard problem that has many real applications in manufacturing, ride sharing, pickup and delivery, etc. 3.The results presented are built on theoretical results. 2.The  use of synthetic data for the experimental results is reasonable, but it would have been interesting to use existing data sets (for instance, data sets for vehicle routing problems with temporal constraints or data sets for job shop scheduling) even if not exactly for the same problem to enable comparison and duplication of results.
Reject. rating score: 4. rating score: 5. rating score: 7. <BRK>The paper considers learning Boolean functions represented by read once DNFs by using neural networks. Given a whole 2^D instances of some read once DNF, the authors showed that (1) weights corresponds to the true DNF is the global minimum of the loss minimization problem with the network, (2) they empirically observe that gradient descent with a rounding heuristics finds the true DNF expression, and(3) the solution of a 2 norm minimization recovers the true DNF. It would be much nicer, given a partial set of instances S \subset X, one can learn a consistent DNF by using neural networks. So, I am afraid that it should not be stated as a theorem. As a summary, I feel that the technical results are still preliminary and not mature enough to be published.<BRK>This paper investigates the problem of learning monotone read once DNF formulas using convex neural networks. The main contribution of this study is essentially empirical: convex neural nets, trained with GD for minimizing the cumulative hinge loss, converge to global minima for which neural units coincide with the monomials of the target DNF. This remarkable stability is corroborated by theoretical insights about global minima. I don’t think that this restriction has a major impact on the result since read once DNFs are unate (i.e.we can rename negative literals in order to obtain a monotone variant). Next, the learnability result about read once DNF formula should be clarified. In the introduction, it is indicated that this concept class is efficiently PAC learnable under the uniform distribution, by quoting Fiat & Pechyony (2004). So, for the sake of completeness, it would be legitimate to provide such a result (in the Appendix, for example.) On the one hand, as indicated above, the authors empirically demonstrate that convex neural nets are able to learn monotone read once DNF concepts by converging to global minima that capture the target concept. This is indeed interesting in practice, but there is no formal proof that convex neural nets are able to learn “any” monotone read once DNF in polynomial time with polynomial sample complexity. Actually, many subclasses of DNF are known to be efficiently learnable under product distributions, using spectral approaches (see e.g.Feldman, 2012). Learning DNF expressions from Fourier spectrum. Thomas Hancock and Yishay Mansour.<BRK>In this paper the aim in to understand the inductive bias of neural networks learning DNFs. The focus is in convex neural networks and gradient descent. Further, experimental evaluation demonstrates that gradient descent can recover read once DNFs from data. Learning functions over Boolean variables is a fundamental problem and there have been an increasing interest towards using neural networks for this task. This paper sheds light to this task in a very specific case. However, I found the analysis of the inductive bias of gradient decent towards logical formulas interesting. Computer assisted proof and experiments are used to complement theoretical results3. (intro, start of Sec.6) where it is unclear what this property is. 6.3.It could be better to explain this in short in the earlier mentions, because in their current format they do not really help the reader, but rather make them wonder what is this property.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 6. <BRK>This paper proposes a new method for applying the TRUST TECH method to the ensemble of deep neural networks (DNNs). Since it uses more budget, the performance should be higher, but it seems that FGE is not implemented properly. The explanation about this result is missing, making it difficult to trust the paper s experimental results. [+] Propose a method of applying TRUST TECH to find local optimal solutions (LOS) of DNNs\[+] Introduce DSP for exploration in high dimensional parameter space\[+] High ensemble performance through DSP TT[ ] Claims not sufficiently proven or supported\The paper repeatedly claims that the solution found by the proposed method is high quality and diversity.<BRK>The authors propose a method to obtain multiple local optimal solutions around the existing one. The paper is a bit hard to read and would benefit a lot from proofreading. I quite like the idea behind the the algorithm. The algorithm consists of moving away from the local minima along some random direction, but biasing the direction with a true gradient of the loss.<BRK>##########################################################################Summary: The paper describes a technique based on the modified generalized gradient descent for finding multiple high quality local optima of deep neural networks. While I was not entirely satisfied with how the paper is written and how the approach is introduced and explained, I find the results to be quite interesting. While I do not know the literature sufficiently well, I think this method is original and well founded. Certain parts of the publication are not entirely well written and some sentences are a bit confusing. I am not sure these particular papers need to be included in the prior work section, but I do think that this publication would benefit from a more in depth literature overview.<BRK>This paper  proposes an intersting Dynamic Search Path TRUST TECH training method for deep neural nets. In contrast to other global solvers, the proposed method efficiently explores the parameter space in a systematic way. I think the idea is interesting. The experimental results validate the effectiveness of the proposed method.
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 7. <BRK>This paper proposes an improvement over fast adversarial training to improve the robustness of the model in an efficient manner. Overall it s a well written paper but it can be improved in certain ways as follows... If yes, what are the PGD parameters? In most of the experiments, the PGD step size is missing. I think authors use R+FGSM and fast adversarial training interchangeably which creates some confusion.. it will be nice to be consistent with the terminology.<BRK>Summary:This paper proposes a method called: improved fast adversarial training (FastAdv+) which improves fast adversarial training by replacing randomized initialization with PGD adversarial training when there is overfitting issue during training. However, FastAdvW does not have this advantage anymore. When use the FastAdv+ result as starting point then do PGD adversarial training, it outperforms PGD adversarial training. 2.The advantage of Fast adversarial training is that it is less computational expensive than PGD adversarial training so it can scale up to large dataset like ImageNet.<BRK>The authors conducted a series of experiments to figure out the key to the success and properties of fast adversarial training. The experimental results showed that fast adversarial training cannot avoid catastrophic overfitting, but could be able to recover from catastrophic overfitting quickly. Based on all of the observations, the authors proposed a simple method to improve fast adversarial training by using PGD attack as training instead of R+FGSM attack (proposed in fast adversarial training) when overfitting happens, or using fast adversarial training as a warmup. #####################################################################Overall, I vote for weak reject (marginally). Pros:#####################################################################Pros:  1.Attempting to interpret the successful reason for a previous work is interesting. Does it mean that stronger attacks could mitigate it?<BRK>################################## Summary ##################################This paper shows that the main reason for the success of Fast Adversarial Training ([1], will be referred to as FBF in this review) is its ability to recover from catastrophic overfitting. The paper presents insightful findings about FBF, and proposes a simple and effective method for stabilizing single step adversarial training. Further, the authors also propose to use this improved Fast Adversarial Training (FastAdv+) as a warmup for PGD Adversarial Training, and demonstrate improved performance over PGD AT, at a significantly lower computational cost. Although the gain in robustness is marginal, stabilizing single step training is useful. The authors also propose a computationally efficient method of achieving robustness similar to PGD training. It would be insightful to study the properties of this intermediate model that suffers from catastrophic overfitting, to understand more about why it is able to recover so quickly.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 6. <BRK>## SummaryThe authors present "HyperDynamics", a novel method for system identification and learning of flexible forward models that can be used in planning tasks. The presented methods is generic and is shown on both locomotion and pushing tasks with different simulated robots. I would recommend the following changes: (a) trim this severely, only mention that there are model based methods that usually do only one environment and there s meta learning and how your method is more adaptable than either, (b) move this into the literature section, where you have to come back to it anyway, (c) move the Hypernetworks section from the literature into a separate "Background" section and develop it a bit further, since it s less "competing method" and more "you should know about this to understand out method". ## Strengths & Weaknesses#### Strengths1) The method is generic (shown to work across tasks and environments). 2) The baselines are strong. You can t move the amount of training data to the appendix and it s not good practice to only include the network architecture by name in the main paper. This can be as simple as a 180USD RealSense and a 500USD robot arm plus a few objects and a playfield. And since you don t have ShapeNet data for many real world objects (which you seem to need for pretraining), could you at least add a sentence or two detailing how this would transfer to real world problems? 5: What do you mean "predicting both the structure and parameters of the target dynamics model"? Parameters is clear (mass, friction, etc.)<BRK>  SummaryThis paper proposes a framework, HyperDynamics, that takes in observations of how the environment changes when applying rounds of interactions, and then, generates parameters to help a learning based dynamics model quickly adapt to new environments. I m curious, are there any gaps before applying the method to the real world, and what are these gaps? WeaknessesAlthough I like the idea of this paper, I believe the authors should provide more clarification and illustration of the experimental results to solidify the claims in the paper:(1) What are the objects used in the pushing task? In other words, even if some baselines have a poorer forward prediction performance, will MPC be able to bridge some of the performance gaps? This sentence seems weird. Purely from the numbers in the tables, it is hard for the readers to imagine how well the proposed approach solves the tasks. (5) The beginning of Section 3.1 describes that an object s orientation is represented as a quaternion.<BRK>#### Summary:This paper proposes an adaptive dynamics model based on the idea of hypernetworks. It is demonstrated that this approach compares favorably to other ways of adapting dynamics models such as conditioning on a separate feature input and meta learning by gradient based model updates. Are components pretrained and how? Which losses/data are used for training? Its unclear why moving from a canonical to an oriented shape representation in Sec.3.1 should improve results. #### Recommendation: The paper reads well and proposes an interesting novel approach which could deserve acceptance. Maybe replace it by $[\cdot,\cdot]$. I keep with my rating "6: Marginally above acceptance threshold".<BRK>The newly added clarifications and sanity checks have greatly improved the quality of the paper, and I am therefore increasing my score from 4 to 6. Original Review  The paper proposes a model for predicting the dynamics of a physical system based on hypernetworks:given some observed interactions and some visual input, the hypernetwork outputs the parameters of adynamics model, which then predicts the evolution of the system s state over time. Strengths: 1. The paper addresses an important question, namely, how a dynamics model may adapt to    environments that don t fully match its training distribution. 2.The proposed use of a hypernetwork is plausible and novel to my knowledge. 3.The related work section appears comprehensive, and, to my knowledge,  does not miss any major    prior work. For the MB MAML baseline, this    is not specified. As a result, only the  Direct  baseline clearly operates in the    same experimental regime as HyperDynamics. If the goal is to evaluate    the new architecture, these should be disentangled. is cited, but that paper focusses on comparing recurrent models based on graph networks to    those based on MLPs, and it is unclear which model was used.
Reject. rating score: 2. rating score: 3. rating score: 5. <BRK>These papers should be considered by the authors, discussed in related work, and used as a basis to propose something new in future work as there are still problems in that area that need solving.<BRK>While the notion of adding resilience to ML architectures is the main motivation, the empirical results do not seem to provide convincing evidence for the utility of SP architecture in achieving it. The addition of SP architecture does not appear to impact the performance of the three architectures evaluated. 2.Significance of Results: The major issue that I find with the paper is the significance of empirical results.<BRK>+ve Code shared. Feel free to convince me otherwise :) It could be that I missed some core points in the paper but I don’t get the results in Table 2.
Reject. rating score: 4. rating score: 4. rating score: 6. rating score: 7. <BRK>Pros: The paper nicely introduces a different way of seeing GANs, not as a difference between the generated and real data, but as a an integer of the ratio between generated and real distribution times the discriminator. A detailed comparison of this work to that work would also be helpful.<BRK>**Summary of contributions:**This paper proposes a new framework to design new loss for GANs. They then compare experimentally the different existing loss and the new proposed loss that fall under their framework. **Main comment**:The framework proposed in the paper is interesting since it s quite general and the authors are able to derive a large number of existing as well as new loss from it. 2.The benefit of the framework is not clear, while it provides a way to derive new loss it s not clear what are the advantages of the new loss.<BRK>This paper generalizes the min max problem of GANs to form a richer family of generative adversarial networks. The family in this paper is shown to have a connection to WGAN except that the Lipschitz condition is omitted. In addition, apart from providing a richer family, this work does not significantly influence the practical aspects of GANs. I have some following questions:1.<BRK>However, it would be great if the authors can discover some trends in the results to demonstrate which type of functions work well with which type of datasets. The authors explain the differences between f GAN and this paper. However, it is not super clear to understand. It can provide a new loss function for the generative models which can further extend the success of generative models in the future. But it seems too much to ask this thing to the authors of this paper.
Accept (Spotlight). rating score: 8. rating score: 7. rating score: 6. <BRK>This paper introduces a new offline imitation learning algorithm, to be compared with BC. The main idea (and difference with simple BC) is to re use the old learned policy as a reward function in a policy gradient step. As an effect, the action distribution is bootstraped in a way that enforces the action near the mode of the observed demonstration. Consequently, if the demonstrations are noisy but the mode of the optimal behaviour is conserved, then it will learn a policy closer to the optimal behaviour than the demonstration itself. I think this idea is original, simple and smart. The empirical results show that it works very well on classic locomotion tasks with artificial noises (uniform/gaussian). I only regret two things:1) The first part (theoretical) of the paper, and especially section 4, was unclear to read and somehow misleading :  In section 3 assumption 3, if the noisy expert has never followed the noise, then the reward should not depend on the noise. And I don’t get at all the link between assumption 3 and its interpretation in the paragraph above. Also, as it is a lower bound of the performance of BC, I don’t get how it can show any limitation of this approach (I would have expected an upper bound in that perspective). 2) It would have been interesting to observe experiments with more chaotic noises (for example generated by biased learning algorithms or real world human demonstrations).<BRK>Summary This paper proposes an IL method for learning from noisy demonstrations. The paper develops a bit of theory to illustrate an upper bound on BC performance in the noisy observation setting, and uses this bound to motivate the construction of an algorithm. First, a minor point, "Corollary 1" has no accompanying theorem, rename this to "Theorem 1" (or is it actually supposed to be a Corollary of Theorem 1, which is actually in the appendix?I would expect the Theorem, not the Corollary, to be in the main text). (2) Section 5 does not clearly connect the algorithm with the results in Section 4. Other clarity issues  In S4.2, it s not clear what the constraints on \zeta are, or how it s connected to Eq (1). If my understanding is correct, it would be clearer to explicitly say that BC from obs is difficult because the maximization of the LHS of (4), i.e.the BC objective, is upper bounded by the difficult to optimize RHS of (4). Alg 1 L6   should this be objective (8) or (9)? I cannot parse the last line of 5.4, this requires a clearer explanation. Originality   One closely related work that is undiscussed is Disagreement Regularized Imitation Learning (DRIL) (Bratley et al., ICLR 2020), which also incorporates policies trained with BC into a new training objective to regularize the training of a new policy. Quantitative and qualitative comparison to DRIL would give the paper more context and clearer originality. Other comments   abstract: "but the non optimal"  > "but not non optimal" ? I cannot unerstand the sentence at the top of p3: "We refer to J (π, R) as on policy expected T step reward since the policy the expected reward is evaluated for and the one inducing the state distribution dπ are the same"  Replace the comma in Assumption 1 with a semicolon or other separator, currently it could be misinterpreted to mean J(\pi, R) \leq J_\beta(\pi, R).<BRK> SummaryThis paper studies the problem of imitation an expert from noisy demonstrations without interactions with the environment. It proposes an algorithm, which utilizes an ensemble of behavioral cloning policies and is analogous to the mean shift algorithm, to find the mode from noisy demonstrations. **definitions of $S_e^{\pi_e}$ and $S_{e+*}^{\pi_e}$**Thanks for clarification. I d appreciate if the authors can indicate what policy is used in Figure 1. However, in this definition, how about the states where the policy has always followed the expert policy so far? Are $S_{\pi_e}$ and $S^{\pi_e}$ the same? CommentsTheorem 1 seems to be incorrect. **The first term on the RHS in (7) thus works as a “noisy” regularizer. In addition, the theorem only provides a lower bound of performance, so the sentence "It also shows that BC can not boost ... Overall, I think this is a neat algorithm and it seems to work pretty well. I don t see the meaning of divergence (5) and (6), either. $$> Since the objective of IL is to obtain policies that can generate or keep being on optimal state trajectories as the true expert does, minimizing the divergence (5) over the states that are out of the optimal trajectories have a little effect to learn the $\pi_e^*$. By this argument, if we remove $S^{\pi_e}_e$ from the training set, will the performance change much? I think the reason why more data can help is that more data covers more states, so the correction effect is stronger. Assumption 4 seems a little bit weird. Why the number of Hopper v2 is quite different from Table 2? 4.L6 of the algorithm: is the objective (1) or (8)? Also I d like to see more discussions in the related work, e.g.,1.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>  SummaryThis paper aims to decode both content and ordering of language models and proposes Variational Order Inference (VOI). The authors model the posterior distribution of z as a Gumbel Matching distribution which is relaxed as a Gumbel Sinkorn distribution. To training the encoder and decoder networks, the ELBO is maximized using the policy gradient with baseline. The research on non autoregressive orders to generate language is interesting, and the proposed method using Gumbel Sinkorn distribution is mathematically well sound and novel. 2.The proposed method outperforms the previous Transformer InDIGO and other baselines (Random, L2R, Common, Rare). This paper analyzed the learned orders globally and locally, and conducted ablation studies.<BRK>The authors  propose the first domain independent unsupervised learner that discovers high quality autoregressive orders through fully parallelizable end to end training without domain specific tuning. To solve  the non differentiable ELBO ( discrete latent variables), they further construct a  practical algorithm with the help of policy gradients. The experiment results, such as the  global and local statistics for learned orders, are convincing.<BRK>This paper designed a new generative model by capturing the auto regressive order as latent variables for sequence generation task. Based on combinatorical optimization techniques, the authors derived an policy gradient algorithm to optimize the variational lower bound. Empirical results on image caption and code generation showed that this method is superior than both fixed order generation and previous adaptive order method transformer InDIGO. The authors further analyzed the learned orders on global and local level on COCO2017 dataset, demonstrating that the arrangement tend to follows the best first strategy. Also one strength of the approach is its potential of fully parallelizing.<BRK>This paper proposes to model the generation order as latent variables for sequence generation tasks, by optimizing the ELBO involving a proposed process of Variational Order Inference (VOI). To alleviate the difficulty of optimizing discrete latent variables, the authors propose to cast it as a one step Markov Decision problem and optimize it using the policy gradient. The authors also introduce the recent developed Gumbel matching techniques to derive the close form of the posterior distribution. Casting the optimization of discrete latent variables as a one step MDP is interesting3.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. rating score: 6. <BRK>(To be more clear, the algorithm SGD with Nesterov momentum is already in PyTorch’s library [[1]]). There are some specific reasons that make the theoretical analysis of the paper not convincing (please see details below). This gives no information on the non convergence of NASGD. I checked the appendix to look closer, however the proof is not rigorous enough to see what the authors want to prove. About Theorem 2: The authors imposed a new assumption: the set $ \mathcal{A} :  $ { $ \{ k \in \mathbb{Z}^+ | \mathbb{E} f(w^{k+1}) \geq \mathbb{E} f(w^{k})  \}$ } is finite. About the empirical aspects, it is nice that the authors provided numerous experiments using different data sets and network architectures. I think it could be better to see this aspect compared not only to plain SGD but also to Adam and other adaptive algorithms.<BRK>However, the idea is the combination of restart schedule and NGD, and the convergence result is not that hard to go through with the existence of current proof of SGD. In specific,1. the convergence result of SRSGD is O(1/epsilon^2), which is exactly the same with the convergence result of vanilla SGD, this implies that the the NAG momentum does not contribute to the theoretical part. 2.I understand the acceleration of convergence may happens in experiments but not in theory, and the main focus of this paper lies on the experiments. At least, the authors should provide more explanation on this. Overall, I think the contribution here are not enough to be published in ICLR.<BRK>The authors particularly provide a particular momentum restart scheme and characterize its convergence for both nonconvex and convex objective functions. Furthermore, various experiments have been conducted to highlight the strength of their algorithm. For instance, [1] has also developed a multistage variant of NAG with momentum restart between stages. My major concern is regarding the theoretical contribution on this paper. First, I encourage the authors to clearly state the assumptions that they are making to obtain their results such as Theorem 2. For instance, looking into the details of their analysis (Lemma 6 and Theorem 5 in Section C of the appendix), it seems that the authors are assuming the gradients are bounded (by $R$). Of course for finite number of iterations $\mathcal{A}$ is finite, but as the proof shows, its growth should be constant relative to $K$. However, Nesterov s method is known to be non monotone, and hence, this is not a straightforward assumption to make (see Figure 1 in [3] for instance).<BRK>UPDATE:I find the main contribution of the work to be the empirical analysis. I personally liked this paper, but I must agree with the other reviewers that improving the theoretical results will strengthen the paper s impact. #### Quality and SignificanceI think this work does not reach its full potential. *On the results:*The deep learning experiments are very thorough. It is common to include an additional decay at the 80th epoch (not just 30 and 60) on ImageNet; so I suspect this may be the reason for the slightly weaker baselines. The assumption of Theorem 2 that the cardinality of $\text{set}( k | E[f(w_{k+1})] \geq E[f(w_{k}] )$ is finite also seems strong. Firstly, it should be clearly stated in the introduction and the abstract that theoretical results in this work considers Nesterov momentum with time varying momentum (i.e., following Nesterov (1983)).<BRK>The strong point of this paper is its extensive experimental evaluations, which justify that SRSGD significantly improves the convergence speed and generalization over standard momentum SGD. The empirical analysis also sheds some light on the parameter tuning and interpretation of SRSGD. The experiments are comprehensive and constructive. Although the proposed restarting schedules seem to be hard to tune in practice, the authors provide empirical analysis on the impacts of different parameters, which gives some guidance. On the other side, I have some concerns on the theoretical aspects:   Theorem 1 (and 2) assumes a bounded variance of the stochastic gradient, which may not be true for ERM (e.g., for least squares "Jain, P., et al.(2018).Accelerating Stochastic Gradient Descent for Least Squares Regression. Thus, the "mini batch stochastic gradient" is not precise in Theorem 1 (and 2) and should be replaced with certain assumptions.
Reject. rating score: 5. rating score: 5. rating score: 6. <BRK>##########################################################################Summary:The paper proses a new adversarial (poisoning) defense based on a known graph reweighting scheme known as the ricci curvature. What are the limits of ricci curvature? The authors propose a new sampling method based on the ricci curvature and use it within their novel training scheme. ##########################################################################Reasons for rating:Overall it is an interesting work and the empirical performance seems to be good. ##########################################################################Pros:+ Interesting and promising approach+ Consistently improved performance over the baselines+ Interesting analysis of their defense via SBM graphsCons:  Only one strong attack on real world graphs is used to benchmark to other architectures. The authors seem to use transfer attacks and the referenced literature claims only robustness against random attacks and this is also only evaluated empirically. The paper lacks clarity at some points and has inconsistencies in notation.<BRK>Using Ricci flow for distance computation is a well studied area (as indicated in related work). The only novel part is that each layer gets a new graph; however, this choice is not motivated (why not to train all layers of GNN on different graphs instead?) and has problems (see next). was designed for the defense of adversarial attacks, so choosing them for comparison is not fair. Since you use GCN, why the performance of Ricci GCN is so different from GCN when there 0 perturbations? Also, an experiment with different choices of GNN is desirable. In most of the baselines, there are at most 2 3 layers. Considering that you didn’t provide the code (can you provide an anonymized version of the code?) and that your baselines (GCN, GAT, etc.) I applaud the authors for greatly improving their paper via the revision. This is a crucial part of your algorithm and not seeing discussion of it in the paper, raises concerns about the validity of experiments. I still have several concerns about the practicality of Ricci GNN.<BRK>Summary:In Ricci GCN new graphs are resampled in each iteration of the training phase based on the Ricci flow metric. This leads to improved robustness against adversarial attacks on the graph structure. The idea is well motivated, the paper is well written, and the experiments show a clear increase in robustness on real data. Typos:* Figure 5 Captions: Purterbation Rate ## After RebuttalThe authors  response clarified some of the issues and partially addressed some of my concerns. Strong points:* The main idea of using Ricci flow is interesting, well motivated and well executed. This was shown in the original Meta Attack paper, as well as multiple follow up defense papers." I would like to again point out that the fact that this is a common practice is not ideal, even though multiple follow up defense papers use the same strategy.
Reject. rating score: 6. rating score: 7. rating score: 8. <BRK>The authors propose *L conv*, a layer which is equivariant to transformations from a group $G$ in the neighborhood of identity. However, the experimental results are not sufficient for proving the advantage of the proposed approach. The theoretical contribution of the paper is not sufficient for considering it as pure theoretical. The paper is interesting and is easy to follow. The paper seems raw for acceptance at the current stage. The conducted experiments do not demonstrate the advantage of the proposed models over other models that use the power of data symmetry. * The decision of using very shallow networks is not clear. The *L conv* model contains 2 times more parameters than the standard CNN. * There are no demonstrations of the learned generators $L$.<BRK>This paper describes an approach to making deep learning robust to arbitrary symmetries. The L conv layer works somewhat like the convolutional layer, but instead of using the kernel mechanism, it multiplies the previous layer by L_i which is the basis spanning the Lie algebra around the corresponding Lie group’s identity element. Further, the authors compare their approach to a very similar one submitted concurrently to ICLR. The work presented in this paper is certainly relevant. While the approach presented here is similar to a couple of papers reviewed in related work (especially Zhou et al.), I think that the description of the symmetry relations in the language of Lie groups and Lie algebra bases is novel and well presented here. If the latter is not the case, I think a discussion as to why not would make the paper clearer. If there are other theoretical results on the structure / properties of Lie algebra bases that the authors make use of, I think they should be provided too. *****Typos:*****Theorem 1 (the third sentence) should be rephrased  > for this problem to be equivariant?<BRK>The paper also demonstrates how the underlying generators can be learnt from data and provide convincing supporting experimental results. The proposed L conv layers also use much fewer parameters compared to previous works. I checked the maths at each step and am convinced that it is correct, to the best of my knowledge. The experiments, though limited, in the main paper, are quite convincing. Questions:For Figure 2: For CIFAR100 and FashionMNIST, CNN seems to do better on "rotated+scrambled" compared to "rotated". This is not seen in any other method or dataset. The derivation in the supplementary material was a little bit clearer. 4.It would be worthwhile checking the paper for typos. It s also not referenced in the text. Overall comments:The main contribution of this paper is the development of the theoretical framework for Lie algebra convolutions. The paper does so very convincingly and I regard this as an important contribution to the area of deep learning.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. rating score: 4. <BRK>The method has two stages 1) From a smaller set of predicted variables, compute the remaining ones so that equality constraints are satisfied; 2) Take a few gradient steps (w.r.t soft constraint) in case inequality constraints are violated. The results look very promising. While most of this review will focus on the negative aspect, I want to begin by stating that I like the paper very much. The writeup is very nice and the experiments are sufficient. Having said all of that, I cannot recommend acceptance of the paper in its current stage. In several places, the paper claims to have orders of magnitude faster runtimes than standard solvers. It is even the only claim of the paper appearing in boldface. CVXPy was never developed to be a fast solver. Even if other bottlenecks are factored out of CVXPy, there is an option to use an almost SOTA QP solver on the backend with prob.solve(solver cp.OSQP), which the authors **do not use**. For these two reasons, it would probably be better to compare against the freely available OSQP directly, without incorporating CVXPy   (I feel, CVXPy, in general, shouldn t be promoted as a baseline for runtimes)  I didn t go through all the code but the time measurements I found were around large blocks of code, not the pure runtime of the solvers. This leaves a lot of room for inaccuracies. The baselines optimizers do not enjoy any parallelization over the instances.<BRK>There has been an increase of works using deep neural networks to heuristically predict solutions to constrained optimization problems. In this paper, the authors propose a method to build neural networks that output vectors that satisfy hard equality and inequality constraints. This would make for a much stronger paper. It is a direct competitor when constraints are convex. Against, this is a direct competitor when constraints are linear. So in what way does the current approach compare to these previous ones? The main advantage I see is that the method can be applied to nonconvex problems. But at least, it seems it does converge for the nonconvex "ACOPF" problem of the experiments, so there seems to be some use case. Regarding the timings in the experiments: all the machine learning methods use a GPU, so it would be interesting to compare against a baseline solver that can use a GPU when possible as well. First, since comparisons on QPs are presented, I would have liked to see comparisons against these methods.<BRK>Page 5: I would have preferred to see the discussion of the very relevant related work in the main text and not relegated to an appendix. The applications are well motivated and it appears that deploying the method for the ACOPF could have immediate impact. ### Cons The results may be more impactful and interesting in an operations research venue. The gap between the learned optimizer and the exact QP solver is non trivial for some problems. Is this assuming that the constraint set is convex? I was confused as to why the equality and inequality constraints are treated so differently. ### The  Optimizer  Baseline Table 2: The gap between the exact QP optimizer and DC3 is non trivial, with a scale that is often bigger than the gap between DC3 and some of the other deep net baselines. The claim "we find that DC3 achieves comparable objective" may be over stated. Perhaps you should do a paired analysis across optimization problems. You should update to emphasize that this is not the case. It should be GD. Why do you expect this is true?<BRK>Strength:+ The paper proposes a general framework to deal with constraints in optimization problems using neural networks. In my opinion this is an important problem since there exists no standard method in many existing deep neural network frameworks to deal with constraints, which are also inapplicable even if the constraints are only slightly nontrivial. The paper proposes to deal with equality and inequality constraints differently which may be often easier in large scale settings. Weakness:  While the related work from the recent years has been discussed to some extent, the paper fails to show how the technique proposed is better than them. The main idea of the paper has a rich history in optimization literature and is referred to as "elimination" of constraints. To this end, the paper does not have any discussion regarding the convergence aspects of the framework which is a crucial subject for this paper. For a generic optimization framework (proposed), the paper only provides experiments on well studied optimization problems   all the experiments provided in the paper are with quadratic programming problems which are known to be easy both in theory and practice. It will be interesting to see how the framework performs in different problems. Also, it is not clear whether the experimental benefits will carry to other networks that are used in practice. After response: While some of my concerns were cleared after the response, the experimental evaluations presented do not sufficiently support the claim that the method can handle nonlinear constraints.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>Based on PMI word embedding, the authors categorize the knowledge graph relations into three types, which serve as the foundation of knowledge analysis. This paper is not well motived but presents the methodology, well. The motivation is not crystally clear. Good presentation, the clear methodology, and promising results. Besides, based on PMI embedding and relation categorization are still very regular in the field. Thus, I doubt the novelty of this paper. However, compared to these papers published in this field, I think this paper can be accepted. Discussion:I think the categorization shall benefit the performance most when the difficulties between relation categories are balanced. I just want to provide a new idea for your paper to improve.<BRK>This paper builds on this understanding of (PMI based) word embeddings aiming at the task of understanding the latent structure of low rank knowledge graph representations. Strong evidence to this premise is provided by starting at encoded semantic relations of word embeddings generalizing them to three types (R,S,C) of knowledge graph relations. The authors analyse the performance of different state of the art knowledge graph models and identify the best performing model per relation type.<BRK>It is not discussed the possible benefit of the learned latent structure of knowledge graph models for the performance on downstream tasks, e.g.text classification, or natural language inference. The goal is to show that word embeddings and knowledge graph representations learn a common latent structure even if both types of models have different learning objectives. Which is the relation between the used knowledge graph categorisation and the related work? Summary and ContributionsThe authors study the latent semantic properties of word representation models by categorising relations between entities.<BRK>This paper aims to establish a theoretical basis for geometric properties of knowledge graph relations and embedded entities by comparing knowledge graph embeddings with word embeddings. The analysis claims to show that when the KG architecture conforms to the presented relation types and conditions (divided into similarity, relatedness, and context shift types), it has better performance of link prediction for that embedding scheme.
Accept (Spotlight). rating score: 8. rating score: 7. rating score: 7. rating score: 7. <BRK>Summary:The authors proposed the idea of using invisible noise to make personal data unusable to authorized deep learning models. To achieve this goal, the authors proposed the idea of error minimizing noise crafted by a min min optimization method. The idea is very well motivated and explained. The experiments not only confirm the exceptional effectiveness of the proposed method but also show its flexibility. Pros:1.The paper is very well written and easy to read. It seems that breaking the essential assumptions in machine learning can break the model. Although this is not new, however, it turns out to be very interesting if used for data protection or similar ideas. 2.The proposed noise seems not strong against adversarial training. I think the “clean data” in the proposed setting should be the “poisoned data” rather than the ground truth clean data since both the training and testing data will be collected at the same time.<BRK>They proposed one kind of error minimizing noise to make the data (added noise) unlearnable. The noise is imperceptible to human eyes, and thus does not affect normal data utility. Its motivation is intuitive and well explained. Considering adversarial training is to find the worst case example to make the training process robust, the authors proposed an opposite direction to find the easiest case to make the training process to learn nothing. 2.The paper revealed an important problem to protect privacy, and proposed a simple yet effective method to prevent our data from unauthorized exploitation for training commercial models. I think it will attract a broad audience in the ICLR community. Cons:1.What is the overhead of generating and adding this kind of noise? especially for random and error maximizing noise?<BRK>This paper describes a method for making user data unusable for training machine learning models. It focuses primarily on image data. The code and the datasets used for the experimentation have been provided. ########################Overall, I would recommend accepting this papers. My only concern is with the effectiveness of the proposed technique given what authors discussed in the appendix (see questions below). ########################Questions:From the appendix notes: it appears that adversarial training can significantly negate the effect of adding error minimizing noise. (due to the effectiveness of adversarial training and the outsized influence of a relatively small number of clean data samples on model performance)Can the authors discuss the issues with the effectiveness of their presented technique?<BRK>$\textbf{Comments:}$The paper s motivation is based on protecting private data and preventing its being scraped and used to train models. The key difference is to apply Projected Gradient Descent (Mandry et al.2018) in the reverse direction iteratively to *minimize* the loss function. 100\% of the training data was perturbed. In a sample wise setting, "error maximizing noise" is still learnable and performs very well; however, it performs around 20 and similar to "error minimization" in a class wise setting (Figure 1). If I am not wrong, Projected Gradient Descent, as proposed and applied in Mandry et al.2018 (Figure 1, right side), reduces the performance the same as the proposed error minimization approach, and there is no performance gain. https://arxiv.org/abs/1707.04131https://arxiv.org/abs/1610.00768https://arxiv.org/abs/1912.11852$\bullet$ *Different source target models:* In all experiments, the source model is Resnet 18.
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 6. <BRK>+ Promising experimental results are obtained by the proposed method. Weaknesses:  Duplicate task settings. The novelty of the proposed method is limited. Combining recognition head and detection head is not new in weakly supervised object detection. However, I would like to keep my original reject score. Therefore, the task itself cannot be one of the main contributions of this paper (especially the most important contribution of this paper).<BRK>I still hope to see this paper accepted, but cannot endorse it due to insisting on inaccurate claims. I agree with R3 that the limitation of novelty should be considered from the two perspectives of "task" and "method". The task is certainly not new, which should be made clear in the paper.<BRK>**Weaknesses**:* How does this work compare with other WSOD recent methods, e.g.C MIL, [A, B, C] from the experimental results point of view, and the amount of supervision? * The text/figure should be adjusted to better map one each other: Sec.3.2: “The image and proposals are fed into several convolutional layers”, but in the figure, the proposals skip those layers. **Quality**:The paper is technically sound.<BRK>It proposes a unified framework along with a spatial correlation module for the task. The spatial correlation module is used for transfer mapping information from base categories to novel categories. (2) In Table 1, are the experimental settings of those competitors such as MSD VGG16, MSD Ens, and Weight Transfer et al.exactly the same as those used in this paper? (4) Some similar problem settings are defined in [a] and [b]. However, I still have some doubts about the structure of this module.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 5. <BRK>2.The originality and significance of this paper is not enough, as it applies RuBERT on EHR data to do medical EE. 2.Apply pre trained transformer based model to finetune the proposed model in EHR data to do medical EE. The paper does not give the performance comparison with the state of the art EE model.<BRK>This paper presents a distantly supervised approach to extracting medical entities from EHR. Based on my understanding (according to Figure 1 and Table 4), the proposed approach is performing multi label text classification instead of entity extraction. The NER task aims to detect token spans in the text that represent an entity.<BRK>This paper presents a model that performs Medical Entity Extraction in a way that 1) allows to generalise better certain types of entities, resulting in a better performance and 2) can achieve human like quality despite relying on a distantly supervised training. Include experiments on Russian, good for a domain where most of the research is focusing on English. However, imho the contributions stated are not significant enough for the paper to be accepted for publication. Please include more details when you refer to model s mistakes.<BRK>This paper introduces an end to end task that identifies medicalentities and links them to UMLS concepts for Russian biomedicaltext. The main weakness of the paper stemsfrom the way the training/validating datasets are created. I think 1could include more details with a concrete example.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>Cumulative distribution of class scores were used to estimate accuracy on a multi class classification problem. Technical novelty is limited but the proposed approach could have a decent impact in the literature for a variety of problems involving large number of classes such as extreme classification, zero and one shot learning, open world classification, multi task learning etc. Once NN is trained it can be used to obtain C_x for k_2 classes (k_2>>k_1), which are in turn used to predict accuracy for the k_2 classes. The paper introduces the reverse ROC concept to offer an interesting interpretation to expected accuracy of k classes. Unlike earlier work that uses KDE and non parametric regression to predict the classification accuracy for a larger number of classes the proposed work uses a neural network for the same task. In Algorithm 1 I am not quite sure how a neural net with a fixed number of input nodes deals with varying number of inputs? The size of the input is k but k changes from 1 to k_1. Need some clarification here.<BRK>The authors discuss how a classifier’s performance over the initial class sample can be used to extrapolate its expected accuracy on a larger, unobserved set of classes by mean of the dual of the ROC function, swapping the roles of classes and samples. Grounded on such function, the authors develop a novel ANN approach learning to estimate the accuracy of classifiers on arbitrarily large sets of classes. Effectiveness of the approach is demonstrated on a suite of benchmark datasets, both synthetic and real world.<BRK>The authors show a relationship between classification accuracy and reverse ROC in multiclass classifiers, when there are new classes not seen in the training data. They propose a method called CleaneX that learns to estimate the accuracy of multiclass classifiers on arbitrarily large sets of classes. Major Comments:The concept seems rather novel; however, I am not very familiar with the literature in evaluating one shot learning classifiers. The paper in general seems rather "empty" in that the authors simply show things, but do not explain the importance of them or spend time motivating the problem. For example, what practical use is there to know the expected accuracy of a classifier for predicting k classes? For Figure 3, the authors should find a better way to display the performance that does not require eyeballing how well the colored lines follow the shape of the black one. In general, the authors provide some intuition on why the CleaneX method performs better than the others in the discussion, but this should be more closely tied to the experiments. There should also be more discussion on potentially why the CleaneX method is better than the others.
Reject. rating score: 5. rating score: 5. rating score: 7. rating score: 7. <BRK>This paper is about keystroke inference attacks and proposes a method to assess the threat of deep learning based approacheswhen only limited real life data are available. To this end, it is introduced a video domain adaptation techniquethat is able to generate data into separate style and content representations. This is an interesting applicative approach, but I believe that the introduced novelty is probably limited for ICLR. Two claimed contributions are the assessment of deep learning methods for keystroke inference attacks when limited data are available and the domain adaptation approach to generate synthetic data. Given these similarities I would have expected more discussion in the related work about this paper and also some comparison between single keypresses results vs complete sequences. Furthermore, from Table 1 it appears that ADDA provides much worse results with respect to the proposed method. Then a discussion is needed since it has been used in Lim et al.achieving good performance in a similar context. For what concern the supervised disentangled learning based approach, I think that the authors should make very clear which is the introduced novelty with respect to current state of the art methods. In particular, which are the new components of the proposed method. In this respect it would be helpful to make comparisons with some baselines. As a final minor comment I think it would be important to provide more information about the real life dataset used in the experiments, for example by indicating how many participants where involved. Post rebuttal comments  First of all, I want to thank the authors for answering my questions. In my opinion this is not sufficient to carry out a significant evaluation.<BRK>The authors investigate video domain adaption for keystroke inference attacks on synthetic and real life data. Also, I suspect the paper is more suitable for the security conference, e.g., S&P. + lacks comparison to the state of the art domain adaptation methods, such as:    Ehsan Hosseini Asl et al,  AUGMENTED CYCLIC ADVERSARIAL LEARNING FOR LOW RESOURCE DOMAIN ADAPTATION , in ICLR 2019. The background is black and only a thumb is generated. + The paper claims that the proposed method can separate the style () and content.<BRK>In this paper, the authors introduce a video domain adaptation technique that learns to disentangle style and content of a video in order to generate as form of data augmentation. They motivate their problem by recognizing that deep learning based keystroke inference attacks are trained with a small number of real data along with a larger number of synthetic data. This results in the need for data augmentation via domain adaptation. Multiple evaluation metrics were used, and it is clear that the framework is useful under the considered setting. However, it is not clear to me how would style representation be useful in this data augmentation. In general, I find this paper well written and well motivated. The method is novel and produces good results over other baselines.<BRK>In this paper, the authors focus on keystroke inference attacks in which an attacker leverages machine learning approaches,  In particular, a new framework is proposed for low resource video domain adaptation using supervised disentangled learning, and another method to assess the threat of keystroke inference attacks by an attacker using a deep learning system, given limited real life data. The novelty of the approach and its theoretical foundation is appreciated. The paper is clearly written and well organized, and all the key concepts and motivations are described in enough detail to understand the paper. However, it is not clear from the outset the amount of limited real world data should be collected from the target domain. It should also be clarified at the beginning why their data augmentation that prevents models overfitting, and why translated to better security against keystroke inference attacks. It however seems like their code is not made available, so there is a concern that the results in this paper would be very difficult for a reader to reproduce.
Reject. rating score: 4. rating score: 4. rating score: 6. rating score: 9. <BRK>The paper highlights two main observations: 1) there is no clear dominating optimizer, and 2) selecting from a pool of optimizers with their default parameters is often as good as tuning a fixed optimizer. I am maintaining my score, because I don’t think the current version of the paper is enough of a contribution to get accepted. The main contribution of this work is the open sourced experiment results for a multitude of cases which can serve as a baseline for future research in optimizers for deep learning. Update:I have read over the changes made by the authors, and also the other reviewer’s responses. The problems considered are of varying difficulty, and includes tasks other than image classification. Furthermore, the hyperparameters that produce the best performance for a specific seed tends to be more unstable (which the authors agree to in appendix C); evaluating such an unstable setting on different seeds unnecessarily penalizes the optimizer. What we want to see with the tuning experiments is how well the optimizer can do (as an upper bound), and how stable it is. I believe the experiments lack results for the “well tuned” case. The optimizers all use a fixed hyperparameter search range for all problems, which can’t be competitive over different tasks of varying difficulty. I understand that this study assumes the model practitioner to be someone who doesn’t have prior knowledge about the optimizer, let alone the search ranges. However, I think it’s reasonable to believe that a practitioner would try to verify the search range by testing some hyperparameter values before committing 25 or 50 trials to the search range. At the very least, it would be useful to see the performance vs hyperparameter value plotted for the existing experiments to see whether the ranges could have been trivially improved (for example, if the performance tends to increase/decrease with the learning rate, but the best performance lied on the boundary of the search space, the range could have been shifted). This sort of tuning procedure is not unknown in the community.<BRK>### SummaryThe authors of this paper conducted a thorough evaluation of deep learning optimizers across different compute budgets and learning rate schedules. As such, I do not believe the paper in its current form should be accepted to ICLR. The design decisions are well reasoned and explained throughout the paper. ### Comments* As the authors note, there is certainly value in understanding the practical tradeoffs between optimizers: "for most algorithms, the only formal empirical evaluation is offered by the original work introducing the method"* The writing is clear and easy to follow. * Open sourcing the data is great and beneficial for the community. Investigating whether there are systematic differences in optimization on larger problems e.g.machine translation or ImageNet would be valuable. ### Recommendation / JustificationI vote that this paper is below the acceptance threshold. There are many things to like about the approach taken is this paper, as highlighted above. However, the lack of larger scales datasets lessens the significance of the conclusions. I d increase my score if concerns about the datasets used were addressed. * I think it is worth acknowledging techniques for averaging the weights of neural networks, as these can have a substantial impact on final performance (Polyak averaging, exponential moving average, Stochastic Weight Averaging). * I am surprised by the choice of \alpha when tuning the lookahead optimizer. I agree with Reviewer 3 that tuning with a fixed seed and the lack of search space refinement is a major weakness.<BRK>This paper presents an extensive independent benchmark of 14 popular optimizers on a variety of deep learning tasks from DeepOBS (Schneider et al.2019).They compare them at three different tuning budgets and with 4 learning rate schedules. The authors are realistic about their setup. While there is no clear cut answer that tells practitioners which optimizer to use in what scenario and how to tune it, these experiments are valuable and I believe it is important that these results are shared with the community. The quality of the presentation and the writing is good. In terms of novelty, the authors model the target audience slightly differently from previous work (Schneider et al.2019, Choi et al.2019, Sivaprasad et al.2020).I am not convinced that this approach is better per se than others, but a different angle and a different set of optimizers is a valuable contribution to the community. They do not compare hyperparameter tuning methods, but rather benchmark optimizers similarly to this work at a continuum of hyperparameter tuning budgets (all with random search). Finally, let me share two concerns:1. The intro mentions three contributions: (i) performance varies greatly, (ii) trying different optimizers works as well as tuning a single one, (iii) they identify a significantly reduced subset of algorithms and parameter choices that perform well across experiments. Similarly, for point (iii) it is not clear from which results this is concluded, and what the high performing subset is. Such a list would be valuable to many practitioners and should be clearly stated in the main text.<BRK>Overview:Overall I believe this paper is extremely well written and organized. The introduction and limitations are very useful groundings of optimization research, and I hope that the community reads this paper and internalizes its message of making more meaningful research in optimization (instead of yet another Adam variant!). In a previous comment I brought up a serious concern about the tuning ranges of momentum like parameters, which I believe could bias the results towards optimizers that were tuned with ranges whose lower end was 0.5 (Adam, AMSBound, AMSGrad, AdaBound, LA(RAdam), NAdam, RAdam). **NOTE: updated score after seeing author replies and updated draft. I believe that this work is exemplary in terms of being careful about baseline construction, something that is unfortunately too often overlooked in our field. The authors clearly put a lot of careful thought into how to study and present these results, taking into account numerous caveats that almost all other papers totally ignore, such as the limitations of their study, the importance of tuning ranges, learning rate schedules, etc. The authors discuss that GANs and RL are not included, and those seem like very different types of optimization to me so I am more understanding of not including them. The experimental results could partially be explained by the No Free Lunch theorem, and the authors could at least reference this in the section. What regularization, if any, was used for these problems, and was that also tuned? One could argue that, while optimization and regularization are in theory orthogonal to each other in what they try to accomplish (train vs test performance), they are both part of the update rule whose hyperparameters are being tuned. Writing: In the “Tuning method” paragraph in section 2.3, “In case there is no prior knowledge provided in the op.cit. Prior work:The authors do a very thorough literature search, and properly reference and discuss similar prior studies. Additional feedback, comments, suggestions for improvement and questions for the authors: Awesome job providing per step values for results, it would be further useful to have code that could easily plot them side by side so that future researchers would be further encouraged to include them in their figures. The trapez schedule always seems to be the best, and I wonder if this is due to only one of the learning rate ramp up, which has been shown to be beneficial to stabilize training (although it is unclear if this is required), or the learning rate becoming quite small at the end, which has been shown to be necessary so that the optimizer can better learn the noisy directions of the objective.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK>Authors state that they randomly split images into training, validation and test sets. Strengths:+ Interesting problem setting. I find the medical imaging use case convincing. + The proposed idea is simple and clean+ Empirical evaluation uses a comprehensive set of baselinesConcerns / weaknesses:  The technical novelty in this paper is unclear.<BRK>Negatives:  The novelty of the paper seems to be somewhat limited. Using unpaired image to image translation networks for cross modal medical image synthesis has been studied in [1, 2] (as the authors pointed out in the paper). Moreover, they demonstrate the effectiveness of the proposed algorithm via extensive numerical experiments on a prediction problem and compare their method to several different approaches ranging from transfer learning to data augmentation. It is not clear how the discussion on text to image generation methods in Section 2.2 is relevant to the paper.<BRK>The difference is that the proposed method is applied to regression task and [1] solved the segmentation task. This paper combines the Cycle GAN with the predictor for downstream task to augment the training data in the target modality. Proceedings of the IEEE conference on computer vision and pattern recognition.<BRK>Is this related to the amount of training data? Reasons for score:The objective of the paper (i.e.addressing small training set size in medical imaging) is quite important, and the approach is interesting. The loss for L_adv has been described on p6 but not the loss in eq2. Such a comparison would show beyond doubt that the translated images are anatomically correct or not.
Reject. rating score: 3. rating score: 5. rating score: 7. <BRK># SummaryThe authors introduce a novel VAE based approach for unsupervised learning of disentangled representations of image data. The approach trains an ensemble of VAEs along with pair wise linear transformations between their latent spaces. # Cons:* The authors’ baseline results give unexpectedly low metric scores. * The traversals in Figure 8 from the authors’ model are much less disentangled than other models in the literature. For example, they are much less disentangled than the traversals shown in the beta VAE paper and the FactorVAE paper on the same dataset. Figure 3 A also suggests that the authors’ model is using too many informative latents, i.e.not disentangling well. * The purpose of the cross model reconstructions is not clear, particularly given that I’m not convinced by the authors’ intuitive justification of them. The L2 regularization between the transformed encodings should pressure the cross model reconstructions to be good, so I do not see the reason to include them in the model objective. Ensemble training is very computationally expensive, so the authors should include some discussion about it as well as runtimes and memory requirements for their model. # SummaryI do not recommend accepting this paper.<BRK>This paper proposes a simple and effective technique to improve disentanglement by coupling the latent spaces of different VAE models. It builds on Duan et al.(2019)’s proposed method to rank the representations of different models. By learning a VAE ensemble with linear transformations between the latent spaces and an additional “cross model” reconstruction loss, the authors show that they can achieve significantly better disentangling. Strengths:  The theoretical justification seems reasonable and builds on previous work. The results do suggest the VAE ensemble learns better latent representations which can be converted between models with simple, orthogonal linear transformations. Or is there a stopgradient on z_ii when used in computing this loss term (i.e.no gradients through VAE i from this loss term)? Minor:  In Figure 2(a) I assume the curves are overlapping? Does it help to use a log scale for the y axis? Are the scores in Table 2 across different training runs?<BRK>### Strengths:  Significance / Novelty: The proposed approach builds on recent work by Rolinek et al.and Duan et al., which show PCA like behaviour in VAEs and leverage these results to develop disentanglement scores for model selection. The authors were able to address my concerns adequately and I believe that the revision improved the quality of the paper quite a bit. This submission uses these insights for training an ensemble of VAEs in order to improve learning of disentangled representations. Clarity: I consider this paper well written and well structured. The examples for the latent traversal (in the appendix) are slightly less convincing and a comparison is only done w.r.t.a standard VAE. Similar to the last point, in figure 2, it would be quite insightful to see the DtO results for the beta VAE and especially the FactorVAE. ### Additional Feedback:  Figure 1: I like the illustration, however I do not understand the bar plot (“VAE, BetaVAE, FactorVAE, VAE Ensemble”).
Reject. rating score: 3. rating score: 5. rating score: 6. rating score: 6. <BRK>This paper proposes a method for estimating conditional and average treatment effects under unconfoundedness. There are two main ideas: (1) train a VAE with latent space aimed at the adjustment relevant information, and (2) incorporate the targeted regularization of Shi et al into the training. The problem of estimating causal effects using deep learning is important, and I think paper has a promising direction. It is closely related to a growing literature on this subject (as noted in the paper itself), and does a reasonable job of explaining the innovations. However, there are some significant issues. The exposition is generally unclear. The paper needs a complete rewrite, with particular attention to clarity about the (salient parts of) TMLE, and the claimed advantages of this approach over closely related methods. There also needs to be substantial improvement around the VAE component of the model, and precisely what the identification assumptions are. The empirical results are inadequate. Further, the ablation study doesn t seem to test the right aspects of the model. Some further comments and questions:1. missing expectation symbol in 2nd paragraph of background (in def of \tau)2. equation 1 is meant to be describing an observational quantity; the do(T t) should just be t3. the discussion about optimal epsilon 0 is confusing (I understand that you mean that running more than 1 round of the TMLE update doesn t make a difference, but this is not clear in your writing)4. indeed, all of the prose around equation 2 should be rewritten. 8. when you compare to  Dragonnet  are you comparing to Dragonnet + Targeted Regularization, or just vanilla dragonnet? I presume that at least the shared latent representation is still affected by the inclusion of the targeted regularization term?<BRK>This work provides an improved method for individual and average causal parameter estimation. The main idea is to apply targeted learning to deep latent variable models. The proposed approach is based on two existing work:   Disentangled variational latent model [Zhang et al., 2020]. However, it provides a nice combination of those works and adds the missing element of z_0 to model factors unrelated to treatment and outcome and to give more flexibility to the design of the latent variables, which all together makes a complete picture. The authors compare the proposed method with some alternatives on two datasets and the results show the better performance of the approach in most of the cases. In general, the presentation of the paper is subpar. I do not believe that the explanation for equation (1) will clarify the matter. The last paragraph of page 5 regarding the difference between the training in this work and [Shi et al., 2019] is not clear. On page 4, the authors mention that: "the use of deep latent variable techniques enables us to attempt to infer these hidden confounders from what are known as noisy proxy variables present in the dataset" This is in general not true. If the authors are indeed assuming that we have latent confounders and they are estimated using VAE, then nothing can be said about the correctness of the outputs of the method.<BRK>The paper proposes Targeted Variational Autoencoders for ATE and CATE estimation. It is assumed that all relevant confounding variables can be measured through proxy variables and then a VAE architecture is used to disentangle the latent variables into 4 sets: z_t   confounding between the treatment and covariates, z_c   confounding between the treatment, covariates and target, z_y   confounding between the covariates and target, z_0   solely related to the covariates. The proposed architecture builds on an existing approach, TEDVAE. The main difference is that TEDVAE did not include the z_0 term. A targeted regularization approach is also used which is similar to Shi et al.2019The proposed method outperforms existing approaches on to real world datasets. In general the paper is well written and the approach appears sound and reasonable. The novelty is somewhat limited as both the architecture and regularization build incrementally on previous approaches. However, the proposed method appears to lead to a decent increase in performance over SoTA approaches.<BRK>The proposed contribution of this work is to build of the existing literature which uses variational autoencoders for causal inference by (1) allowing an explicit mechanism for modeling irrelevant covariates and (2) incorporating targeted regularization into the latent variable nnet framework. My largest concern is with novelty–each contribution borders on incremental, and neither necessarily open doors for substantial amounts of follow on work. With that being said, I think that this work does provide value to the community given (a) the sensible, simple, model changes proposed and, (b) the pretty compelling empirical evidence. Comments / Questions:* You note that the gradients are taken with respect to zeta and not with respect to g_p or g_q in the paper. This language is kind of confusing at first read. This is very sensible, but would benefit from more explicit discussion relating back to the TMLE literature. Given that the introduction of zeta provides substantial benefit on its own, it would be interesting to see a variant that does not use z_0 but uses zeta. Small edit notes:* In the main text you make reference to equation 14 which is in the supplement.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK>The paper introduces a new, interesting definition of "sparsity" in a deep network. When combined with an activation function such as ReLU, they show that this allows to remove entire layers by merging two adjacent weight matrices. The paper is well written and relatively easy to follow. However, the experimental results are definitely below what one expects in a deep learning paper today. In my opinion this is not sufficient to show that a method is useful in a realistic context. The paper should provide some discussion of the relation with recent literature on the lottery ticket hypothesis, especially when introducing its retraining procedure.<BRK>This paper proposes a new notion of layer sparsity for neural networks that aims at simplifying network architectures and reducing the number of parameters. Overall the paper is well written and the idea is interesting. The numerical results are not sound. Using a fixed number of epochs in each setting, we have no idea of the convergence status for different methods. The authors claim in the end of the results section that all methods can sometimes provide accurate prediction if the number of epochs is extremely large. Plotting some training curves might help readers understand more of the behavior. It is fairly standard practice to tune the regularization parameters based on cross validation or an independent validation set. The authors should include this for the completion of the manuscript, even if the numerical experiment might serve as a proof of concept. It would be better to include comparison with other sparsity inducing methods such as connection sparsity and node sparsity.<BRK>I will be excited to see an updated and improved version of this paper in the future, but in my opinion, the current version still needs a bit of work and is not entirely convincing. The authors proposed a regularizer that can lead to such layer collapse thus resulting in shallower and more compact models. In my opinion, the proposed regularizer is quite simple and intuitive, but is at the same time novel and could potentially be useful in practice. I am afraid that the current empirical evaluation is not sufficient. The core idea of the layer sparsity regularizer is introduced and explained with a sufficient level of mathematical rigor. (I will update the score depending on the authors reply.) I would like to thank the authors for their reply, which addressed some of my questions. While I now agree with the main theoretical result when applied to a ReLU nonlinearity, this of course also reduces the area of applicability of the proposed technique. But I think the results are still a bit insufficient to make this submission sufficiently strong.<BRK>__how I would summarize the paper.__The paper gives an interesting new paradigm of the neural network compression called the *layer sparsity*. Based on the observation, the paper designs a regularizer that enhances the possibility of such aggregation by regularizing the negative parts only. __review tl;dr.__While the idea itself is quite interesting, I believe that there should be more practical evidence and algorithmic extension of the framework, for the idea to fully bloom. Also, potential practical impact of the notion of layer sparsity seems to be big, as reducing the number of layers has a more straightforward practical benefit, in terms of reducing the inference flops/time. Perhaps this is why most of the works on network pruning focus on evaluating their algorithms on compressing deep convolutional networks trained on real (as opposed to synthetic) datasets. A minor concern is that the refitting step does not seem particularly novel, as the retraining step is already typical in model compression literature.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. rating score: 5. <BRK>My assessment of this manuscript is that it sits right on the threshold separating acceptance and rejection. On the other hand, the paper s novelty is minimal and the mathematical descriptions of some important parts of the manuscript are insufficient. In this manuscript, the authors  contribution can be summarized as placing this architecture in a GAN and observing similar improvements, which is unsurprising and in my assessment a marginal contribution. How can this work when $g \in p4$ is a rotation and not a translation? Your Group Equivariant networks have less parameters, and you are operating in a self professed limited data environment. I am not convinced that, in table 2, the potential for augmentation leaking means that the CNN with standard augmentation is "inapplicable".<BRK>This group shall be a group of symmetry of the signals to generate, and in this paper, the authors focus on the group generated by translations, pi/2 rotations and reflexions. Consequently, I would substantially lower the claim "to our knowledge, we are the first to introduce group equivariance to GANs, and use geometric considerations in both generator and discriminator". Why isn t the linear  averaging along the group in the G equivariant architectures, and why is it only w.r.t.translations? [Post rebuttal] I ve read the rebuttal, which answers to many points I raised.<BRK>Summary:The submission concerns an application of group convolutions (Cohen & Welling, 2016) to the image synthesis setting, where images are produced by the generator of a GAN. Experiments indicate somewhat lower FID scores on both synthetic and real settings. Overall, however, I see novelty and impact to be fairly limited, as all insights come from empirical evaluation, which is notoriously difficult for visual synthesis applications. Although the authors claim in the discussion section that visual fidelity and sample complexity are meaningfully improved, I miss an attempt at quantitative analysis of this claim, either through algorithmic metrics or user studies. Since one of the strengths of the approach may be in the limited data regime, I would have liked to see stronger evidence of a major impact there; I can t quite see such a trend in the numbers of Table 1.<BRK>This paper presents a method for incorporating inductive symmetry priors into the network architectures of GANs. It would be interesting to see a comparison with other flavours of GAN also. The paper is well written and comprehensive. The experiments are fairly convincing, although I would like to see more discussion about when to use the group convolution in the discriminator vs generator vs both, as this seems to make a difference for different datasets.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 6. <BRK>*SummaryThis paper analytically considers two flavours of adversarial training in a Gaussian mixture model. It s nice how the authors are able to get algebraic results in this setting. The direct comparison between two different forms of adversarial examples (on  and off )manifold is a nice framing that illustrates the difference between the two types of adversarial training. The exposition of the theory could be motivated much better. It seems like this is assumed in Corollary 3 (rank \Sigma_*   q) but then  not assumed in theorem 4, since then \lambda_min   0. While the analytical results are nice, the experiments are not very convincing that the analysis carries into real data. Furthermore, the paper as a whole seems rushed: missing details in the experimental section and a lack of motivation in the theory section.<BRK>Paper is well motivated and is focused on some recent and interesting aspects of adversarial robustness. To be more specific, I have the following two main concerns about this paper. However, the theoretical analysis in this paper is completely centered around a highly restricted linear generative model with a carefully chosen design. #2:I am not completely sure about the mathematical validity of the main claim of the paper (at least, from a theoretical standpoint), which is the "The comparison between Corollary 3 and Theorem 4". The other crucial difference between the results of Theorem 4 and Corollary 3, is the appearance of $\\lambda_{\\min}$ in Theorem 4, but not Corollary 3. page 4: Data  > data.<BRK>The present paper proposes an interesting study about two different kinds of adversarials, depending on the area of the variance and manifold that is attacked. Robustness to adversarial examples can be improved with overfitting. Introduction Why the method is not applied experimentally to real datasets instead of just claiming that theoretically proved that it works? In line with previous comments, more recent approaches for this kind of attacks should be preferred. General:•	More references regarding the adversarial robustness trade off should be added to the text and discussed, as a key point presented in the introduction and suggested as future work in the conclusion.<BRK>In summary, the authors provide a theoretical study on theoretical analysis of the attacking mechanisms of the two kinds of adversarial examples: regular and generative adversarial examples. They should w that adversarial robustness can be disentangled in directions of the data manifold. The extension to nonlinear model is unclear, which may be more common in practice.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 7. <BRK>In comparison to previous work, this model has an expert cost function which gives priority to the expert behavior, not only using the expert demos (like in DQfD), but also with a model trained with those demos using  behavioral cloning, such that it could be evaluated in states that were not visited during the demonstrations. The paper presents an interesting idea with potential, although there are some aspects to consider mainly about the presentation of the paper, in order to improve its content, as listed below. "Behavioral cloning is an end to end learning method" is not a right statement. in "...adversarial learning method and has yielded impressive results...", impressive could be replaced by an objective word. lines 8 and 18 have different notations for the same operation,  authors could use only one of them to be more consistent.<BRK>The BC model is used in the expert loss function, where the DRL model s actions are compared with those obtained from the BC model for policy improvement guidance. The experimental results in OpenAI Gym environments show that the proposed approach adapts well to different demonstrations  imperfection levels and accelerates the learning processes. The ablation study also indicates that the new method improves the learning convergence performance compared with the original DQfD model. Reasons for score:Overall, based on the description of the paper, the proposed approach works well. The experiments show that the performance of DQfDD BC is better than DQfQ, and it works well for imperfect demonstration scenarios. What if we use some heuristic approaches to replace the replay buffer with higher quality data? Can some simple strategies like this achieve similar performance as building a BC model?<BRK>PositivesThis paper is well motivated. The experiments back up the authors’ claim on the benefits, especially on the improvement of training speed. In the CartPole tasks, the training speed is improved by 2 folds. NegativesThe extensiveness of experiments is underwhelming, both in the diversity and complexity of the tasks chosen. For example, DQfD was validated on a broader and more challenging set of Atari tasks, including Montezuma Revenge where (human) demonstrations are critical for a learner agent to tackle the task. The authors are strongly encouraged to further improve the experiments. Especially in the top half, “Demo” rewards are juxtaposed with other numbers in a completely different units (number of episodes). In Section 1 “difficult consistent”  > “difficult to be consistent”.<BRK> POST REBUTTAL COMMENTS  I thank the authors for the response and the efforts in the updated draft. Most of my concerns were addressed. This is a simple, but nice idea. It would be better to compare against more complex benchmarks such as Atari. The writing needs improvement for clarity. The ablation study in Figure 4 shows the benefit of the proposed method. Won t this add a lot of really bad trajectories in the beginning of learning which seem like they would have a negative impact on the BC policy. 2.Equations should be introduced before they are included. Equations 1 4 come before the text that talks about them and this makes the paper more confusing that it needs to be. 3.Equation (5) seems incomplete. 4.Average stop episode is not clearly defined in the text.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 5. <BRK>The paper is to train a meta model in a small number of selected nodes in a federated learning environment, and then use the meta model to assist the federated learning in reducing the communication rounds. Therefore, in the experiment part, the authors need to add more baseline methods, for example, some personalized federated learning method should be selected as baseline methods. Moreover, given the assistance of the meta model, there is no guarantee that the federated learning environment will converge in a few round.<BRK>The experimental results are not convincing because the data partition is not for federated learning. Reusing data partition in a meta learning context is unrealistic for a federated learning setting. The comparison is unfair to FedAvg. I suggest the authors introduce meta learning and its advantage first in the Introduction. Several recent published knowledge distillation based few shot FL should be discussed. I cannot see any technical contribution. 2) Claiming the few round adaptations can reduce communication costs for federated learning is misleading, since the meta training phase is also expensive. I hope the author can obviously compare the total cost of meta learning phase plus FL fine tuning phase with other baselines.<BRK>This paper studied the combination of federated learning tasks in a meta learning setting. It was inspired by the meta learning method used in few shot learning scenario. It is an interesting topic to combine meta learning with federated learning. 1.The proposed method updates meta model in each client. More details about the experimental platform used in this paper should be given. Since the algorithm is an important part of this paper, the definition and use of these parameters should be much clearer. If possible, the authors can add a detailed interpretation of these two parameters.<BRK>## SummaryThis paper proposes a new paradigm to train federated learning models. In particular, following the spirit of meta learning for few shot learning, the authors propose to meta train an initial model so that starting from this point, only $R$ (eg, 3) rounds of FL are needed to produce a satisfying test accuracy. ## Pros1.The authors made significant efforts in designing the meta learning strategy for few round FL. 2.The proposed algorithm has the potential to redefine FL training paradigm. The training task the authors selected is more like a meta learning standard setting and is not common in federated learning. The authors are supposed to discuss this additional overhead.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 8. <BRK>Summary:This paper proposes a new method that adaptively merges intervals to form a discrete action space where on each interval Q_I values are learned via deep neural networks. Then it applies ready methods designed for discrete action spaces to do off policy evaluation. ##########################################################################Reasons for score: The paper offers a new way to apply methods designed for discrete action spaces onto continuous action spaces and it seems to perform better than the two chosen baselines as seen from the experiment results. Although the authors mentioned problems with the baseline models quickly, it would be nice to see a more in depth analysis in the experiments to demonstrate these problems that this paper has set out to overcome. It is also not very clear to me when and why DJQE performs better than the baselines (does it always perform better than the baselines?). I gave a conservative score 4 but I m willing to change my evaluation if convinced. ##########################################################################Pros: The paper provided theoretical support to the proposed method by proving its consistency under two reasonable assumptions. ##########################################################################Cons: Overall the paper is not very clear to me. It would be nice to see more in depth theoretical analysis on the main advantages of DJQE compared to the baselines, the lack of which generates the following questions:    Will this method always achieve lower biases than baselines on new datasets? I m not sure about the quality of evaluation from a simulation model on the personalized dose finding application. What are the potential problems/limitations of DJQE if there are any? How accurate is the simulation model trained on Warfarin?<BRK>SummaryThis paper proposes a new method for offline evaluation when the action space is continuous, one dimensional. This overcomes the drawbacks of the kernel based method, which cannot be applied to non smooth Q functions and requires heavy computation to optimize the bandwidth. The proposed method can be applied to discontinuous Q functions like step functions, and achieves smaller bias. This is made possible by the adaptive jump q learning method. ProsWhile the kernel method requires a single bandwidth to control the bias and variance of the value estimator, the proposed method adapts to the shape of the Q function by dividing the action space in an adaptive way, so that the MLP fitted in each interval of the action space approximates well the real Q function. Hence, the intervals can have possibly different lengths according to the shape of the true Q function. A multi scale change point detection method is used for determining the intervals, which requires only a linear computational cost. Experiment results are convincing. Hence, computation will become heavy when the number of initial intervals (m) is large. Authors should add discussion about this point. 2) Some notations are confusingMinor comments 1) Gamma appears before it is defined. 2) L is both the number of subsets and the numer of layers in neural networks. Are they meant to be the same (as they increase with n), or are they different? In the latter case, they should be distinguished.<BRK>This paper considers the problem of off policy evaluation with continuous actions. The DJQE method is theoretically analyzed under both the cases that the Q function is either a piecewise function or a continuous function. For continuous function, it is not surprising that as the number of splits m goes to infinity as n, the estimation is consistent, while additional results in Theorem 2 also shows that for limited m, the estimator can also be shown as a uniform approximation of the Q value. Experiments consider both a toy dataset and a real problem in personalized does finding, and the results show that the DJQE method is superior than existing methods for continuous Q evaluation. The paper is clearly written and easy to follow. I only have a few comments:1. In the experiments, since computing the optimal bandwidth is very time consuming for the baseline methods, it would good to provide a detailed computation cost comparison. 2.As mentioned in the method part, m is initially set to be proportional to n, and the final partition size is much smaller than m. Would the authors shows these detailed numbers in the experiments? 3.It could be great if more real world problems can be evaluated in the current experimental section, such as the dynamic pricing example introduced previously.<BRK>Summary of paper: The main contribution of this paper is a new algorithm to learn the expected reward function for a given target policy using the historical data generated by a different behavior policy in continuous action domains. The algorithm proposed in this work adaptively discretizes the action space by combining methods in multi scale changepoint detection, multi layer perceptron regression and OPE in discrete action domains. To generate synthetic data, four scenarios are considered, where in each case the Q function is continuous in the action domain or is a piecewise function of the action. In almost all of these cases, DJQE outperforms the two kernel based methods. Plus points:  The experimental results seem to demonstrate quite convincingly that DJQE outperforms the two kernel based methods in almost all cases. The methodology seems sound. The theoretical results also appear correct and prove the soundness of the method for a fairly wide range of functions   those that are continuous in the feature space and action domain, as well as those that are piecewise constant. The method can model jump discontinuities in the Q function. Questions:  Why is it reasonable to assume that the Q function can be well approximated using piecewise linear combinations of MLPs? How is the performance of DJQE affected by the choice of the regularization parameter \gamma? Minor comments/questions:  Page 2, line  2: exists  > to exist  Page 3, line 4 of Section 2.3: segments  > segment  Page 3, line  4: Was there a reason for choosing the logarithm function here? Page 4, lines 12 to 13: Is there a theoretical justification for such a choice of m, or is it based on empirical observations? Also, to what extent does the performance of DJQE depend on the initial choice of m? Page 5, Equation (4): Could it be justified why the minimizer is unique? (There is no Figure A.) Page 5, line  4: How is \hat{Q} used in the solution of Equation (5)? Page 6, Assumption 1: "...number of nodes [in] each hidden layer..."  Page 6, last line of the statement of Theorem 1: Should D be D_0? Page 6, lines  7 to  6: I did not fully understand what this means; do the change points of \hat{D}^{\ell} vary with m? * Update after reading author(s)  response: Thank you very much for the detailed answers to my questions (as well as the other reviewers  comments/questions). I have upgraded my score; wishing you all the best.
Reject. rating score: 6. rating score: 6. rating score: 7. rating score: 7. <BRK>Summary: This paper uses asymmetric self play to train a goal conditioned policy for robot manipulation tasks, which can also generate curriculum automatically and generalize to unseen goals and objects. The experiments contain various challenging manipulation tasks, where the proposed method outperforms all the manually designed curriculum baselines. Pros:+ The paper is well written and easy to follow. + The goals and skills discovered by Alice interesting. The proposed method can generate some meaningful goals which work better than explicit curricula. + The novel shape/geometry manipulation results are promising. Questions & Concerns:  Lack of important baselines. The author uses PPO, but this framework will still support off policy learning. This adversarial training idea is very similar to CER which is not cited. (minor) Are there any exploration strategies in Alice? However, since there is no comparison with HER based methods (or HRL), it s hard to justify whether asymmetric self play is necessary for these kinds of problems. Advances in Neural Information Processing Systems. I also notice that the author added more references and details based on all 4 reviews. Thus, I decided to improve my score on this paper.<BRK>This surprise was reduces when by the supplementary material where it is clearly explained that the observations include the full ground truth information on the robot state and the manipulated objects: "object state observation contains each object’s position, rotation, velocity, rotational velocity, the distance between the object and the gripper, as well as whether this object has contacts with the gripper." I also feel that showing movies of vision inputs is a bit misleading because, in my view, it coveys a message that this work is on vision based manipulation learning while in reality there is nearly full knowledge of all the items  true states. There are multiple figures showing the merit of the approach within the designed task framework in the form of comparison with baseline methods that do not use the asymmetric self play framework and an ablation study. ~~The one comparison with (Sukhbaatar et al.2018b), which this study is a variation of, appears at the very end of the supplementary material and seems to show that the two methods do equally well on tasks presented in this study. **I found the paper lacking in clarity regarding both method and evaluation:The method: Section 3 describes the method. What I found to be unclear is   how were the experiments on hold out tasks different from the other tasks. The unclarity in the description of the method may have contributed to the unclarity of the evaluation section (in that it is not clear to me how the framework is modified between these task groups). **The reliance on behavioural cloning (shown to be crucial to achieving non zero success in this study s modelling) limits the proposed method to cases where the environment can be reset which would make it much harder to implement outside of a simulation environment. Overall, due to the issues mentioned above, I find this study to add little to the methodology and understanding of asymmetric self play as an RL method for robotic manipulation. **EDIT: with author feedback and changes to manuscript (and supplementary) I think that the study is more interesting than expressed in my original review, however the sim real gap with respect to applicability to robotics is still a major concern IMO. I have updated my score accordingly.<BRK>This paper presents an approach to learn goal conditioned policies by relying on self play which sets the goals and discovers a curriculum of tasks for learning. It sounds to me now that if Bob fails in the first game, it fails in the whole episode, is it so? What about the data complexity of the proposed approach? In Fig.6, why are the results different with a single block and two blocks? The experimental results are very encouraging. The analysis of the method helps to understand which components are important. The evaluation on the hold out tasks is very impressive and pushes the state of the art. The paper is well written and very easy to follow, the illustrations are informative and appealing. Although this approach is based on previous work on asymmetric self play, the authors clearly describe the contributions of this work (training clearly from self play, zero shot generalisation). Post rebuttal I would like to thank the authors for the detailed response and clarification of my questions. What does this state include? Why can t the policy be trained directly from vision? Does including the state mean that the learning procedure is not applicable to real environments? Questions:  "Multi goal" game: the experiments show that it improves the results, but I still cannot understand why it is crucial. Does it help because of several goals or because of longer episodes?<BRK>This paper presents a self play approach to learn a goal conditioned policy for robot manipulation. The Alice and Bob approach is able to generalise to unseen objects and results in a natural curriculum. The paper is well written and well structured, the proposed approach is framed within the relevant literature and claims are supported by experimental results. Experimental evaluation shows that the proposed approach outperforms other methods on several manipulation task. Ablation studies complete the analysis of the proposed approach and help understanding the contribution of each part of the approach. Comments:  How does the proposed method compare with other goal based approaches (e.g.multi task approaches or goal based intrinsic motivation exploration)? What is the main cause of the low success rate with increasing number of objects? Can you discuss the main limitations of your approach with respect to other methods? What is the expected transferability of the proposed approach to a real world scenario (e.g.with a real manipulator and real objects)? How does this affect generalisation in your specific case study?
Reject. rating score: 4. rating score: 5. rating score: 5. <BRK>Specifically, it tries to find the node mask $V_s$ and feature mask $F_s$ which can identify the most important information of the input such that the masked information can yield a high fidelity score. 1.There are a lot mask based methods for interpretation in different domains [1] [2] [3] [4]. + Compare with GNN Explainer, this work focuses on using hard masks to explain GNN predictions. Is it possible that different nodes may have different important features? Top nodes, in the beginning, may not be top any more after some nodes/features selected? The method itself is very straightforward, which is a simple greedy algorithm.<BRK>The core idea is to identify, for each node v in the graph, the nodes and features of the graph most relevant to the behaviour of the GNN for node v. That is, the goal is to find a subgraph of the computation graph associated with a node v in the graph. For instance, it would also make sense to compare based on the measures introduced in the GNNExplainer paper. So it should be written as argmax F((V_p, {f})). Also, what is the p here? Another worry I have is the efficiency of the approach.<BRK>ZORRO leverages rate distortion theory to generate masks that select nodes in the target node neighbourhood and their most important features. * The problem is very relevant to the GNN community, and I am glad to see more works coming in on this topic. * The paper is well structured and organised. * The original contribution is sufficient. This seems to be at odds with the authors  claim to explain the behaviour of the model. B) A drawback of this work is the complete absence of human based evaluation. If humans are not important, then what is the reason you explain your predictions? Aside from a full fledged evaluation campaign (see A. above), some examples would really help make the case. If choosing a desired fidelity is not crucial, the paper should show so.
Reject. rating score: 4. rating score: 6. rating score: 7. rating score: 7. <BRK>It is mentioned at the beginning of sec 5 without being properly introduced what really is meant. typo first line sec 4.1: detectpr  > detectorI really would have liked to see a strong rebuttal given the good results for the ECCV challenge and the importance of the problem. However, the rebuttal nearly caused me lowering the score. So overall the rebuttal has made me less convinced about the paper than before. The formulation to address the problem discussed in section 4 seems to make sense. As a reader I am left with guessing  The authors introduce a novel metric (mEAP) and it is unclear why this should be a better or more suitable metric. I see no reason why standard metrics could not be used when evaluation is done on a per dataset level. Other results are reported in standard measures in table 2, 3 and show very small differences between different methods thus showing no real advantage of the proposed method. Additional comments after the rebuttal phase: On the positive side  the authors are clarifying a few things in the rebuttal and also in the paper such as table 1   for that table results are clearer now by updating the table and caption. On the negative side, however, I am less convinced after the rebuttal than it looked to be prior to the rebuttal.<BRK>The only supervision required by the approach is constituted by the single dataset label spaces and respective annotations. Strengths  Unsupervised approach not requiring to hand design hierarchy or label space correspondence  Results are above state of the art (challenge winner)  Elegant formulation via constrained optimizationWeaknesses  missing clear specification of loss in the optimization problem. a full clear specification would enhance the reproducibility of this work. lack of simpler baselines in evaluation. Why this has not been evaluated as a simpler baseline? wouldn t be better to perform an actual fine tuning of the whole detector according to the new found label spaces? no comparison with handlabelling counterparts. In the intro two works Zhao 2020 and Lambert 2020 are referred as manual counterparts of the proposed method. While it is not expected that for the presented approach to perform better than this kind of algorithms it is surprising that this kind of comparison is completely avoided. The problem at hand is very interesting, especially for industrial applications and the formulation is elegant and leads to superior results with respect to other approaches. The lack of some experiments and the not so clear specification of the loss (especially from a reproducibility point of view) are the main concerns I have and why the current paper rank is marginally above the acceptance threshold.<BRK>One common problem for these approaches is how to deal with the different label spaces of each datasets, naive approaches would define an union over all label spaces, which has the undesirable effect that a CAR in one dataset is a different CAR in another dataset. In this paper this is solved by  a linear integer program which finds the optimal number of labels in the unified label space and their mappings and weights. Moreover this approach shows promising results over the disjoint/union label space. Also note that the  human expert  is a *"best effort" mapping, which can be a good starting point. I think this should be carefully considered since this is the most important section of the paper. [why is at most 1 label per dataset merged? That only holds for a specific label in the unified space] 3. It would be interesting to also use the semantics of the provided labels [for the current method it doesnt matter whether a label is called CAR or BMNASDJHASD]. The paper is somehow difficult to follow, but that should be solvable within the rebuttal phase. **Post Rebuttal**After reading the rebuttal, the updated manuscript and the other reviews, I became *less* convinced about this paper. While I remain positive about the conceptual idea of learning a unified label space. The authors did not successfully convince me in improving the understanding of the method.<BRK>While previous methods do the label space mapping, from each dataset specific label space to the common universal label space, manually, this paper proposes to learn such mapping automatically. Positives  The paper is well written and well organized, it is very easy to appreciate what is being done  The problem formulation of mapping each dataset label space to a joint label space using Boolean linear transforms, and integer programming formulation is novel and interesting  Results are given on challenging datasets which were part of the ECCV Robust Vision Challenge (RVC), and the experiments and the performances of the proposed method are convincing. The proposed method was one of the top performing method in ECCV2020 RVCNegatives  The shortcomings of the method are:* the method does not train the detectors end to end; it trains a final projection layer, which is put on top of the penultimate layer features of individually trained object detectors* (if I understand correctly) the training is done only with annotated objects in the different datasets, i.e.if there is a face object in the image of a dataset which doesn t have face in the label space (e.g.COCO does not have face label, but faces appear in the images of the dataset), that will not be used for training face part of the detector. For evaluation on new datasets, as the authors noted, annotation over all the labels in the unified label space would be required. Zhao et al.provide such a test set, for a different collection of dataset, which might be useful in the future to tryThese points should be discussed more in the paper. Overall the paper is well written and has a novel formulation and solution to the problem of label space merging. It also evaluates the method convincingly on challenging public benchmarks.
Accept (Spotlight). rating score: 9. rating score: 8. rating score: 6. rating score: 5. <BRK>RobustFill and DeepCoder have demonstrated state of the art performance, historically, so I believe these comparisons are sufficient for this paper’s acceptance.<BRK># Resons for ScoreGenerally speaking, I really enjoyed the reading of the paper. The paper is well structured and written as well as easy to follow.<BRK>The paper proposes to combine bottom up program synthesis from input/output examples with a machine learning model. Does the set E[1] include the entire input vector for input examples as an element and each constant as a vector of the constants with this length (In this case it should be E[1]   {I} \cup C)?<BRK>The paper was a pleasure to read. I would say that the idea to use NN for this is not significantly novel. All sections of work are well presented, understandable, and easy to follow.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. rating score: 5. <BRK>Summary:This paper provides the convergence analysis of the Heavy ball method with the individual convergence and provides the convergence of its adaptive version. Pros: 1.The motivation is interesting, most of time we use the last iteration while most of theorem can only provide convergence result for the average output. I am glad to see that the theoretical proof of the individual convergence for stochastic Heavy ball method is studied in this paper,  2. The convergence result of HB methods achieved in this paper is 1/\sqrt(t) which is better the optimal result of SGD. This demonstrate the advantage of momentum based methods. However I am not an expert on this area and I will not be sad if this paper is rejected by others.<BRK>In this work the author considers the individual convergence of the last iteration. The accelerated convergence rate O(1/sqrt(t)) is established. Pros:1.The individual convergence of momentum methods is pretty interesting. It is very valuable to established the accelerated convergence rate, which is missing in the existing litterature. Cons:1.This paper only consider convex optimization. This limits its effectiveness to explain the good performance of momentum method in training deep neural networks. 2.I suggest the authors to reword their statement of the third contribution on page 3 "However, in order to get the optimal individual convergence, β1t has to be time varying". Theorem 4 only shows that adaptive momentum can achieve individual convergence. Theorem 5 shows that constant momentum can achieve average convergence. Moreover, the author should add more discussion on the difficulty of establish constant convergence for constant momentum3. There is a large gap between the numerical and theoretical results. In the theoretical result, the authors consider convex optimization which is quite different from training deep neural networks. Moreover, when training DNN, the algorithm used is SGD with momentum but not GD with momentum. The abstract mentions that the numerical result on convex optimization is included. However, I don t find this result. Please add it.<BRK>The authors investigate the convergence of the projected Heavy ball method (and an adaptive variant) for convex problems with convex constraints. Notably, in their proofs they require an increasing (from 1/2 to 1) momentum parameter and a decreasing stepsize. Finally, the authors present some experimental results. Also, to the best of my knowledge, the individual convergence rates proved by the authors are novel. However, I do not think the paper is ready for publication, for the following reasons (in order of importance, the last points are easy to fix)1) The paper is short – the main section is only 2 pages and the proof (in the appendix) does not exceed one page. 2) Reading the paper, it seems the authors are the first to provide an individual convergence rate for HB. This is not true, indeed Ghadimi 2015 also has a rate of O(1/t) for the last iterate of HB under momentum t/(t+2)  and stepsize 1/t. In essence, I think the experiments do not back up the theory: the authors should at least compare with a fixed momentum method. I think the authors should rephrase that. I think the results are interesting, just they need to be discussed/complemented/verified a bit better. Score updated after rebuttal, please see comment below<BRK>This paper s major contribution is analyzing the HB method for non smooth objectives functions and showing the last iterate convergence for it. Comparing to existing results, they show a tighter upper bound by dropping a log(t) term from thesuboptimality s upper bound. It also analysis an adaptive variant of the HB and show similar results. All the proofs are clear and straightforward. Comments: 1  This paper needs major rewriting and restructuring and in its current format is not ready to be published. The abstract talks about a specific parameter and its value, which the reader has no idea about it. b)  Typos exist in their proof. For example, in the proof of Lemma 1, and in the last equation, there is no u, but the line above reads for all u in Q. Also, in the paragraph for the paper s 3rd contribution, it says decaying \beta_1,t goes to 1. c)  There are two appendices, one at the end of the main part, and the other part is in a separate file. On the convergence of the Stochastic Heavy Ball Method      Sun, T. et. c)  In the experimental section, it would be nice to see the results on bigger datasets like Imagenet and also different tasks such as NLP models. To be fair, similar to your sep size, which is decreasing \alpha_t for SGD should be decreasing with an appropriate rate. Finally, since you run the experiments for 5 runs, it would be useful to add the error bar to your graphs.
Reject. rating score: 1. rating score: 2. rating score: 3. rating score: 4. <BRK>There is a massive body of research doing this. There is a bold statement in the introduction which shapes up the problem that the paper is trying to address and I quote: "Recently, many such approaches employed the VAE framework which aims to learn a smooth representation of its domain.<BRK>Cons:One big issue with the paper is in its lack of experimental rigour.<BRK>The goal and the description of the experiments are clear and detailed, the figures clear and easy to understand. The broad topic addressed in the paper (model training in RL) is interesting, but the contributions are not novel enough for this venue.<BRK>I would like to see a more thorough evaluation with comparisons to other baselines outside of this paperOverall this paper is working on an interesting research direction, however I cannot recommend to accept it in its current state. The VAE is trained using the variational lower bound and the policy is optimized using PPO.
Reject. rating score: 3. rating score: 5. rating score: 6. <BRK>Strength:I appreciate the experimental results demonstrated on challenging datasets like CIFAR100 and ImageNet. Weakness(1) There are two existing papers emphasizing direct training SNN with extremely low latency [1][2]. In the experimental results, the paper compares performance with [1]. I think a comparison of the same network size can help to demonstrate the effectiveness of the proposed method. As far as I know, the only difference is that the threshold and leaky parameters are also trained in this paper. Direct training for spiking neural networks: Faster, larger, better. Temporal Spike Sequence Learning via Backpropagation for Deep Spiking Neural Networks. Slayer: Spike layer error reassignment in time.<BRK>This paper proposes a Spiking Neural Network (SNN) training method that jointly optimizes input spike encoding parameters, spiking neuron parameters (membrane leak and voltage threshold), and weights in an end to end fashion using gradient descent. The reviewer’s main concern with the paper is its limited novelty. Two primary factors that determine the energy efficiency of an SNN are inference latency and activation sparsity.<BRK>The time scale of the leak is an important parameter in SNNs. Yin B, Corradi F, Bohté SM (2020) Effective and Efficient Computation with Multiple timescale Spiking Recurrent Neural Networks. arXiv.Zimmer R, Pellegrini T, Singh Fateh S, Masquelier T (2019) Technical report: supervised training of convolutional spiking neural networks with PyTorch. But the threshold affects the activity of each neuron, whereas the weights are shared among multiple neurons." Plus earlier the authors said that the threshold is the same for all neurons of a given layer. So in short, the authors successfully combine known approaches, but do not propose anything new at a conceptual or theoretical level.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 5. <BRK>This work proposes a golden symmetric loss (GSL) for the noisy label learning problems. Compared to previous weighted symmetric cross entropy losses, the proposed loss estimates the trade off parameters using the transition matrix. Empirical studies demonstrate that the GSL method is better than some baselines. My main problem is that why should we estimate $A(\hat{C})$ and $B(\hat{C})$ simultaneously? The references are informal. 2.The experimental results seem to be far away from state of the art noisy label learning methods, such as DivideMix [1]. Some state of the art noisy label learning methods (in 2019/2020), e.g.DivideMix, are not discussed.<BRK>The authors of the paper propose a novel loss function termed the golden symmetric loss to tackle the important problem of learning with noisy labels. The proposed loss function, in essence, involves the use of a corruption matrix to correct the regular cross entropy loss and, at the same time, to estimate the relative weighting of the corrected cross entropy loss and that of reverse cross entropy loss. A series of empirical experiments were conducted to demonstrate the effectiveness of the proposed method. The idea to use a confusion matrix to automatically tune for the relative weighting between cross entropy and reverse cross entropy is also interesting. Does this mean that the exact weighting still needs to be tuned? 2.Why is the corruption matrix applied only to regular cross entropy but not reverse cross entropy?<BRK>###############Summary:This paper proposes a robust loss function that combines both cross entropy loss and reverse cross entropy loss. But the novelty of this work does not meet the standard for an ICLR publication. However, the why GSL can handle that is not clear to me. As the corruption matrix 1a) illustrates that there is no corruption. However, the corresponding optimization problems are the same, since the values of \alpha and \beta can be absorbed into the learning rate. The whole improvement could be resulting from changing the learning rate. 2.The clean data only used to estimate the Noise Transition Matrix proposed by GLC method (Hendrycks et al.(2018)).Then they are added to the final loss as the clean cross entropy loss.<BRK>##### SummaryThe paper proposes a golden symmetric loss method that combines cross entropy loss and reverse cross entropy loss, and at the same time performs forward loss correction for cross entropy loss, under the problem where we have noisy training labels and clean training labels. My concern on theory is resolved, but I still think an additional baseline of regarding it as a hyper parameter would be helpful as a comparison, having at least a few trials with mean and standard deviation for the experiments would make the conclusions stronger, and unifying the baselines is important. It would be nice to have the same baselines for vision and text.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 5. rating score: 6. <BRK>CHEF  is applied to various cross domain few shot tasks and cross domain real world applications from drug discovery. I recommend the authors that include more description of Hebbian Learner. 2  This paper claims that using "Hebbian Learner makes CHEF extremely fast," but I did not find any supporting experiments in the main paper to prove this statement. 3  This paper performed the experiments on a few shot learning setup for that chosen 5, 20, and 50 examples per class to train the model and observed continuous improvement on model performance. 5  It would be better to show the individual contribution of the Hebbian Learner. Therefore result should also be included in the ablation analysis section without using Hebbian Learner in the same setting. But it is not mentioned about the initialization technique. Overall: The paper needs to include many things for better clarity. It needs to include experimental results for different settings, as I mentioned in the weaknesses section, to prove the model s efficacy.<BRK>This paper introduce a learning mechanism that combining few shot domain adaptation with a Hebbian learning rule. Basically, the authors fused multiple layer feature representations in weak learner and ensemble the classification results. This approach is trivial. I would suggest the author can introduce the benefit or provide the reason a Hebbian learning can improve adaptation performance.<BRK>Summary:This paper primarily deals with cross domain few shot learning. Under this setting, there is a large shift in domain going from the meta train dataset to the few shot datasets. They propose a Cross domain Hebbian Ensemble Few shot (CHEF) learner, that learns an ensemble of classifiers at multiple levels of a deep neural network, thus making use of both low and high level concepts. Experimental results show that CHEF does better, in most cases, than learning a separate classifier at a given level. It learns additional weights for classification. Cons:1.The paper is missing details. The authors talk about Hebbian learning, FID, etc. but do not give details about it. 2.Is the proposed algorithm a Hebbian learner? If so, this is not an apples to apples comparison. Even though p(x) does not change much going from meta train dataset to the few shot datasets, the samples seen in the two scenarios are disjoint.<BRK>The experiments show results on miniImageNet and tieredImageNet, where the domain shift is small and also on the cross domain few shot benchmarks with larger domain shifts. 2.The equation (1) basically depicts the Hebbian learning rule where V is the matrix of postsynaptic responses v_i which are effectively the gradient of the loss in equation (2). I also wonder what does combining several Hebbian learners mean in the case of few shot learning? Based on my current understanding and the above comments, I currently recommend the paper as "marginally below acceptance threshold". I would like to hear clarification on Hebbian ensemble learning. A reference to Hebbian ensemble learning will be useful. 3.In ICLR 2020, there were few works that proposed to learn mutual information from diverse domains. I think it is worth to provide to have a discussion on them. I think it is better to make them exactly perpendicular to the x axis.<BRK>In this paper, the authors focus on cross domain few shot learning in the case of large source target domain shifts. The literature e review for methods on DA is not up to date, and not reflect SOTA methods on deep DA. +  The supplementary material provides additional information that should be useful to the reader (experimental setup and results). However, I am not convinced about the results shown in Section 4.3. If I understand, Table 4 compares a deep NN (FCN). + CHEF with some conventional ML models (SVM and RF)?
Reject. rating score: 6. rating score: 6. rating score: 6. rating score: 6. rating score: 6. <BRK>I am raising my score, with an expectation that the authors will make additional textual revisions based on their responses to make it clear in the abstract and introduction that (1) authors only consider the "reconstruction based" SSL, instead of SSL in general, and (2) address the discrepancy between the practice and the proposed framework. __summary of the paper__The paper provides a mathematical framework to theoretically understand and quantify the benefit of self supervision on the downstream tasks. The key underlying mechanism is what authors call the _approximate conditional independence.___Strengths__(1) The theoretical analyses appearing in the paper are concrete.<BRK>The estimation error refers to the distance between the best function in some function class H, and the optimal estimator computed based on the given data. The definition at the end of page 3 may have some typos since f^* is the universal optimal predictor. Some CI results may inspire this utilized ACI; however, the provided generalization bound seems more general with a weaker assumption. Overall, it is a good sunmision and offers much insight for SSL from the theoretical perspective.<BRK>This paper proposes a mechanism based on approximate conditional independence (ACI) to explain why solving pretext tasks created from known information can learn representations that provably reduce downstream sample complexity, as a sufficient condition. One concern is that, in terms of the theory for self supervised learning, I am not sure how big is the proposed theorem contributes to the community, as there are works such as Tosh et al.(2020b) in contrastive learning also provide theoretical guarantees using assumptions similar to CI. So I would rather hear more comments from other reviewers. Another point is that the authors spent around four pages to illustrate their results with lemmas and theorems, which is good. But for the benefit of a broader audience, I would suggest to include more intuitions and discussions at the beginning, include some proof steps and ideas only with limited but key lemmas and theorems, and move the rest of them into the appendix. It addresses an important question that perfectly fits into the ICLR topics: what connection between pretext and downstream tasks ensures good representations? Update after rebuttalThanks for the author s response. I suggested this paper being marginally above the acceptance threshold.<BRK>The paper shows theoretical results to support the claim that (approximate) conditional independence is a good way to quantify the link between the downstream task and the pretext task in self supervised learning. The authors also provide numerical illustrations. The paper has some experiments that support its claim. * Weak points: Use of mean squared error for a classification task which negates most of the practical aspect of this paper. * Lemma A.7 is not true for any delta, please state in the lemma for which delta it is true (delta is at most $ke^{ n}$ if I am not wrong)    * Generally, when reading the proofs I would have appreciated if you included more details. * In the proof of Lemma C.2, please provide a reference for "Davis kahan", for those that are not familiar with this result.<BRK>This paper attempts to understand why self supervised learning works in the following sense: will the sample complexity for a downstream task be decreased (compared to the standard supervised learning without pretraining) if it is pre trained according to some related auxiliary task? The relation between tasks is formulated as the approximate conditional independence of the dependent variables. In such cases, does the analysis in this paper still apply? I have some comments to improve clarity. i) I m not quite sure about the motivation of the conditional independence assumption in the introduction. If so, please also add detailed discussion and some validation experiments in such models. However, I hope the authors to address the above issues to improve paper quality. Self supervised learning has outperformed other methods in transfer learning and semi supervised learning and it would be better to at least discuss more. I m aware that I may underestimate the theoretical contribution of the paper since I m not familiar with the related work. Welcome any feedback on this during rebuttal and discussion.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 6. <BRK>  Summary:This paper shows that introducing an abstention class for out of distribution (OOD) works well for detecting it when the in distribution dataset is CIFAR and TinyImageNet is available during training as an OOD dataset. During the rebuttal period, R3 corrected it that "the main question investigated by the paper is how to best use the outlier exposure set," and this sounds better. While the observation is interesting, I think the contribution is not enough as a full ICLR paper at this point. The proposed setting with a large OOD dataset has already been proposed by [Hendrycks et al.], and the proposed method has been experimented in [Lee et al.(a)] and [Dhamija et al.], so the technical novelty of this work is limited. is not common, and the observation in this paper is limited to this setting. 2.The experiments are not thoroughly conducted. However, even with more experiments, I am not sure this work is significant enough for publication in ICLR, because of the lack of novelty in both the experimental scenario and method. Deep anomaly detection with outlier exposure. In CVPR, 2020. **After rebuttal**I d like to thank authors for their efforts to address my concerns. I didn t change my initial rating, due to the two main concerns below:(1) To me, the main argument of this paper sounds "when a large (and maybe diverse) OOD is given, adding an OOD class to the classifier is better than baselines."<BRK>  Summary: This paper presents experiments and results from using a reject class in multi class classification for the auxiliary task of out of distribution (ood) sample detection. Review: 1  The idea of using auxiliary task or reject class for ood detection has been well explored in the past. Examples include Neal et al.[1] mentioned in the paper and recently by Mohseni et al [2] which is missing from the reference. arXiv preprint arXiv:2007.09070 (2020). Strengths:  The ood detection problem is an important and interesting topic  Authors reviewed results and compared the proposed ood detection technique with multiple uncertainty based techniques. Experiments and results are limited. Contrastive training for improved out of distribution detection.<BRK>Note that unlike most works on OOD detection, they use more data, \tilde{D}_out, similar to outlier exposure. As I understand, Hendrycks et al are basically saying the (simpler) method in this paper does not work as well   but Hendrycks et al don t provide experimental evidence for this claim, it s merely stated. Conceptually, the approach in Hendrycks et al also seems more brittle and there are distinctions between these two methods (e.g.see Vernekar et al 2019, “Analysis of Confident Classifiers for Out of distribution Detection”). Results on CIFAR 10  > CIFAR 100, and CIFAR 100  > CIFAR 10 would be good. To me the decision hinges on the quality of the comparisons. The earlier work by Vernekar et all doesn’t quite explain things for high dimensional data like images. #########################################################################Summary:This paper tackles the problem of out of distribution detection. #########################################################################Reasons for score:The main technical difference compared to outlier exposure is that they classify inputs from \tilde{D}_out into a K+1 th class, whereas outlier exposure enforces that the prediction over the K classes have high entropy if the inputs are from \tilde{D}_out. In other words, I’m holding them to a higher bar experimentally, than if say their idea was novel or they had conceptual insights into the problem, because without more complete experiments the contribution to the community is limited.<BRK>I read this paper with great interest. The method involves augmenting the training dataset with  an out of distribution dataset and adding an additional class in the classification layer for out of distribution. The paper describes several experiments—some in computer vision, some in NLP—and then compares them to other OOD techniques. The results are comparable to other techniques, although the proposed technique is definitely simpler. How and why was the tiny images dataset chosen to augment the training set? What should the balance between in distribution and out of distribution images be in the training set? 3.Does the task performance remain the same? If not, then how does the OOD detector (task + OOD classifier) proposed in the paper compare against, say, a model that just learns to classify in vs out of distribution?
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 8. <BRK>**Paper Summary:** This paper proposes a decentralized federated learning algorithm called Online Push Sum (OPS) for peer to peer learning in the context of social networks with the property of single sided trust . Sec 1, it is mentioned that “Only models rather than local gradients are exchanged among clients in our algorithm.” My understanding is in most of the federated learning algorithms, either model parameters or difference of model parameters are exchanged (also noise can be added on top of them to guarantee differential privacy). I am afraid this may not be the case in practical applications. Interested to see the authors’ thoughts on this. **Novelty**The paper to me seems an incremental extension of the previous work (Zhao et al., 2019), and I think the novelty is a little thin.<BRK>I do spend lots of time and carefully read the paper. They are based on my understanding which might contain misunderstanding points if any. ## 1Major Comments:  The motivation of the manuscript is really strange. However, the explanations and discussions are falling in the edge server setting. I still cannot understand why the online push sum algorithm is a federated learning algorithm. From my perspective, it is only a generalization of the push sum algorithm in the online setting with single sided trust constrain. They provided a regret analysis of the proposed algorithm. However, the authors never showed that the contributions of the proof skill.<BRK>##########################################################################Reasons for score:  Overall, I think the current manuscript is marginally below the acceptance threshold of ICLR conference. However, the major concern is that the problem setting may not be novel enough. The paper is well written. 2.The formulation and theoretical analysis of the proposed OPS method looks promising. The major concern on the proposed OPS method is its novelty. A series of central server free federated learning algorithms have already been developed e.g.[1] and it seems the main contribution of OPS is to study one specific setting e.g.decentralized FL under the single sided trust social network graph. But it is not quite clear why the online setting is important in the decentralized FL scenario. The authors are highly encouraged to extend the scale of the experiments.<BRK>This work focuses on single sided decentralized federated learning. The authors analyze the regret bound, as well asrun some numerical experiments to demonstrate the correctness of the proposed algorithm. ***Strengths***:Overall the paper is well written. The motivation is sound. The code is very readable andwell documented. ***Weakness***:I can understand that this work focuses on the algorithm rather than provides a privacy guarantee. The authors only mention an important work in the related works section: “Notably, Zhao et al.(2019) shares a similar problem definition and theoretical result as our paper. However, single sidedcommunication is not allowed in their setting, restricting their results”. I suggest the authors discussmore about it and distinguish the contributions in theory analysis. In section 4.4, more privacy related works should be mentioned.
Reject. rating score: 5. rating score: 5. rating score: 6. <BRK>The algorithm iterates between minimizing a contrastive loss and maximizing an intrinsic reward derived from a k NN entropy estimation of the state distribution. Unfortunately, after authors clarifications, I still have some doubts on the concerns raised with C1 and C2 (see below). However, in Figure 6 MEPOL does not seem to suffer a particularly high variance. I would suggest the authors to rephrase this work to give a more central role to the scalability to high dimensional observations, which I believe is the main contribution of the paper, and to include a more thorough discussion of (Mutti et al., 2020) in the main text (beyond the related work section). Moreover, it is not completely clear from the aggregate performance where and how APT is helping in these experiments. A policy gradient method for task agnostic exploration. I believe that the multi environment pre training setting is quite interesting and, to the best of my knowledge, completely novel.<BRK>The paper presents a pre training scheme (APT) for RL with two components: contrastive representation learning and particle based entropy maximization. The proposed method is conceptually simple and intuitive, yet achieves some promising results. Is it possible to show a game breakdown to understand where these methods work better? **Novelty**: The two cores of the paper, contrastive representation learning and entropy maximization, are quite established in RL. Is it possible to just use eq.(1) as reward also?<BRK>ICML 2020. Previous works (Hazan et al., Lee et al.) The authors propose to overcome these limitations by using a particle based entropy estimate in the learned representation space. The method is novel and the experimental results are strong. How does this version of APT compare to RL from true state on DMControl? A single unsupervised pre training can be leveraged to solve multiple tasks as long as the environment does not change, as shown in the DMControl experiments.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 7. <BRK>The paper presents a novel unified model that jointly harnesses the power of graph convolutional networks and label propagation algorithms based on the unified message passing framework. The UniMP first employs graph Transformer networks to jointly propagate both feature and label information. Pros:* The presented method shows strong empirical performance on the open graph benchmark dataset. * The whole framework is simple and the idea is easy to follow. * The writing of this paper is poor. * Although the proposed UniMP achieves state of the art performance, the experiments are not convincing enough. * Experimental results are not consistent, c.f. It seems that the standalone Transformer even surpasses UniMP on ogbn products and ogbn arxiv. However, no ablation studies are provided to demonstrate the impact of these two independent components.<BRK>The authors proposed a unified message passing model to make a graph neural network to be able to incorporate both label propagation and feature propagation. Experiments on three OGBN datasets show that the proposed methods achieve promising performance. Pros:    The motivation of the paper is very clearly stated in the text and the experiments successfully show that by incorporate label propagation and feature propagation, the performance can be improved.<BRK>This paper proposed a novel unified massage passing strategy for semi supervised graph learning scenario. It combines the advantages of the GNN and label propagation algorithms. The extensive experimental results demonstrate the effectiveness of the proposed model. Cons:The insight/motivation of this method is not very significant. This paper shows empirical discussion compared with GCN LPA, while it could be better to show some insight/theoretical discussion/analysis. The improvements compared with other baselines are not significant.<BRK>Summary:The paper proposes a new Graph Transformer (UniMP) based model with the motive of combining two powerful semi supervised node classification techniques, GNN and LPA. Proposed Graph Transformer unifies feature and label propagation in conjunction to provide a better performance in semi supervised node property classification task. + This paper is able to support all the claims with rigorous experimentation. The experimentation is done using Open Graph Benchmark datasets and the proposed method achieves SOTA on all the datasets used. I personally like the simple yet effective nature of the method. Is it a linear model? Overall, I vote for accepting the paper.
Reject. rating score: 3. rating score: 4. rating score: 5. <BRK>The most critical problem is the lack of rigorous, quantitative evaluation of the proposed method. ##########################################################################Strength:  This work is trying to address a real world problem and has posed a reasonable framing  Indeed all three components of this framework are important research areas and the overall problem is also a critical application scenario  The overall presentation is easy to follow##########################################################################Weakness:  Limited technical novelty of the proposed methods  The overall execution needs significant improvements  A lot of related work is missing##########################################################################Detailed Comments:Despite the importance of the problem, unfortunately the overall technical novelty and execution of this work does not reach the standard of an ICLR research paper   therefore I vote for a clear *rejection*.<BRK>However, there are several concerns. how to formulate these conceptions? There is no definition of personalization in the promotion settings.<BRK>The paper has limited novelty as the authors use known techniques such as TCN and linear programming. More detail in comments. Is this a broken latex reference? As is, they all look to have similar offer elasticity. 8.This is a nitpick, but some of your features are "numeric" rather than "continuous" since they can only take on integer values.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 6. <BRK>This paper proposes an RL based neural architecture search approach to decrease the searching cost by introducing three modules to estimate uncertainty, restore parameters, and store old models. This paper is generally well written and well motivated, except for some unclear sentences;       Architecture knowledge is not well described. Moreover, details are missing on how to sample 100 optimal models. Even if it accelerates the search process, it entails additional memory due to the proposed module; it stores learned networks. So, I think there’s a trade off between search cost and the total memory we need to reserve. From this, I wonder reducing the search cost is more significant compared to increase the required memory. It would be great to see the results to see which component actually affects the performance.<BRK>In this paper, the authors propose to use a sampling based approach to neural architecture search, which combines a life long knowledge pool, uncertainty aware critic, architecture experience buffer. This approach has been demonstrated with vision tasks involving days of TPU training. NAS is an underexplored topic. But the papers seems like an engineering project that combines multiple existing ideas from the others  work, and there lacks theoretical depth about clear mathematical formulation of the approach, and reasoning on why the approach should work. How do we know that the experimental results is not some coincidence?<BRK>Summary: The paper propose a few improvements to the sampling based NAS using RL: 1) an uncertainty aware critic to decide whether the sample needs to be trained; 2) a life long knowledge pool to initialize the sample that needs training; and 3) an architecture experience buffer to reuse old samples for RL training. There could be more discussion on related work studying uncertainty in RL or in supervised learning, given that it is one of the core modules in the proposed pipeline and uncertainty an important topic in general. Finally the paper finishes with ablation studies on both the effectiveness and transferability of the proposed modules. The three proposed modules are novel and improve the search cost significantly while achieving better performance.<BRK>Summary This paper proposes a fast general framework (FNAS) for neural architecture search (NAS) problem to enhance the processing efficiency up to 10x times. However, the authors paid more attention to introduce the fact based on observations and the thoughts of the framework design, thus neglected the technical depth for the key component (UAC) that has highest impact on the overall performance. The proposed framework (FNAS) is general, practical and convincing. For example, the  Uncertainty Aware Critic (UAC) should be considered as the key component of FNAS framework because it was shown that the UAC has highest impact on (biggest contribution to) the overall performance in terms of efficiency in Table 4. Is there any potential loss functions or improvements that would get better performance?
Accept (Poster). rating score: 8. rating score: 7. rating score: 5. rating score: 4. <BRK>They show the superiority of their method (4 variations) by comparing it with many state of the art methods, in 144 synthetic datasets (with varying homophily and average node degree) and 17 real world datasets (again with various ). I find very interesting the idea of using self supervision to improve graph attention networks and the experiments are nicely done and convincing. The authors do an impressive work to include as much information and results as they can in the given space. Strengths:   The paper is about an interesting problem in the ICLR community. The contributions of this work include the proposed method (GANs with self supervision), but also an analysis for the selection of the best model depending on two important features of the graph (homophily and average degree). In Appendix, A.3, the description and discussion for t SNE plots is limited or absent. I would like a comment and clarification from the authors regarding Appendix A.3 Figure 5 (t SNE plots), even though it is not in the main paper submission. In the repository that the code will be released, it will be useful to also add links to all 17 public datasets to ease research in the field.<BRK>They used information in the edges as an indicator of importance of relations in the graph, then they learn the relational importance using self supervised attention. They introduce a recipe based on two graph characteristics: homophily and average degree. ********Positives  One thing I liked about this paper was thorough and neat experiments. I enjoyed the way they designed their experiments by mentioning several important questions followed by their answered backed up with their experiments. I also liked that they examined their recommendation for the choice of attention model on real world datasets, and their answer for real word data was almost similar to synthetics data. Based on the argument on page 6. The innovation is not significant; however, their experiments were interesting, and they prove how well their method works well empirically. I think this research will be useful for people in this area. ******* After RebuttalI have read the author s response<BRK>The authors then make the observation that the homophily and average degree of a graph influence the design of the attention mechanism. Extensive experiments are shown, where the various versions of SuperGAT are tested on 17 real world dataset and many synthetic ones, and these results are compared against other state of the art models (including the original GAT work). However, the paper has some weaknesses that I try to summarize below. The choice of studying two graph properties homophily and average degree seem arbitrary. This is particularly of interest due to the fact that experiments do not entirely support/explain the importance of these two properties, such as in the case of Flickr and Crocodile datasets. And even then, SuperGAT does not have the best performance across all datasets. Perhaps instead of concentrating on performance superiority, the authors can look at the explainability aspect of their proposed architecture and look deeper into other graph properties that may guide the design/use of self supervision.<BRK>In general, the paper is written well and easy to follow. However, there are still several issues the authors need to address:First, the authors need to better justify the novelty of the proposed method. The claimed self supervision task can be considered as general link prediction task where the attention weights is used as features. However, some assumptions in these research questions are questionable. For example, in RQ1, the authors claim that “ideal node representation can be generated by aggregating only neighbors with the same label”. I suggest the authors to provide more justification on this assumption. In RQ3, the authors hypothesize that “different graph attention will have different abilities to model graphs under various homophily and average degree”.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 7. <BRK>Formulation is interesting but experimental setup has various deficiencies:1. 4.DNS performance is good but best is achieved without competition mechanism, which shows that independence and competitiveness are not complementary? Also, in my opinion, increasing number of heads of TIM to match parameters with vanilla is not fair.<BRK>It is an interesting attempt to integrate this idea to Transformers. 2.I didn t get what does "mechanism" mean in Algorithm 1. 3.Experiments only demonstrate n_s   2 mechanisms. All results suggest that TIM can make some neurons focusing on the foreground, which isn t that surprising as this is most attention based models aiming at. How are they computed?<BRK>The competition mechanism benefits only some tasks. The evaluation results are not convincingSeveral issues and questions for the authors:1. The main thing that has been missing in this paper is the fine details of the approaches.<BRK>The experiments on BERT pre training and fine tuning do not bring much to the table, as the text data is relatively uniform and does not benefit much from the introduced structural inductive bias. One obvious limitation of the proposed approach is the necessity to know the number of "independent" components in the data.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 6. <BRK>## Summary    The authors propose a method for "zero shot navigation" that learns to navigate mazes from a map of the maze and the start and goal location in the maze. ## Weaknesses   My major concern about this work is that I just don t understand why this is a hard problem. Any classical planning method would have no trouble generating a plan for agent navigation from the 2 D map provided. The problem then, it seems to me, is reconciling the plan on the 2 D map with the actions that are needed to be taken in the actual environment. But given that there is a deterministic scaling between occupancy grid map and the agent world ("Each cell on the abstract map corresponds to 100 units in the agent world.") AND your agent s transition function is deterministic, then this problem is solvable zero shot with a traditional planner (e.g.one based that generates a roadmap and then searches it but others would work)  and then executing the plan. Learning the dynamic model is also straightforward because you assume that you can directly observe the the joint state $o_t$. As a result, my impression is that the solution is "over engineered".<BRK>This paper tackles the task of going to a point goal using an abstract 2D map of a given environment. The central idea is to use the given abstract 2D map to predict parameters for the transition function in the environment depicted by the map. This predicted transition function is used to search for actions to execute via planning. Strengths: The paper tackles an interesting problem, that of how to use an abstract 2D map to navigate. It proposes interesting neural architectures for solving this task. While the paper uses 3D environments from DeepMind lab, actual experiments in the paper employ a 12D agent state as input to the policies. If my understanding about the problem setup is correct, I wonder how would a hand crafted transition function based on the map do   we already know where is the free space, the starting location, the goal location, and the fixed scaling between the actual space and the map, can t we build a conservative transition model based on this information? Section 3.2 is somewhat confusing: are we learning a policy \pi_t that mimics the outcome of a monte carlo tree search? Without these details it is hard to fully understand the proposed model. In summary, I am not convinced of the experimental setup being used to study the proposed approach. Update: I thanks the authors for preparing a response and for providing additional experiments.<BRK>This submission tackles the Point Goal navigation task given access to the agent’s starting location, current state (position and velocity), the goal location and a top down map. First is MMN (Map conditioned Multi task Navigator) which is model based approach which learns to hyper network to convert map input to a transition function. Second approach is MAH (Map conditioned Ape X HER DQN) which is model free approach using Ape X DQN with Hindsight experience replay. The submission has several weaknesses:  I believe the key challenge in solving the point goal navigation task given access to a top down 2 D map is localization. This assumption simplifies the task a lot. The starting location, the goal location, and the map are given as input to the agent. The agent can use any planning algorithm like Dijkstra to plan the path to the goal location. One or more of these methods should be used as baselines. The submission uses mazes in the DeepMind Lab for evaluation. I am not convinced that performance in this environment is indicative of performance in realistic environments as realistic maps are more complex and very different from mazes. One of the desirables when learning to navigate with access to a 2 D map as opposed to navigating without maps is efficiency. The submission also lacks competitive baselines. Suggestions for improvement:  The agent should not have access to the true pose and velocity. The actions and motion should not be deterministic. Empirical evaluation in a more realistic simulation environment would be more indicative of the efficacy of the proposed method.<BRK>[/EDIT]##########################################################################Summary:This paper presents addresses the problem of zero shot navigation in environments with novel layouts. The paper also introduces n step relabelling as a way to leverage failed trials and make learning more efficient. Experiments on the DeepMind Lab environment show that both methods perform well against a random baseline and that MAH extends better to larger maps. ##########################################################################Reasons for score: This paper presents a novel approach to an interesting problem. ##########################################################################Pros:  Clarity: the paper is well structured, clear, and easy to read  Impact: the paper addresses an interesting problem, in particular trying to use a general approach that is not specific to map based navigation  Rigor: the work presented in this paper is detailed and follows a clear methodology. The experimentation study is fairly detailed and the appendix provides significant details about the methods. This work seems to achieve significantly better performance, at the "cost" of using a more task specific, map based navigation approach.This raises a few questions:1. Could the authors have used Brunner et al.as a baseline for this work? This statement would be a lot stronger if they actually proved it in the paper, i.e.if they used the same technic to solve a different problem. Could other (external) stronger baselines have been used, such as asynchronous advantage actor critic ((Mnih et al.,2016) or model free episodic control (Blundell et al., 2016)? #########################################################################Some typos:   Page 2: "betweend" in the first paragraph  Figures on page 6 are hardly legible on a printed version of the paper.
Accept (Poster). rating score: 9. rating score: 7. rating score: 6. rating score: 4. <BRK>The paper proposes a new neural net architecture based partially on the previously proposed GeneralizedPageRank (GPR). The main advantage of  this approach is that   unlike previously proposed GCNs   this approach works well for both homophilic and heterophilic graphs (due to the use of GPR). ############I recommend this paper for acceptance due to its novelty and its algorithmic contribution.<BRK>The paper proposes a new GNN architecture based on Generalized PageRank to handle two weakness in some existing GNNs: the difficulty of neighborhood aggregation on heterophilic graphs, and the oversmoothing problem when stacking GNN layers. The paper is technically sound.<BRK>In this paper, the authors proposed a generalized pagerank version of a graph neural network (GNN). please make the text in figures 3 and 4 larger. The paper is clearly written. My main concern is the overall novelty of the paper, with respect to ICLR.<BRK>Unfortunately, some of these metrics are more computationally expensive to compute for large graphs. The authors conduct extensive evaluation using bencmark datasets, and show improvements over some existing GNN methods. + Evaluation using both synthetic data based on  contextual stochastic block models (cSBM) and real world "heterophily" datasets. However, the bulk of the paper is focused on using GPR as a more "general" graph filter as in the "standard" GNNs.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 6. <BRK>Overall, I’m scoring the paper with a reject. Quantifying model information can be an important tool. However, the examples lack falsification and depth. This reviewer would wish for a more in depth treatment of the various examples in order to be thoroughly convinced. I am not sure I can follow the argument about Kolmogorov complexity.<BRK>This paper discusses the use of model information to assess properties of deep learning models and problems. I have some doubts also in Section 7, on Equation (8). In particular, it illustrates how model information may be used to capture the difficulty of a task, the degree of similarity between domains, the capacity of a model The idea of using a quantity related to codelength to assess properties of a learning model is interesting; indeed quantifying the properties chosen by the authors (task complexity, domain similarity) would be certainly useful. This particularly important as it defines the set over which the maximization is computed and with respect to which capacity is defined.<BRK>Also, it would be interesting to know if there are any potential application for the proposed descriptive statistic on task complexity. use the codelength of a dataset subtracting the cross entropy of the final model. Clarity&Originality&Significance: Below I will take Section 3 as a specific example to elaborate. Also, please clarify how it relates to mutual information, whether there is any related work that studies the approximation of mutual information between X and Y, and their relationship with the current work.<BRK>How would task difficulty behave in the small data regime? **Ablation**  How do we know that quantity defined in equation (7) truly captures how important a component is and that we should trust the conclusions of experiments presented in Fig.7?**Minor notes**  To make the paper more self contained, please mention in the main text which definition of model information is used in the experiments. How do these domain similarity measures compare to other measures, such as domain similarity computed by Task2Vec [3]. "For example, if $S^\text{uni}(A, B) < 1$ and $S^\text{uni}(B, A)   1$, then one can tell that A is a subset of B."
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 7. <BRK>This paper introduces a randomized entity based attentional mechanism to regularize the observation space for efficient multi agent reinforcement learning. Specifically, the authors expect that their method can help agents focus on entities that are relevant to their decision making process. However, I have some doubts about whether the proposed method can address the target of the paper. I was expecting that the authors provide a thorough comparison with [Agarwal et al., 2020] in their experiments.<BRK>This paper proposes an observation factorization method to avoid the influence of the irrelevant part on value estimation. [1] Roma: Multi agent reinforcement learning with emergent roles. The authors mentioned there are two groups of entities. *****Some specific comments:*****It is not clear that what is the initialization of two masks, and how to update the masks.<BRK>Summary: This paper proposes to incorporate a masked attention mechanism in QMIX for value function factorization to disentangle value predictions from irrelevant agents/entities. The paper conducts experiments on a simple game to understand the effect, and then test on 3 SMAC games, which shows the effectiveness of the proposed REFIL method. Concerns:The main focus of the paper is to “disentangle value predictions from irrelevant entities”.<BRK>The paper proposes a method for randomized factorization of multi agents for efficient learning. Since the groups are randomized, this helps the agent to create groups of variable size based on its utility prediction of in group and out of group entities. Although the paper only uses two groups for derivation and experiments, it claims that the same method can be applied to more than two groups but is yet to be demonstrated.
Reject. rating score: 5. rating score: 6. rating score: 7. <BRK>Summary:The authors propose a hierarchical model of neural ODEs, which they fit to CIFAR10. They find performance on par with ResNets, and include qualitative analyses on the filters learned by the models, their ability to fill in occluded features, and robustness to contrast at test time. Strengths:The filter parameterization is interesting. I can imagine this improving sample efficiency in certain contexts — perhaps the authors should seek out those kinds of tasks to complement their CIFAR results? The discussion does a nice job of explaining the issues that neural ODEs have when scaling to large image datasets. Why?Your models are applied to 2d images. Maybe change "blocks" to a citation? Or V1/V2 depending on which implementation it is (unclear from the text). There s essentially no difference between the performance of any of the models tested. It is important to show that the proposed method does *something* different than the standard ResNet. The authors attempted to add some qualitative experiments towards this goal in Fig 4, but those results are not very convincing.<BRK>I didn’t understand “… time (or network depth)” on pg. The paper develops a spatio temporal network that is defined in terms of continuous spatial functions and continuous temporal dynamics. List strong and weak points of the paper. Deep convolutional networks are increasingly used in brain modelling, but they are somewhat disconnected from earlier computational neuroscience in ways that this paper tries to address. Performance of the model and baselines on CIFAR 10 is not strong, which raises the question of how compatible the approach is with higher performance. The choice of CIFAR 10 as a test of the approach does not seem to be well motivated. Clearly state your recommendation (accept or reject) with one or two key reasons for this choice. I recommend to accept the paper. There is a growing body of work that compares deep CNNs to biological neural networks, and the conventions of discrete time and space in CNNs unfortunately distance this work somewhat from much previous computational neuroscience. Ask questions you would like answered by the authors to help you clarify your understanding of the paper and provide the additional evidence you need to be confident in your assessment. Could you please expand on the motivation for continuous space with respect to biological vision? Analytic tractability is mentioned on page 1 as an advantage of some continuous models in neuroscience, and it seems to be implied at that point that such benefits are sought in the paper, but it doesn’t seem that the approach ultimately offers much hope in this sense. If there is some potential here, please expand.<BRK>## Review### SummaryThe authors define continuous deep networks by expressing 2D convolutional filters as a linear combination of Gaussian function and its derivatives. By combining this description with the previously proposed neural ODE framework they obtain a spatio temporally continuous description of a convolutional neural network. There are 3 main contributions:  1. they are able to estimate the support width of the filters and they show that  it increases with the network depth as observed in the visual cortex  2. they show that their network performs as well as alternative non continuous neural networks on CIFAR 10 while having less parameters  3. they exploit the temporal dynamics to resolve a pattern completion taskOverall, I think the work is good but I am not as enthusiastic as the authors about the importance of the work for neuroscience and machine learning. ### Strengths* The paper is well written and easy to understand. The goals and contributions of the work are clearly stated. * The qualitative results (filter support width, pattern completion and contrast robustness) are interesting and relevant to neuroscience### Weaknesses* I fail to understand how the work could be relevant to neuroscience beyond what is presented here. It would be great to reconstruct the missing part of the input. * The increase of filter support width with network depth correlates with what is known for the visual cortex but I fail to understand how it could be relevant to current work in experimental neuroscience. What is the benefit of learning continuously changing support for machine learning ? About the relation to biology, the increase in size might be more related to the specific task on which the network is trained than to what is observed in the visual cortex. * The observed contrast robustness is not compared to other neural networks nor discussed in the light of experimental neuroscience observations. It should be the same size as the main text.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 7. <BRK>Summary:This paper presents a new activation function "Leaky Tiling Activation", designed with the goal of learning sparse representations (where only a few units activate for a given input). This is trying to game the page limitations. Reasons for score:Although I have some issues with the sparsity claims of this paper (see my comments below), I found the paper intriguing and I am curious to try similar ideas in other problems. page 5: it seems all results are in the appendix, which is unfortunate, as papers should stand on their own and appendix only used for additional explanation, not for main results.<BRK>This paper proposes a novel activation function based on tiling, with a careful consideration of the tradeoff between differentiable regions of the tiling (which would otherwise be an undifferentiable one hot) and sparsity. "such encodings can enable faster learning and reduce interference", that may be true, but the authors need to test this. If it s not clear what I mean, let s say we have 100 LTA units with k 4 and so 400 outputs. When a new method outperforms an old method in settings where it shouldn t, it s often because the setting is unfair to the old method.<BRK>I hope that this includes the code for the experiments. While the proposed new activation function may be useful in some settings, there is not enough evidence in the paper that it would become a go to solution, or significantly change the way we think about interference in continual learning and RL. It presents a novel method to combat this problem, and demonstrates its usefulness in a number of experiments. The authors could have benchmarked against stronger baselines. As it is, the paper s work is not reproducible.<BRK>(I will keep my review short, as I don t have much constructive criticisms to share)The authors propose a new activation function that guarantees sparsity, which can reduce interference/forgetting when learning on non stationary data. Next, LTA is experimentally evaluated in online supervised learning on non stationary data, as well as in reinforcement learning (RL). This is why I chose a confidence score of 4. I am willing to keep my score and fight for acceptance if the authors commit to releasing the code upon acceptance. _________**Post rebuttal**I am happy with the response.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 7. <BRK>It is easy to read. 3.The qualitative results show better depth estimation in object boundary areas. “How to generate semantic labels” is a very important problem in this paper. 5.After reading (Zhu et al., 2019), I find that the ground truth semantic labels are still required, and their method can be regarded as a data augmentation. In my opinion, the former one is more expensive than the latter, so the requirement of training the proposed model is higher than existing supervised methods. b.By comparing the results, I find that the proposed method is not as good as supervised methods or self supervised methods that use stereo pairs for training. c. Due to the requirement for ground truth semantics, the claim of self supervision is not convincing. 6.The proposed method leads to very limited performance improvement, e.g., final results (absrel 0.105) vs baseline results (absrel 0.110) as shown in the Table 2, while it requires additional semantic labels for training. Considering the trade off between performance improvement and requirement for training data, I have serious concerns about the contribution of this paper. This paper improves that in depth estimation by using the well labeled semantic maps. However, in practice, obtaining accurate semantic labels is difficult, and this is also a challlenging problem for semantic segmentation. post rebuttal: Authors pay much effort on addressing issues that I mentioned, and I appreciate that very much. My issues can be partially resoved, but the main issue is regarding the high level thinking on semantic supervisions, which is impossible to be fully addressed in a rebuttal. Besides, I agree with AC that (a) the improvement is very limited and (b) semantic labels are hard to obtain.<BRK>The paper presents a method for semantic guided self supervised depth estimation from monocular images. They propose semantic guidance to improve depth estimation performance. This is obtained by applying semantic guidance at multiple levels in the decoder via an attention layer and via feature enhancement in edges areas of the image. Experiments show the proposed method reaches state of the art performance for self supervised monocular depth estimation in structure from motion setting. Pros  given semantic guidance, this method seems well designed and effective to enhance depth estimation performance with a relatively low computation overhead in order to provide semantic guidance  interesting semantic guided edge point sampling strategy  the paper shows an improved semantic guidance with respect to Guinzilini et al 2020bCons  not clear how the decoder was designed, why on some layer there is attention and on other concatenation + convolution? Could the authors give some explanation about it? Second, "point feature enhancement module, which merges and enhances these point representations via a set of 1 D convolutions", the PFE is not clear, set of 1 D convolution means that for each channel of the feature map there is a convolution? Eq.8, i \in [2,4] is related to the darker box around the decoder blocks in Fig.1?the training set for Cityscape is the train split with fine annotations or also with coarse annotations? Overall, the paper is well written, the results are sound and minor improvements could make it worth acceptance.<BRK>Summary:This paper proposed a novel framework to improve self supervised monocular depth estimation leveraging semantic features at local and global level. The proposed framework includes a semantic guided edge enhancement module to extract and enhance point based features around semantic boundaries. Evaluation on KITTI datasets compared with recent state of the arts are provided. The proposed method outperformed other monocular depth estimation methods including the ones that also leverage semantic information. Cons:* The cityscapes experiment provides very limited contribution to the paper as only qualitative results are provided. * The prediction accuracy of semantic segmentation branches is missing. Even Though it is trained with pseudo labels, it is still important to provide this result in order to justify that the semantic branch is providing reasonable predictions. * There is some other confusing description in the experiment section that raises concerns:    The paper keeps mentioning that the semantic pseudo labels are generated from off the shield algorithms which require no groundtruth. This argument is confusing as the pretrained model from reference work did use ground truth. The authors mentioned semantic labels for the test set of KITTI and Cityscapes. Why do you need to use these labels for testing when no semantic quantitative result is provided? Please provide more details there. Other detail comments:  In related work, self supervised depth estimation, I think it would be good to mention the effort of network architecture improvement for depth estimation (E.g.Guizilini 2019a). Then emphasize that the experiment comparison rules out the impact of different network architecture in either related work or experiment section, given that the main result in Guizelini 2019b using another network is not compared in the table. It is a simple sum up or weighted? The description “insufficient depth representation” is a bit confusing. The solution to “insufficient” should be “providing more”, while the paper is improving and augmenting the representation.<BRK>The authors tackle the problem of self supervised depth estimation and particularly address the issue of poor depth estimation on object boundaries. The authors propose two main modifications allowing them to leverage an off the shelf semantic segmentation network: SEEM (semantic guided edge enhancement module) and a multi level self attention mechanism that fuses depth and semantic features at different levels. Overall the paper is well written and the method clearly explained; the 2 diagrams in Figures 1 and 2 are very well done and explain the workings of the system quite well. The authors ablate their contributions clearly showing their respective impact on the overall performance. The evaluation on KITTI shows that the method compares favorably with related work. The presentation could be improved by addressing the points mentioned below, and specifically: (i) it would be interesting to see how the quality of the off the shelf semantic segmentation network affects the depth results (see below) and (ii) it would be nice to see a per semantic class evaluation (similar to Fig 4 in  (Guizilini et al., 2020b)) showing specific improvements after using the proposed modifications. * What data is the pretrained semantic segmentation network of Zhu et al.(2019) trained on? Could the authors report the performance of the network on the original training dataset, for reference? Guizilini et al.(2020b) report that their pre trained semantic segmentation network achieves an mIoU of 75% on the Cityscapes val set; how does that compare to the network of Zhu et al.(2019) used by the authors? * Table 1   the numbers reported for Guizilini et al.(2020b) don’t seem to include the PackNet numbers reported in that paper (i.e.the abs_rel reported for 640x192 seems to be 0.102); while that network architecture is different, I would argue it’s still relevant as a comparison with state of the art methods* Table 3 in the supplementary is missing the Guizilini et al.(2020a) numbers on the KITTI improved ground truth. * The paper is missing some details regarding the network architectures used   the authors should provide this information if possible, either in the main text or in the supplementary* It would also be interesting to see the performance of the method with using full resolution images   would using semantic segmentation further help improve results in that setting? Minor comments:* Table 1 caption “D” denotes depth supervision   since there is no method using Depth supervision in the table this can be removed* It would be nice if the authors could add F_E, F_D and F_S on Fig 1 to make it easier to follow the description in Sec 3.2* Setting i \in [2,4] means using the self attention twice? Post rebuttal: I thank the authors for their detailed response and paper revision. My concerns have been addressed, and I particularly appreciate the experiments presented Table 3, Figure 5 and Section 5.4.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 7. <BRK>“90% attributes” should be “90% of attributes”. So it’s not clear what the authors meant by this. Relationship to prior work: The state of the art alternatives have not been well addressed and compared with. It seems that the performance of the proposed method is exactly the same in both settings.<BRK>The idea in this work seems to be a combination of techniques from different previous works. Third, the experimental section is not persuasive. The paper is written well and easy to follow. These better baselines should be used.<BRK>Overall, the idea is interesting however the writing of technical ingredients is not convinced enough. There are points not clear and can be improved to get a better version of the paper.<BRK>None of these components is novel in this setting but the combined algorithm is interesting and the provided code is helpful and demonstrates the naturalness and ease of implementation of the proposed model. The typesetting of text in equations should be placed in something like \operatorname so that it isn t squashed and italicized.
Reject. rating score: 5. rating score: 6. rating score: 6. <BRK>[Critiques and weaknesses]1. Instead of element wise MCMC, LAE collected samples from the posterior using the amortized Langevin dynamics of a potential energy distribution. 2.Although the author claimed that LAE is able to collect samples efficiently from the posterior, the runtime comparison in large scale data was not shown in the result section. The proposed model replaces the posterior in VAE with a potential energy distribution, enabling fast sampling with its Langevin function.<BRK># Summarize what the paper claims to contributeThe papers introduce an amortisation for inference by Langevin dynamics (LD). # Strong points:+ The idea to amortise inference for a collection of posterior distributions is not new, but I have not seen before its extension to posteriors induced by sampling dynamics, so this paper makes an interesting proposal. The toy experimental results are helpful and demonstrate the power of the amortised Langevin. # Comments on experiments. The authors claim this but did not show results on the comparison. Due to limited time, I wasn t able to quickly follow up on the discussion.<BRK>The authors proposed a method for amortizing latent variable sampling that is applicable to a variety of problems. The paper is relatively easy to follow. 2.A key idea in this paper is to find parameters of a parametrized mapping between data $x$ and latent variables $z$ using Langevin like steps. The updates to the parameters and to the latent variables are done alternately.
Accept (Poster). rating score: 8. rating score: 8. rating score: 6. rating score: 5. rating score: 3. <BRK>Convincing experiments are also provided to demonstrate the effectiveness of the proposed variance reduction algorithm. Overall evaluation: This paper is reasonably well written and presents interesting technical results. The Greedy GQ algorithm is an important and efficient value based approach for off policy control. Detailed comments: In the existing study, variance reduction techniques have been successfully applied to value based TD learning algorithms for policy evaluation (e.g., VRTD, VRTDC), but they have not been explored by value based algorithms for control, especially in the off policy setting with Markovian samples. Hence, a larger M should gives smaller error terms, and this is also suggested by the bounds in Theorem 4.5. In particular, the coefficient c_t is specially chosen so that the quadratic term on theta can be totally absorbed into the Lyapunov function for telescoping.<BRK>### Summary:The paper introduces a variance reduced version of the Greedy GQ algorithm for off policy control, based on SVRG. The theoretical findings are supported by some experiments. ### Pros:  Proposed improvement to existing method  Theoretical support for proposed improvement  Markovian noise assumption  Some new techniques in the theoretical analysis  Well written### Cons:  SVRG setup involving inner and outer loop conflicts with the original motivation for Greedy GQ (online and incremental latent learning)#### Clarity/Quality:The paper is well written and not too difficult to follow despite the complicated topics. ### Suggestions for improvement:  In the related work section, it could be helpful to explicitly write how the related work is different from the current work. Least squares policy iteration.<BRK>This paper proposes a variance reduced Greedy GQ algorithm and proves that it enjoys a lower convergence rate compared with vanilla Greedy GQ. The paper is well written and clearly presented. In the algorithm, what specific policy improvement is used? In other words, the convergence rate in Theorem 4.5 is not for the proposed algorithm. In this sense, the convergence results may not be as useful as other algorithms that can guarantee the convergence of the last iterate of the average iterate. It would also be interesting to see whether the proposed variance reduced Greedy GQ method can match the performance of off policy policy gradient (or actor critic) methods and variance reduced policy gradient methods in controlling problems.<BRK>This paper combines a widely used variance reduction technique SVRG with the greedy GQ. It provides a finite time analysis of the proposed algorithm  in the off policy and Markovian sampling setting (convergence to the stationary point) and improves the sample complexity from the order $\epsilon^{ 3}$ to $\epsilon^{ 2}$ comparing with the vanilla greedy GQ. cons:1.The main contribution of this paper is its theoretical analysis. 2.I am not sure whether the variance reduction technique is useful in practice. There are some evidences that SVRG does not work well in the training of deep learning problem.<BRK>While Greedy GQ asymptotically converges to a stationary point, it does so with high sample complexity. My major concern is that the paper uses SVRG for variance reduction, which is not the state of the art method. The reviewer strongly suggests the author to do so. 4.Wordings of the experiments in 6.2 is a bit confusing to me. After the target policy is generated via the uniform distribution, is it getting improved as the off policy control algorithm should, or is it only evaluated against the behavior policy? However, the proposed algorithm doesn’t seem to be related to it. 4.In section 5 step 3, it is better to say lemma D.7 and D.9 is in the Appenix. These two algorithms are very similar, but they are not identical, as pointed out by several papers.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 7. <BRK>Summary The paper studies the importance of similarity between augmentations and corruptions for improving performance on those corruptions. To measure the distance between the augmentation and corruption distributions, the paper proposes a new metric, Minimal Sample Distance (MSD), which is the perceptual similarity between an average corruption and the closest augmentation from a finite set of samples sampled from the augmented data distribution. I appreciate the authors  efforts towards the rebuttal. This seems to be lacking at this time in the work. * Regarding the robustness of MSD to the choice of feature extractor (as also asked by R1), the revised paper includes results on VGG as feature extractor (thanks to the authors for this), but uses a model that is finetuned for CIFAR 10. However, this notion of the relation between augmentations and test time corruptions does not seem very surprising. It’s perhaps well known that DNNs will generalize well only when test distributions are fairly similar to training distributions. Would we need a Imagenet C bar bar to check the goodness for corruptions that may be beyond perceptual similarity (such as stylized transforms)? This is not very convincing. * The utility of Imagenet C bar as an additional benchmark to check the goodness of performance on Imagenet C seems a bit convoluted. However, the paper chooses to use WRN 40 10 trained on CIFAR 10. The paper should explain the rationale behind their choice. The practical utility of ImageNet C bar seems limited and unclear.<BRK>This paper proposes ImageNet \bar{C} which uses a smaller number of carefully chosen corruptions, compared to ImageNet C. The authors try to argue that previous work is overfitting to ImageNet C. They claim "overfitting indeed occurs." "Is Robustness Robust?" It seems like the answer is "yes" but the authors are trying to argue that the answer is "no." If these papers have very different ratings, then there s a problem with this review process. "Second, AutoAugment is the only tested augmentation scheme that was designed before the release of ImageNet C"AugMix uses a proper subset of AutoAugment s augmentations. It s not as though AugMix added in distortions to fit ImageNet C; it removes augmentations because AA fits some of ImageNet C s corruptions directly. "having a held out test set that is not used during model development seems necessary." This section might be negligent or worse for not acknowledging the already existent ImageNet C validation set. ImageNet C provides a validation set with about half the corruptions of ImageNet \bar{C}. There are 19 available ImageNet C corruptions, so the community already has a validation set. An example is "The Frechet Distance of training and test distribution predicts the generalization gap."<BRK>The paper introduces the Minimal Sample Distance (MSD): a measure of the minimal distance, in a trained network representation space, between samples modified with an augmentation and the average of all samples modified by a corruption. This way, it claims that focusing on benchmarks like ImageNet C may lead to overfitting to the corruptions present in that benchmark. One problem is that this correlation isn’t true for all augmentations. Only 4 are highlighted in the main text. Additionally, why was AutoAugment, but not RandAugment tested? The paper then suggests that one solution would be to use MSD to sample dissimilar corruptions to test on. However, given that it seems like there’s no evidence of overfitting for augmentation policies that encourage a diversity of augmentations, such as AugMix (which is in line with Yin et al 2019). Then I’m not sure what the benefit of expanding the robustness benchmark is. Overall, the paper presents interesting results and discussion. Update after rebuttal: I appreciate the authors  response and clarifications.<BRK>The paper introduces a metric to quantify the perceptual similarity between different kind of corruptions and uses it to show that training on a corruptions induces robustness to other corruptions which are perceptually similar. Analyzing the current data augmentation based methods for robustness using this lens, the authors hypothesis that we are currently overfitting to the existing robustness benchmark (ImageNet C) and proposes a new benchmark with a new set of corruptions. I think the author s observations that training on some corruptions helps the network to be robust to similar corruptions in test time is quite intuitive. I appreciate that the paper tries to quantify this and performs an extensive empirical study. What are the augmentations that were used while training the network used to extract features for the metric? Does the metric will probably depend on the architecture of the network used as well. I am not sure if this is in the supplementary material somewhere and I missed it. Update after rebuttal: I appreciate the author response.
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 7. <BRK>The authors demonstrate that their objective yields relatively good results on image modelling, compared to other standard VAE methods. I think the paper is generally well written, with sufficient clarity around the major hypotheses and results. The variance of the output distribution is substituted by the current expected MSE of the reconstructions, which is its optimal value at any point in the optimisation. This results in the variance parameter becoming implicit. There is an empirical comparison to the more obvious approach of just optimizing the variance parameter using gradient descent along with the neural network parameters. However, it is in practice quite rare to actually have continuous data. Another issue I have is that the experimental verification of the AR ELBO deals only with the sample quality (of either reconstructions or true samples), but this is not the full picture of a VAEs performance. Overall, I quite like the simple and well presented nature of the paper, but the limited scope as raised above means I think the paper should be rejected in its current form. I would be willing to increase my score if the authors addressed some of the concerns I have raised.<BRK>This paper studies the Gaussian VAE and figures out that the decoder variance regularizes the VAE and affects the model smoothness, and an inappropriate estimation of this parameter would raise posterior collapse, which is supported by theoretical analysis and empirical demonstrations. Overall, the idea is interesting and provides some new insights for our community. Since the variance parameter affects the performance of VAE, this paper proposes an adaptive training strategy using alternative updating of variance and the remaining parameters. The authors are suggested to make more explanations. From the comparison results in Table 3, it seems that the parameterizations of variance have a large impact on the performance of the proposed VAE.<BRK>It is argued that a major contributing factor is a mismatched output variance parameter. A simple procedure, where a maximum likelihood estimate of the noise is employed, is proposed. Experiments on some standard datasets provide an empirical evaluation of the method. The paper is in general well written and fairly easy to follow, at least for readers familiar with the Gaussian VAE. 2.Since this is not the first work to address estimating the noise variance in a VAE setting, I would like to have seen a more direct comparison with competing methods. This could include results on how precisely the variance is estimated and how the procedure influences the convergence of other parameters. 3.The novelty of the proposed method is fairly limited   I would expect that learning the noise is common practice in applied work.<BRK>In particular, the authors analyse the impact of the usually fixed covariance $\sigma_x$ of the decoder Gaussian on the learned encoder variance. They show that the former can be seen as a regulariser for the latter and therefore impacts the "smoothness" of the encoder. To achieve this, they propose to optimize the ELBO objective by a block coordinate descent approach where the parameters of the decoder covariance are considered as a separate block. The possibly remaining prior posterior mismatch in the latent distributions is mitigated by an second stage VAE as e.g.in Dai and Wipf, 2019. The paper is well written and technically correct. In these models, usually all(!)
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 6. <BRK>**Summary**The authors propose KFL, an efficient reparametrization of monotonic lattice regression using Kronecker factorization. The goal is to achieve efficiency both in terms of computations and in terms of the number of parameters. The authors show that the proposed KFL has storage and computational costs that scale linearly in the number of input features. The authors also provide necessary and sufficient conditions for a KFL model to be monotonic with respect to some features.<BRK>The paper proposes to use Kronecker factorization on a lattice for a monotonic lattice. Thanks to the factorization, the computational cost is improved to linear compared to exponential. I am not familiar with the motivation for monotonic functions and why the datasets/tasks such as query result matching requires the resulting function to satisfy monotonicity. For this reason, I am not sure about the significance of the work.<BRK>In this paper, the authors study statistical models based on the Kronecker factored lattice (KFL) and investigated the condition that the monotonicity holds. Some numerical experiments indicate that the KFL trains faster with fewer parameters with comparable prediction accuracy to existing methods. The paper provides a computationally efficient method for the modeling of monotonic functions. Proposition 2 is an interesting result. However, more detailed studies on the shape would be necessary for the publication. For example, is it possible to derive the condition that the function is increasing in a variable and convex in the other variable? In the numerical experiments, the authors found that the hyperparameter V is important to tune the capacity of the proposed statistical model.<BRK>Gupta et al.(2016) also proposed a method for learning monotonic function using a function that interpolates lattice points linearly. I think the authors answered my questions. In the experiments, this paper learns the monotonic function using the ensemble of KFL, whose weak learners are monotonic. **Summary of the Paper**This paper proposed Kronecker Factored Lattice (KFL) for learning monotonic functions, which is computationally efficient than the existing method in terms of input dimension. This paper used three datasets in experiments. Claim 1: The computational and storage cost of the proposed method is efficient. (Contribution 1, 4)  Claim 2: We can use KFL to learn monotonic function (Contribution 2)  Claim 3: The ensemble of KFL is sufficiently expressive (Contribution 3)  Claim 4: The proposed method empirically performs well (Contribution 4)**Soundness of the claims**Can theory support the claim? The proposed method is $O(D)$ evaluation time and $O(\sum_d \mathcal{V}[d])$  parameters, which is typically linear in $D$. Claim 3 is supported by Proposition 3.
Reject. rating score: 3. rating score: 3. rating score: 5. rating score: 5. <BRK>In particular, it is not reflective of policy performance, as it fails to account for the fact that the future states in the imagined rollout are also functions of the policy. Regular actor update using the critic at the current state2. Another regular actor update from the last state in the rollout. I would like the authors to explicitly address these issues in the paper and present a clear explanation of why their FORK should be a better policy update. With regards to their comment on how their method is somehow simpler?<BRK>Specifically, the authors propose a model based reinforcement learning method on top of actor critic methods. Therefore, I’m not convinced about the novelty of the proposed method. ### CommentsThe paper is well written and the idea proposed in this paper is really easy to understand.<BRK>Specifically, the paper proposes to learn auxiliary models of environment rewards and dynamics and use a two step rollout from these models during the computation of the policy gradient. In terms of motivation for the method, I was not entirely convinced of why the proposed update is needed. I appreciate that the authors didn t just evaluate on the extremely common (and somewhat saturated) MuJoCo benchmarks, and presented additional results on Bipedal Walker.<BRK>The authors demonstrated that algorithms such as TD3, DDPG, and SAC can all benefit from their approach. Pros:The paper is clearly written and easy to understand. The results indicated that the training sample efficiency and final policy performance of the tested algorithms have improved for 6 benchmark tasks, compared with their "vanilla" version baseline.
Reject. rating score: 3. rating score: 3. rating score: 6. rating score: 6. rating score: 7. <BRK>The main contribution of this paper is that they use GAN to generate a 3D volume MRI in which the face has been remodeled rather than removed. Experiment results showed the proposed method outperforms several existing removed based methods.<BRK>The paper is well written, and seems to solve the proposed problem well. Part of your method requires or extracts the brain from the MRI. I also have an issue with the experiment for assessing privacy preservation.<BRK>The authors proposed a conditional GAN based de identification method for MRI scans. However, the applied methods lack enough novelty in the aspect of deep learning theory. C GAN that can manipulate the generation results have been proposed and applied to a lot of tasks in recent years.<BRK>Authors can improve the presentation to clarify these concerns. • The presented de identification framework is non reversible and also preserves medically sensitive regions. Cons.• While experimental results supports author’s claims regarding privacy preservation without adversely affecting the medically relevant features, further justification is required for the superiority of the proposed method over existing algorithms. For instance, while with the DEFACE method illustrated in Figure 1 it seems impossible to identify the patient, the de identification quality for DEFACE in Figure 3 does not agree with that.<BRK>The methodology development and the experimental validation are well done. The scope of the paper is very relevant to the community. * Page 6, Benchmark De Identification methods: The authors state that the comparison methods are used “day to day basis” in the clinic. Where does that come into play with regard to the DeID method?
Reject. rating score: 4. rating score: 4. rating score: 7. rating score: 8. <BRK>The authors propose a biological model for loss function differentiation which reuses a single pool of neurons to compute predictions and prediction errors (i.e., derivatives of a loss function with respect to the neural acitivity variables) in two sequential phases. I have a major concern that I need to see addressed by the authors:  In my understanding the proposed method is not local in time and this is not at all clear, the way the paper is written. This is best seen on the last line of Algorithm 1: the apparently innocuous term $\frac{\partial x^l}{\partial W^l}$ should be evaluated at the _feedforward activation values_, not using the equilibrium $x^l$. Note that this is very different from dropping $f^\prime$ (as per footnote 5) from the gradient, something that has been experimented with in some of the feedback alignment implementations in the past (for example, some of the simulations in Lillicrap et al., 2016, do not backpropagate $f^\prime$, although the local ${f^\prime}^l$ is still used when updating $W^l$). A proper evaluation of the significance of the advance brought by the method requires clarifying this issue. I leave some additional comments for the authors:  "Given that backprop provides an optimal solution to this problem (Baldi & Sadowski, 2016)"The claim that backprop provides an optimal solution to the credit assignment problem is a very strong one, and I m not sure that it is supported by the cited reference. In the equation below (2), should $x_i$ be $x^l? How is (11) related to the dynamics of the algorithm? Edit after rebuttal:I am sorry for not yet being able to support acceptance of this paper. I appreciate the authors  work during the rebuttal, and I agree that the execution of the paper improved in this version. Most importantly, the algorithm is very similar to exact standard backpropagation, and it still requires the same coordinated phases as backpropagation: the forward phase and the backward phase. The learning rule is non local in time and while it does not use activity differences, that is not necessarily a good thing. The authors verify that learning still works after important approximations are made (notably to avoid weight transport), but these approximations were already previously reported and are more or less obviously applicable here, given how close activation relaxation is to standard backpropagation. Finally, I really recommend the authors to more explicitly state and discuss the temporal non locality of the algorithm when introducing the method (and in Algorithm 1), which remains insufficiently clear to me.<BRK>## SummaryThe authors suggest a "local" algorithm to replace backpropagation in feedforward neural networks. Therefore there is no need for a second implicit error network or error neurons. This method introduces dynamics in the neuron to converge towards an equilibrium and remark that, at equilibrium, the activity transmitted backward becomes equal to the back propagated gradients. The method is tested on MNIST and fashion MNIST. ## Critical reviewThe methods seems extremely similar to equilibrium propagation and the account of this resemblance does not seem to be acknowledged in a fair manner. Rather the introduction summarized a large spectrum of plausible learning rules which are more or less related to AR and the authors do not comment in a clear way about the technical resemblance with these algorithms. The derivation originate from an interesting observation at equilibrium, but this is certainly not as rigorous as the proofs given for equilibrium propagation. Equilibrium propagation has been demonstrated to work on tasks harder than MNIST and fashion MNIST. The simulations are not sufficient to show if the method is competitive or practicable. A good back prop implementation is expected to reach 98.2% accuracy on the test set on MNIST. I do not know whether the BP baseline reaches this level in this paper. I did not undertand what is meant by "the Jacobian is negative definite".<BRK>After carefully reading other reviews and quick modifications introduced by authors, I believe this work is richer and has shown some potential towards building scalable and robust alternative to BP. Thanks for including angles as suggested, this further supports  hypothesis proposed in this work. 2.3] Did you perform multiple trails on your experiments? It seems that other reviewers do not appreciate that training a network using hebbian like updates and without  BP requires some nontrivial engineering tricks and theoretical considerations which are now well described in this paper (updated manuscript). A better justification on non local updates as raised by other reviewers is required to further strengthen this work. But i liked the results with activity relaxation and how close gradients are with respect to BP. Current manuscript does not show any comparison with other bio inspired approaches (DFA, FA, DTP, DTP sigma, LRA, Weight mirroring). If goal is to show model is robust, close to backprop updates and gradients, then it is better to show comparison with these approaches or at least show angle to understand closeness w.r.t BP updates. It is been argued that bio inspired approaches (DFA, FA, DTP) struggle to match backprop performance when evaluated on large scale dataset with deep CNNs[Bartunov 18] . ## Minor commentsFigure 4 c) change angls > anglesIn appendix, change caption of fig 5a) from MNOST to MNIST, I believe it is a typoAdd results with FA or DFA with fixed and learnable weights, will further support robustness and closeness of BP claim in this work. However recently weight mirroring [Akrout 19] and LRA[Ororbia and Mali 19]  have shown that they can come closer to backprop in terms of performance on large scale datasets. Unlike prior work on bio inspired learning, AR only utilizes single neuron to compute its gradient helpful for neural circuitry. But in this current manuscript, there are no results w.r.t CNNs and updates w.r.t.filters when AR is deployed on such challenging visual recognition tasks. and Lillicrap, T., 2018. Assessing the scalability of biologically motivated deep learning algorithms and architectures. ## ReviewMost of the paper is clear, but experiment section needs more work in approving the hypothesis w.r.t AR. It is better to show model update angles w.r.t to BP, DFA and other bio inspired approaches. What happens to the model at various initialization and how does it behave when tested with various model hyper parameter changes?<BRK>The authors propose an algorithm for estimating the correct backprop gradients that is described as biologically plausible. I have very little to contribute as I think this is a clearly written paper — as such I recommend acceptance. I would be happier if the authors gave a very very strict definition of the kind of plausibility they wish to capture with falsifiability criteria and deep theoretical underpinnings, as I think it s a bit of a semantic trap without reference to some explicit level of analysis, etc. Minor: A paper the authors might appreciate is, given some of the work they mention to motivate their neuro/bio plausible claims: Xu, Y., & Vaziri Pashkam, M. (2020). Limited correspondence in visual representation between the human brain and convolutional neural networks.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 5. <BRK>  Summary  This paper proposes a "decentralized" method for representation learning in knowledge graphs that doesn t explicitly depend on a learned embedding for the entity node of interest, e_i. Rather, the embedding for e_i is constructed in a distributed fashion (similar in motivation to the distributional hypothesis/skip gram word embeddings) from its neighbors via a second order attention mechanism. The main idea is that this is better for "cold start" problems in which unknown entities might have no features, which makes building any representation that explicitly depends on entity centric features hard. It is also not clear from the paper why this method should work better than simpler baselines (see concerns). Some mixed empirical results w.r.t.other compared models, but generally positive. Another approach would be to use a framework reminiscent of label propagation to deal only with imputing missing features at test time. Fully dropping all entity specific features seems like overkill, and potentially harmful. After all, at test time, a majority of the KG will be known. In general, in addition to the comment above, though I am not intimately familiar with their details, it appears that none of the considered baseline methods (AliNet, RSN, etc) are specifically designed to accommodate missing entities. I feel that it would benefit greatly from an overall more compelling re write. Lemmas 1 and 2 don t add much to the paper in my opinion I would recommend moving them fully to the appendix. Response After Rebuttal  I thank the authors for their responses to my comments. After reading the response as well as the other reviews, I still stand by my original rating.<BRK>This work proposes a GNN based model for learning KG embeddings purely from the embeddings of its neighbours, which would enable learning entity embeddings for previously unseen entities. I encourage the authors to revise the paper and resubmit to the next big ML/graphs conference. The score function of other KG embedding models mentioned (e.g.ConvE, Dettmers et al.2018 or SimplE, Kazemi & Poole 2018) is of a different type and does not even impose the analogical property of relations. Sec.3.1\"Intuitively, if e_i is the embedding of an unseen entity, it is rarely useful in computing the attention scores (as it is just a randomly initialized vector). Thus, purely relying on its neighbors may be a good choice." Sec.3.2\After being introduced, n_i is not used anywhere. Given that TransE and DistMult haven t been state of the art for quite a while, it would be interesting to see how decentRL performs with recent state of the art link prediction models, such as TuckER or RotatE. Other comments:Writing quality should be improved, as the paper is hard to follow. \After rebuttal:\I have read the authors  response, but since the actual body of the paper has not changed much from the original submission, I stand by my original rating.<BRK>The paper also proposes to a self learning method to learn the parameters by optimizing the mutual information of the final and initial embedding vectors for the nodes. A distillation approach is also employed to use the initial embedding vectors as the teachers and the final embedding vectors as the students. Some experiments on unseen entities and ablation studies are also conducted to demonstrate the benefits of the proposed method. It introduces an extension of GAT to address the unseen entity issue and the experiments seem to demonstrate its benefit. I have several comments/questions as follow:1. 2.The paper seems to assume that for unseen entities, although pre trained embeddings are not available, unseen entities are still connected to some seen entities so unseen entities  embeddings can still be obtained via the averages of the embeddings of the seen neighbors. As such, how do we handle two unseen entities that are neighbors of each other? More importantly, as the current method does not use any information specific to the nodes, e.g., node content (so only the connections of nodes are employed), can we just include the unseen entities in the graph and retrain the whole model? This is certainly more expensive, but as the paper is mainly considering downstream task performance, this might be a method to address unseen entities in this work. In general, without the guidance from some specific downstream tasks, it is unclear which information the model would learns when the training finishes. Maybe the auto distiller should be jointly trained with downstream tasks and the model can be better justified with the idea from information bottleneck? A discussion about the connection of the proposed method and information bottleneck is also helpful.<BRK>The paper presents "a decentralized KG representation learning approach", named decentRL, which encodes each entity from and only from the embeddings of its neighbors. This approach can therefore account for new entities that have no known features to initialize its embeddings, but do have known links to other entities in the graph. The main contributions:  The paper outlines decentralized attention network (DAN), an adaptation of graph attention network (GAT). Competitive performance is obtained in the general case, with improvements when new entities are considered. Also, there exist other methods like DeepWalk which also rely solely on structure information. The paper does not read easily  It is not clear if the code and data will to be released.
Accept (Poster). rating score: 8. rating score: 6. rating score: 5. rating score: 5. rating score: 4. <BRK>This paper identifies the inherent problem of over squashing that exists in popular information propagation mechanism in GNN. The hidden dimension of a node vector is fixed while the amount of information need to be preserved can grow exponentially (vs linearly in RNN decoder) with the increase of the depth of the networks. This problem becomes critical when modeling long range interaction is required. The paper provides theoretically analysis on the bottleneck, and further analyzes the relationship of required depth and the hidden dimension, showing the simply increasing the hidden dimension will not solve the problem. The paper provides a simple yet effective solution to mitigate the problem, supported by extensive empirical results on multiple datasets.<BRK>3.While the paper proposes a simple solution (FA) to the bottleneck problem, there s not much detail for readers to understand empirically or analytically the extent to which bottleneck issue is mitigated. The paper provides intuitive examples why long range information is needed in some tasks and why the message passing bottleneck is happening2. instead of a fully adjacent layer, what about make 50%, or 75% of the nodes directly connected to the node beyond the original neighbor? Is this long range problem task specific?<BRK>In general, the paper is structured well and easy to follow. However, there’re a few questions the authors need to address:First, the differences between over smoothing and over squashing has not been discussed thoroughly. Also, I suggest the authors provide more rationale on why making the graph to be fully connected in the last layer of GNN can break the bottleneck introduced by this over squashing issue. It is good to see the authors attempt to validate their hypothesis by first using a synthetic dataset, I suggest the authors to further justify why it is the over squashing issue that makes the GNNs fail to perform well when the number of layer increases. There are still issues the authors need to address.<BRK>This paper introduces the over squashing problem in GNNs. The problem is more evident in the graph structured data where both local neighbors and long range interactions are required. It also shows the improvement of breaking this bottleneck by adding a fully adjacent layer. Since the paper shows the existence of the over squashing by comparing the performance of these two types of models, I believe it would be much better and more convincing if it can propose a new solution that can avoid changing the data directly. But the solution proposed by the authors may be potentially problematic and hence, weaken the results and the conclusions.<BRK>Overall, the paper identifies a well known problem in GNN, that is how to incorporate long range information into GNN computation. The papers propose that this is due to the "over squashing problem", and provides some arguments and evaluation on this problem. However, I think the motivation is questionable and the solution is too simple. However, this example seems trivial for a GNN to solve, and the argument does not make sense to me. [Too simple solution]It is unclear to me why "Adding a fully adjacent layer (FA)" is a good solution to the long range problem. This way, it is unclear how FA can get rid of the problem (the paper claims that it can "prevent over squashing"), as much of the useful information has already been "squashed" and cannot be recovered.
Accept (Poster). rating score: 9. rating score: 8. rating score: 6. rating score: 4. <BRK>The main aim of the paper is to make use of human interaction/motion to learn a visual  representation that can be re used for classic visual tasks such as depth estimation. The authors claim that by encoding interaction and attention cues in the self supervised representation, the method can outperform visual only state of the art methods. + Novel intuition: The idea of the paper is intuitive, where it proposes to incorporate body part movements and gaze information in learning visual representations. Attention does play an impact in many tasks like action recognition and scene classification, which might benefit from the proposed representation learning. + Experimental setup and ablation studies: The intuition of the authors to incorporate body part movements and gaze information in their representation has been well justified by the experimental setups and ablation studies. The importance of using each objective in the representation learning has been effectively demonstrated by showing its impact on various target tasks.<BRK>I guess it is implied that the baseline ‘vis’ approach in the paper  is identical to the ‘vis’ in He et al 2020, but it would be nice if this was explicitly in the caption for Table 1. and the result of a more traditional auxiliary visual pretext task. The paper also shows the benefit of gaze and motion is present for two different visual auxiliary tasks. In the first benchmark, adding either attention or movement increases results by 6%, but adding both only improves results by 7%, not 14%! It is interesting that gaze and motion are not as helpful with Lae ( 3% gain vs 7% with Lnce) but it is not clear why. Some thoughts on this would be interesting. # Recommendation  Accept   it is an interesting and surprising result that adding self supervised tasks for other modalities significantly improves self supervised representations and that gaze and motion are redundant with each other.<BRK>They train a network to jointly predict the visual focus of attention in scenes and body motion besides visual instance recognition via an NCE loss to learn good visual representations. The authors show consistent improvements with their proposed method of incorporating knowledge of gaze and body motion versus using visual information only for many downstream tasks they consider. How do the authors justify this? To clearly show the superiority of their dataset versus ImageNet, the authors should also include the results of MOCO trained on ImageNet for each of downstream tasks shown in their paper. Post Rebuttal:I thank the authors for their response and additional experiments to show the performance of MOCO trained on ImageNet for the various downstream tasks considered in this paper. It is evident from the results that the authors presented in Table 5 that their best method (using their multimodal data) performs worse than MOCO trained only on ImageNet with InfoNCE. Using such abundantly available existing data on the web, which can often times simply be downloaded for free without any annotations, is what I believe is likely to be a much more practical and broadly applicable approach to solving the problem of representation learning via self supervision. This concern is also shared by Reviewer 3. On weighing the various pros and cons of the proposed approach, I will maintain my previous rating.<BRK>The paper proposes an interesting video dataset with body part movement signal, gaze signal. And they also treat these signals together with infoNCE/or AE as self supervised signals. I agree with the author that their supervision is working and working together with infoNCE. However, this supervision does not come for free like Rotation/Speed/Contrastive learning. Pros: a new dataset contributing to the  affordance  society, a novel and useful supervision signal. If the author can make the supervision more free? (self supervision), I can change my rating.
Reject. rating score: 3. rating score: 6. rating score: 7. rating score: 7. <BRK>The draft proposes to bound the model generalization by controlling the adversarial perturbation of the model weights. If the weights in the neural network are bounded and the activation function is Lipschitz, the change of output of the network, as well as the Rademacher complexity of the hypothesis class, can be easily controlled. Connecting perturbation with the generalization is not something new no matter in theory or in practice. This is another draft formulating the network perturbation and generalization so that a norm product bound is derived. There are plenty of previous works on this already, e.g., the work by Neyshabur et. Perturbing the weights and directly applying norm product leads to a bound not as tight, to some extent, it is mostly vacuous. Frankly, the method gives a pessimistic norm product bound and ignores all the effects caused by the “alignment” between the internal coefficient matrix and the input vectors, which is crucial in terms of understanding how input signals are handled throughout the network. I would encourage the authors to read some recent work by Barron et.al. which reduces the generalization bound from norm product to product norm.<BRK>Summary: The paper discusses learning neural network models under weight parameter perturbations. In particular the paper motivates the use of a new loss function (equation 9) based on the analysis of neural network robustness (Section 3.2, 3.3) and generalization properties (Section 3.4) to perturbations of the weight parameters. Section 4 has experiments supporting the theory that the loss function in equation 9 is robust and has good generalization properties to weight perturbations. Review: I think the loss function in equation 9 is well motivated and clearly explained. Also the experimental results can be reproduced as the code has been included as part of the submission. Overall the paper is well written. Firstly, the work does not cite some relevant papers to the topic a couple of which I list below:a.	V. Nagarajan and J.Z. Shamir.Size independent sample complexity of neural networks. I believe the discussion in Section 2 of Nagarajan and Kolter, 2019 is very relevant to the results in the paper. Nagarajan and Kolter, 2019 show that the norm bounds of the weight matrices increases with the number of samples and hence conclude that the generalization bound results in Bartlett et. The current paper uses the results in Bartlett et. al., 2017 to derive generalization error bounds and additionally has another term dependent on the norms of the weight matrices due to perturbation. Given the discussion in Section 2 of Nagarajan and Kolter, 2018 I am concerned if the bounds obtained in Section 3.2 3.5 are vacuous. I will like to see a discussion and experimental results in light of the observations in Nagarajan and Kolter, 2019. In the experimental section, I am interested in also knowing the results for the case $\epsilon   0$ in Figure 1(a). Also, if possible can the authors include a discussion or guidance on the sensitivity of the results to the hyper parameters $\mu$ and $\lambda$? For example the performance seems to get worse when moving from $\lambda   0.01, 0.125$ to $\lambda   0.015$ in Figure 1(b).<BRK>The paper investigates the effects of weight perturbations on the output margin for multiclass classifcation problems. The paper shows that robustness to weight perturbations can be bounded using the (1,\infty) norm of the weight matrices. The paper then suggests that a low (1,\infty) norm of the weight matrices leads to better generalization. Moreover, the robustness against weight perturbation implied by low (1,\infty) norms of weight matrices should increase the robustness against adversarial perturbations. To support these claims, the paper presents a generalization bound using the (1,\infty) norm of weight matrices as well as an empirical evaluation using a novel surrogate loss function that, when used in training, is empirically shown to reduce the generalization gap and increase the robustness against adversarial perturbations. The paper is very well written and the theoretical analysis is sound. My only concern is that the generalization bound derived in theorem 4 might not be very informative or conclusive. Since the authors furthermore did address my main points in my review during the discussion, I have increased my score to 7. Since the generalization bound presented here relies on an upper bound on the Rademacher complexity using the already not too tight upper bound of Bartlett, 2017, it could be vacuous. In earlier works on input robustness [1], obtaining meaningful bounds required some covering of the input space with examples rather than a uniform bound over the model space. 4 in this paper are not conclusive. In [3], the robustness to weight perturbation was also used as a building block for a potentially non vacuous generalization bound, so it might be worthwhile to discuss the relation to that paper. Another line of work that might be worthwhile to discuss is weight noise injection during the training process, which leads to better generalization (e.g., [4]). For that, at least a runtime analysis should be provided. The paper is lacking some discussion, but overall I would argue that the paper makes a valid contribution. arxiv preprint, 2020. However, the authors provided new experiments that indicate that the bound might not be vacuous in the considered setting, though.<BRK>In this work, the authors theoretically analyze the robustness against weight perturbations in neural networks. Upper bounds of the pairwise class margin for single layer, all layer, and selected layer perturbation are established. Based on the analysis, the authors propose novel robust surrogate loss functions for 0 1 loss and cross entropy. Furthermore,  the authors analyze the Rademacher complexity of the perturbated network with the proposed loss, which leads to generalization bounds based on (Mohri et al.(2018)) and (Bartlett et al.(2017)).Pros1.I think the theoretical analysis part is clear and systematic. Efforts of each term in the bounds are well explained. 2.The analysis is useful for a better understanding of the robustness of networks against weight perturbation. 3.The proposed loss is also interesting. The explicit upper bound reduces the computation of the maximization step in adversary training of weight perturbations. Cons1.The product form in the bounds may grow fast and become loose. It is better to report the value of each term in the bound in Theorem 2 or 3  for a better understanding of the bound. The authors can employ the same experiment s setup in Figure 1. If the value of this term is large, the margin term minus the worst case error will always be negative. Thus, the loss will remain a constant one, which is very harmful to training and optimization. 3.In the experiments, only a simple MNIST dataset is evaluated. I concern about the performance of the proposed loss in more practical cases. As stated above,  I think the proposed loss may be challenging for optimization. This phenomenon may become more significant in difficult datasets. It is better to state it clearly to be self contained. In Theorem 2, the definition of W* is not given. The definition should be included in the main paper instead of in the appendix.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 5. <BRK>The reasoning for studying this is sound, and using CIFAR 100 to do so seems quite reasonable (The fact that the authors focused on CIFAR for their work is fine for me.If the authors wish to also do experiments on ImageNet, hierarchies based on ImageNet can also be created based on https://wordnet.princeton.edu/, for example, as in https://arxiv.org/abs/2008.04859.) However, the paper has weaknesses in terms of its baselines and evaluation. Next, it is not surprising that standard models, which are trained to be robust to standard (fine grained) adversarial examples, are less robust to coarse grained adversarial examples compared to HAR, which is specifically designed with the task of coarse grained adversarial examples in mind. Otherwise, it is hard to understand how much the HAR architecture itself is important (as opposed to focusing on coarse grained adversarial robustness rather than fine grained adversarial robustness during training). This is a particularly problematic aspect of the robustness field, as pointed out in https://arxiv.org/abs/1802.00420, https://arxiv.org/abs/1902.06705, and https://arxiv.org/abs/2002.08347.<BRK>Misclassifying a person for a car is a “coarse misclassification”, but misclassifying a bus for a car is a “fine misclassification”. * It is unclear to me how the adversarial class is defined. * Motivation is not backed by data or citations. I’m aware that the ICLR conference places no restriction on the structure of the paper, or the number of experiments, but I suggest expanding either the experiments or the related work to increase the arguments in the paper. To quote from the paper “We use models with a lower capacity so that both vanilla models and the hierarchical classifiers have the same order of magnitude of parameters”. Please either a) use terms such as accuracy/error rate to indicate what the numbers represent, or b) write in the caption whether a higher score or lower score indicates better performance.<BRK>Summary: This paper tackles the problem of building hierarchical adversarially robust (HAR) models i.e., models that are less prone to coarse grained misclassifications in the face of adversarial manipulation. Specifically, the authors propose HAR networks, wherein the learning/inference problem is decomposed into coarse grained classification followed by a number of fine grained classifications. Comments: The problem studied in this paper is a natural and pertinent one. This finding seems to go against the main claim of the paper. * The authors do not justify why they choose separate networks for the coarse and fine classification tasks. One could imagine training a single flat model with a hierarchical robust loss. Overall, I think this paper studies an interesting problem, but could be improved in terms of experimental evaluation and comparison to baselines (as described above).<BRK>There are some general issues with the paper. For starters, it is usual in benchmarking to show a plot of accuracy against PGD steps. It is not clear why the authors settle on 20 steps. I would also like to see how sensitive the coarse model is to the choice of z input; an ablation study should be performed. One possible suggestion for this would be to train two models, p(z|x) and p(y|x,z) but where z is just the repeated, fine grained y label.
Reject. rating score: 2. rating score: 4. rating score: 5. rating score: 7. <BRK>The evaluation performance is also a little unfair as they use ECE as an objective and then only show better performance on ECE. ## Review### SummaryThis paper proposes a post hoc calibration method which aims to preserve the accuracy of the classifier as well as improve its uncertainty calibration. I like the justification of the authors about this: " a calibrator that does not maintain the accuracy may attempt to improve the accuracy at the cost of hurting (or not improving) the calibration". * A rank preserving method has the advantage of optimizing loss functions which do not have to "care" about accuracy. In this case, the authors propose to use the ECE loss. A method which does not preserve the rank would completely decay the accuracy in an attempt to optimize the ECE. So it does have the advantage of optimizing ECE, but the paper does not really show the benefit here as it only shows that optimizing the ECE performs better on ECE (this is expected) but performs worse on the other metrics (see Tab.1).Also, I would like to point out that I include DECE when I talk about ECE in this review. * Better calibration performance compared to the Baseline, TS and LTS. The major weakness of this paper is its view of the current post hoc calibration literature seems to be outdated. The post hoc literature has come far beyond simple methods such as TS. 2.The related work paragraph also only talks about three post hoc calibration methods. One example of such a statement can be found in the appendix: "Existing post calibration methods such as temperature scaling recalibrate a trained model using rather simple calibrators with one or few parameters, which can have a rather limited capacity." A method which optimizes the metric A directly will perform better at this metric A. But overall this is not really an impressive result, especially given that the ECE has lately been criticized (see above). The authors of this current paper do not claim that they obtain state of the art but they imply this by mentioning that they perform better than a method which is previously state of the art. I looked into the LTS paper and they also did not make the claim of obtaining state of the art performance. So where is this claim coming from? Again, I find this a little misleading that the authors of this paper seem to imply many statements to make their method seem good. So even though I do see the benefit of having method which preserves the rank, it should clearly be pointed out that it has the disadvantage of not being able to improve the accuracy. They show that the ECE of scaling methods are non verifiable. So even though the authors are aware of this paper, they do not at all comment on the use of ECE for continuous output scaling methods such as the one presented. How many bins were used to estimate the ECE? As the authors are aware of this work, they should discuss this weakness of ECE in their paper and more importantly use approaches used in other recent works in the literature to address these issues to some extend. Again, it seems that the authors need to do a more thorough literature search on post hoc calibration. Maybe the ECE was evaluated differently? Transforming classifier scores into accurate multiclass probability estimates.<BRK>The authors argue that previous attempts to generalize temperature scaling have a tendency to overfit not because of the additional parameters, but rather because they are not rank preserving (and therefore can change model predictions / accuracy). The key idea is to use a higher capacity (2 layer neural network) calibrator that is constrained to be rank preserving so that model accuracy remains unchanged. This is interesting and (as far as I know) novel. The experiments are very thorough   all metrics are averaged over multiple runs and several complementary performance metrics are provided. The proposed method beats the baselines in most (but not all) of these cases. For instance, the authors observe peculiar behavior for their proposed method on high confidence examples but do not explain why. There could also be more effort invested in discovering why the proposed method outperforms the competition. Are there examples that are actually calibrated worse with the proposed method? While space constraints limit what can be included in the main paper, I think this kind of deeper analysis would improve the work. Overall: While I d like to see some deeper analysis, I think this is paper makes an interesting and useful contribution to the calibration literature, backed up by solid empirical results. Section 3: In the "Instantiation via monotone two layer networks" section, it would be helpful for clarity if $a_j$, $\theta_j$, and $b_{\theta_j}$ are explicitly described. Appendix B.1: I think "consistently performs temperature scaling" should be "consistently outperforms temperature scaling" under the LTS heading. **UPDATE AFTER DISCUSSION:** R4 s exceptionally thorough review raised a number of important concerns which I initially did not recognize. Since the paper has not been appropriately revised, I have decided to lower my score from a 7 to a 4.<BRK>**Summary**This work proposes a method for calibrating outputs of deep neural networks using higher capacity learning than previously possible. UPDATE  After reading the concerns of the other reviewers and the author response, it seems that many of the concerns remain unaddressed especially the top concerns. The learned NRPT transform is monotonic, it preserves the rank of the probabilities between logits. The authors also propose to train using a loss for the Expected Calibration Error (ECE), which is the metric that calibration models seek to minimize. Accordingly, I have decided to lower my score. The insight as to why high capacity calibration methods overfit and show poor ECE, which leads to the insight to preserve the accuracy of the model via enforcing monotonicity. As mentioned above, the use of ECE as a loss term also appears to be novel. **Clarity**The paper is reasonably clear and well written. However there are some strange organizational choices, such as the lack of a clear related work section. This has led to the omission of several baseline methods that should have been cited and compared against. **Evaluation**The evaluation seems reasonable, appropriate metrics and datasets are chosen, however I am not sure how much insight about calibration we can really glean from NLL and PEnt. As the authors point out, it is curious that the +E models are outperformed by the vanilla models on the MNLI datasets. Since this happened on ½ of the datasets, I would have liked to see more datasets to verify that this is truly an anomaly. But this is typically not their aim. I do see some minor value in showing the results in Figure 3, but I take issue with saying the method performs “uncertainty post calibration”. Another criticism is that the authors do not acknowledge that other calibration methods such as temperature scaling have the desirable property of being monotonic not just in terms of the logits, but in terms of the rank of the confidences. That is, the order of the argmax classes will be preserved. Finally, as mentioned above, there are some weaknesses in the evaluation.<BRK>This paper introduces a new post processing calibration method that can keep classification ranking, like the simple (local) temperature scaling methods, but has larger capacity, comparable to matrix scaling. Essentially it tries to achieve the best of both approaches by avoiding the underfitting and overfitting issues of temperature and matrix scaling, respectively. The method proposed essentially learns a temperature and a bias parameter that depends on the image, and two matrices that are global to all images. During post processing calibration training, the constraining of the matrices and the temperature to be strictly positive guarantees that the calibration is monotonically increasing, guaranteeing the preservation of classification ranking. The paper further shows that this calibration model can be trained in a post processing approach with NLL and ECE losses (the training with the ECE loss is claimed as another novelty). Experiments are performed on CIFAR100, ImageNet, and MNLI, and results show that the proposed post processing calibration stage trained with NLL improves NLL and Entropy for CIFAR100 and ImageNet, and the one trained with ECE improves the ECE related measures. For MNLI, results seem to show that the calibration with NLL loss provides the best general result. Additional results that show the value of the method for other calibration measures also display the value of the proposed approach. Nevertheless, I like the simplicity of the approach and the experiments that show the potential value of the approach. In general, I think it is a good paper and would support its acceptance. 2  The result of NRPT+E on MNLI mm for DECE is 0.10, which is best for the row   that should be the boldface number for that row. 3  In section 4.2, the paper says "In Figure 2a, we confirm that the NRPT has a lower classification error than TS and LTS for the majority of the percentages, except at the very tail."
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 7. <BRK>The paper proposes a multi hop communication method for multi agent reinforcement learning. This method is based on the loosely coupled reward structures among agents, which, as far as I am concerned, are generally held in complex multi agent settings. The authors use experiments on CityFlow, MPE, and MAgent to demonstrate that their method can outperform the SoTA methods and is scalable. One way to interpret the proposed communicating network structure is a normal multi hop communication mechanism, but only with a softmax activation function. Compared to previous works studying communications of local observations, the proposed work (1) needs to address the problems induced by the joint policy, like sampling from it. However, this approach may hurt the scalability.<BRK>The experimental results looked promising, so there seems to be an idea here worth communicating. The paper was very hard for me to follow. I still find the paper a bit hard to follow, but none of my original concerns remain. J is a loss: which one? However, the exact assumptions are not clear, and the chain of issues discussed throughout section 4 seems to include discussion of approximation. This would seem to need a clear statemenOne of the claimed contributions is this is *principled* method. How does the computational cost compare to the baselines in the experiments? Proposition 1: "The optimal policy has the form ... 1/Z exp(...)"I found the use of optimal slightly hard to follow throughout this. The cited PRL article (Levine 2018) seems to retain this standard use of optimal: it uses a distribution over trajectories with an equation similar to here (a softmax over accumulated trajectory rewards), and makes use of the property that trajectories corresponding to an optimal policy have maximum probability in that distribution. Can the authors clarify this use of optimal? Proposition 1:For clarity, explain the intention of psi.<BRK>The paper proposes a scalable approach via intention propagation to learn a multi agent RL algorithm using communication in a structured environment. An agent encodes its policy and sends the “intention” to the neighboring agents with the assumption that only the closest agents would be the affected by it. So this approach helps in avoiding the need to factorize the value function explicitly. I have a few questions about the clarity of the presentation. How important is the graph structure defined by k means? How does adding/removing agents to the set of neighbors affect learning? A comparison with a fully connected graph should be sufficient. The plot in the Appendix shows results on the CityFlow task which has very structured observation with the set of immediate neighbors always set of 4. Doing such an analysis on a more dynamic environment like MPE would be helpful. Overall I feel some restructuring of the paper would benefit the reader explaining some missing portions of the algorithm.<BRK>Paper SummaryThe paper considers the cooperative multiagent MARL setting where each agent’s reward depends on the state and the actions of itself and its neighbors The paper has a theoretical claim that, for such reward structure, the optimal maximum entropy joint policy in the form that can be factored into potential functions, one for each agent. In particular, if the sum of all agents’ rewards is a function on pairwise actions, those potential functions are one for each agent and one for each pair of actions (i.e.the equation after Proposition 1). Then, the paper proposes to use mean field approximation to approximate the optimal joint policy (Equation (3)), which leads to a concrete algorithm that relies on passing the embedding of each agent’s local policy around to neighbors. The paper then empirically shows that the algorithm is particularly effective for domains with a large number of agents. I do not see too much algorithmic novelty here. The beginning of Section 3 says the paper considers maximum entropy as the optimization objective, while eta(pi) at the beginning of Section 4 says the objective is long term reward (no entropy). This seems to be an inconsistency here. It is a little bit unclear what assumptions are required for all the theoretical and experimental claims of this paper. 3.Is there reason to believe that the multi round message passing will converge to the fixed point of Equation (2)? The insight that optimal maximum entropy joint policy takes the format of Markov Random Field might be of some value and interest. In particular, it seems that the loss functions do not drive mu s represented by NNs to the fixed point solution of Eq (3); psi shows up in Eq (3) but does not play a role in the following development of the method.
Accept (Poster). rating score: 8. rating score: 8. rating score: 6. rating score: 5. <BRK>It is well known that neural networks (NN) perform very well in various areas and in particular if one looks at computer vision convolutional neural networks perform very well. Although convolutional neural networks (CNN) are limited in their architecture (since they only allow nearest neighbour connections) compared to fully connected NNs (FCNN), their superiority in performance is unclear. In this paper they answer the following fundamental question: can one formally show that CNNs are better than FCNNs for a specific learning task? The proof of the lower bound for FCNN uses the well known statistical query framework which shows that using gradient descent to learn k parities requires time n^k (and in their application n^ log n). Overall, I think this paper resolves an important and interesting question. Given that most of machine learning is well understood in the non rigorous setting, giving a rigorous separation between CNN and FCNN is an important problem and they have resolved it. If I had to be nit picky, I would say the concept class they give this separation for is well suited for their separation and doesn t really say much more than that, but I think it s already interesting that one can show such a separation theoretically.<BRK>The paper establishes a separation result between a convolutional network (CNN) and the fully connected network (FC). Specifically, the authors show that a polynomial size of CNN trained by gradient descent with a polynomial number of iterations can learn functions that depend only on a small pattern of $k$ consecutive bits of the input,where $k   O(\log n)$ and $n$ is the input dimension. On the other hand, they show that for FC, gradient descent fails to learn a $k O(\log n)$ parity function unless the network size is $\Omega( n^{\log n})$. Hence, the separation result in the computational aspect is proven. I believe this paper makes a significant contribution. The writing is good and the proof is short and concise. The authors first discuss other possible explanations for the observation that CNN performs better than FC in practice and argue that parameter efficiency and weight sharing might not be the main reason for the superiority of CNN. Q: It appears that in the analysis of the positive result regarding CNN, the training is in the neural tangent kernel regime, as you have to bound the deviation from the initial point and that you show a randomly initialized CNN already works well for the $k$ pattern problem. Does it suggest that the kernel method can learn the $k$ parity function?<BRK>Summary:The paper studies the problem of computational separation between two layer convolutional neural network (CNN) and fully connected neural network (FCN). It shows that there is a class of function, which is defined in the paper as k pattern, such that CNN could learn this class within polynomial time with polynomial number of neurons while FCN needs exponential number of neurons to avoid getting stuck at initialization. Pros:  The problem about the separation between neural network architectures, such as CNN and FCN in this paper, is an interesting and important topic. Experiments are also provided to support the results. However, given that current proof also relies on the fact that the first layer does not move to much from initialization, which is quite similar with the idea of NTK analysis, I was wondering if some techniques in NTK analysis could be applied to show the convergence of last iteration. Otherwise, one can take large enough step size to escape the initialization. It would be better to explicitly mention this. In Theorem 4, it is u^{(0,j)}, while in Lemma 8 it becomes u^{(j,T)}. After authors  response:Thanks for the response! I will keep my score.<BRK>This paper tries to provide an explanation for the performance advantage of CNNs over FCNs  by trying to identify labelling functions where the performance gap between the two for supervised learning can be proven rigorously. The identify an interesting class of such labelling functions : the pseudo Boolean functions called the k pattern functions and they are able to show that depth 2 CNNs can easily learn this class. I think this is a very cool observation and this could lead to a lot of future work. But unfortunately this is where the concrete part of the argument in this paper stops. I dont see why the argument in Section 4 is enough to prove that this k pattern is hard to learn for FCNs. I think Theorem 12 is essentially motivating the conjecture that if the expected gradient at initialization can be arbitrarily small then maybe GD will find it hard to learn. But this conjecture is not getting proven here. Currently I can think of "counter examples" which I would like the authors to address :  Consider the ODE : dx/dt   (2/t^3)*e^{ 1/t^2} with the initial condition x(0)   0. There noise should maybe only help rather than hurt.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 6. <BRK>All of these works attack object detectors and target transferability. The paper is poorly written and ambiguous. No use of the proposed dataset. The authors propose a new dataset of adversarial objects but never mention or showcase the dataset s usefulness. No enough ablation is performed. The novelty of the proposed methodology is limited. While the use of relevance maps to improve the transferability of attacks on object detectors is novel, no proper explanation is provided. The attacks are based on PGD, and the relevance map is adapted from SGLRP. The paper offers no theoretical results or exciting insights. Table 2 5 could have been visualized better by using a bar chart, for example, to observe the relative performance of attacks and defenses.<BRK>A transferable adversarial attack method is proposed for object detection. As the major contribution claimed is on the relevance map, the minor modification makes the contribution limited. Also, the gradients are utilized to update the relevance map.<BRK>Summary:The paper found a new way, called Relevance Attack on Detectors (RAD),  to generate high transferability adversarial examples against detectors by suppressing the multi node relevance. Moreover, a dataset generated by this attacking method is introduced. Pros:1.The designed RAD is new and technically sound. 2.The experimental results on multiple detectors, like YOLOv3,  RetinaNet, Mask R CNN, etc. are promising. Cons:1.It s not surprising that the object detectors can be attacked together. One naive way is that using the ensemble attack by average the negative CE loss like the paper "Making an Invisibility Cloak: Real World Adversarial Attacks on Object Detectors" proposed. 3.I cannot see a very significant value of the generated dataset.<BRK>This paper presents a method for adversarial attacks on object detectors by exploiting relevance maps that are originally intended for model interpretation. Unlike most of the existing methods that attack detection scores directly, the proposed approach focuses on suppressing the relevance map associated with target objects by image perturbation. The idea is interesting and demonstrates good transferability on the tasks of object detection and segmentation. The paper is mostly well written and easy to follow. The adversarial object dataset can also be helpful to the research community.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 6. rating score: 6. <BRK>This paper studies transformations in GAN latent space that map to meaningful transformations in the generated data. The paper also contributes new methods for nonlinear latent transformations, disentangled transformations, and an application to attribute transfer. The positives I see are:+ Simple and elegant alternative to prior work on finding latent transformations+ Nice qualitative and quantitative results+ Practical benefits including speed up, analytical transformation end points, and better disentanglementMy main criticisms are:  toward the end of the paper it’s a bit of a hodgepodge of ideas  the ideas and methods in Section 3 are mostly disjoint from those in Section 2  the attribute transfer application especially feels tangential, and receives minimal analysis or evaluation  the experiment in Figure 7 is not fully convincingI think everything in this paper is interesting, but the different sections don’t fully cohere together.<BRK>I am pretty enjoyed reading this paper. The task of finding meaningful steerability within the latent space is relatively a new research area. Experiment wise, the authors provide a huge set of qualitative results in the appendix. Though the results are already convincing enough, I would still recommend the authors to provide some quantitative numbers for the comparisons on the diversity of latent directions against other methods (Figure 34 38). Despite the submission is pretty strong and I pretty much has no problem with it, I ended up rating it with an 8, as the impact is relatively narrow in a specific area/task.<BRK>The paper is clearly written. Also, it discovers endpoints for latent space trajectories. Section 3.4 where the attribute transfer is described is not clearly written. Reading this paper, one is not able to understand how background or texture  transfer is obtained through this work. The method of (Voynov and Babenko 2020) also operated on a pre trained generator and obtained semantically meaningful directions in an unsupervised way through only optimizing two other simple networks to obtain meaningful directions.<BRK>The authors propose two new techniques that extract interpretable directions from latent spaces of pretrained GAN generators. Furthermore, the authors describe additional details of the method, like determining the transformation end points, which are important for usage in the practical visual editing. The paper tackles the important problem, provides a thorough description of the field, demonstrates an in depth understanding of the area. This paper convincingly addresses this gap. 2.The proposed techniques are both simple and efficient, much faster compared to existing methods.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 6. <BRK>This article introduces a VAE based method for separating local variation factors from global variation factors in the data in an unsupervised manner. The article provide an detailed experimental analysis on MNIST and CelebA, and experimental evidence that all parts of the model (notably the discrete d variable) are relevant. The article provides a well detailed description of the proposed UG VAE, and how it compare to similar models from the literature (notably ML VAE, from which it is inspired). I have several questions/remarks regarding it:1. The text of the paper present this as if it was obvious, but does not seem that obvious to me why "beard" is a global feature but "hair" is a local one. It empirically does to some extent, but there is no clear theoretical justification. For this last point in particular, the proposed structure seems pretty close to an other one that would be I believe much more natural: have the global parameter β not shared across the entire batch, but depending on the class d the encoder assigned to the datapoint. Is that something that has been considered, and if yes why was it not satisfactory compared to the proposed model? Is the interpolation done only along the diagonal, moving from (μ1   3, μ2   3, ...) to (μ1 + 3, μ2 + 3, ...) ?<BRK>This paper presents a novel deep generative model based on noni.i.d.variational autoencoders that captures global dependencies among observations in a fully unsupervised fashion. The proposed model combines a mixture model in the local or data dependent space and a global Gaussian latent variable, which captures interpretable disentangled representations with no user defined regularization in the evidence lower bound. The proposed model is being evaluated in two tasks: (1) disentanglement, and (2) domain alignment. Relaxing this constraint will enable wider adoption of this line of models, which can be very useful. Cons:My major concern on this paper is the quality of the evaluation section. (1) first of all, there is no quantitative or qualitative comparison between the proposed method and previous works. Although disentanglement is a task that is hard to quantify, it is hard to show that UG VAE outperforms other methods. (3) It is not convincing that the global disentanglement features are learned in the global latent variables. (4) "Composing graphical models with neural networks for structured representations and fast inference" from NeurIPS 2016 also has a mixture model as latent space.<BRK>This paper proposed a deep generative model based on the non i.i.d.VAE framework in an unsupervised version. The model which combines a mixture prior in the local latent space with global latent space has three advantages: First, the latent space can capture interpretable features. Although this paper has mild improvement on the basic VAE structure, the model displays a good interpretability power, and the setup of the latent variables are illustrated reasonably in the paper. This paper demonstrates how the features are controlled by the global and local variables, and also it shows the necessary of including the mixture prior in the local space to acquire interpretable information. 3.This model performs domain alignment, the global variable β can capture domain knowledge. The paper only discussed one prior distribution for the latent variable d and β. The power of choosing other types of prior distribution is unknown. I have read authors  feedback and will keep my original score.<BRK>Response to rebuttal: the authors have drastically improved the quality of the submission with the new experiments and clarifications, I have therefore increased the score to a weak accept. This paper introduces a non iid VAE architecture that uses a mixture of gaussian latent space and a global latent variables shared among all the elements of a mini batch to capture global information in correlated datapoints in an unsupervised way. Overall the paper in well written, and I believe in focuses on two important research directions, namely unsupervised learning of disentangled representations and domain alignment. The model itself is novel and well explained, but I feel the technical explanation is missing intuition on how the model can learn disentanglement in beta from purely random batches, which is not obvious to me. My biggest concern is in the experimental section, that I did not find convincing enough for a number of reasons:1. Why is light a local factor but contrast a global one? How are you guaranteed that you will follow the data manifold? In its current state I believe this paper is not ready for acceptance, but I hope the authors will be able to clarify some of my concerns in which case I will increase the score.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. rating score: 4. <BRK>The authors look into GC relationships between variables including the sign of the relationship which they can successfully infer using their approach. There are no obvious inconsistencies or errors that I can see in the paper to the best of my knowledge. The paper is well written and conveys the information clearly enough. An exciting next step is to apply this approach to real data, could be a great addition to this paper but not necessary for publication.<BRK>The novelty of this paper is 1 and 2 in the Pros below, and the methods utilized the existing stable frameworks such as a heuristic stability based procedure that relies on time reversed Granger causality (TRGC) (Winkler et al., 2016). The method is interpretable based on a self explaining neural network (SENN) framework and allows exploring signs of Granger causal effects and their variability through time. Cons:1.The detailed explanation and the difference from the previous TRGC work were unclear (below). 2.There was little discussion about the reason why TCDF outperformed the proposed methods on the simulated fMRI dataset. 3.There were no results in cost function (e.g., prediction error) even in the appendix. This detecting Granger causality would be unsupervised learning, thus the learning results based on the cost function are unknown. The information might help us understand the reason for 2. This may be critical information to evaluate the method.<BRK>My comment here is that there ought to be more discussion on sign detection. Especially for the Lorenz 96 model. But since you have not added any such metrics, it is hard to know (computational complexity that is). Naturally, you only have so much space, but a comment would be helpful. At the bottom of this section you say "signs of Granger causal effects and their variability in time can be assessed as well by interpreting matrices"   it would be helpful if you could provide a toy example, of matrices, at this point to show us how precisely we are to interpret Psi to get to GC. But here you are using it in a much more novel way. It would be very interesting to understand your reasoning and logic here. Easy to follow and understand. The algorithm is meant to summarise your method exposition of your method in the body, not introduce new items such as $\boldsymbol \alpha$ and BA.<BRK>The authors also augment the framework by learning Granger causal structures that are stable on original and time reversed data. Exhaustive empirical analysis is done with recent GC baselines. The idea of bringing SENN within this is useful but not enough to claim significant novelty. Can such theory be developed for the method proposed here ? However, given the double computation and higher number of parameters involved in the method, I was expecting some discussion on time complexity and how to improve scalability of the method in practice.
Reject. rating score: 4. rating score: 5. rating score: 9. <BRK>1.It is interesting to model the noises mixed with the very weak physiological signal; however, the assumption used in this paper, i.e., most regions outside of the attention masks do not contain the signals of interest and consequently contain noise does not really hold, because, besides the weak physiological signal, the dominant signal inside the face video is the face content, i.e., the texture of the face describing the appearance of a person; these background signal dominant the video but cannot be treated as noises. 2.The novelty of the proposed method is quite limited. The general idea is to use an inverse mask, i.e., {1  attention mask}, to determine the regions less correlated with physiological signal, within which the average intensity is computed and treated as noise. As a result, even though there a number of SOTA methods in this topic as listed in the references, they were not used to compare with the proposed approach. So it is not clear how the proposed approach can advance the methodology.<BRK>In fact, using reverse attention to focus on other regions has been studied in other areas (e.g., for saliency detection). The paper applies the idea to a specific task, i.e., the camera based physiological measurement task. It is then hard to evaluate the significance of proposed method. Given that, the denoising model seems to be a refinemet model that considers reverse attended regions and the output of the first model. It is hard for me to understand the second model as a denoising model. Minor issues. However, it is not easy to understand the last two, i.e., how the noise subtraction is performed in frequency or time domain. 5.Tables 3  > Table 3 (page 8). 6.The abstract mentions two datasets but there are actually three datasets in the experiments.<BRK>The article proposes a model for estimating physiological signals from videos. The novelty of the proposed approach is to use the low attention regions to estimate a model of the signal noise. The model is well described and sensible and the experimental section demonstrates state of the art results on what is a very relevant task. Overall, the article describes an interesting approach and contains valuable insights into the limitations of attention based approaches in noisy signals.
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 5. <BRK>This paper proposes an RL based method to learn the causal ordering of variables. 3.The idea of searching the causal ordering, instead of in the DAG space,  to improve the efficiency is reasonable. Cons:1.However, my main concern is that the proposed method lacks a theoretical guarantee of the identifiability of causal ordering, which is very important in the field of causal discovery. If the authors can solve the above issues, I will increase the score. Causal discovery with reinforcement learning. In International Conference on Learning Representations (ICLR), 2020. Post rebuttal:Thanks for the feedback.<BRK>Original review:In this paper, the authors proposed a new RL based algorithm to discover causal DAG models from observational data. Unlike the previous RL based causal discovery algorithm that performs causal discovery via searching through DAG space, it propose to instead search through the smaller ordering space, thereby achieving better empirical performance. Cons.1.Ordering based causal discovery is a well established field.<BRK>The reduced search space enables a better and seemly more efficiently learning than the existing RL baselines. Empirical studies has confirmed the better graph learning accuracy results than the existing RL baseline as well as some non RL structure learning algorithms. Concerns:1. it is not accurate to state that the exact algorithms can only handle 20. Would random sampling order instead of learning it via the proposed approach gives similar performance? How about the orders from other order based learning approaches (as indicated by authors in the paper)? This seems critical on the evaluation.<BRK>This paper describes an RL approach for DAG structure learning. Why does the state encoder use attention networks? But to the best of my knowledge this paper is the first to search over the space of variable orderings, instead of directly searching over the space of DAGs. Detailed comments: This is a well written paper, but I would need more information to better evaluate it.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 5. <BRK>The work proposes 1) classifying states according to their observed reward roll outs and 2) using the resulting classifier for reward shaping. This reviewer finds the work difficult to read. A better version of this paper would justify the technique analytically. For instance, the optimal critic would provide the true Q values as reward. Under what conditions does a logistic binary classifier output a probability which is close to the true Q value (e.g., with very sparse reward rollouts)? The experiment section at the moment is structured as "it works on these 6 atari games", but this reviewer does not find this compelling (e.g., what about the other games?do these 6 games have an obvious desirable/undesirable reward boundary?). However, the claims about robustness to reward delay are interesting.<BRK>The paper deals with RL problems with the reward is either very sparse or delayed, in which case the Q values would don t get updated. Honestly, I found the write up to be really confusing, partly because of the structure of the paper and partly because of frequent carelessness in wording. Something that would really help is to take a difficult RL problem and just use it as a running example when discussing the different concepts in the paper. As for the results, the authors conduct experiments on 6 Atari games, which I didn t find very convincing, in particular since games like Montezuma s Revenge, which are known to be difficult because of their reward sparsity are not included. In short, I think there are some interesting ideas in the paper, but both the writing and the experimental results need significant improvement.<BRK>The paper presents an experimental approach to tackle the problem of appropriate credit assignment under the delayed reward scenario and proposes an empirically sufficient condition to calibrate the reward policy in a Deep Reinforcement Learning framework. Now, depending on the structure of the space, it could give rise to many topological properties. Further, the intuitive explanation presented in the paper is not sufficient. The authors indeed present some experiments to highlight the improvement accomplished by their architecture w.r.t.to some hind sight results (Table 1). However, the end goal of solving a task (e.g.playing games) is to maximise the total reward. Hence, it is essential to compare the total reward obtained by the proposed method with that obtained by the latest DRL based algorithms, and it is missing in this paper.<BRK>The method is based on building a classifier that can detect states that will lead to environmental rewards in the future and assigning an additional reward to these detected states. **Pros:**  The overall idea is simple and intuitive. The paper is not particularly easy to read. Related to the previous point. This term has a particular definition in the literature that should be refer to and justified. 2.I’m not sure the Atari levels selected for the experiments have a particularly delayed reward. 3.The results are not particularly strong. Is this because of the choice of levels?
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>The paper proposes to distill knowledge from large teacher networks to small student networks in self supervised learning. It is quite similar to a very recent paper as shown in the following, it would be good to discuss the differences in the paper. It seems that improvements on object detection and instance segmentation (Table 2) are relatively small compared to other experiments, are there any explanations? Could it be possible to use smaller student networks in this experiment as well?<BRK>Strengths: The paper is clearly written and well organized. The SEED approach  is well motivated and sensible. Experimental validation and the ablation studies are quite thorough. The method is simple to implement. 3.Minor point: Some citations, which should not be in parentheses, are in parentheses (e.g., Romero et al.page 8).Please fix this in the revision.<BRK>The key weakness is that the knowledge distillation approach and the instance queue approach are previously proposed and known to the research community. The paper provides comprehensive empirical results to justify the efficacy of the proposed approach. Weakness:   The core novelty of this work is the idea of conduct knowledge distillation in self supervised learning. The key contribution is the idea of apply knowledge distillation for self supervised learning.
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. rating score: 6. <BRK>Summary:This paper proposes Group Supervised Learning (GSL) that can learn disentangled representations by swapping components in latent space, and enable one shot novel view synthesis. The authors can demonstrate their idea well in Section 3 and 4. 2.Swapping attributes is an interesting and novel idea to encourage disentangled representation learning, which are easy to implement and could have wide applications in many fields. Cons:1.In Equation (4), can you explain the defination of  separate losses, L_r, L_sr and L_csr along with the equation? Can you please offer ablations for each loss?<BRK>It first maps training images into disentangled latent representations, which can be decomposed into multiple components, which in the specific implementation correspond to semantic attributes. In addition, the authors perform a more detailed analysis of their proposed approach and show that it results in good downstream performance on an object recognition task. They have responded to the questions I had, and they address many of the concerns I had. Positive:  The proposed task, zero shot synthesis, is interesting. I feel the paper is borderline, with a slight inclination to reject. to see if results hold?<BRK>And GSL enables learning of disentangled representations of tangible attributes and achieve novel image synthesis by recombining those swappable components under a zero shot setting. Overall, I think the paper is well written and the idea of GSL leveraging the underlying semantic relationships between samples to learn disentangled representations of tangible attributes is pretty novel and interesting.<BRK>This paper proposes a Group Supervised learning method for zero shot synthesis. However, the authors may pay attention to the following questions:1. Besides, the total framework is commonly used in GANs for attribute control generation.
Reject. rating score: 5. rating score: 6. rating score: 6. <BRK>However, the novelty of this paper is limited since the proposed method is just an extension of TAS. Based on TAS, the authors propose a neural channel expansion (NCE) method to search for the width across the layers under uniform precision quantization. 3.Experimental results on CIFAR 10 and ImageNet demonstrate that NCE is able to improve the performance of the quantized networks. The novelty of this paper is limited. 2.This paper is quite similar to the work of Shen et al.[2], which also explores the optimal layer width for quantization. Thus, it would be better to compare the proposed method with Shen et al.’s.3.As mentioned in Section 2, there are several existing works [3][4][5] that reducing the quantization error by increasing the widths of the network. More comparison between the proposed method and the existing works would make this paper more convincing. However, Figure 1a only provides the results of W2A2 and W32A32. Thus, it cannot illustrate whether the large quantization error results from the quantization of activation or not. At least, the results of W32A2 and W2A32 should be provided. However, it is not clear how the dynamic range of activation impacts model performance. 8.In Algorithm 1, the threshold T is important to the proposed method.<BRK>The response clarifies some of the novelty issues, and it clearly shows the advantage compared to previous methods like TAS. ### ClarityOverall, the paper shows the concept clearly. However, I still have concerns about the novelty; the insight why the proposed method is better than TAS is still not very clear to me. The method itself is not complicated. 2.It focuses on an important topic: neural architecture search under (low bit) model quantization, which has been less explored before. The authors claimed that "none of the approaches shed light on understanding the benefit of the increased number of channels when a network is trained for quantization", which I think is an overclaim. As for the NCE method itself, it is like an extension of TAS by allowing expansion in search space and under quantization. The core novelty seems to the extended search space with channel expansion, but the other parts are largely the same. However, the authors did not compare to existing results that modify/expand the network architecture for better quantization accuracy (e.g., Zhao et al.2019).Without comparing to existing methods on the same topic, it would be difficult to evaluate the contribution.<BRK>The result is uniform precision, hardware friendly DNN. A few experiments show convincingly that a wider layer is indeed less sensitive to quantization. The paper is that the idea is fairly simple, and the results are not too impressive. LSQ seems to already do very well and it also uses uniform quantization. One major issue I have with the comparison in Tables 1 and 2 is that, on some of the smaller networks (ResNet 32 for CIFAR and ResNet 18 for ImageNet) the NCE result has more params than the other methods. I would also like to see exactly which layers were reduced in size on these networks. I understand that NCE only requires one training run while the original NAS required many retrainings. Minor issues:   Section 4.3.2 Typo: "results of the 2X case are inferior to the 2X case"   Table 2, ResNet 18, you highlighted your own result but LSQ seems to be better in accuracy and param size? EDIT: Raised score from 4 to 6 after the authors clarified some points and added additional experiments.
Accept (Spotlight). rating score: 7. rating score: 7. rating score: 7. rating score: 7. <BRK>To this end it proposes two different modifications to the standard PSRO setup: 1) Mixed Oracles, and 2) Mixed Opponents. The paper is mostly focused on two player zero sum games. **Minor**   Environment dynamics should be SxA rather than OxA  Algorithm 4.1 does not exist? Fig 1.Is it because of PSRO with Nash. Would a different MSS work differently?<BRK>StrengthsThe paper proposes two convincing alternatives to reuse previous knowledge in the PSRO framework. The two ideas are based on Q mixing approach: the first one uses this method to transfer Q values across epochs, the second one to design a new training objective. The experiments show that the proposed methods find a good solution using less simulation than the original PSRO framework. WeaknessThe paper is not very novel, since it uses previous approaches (PSRO and Q mixing) to transfer knowledge for the PSRO framework. I am not aware of recent works in this framework but could be useful to compare the proposed approaches with P2RO [1].<BRK>##########################################################################Summary: The paper provides an interesting approach to speeding up the convergence time of the Policy Space Response Oracles framework by re using the Q functions of past best responses to transfer knowledge across epochs. ##########################################################################Reasons for score:  Overall, I am low confidence on my assessment of this paper due to the exposition in the algorithm section being relatively confusing. I would certainly be willing to update my score if the paper was clearer to read. 2.The idea of reusing the prior Q functions and just mixing them together rather than re learning all of the policies is very good. They are defined in the algorithm boxes but are not clearly defined elsewhere.<BRK>The paper suggests two techniques to improve the calculation of empirically figuring out a Nash equilibrium using an iterative application of best response dynamics. The experiments show a faster reaching to NE than without these changes. The paper is well written and explained, and is accessible even to researchers not well versed in ML topics. I was convinced by further introspection that this is a significant enough contribution, to merit acceptance. Of course, a more significant change to the algorithm, leading not only to a shorter time but to convergence to better equilibria (in cases where multiple exist) would be far more compelling.
Reject. rating score: 3. rating score: 4. rating score: 5. <BRK>In addition, the experimental part is hard to follow : the outputs of the model and the human labeling information are not clearly stated (what are the different diseases type ?it is the same with diseases status ? In summary, I can not state if the problem that the authors addres is a hard one or with simpler models same results can be achieved. In my opinion, the paper is off topic. I am sure that the subject is interesting and important but I don t think that ICLR is the right conference for this paper.<BRK>And the proposed model based on GPN makes sense. Although, it seems the GPN may not outperform a baseline MLP from Table 1. Overall, this paper is a demonstration of utilizing machine learning methods and applying them in a plugin and test fashion to problems in natural science. Keeping the nature of the venue in mind; my main concern is the relevance and appropriateness of the technical content of this paper to ICLR   i.e., the authors chose a wrong venue.<BRK>Sensor and environmental data collection from beehives together with sparse inspection data. This may be important contribution to the domain, but not a direct contribution to ICLR. 2.I found the figures to be illustrative, but not very informative. I am not an expert in beehive monitoring, so it is hard for me to assess the novelty of the work done as specific to this domain.
Reject. rating score: 3. rating score: 3. rating score: 4. rating score: 4. <BRK>The paper contains two curriculum learning algorithms of which one assume knowledge of the parameters found by the baseline, uniform sampling, model to push updates in that direction, and the second orders images according to an increasing stddev/entropy of pixels. It s not clear why would one even need optimization again, if (a good enough) result is already known and gains from this re optimization do not significantly improve over this baseline. Consider deferring the use of term to that place. I m keeping the rating at 3. 1)+2).I m still not convinced that it s fair to claim an improvement of X% for a curriculum that, relying on final weights of a "vanilla" SGD trainedmodel, converges in _additional_ X% to the 100% of "vanilla" time. 3).Fair enough, but the revised draft still reads like examples are sampled from it.<BRK>The experiment results show that the proposed curriculum learning strategies can speed up the convergence by a large margin and the authors provide some insights about why curriculum learning works. The proposed "dynamic curriculum algorithm" can speed up the convergence by ~45% and the proposed task specific curriculum strategy based on standard deviation and entropy can yield an average speedup of ~43%. 2.The code and data are shared and helpful for reproducing the experiments conducted in the paper. Although some related works are discussed scatteredly in the paper, it might be helpful to have a specific section to compare related works with the proposed methods, which makes it much easier to identify the contributions and novelty of this work. Besides, although I found the experimental settings in the supplementary, the main paper at least should have discussed the basic experimental setup to understand how the experiments are conducted. 2.The proposed DCL algorithm requires an optimal weight or a local optimal weight to calculate the difficulty scores. After reading the rebuttal and other reviews, my main concerns about the novelty and computation cost are still unsolved.<BRK>They first propose a curriculum named DCL+ that is designed to order data points based on their alignment of gradient with the direction of optimization. Both DCL  and DCL+ eventually seem to classify the test set correctly. Cons:  The empirical setup needs improvement. The theoretical contributions of this work is not enough to justify having limited experiments. The combination of these makes it hard to make any conclusions. No comparison is done with optimization methods other than SGD. The ideas are not formalized well enough. Also, it is not clear why standard deviation of an image or its entropy could be a proxy for how useful a data point is for training. There is no empirical comparison between the DCL method proposed in prior sections. “We approximate w ̄ with w ̃, which is a local minima obtained from training the vanilla SGD model”, isn’t the goal of the entire training to find w^bar? Tune both base and the exponent? There are numerous other learning rate schedules that are more common like the step decay.<BRK>#### Summary  This paper considers curriculum learning for neural networks in the context of supervised learning (specifically image classification). #### Questions  Why does this work not consider comparisons with prior CL methods other than the vanilla baseline? DCL+ uses a scoring function based on the alignment of an example s gradient with the vector from the current weight to a local minimum weight (obtained via a previous "vanilla" run with standard SGD). DCL+ is empirically shown to result in marginal improvement over the vanilla run in terms of test performance at convergence, and a significant speedup to reach vanilla performance. Do you mean that there is a lack of theory, or a lack of empirical evidence, or both? Second, the authors propose and assess a few curricula based on scoring functions that only use per example statistics, e.g.standard deviation (stddev) or entropy of pixel values. The comparison between DCL+ and DCL  shows that minibatch ordering can matter in CL. This is sorely needed for, e.g., Experiment 4. #### Recommendation  I currently recommend rejection (4). While the ideas are simple and interesting, the weaknesses in this submission preclude it from being very informative or useful to the community.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>I can see that the authors have spent a lot of efforts in paper writing. The proposed model is novel, and the theoretical contributions are also new to the community. The topic is very important for realistic machine learning problems, and is helpful for reducing the human annotation efforts. 2.The theoretical study of this paper is quite impressive. Cons:1.The authors claim that their method does not need to estimate the label transition probability or noise rate, which I think is nice!<BRK>The authors of the paper addressed carefully the concerns I raised above. All in all, the paper is clearly written and easy to follow. I find the intuitive justification for confidence regularization in Section 2.1 to be quite unconvincing. From my understanding, this is not necessarily true at all.<BRK>And there may exist some adversary samples that make the rough estimation impossible. The author s approach is to rule out samples whose losses are larger than an adaptive threshold. Does CORES2 fail in this setting in practice? Overall, the paper s approach is novel and easy to implement. Unfortunately, I think the theoretical analysis for the noise robust loss is orthogonal to their sampling sieving approach. CORES2 achieves the SOTA results in all the experiments. ii) The proposed loss is proved to be noise robust. We can get the same $\beta$ by very rough estimation(their approach) in instance independent noise settings. Overall, following author s response, I am leaning towards acceptance, but will let the AC judge the importance of the points above for the final decision.
Reject. rating score: 4. rating score: 5. rating score: 6. <BRK>However, I doubt the novelty and machine learning contribution of this paper: 1) The different routing strategies are natural and seems to have already been proposed by previous works. e.g., for task level routing, [1] used similar kind of mixture of experts in the language level. 2) This paper simply studies different routing strategies, which is more like empirical analyses. The MoE models should compare with a single multilingual model with the same amount of parameters. However, Arivazhagan et al., 2019 shows the authors from Google, which reveals the organization of authors in this paper.<BRK>This paper introduces several routing strategies for multilingual neural machine translation. The motivation is to train a single mixture model that can serve the training and prediction of multiple models. Specifically, several strategies are proposed: token level, sentence level and task level. While this approach is simple and straightforward, I have some concerns. Building a mixture model for multi task learning has been well studied in the literature [1,2] (not cited).<BRK>In this paper, the authors explore alternatives to the standard token based routing in sparsely gated MoE models for multilingual NMT. This exploration is motivated by the need for efficient inference in MoE models, for which token based routing is a limitation. The results show that task level routing is comparable to token level routing with the added benefit of inference efficiency. The experiments are well described and the paper is well written.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 7. <BRK>This paper introduces SDRL, a representation learning algorithm for supervised learning. In particular, they pick the optimal transport one given by Brenier s theorem. I think the paper needs more work in the following on 4 very important aspects:A) The paper is not very well motivated in my opinion. Why do we want to learn a representation with statistically independent and rotation invariant coordinates? This is very concretely seen in the sentence "We also require that the representation is rotation invariant in distribution. B) The method is very complicated. I know the authors have several choices for V, but the choice they pick is quite particular and a complicated one at it. Without much motivation or experiments ablating this choice it s hard to justify it. D) The experiments are extremely underwhelming.<BRK>The authors present a new representation learning algorithm that trades off between a sufficiency condition (that is, the label should be independent of the input conditioned on the representation) and what they call a "disentangling" condition   that the representation vectors should be independent of one another and rotationally invariant. While the first condition has been used to define disentangled representations, the second is not standard. The paper is written with quite a lot of complicated math. These are 3 of the most over studied datasets in all of machine learning   literally thousands, maybe tens of thousands of papers have been written on various deep learning algorithms applied to these datasets in the last several years. The experiments presented here do not seem to be a proper apples to apples comparison, however. Yet in this paper, the non identifiability of different directions is given as a *necessary condition* for disentangling. Given this, I would recommend that the authors remove any reference to disentangling, and rewrite this purely as an alternative approach to supervised learning.<BRK>This paper proposes a new representation learning framework for supervised learning In order to achieve good prediction accuracies while maintaining some desired properties. The sufficiency and disentangled properties of the representation are formalized inthis paper to achieve this goal. This seems to be related with statistical correlation,can this be written out in the paper? As when I read the proof of theorem 4.2, I do not know what is the \rho(R,R*). For multi class classification problems, it is not always clear how to choosea metric on Y. I recommend the authors to clarify this choiceat least in the numerical experiments. The objective (7) is formalized as a bi level optimization problem, and the computation problem is addressed using recently proposed methods based on gradient flows.<BRK>This work proposes a method, SDRL, for learning sufficient and disentangled representations, with the additional property that the representations should also be rotation invariant. On the other hand, the repressentations are required to be sufficient for predicting the target labels. These two goals motivate the Lagrangian formulation of the objective function, based on which the authors apply two different estimators for these two goals. Other than the technical convenience brought by this property, is there any other reason that could motivate this criterion? The original Z   g(X) is only sufficient, but not disintangled nor rotation invariant.
Accept (Spotlight). rating score: 9. rating score: 7. rating score: 7. rating score: 7. <BRK>A little more discussion would be beneficial in this regard. Apparently, it does not seem difficult to act sooner, as one only has to reduce the width of allowed boundaries, but does this have any effect on exploration? that is not clearly discussed in the article.<BRK>The paper starts with outlining the challenges of the problem, statespace size, action set size and exploration issues. It then introduces the "afterstate" abstraction, which the authors argue is useful for the task because it should be more compact than the "true" MDP state representation. The paper finishes by showing that the proposed approach outperforms several baselines by a big margin.<BRK>> It is not entirely clear to me that the representational innovation (combining sMDPs+ above is in fact original.The authors seem somewhat timid in claiming it. Table 2: Advisable to mention the metric used in the caption of the table, to make skimming the paper easy.<BRK>For example, the intro talks about the difference between a state action pair and an afterstate, but it is not apparent by that point in the paper what the difference is. The clarity of the paper can be improved.
Reject. rating score: 3. rating score: 5. rating score: 5. rating score: 5. <BRK>This leads to the consideration of reinforcement leaning algorithms for mean field games (i.e.with an infinite number of agents) and obtain the convergence of a single loop fictititious play algorithm to the Nash equilibrium of entropy regularized mean field games. The paper is well written and technically sound. Recommandation; Given the scope of the paper in comparison to the one of the ICLR conference, together with the weaknesses detailed above, I recommend to reject the paper.<BRK>This isn t exactly the case here, and the title+abstract make it look like the contribution is to study the behavior of FP in a mean field game, and not to solve a Mean field game with an algorithm that has some resemblance to FP. (is this even the case?) Since you do provide the convergence rate, converting that to complexity seems straightforward, isn t it? Assumptions   I think that providing more discussion and intuition about each of the assumptions will make the paper more interesting.<BRK>While existing fixed point algorithms require to solve the MDP induced by the current mean field state exactly, the authors propose to take the model free RL approach which is able to handle continuous state space. Also, the single loop structure of the algorithm is often favored over the nested loop scheme in terms of the practical performance (which unfortunately is not validated through empirical study in this paper).<BRK>This is the actual source of the limitations of the existing double loop algorithms, which still cannot be avoided by the proposed algorithm in this paper. To this end, the authors propose a PPO based fictitious play algorithm and show that it converges to approximate NEs at a sublinear rate. "Q learning with nearest neighbors." However, due to the technical limitations and weaknesses mentioned above, I think the paper is still not ready for publication in ICLR in its current format.
Reject. rating score: 5. rating score: 5. rating score: 7. rating score: 8. <BRK>After discussion with the authors and reading the other reviews I still believe that the paper should not be accepted and therefore stay with my original review. While the authors have shown improvement over previous work (MH GAN) using a very natural idea, this previous work in turn has not provided sufficient evidence that MCMC GANs are useful. ### Questions/Suggestions  The only motivation for the use of MCMC techniques is to not "waste the discriminator".<BRK>The idea is clear: for high dimensional x, making a good proposal is difficult, so they propose to do that in the latent space. * One of the arguments of the paper (in the related work) the method overcomes sample low sample diversity. The authors need to report LPIPS [1] and compare different methods.<BRK>As an example, the authors provide a Langevin version (which uses gradient information) of their method. I enjoyed reading this well written paper and think the re parameterized MCMC chain is a very neat idea that fits very naturally in the GAN framework. This would make the paper stronger.<BRK>The paper is a good idea. The paper is able to do much of the sampling in the latent space, which enables the use of gradient based sampling. However, this is not explicitly stated at all in the paper.
Reject. rating score: 6. rating score: 6. rating score: 6. rating score: 6. <BRK>This paper studies how to estimate the performance of pruned networks using regression models. The authors first empirically observe that there exist three distinct regions of sparsity: (1) In the low sparsity regime, pruning does not decrease the accuracy (2) In the mid sparsity regime, a linear relationship between the sparsity and the accuracy is observed (3) In the high sparsity regime, pruning does not decrease the accuracy again. Under this observation, the authors proposed a regression model called the rational family and empirically verified its performance. The authors further extended this model to incorporate the network width and depth under some empirical observation called the error preserving invariant. The authors performed experiments to verify different perspectives of the proposed functional form. Overall, I like the main idea and experiments on the interpolation/extrapolation using the proposed functional form. The authors removed a huge amount of test configurations: about 8000 configurations among 12000 configurations are eliminated for CIFAR 10 ResNet. It significantly reduces the reliability of estimations from the proposed functional form as the removed cases are occasionally observed in practice. The authors only consider limited experimental setups.<BRK>This functional approximation depends on a number of hyperparameters that are fit on the error of already trained and pruned networks on a certain task (in this case, image classification on CIFAR 10 and ImageNet are the tasks under consideration). +The experimental section is very strong. While the authors have convinced me that the behavior of a network after pruning under different densities does follow a power law, the expression they derive depends on fitting 5 hyperparameters (equation 2), and after the fit can only be used only for that specific architecture and dataset. Trying different CNN architectures and datasets (small datasets such as SVHN, mnist or fashion mnist would have been fine, as long as quite different architecture types would have been probed). RecommendationI recommend a weak accept for this paper, in light of the following considerations: the paper is well written and it provides an interesting insight (the power law structure of the error as networks are pruned), the paper’s formula could be useful to applied practitioners. However, I believe the way the results are presented are strongly overstated. Maybe a plot with fit residual as a function of fit points? * In Eq.1, it would have been useful if p had been introduced in the notation section.<BRK>This is interesting for practitioners when determining how to change an architecture to achieve desired performance by fitting the proposed power law to a few trained networks. The resulting parameters of the functional forms yield a direct interpretation. The work is self contained and well written## Cons  This is a very high bar, but let me phrase this distant goal anyway: This work does not provide an ab initio derivation of how network depth, width, dataset size and pruning are related to the test error  There is room for considering more datasets, architectures. I would imagine that e.g.different choices of hyper parameters might yield different laws. If the primary goal in the pruning literature is badly chosen in terms of resulting number of parameters, does it still serve as a benchmark for comparing pruning methods? Maybe you can reduce the number of footnotes in the paper.<BRK>The paper investigates the behavior of the test error as a function of the density of thenetwork after pruning and identifies 3  regimes:1) a low density high error plateau; 2) a high density low error plateau; 3) a power law behavior for intermediate density. The approximating function contains the unpruned network s error and other 3 parameters that have to be fitted on each architecture. Two algorithms, iterative magnitude pruning and SynFlow, are used for pruning. The proposed functional form is in good agreement with the experiments on each combination ofarchitecture, dataset and pruning algorithm presented. Numerical experiments involve only image classification tasks, on a small set of datasets and architectures. It implies that the region of parameters for which the predictionis a good approximation to the error is not well characterized. Therefore the applicability of thistheoretical framework seems very limited. Minor Comments:  Fig.3: would you consider adding depth vs width?
Accept (Poster). rating score: 8. rating score: 7. rating score: 4. <BRK>This seems like a limited setting. Section A.2:  “Equation equation 7” —> “Equation 7”Figure 7:  “Unconditional CIFAR10 Samples” —> “Unconditional MNIST Samples” A more complete set of experiments would also include other learned sampling methods, such as MEG. The authors demonstrate their method in settings where exact log likelihood estimates are feasible (a flow based model and probabilistic PCA). This allows the authors to demonstrate both improved samples and log likelihood performance. With a more detailed motivation/explanation of the design choices, discussion of the quality of the approximation, and some additional sampling model baselines on the JEM results, this paper could be further improved. For these reasons, I would be in favor of accepting this paper. Is using NICE in Section 5.1 representative of energy based models? The main contribution of this paper is in estimating the integral of the sampling model’s entropy with an importance weighted variational approximation.<BRK>This paper proposes a new method on training energy based models with maximum likelihood. Instead of using MCMC approaches to sample from the EBM, authors follow previous work on training neural generators for faster sample generation. Authors demonstrate that their method can train JEMs in a stable way and outperforms baselines on classification accuracy, sample quality, out of distribution detection and semi supervised learning. * Writing is clear and easy to follow. In VERA training, $\gamma$ controls the gradient penalty and $\lambda$ controls the contribution of the estimated entropy gradient. * The idea of training a neural generator with the dual form of the likelihood objective has been explored before. It would be better if authors will include it in both NICE and JEM experiments. I do think that proper comparison with previous work is important, as it allows us to know better when and where the proposed approach is beneficial.<BRK>This paper proposes an improved algorithm to train EBM based models, called Variational Entropy Regularized Approximate maximum likelihood. To estimate the gradient of the entropy term, the authors then propose a variational formulation for approximation. The idea is interesting but not very exciting, and there consists of technical details that need to be carefully deal with. One of my biggest concerns is that since the algorithm relies on importance sampling to estimate the gradient of log marginal likelihood, the variance can be arbitrarily large. One reason is that even if they fix z_0, the proposal distribution will be a z_0 mean Gaussian, and the mean is randomly drawn from N(0, 1), which will not match the true posterior distribution (they only optimize the variance parameter). I think this should be make clear and investigated in more details.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 6. <BRK>The paper presents a new way for approximating posteriors in Bayesian DNN. One uses only point estimates while another one uses full (non diagonal) Gaussian approximation. The structure of that subnet is found by taking largest second derivatives of Hessian of linearized DNN (the authors call it generalized Gauss Newton (GGN) matrix). In the experimental part they provide a set of explorative experiments showing that it may be better to use their approximation for inference in large newtork than both using standard (simple) approximations in large network and full Bayesian inference in small network. This is very nice methodologically and I welcome such demonstration but this can only be considered as a (good) proof of concept. Pros.1.Interesting idea of finding a better approximation for the posterior on subset of parameters. I would not call it an optimal strategy   it rather looks like a reasonalbe heuristic. The authors claim that their method estimates uncertainty better than all baselines including deep ensembles (DE). They use only DE of size 5 while their method requires approx.<BRK>The authors present a new method for Bayesian deep learning motivated by the difficulty of posterior inference in the "overparameterized" regime of deep neural network models. The proposed method provides a principled strategy for selecting a subset of the neural network s parameters (forming a so called "subnetwork") for which a full covariance approximate posterior can be computed. The authors use the well studied Laplace approximation with the generalized Gauss Newton Hessian approximation for the covariance. The case for improved "robustness" also appears to be equally unfounded on these grounds. The presented approach is novel and appears to be a promising contribution to the study of Bayesian neural networks. As a fellow Bayesian, I applaud the authors  efforts. Unfortunately, the paper has a number of significant weaknesses which I detail below. The authors  experimental results appear to me to not sufficiently support some of their claims. There are also a number of formatting issues. I would be happy to revisit my rating after revision and discussion. Please try rephrasing to make it more clear. This seems to, at least, preclude any ideas about possibly applying gradient optimized MAP directly to the subnetwork posterior. "We now analyze the following procedure..."Are you analyzing the procedure for selecting a subnetwork? This is not made clear. Which one are you using? Related to previous point, is this the same step as outlined in section 3? 6.In general, this section needs work. In light of points 15 and 16, I do not think you have sufficient empirical basis to make the claim that "...it is better to perform subnetwork inference in a large model than full network inference in a small one".<BRK>The authors select a sub network and infer approximate posterior distributions over the weights in the sub network. All other weights are estimated via MAP point estimation. A sufficiently small sub network allows high fidelity posterior approximations that do not make restrictive mean field assumptions to be tractable. The paper is generally well written and easy to follow. The idea that BNNs have too many parameters for reliably inferring posterior distributions over all of them is a reasonable one. However, since the true posterior is intractable, the authors instead appear to minimize the distance to an Laplace approximated posterior q(W | data). While this is fine for smaller models when the approximation can use a full covariance,  why does it make sense for larger models where the authors use diagonal approximations to the generalized Gauss Newton matrix (Section 5.3)? How much worse is the  random selection strategy on the tasks listed in section 5.3? If instead the diagonal posterior approximation is used to select the subnetwork in these models, does the resulting approximation still improve upon the diagonal approximation (as seems to be happening in the experiments in 5.3)? * The experiment in section 5.3 has other curious issues. Different methods appear to be using different priors (Gaussians with different precisions). What priors were used for deep ensembles and SWAG? * The grid search by which the prior precisions were selected needs more details. This does not seem like a sensible prior. * At the very least there needs to be a discussion about sparse Bayesian deep learning techniques (and preferably an empirical comparison)[1, 2, 3], that use sparsity inducing priors to prune away weights and nodes from a larger network instead of the approach presented here. Overall, although I have several concerns they primarily stem from experimental issues in 5.3.<BRK>More specifically, they propose a method for scalable BNNs via a (full covariance Gaussian) Laplace approximation on a (Wasserstein based) pruned subnetwork within a deterministically trained model. From the experiments, they show that their method generally outperforms comparable methods (including deep ensembles) on metric performance and on the ability to capture in between uncertainty. #### Strengths  Scalable approximate inference for Bayesian models is an important research area. In general, the paper is well written, clearly motivated, includes both theoretical and empirical results, and adequately compares to, or discusses, relevant literature in the space. #### Weaknesses  The authors push on the idea of *scalable* approximate inference, yet the largest experiment shown is on CIFAR 10. Given this focus on scalability, and the experiments in recent literature in this space, I think experiments on ImageNet would greatly strengthen the paper (though I sympathize with the idea that this can a high bar from a resources standpoint). #### RecommendationOverall, I believe this is a good paper, but the current lack of experiments on a dataset larger than CIFAR 10, while also focusing on scalability, make it somewhat difficult to fully recommend acceptance. How does your proposed method compare? I think this would be an interesting experiment to include, especially since the consensus in Ovadia et al.(2019) (and other related literature) is that full variational BNNs are quite promising but generally methodologically difficult to scale to large problems, with relative performance degrading even on CIFAR 10.
Accept (Poster). rating score: 8. rating score: 7. rating score: 5. rating score: 5. <BRK>2.The paper is close to PDDM as acknowledged by the authors in the introduction sections. The reader is referred to Figure 2 and the results not discussed at all. The paper is well written, the related work section is thorough the results  convincing. The model based approach to off line learning is a very promising avenue.<BRK>I feel that Algorithm 2 should be explained better. 2.MBOP shows to be a strong baseline for offline reinforcement learning, especially in small data regimes, which is especially interesting. The constrained objectives are especially interesting with this respect. MBOP also learns an expected return model in addition to the reward model in MOPO. If these are the factors that affected the boost in performance, it would greatly reduce the general interest of the proposed approach. When and why do the authors assume such interpolation should work? Overall it feels like the paper is not self contained and hard to understand.<BRK>The paper demonstrates experimental results on standard benchmark tasks (RLU and D4RL) as well as zero shot adaptation results. The strengths of the paper are showing results with an MPC based approach and zero shot adaptation results. Thus, while MBOP provides an interesting case study in the use of MPC for offline RL, there is limited novel insight. (1) **Exaggerated novelty:** This paper must be considered in the context of a number of recent model based offline RL papers. I would recommend the authors rewrite the paper, to better situate the contributions of this work relative to what is known in literature. The Adroit tasks were proposed in [5] but this paper is not cited. If so, the consistent trajectory sampling approach has been utilized in a number of prior works including Game MBRL.<BRK>This paper offered a model based offline planning by taking advantage of learned behavior cloning  policy and learned value function in addition to learning the model of the environment. Model learning is adapted from a work from Nagbandi et al.As the method takes advantage of behavior cloning  policy, its performance boosts when the prior policy is reasonably well (medium or expert level). Having said that, at least from the algorithmic point of view, this work does not seem to have a huge contribution. Moreover, I think it is useful to mention it in the paper as a related work. Also, the authors mentioned that  "Along with this high level design, many implementation details such as consistent ensemble sampling during rollouts, or averaging returns over ensemble heads, appear to be important for a stable controller from our  experience."
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 4. <BRK>This paper investigates the very important problem of incorporating physics into machine learning model with special focus on the missing flow estimation problem. The idea is combining reverse mode differentiation into the modeling is intuitive and also shown to be effective.<BRK>Can the authors comment on learning more general regularizing matrices? The authors propose a parametric regularizer for estimating unobserved flows in networks, incorporating edge features and other side information. The paper is clearly written and easy to follow.<BRK>The estimationg of unobserved flows is based on flow conservation and edge features. is the learning problem (bi level optimization problem) intriniscally hard (non convex)? the numerical experiments are not very convincing.<BRK>In this paper, the authors introduce a method for missing flow estimation. The model is based on the assumption of flow conservation. What is the difference between this predictive framework and the proposed bilevel optimization framework?
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 7. <BRK>Pros.The results are promising. Cons.i) The motivation to propose LRGA by replacing the softmax layer in the self attention mechanism [1] by the global normalization is not well enough. Thus, LRGA is incremental and not technically sound. ii) The paper does not discuss the most closely related work, Dual Graph Convolutional Networks (DualGCN) [2]. iii) The roles of m1, m2, and m3 are similar to the query, key, and value matrices in the self attention mechanism, respectively. iv) Given the same GNN module with the same hidden size, the proposed LRGA+GNN has much larger parameters than GNN. This limitation restricts to use of deeper layers.<BRK>##########################################################################Pros:+ Overall, the paper is well written and structured. The theoretical analysis of RGNN and its augmentation with LRGA is interesting. A similar theoretical analysis is also conducted in [Puny et al., 2020]. It should be 81.25%. The introduction of LRGA into the RGNN is interesting with the theoretical analysis, although the network design is incremental.<BRK>So if as claimed by the authors that if one network has algorithmically alignment with an algorithm, the network has good generalization guarantee, then the theoretical results in this work can guarantee better performance of the proposed LRGA. 2)	The experimental results also show the superiority of LRGA. This is hard to understand intuitively. The authors well prove the algorithmically alignment of LRGA.<BRK>It is unfortunate that a comparison with diffusion graph augmentation could not be added, but I understand the reason provided by the authors and this anyway does not significantly detract from the presented results. Along the way, they also provide interesting insights into the universality of random GNNs, which I must say, may be a bit lost when only presented as a sidenote that is not directly related to the main point of the paper here. Nevertheless, it is an interesting contribution. The proposed approach here is interesting, well motivated, and supported by solid theoretical and empirical evidence, and therefore I recommend accepting it to the conference. A comparison seems highly warranted here.
Reject. rating score: 5. rating score: 5. rating score: 6. <BRK>Summary:This paper proposed a lower bound on the mutual information by introducing a conditional mutual information term. Reason for the score:I think the authors could improve their paper by comparing with more methodologies such as the information bottleneck (IB), and various approximations such as MINE from ICLR 2018 (https://openreview.net/pdf?id rJHOuiqaf). Pro:  The authors provided experiments in vision, dialogue, and synthetic datasets on different models. Con/Questions:  Some experimental setups are not too clear. For example, the authors use both $x, y$ to denote data features instead of data features and data labels which is quite uncommon. "views" is also used frequently in this paper which I can only assume that the authors meant "data samples". I have now updated the score.<BRK>This paper proposes a contrastive learning approach where one of the views, x, is converted into two subviews, x  and x , and then separate InfoNCE style bounds constructed for each of I(x ;y) and I(x ;y|x ) before being combined to form an overall training objective. Experiments are performed on both vision and NLP problems. Further, I feel the experimental results are somewhat weak, both in terms of the fact that the gains shown are quite small, but also because there are doubts as to whether these gains are actually statistically significant at all or just a somewhat "engineered" win that does not accurately reflect the underlying approaches. + This is a highly active research area that is very much of interest to the ICLR community. + The paper is mostly quite easy to follow. Though not bad, the writing of the paper could definitely be improved. This is at the core of the problem and arguably why most papers have steered clear of dealing with conditional MI directly. Though this is far from fatal in the context of the overall approach, it feels like if we are able to learn a good approximation for p(y|x ), we have already solved much of the challenge contrastive learning was trying to address in the first place.<BRK>This work proposes to decompose mutual information estimation into subtasks, based on the chain rule of MI, i.e.Eq.(6).A novel bound for the proposed decomposition insight is achieved based on the classic InforNCE bound. The proposed insight is understandable and reasonable to achieve the desired goal. it is also presented to be effective to achieve higher MI and results in better representations for classification. Can I understand your work as a variant of this work performed with InforNCE estimation?
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 7. <BRK>A major issue of the paper is that the authors just do predictions in a single stock price. The authors need to add more datasets. It is not enough. This is interesting work and the authors do good work at explaining the different parts.<BRK>However, I have the following comments on the paper:   Dataset collection is not convinced. However, Tesla also has the similar issue. The dataset with only one company contains bias to the model that may not be generalized to other cases. Lack of novelty. All the technical components in this paper are not new.<BRK>2.The writing can be improved. For example, in the experiment section, the authors used a lot of evaluation metrics such as sharpe ratio, greedy, threshold. The topic is interesting. Overall comments:I think the research topic of this paper is interesting, but the methodology seems lack of novelty.<BRK>Pros: The paper is clear with a significant contribution. Cons: 1.The title of the paper does not describe the actual work done. That is, how would this work be reproduced or be beneficial to the community?
Reject. rating score: 2. rating score: 5. rating score: 5. <BRK>I hardly agree with the assumption made in the graphical model c) and d) that each cluster of data has a different generating mechanism from others. The ICM, in the literature, originally describes the independent autonomous for generating a sequence of variables in the causal graph, e.g., p(v_1,...,v_k)   \Pi_k p(v_k | Pa(k)), based on the assumption that the exogenous variables are independent with each other. Since the latent variable z is defined to describe the high level abstractions or concepts, including but not limited to the thickness, width, length in the example of MNIST. The high level spirit of this paper is based on an invalid assumption, which makes the promising results in experimental parts non important.<BRK>In more detail, it is assumed that each observation in a given dataset was generated by exactly one out of a set of independent generative processes referred to as independent causal mechanisms (ICM) in combination with a global mechanism that is shared across modules. Learning is performed by combining the GAN loss with self supervision by using a separate encoder and adding loss terms resemblant of cycle consistency losses. The paper claims to prove a notion of identifiability of the coarse grained modules in the sense of separation into disjoint manifolds and conducts some experiments on (variations of) MNIST and Fashion MNIST, comparing their method against vanilla and disentangled VAE variants on some downstream tasks. **Pros**  the paper tackles a very interesting and highly relevant topic (disentanglement with non factorising latent space and connections to causality)  the coarse grained view and the separation into independent and shared mechanisms seems like a useful abstraction and (to the best of my knowledge) has not been investigated before in this form  the authors support their proposed method with some theoretical analysis  the latent traversal in Figure 2 and some of the quantitative results look promising **Cons**  even though I am very familiar with the topic of the submission, I found the paper very hard to follow  many of the statements relating to causality and ICMs are inaccurate, misleading, or wrong (see detailed comments below)  the notion of identifiability used in the paper is very unconventional and not consistent with prior work  the assumed generative model is never fully specified (in particular, the mapping from latents to observations and the role of the mechanisms is not described in the text or given as formula) and the mixture prior is inconsistent with the graphical model given in Fig.1  some of the cited works are misrepresented or wrongly protrayed (see detailed comments)**Evaluation**I think the paper tackles a very important problem and presents some interesting ideas. However, the paper contains too many errors, unclear parts, and inconsistencies in the current form which makes it very difficult to gain a clear picture of the proposed method and to assess its correctness. Wasserstain  > Wasserstein  4. Note that this is different from the notion of isolated, as different ICMs may feed into each other. Based on the proposed changes I will slightly increase my score, but I still believe the paper needs additional work before meriting publication. I have never heard or read about this and would like to ask about the source of this characterisation: "If each function in FCM represents an autonomous mechanism, such FCM is called a structural model. ?This seems strange to me. Why the need for the many $z_M$ s?<BRK>Authors propose a new method to learn independent causal mechanisms from data, which is achieved by designing a mixture prior consisting of shared mechanisms and Independent Causal Mechanisms (ICM) conditioned mechanisms. A single generative model is used with such a mixture prior to learn mechanisms from data. Experiments on MNIST dataset demonstrate that the method is able to learn independent mechanisms and it improves the robustness against intervention, co variant shift and noise. I am not quite clear about the meaning of ICM conditioned mechanisms. However, in this paper, authors are considering a very different setting where they argue that for different data groups, the generative mechanisms are different. 3.Authors use single generative model which is claimed to be one of the advantage of their method. 4.It appears to me that the method can only work in the scenario where we know beforehand the number of different generating mechanisms in the data. However, for real applications it is difficult to have such prior information. 5.It appears to me that the proposed method is incremental to existing disentangled representation learning methods except for the mixture prior.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>In addition, REDQ adopts a high update to data ratio to improve the sample efficiency. Empirical results show that the proposed method outperforms state of the art model based algorithms in certain tasks with continuous action space. Overall, I think this is a well written paper. Moreover, this idea is quite general and can be easily plugged into many existing off policy model free algorithms. The numerical experiments are quite extensive. The results convincingly show that the proposed REDQ algorithm achieves a minor underestimation bias with low standard deviation, leading to better overall performance. The theoretical analysis of this work is quite limited, e.g.lacking the (most basic) asymptotic convergence analysis in the tabular case.<BRK>The paper also performs extensive ablation studies to prove the importance of each technique. REDQ is simple and effective. 3.The paper is very well written. It would be great to see analysis on the gradient as well. Does training the policy more frequently help?<BRK>Experiments on Mujoco show that REDQ achieves better sample efficiency than popular model free methods such as SAC and is comparable with model based methods such as MBPO. The paper also provides some theoretical analysis of the Q estimation bias. [Pros]  The empirical performance of REDQ seems rather strong: significantly better than SAC and can match or exceed MBPO depending on the environment. Overall the paper is quite clearly written and conveys the message quite clearly. [Cons, and comments]  My main concern is that the most similar approach Maxmin (Lan et al.2020) which the authors cited multiple times was not comprehensively tested in the experiments.<BRK>Summary: This paper proposes a new model free algorithm called Randomized Ensemble Double Q Learning (REDQ) for the optimal control problem. This statement is based on the empirical studies or intuitive explanations rather than theoretical analysis. Con.1.In Section 3.1 Theoretical Analysis, the theoretical result is not complete enough. The Maxmin Q Learning paper, Lan et al.(2020), also proves that Maxmin Q learning algorithm has a vanishing approximation variance (with N tends to infinity).
Reject. rating score: 4. rating score: 5. rating score: 7. <BRK>**1.Summary and contributions: Briefly summarize the paper and its contributions**This work analyzed the optimal solutions for the Generative adversarial training (GAT) and the convergence property of the training algorithm. The related works are very briefly discussed in the introduction, but I think that is far from enough.<BRK>Although K   0 can be thought of as exposing to outliers, the resulting algorithm is still under GAT framework. As OOD detection is suggested as a main application of GAT, it is important to compare it with state of the art. .Section 3.2: a practical consideration is that Step 2 cannot be perfectly solved and the authors thus proposed to use a p_{ k} that is uniformly distributed in the data space.<BRK>In this paper the authors provide a theoretical and empirical analysis of the Generative Adversarial Training method (GAT) which is used to train models for OOD and adversarial example detection.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 6. <BRK>This paper aims to improve sample efficiency in model based reinforcement learning (MBRL). The approach termed ReaPER is based on Dreamer (Hafner et al.2019) and the paper integrates several ideas from prior works that are known to improve sample efficiency. Experiments on DeepMind control suite show that the proposed approach outperforms dreamer across 8 environments. **Pros:**  ++ The paper combined ideas from recent papers to improve sample efficiency of MBRL. The ablation studies shed some light on how each design helps with sample efficiency. ++ The overall model outperforms prior work, Dreamer. ++ The paper is relatively easy to read. I don’t think applying these techniques to MBRL makes a compelling case. For example, L1+Contrast seems to perform worse than Dreamer, L1, and Contrast. I would recommend plotting the performance of each environment in the ablation study just like Fig.7, 9, 10 in (Hafner et al.2019).Instead of providing an average reward across different environments, providing these plots will help better understand the convergence properties of each method. **Rating Justification:**  I commend the authors for their attempts to the important problem of sample efficiency in MBRL. Thus, I feel it has not met the bar of ICLR.<BRK>The authors propose a model based RL method which is built upon a baseline DREAMER [1] with additional components which empirically improve performance:* Prioritised experience replay* Temporally consistent data augmentation with a contrastive loss* L1 regularisation# High level comment:* The contribution of authors is a combination of all the proposed ideas into one framework. The ideas are not novel and were considered in other works, so the contribution is really is a combination of these. From the experiments proposed by the authors, it s not easy to infer the impact of each of these ideas on the performance, the ablations do not provide a clear consistent signal and lack of explanation on why it provides an improvement. On top of that, the paper lacks experimental details, such as how the hyperparameters were optimised and what ranges were considered. It makes it hard to reproduce the experiments. More importantly, the proposed method underperforms with respect to the model free variants which use one of these ideas, therefore it is not clear how useful the method could be for the community. # Strengths of the paper:* Paper is relatively clearly written and high level ideas are easy to follow* Equations (1) are very helpful to be able to compare the differences of the model to any other baseline# Weaknesses:* It is hard to judge whether the proposed combination would be a method to use due to multiple reasons. First of all, the proposed ideas are not novel and were considered in [2, 3, 4] for data augmentation, in [5] for prioritised replay. * It is very good that the authors provided the ablations, but I think they do not completely answer the question of impact of each of the component. For example, for data augmentation + contrastive loss, is the most of the positive impact coming from the random crops or with both crops plus contrastive loss ? In some cases, adding L1 + Contrast underperforms with respect to Dreamer, why is it the case ? * The impact of prioritised replay is negative but when combined with other tricks, is positive. There is no explanation of this phenomenon in the paper. Maybe it would only work for control suite and not for other problems ? How do the results look like?<BRK>The experiments on DeepMind control suit show that the proposed method outperforms dreamer and the ablation study further verifies the contribution of each component of the proposed algorithm. This paper tackles a valuable problem of improving the sample efficiency of model based RL. 3.The paper is well written and the results section is well structured. They outperform baseline methods on a popular benchmark and conduct an ablation study. They use a series of techniques that have already been respectively proved to be useful. 2.This paper has a much more complex architecture than Dreamer but the performance improvement is not very significant as shown in Figure 3 but3. The authors claim the contrastive and sparse representations are useful for sample efficiency but they do not visualize and check the learned representations. They should first show their proposed loss functions can truly lead to more sparse and contrastive representations and then show such representations can contribute to improve the sample efficiency. In Section 2.2, why do the authors choose the model error for the sampling probability. Can you use the policy error or value error？2. Can you give some insights? 2.In Figure 2, it is better to compare ReaPER with Dreamer on the same state.<BRK>This paper extends and ablates several modifications to Dreamer, the current SOTA in MBRL for control tasks. More specifically, they introduce a L1 sparsity prior on the representations, a contrastive loss with random crop data augmentation and prioritized experience replay. They also test additional changes in the Appendix, like Bisimulation distances (from Zhang et al, 2020), and exploration via latent disagreement (from Sekar et al, 2020.). Overall, this is a well executed model exploration, and if the code gets open sourced will be very valuable for other researchers to build upon. The results aren’t extremely strong, but do appear significant, so might still be valuable to share more broadly. Appendix 6.2 and 6.3 could use some contextualization, as they seem to come out of the blue, even though they present some valuable extra baselines. It’d be valuable to point to them from the main text as well (it is only mentioned in the Related work quite quickly). It was pretty confusing to see the orange line flattening and having to interpret this as being better than the blue curve.
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. rating score: 6. rating score: 6. rating score: 5. <BRK>Given that there is a one to one mapping between cuts in the primal formulation (Gurobi 1 cut) and variables in the dual formulation (the new approach), I am curious why the authors did not choose symmetric generation schemes for the two. The authors present computational results showing that, due to the amenability of the methods to GPU accelaration, they can produce stronger verification bounds than comparable methods working with weaker relaxations in a modest amount more time. I like the paper and think it makes a good contribution to the literature.<BRK>The authors describe a sparse dual solver for a new relaxation which is tighter (but has higher computational complexity). The solver is represented (for the most part) as standard operations built into pytorch, and so it can be easily run on GPUs (they do require a specialized operator to support masked forward/backward passes, and they describe how this is done efficiently for convolutional networks). Experimental results are promising in that it outperforms generic solvers in terms of both the bounds achieved and the time taken to do so. This does seem to be a promising approach.<BRK>What makes the contribution in this paper strong is (1) A good implementation in multiple ways. The authors do a good job explaining how their work fits in and how it compares to other approaches. Two things I would like the authors to address:(1) Experimental results   CPU onlyHow does an all CPU version of the algorithm compare to Gurobi? This would be a better apples to apples comparison since commercial solvers do not make use GPUs. (2) Comparison with Tjandraatmadja et al.(2020)I would like the authors to contrast their approach more against the the one by Tjandraatmadja et al.The formulations used there are different (and the focus of that paper is different), they do describe a cutting plane approach, which in a sense can be viewed as an approach that incrementally increases the number of dual variables. Update after author response:Thanks to the authors for addressing my questions!<BRK>Some experiments are conducted in order to benchmark the relaxation with others. 2) Strengths+ The paper is mostly well written. It does not seem that the paper makes code and data available to the public. On the one hand, $x^hat_n$ needs to be a scalar quantity in order to have a meaningful objective and on the other hand, the only free variable is $x_0$ as all the other variables depend on it. Do you require that $min_{x in C} a *x$ can be computed in $O(1)$? d) What restricts the approach to piecewise linear functions?<BRK>It is also easy to imagine that there could be many alternate ways one could use Anderson et al.s recent formulation to improve the state of the art   in that sense, the results achieved by the authors are not surprising. Minor remarks: page 2: "if more compute budget is..."  should be "if a larger computational budget is..."Post rebuttal comment:I acknowledge having read the authors  response and I have also glanced over the updated version of the paper. Unsurprisingly, the bottle neck for this approach is the computation of lower bounds, which can be translated into a non linear (and NP hard) optimization problem.<BRK>In particular, the authors consider a linear relaxation of neural networks with relu activations and develop an active set based method. It would be great if the authors can clarify several points raised below. It looks like exponentially many optimization variables in eq 6 are initialized at zero, which provides a lower bound on eq 2. However, it s not clear if the produced lower bounds are effective. Does this approach provably converge to the solution of eq 9? 6.Last paragraph on page 2, by an optimal solution of the problem 2 are you referring to the linear relaxation?
Accept (Spotlight). rating score: 9. rating score: 8. rating score: 7. rating score: 7. rating score: 6. <BRK>The paper is very well written, clear and scientifically sound and provides  extensive contributions, e.g.a database in addition to the algorithm. Performance of the algorithm is demonstrated via three experiments. The only thing I would still like to see more is the discussion at the conclusions. Why does this seemingly simple modification to the existing SPRT method provide so superior performance. The appendices are referred a lot in the text but they are missing from the paper? I guess it should be : Long short term memory (LSTM) variants LSTM S and LSTM M impose monotonicity on classification ...<BRK>At the same time, the main text does not provide much intuition about the novel LLLR loss, which is given very little explanation considering it is presented as one of the paper s major contributions. The paper proposes a new algorithm for early classification of sequential data, exploiting approaches to density ratio estimation to enable applying a sequential probability ratio test type algorithm on perceptually rich data with no explicit likelihood. I very much hope to see it at the conference, and have updated my review and rating accordingly.] Finally, the LLR trajectory figures can use partial transparency to make the individual traces easier to see. The empirical results look pretty good as well. I appreciated the fairly detailed review of past work related to the SPRT. This is a minor issue. Broadly, a sequential test consists of an update rule and a decision rule   for the SPRT the update rule and decision rule are both optimal. SPRT TANDEM seems to be in the family of such extensions: it still applies Bayes  rule sequentially, and the stopping is given based on a fixed threshold. # Stopping rules # The paper does not provide guidance on stopping rules, which limits practical use, and does not report on the thresholds used to generate the speed accuracy tradeoff figures. Does this work for SPRT TANDEM to achieve a given accuracy? Relatedly, the paper criticizes Mori et al.2018 and Hartvigsen et al.2020 for using a separate objective for determining stopping and accuracy, but in fact SPRT TANDEM would likewise need some dynamic programming or RL solver to have an optimal stopping policy, similarly to that prior work.<BRK>SUMMARY:This work describes a new algorithm, SPRT TANDEM, for classifying sequential data as early as possible. STRENGTHS:  This work is exceptionally documented, on all accounts: related work is multidisciplinary, broad and thorough; all losses and algorithms are derived from first principles; experiments are varied and include every last details regarding their setups, evaluation metrics or outcomes. Albeit only briefly mentioned in the main, the method has strong theoretical foundations, as documented in Appendix A. Although the classification of sequential data is not among the most popular topics, I believe this contribution to be significant for the field. WEAKNESSES:  I believe the 8 page limit of ICLR does make this work justice, given the extensive documentation that comes along in the supplementary materials. The short format of the main paper makes it difficult at places to fully follow or appreciate the contributions presented in this work. (Should this paper be rejected, I would recommend it to be submitted to JMLR, which format is certainly a better fit.)<BRK># SummaryThis work introduces SPRT TANDEM an algorithm to train a sequential probability ratio test (SPRT) as a neural network. The main contribution of this work is to enable Wald s SPRT without actual knowledge of the ratio, learning a neural network to model it. # Major comments## Pros:The paper does a good job of introducing the problem statement, that is the "fast" classification of sequential data. Overall the paper is pleasant to read and introduce a new method that could be helpful for some practitioners. In particular, I would have liked a deeper comparison with LSTM s/m and EARLIEST, discussing the drawback/advantages of these methods with respect to SPRT TANDEM. This seems wrong to me. 4) It is not very clear to me what are the respective roles of LLLR and MCEL. The ablation study you did is interesting but I would have liked to get further insights about what is happening there. For comparison on NMNIST it could be interesting to see the performance of a simple classifier on the 19th image alone. A word about how it is working could be nice.<BRK>The authors propose how to use the neural networks to estimate the posteriors for each label y for the log likelihood ratio (LLR) estimation. The paper contains an interesting idea of using the neural networks for the prediction of likelihoods and accumulating the information for the conventional sequantial probability ratio test (SPRT), which is well known for explaining the speed accuracy tradeoff for decision making. The experimental results using various datasets show the relevance of the algorithm in terms of improving the speed accuaracy tradeoff. Though the authors did not mention explicitly, one is the objective for LLR which is ill posed because pairs of high biased neural network outputs can result in a small LLR by preserving only the ratio of the outputs correct. My question about this paper is the learning procedure.
Accept (Spotlight). rating score: 9. rating score: 7. rating score: 6. <BRK>The authors tackle the problem of self supervised representation learning, and validate their approach on downstream Reinforcement Learning tasks. Building on the insight that predictability in local patches is a good inductive bias for salient regions that characterize objects, the authors propose a well reasoned, well engineered and thoroughly validated pipeline for learning object keypoints without supervision. + The authors demonstrate impressive results on a number of Atari games. Would it be possible to optimize both the keypoints and the policy together, end to end?<BRK>The demonstrate their model on Atari tasks comparing to other key point detectors. Originality and Significance:This approach is novel and interesting and offers a new perspective on what an object can be and what definitions of an object may be useful for training agents. What about if you have new objects that are not predictable, but are distractors? Perhaps the authors are hypothesising that this interpretation of objects will be more useful for downstream tasks?<BRK>The keypoint discovery is based on predicting "predictable" local structure. I.e., the authors consider points that can not be predicted from its neighbor as good. Experimental results in Table. #### Weaknesses  The ablation studies are not exciting. This is not clear to the reviewer. #### SummaryThe paper proposed an interesting idea with reasonable results (better than a recent counterpart, Transporter). However, the reviewer does not have backgrounds in the specific experimental settings (Atari games), and can not assess the significance of the improvements. Comparisons with more keypoint discovery methods would make the results more convincing. However a proxy evaluation of key point evaluation is still missing and it will further strengthen the paper (I don t have a clear idea for the evaluation either).
Reject. rating score: 2. rating score: 3. rating score: 4. rating score: 4. <BRK>The definition of STI is unclear. While the topic is interesting, the paper is hardly understandable in its current form. The descriptions of the model and STI effect are confusing. Recommendation:   I strongly recommend a reject.<BRK>Crucially I don t clearly understand the STI effect as explained in definition 1. "However, all deep learning based solutions achieve several times to tens times better accuracy. See also the additional references [1, 2], and the large number of works citing them. In summarizing the results (4.1), I think the word improves is a bit unclear when talking about reduction in loss.<BRK>Figure 6: I find it hard to interpret the caption from the figure. %%%%%%%%%%%%% The definition of “spatial temporal induction” is rather vague and does not clarify what STI effect means, both theoretically and intuitively. %%%%%%%%%%%%% The paper is rather tough to read. Majority of them include details that are not explained in the text (or the caption) and a number of them are hard to interpret. For example, Figure 1.b: what is the red box for?<BRK>The main idea is to employ the unique properties of the network traffic flows, i.e., flowing invariance and variance. However, the paper should be improved in the following aspects. I strongly suggest that the authors write a section for introducing the problem statement. Experiments: 1) The baselines are weak. If possible, more datasets should be considered.
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 6. <BRK>I m willing to increase my score based on the responses, but unfortunately I m still not ready to recommend acceptance. 2.The authors promise some new experiments on more realistic stimuli, but as it stands the paper still only includes experiments on static images with mostly toy data and I have no way of knowing whether any of their results would generalize to more realistic data. We can never know this unless the experiments are done more rigorously. The authors need to make these a lot clearer. (2) The models and datasets used in this paper are extremely toy, there is no reason why more realistic datasets with actual temporal structure could not be used for this research. Please note claiming that these mechanisms are brain inspired is not an explanation.<BRK>This work aims to propose a method to train on this  less processed  form of data. Places for improvement: Despite making the case that data in the real world is temporally smooth, the datasets which were used were artificially generated from a dataset that is not smooth (MNIST). Is there any issue in applying this method to a video segmentation as described in their example in Fig 1A? For the unsupervised learning scenario, the toy data only has variations that exactly match the timescales setup in the architecture of the network. What happens if these are mismatched with the input data? The accuracy should be well above 90% on MNIST. Temporally aligned mini batches does not seem like a realistic assumption to make.<BRK>So I will increase my score to an accept at this point. The authors also further study the representations that emerge from said mechanisms. Overall, the work is quite interesting and insightful. Weaknesses:  With temporally correlated data, once expects that the ability of the network to temporally process the data would also have a significant effect on performance. This could either indicate that the experiment they consider is too simple, or it could point to some more fundamental underlying issues with the combination of auto encoders and sequentially correlated data. This is a major weakness in the paper. * Visualisation of the dynamics of units of the AE could prove to be quite interesting.<BRK>However, in the temporarily smooth approach this can become a bottleneck due to the large size of datasets used to train DNNs. In that respect I would have preferred to have seen experiments with more datasets and more complex neural networks. Doesn t that mean no memory is the best? The authors of this paper first verify this hypothesis and prove this to be true.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>##########################################################################Summary:The paper proposes a parametrized embedding based on optimal transport in the kernel space. Overall, the proposed embedding seems to be novel and well motivated. The empirical results are impressive. I don t have major concerns about the paper.<BRK>(PS: I might be slightly biased because I like OT and have also been thinking about ideas on similar lines connecting attention and OT.) * > It would be great if the authors can answer the questions below:* Empirical analysis: In the experiments for Table 2, 3, 4, do you use the same number of heads for other baselines as the number of references in your case?<BRK>The paper proposes a transport based feature representation for the input of a set of vectors. The number of supports can be different for each one of references. My major concern is on about the settings of the number of references and supports, which would have a large number of possible settings.<BRK>This paper proposes a kernel embedding for a set/sequence of features, based on the optimal transport distance to a set of references, leading to a fixed dimensional embedding of variable length sequences. Strenghts:* Well motivated novel layer for embedding a variable size set or sequence to a *fixed dimensional* embedding space (a point that s not directly apparent though, consider emphasizing this point)* Relatively elegant formulation grounded in OT and kernel methods. Weaknesses:* Experimental results are not very strong. * Given weak results, the impact of this method is not very clear. It is only shown on shallow single layer OTKE, without clear discussion how this could be extended to deeper architectures. The revision of the paper also improves the clarity, but my comment around Sec4 remains.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 7. <BRK>It builds upon a line of recent work that solves linear programs in time proportional to the cost of inverting a single matrix. Therefore, I m concerned whether this paper is suitable for this venue. There are no experimental evaluations of the algorithm, and the theoretical gains are too reliant on the matrix multiplication exponent being < 2.5.<BRK>The paper s contribution is in a new type of sketching used inside the interior point method, that demonstrates some advantages over those of Lee et al and Cohen et al.Notable among these advantages are (1) the ability to preserve sparsity of an LP, and (2) exact solutions to the system of linear equations obtained from optimality conditions. 1.My biggest issue with the paper is that the motivation doesn t seem strong enough to me. 4.I feel the writing also needs to be significantly improved. Leaving aside error of language and grammar, what I think needs a vast improvement is the story telling aspect of the paper.<BRK>The algorithm is relatively simpler to describe. The advantage of the oblivious sketching is not well explained and motivated. Oblivious sketching has clear advantages in streaming data settings, however it s not clear what would be the benefit in an iterative algorithm with an obvious sketch. 2.Maintaining feasibility. Is there a known JL construction that doesn t satisfy the coordinate wise embedding property?<BRK>In particular, the running time of proposed algorithm in this paper matches the running time of the best known algorithms [Cohen et al(19b) and Lee et al(19)]. The problem that is studied in this paper is very important and the sketching based approach is also very practical. The paper would benefit from an empirical comparison of the proposed algorithm with the existing methods for solving LPs. While I am supportive of the paper I can understand the potential objections regarding lack of empirical evaluations. However, as correctly raised by other reviewers, ICLR may not be the right venue for this paper and also it would be beneficial if authors improve the presentation of their result and the motivation of their work further.
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. rating score: 5. <BRK>The paper is a theoretical analysis of two different classes of graph neural network: 1) GNN based on neighborhood aggregations (GNN) and 2) feature augmentation before MLP (GA MLP). showing a gap in expressivness between GNN and GA MLP  showing that the choice of operator in GA MLP is crucial and may have expressive power beyond WL.<BRK>The paper compares graph neural networks (GNNs) with graph augmented multi layer perceptrons (GA MLPs) where GA MLPs are MLPs over nodes with additional node features computed over the graph. The paper is well written and there is a nice balance between theoretical results and empirical ones. Thus, in my opinion, the paper is an original contribution to the field and can be published. I think that the paper could be improved by stating clearly (at the beginning of the paper) that the expressive power of GA MLPs heavily rely on the choice of the operator family. GA MLP mean GA MLPA ?<BRK>The paper presents a theoretical analysis to compare expressive power of Graph Neural Networks (GNNs) w.r.t a class of simpler graph modles called  Graph Augmented MLPs (GA MLPs). I enjoyed reading this paper, and in my opinion it s an important contribution to the field. This paper employs a number of technical tools to formally establish that for some problems GNNs (at least in theory, and conditioned on being able to learn them well) are significantly more expressive than GA MLPs.<BRK>The paper studies a variant of Graph Neural Networks (GNNs) namely, Graph Augmented MLPs (GA MLPs). Unlike in GNNs where nodes send messages to neighbors, and aggregate received messages via non linear MLPs,  GA MLPs rely on a single augmented embedding computed once and then applying an MLP to the new embeddings. On the theoretical side, the paper establishes a somewhat expected performance gap between using a full GNN and an approximation such as a GA MLP.
Reject. rating score: 3. rating score: 3. rating score: 3. rating score: 4. <BRK>The paper proposes a hybrid imitation learning/reinforcement learning method for learning hierarchical policies where the top layer provides sub goals and desired cumulative rewards and the bottom layer learns to meet these goals. The advantage of such a decomposition is interpretability of the learned policy. The authors show that their imitation learning/RL scheme is able to solve both tasks while producing reasonable sub goals. I found the sub goal for interpretability idea very interesting. However the paper lacks in clarity when presenting the algorithm and in depth when evaluating it. Since most of the learning is carried away by HER (which is not even properly referenced in the algorithm box), a background section should be added. The paper should aim to be more self contained. How is the proposed policy immune to this phenomenon? Overall, I like the direction and I think the plots showing the produced subgoals are very encouraging.<BRK>The manuscript proposes (1) HAC General with Teacher (GT), an extension of Hindsight Actor Critic (HAC) that incorporates the environment reward as part of the goal formulation, and (2) "goal based explanations", a framework in which the agent is tasked to produce intermediate goal states. Note that I do not wish to be nitpickinging about this particular naming choice, but "explanations" almost raises the expectation that the end user would see some sort of dense (and possibly language based) description of the policy. So, while section 3.1 does attempt to define then the differences between HAC and GT, it is extremely difficult to understand GT without clarifying how the manuscript intends HAC to look like. What does it mean for a policy to reach it? However, (a) HAC does not require such a training setup, so it s unclear how to fairly assess whether GT is an improvement over it, and (b) it brings the method much closer to imitation learning, which the paper does not review at all. There are in fact methods that distill this sort of knowledge from teachers (e.g.https://arxiv.org/abs/1803.03835, https://arxiv.org/abs/1511.06295 https://arxiv.org/abs/2002.08037), and the proposed method looks extremely similar to them. 7.The experimental setting can be greatly improved. Improve the experimental setup by:  a.<BRK>SummaryThis paper proposes a hierarchical reinforcement learning algorithm in an attempt to improve the explainability of RL agents via the goals proposed by the higher level policy. First of all, the proposed approach is only evaluated on two very simple tasks (Mountain Car and Lunar Lander) and it is only compared with two ablations of the proposed method. There needs to be better  motivation. I also do not think the choice of environments is well motivated. In section 4.1 you make claims regarding the “resistance to small errors” and to “dynamics” / stochasticity which are not supported by any experiments or theory. I suggest either removing them or backing them up with some empirical evidence. After reading the paper, I am not fully convinced that the proposed architecture to generate goal based explanations is very useful for understanding and supervising an agent’s behavior. This seems like an important design choice which is not discussed very much in the paper. RecommendationGiven the above points, I do not think the paper is ready for publication.<BRK>This paper proposes a hierarchical RL method where the high level controller produces a series of sub goals in an open loop fashion which the low level controller attempts to reach sequentially with the aim of maximising task rewards. To my understanding, the method only works if the low level controller has near optimal performance as otherwise setting open loop goals would not be plausible. This limits the usefulness of this method significantly, making it only plausible in deterministic settings and it is unlikely that it would generalise outside of the distribution of data seen during training. In summary, while the motivation seems sound and important, the problem the paper is addressing is not particularly novel. Hence unfortunately, I don’t believe this paper, in its current form, is appropriate for publication. But I encourage the authors to improve their related work section, discuss and compare their method with existing approaches to subgoals discovery and model based HRL to better motivate the novelty and significance of this work.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 6. <BRK>Pros:1.A novel coding scheme is proposed based on Discrete Cosine Transform (DCT) for efficient information expression in place of conventional Poisson distribution method. I am afraid that the descriptions about the reconstruction of input image is wrong. 3.The specific equation of DCT should be with the discussion of the desirable properties and constraints in the section of encoding scheme to provide a clear picture of the method. Could the authors try to provide some explanations for why DCT is able to outperform Poisson method or directly exposing the original image to the input spike neuron, since DCT’s reverse transform is a reconstruction of the original image over time? I would like that the authors could further show the required time steps for reaching strictly equal (or better) accuracy results with other SNN works (especially those directly trained). It is because that the trade off between accuracy and the number of time steps is natural in SNNs. I would like a more comprehensive and fair comparison of results. Wu, Yujie, et al."Direct training for spiking neural networks: Faster, larger, better." Vol.33.2019.Clarity: The paper is fairly well written. Originality: DCT is a widely used transformation technique in signal processing and data compression, but this paper creatively explores it as a coding scheme in SNNs.<BRK>This paper proposes an encoding method based on the Discrete Cosine Transform (DCT) for Spiking Neural Network (SNN). Compared to the Poisson coding method used in most SNN studies, the proposed encoding method significantly decreases the latency that the SNN needs for image classification while having minimal accuracy decease. However, it needs to be explained in the text or figure caption. 2.In Table 2, it s not clear whether DNN d uses DCT coefficients or the reverse transformed image. One of the problems that prevent the SNN from using fewer inference timesteps is the ineffectiveness of encoding input information. Using DCT, the method can potentially filter out less important information and more effectively encode the information in limited timesteps (as shown in Fig.6 and Fig.8 in the paper). Is there any reason for that? This is desirable because low frequency information is more important than high frequency information in the image for classification. The paper lacks experiments to show that DCT directly contributes to the decrease of timesteps for classification. Although comparisons with earlier SNN works that use Poisson encoding are shown, there is a lack of comparison with any SNN methods that directly convert pixel values into spikes using IF neurons and threshold selection. The reviewer suggests conducting additional experiments for this. The paper lacks experiments to compare the performance with these more recent results. However, this is not a fair comparison since their encodings are different. If the paper wants to compare with these ANNs, it needs experiments using the same encoding input. However, the paper doesn t explore other possible approaches for spreading the information.<BRK>The reported experimental results show some novelty and good performance. In the early stage of the spiking neural networks, the encoding methods are very important, especially for the training process. As the authors said that the rated based encoding method brings much more time latency which is time consuming. Therefore, the topic of this work is critical. However I have some worries about the proposed methods, from my point of view, this work is just combing the DCT and ANN SNN method, the novelty is significantly limited, but the idea is interesting. Then, the experimental results reported by this paper were not well designed. The authors argued that the rated based encoding methods are time consuming, but they just did not compare the much more temporal encoding methods in Table 3. I even did not know what parts of the final results works, the deep CNN based network architecture? The computational efficiency is nice; especially the authors calculated the spike rate of each single layer, but if you just argued the proposed the method is energy consumption, you should at least consider the ANN training process, it is not a single trade off between inference accuracy and latency. I have run the code from authors provided, the reproducibility is reliable. Also there are some writing errors, such as thy >they,  s, etc.<BRK>The increasing spatial frequency components are known to be perceptually less sensitive (they need to include this) in images, so the low freq components can be presented first. I think the solution proposed is well founded and will indeed mitigate the latency problem for spiking neural networks. However, this feels a bit more like an engineering solution to a specific problem rather than a new concept. I see that as the main value of the paper in addition to helping to make spiking neural networks a POSSIBLE viable solution to edge deployment. This does weaken the motivation for the paper though. This is used in MPEG/AVC. It is a reversible transform that is an integer simplification of the DCT. Given that the point of the paper is to decrease latency and computing requirements for edge deployments, this could help. Section 3.2: The authors do a good job of sweeping performance for different block sizes. Figure 5:  Isn’t it an obvious result that more time steps are required for Poisson vs DCT?
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. rating score: 6. <BRK>The paper introduces a new training criterion based on Information Bottleneck theory, which increases the diversity in an ensemble by minimizing the mutual information between latents of the different ensemble models. ### StrengthsThe paper is written very well.<BRK>Summary:This paper proposes a method of learning ensembles that adhere to an "ensemble version" of the information bottleneck principle. It is well written and thoroughly cites and explains prior work   in several cases, I find their explanations of prior work more clear than the original papers.<BRK>This paper  addresses the problem of training an ensemble of learning algorithms so to boost the diversity among single learners while preserving the accuracy of each learner. The paper is well written, the addressed problem is well exposed and relevant for this audience, the literature review is more than adequate.<BRK>## SummaryThis paper introduces a training procedure for ensembles of neural networks that improves intra member diversity to achieve better accuracy and calibration. ## OriginalityThis paper augments the VIB training objective of (Alemi et al., 2017) with two modifications:   adding a mutual information penalty between encodings of the same input by different ensemble members  conditioning the mutual information between encodings on the _input label_The use of mutual information in training neural networks is not novel; indeed, as the authors point out, using mutual information between input and encoding; encoding and output during training is common.
Reject. rating score: 2. rating score: 4. rating score: 6. rating score: 7. rating score: 7. <BRK>They are both from Section 2 after the paragraph heading **Definition of constrained black box uncertainty modeling**. Due to the risk nature of the internal problem where it is applied, uncertainty modeling is important." Here, $q$ is given by specifying the quantiles, and is approximated by a Chebyshev polynomial of degree $d$. However, this has not been the case.<BRK>This paper considers the problem of uncertainty estimation and it proposes to compute quantiles of a black box predictor using Chebyshev interpolated quantile regression, where the interpolation parameters are computed with the aid of a neural network. Can the authors please clarify their use of the term “aleatoric uncertainty”? However, the paper refers to this as aleatoric. The proposed approach has questionable utility, because estimating the uncertainty of a prediction is arguably a job for Bayesian methods.<BRK>This paper presents an approach to model aleatoric uncertainty for the black box model, through the combination of Chebyshev polynomial approximation and quantile loss. I would suggest the authors add more discussion on the motivation of the problem settings, e.g., whether it arises from an actual business problem, why satisfying constrained black box uncertainty problem is a must have, etc.<BRK> # UpdateAfter reading the updated version of the paper, I think my main issues with presentation have been addressed. This is done by having a NN predict the coefficients of a Chebyshev polynomial which is then integrated to evaluate the predicted cumulative density function over targets. * The introduction of post hoc noise modelling for black box decision making systems as a task. #### Cons* The paper is not very well written and hard to follow. In the limited data regime, this is often a crude approximation to the inherent uncertainty in the generative process of the data.<BRK>This paper proposes a novel approach to modeling uncertainty, as an layer added on to an otherwise black box system. Importantly, the Chebyshev polynomial formulation gives the practitioner some flexibility around deciding which statistic of the conditional CDF should be matched by the black box system. The experiments use datasets comparable to prior/related work. https://arxiv.org/abs/1906.04032 is perhaps relevant there.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 7. rating score: 5. <BRK>* Periodic updating of the policy used to compute importance weights: why is this necessary? This paper proposes a new reinforcement learning approach to training generative neural models of text. with the current value, how many of the importance weights are actually used?<BRK>This paper formalizes training of conditional text generation models as an off policy RL objective, specifically, in the limit case where samples are only obtained from the training data. While for certain tasks such as MT or summarization it is often sufficient to focus training to generate 1 single correct Translation or summary (see cons: for comments on the effect of this on sample diversity).<BRK>This paper proposes a method to train generative models of text using reinforcement learning from off policy demonstrations. Experiments are well executed and provide interesting insights. Writing is mostly clear.<BRK>The authors also discuss why off policy learning is more suitable for text generation than on policy learning. Also, examples are not provided when the proposed method fails to generate good output when other methods can.<BRK>Again, these are crucial to performance. The significance of the results is not clear. Strengths:  The paper is well written, and the approach is straightforward and clearly explained (caveats below). After reading the reviews and the authors responses, I have several remaining concerns.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 7. <BRK>When the natural policy gradient is used as the  policy optimizer,  a sublinear global convergence rate is proved for both the tabular setting and the function approximation case. I am not convinced how the proposed method achieved global optimality. Even the high level proof idea is unclear to me. 2.I am also confused by the assumptions. Are these assumption true for the two examples considered in this paper? 3.The numerical examples need some improvements. It will be more convincing if the authors can demonstrate their methods on other standard benchmarks for constrained policy optimization (e.g.Gather, Circle, Half Cheetah Safe, etc).<BRK>Thus I think the pure primal algorithm is an interesting direction of constrained RL. I did not check the details of the proof, but the  roadmap of the proof is clear by combining existing results on RL and deep learning. My main concern comes from the novelty on the theoretical result, particularly the proof technique. The result on the policy evaluation step mainly comes from Dalal et al, while the policy optimization step stems from Agarwal et al.When combined with function approximation setting, it adopts the analysis of the neural networks such as the result in  Du et al.The whole framework is similar to the Lan & Zhou. Can you emphasize the main theoretical contribution of this paper? Although this work avoids the primal dual formulation or projection step, my feeling is that it still needs careful hyperparameter tuning.<BRK>The authors also supplemented this work with several numerical experiments. The reviewer has a few comments / suggestions regarding the NN function approximator setting as follows:  Convergence results of Theorem 2While the convergence is well analyzed for the tabular setting in Theorem 1, the reviewer is concerned about the result in Theorem 2 for the case analyzed with function approximator. Or is there any numerical evidence which shows that such dependence on $m$ is unavoidable? In the case of using NN as function approximator, Lemma 1 for neural TD (Cai et al., 2019) holds with high probability w.r.t.the NN initialization, and the analysis in this paper suggests that the NN needs to be reinitialized every time when the policy is changed. Is it the same way that the algorithm is initialized? The authors may shed more lights on the similarity between the proposed algorithm and (Lan and Zhou, 2016). Post Rebuttal  The reviewer is satisfied by the authors  response.<BRK>The paper establishes global convergence in the tabular and NTK approximation cases. While I seem to can not find the corresponding parts and would like to ask about the intuition behind that. The assumption on the one hidden layer neural network is standard, as it is used in a series of recent literature, although it is strong compared with practical algorithms.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 7. <BRK>This paper addresses the problem of code similarity detection and proposes an approach called MISIM. Essentially, this paper proposes a tree representation, called context aware semantic structure (CASS), which is very similar to the abstract syntax tree (AST). Then, different neural architectures (e.g., GNN or GRU) process the CASS and encode the program as a vector for similarity modeling. While the title of this paper claims "a novel code similarity system," I do not feel the model novel at all, as it just follows a standard Siamese structure. To be convinced, I expect 1) The authors show the results of controlled experiments with different structures. 2) The authors show the generality of the CASS in different tasks, such as clone detection.<BRK>* CASS seems to be closely related to an AST. The paper combines two ideas: (1) a new “context aware” representation for code which is used as a basis for computing code embeddings, (2) code similarity based on cosine similarity between the embeddings. ### Weaknesses ###* Looks like an extension of AROMA, with a slightly more general approach for feature engineering, but no significant innovation beyond that. I am sure that CASS+GAT would do better, but I conjecture that their contribution would be marginal in many mainstream languages.<BRK>The experiment results show that MISIM can outperform code representing models like NCC and code2vec to detect code similarity. The configurable code structure representation gives the model the flexibility to handle multiple tasks. It is difficult for other researchers to apply CASS to different scenarios until a detailed configuration explanation is provided. This paper compares 3 different baselines: code2vec, NCC, and aroma, in which code2vec and NCC are designed for general code representation. Please provide some concrete examples that modeling those "syntax" in AST actually HURTS the performance, i.e., state of the art methods learning from ASTs (e.g., the methods listed below) performs worse than the proposed method.<BRK>The main contribution of this paper is the CASS structual feature for source code and its high performance. Three kinds of deep learning models (GNN, RNN and BoF) follow the CASS representation to demonstrate its performance on two code similarity experiments. The scoring part uses a Siamese network to evaluate the different kinds of distance which is reasonable for solving this task. It s more like syntax to me in its current form.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>The paper presents a graph neural network formulation specific for directed acyclic graphs. In this formulation, the aggregation function also considers the information about the current layer. The model considers nodes following a topological batching in order to have the information of the predecessors when calculating aggregation of a node. The system has been compared with several competitors on three datasets. The results are good in terms of metrics and seem promising also in terms of training time. The proposal is clearly defined, but I would be happy if the paper also contains a very simple example considering a small DAG and showing some steps of the computation. Graph Neural Networks have come to the fore in recent years as a promising approach to solve many tasks and this work opens a good line of research. One possible cons is that it focuses only on directed graphs. It is important to continue to test whether this choice will lead to a system that can achieve significantly better results than other systems that are more general and do not pose this limitation. To test this aspect, three datasets are not sufficient, but the results look promising. I have only minor issues to highlight in the paper. In formula (1), L is not defined, its definition is given later. In figure 1 there are strange lines that should be removed.<BRK>This work considers DAGNN for learning representations for DAGs. Compared with message passing neural network, particularly D VAE, there are three subtle and notable differences motivated by the properties of DAG: (i) attention for node aggregation, (ii) multiple layers for expressivity, and (iii) topological batching for efficient training. Experiments show an improved performance, and ablation studies validate the proposed modifications. I recommend a weak acceptance based on the present version and am happy to improve my rating if authors address my concerns. Can the authors explain more here to validate it is indeed an advantage? Indeed, I did not see why  Corollary 3  is a corollary. Can author give more details here, when graphs have different lengths? 4.Authors proposed a parallel strategy for an efficient training. However, compared with D VAE which is a sequential execution, the training time seems to be very close. 5. about the BN experiment: compared with RMSE, BIC is a more reasonable metric to evaluate the final performance of the obtained DAG, as a better RMSE may indicate spurious edges wrt. the true graph. Can you also give more details about this reasoning and if possible, can you try other BN problems to validate this reasoning? 7. the overall writing is OK, but may be improved in several parts. For example, 1) the introduction part has many comparisons with other approaches when stating contributions; separating them into two parts may be more clear. *** after reading rebuttal *** All my concerns are addressed, and I decide to improve my evaluation.<BRK>DAGNN can be regarded as a special case of previous GNN models, but specific to directed acyclic graph structures. Reasons for score: Given the ubiquity of directed acyclic graphs, DAG based graph neural networks have potential impacts on various fields. The authors propose an elegant and effective deep learning framework to learn node and graph representations on DAGs. Pros: 1) The problem is interesting. 2) The paper is well written. The authors introduce the framework based on the existing message passing neural network, which makes it easy to follow. The authors also present how this work handles special characteristics of DAGs. The techniques of the model are clearly stated. In particular, the comparison with highly relevant previous work is well explained. 3) The authors provide sufficient experimental results, including comparative studies on four datasets with the state of the art benchmarks and ablation tests, which show the effectiveness of the proposed framework. 4) This paper provides a theoretical analysis of properties of the proposed model, which demonstrates that graph representations extracted by the proposed model are  discriminative. Cons:1) The framework needs more explanation. The input and past states are defined by the node representation at the last layer and the aggregated information from its predecessor nodes (i.e., message). As the authors introduced, these two arguments are switched in existing work. It would be better to provide more details about the design. 2) Although the paper provides several ablation studies, I still suggest the authors to consider the following ablation studies to enhance the quality of the paper: 	  What is the performance of the proposed model by changing the type of attention mechanism? Recurrent models are usually relatively slow in the training process and sometimes unnecessary. Such knowledge of message passing neural networks may not apply to the DAG based framework. It would be better to provide some sensitivity studies or theoretical proof.
Reject. rating score: 6. rating score: 6. rating score: 7. rating score: 7. <BRK>The approach is motivated by CEVAE which uses a VAE to learn a single latent representation of confounding between the treatment, target and covariates, but attempts to disentangle the confounding between the covariates and the treatment, confounding between the covariates, treatment and target, confounding between the covariates and the target and covariates only information. They also include an MMD term in the loss to encourage independence between one of the latent representations and the target. The paper is generally well written. Given that this is more of an architecture paper, the evaluation of the specific architecture choices could be more thorough. The attempt to account for the contributions of the different latent factors is appreciated, but the results are not strongly convincing.<BRK>The authors provide some novel VAE architectures for causal inference. Is there a tradeoff between performance and complexity? Does it depend on the task? Especially the ablation studies (Fig.6) seem to suggest that most of the loss terms are not actually needed for the proposed model (H VAE CI) to perform well. For this, it could also be interesting to construct even "deeper" models than the (H VAE CI), which would be less informed by prior assumptions, and see if they might not work even better. (2019, May). Challenging common assumptions in the unsupervised learning of disentangled representations.<BRK>Summary:This paper introduces a new VAE architecture for performingcausal inference. Technical Quality:The method and experiments are well thought out and very clearlyevaluate the work. What s not clear to me is how much the extra performance is from justhaving a larger model and more parameters that can be fit. It s also unclear how much the other architectures weredesigned to estimate the heterogeneous effect. "follows a(n unknown)"  > "follows an unknown"* Some of the figures don t look like they are vector graphics* The code doesn t seem intended to be executed.<BRK>Summary: Some generative models have been proposed for causal effect estimation but they often do not have a competitive performance. Recent work suggested that a combination of generative and discriminative model may improve treatment estimation with observational data, and further suggests a generic latent variable model for factorizing selection bias, as well as outcome. The author(s) build on this work and propose a set of deep generative models, with a hybrid objective function (generative + discriminative), that outperforms current approaches for ATE. The proposed objective function is motivated from previous literature. Still, the experimental section makes a strong point that the method is effective and outperform previous approaches. What are the results of the hyperparameter search for beta? 2.Would it be more advantageous to use other disentanglement constraints instead of the beta VAE?
Reject. rating score: 4. rating score: 5. rating score: 6. <BRK>This paper proposes a new framework for unsupervised domain adaptation by applying the disentangled representations learning (DiCyR). Pros:1.This paper is well organized with clear logic to follow. The authors have introduced the problem statement and related works very well. Moreover, there are a lot of previous papers working on similar topics [1, 2, 3, 4]. Moreover, there are more recent works, such as [5] should be compared.<BRK>This paper studies the domain adaptation problem by addressing the challenge of splitting task specific and task orthogonal information in the target domain using the proposed disentangled cyclic reconstruction method. The authors further develop a variant for the unsupervised domain adaption (UDA) task. The authors argue that the existing adversarial classifier based UDA solutions do not guarantee that the domain specific information does not contain any information that overlaps with the shared information in the target domain. This paper is well structured and easy to follow. [ref 2] X . However, unfortunately, I still think more needs to be done to show the superiority of the results.<BRK>#####################Summary:The paper proposes a disentanglement method, named DiCyR in the problem of unsupervised domain adaptation. However, the proposed method cannot consider the domain specific and domain orthogonal information, which may degrade the performance in experiments and deserves to be discussed further. Please explain more details. #####################Reason for score:Overall, the paper is above the borderline.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK>Furthermore, a model size based regularization is introduced in the selection stage. Strengths1) The paper is well written and easy to follow. 2) Some empirical evidences are provided to explain the limitations of the existing RandomNAS. Comparison with Full batches validation (the whole validation set) should be considered. RandomNAS (RSPS) achieves better perplexity with less search cost. The novelty and the similarity with previous works are my main concerns. I am currently leaning towards a negative score but would like to see the authors  responses and other reviewer s comments.<BRK>This paper claims that random search based NAS methods show a low ranking correlation among top 20% candidate architectures in the search phase. Pros  This paper analyzes the behavior in the random search based NAS in detail and proposes a new strategy based on the observation to tackle the issue.<BRK>This paper proposes Evolving the Proxy Search Space (EPS) as a new RandomNAS based approach. EPS runs in three stages iteratively: Training the supernet by randomly sampling from a PS; Validating the architectures among the PS on a subset of the validation dataset in the training interval; Evolving the PS by a tournament selection evolutionary algorithm with the aging mechanism. The paper is well written and easy to follow. However, it is kind of incremental work since the EPS (Algorithm 1) is exactly similar to the aging evolution in Real et al.(2018, 2019) and this paper is using this search strategy to gradually build the search space on the fly. While a random sample from GS is simply random search, proposed EPS is exactly the evolutionary strategy to build search space.<BRK>As a result a new sampling method based on tournament selection for NAS is proposed. +Clear experiments are devised to demonstrate the insights. It would be interesting to show performance on larger dataset. I am also interested to know the range of performance between best and worst for top 100/60/20% of models to better understand how poorly correlated top architectures in GS actually translate to loss in performance beyond performance of the final architecture.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 7. <BRK>Summary: This paper is concerned with the approximation of conditional densities in trained normalizing flow models. The paper also contains a theoretical result showing that exact conditioning is NP hard in additive normalizing flow models. Pros:  Relevant problem. In particular, it would be useful to have baselines without likelihood smoothing and also comparison with (simulation based) forward KL methods such as [1,3]. The main idea behind the method is to perform inference in the latent space, this in my opinion is not a noteworthy contribution as it is just the obvious way to do inference in this setting. Major concerns:I could be missing something but I do not understand why the authors are smoothing the likelihood since the flow already give a perfectly well behaved joint model. The authors also seem to motivate this choice using their theorem showing that exact conditioning is NP hard. However this is only a valid motivation for using approximate inference, not for adopting an approximate likelihood. However, the treatment of the theorem in section 3 should be expanded. The experiment section is very well structured and the analysis of the results is very good and definitely above average. I am pleased that the authors took the time to analyze the bad performance of the native VI method. However, the author should also discuss methods based on synthetic sampling and forward KL divergence as they are a very viable approach to generative model conditioning in the latent space.<BRK>Summary: The paper proposes to solve the conditional inference problem by performing a relaxed version of variational inference in the prior space of the flow based model. I am still leaning towards rejection at the end given the limited originality of the proposed method and the lack of a more comprehensive discussion of different possible approaches, but as means to the same end. For example, the relaxed inference problem can be solved with an MCMC method. These should all be discussed and compared if the contribution is about repurposing a joint likelihood model using flows. The key novelties (A and B) are incremental in nature given the above related works that are not cited. In the qualitative results (tab 2, fig 3 and fig 5), the generated samples do not really satisfy the hard constraints that they are conditioned on (e.g.the subsets of x for inpainting), possibly due to the relaxation via the dummy variable. This is not desirable. The authors claim the hardness result is surprising. It has been long shown that sampling from a general Bayesian Belief Network is NP hard [6]. If this is the case it is consistent with the presentation of the previous section (VI). However the proposed method seems to require a more general treatment to take account of the other tasks, e.g.inverse problems. The discussion of hardness seems to be used to motivate the relaxation of the hard constraints (of the givens).<BRK>If the flow variable is $x$, then querries are either conditioned on the part of $x (x_1,x_2)$, or on a differentiable transformation of $x$. However, after reading the discussion with other reviewers and their reviews, I believe the manuscript can benefit from another review round. The paper then proposes a framework that affords such querries by working in the latent space empirical evidence favors the proposed approach over contemporary methods. ### Strength:The problem is well motivated. Specifically, the authors can benefit from a thorough revision of the claims in the paper. Further, I appreciate the use of proof that the problem is hard to motivate the solution. I also commend the author s effort to collect empirical evidence for their method. ### Concerns:One primary with paper is the lack of clarity and overload of notation. This is especially true for section 3. However, it is not immediately clear from the discussion. The authors offer no discussion on this. I also believe if the above explanation is true, then it is bit of a leap. However, these concerns vanish for the section with differentiable transformations here, we do not talk about the partitioning of x, so the expressions are straightforward to evaluate. It will be great to get an algorithm like the one in Appendix C for the partitioning case. It is the fundamental idea of VI to use ELBO over KL divergence; however, the current presentation makes it feel that this is a novel observation made by the authors. I will suggest being unambiguous with terminology. Section 1, fourth paragraph: "Specifically, we use variational inference tolearn a distribution in the latent space ..." I find this sentence hard to parse.<BRK>In this work, the authors propose a novel method of estimating conditional distributions over arbitrary partitions of variables $x   [x_1,x_2]$ using an existing pre trained flow model for $p(x)$. The authors present comprehensive experimental results which show a clear improvement over existing methods for conditional inference but with the drawback of needing to re train the pre generator for each individual observation. Pros:  Very well written, clear, easy to understand  The proposed method is intuitive and well defined  Placement of the method relative to recent work is very clearly explained  Comprehensive theoretical analysis including an interesting hardness result, which is uncommon for the deep learning literature  Comprehensive and convincing empirical analysis with clear resultsCons:  There is very little discussion on how the construction of the base generator $f$ affects the results of the proposed method  The proof of hardness is somewhat opaque and feels contrived; but this is often the case with hardness proofs! The method has a clear weakness in needing to be retrained for each observation. However, this is clearly stated by the authors and left open as a direction for future work. Overall, I think this is an exceptional paper which makes a significant contribution to the field. I think it is suitable to accept as is with only a few minor adjustments which I will enumerate below. Intuitively, I would think so. 2.A few notes on the proofs:  The variable lower case $m$ shows up in several spots in the hardness proof but is never defined. Perhaps these are typos and you meant to write $M$? In the proof for equation 3, the notation for expectations (i.e.$\mathbb{E}$) is inconsistent in a few places. Presumably just typos. I may be missing something, but it s not immediately clear why $y T(x)$ can be substituted for the conditioner $x$ in $p_{\sigma}(\tilde{y} y^*|x)$. Please correct me if I am wrong, and preferably add a clarification to the proof as to why this is justified. Congratulations to the authors on a job well done!
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 7. <BRK>This draft proposes to use the relu activation pattern of the neurons in the neural network as the hash code for the input. Essentially the input features are bucketized into small piecewise linear regions. The authors show empirically that the proposed hash code has small collision and high accuracy given certain conditions including1. The features are around the sample manifold 2. the training time is long enough3. the network is wide enough4. the training sample size is large enoughThe authors also found empirically the effect of regularization is relatively small on the encoding properties. (of course the accuracy will be bad)2. The authors used K means as another benchmark to compare. To me k means is an unsupervised clustering algorithm. MNIST is too easy to show any conclusive results. Why can a larger sample size also help reduce the redundancy ratio? Overall I think this draft has some really good ideas but the empirical result is not quite conclusive due to the lack of extensive experimentation.<BRK>The responses addressed most of my concerns. Given these, I m happy to increase my score to a weak accept (a weak accept, because I m still not quite sure about the significance of the results reported in this paper). Unfortunately, I have several major issues with the paper. The motivation is initially couched in terms of hash codes, so one gets the impression that the authors are going to propose a new hash coding scheme using neural networks. This is also why the authors are stuck with the toy MNIST dataset throughout the paper, because the proposed scheme is completely impractical for any reasonably large dataset and model. (2) This begs the question: what exactly is the significance of the observation that linear regions satisfy some properties of hash codes, if they’re not going to be used as hash codes? (7) Most importantly, one of the main phenomena observed in this paper (the change of redundancy ratio over training) has already been reported in Hanin and Rolnick (ICML, 2019): they note that the number of linear regions in a deep ReLU net first decreases and then increases during training (please read their section 3 carefully).<BRK>In this paper, the problem of linear partition in the linear spaces of neural networks with ReLU like activations is studied. It demonstrates that such a partition exhibits two properties, including determinism and categorization, across a variety of deep learning models ————————————————————————————————————————————This paper presents an interesting work, however, there are a few issues/comments with the work:1.This paper mentions that “simple classification and clustering algorithms, such as K NN, logistic regression, and K means can achieve fairly good training accuracy and test accuracy on the neural code space”,  About “achieve fairly good training accuracy and test accuracy”, maybe some comparisons with existing methods will strength this statement. 2.The paper concludes that model capacity, training time, and sample size play important roles in shaping the encoding properties, while several popular regularizers have little impact on the encoding properties. As for the conclusion of the regularization technique, it is better to try other methods, such as drop out, to verify gradient clipping and weight decay not only on MLP but also on RNN, before reaching such a conclusion. It will be better to try out some existing/well known NN models on more datasets. 3.I was confused about the sentence below Section 5.4 “we trained 345 MLPs on the MNIST dataset” and the last sentence in the caption of Figure6 “Totally, 115 models are involved in one scatter. If so, it would be more interesting and useful.<BRK>**Strengths** * Observed hashing effect and hypothesis stated fits quite well with the wider literature (which is also well cited in the paper). Ideally show some examples of the random data in the appendix. **Summary**The paper investigates activation patterns in ReLU networks, which are known to be piecewise linear. If not what are the consequences (can you still easily make the claims on P1: “This linear region partition can be extended to the neural networks containing smooth activations”)? Additionally, at least some core parts of the analysis does not require training networks (but could even be performed e.g.with pre trained classifiers on ImageNet)   there is thus no severe computational bottleneck, which is often the case when going beyond MNIST. The main finding is that trained MNIST classifiers produce unique activation patterns for most points from the data distribution (but not for random data). Importantly, simple clustering/classification algorithms (K means, K nearest neighbors, and logistic regression) on these hashed patterns lead to good classification accuracy, implying that the hashed patterns follow some underlying geometric regularity (which is not trivially expected from an arbitrary hashing function). Please use the term ‘model size’ instead of ‘model capacity’ throughout the text. **Main contributions, Novelty, Impact**1) Systematic study of the “hashing” behavior of MNIST classifiers. The question is whether this really happens at the very last layer or already earlier in the network. how the chart is meant to be used (hyper parameter tuning, or rather as a diagnostic tool, …?). It is thus currently unclear whether such a chart could have wider impact in the community.
Accept (Poster). rating score: 8. rating score: 7. rating score: 5. rating score: 4. <BRK>The author further proposes using stacked rank 1 projection of the free algebra as an approximation to the sequence representation in order to make it computationally feasible neural networks layers. Clarity: While the paper is highly technical, the author did a good job explaining the idea, concepts and objectives. Originality: I am not aware of any other work explore free algebra and its usage on sequential data representation.<BRK>There is no intuitive explanation to show how the free algebra improve the models  performance. Detailed proofs and theoretical results are given in the appendices. The use of free algebras in ML seems novel and under explored, although it is classical in mathematics. Pros:+This paper shows that the free algebra T(V) can capture the non commutativity among sequential data theoretically and experimentally, which is under explored in ML area.<BRK>The idea is worth of  interest and the appendix gives large amount of information on both theoretical and experimental sides of the work. Moreover, the experiments are conduced with both small models and datasets. The experiment for sequential data imputation for time series and videos is not clear, and the way of writing and giving explanation is quite confusing for the reader. Since the appendix is large, more details from the appendix in the paper itself will help the reader to more easily follow the idea.<BRK>This paper proposes an interesting low rank tensor representation learning model called Seq2Tens for sequential data. On one hand, there are some solid contributions made in this paper. On the other hand, I do find it is difficult for the readers to appreciate its merits. In fact, there is only one citation (on page 3) on the universality in the first three pages of this paper. In the equation between (1) and (2) (defining Seq2Tens), the right hand side is a capital V without bold, which I assume is a tensor.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 7. <BRK>Summary:This paper presents a method on the task of Scene Graph Grounding, where the object and (object, object) relationship ground truth labels are only at the image level but not localized, i.e., instance level. The motivation is not clearly justified in the Introduction. For example, for readers not familiar with this task, the authors may want to provide a clear illustrative example of this task, to show why scene graph grounding is challenging and significant. 2.I am not objecting to use the simple EMD to solve weakly supervised matching problems, as it is straightforward. 3.The overall pipeline is not clearly illustrated.<BRK>They should also show the images with objects, not in the scene graph. The setting of this paper is new and interesting, and I think this task would be beneficial to downstream tasks. **Pros**:  This paper proposes the weakly supervised scene graph grounding task and formulates the mapping between scene graph and objects as a minimum match problem on a bipartite graph instead of graph alignment. As for visualization, the authors only show the scene graphs with the same objects.<BRK>Although this paper is the first work to explore weakly supervised scene graph grounding task, the task is very similar to the existing weakly supervised phrase grounding task. Given the image and a paired scene graph, this task needs to link each scene graph node to an object in the image. #################################Weaknesses:  The novelty of the idea for weakly supervised scene graph grounding is limited.<BRK>This paper introduces a new problem called weakly supervised scene graph grounding, which is potentially very useful but barely studied in the literature. The empirical findings of the paper provide new insights that do not exist in the literature, and can directly guide future work. However, I recommend accepting this paper as it introduces a new task and presents strong results. The authors clearly define the problem and propose an evaluation setting for it.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 4. <BRK>These tasks are automatically generated in a way that helps an RL agent to learn hard exploration problems. I find the paper interesting, the results convincing and don t have too much to critique. My main critique would be to add what the limits of the current approach are. Minor comments:  Based on how many independent runs are the results based on that are shown in Figure 3, 4, 5?<BRK>(Minor concern) POET is an important baseline but the results are not included. I m willing to change my score if some concerns are addressed. Pros+ The paper is well written and easy to follow. + This paper tackles a hard problem that incorporates exploration, policy learning, and curriculum task generation. 5.I understand the author may use some empirical way (an optimization problem with regularization) as they showed in their appendix, rather than primal/dual optimization.<BRK>This paper tackles the problem of facilitating RL agents  learning in sparse reward, hard exploration problems. [+] The experiments are well designed with illustrative figures and analyses. [ ] Although the general idea of generating easier tasks to facilitate agents  learning in complex tasks is intuitive, how the authors  justify their method has the same property is not fully justified in my mind. Then in this case, for example in a simpler scenario,  will this be a problem that affects RL training? The authors addressed my concerns in the discussion period and I therefore created my score to 6.<BRK>The paper is well structure and clearly written. I think the paper will be more convincing with precise examples and the exact numbers of the dimensions. This aspect is too central for the paper that it cannot be relegated to the appendix. Similarly, the justification of the baseline selection is not sufficient in the paper to understand the motivations of the authors and why these algorithms are used in this study, and also why some approaches are not included. For instance, the POET seems to follow a similar approach, yet working on continuous parameter spaces.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 7. <BRK>This paper presents a method for reconstructing trajectories of protein structure from MD simulation. The main technical contribution is a geometric autoencoder architecture with separate latent spaces for representing intrinsic and extrinsic geometry.<BRK>The application is a very interesting topic in general, and in this sense the work would be important, while the organization of this paper is not good enough, particularly the method section, which should be written in a more organized manner. This is because "dynamics" means time series, by which I thought the latent embeddings are also time series (like exactly simulation), while the reality is slightly different. 2.Again the method section particularly encoding should be more clearly described. 3.Datasets are limited to only one, two types of proteins. I think the authors explain what "intrinsic and extrinsic properties" mean somewhere earlier in brief.<BRK>The authors address a problem of learning protein dynamics directly from the protein structures. They propose an autoencoder type approach with disentanglement intrinsic and extrinsic features. However, there are several concerns with this approach:1. Could the authors please elaborate more on the methodological differences between these two approaches? 3.The idea of intrinsic and extrinsic components is nice, however from the experiments the contribution of the intrinsic component to the reconstruction is not very clear.<BRK>**Summary**The paper introduces a geometric variational autoencoder for capturing protein structural ensembles, disentangling intrinsic and extrinsic geometry into separate latent spaces. Perhaps it is merely a question of defining what the "trace" of a protein is. The paper is thus interesting both from a methodological and an application perspective. These two sentences are central to the paper, but not entirely clear.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. rating score: 6. <BRK>The paper addresses hierarchical classification, where the classes live in a hierarchy, and the cost of a mistake is the tree distance between the nodes. By comparing hierarchical loss to top 1 loss with modified loss functions, there is a tradeoff, and there does not seem to be an advantage in using the modified loss function. These support the use of CRM. So I would argue for acceptance. One thing for the authors to think about:When they test the calibration of the modified loss functions, they find them to be poorly calibrated. that is, a gain and offset for all classes after the first one?<BRK>This paper proposes to use conditional risk minimization (CRM) for hierarchy aware classification. The proposed method simply amends mistakes using a cost matrix with the lowest common ancestor information. The method outperforms SOTA deep hierarchy aware classifiers by large margins at ranking classes with little loss in classification accuracy. In this case, it is possible that more than one label can have p(y|x) > 0.5. I wonder whether your approach is still effective for models using binary cross entropy. It would also be better if the authors can show some experimental results on hierarchical text classification.<BRK>First, they claim that the metric used in prior work, average mistake severity, is flawed because it rewards methods that make many "easy" mistakes, as opposed to fewer, "harder" mistakes. I agree that that indicates a problem with the metric. c)As in 1), soft labels is essentially on top of CRM and Cross entropy (for iNaturalist19, it looks like a higher beta value would be directly on top, it s unclear why the authors did not extend the curve further)2)These results, at first blush, seem fairly impressive. The remaining experiments are fairly convincing, though. While I suspect that CRM is a good method that I would like to use, some of the core arguments (Figure 2/Section 4.1) in the paper appear to be fatally flawed.<BRK>Their model is a post hoc procedure and is based on the tree structure of WordNet. The model revises the classifier output based on the distance of the labels in the tree. They also experimentally show that the previous evaluation metrics are inconclusive. Pros:  The authors provide a different perspective on the evaluation procedure of the previous studies and experimentally show that it was incomplete. Their experiments are thorough.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 8. <BRK>To do so, the linear transformation is turned into a new linear transformation based on the Kronecker product. Such an analysis is crucial to validate the fact that the work proposed here is a generalisation of what have been done before to N dimensions for neural networks. Remarks and questions:1. While the theoretical aspect of the degeneration of the proposed approach to the Hamilton product "sounds" plausible, I would like to see a more empirical demonstration.<BRK>The authors focus on the area of using hypercomplex multiplications (multiplications involving numbers with multiple imaginary components) in deep learning models. The novel contribution of this work is to parameterize the hypercomplex multiplication operations, enabling the model to discover new operations rather than relying on the small number of existing operations and the small number of dimensions for which such operations exist. Thus, the evidence presented does not make a strong case for the necessity of this parameterization.<BRK>The authors propose a novel way of parametrizing hypercomplex multiplications. The proposed parametrization helps with: (a) Generalizing the multiplication to arbitrary dimensions, and (b) Reducing the number of parameters.
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 6. <BRK>In particular, the main goal of the proposed technique is to find linear projections of the genes expression which vary maximally with one phenotypical aspect and minimally with the others. The phenotypical features are assumed to be categorical. *Positive points:* > The paper starts from a real and challenging bioinformatics problem, i.e.the analysis of single cell RNA sequencing data for neurobiology; data analysis tools for these data are nowadays fundamental to unravel the high complexity of this bio medical field. The main problem of this manuscript is that the proposed method is not well inserted into the state of the art. This comment is related to the previous one, but contains more an open suggestion rather than a comment. If I correctly understand, in the experiments authors apply a PCA to reduce the gene expressions before applying the proposed method (last two lines of page 5). Is this a reasonable choice?<BRK>This manuscript describes a generalization of ANOVA that is intended to be used in the interpretation of single cell RNA seq data. The method requires specification of an orthogonal, discrete categorization of cells, nominally by phenotype. I found this paper frustrating to read. Having understood the problem, it seems to me that the use case for this approach is quite specific. The claim, in the discussion section, that "our approach can be easily generalized to .. additional characteristics such as electrophysiology and connectivity" was not clear to me, but I assume this still refers to phenotypes that are inferred from the scRNA seq data. But this seems like it must also be true of the proposed method, since prior to any analysis the data is transformed via PCA (Section 5.2). Overall, I am still not convinced that this is a problem that needs to be solved.<BRK>Pros:The paper studies a very important problem in gene data analysis. The proposed method is technically sound. Cons:   Relevance and Generalizability are unclear: The paper is relevant to researchers in subareas only and it is best suited to a bioinformatics or neurobiology community. I find such discussions are inadequate for the readers to understand the baseline or state of the art in this field. Experimental evaluation is incomprehensive: As a largely application work, the comprehensiveness or tricks in experiments should be explained more clearly.<BRK>## SummaryThe paper provides and ANOVA inspired method, called FLDA, for creating a low dimensional embedding of scRNA seq data. They compared FLDA to two simpler and similar approaches, namely linear discriminant analysis (LDA) and a more feature aligned version 2LDA. Additionally it is theoretically well founded on existing statistical methods. However, my main concern is FLDAs dependency on correct feature annotation, which is not mentioned by the authors. Or in other words how robust is the method to the annotation.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 7. <BRK>This approach is in line with the initial methods on neural network compression: direct (task independent) compression of weights, which is followed by NN task dependent fine tuning. In this case, the direct compression is obtained using the Palm4MSA method of Magoarou and Gribonval (2016), and then models are fine tuned in an end to end fashion using TensorFlow. **Decision**: Given the contributions of the paper and the lackluster experimental evaluation, I am recommending rejection. However, the evaluation of the proposed approach is not convincing:   Effect of chaining sparse matrices for the actual inference is not discussed. As authors are well aware, the support of sparse matrix vector products is limited in major frameworks and hardware. Therefore, chaining multiple sparse products might significantly delay the actual inference, even though it has fewer parameters and theoretically fewer FLOPs. The conclusions under such comparison strategy have a limited value: it says that chosen hyperparameters of compression A beats the chosen (by hand) hyperparameters of compression B in terms of final compression ratio/tradeoff etc. Related to the previous point, authors miss a large body of the methods that instead of expecting the weights to be in a certain form (e.g., sparse or low rank), actually force the network to attain such form using penalties and constraints. For example, in terms of low rank compression, here are some of the relevant works:   1. Coordinating Filters for Faster Deep Neural Networks (ICCV 2017)   3. Please add the comparison to best in its class results from the literature in order to fully evaluate the proposed scheme. *Post rebuttal comments*: I appreciate the author s efforts for the rebuttal, however, the feedback did not adequately address my questions.<BRK>The paper considers compression of neural network weight matrices by decomposition into a product of sparse matrices. Its main contribution is an experimental study of the effectiveness of this type of compression in neural networks for image classification (MNIST and CIFAR) with standard architectures such as RESNET and VGG19. The compression algorithm, Palm4MSA, is taken from previous work. I am not familiar with other experimental studies of Palm4MSA, but the paper has a good overview of other compression methods that I am familiar with, so I trust that the overview is complete. The main reason that I am not too excited about the paper is that the results are kind of expected. As a side remark, I don t think it makes a lot of sense to distinguish between neural architecture search and neural network compression. You can think of neural architecture search as yet another compression method that happens during training (not post training). One way of making the paper more interesting would be to consider the compression objective already during training, rather than merely fine tuning post compression. The "PSM random" and "PSM re init" methods seem like rather arbitrary choices for comparison.<BRK>The authors introduced a neural network compressing method, based on factorization of weight matrix to the products of multiple sparse matrices. The goal is to achieve high compression rate. The author used a previous algorithm (Palm4MSA) to implement the method. Pros:   The introduced method is easy to understand and seems to make sense. The idea of using products of multiple sparse matrices to represent the weight matrix is nice. The experiment result is not state of the art: it is worse than Iterative pruning. The authors stated that "an iterative like extension of our method could reach even better results",  so it would be important to include these results in the paper. In Fig.1, you can use compression rate rather than actual # of parameters as the measurement.<BRK>In this paper, the authors propose to impose sparsity upon the low rank compressions methods. The experimental results are appreciated. There are so many structured matrix schemes to compress neural networks. Now, the authors claim sparsity + low rank can provide better accuracy efficiency tradeoff. We compress neural networks for mobile computing devices, while the "best" accuracy efficiency tradeoff is not the final goal, unless you are theoretically drawing the boundaries. the current compression schemes in the literature are good enough in balancing accuracy efficiency. This is a naturally question that needs an answer. I mean you may add sparsity to achieve better accuracy efficiency tradeoff, but we do not allow it to introducing much computation overhead of sparse computations. 2.I have to raise the principle of "simple and effective" is the best, for this kind of tasks. The previous schemes (circulant, low rank, sparsity only, etc) are simple and shown to be effective. I would hope the authors do not answer too quickly.
Accept (Poster). rating score: 9. rating score: 8. rating score: 6. rating score: 6. <BRK>Cons: The empirical benchmarks are the same as used in the SIREN paper. The authors show that due to multiplicative properties of fourier/ gabor filters the end resulting mapping is also represented by a linear combination of fourier/gabor filters. Pros:  I like the paper very much, it is very well written and clear.<BRK>It is shown that this results in a linear combination of exponentially many Fourier/Gabor functions. Extensive experiments are performed, following the complete suite of experiments in SIREN [Sitzmann 2020], and it is shown that the proposed method outperforms available neural methods, except on video representation and signed distance functions. The paper proposes a novel method and shows superior performance on some tasks. Pro.simple and novel idea that fits nicely into standard frameworks  experiments demonstrate its superiority on several tasks, even generalization compared to Fourier Features  well written and easy to understandCon. (it s not in the SIREN paper)Udpate: with the additional experiments and corrections in the paper, I believe the paper is in good shape and contributes to the literature in the field.<BRK>**Summary**This paper proposes two schemes to learn better representation, one based on Fourier features and another on Gabor filters. **Weakness**      I do not see how MFN  largely outperforms  existing baseline methods. The paper is based on the idea of replacing compositional models with recursive, multiplicative ones, though neither the theory nor the results are convincing to prove this linear approximation is better. I have a hard time getting the intuition of the advantages of the proposed method. the proposed model has shown to be more efficient in training, and I assume it is also more compact in size, but there is no analysis or comments on that?<BRK>This paper propose a new network architecture, multiplicative filter network (MFN),  for implicit function reconstruction. The proposed network incorporate elementwise multiplication of nonlinear filters to replace to conventional nonlinear functions, e.g.ReLU, for better signal reconstruction. Two different multiplicative filter network is studied, FourierNet and GaborNet, which used Fourier and Gabor functions as the nonlinear functions in the MFN. Positive:The paper is easy to follow and the proposed method seems to be working well especially for the GaborNet. There is only empirical study to show that the results of the proposed method is better than SIREN (GaborNet only, the FourierNet performs worse than SIREN). Have the authors of this paper encounter similar problem in MFN?
Accept (Poster). rating score: 8. rating score: 7. rating score: 5. <BRK>This paper presents AVEC, a new critic loss for model free actor critic Reinforcement Learning algorithms. The AVEC loss can be used with any actor critic algorithm, with PPO, TRPO and SAC being evaluated in the paper. Both the theoretical and empirical motivations are strong. It builds on recent work, but the recent work identifies problems while this paper offers an original solution to these problems. Significance: the fact that AVEC provides good empirical results, and can be used as the critic loss of any actor critic Reinforcement Learning algorithm, points at the high significance of this work. It is not crucial to understand the paper, but the generality of the approach proposed in the paper may lead it to be used often by students, and so an intuition of why AVEC works and what it does would greatly help. Author response: the authors clarified my questions, so I maintain my recommendation for acceptance.<BRK>The authors also provide a clearly articulated intuitive motivation and provide experiments to support the proposal. The analysis of the experiments is also quite interesting and clearly presented. ### WeaknessesThe paper is mostly well written and has interesting theoretical insights as well as empirical analysis. Here are a some weaknesses. For the $T$ independent RV case being analyzed, the condition required for the improvement is that $\Delta  \triangleq 2 \mathbb{V}(X_i)   \frac{1}{T} \sum_{j 1}^T \mathbb{V}(X_j) > 0$, which seems reasonable unless the sample in question is an outlier with a very small variance to begin with. Just to be sure, do you measure the fit with respect to $f_\phi$ or the bias corrected version, $g_\phi$? (obviously, the latter makes more sense?). Also, it is a bit confusing to separate the plots into two depending on whether the weighting is less than one; as I m guessing the exact same plot is used for the non alpha versions in each pair of these graphs? Is this simply due to the resolution, as I d expect there to be a drop at least in the initial phase over time.<BRK>Instead of using the standard mean squared loss between critic predictions and value estimates, the authors propose to use a loss function that also incorporates a variance term. The authors combine their approach with popular RL algorithms such as SAC and PPO and evaluated on the standard benchmarks for continuous control. Although the paper demonstrates interesting empirical results, I think that the current experimental evaluation has a number of flaws that prevent me from recommending this paper for acceptance. Also the proposed loss is biased in the stochastic mini batch optimization due to the expectation under the squared term that is not addressed in the paper either. Finally, I have major concerns regarding the experimental evaluation. In particular, Hopper and Walker2d, which are used in the vast majority of the literature, are ignored in table 1 and figure 2. In conclusion, the paper presents interesting results on some tasks for continuous control. However, the paper requires more thorough experimental evaluation to confirm the statements. Also a deeper theoretical analysis will greatly benefit this work. This paper presents a very interesting idea but in the current form it is not ready for acceptance.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. rating score: 5. <BRK>The main idea in this paper is very nice and is as follows:Suppose we have the standard IV setup which isX   m(U) + f(Z) Y   q(U) + b(X)Where U is some unobserved confounder. It is well known that using any other method than linear regression in this context leads to "forbidden regression" problems. Overall, while I think there are many unanswered questions in this paper, I think it should be accepted for the community to build on it. I feel like that is not necessarily correct. For example, in 2SLS if we assume underlying heterogeneity (which, we must assume in this case otherwise why would we be learning a basis for this heterogeneity?) Overall, hyperparameter tuning for these problems is really important and even more so when you have these deep feature generators. I believe this should say "linear features" in the sense that you are learning the correct basis for 2SLS?<BRK>I liked reading this paper. It is reasonably well written and tackles an important problem in an interesting way. The main idea in the paper is learning a set of basis functions such that the structural function is a linear combination on them; the learning itself relies on predicting the basic functions from the IV which ensures that confounding information is projected out. I think the proposed method does not face the forbidden regression problem because of the linear relationship between the IV and the outcome implied by the proposed model. I would like the authors to clarify the following:1. In 4.2, if the image is given as treatment to the model, isn t the confounder posY be specified as a function of the treatment? if it is not, could the authors provides results over a larger number of seeds?<BRK>They assume an instrumental variable setting and use neural networks to fit non linear models as part of the conventional 2SLS approach to parameter estimation in IV models. In general I found the paper to be pretty clearly written and I think it shows promise. Is there a substantive interpretation for rho in the airplane example? **How does the assumption of additive noise constrain the problem? Is the structural function then not identifiable? In general additive noise is a pretty strong assumption and so this could drastically limit the efficacy of the proposed method. Both of these models are _nuisance_ models. But in general I m not sure that s the case   the authors should clarify this point in their argumentation and think carefully about this issue in the context of their experiments. The authors give some commentary on the computational complexity.<BRK>This paper demonstrates that this algorithm has a good performance on several applications. This paper is generally well written. However, this paper does not theoretically justify its solution that is in general relevant in the IV literature. Instead, this paper emphasizes its algorithm s prediction performance. It needs some clarification and justification on why the results in this paper can help people better identify and correctly estimate causal effects, which is the primary goal of IV (inference is important in IV). As a reference, causal effects estimated from 2SLS are consistent and asymptotically normal. 3.Off policy policy evaluation experiment:  First, what is the treatment variable in the problem? 4. dSprites experiments: The outcome variable $Y$ is quite important when people use IV, but it is not stated in the main text that seems to be confusing. Why should we care about this $Y$? an unobserved confounder"  > ".
Accept (Spotlight). rating score: 9. rating score: 8. rating score: 7. rating score: 5. <BRK>This paper combines object centric representations and self supervised HER goal conditioned policy learning to learn efficient RL policies for a robot manipulation task. They use SCALOR as an object based state representation, and use it to propose semantically meaningful goals for a SAC policy to achieve. They can then leverage this on new tasks to solve them efficiently.. * [1] https://arxiv.org/abs/1901.11390* [2] https://twitter.com/cpburgess_/status/1091220207941701632 It demonstrates good early results in the novel field of Object oriented RL. I have a few comments/questions:1. 4.How complex are the observations of the environment? So in summary, I believe this is a strong paper in a budding field, which deserves publication at ICLR and may interest many people there.<BRK>Summary:The paper combines an existing generative world model (SCALOR, Jiang et al.2019) with goal conditioned attention policy. The method is evaluated on object manipulation environments based on MuJoCo (Todorov et al., 2012), Multiworld (Nair et al.2018) and a Sawyer arm. Score justification:The paper is mostly incremental but it provides enough contributions for acceptance. In the visual rearranging task, the proposed method performs better than existing self supervised RL algorithms: RIG (Nair et al., 2019) and Skew Fit (Pong et al., 2020) when using 1 and 2 objects. I would also be interested in a visualization of the latent object representations learned by SMORL. Pros:The paper contributes to improving scene decomposition and object representation learning in model free RL which has practical applications in robotics and object oriented RL. There is a discussion of existing limitations and challenges (limitations of VAEs in visual RL, defining reward functions in goal conditioned RL), and how SMORL is meant to address them (goal conditioned attention policy to handle set inputs, incorporating goal and object representations in the reward). Cons:There is no discussion of the computation cost of SMORL in comparison to the baselines (SAC with HER, RIG, Skew Fit). Experiments with a larger set of objects would help in highlighting the advantage of SMORL in a general multi object visual RL setting.<BRK>This work proposes to use object centric unsupervised representation learning for self supervised goal conditioned RL, as opposed to prior work that assumes no particular structure on the learned representations (eg.VAEs).The proposed method, self supervised multi object RL (SMORL), uses the SCALOR architecture from prior work, then modifies the policy representation with single object attention and also the reward function in RL with imagined goals (RIG). The results show that the method can learn simulated pushing and rearranging tasks in a self supervised way with up to 4 objects in the scene, and outperforms RIG and Skew Fit on pushing tasks. The key novel contributions lie in how SCALOR is integrated with self supervised learning. First, after learning the SCALOR representation from data, the proposed policy uses an attention mechanism to pay attention to reaching the goal for a single object at a time. The other differences to RIG have to do with goals and rewards. The experiments show that the proposed method SMORL outperforms RIG and Skew Fit on visual pushing tasks with many objects (and “rearranging”, which is pushing with random initial positions of objects   having both sets of experiments potentially seems a bit redundant since it seems rearranging is strictly more difficult). Generally, the results on multi object manipulation and self supervised learning are strong.<BRK>### SummaryThe paper proposes to use object centric representations for RL, which can efficiently handle multiple objects in the scene. The goal conditioned attention policy can be efficiently trained with hindsight experience replay on the object centric goal representations. ### Strengths  The idea of learning composable object centric visual representations and goal conditioned attention policy is an intuitive and plausible way to tackle combinatorial challenges in multi object manipulation tasks. It would be more convincing if the proposed method can reasonably deal with more than 2 objects. The baseline could include recent visual policy learning methods using data augmentation [1,2]. The paper claims "Self supervised RL" but it is not clear which part of the method is trained with self supervised learning. The proposed method seems to consist of unsupervised representation learning and reinforcement learning. One of the claims in the paper is that the proposed representations and policy can work with a variable number of objects but the experiments do not cover this setup. However, the experiments and results are not yet convincing to claim the advantage in dealing with multiple objects.
Reject. rating score: 2. rating score: 4. rating score: 4. rating score: 6. <BRK>******************************************************************************The paper is not properly anonymized. ******************************************************************************This paper describes end to end implementation of Federated Learning (FL) on a use case of steering wheel prediction in autonomous driving. It provides empirical evaluation on real world autonomous driving datasets and shows improved performance compared to centralized learning methods. It mainly describes the implementation of FL for a real world application, which, although important, does not contribute to the field in terms of developing better algorithms or better understanding the current ones.<BRK>This paper presents a case study that applies Federated Learning for steering angle prediction in self driving cars. All methods used have been previously proposed in the literature. Pros:+ A case study for an industrial use of federated learning (in autonomous driving application). Cons:  No actual research contribution since nothing new is proposed in this paper.<BRK>This paper applies federated learning to  steering wheel prediction for autonomous driving. Cons  The main contributions of the draft are not clear. I would strongly suggest authors reconsider the design choices where I raised questions.<BRK>The study evaluates federated learning (FL) in the context of steering wheel angle prediction, which is relevant for autonomous driving systems. The work evaluates an existing approach and therefore its novelty and impact is limited. It does provide an interesting evaluation of FL for a relevant use case. For that reason, the impact of the paper is significant despite not being very original.
Reject. rating score: 4. rating score: 5. rating score: 7. rating score: 7. <BRK>Suggestions to improve:    I still think the paper could do a better job of reporting a more complete set of experiments/comparisons. Comparing against the variants of synthetic attention is interesting but not enough given that there are quite a few papers that investigate similar ideas. Why not compare to them on these tasks as well? It seems a bit strange that dynamic convolutions are competitive with self attention in the original paper, but results on GLUE are so much worse. For GLUE, Linformer has results in the original paper, why not also compare against it here? The paper needs some revision to clarify the motivations   it starts out by talking about how self attention may not be necessary, but in some of the results synthetic attention has to be combined with dot product self attention to achieve reasonable performance. They report a wide range of experiments in their paper, but the only setting compared here is T5 pre training, and even there the metric is dev set loss for masked language modeling, which doesn’t objectively mean much. Are there any results from the Fixed random synthesizer in the paper? Doesn’t have to be on the T5 pre training, MT or LM or the other tasks are fine too. __UPDATE AFTER RESPONSE__Hello authors, thanks for your response. Strengths:    This direction of trying to understand how much value dot product self attention adds is very interesting. Synthesizing the attention matrix, rather than computing pairwise dependencies is a cool idea.<BRK>Here are my comments. Should there be a transformation? 2.It will be great if you can also present the dot product attention equation together with your equation 2&3. That would be more helpful to the readers. If it is randomly initialized and further optimized, could you show some learned patterns of this matrix R? The authors propose a variant called the random synthesizer. However, from the experimental results, we can see that the random synthesizer is worse than the baselines (significantly worse on GLUE benchmarks). I am not sure of the reason behind presenting this variant of networks. There is no gain but only a drop for this choice. 2.I couldn t find a place that defines what the model R+V is and what is D+V. 4.In both GLUE and superGLUE data, small task performance (COLA, RTE..) is quite unstable. Overall comments:Dot product attention is one of the key components in the Transformer model, but the necessity of the dot product attention is not very clear. However, from both theoretical and empirical results, I cannot see any strong motivations behind the modification or any empirical benefits. But I am open to further discussions.<BRK>Summary This paper questions the necessity of the standard query key attention in Transformer layers. I found the results rather surprising, and believe that this would be an interesting and worthwhile contribution to the conference. Strengths   Interesting set of baselines/experiments (random/dense, factored, mixtures, etc.) I found some of these fascinating so I encourage the authors to consider including them in the main paper. The paper is very well written and was a pleasure to read. While the point of the paper is not to achieve SoTA performance, there is still nontrivial degradation in performance for many tasks if just using the random/dense approach. Have you tried an alternative where you fix use the same $l$ for $l$ greater than (for example) 40? Have you considered this approach as a baseline? This could be avoided if $F(X_i)$ only depends on the non contextualized representation of the $i$ th word, which means one could precompute $F(X_i)$ for all words in the vocab after training for faster inference. Given that the random baseline works, I imagine this baseline would work as well. EDIT after rebuttal: Thank you for the response.<BRK> SummaryThis paper challenges the common belief that self attention with dot product is necessary to train good NLP models. Several variants of the Synthesizer model is proposed. The authors further showed that mixing synthesizer and dot product attention sometimes achieve better results. The idea is validated on Translation, NLU, Summarization, Dialogue, and Language Modeling. ReviewI enjoyed reading this paper and I imagine it would benefit many researchers in this community. I can t find any reason to reject this paper. Experiment are thorough and cover a wide range of different tasks. Paper is well written. Cons 	This model cannot fully replace dot product attention, although it’s not a big problem in my opinion. Other Questions / Suggestions  Why not finetune from the MLM Synthesizer for GLUE? Does Synthesizer also benefit from MLM pretraining? What D/R/V stand for is not clearly stated in the paper.
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 6. <BRK>This work proposes backward smoothing as an advanced random initialization to improve a model s adversarial robustness. The paper is well written and easy to follow. However, a recent work "Understanding and Improving Fast Adversarial Training" has a very nice explanation and solution on this problem, which is much more convincing to me. 3) At the end of section 4, the authors mention that "Note that the proposed Backward Smoothing seems also compatible with Adversarial Training. However, Adversarial Training does not contain terms using KL divergence loss, which may hinder its performance. My question is: why a better random initialization can not generalize to standard adversarial training? 5) Does backward smoothing really help adversarial robustness or it just plays the tradeoff game between clean accuracy and adversarial robustness? For example in Table 8, comparing FAST Trades and backward smoothing, backward smoothing provides better adversarial robustness but sacrifices a big clean accuracy. In all, I vote for a reject for the current version of this work.<BRK>Then $x + \xi^*$ is used as starting point for one step AT. 2.The empirical results are strong, as Backward Smoothing  achieves better robustness than other fast AT versions and is more efficient that multi step AT (including TRADES) with little loss in robustness. Randomized smoothing aims at estimating better directions for the gradient step, but it s not clear whether this is the case when using a single random point (this would need to be experimentally validated). An alternative explanation of the role of random initialization is presented in (Andriushchenko & Flammarion, 2020), which would be worth discussing. While in that work there s no focus on adversarial training, the overall idea looks similar, and I think the authors should comment on this. 2.In Table 5, the gap between Backward Smoothing and TRADES is larger than what reported in Table 2. Overall, as mentioned above, the paper presents strong empirical results which support the proposed method.<BRK>**Summary:**The paper proposes a new algorithm for performing fast adversarial training. The proposed algorithm consists in solving the inner maximization problem in the following way: first, one takes a step of projected gradient descent (PGD) wrt an auxiliary loss (motivated by the idea of backward smoothing), and then one takes another step of PGD wrt the original loss function which is the KL divergence as suggested by the TRADES paper. Moreover, I find it also quite puzzling that Backward Smoothing even outperforms 10 step TRADES / AT as shown in Tables 3 and 4   not sure about a justification behind this. All the experiments are performed for eps 8/255 which is close to the eps for which even standard FGSM leads to high robustness (e.g., see Fig.1 in Andriushchenko & Flammarion (2020)). I m also a bit surprised that *"Backward smoothing ... even outperforms the state of the art robust training methods"* since the SOTA robust training methods include much more steps of PGD and thus they solve better the inner maximization problem. In the updated version of the paper, the authors improve the results of the baseline "Fast TRADES (2 step)" and add additionally a stronger baseline of "Fast AT (2 step)" (except on CIFAR 10 with eps 16/255 where it s missing). Then I think there is additional work to be done in terms of understanding what the proposed method actually does (even if we don t take into account how it was motivated   via randomized smoothing or not).<BRK>Summary: This paper seeks to reduce the training time of TRADES adversarial training. It tries to understand fast adversarial training methods (single step adversary methods) by viewing the random initialization of the adversarial perturbation in the PGD steps as randomized smoothing, making the inner maximization (the adversary) easier. Sometimes, the robust accuracy is even better than TRADES, but the clean accuracy is almost always a bit worse. Weaknesses:  My main concern is the backward smoothing method seems general but only really works for TRADES (and not PGD AT). The authors mention the natural way to increase smoothing, which would be to use larger random perturbations. This point would be more convincing if there were some experiments just using larger perturbations for random initialization. Fast TRADES seems to be at a different point of the tradeoff curve between natural accuracy and robust accuracy, and perhaps it responds differently to the TRADES hyperparameter. From Table 2,3 etc. , we can see that Fast TRADES tends to improve natural accuracy and be worse than robust accuracy than TRADES.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 6. <BRK>This paper presents an approach to training recurrent neural networks with the mix objective of minimizing cross entropy (or something similar, which is not clearly defined) and minimizing minimum description length (MDL) by using a binary representation of the network which is optimized with a genetic algorithm. The model is evaluated in contrast to RNN, GRU and LSTM  baselines on modelling a number of formal languages. ( ) Nonetheless, the studied tasks are good for a start, but insufficient to motivate the approach because actually neural networks can deal with them quite well [e.g., 1]. In particular, the task that is considered in this paper involves two bit sequences that are fed *bit by bit* in parallel to the system. The version of binary addition which is actually more challenging for RNNs is when the output is produced _after_ all operands are given [2]. I can see two directions in which this work could be improved moving forward:     If the goal is to improve interpretability of models, the authors could aim at tackling problems in gradient based neural network remain obscure (e.g.see https://blackboxnlp.github.io/)     If on the other hand the goal is improving generalization, the authors could consider tackling tasks in which neural networks have been shown to be deficient in their generalization skills [2]. First, as mentioned in the previous point, RNNs can in fact learn the binary addition task, and thus it is unclear why the authors report that they fail. Second, the authors report that the RNNs perform worse than chance on some other tasks, which could be explained by divergent training. Since no code was provided it is not possible to assess how the models were trained. The relation to neural architecture search (NAS) is missing too. ( ) It is also unclear how the training objective is quantified. By "an unseen" you mean 1 sequence?<BRK>The authors proposed a new training framework for recurrent neural networks that involves updating the weights with genetic algorithm under a minimum description length principle. They evaluated it on a syntheic mini task of languange modeling and showed a better performance over classical RNNs trained by backpropagation. + tackling the interpretablity issue of deep networks with symbolic knowledge is a great approach. However, it would be better if what works are done in these related work. And why are some results also included here in the method section? Still this section, "Again, this network is transparent, the task is learned perfectly well, and no RNNs would do as well." The result section is also lack of important details on the tasks and evaluations. SummaryOverall, the project introduced an interesting approach with great potential, but the results and writing appear to be preliminary. We suggest the authors to add more evaluations and improve the writing.<BRK>The importance of finding minimum description RNNs is definitely unquestionable for several reasons. Thus the authors address an important problem. They decide to use GA in order to find such a representation. My main reservation is that the paper is a vanilla application of GA without any enhancements or tailored aspects. In other words, I can t find methodological or algorithmic contributions. The experimental results also don t stand out. The tasks selected are all artificial (learning mathematical operations). To this end it would be great to include some practical cases.<BRK>The paper shows that theprocedure, indeed, learns compact and interpretable networksrecognizing a number of (artificial) formal languages, as well asaddition. The networks are also compared to common RNN modelstuned/trained on the same task, indicating that the networksdiscovered by MDL outperforms RNNs in majority of the experiments. Although study uses well known methods (MDL, genetic algorithms), theapplication and premise is interesting. It is not specific to theparticular study, but strengths of the method also include learningstructure of the network as well as its weights and possibility oflearning a larger class of network architectures. My main criticism likes within two aspects of the paper/study:  Although the authors touch upon this in the concluding remarks, I d  be interested to see a more through proof or demonstration of  scalability of the method. If  the space is the issue here, one can shorten the descriptions of the  artificial language experiments, or even push part of them to an  appendix. I also have some minor remarks/suggestions:  From the description in the paper, I am not sure if the baseline  RNNs got the same care and love as the proposed system. If not tuned  properly, it would not be surprising that they do not necessarily  perform well. For example, for these simple problems there is a fair  chance that at 1000 epochs some of the networks badly overfit. Similar to some of the points noted above, I d be also interested to  see more discussion/comparison with (L1) regularized learning.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 4. <BRK>I can t say that I find this a strong submission because:1. the authors give a confused (mis )definition of coverage; essentially they seem to have taken Barber et al.s definition of "marginal distribution free prediction intervals", mangled it and then called it Frequentist coverage citing Wasserman2. the authors claim one of the contributions of this manuscript to be "introduce coverage and width as a natural and interpretable metrics for evaluation predictive uncertainty" but in fact these aspects of predictive intervals from ML models has been studied for many years, as a simple google search will confirm3. the results shown will not generalise in any meaningful sense: for example, GPs are found to have excellent coverage over the set of regression tasks shown, but in fact GPs are themselves a case study in the difficulties of achieving Frequentist style coverage in the domain of Bayesian non parametrics (e.g.Hadji & Szabo 2019; Neiswanger & Ramdas 2020; Rousseau 2016 ; prior over smoothing being the root of many problems ).<BRK>While these have a straightforward interpretation in the regression setting, for classification the authors use the top K probabilities that captures 95% of the prediction probability mass to evaluate coverage and width. Various UQ methods are tested for both regression and classification datasets, and for the latter case, also under dataset shift. And with increasing usage of DL in high risk applications, an evaluation of this kind might be useful. Justify why you are taking this stance. You say this is not sufficient, but I m not convinced this is the case.<BRK>**Summary and key claims**This paper provides a comprehensive evaluation of the empirical frequentist coverage properties of existing uncertainty quantification baselines on both regression and classification tasks. So I think you have to say that these applies only to unordered categorical targets for this to make sense. I was expecting to see more evaluations that rank baselines w.r.t say calibration or Brier score, and then show that a ranking based on coverage would be significantly different, thereby motivating the usage of coverage in the uncertainty analysis toolbox. Having read the experimental section which is the key section in this paper I was not exactly sure what to make of it. A different approach for tuning hyperparameters may render models other than GPs come on top in your comparison.<BRK>### A few specifics:*    The authors  analogue of confidence interval for a classifier is novel to me, and is a convenient way to unify the presentation of results between classification and regression. If this is a novel definition, I would suggest the authors more explicitly point this out, as future literature may use it and should cite it. However, this work makes an important contribution in focusing on coverage / width, which I would agree are more interpretable metrics for practitioners. The set of methods spans several important strains of the literature: ensembling, Bayesian approximation, Dropout, GPs. The authors state "We see that higher coverage correlates with a higher average width." Figure 4 conveys some visual trends, but also could be improved. Visual understanding method specific results is not possible at the moment.
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 6. <BRK>Summary:This paper introduces “source aware” GMM attention and applies it to offline, online, long form ASR. For online ASR, the results are state of the art amongst sequence to sequence based models. Because it is so ASR specific, I feel that it would be a better fit for a speech conference. It would be good to provide this motivation earlier in the paper. * The addition of section 4.3 felt a bit tacked on.<BRK>The paper describes a simple extension to the location only monotonic GMM attention mechanism from Graves (2013), which takes the source/key context into account when computing attention weights. As above, it seems unfair not to include any experiments using multiple components when comparing different variants of GMM attention. The description of the proposed mechanism is inconsistent with existing literature and is very unclear and confusing in parts. This mechanism, which would be more precisely called "Gaussian attention", is strictly less flexible than the base GMM attention mechanism that was originally described in Graves, 2013. There is no particular motivation given for the proposed method for integrating source keys.<BRK>This paper proposes a monotonic attention to improve the latency of decoding. Some of the concerns can be fixed, and some will need additional experiments. I will revise the score based on the authors  response. The other rows are less useful, since the architectures are different. It would be more convincing to have results without these additions, and would make the comparison to others more meaningful (assuming that others did not use the same additions). The paper did mention this in the translation experiments, but it is under developed and no further hypotheses or evidence are provided. > ... the proposed attention mechanism solves the online and long form speech recognition problems ...<BRK>This paper proposes a novel Gaussian mixture based attention mechanism by incorporating the source (key) information, enabling a flexible representation of the Gaussian attention pattern with the well described formulation. The paper is well written. The precise monotonic attention gains a lot of attention for streaming ASR and simultaneous machine translation, and this paper would gein broad interests. It would be better to make this expression more understandable. These are described in the appendix, but the authors may consider discussing it in the main paper. It s better to compare the normal GMM and the proposed method in the online mode.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 6. <BRK>Summary: This paper seeks to understand theoretically the current bottleneck in few shot learning and address it with a new way of training the embedding. The authors find that the key issue in few shot learning is not the separability of the novel classes but the discrepancy between classifiers trained on large datasets and few shot classifiers. (There are probably more; I ran out of steam at this point).<BRK> Summary:This paper analyzes the upper bound of error rate on novel classes in few shot learning theoretically. Otherwise, it is hard to support the statement that the error on novel classes is dominantly caused by classifier discrepancy. In addition, this paper proposes a new method to lower the upper bound of classification error by reducing classifier discrepancy. 3.Eq.10 is too loose.<BRK>Summary:This paper aims to understand the cause of error in few shot classification. Empirical results show that the latter is the dominant term. Pros:1.This work investigates the cause of error in few shot classification and finds an upper bound for it. This could be due to the dataset being used for the experiments. Can results be provided on such a dataset?<BRK>The improvements are small but consistent which lends some credence to their approach. I find their results modest. The originality of the paper as well as the quality and significance are modest. The authors then say that experimentally classifier discrepancy is the more significant factor, and propose a technique based on reducing such discrepancy.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 5. <BRK>The paper is overall well written, and the motivations are justified clearly. My concerns with the paper include:1. Therefore, I think the notion of inductiveness studied by the authors is at most in parallel to that of TGAT (and GraphSAGE & GAT). If the authors believe their notion of inductiveness is more proper, they should discuss the similar issues for GraphSAGE and GAT since they are more well known to the readers and will raise less confusion. 3. the link sampling strategy that ensures the overall efficiency of the computation lacks support and is not principled. Therefore, I vote for a weak acceptance of this version of the paper. The authors address most of my questions and concerns in the response, and update the manuscript. I decide to increase my score.<BRK>Summary:The paper provides an interesting way to inductively represent a temporal network with the proposed Causal Anonymous Walks (CAWs) which work as temporal motifs to represent the network dynamics. The CAWs can be further encoded by the proposed CAW N which supports online training. This paper proposed an effective solution to this problem. The proposed CAW is novel for capturing the temporal dynamics in temporal networks. The description of the method is easy to follow. This paper provides sufficient experimental results which show the effectiveness of the proposed CAW N model on the link prediction task, including a wide variety of baselines and datasets. Cons:   This paper has been listed on the author s homepage (https://scholar.google.com/citations?user Ch3YUgsAAAAJ&hl en), potentially violate the double blind review rules. Some figures, e.g.figure 4, are not very useful in explaining the corner cases. It would be more convincing if the authors can provide representation visualization comparisons in the rebuttal period.<BRK>The sampled CAWs are then used to learn RNN for the link prediction. The empirical results show that it is a good input representation to be fed to the RNN for training. The empirical results based on six different datasets show that it outperforms a number of existing methods. There is some novelty but it is hard to be considered as a breakthrough as claimed by the authors. It is not clear how this is addressed in the proposed method. Qns  Is the structural and temporal information being addressed disentangled in some way? Can the optimality of the adopted encoding steps be discussed? It seems that they are determined by making references to different existing works.<BRK>This paper proposes Causal Anonymous Walks (CAWs) that are extracted by temporal random walks and work as automatic retrieval of temporal network motifs to represent network dynamics while avoiding the time consuming selection and counting of those motifs. CAWs adopt an anonymization strategy that replaces node identities with the hitting counts of the nodes based on a set of sampled walks to keep the method inductive, and simultaneously establish the correlation between motifs. Novel idea of avoiding the selection and counting of network motifs instead leveraging anonymization and encoding. Hyperparameter study shows the AUC is very sensitive to sampling strategy (time bias or sampling length). Set based anonymization seems to require a computational complexity of counting the number of nodes in different positions. For example in Figure 2, it counts the number of “b” in different positions. There is no computational complexity comparison between counting this vs. motif counting.
Accept (Spotlight). rating score: 8. rating score: 7. rating score: 7. rating score: 7. <BRK>Is this correct? The central idea is to think about the propagation of inputs through a deep neural network as a dynamical system, where the inputs/activations are the signal being propagated, and the weights of the network are control inputs that influence the trajectory of the activations through the network.<BRK>This paper presents a novel alternative to SGD based on Differential Dynamic Programming that views deep networks as dynamical systems. I am very pleased with this contribution and I propose an accept.<BRK>This optimizer views neural networks as a discrete time non linear dynamic system and derives an efficient optimizer from differential dynamic programming. In general, it is a good paper in terms of the novelty and the algorithm. 2.The paper gives sufficient background to help the readers to understand the idea of viewing networks as non linear dynamic systems and optimize it with differential dynamic programming.<BRK># Pros* The connection between gradient descent and DDP is interesting and, to the best of my knowledge, novel. Have the authors tried this? The authors show that this approach is competitive on several benchmarks with a reasonable runtime.
Accept (Spotlight). rating score: 8. rating score: 7. rating score: 7. rating score: 4. <BRK>### SummaryThis paper analyzes the bias of models in pool based active learning settings where the sampling procedure is probabilistic (non deterministic). It proposes two unbiased estimators of the population risk that weight the loss for each sampled data point. For the latter case, the authors provide meaningful insights into why biased estimators of population risk may actually be beneficial for over parameterized models. The authors also propose theoretically sound solutions to addressing the active learning bias. 3.While I did not spend time reading the appendix in detail, it seems that points for the BNN were selected under $\tilde{R}$ (the biased sub sample empirical risk). Words like "surprising" and "greatly" are unnecessary.<BRK>The motivation of the paper is well done and it is written clearly as well as well organized although could be improved for the sake of more clarity and precision. This kind of analysis is important to understand the successes/failures of active learning strategies commonly used in ML and grounding the analysis in bias variance trade off is a very useful start. I have a couple of clarifying questions that I hope the authors can elaborate on. In the discussion, the authors talk about active model selection. What is active model selection and can authors elaborate what they mean the proposed estimators are useful in this setting?<BRK>I think this gives more grounding to the nice theoretical results and insights. Although both unbiased, they differ in the ponderation on the loss for each active sample and do not have the same variance. ### StrengthsThe paper is well written and clear. It addresses the interesting problem of active learning, and it is very insightful of the particular issue of bias in that context. Each expression is explained with the meaning of its components, and the authors attempt to provide insight to both the positive and negative results. This should be more explicit in the main article, referring to the appendix for more details.<BRK>**Summary**: The authors consider the bias (in the risk) introduced by active sampling strategies with respect to the true underlying data generating distribution. The authors consider two toy examples where they show the existence of active learning bias, and the effect of removing this. + The calculations (and the proofs) are presented very clearly; the overall writing quality and the story arc of the paper is very clear**Concerns**   The condition that the proposal distribution needs a non trivial weight on the entire pool of data seems too strong. It would have been nice to see a more nuanced version of this analysis. The authors address the above concern by claiming that this paper is only about the "statistical bias" of active learning.
Accept (Poster). rating score: 7. rating score: 7. rating score: 5. <BRK>3) Concerns  While the results are very interesting, the authors could have been more explicit on how the results of this work could help the ordinary neural network user setting parameters in practice by suggesting e.g.a rule of thumb. b) "Data parallelism" is a synonyme for "batch size" and "sparsity" is equivalent to "pruning". 2) Strengths+ The paper is well written, the figures and the structure of the text is clear and the experimental setup is concise. This could be made more clear in the abstract already.<BRK>After reviewing the authors  response:The authors have agreed to include missing sparsity level results and have commented that such results are in line with the trends observed in other experiments. This effectively builds on the work of Shallue et al.(2019), by considering sparsity in addition to the previously explored behavior of data parallelism. The main theoretical result is nice, and Lipschitz continuity in DNNs has been extensively studied recently. For instance, if $f_{\inf}$ goes to either positive or negative infinity, Equation 2 does not say anything helpful. Furthermore, for non finite $f$, the  expected average squared norm of the gradient could grow at a rate faster than linear K, which would invalidate the claim in the paper: "the expected average squared norm of... during the first K iterations, indicates the degree of convergence; for example, it gets smaller as training proceeds with increasing K."  It is important to be clear and precise for these theoretical results. shouldn t this be minimal data parallelism? 2019.<BRK>The paper presents both practical (based on measurements) and theoretical result on the impact of data parallelism and sparsity on the training of neural networks. Further, the batch size is varied from 2 up to 16384 and the sparsity between 0% and 90%. The downside is that the results are in line what is expected when varying data parallelism and sparsity. Exactly where the different plateaus are and the exact slope of the curves varies of course with the selected datasets and network, but the main picture is as expected. Should be included in a revised version.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>Summary:This paper considers the problem of order learning, which learns an ordinal classification function. Main concerns:  The ORID model structure: The latent representation is separated to h_{or} and h_{id}, and the comparison loss is defined on h_{or}. However, this need not to exclude order relavent information from h_{id}. Also, right after Algorithm 1 in the paper, "DRC is guaranteed to converge to a local maximum" is quite suspicious. At least some references need to be provided as it is a crucial point of the main contribution.<BRK>The authors should show the visual differences between these two feature types. The expression of the article is very clear, but some basic theories need not be explained in detail (Such in Section 3.4)One more concern : h_id and h_or are both used for reconstruction.<BRK>  It is well presented. The results show the effectiveness of the approach. As this is one of the key contributions of the paper, a comparison of ranking performances with and without the use of the repulsive term in clustering would be useful. How sensitive/robust is the proposed approach to the number of clusters chosen? A discussion on these would be useful. In general, I think this paper is above the borderline. But I  would also like to see the comments from other reviewers.<BRK>The paper is clearly written and explains the approach clearly. I greatly believe that this discussion is necessary and the lack of it is one of my top concerns about the paper. The method is intuitive, clearly explained and well motivated. **Strengths**: The authors describe an intuitive and effective method for making predictions on ordered data.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 7. <BRK>In this paper, the authors present and analyze a class of gradient descent algorithms for solving min max problems when the first (minimization) variable is constrained to live on a Riemaniann manifold. * between (22) and (23) "By the function f is strongly concave" Experiments are performed with the setting of minimizing losses of neural nets whose weights are constrained to live in the Stiefeld manifold while an attacker of small norm perturbs the input. This work is one of the first I see tackling explicitly min max optimization over a Riemannian manifold. However, the presentation and writing of the make it quite hard to follow and, in my opinion, not ready for publication at that stage. nabla_y lives in R^n so the transport is identity?... * The authors state that "this is the first study of minmax optimization over the Riemannian manifold" however, there seem to be some works on finding stable points of variational inequalities (by extragradient and the like) on Manifolds, eg. "Monotone vector fields and the proximal point algorithm on Hadamard manifolds." * Theorem 1: In the case of deterministic gradients ( ie (8) ), I do not get over what the expectation is. Or, it is related to the random output (line 10) in which  case the sum seems not needed (as in E[H_\zeta] below). * I do not get how the authors obtain the first line after "By plugging the inequalities (22) into (25), we have" in Appendix AMinor Comments/Typos:* Abstract: "on the Riemannian manifold"  > "on Riemannian manifolds"* Preliminairies: several typos "We define a retraction R maps tangent space" "Exp mapping is a special case of retraction that approximate the Exp mapping up the first order" (!)<BRK>This paper studies the convergence of min max optimization algorithms when the minimizer takes values in a manifold and the maximizer in a subset of R^d. The authors proposed three algorithms and analyzed the convergence rates to stationary points. There are two major issues with the paper:1) There is essentially no novelty in the theory. Specifically, under the very strong assumptions 1 5 and that the maxmization is strongly concave, one can simply pretend that the retraction is a gradient step and then the analysis for gradient descent ascent would hold almost verbatim for the proof provided by the authors. In other words, as the difficulty of analyzing Riemannian algorithms is already "assumed away" by the assumptions, there is virtually zero element of Riemannian geometry in the proofs. 2) The empirical evaluation is rather incomplete and disconnected from the theory:2a) First, the only algorithms presented are the ones proposed in this paper. What is the retraction used and what about its computational efficiency? 2b) The main motivation (and the only considered applications) of the paper is robust training. The authors did not provide any justification as why they could switch the order of \sum and max at will. Based on the above, I recommend rejection.<BRK>1.Summarize what the paper claims to do/contribute. Be positive and generous. This paper considers solving the minimax saddle point of the form \min_{x\in X}\max_{y\in Y} f(x,y), where X is a Riemannian manifold and Y is a closed convex set. The objective function f is nonconvex in x and is strongly concave in y. To the best of the reviewer’s knowledge, this min max problem with Riemannian manifold constraint has not been considered in the literature. If this cannot be done, the authors should state the difficulty. But there are also some subtly in the arguments. Sample complexities of O(\kappa^4\epsilon^{ 4}) and \tilde O(\kappa^3\epsilon^{ 3}) have been derived for the MSGDA and MVR MSGDA methods respectively. The adversarial training of DNN and a specific instance of the DRO problem are proposed as the motivating example of this work. 2.Clearly state your decision (accept or reject) with one or two key reasons for this choice. Though the authors consider the Riemannian constraint on x, from the author’s experience in Riemannian optimization, it does not make much difference as long as the retraction operator is available. (ii).The RSGDA analysis also shares the same novelty concern as stated in (i). Moreover, the sample complexity of (Tianyi Lin et al., 2020) is only O(\kappa^3\epsilon^{ 4}), which is much better than the O(\kappa^4\epsilon^{ 4}). The reviewer believes that following the proof of (Tianyi Lin et al., 2020) step by step while replacing the gradient step by gradient retraction step, O(\kappa^3\epsilon^{ 4}) will be achievable for RSGDA. However, this is wrong on manifold. The authors seems to believe \|R_x(u)   x\|   \|R_x(u) – R_x(0)\| \leq \|u\|, but this is not true. Make it clear that these points are here to help, and not necessarily part of your decision assessment. (I) The authors should carefully describe the relation of their result and that of (Tianyi Lin et al., 2020).<BRK>The paper proposes Riemannian algorithms for min max problems where the min problem is over a manifold and the max problem is a strongly convex problem. It presents a rigorous convergence analysis of the algorithms proposed. Finally, experiments show the good performance of the algorithms for robust training of DNNs over manifolds. The work is relevant for practitioners who use similar min max problems as it provides necessary theoretical backing to the use of such constraints in DNNs. CommentsAssumptions on the manifold are missing, compactness is required? Since the max problem is strongly convex, one can in principle directly work the formulation min_{x in M} Phi(x). (One work in a similar spirit is [1]). Of course, I assume that the max problem is solvable with an iterative solver which is easy to implement in many cases. If that is so, then why go for explicit updates of the max variable y? A discussion on this is missing. However, it can be broadened by including other baselines. This will strengthen the paper. There are some issues with how to use \citet and \citep while citing papers, especially in Section 2.2. For example, in “More recently, Sato et al.(2019) has proposed…,” it should either be \citep or has  > have. Please look into other such issues in the paper.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 6. <BRK>And then, several alternative convolution operations are used to further process features. The novel convolution operation is consisting of intra sphere and inter sphere convolution. They also design a mapping function to convert point cloud data to mesh data. A novel method to discretize space, which can be used for represent 3D data. Math symbols need to be unified. E.g.d_u、d_v in Eq(1)。4. Although the paper seems to propose an effective convolutional operation for point cloud representation, it is unclear how to maintain its rotation invariance.<BRK>This paper proposes a Spherical GNN approach for effective representation of 3D data (point clouds and surface meshes). In terms of applications, it would be helpful to showcase a few application studies and to demonstrate the advantages of adopting the proposed scheme. The empirical evaluation is also somewhat lacking in its breadth and depth.<BRK>Summary:This paper presents a novel multi sphere icosahedral discretization for representation of 3D data. The authors claim that such a structure could better preserve information of 3D objects and showcase on mesh and point cloud classification tasks. Also, RBF needs some explanation. More explanation should be added in Sec.5.2, especially the way to do ray marching in multi layer spherical CNN. This is not super fair since the experiments may be done in different settings. are different?<BRK>In this paper, a multi resolution convolutional architecture is proposed to learn from concentric feature maps. In general, the proposed method is innovative on the basis of spherical discretization, but still has some limitations. A multi sphere icosahedral discretization for representation of 3D data is proposed, which can enhance  representation ability and keep more details over single sphere representations. The complexity analysis is insufficient. #### 2.In the converting of point cloud to concentric spherical signal,  the Gaussian radial basis function is adopted to   summarize the contribution of points. Is there any other function that can accomplish this job? #### 4.There are some typos in the draft.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 5. <BRK>Diversity is undoubtedly an important problem in recommender systems which now relies heavily on neural networks especially RNN. Given the strength of this paper, I still have some concerns. Second, for theoretical results, I think the hardness mainly lies in dealing with $\theta$ calculated by RNN instead of linear form. However, given the strong assumption 4.1, all the theoretical statements and corresponding proof seem to be nearly the same as LinUCB, and I didn t see the role of RNN. In the first line of section 3.3, there are two "can";b. Om the paper, authors assume the variance of $x_k$ is a diagonal matrix, I wonder whether results still hold if the covariant matrix is a general PSD matrix;c. $A_t$ defined in Algorithm 1 and Algorithm 2 are not used in algorithms, which can be removed;d. $\hat{A}_s$ is undefined in the main content;<BRK>The paper claims that the proposed approach can generate better long term better performance. Being able to balancing performance vs exploration is important. This paper also identify another important aspect which is the uncertainty. 2.The paper provides theoretical bounds for the proposed approach. The experiment of this paper is weak. The authors should explain. What is an encoder in an RNN? It is not very clear what are the time intervals to divide the data into different settings. Are you predicting t time steps in the future?<BRK>The authors propose a recurrent neural network approach to jointly perform representation learning and exploration. I think the proposed approach is interesting, has good theoretical support and is presented well. However, I also have several major concerns. First, the exploration score is not used to update the RNN model or representations. Second, I don t think that the experimental set up is fair. Virtually any form of exploration will be beneficial here particularly for the Trivago dataset where there are only 25 alternatives. This can be seen from the relatively easy MovieLens dataset where precision curves at the beginning are all near 0. Netflix experiments make a step in that direction but the time horizon is very short.<BRK>Summary:The authors study the problem of exploration for recurrent neural networks in the sequential recommendation. It shows an approach to incorporate the exploration idea from bandits. The datasets the authors used are subsets of movielens and trivago, which are a bit small. I would suggest not to sell this as a theoretical contribution. Overall, this paper is well motivated and clearly written but has several issues that remain to be clarified.
Accept (Poster). rating score: 7. rating score: 6. rating score: 4. rating score: 4. <BRK>Summary: The paper demonstrates that incorporating equivariance (i.e.symmetries) into model for predicting fluid dynamics improves its performance, especially when the test distribution is transformed by those symmetry groups. Given the growing importance of machine learning in physical dynamics and climate science, better understanding on how to incorporate prior knowledge (symmetries) could lead to better modeling. There could be components of the model that are not equivariant, such as pooling or convolution with stride 2. However, there is no experiment validating this claim. For each of rotation, uniform motion, and scale symmetries, there is a model that incorporates such symmetry. In the conclusion, it is claimed that "all of our equivariant models can be combined". It s not clear how the proposed models different from equivariant CNNs in the literature [9, 10, 11, 47].<BRK>This paper studies improving the modeling of physical dynamics with equivariant neural networks. In particular, this paper focuses on a new type of data governed by physical models. Several special symmetry groups are considered to better characterize the system, including uniform motion equivariance, resolution independent scaling, and resolution dependent scaling, etc. Simulation results show that the proposed equivariant model yields better accuracy and physical consistency than the non equivariant models even with data augmentation, given the type of distributional shift is known. However, how the "ResNet and U net" networks are used to solve the dynamics prediction problem is missing from the main text. Questions:   Is data augmentation available as a baseline for experiments in Table 3? If a system is known to satisfy multiple symmetries, is it possible to incorporate all of them together in a network?<BRK>This work incorporates symmetries into a convolutional neural network to improve generalization performance and prediction accuracy. This work studies an interesting and important question in deep learning. However, the reviewer feels that the paper in the current form is difficult to follow with many places unclear. The overall result is based on a combination of previous work on equivariant convolutional neural network, the reviewer finds it hard to parse (the t and obtain a general methodology (or systematic approach) for dealing with symmetry from the work, and there is no high level message conveyed in this work. 3.It would be better if the author can provide more numerical results on real data other than simulated data. This is a bit confusing.<BRK>Physical dynamics have symmetry properties, which can be leveraged by neural networks for better accuracy and generalization. This paper takes 2D Navier Stokes (NS) equation as an example and re design the convolutions in networks. However, the novelty and technical contribution is limited. 1) the symmetries of NS equations are well studied and borrowed from [37]. Are the proposed re design for uniform motion equivariance in eqn (2) and that for scale equivariance in eqn (5) compatible? How many augmentation for each instance on average? Will the performance increase with the number of augmentation? Is it possible to do Augm for Ocean dynamic?
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>This paper proposes a new hard label black box adversarial attack method based on reinforcement learning. The general idea is to improve the adversarial noise compressing efficiency taking advantage of past queries and the policy network. There is almost no intersection between reinforcement learning and adversarial attacks for three main reasons. This paper innovatively applies reinforcement learning in the neighborhood of the decision boundary and solves the above problem, realizing the adaptive adjustment of the noise searching direction. Secondly, if the policy network is optimized after each query, will the attack speed be severely lagged?<BRK>The paper is well written and well presented. It might open a direction for other fields interested in security besides the usefulness in the adversarial ML community. It would also be appreciated to show the attack s transferability between different models on the same datasets. Missing some references[a,b]. al , "Cross Domain Transferability of Adversarial Perturbations", ( NeurIPS 2019 )  [b] Hamdi et.<BRK>This paper proposed a new black box attack method in the hard label setting. How do we guarantee the proposed attacking method will converge? The proposed method is introduced as an attack that minimize L_2 distance. Is it possible to extend this attacking method to L_inf? I found the idea of the paper original and interesting. And the authors have conducted experiments that show their new method has the best query efficiency, which is reasonable and aligns with their idea. And if the experiments could be conducted on more data sets and more victim models, then it would be more convincing.<BRK>Summary:This paper proposes a new hard label black box adversarial attack method based on reinforcement learning. The authors formulate the black box attacking problem as a reinforcement learning problem, and design a policy network to learn the appropriate attack directions, in order to achieve more efficient attacks. This paper is the first to incorporate reinforcement learning into black box attacking. In the experiments, the paper mainly shows the results with pre trained policy networks. 2.As also pointed out by the authors, reinforcement learning is itself sample consuming. Applying RL to a complicated task could be difficult and non stable.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>Briefing:This paper proposes a new infoPro loss for locally supervised training that alleviates the problem from greedy supervised learning, which collapsing task relevant Information at the beginning of the layers. Strong points:Analysis of the phenomenon when applying the greedy supervised learning (GSL): The two observations from the authors seem natural. Interestingly, the proposed method outperforms E2E training in the ImageNet classification task.<BRK># SummaryThe paper analyzes the pitfalls of locally supervised learning from the point of view of information propagation and proposes a new auxiliary loss that can facilitate locally supervised learning. The authors further introduce now contrastive learning fits in the framework as a lower bound maximization process regarding mutual information. This then brings up the important question, whether the dynamic caching version suffers from the same fate it should not. This is a practice that should be avoided.<BRK>Anyhow, ablation study of different batch sizes for contrastive loss should be added here. (2) The paper is well motivated by analyzing traditional locally supervised training. If the ablation study for training hyper parameter τ in the contrastive loss function is provided, it will be good.<BRK>As a drawback, however, it had been shown earlier that such local training is less optimal than global training in terms of the achievable generalization performance. The authors propose a new training strategy that aims at combining the memory efficiency of local supervision and piecewise training with the error performance of global training. I feel that I have learned something from the paper.
Reject. rating score: 3. rating score: 5. rating score: 6. rating score: 7. <BRK>More GMED edits will thus increase the probability that the initial label is wrong. _________[1] Rahaf Aljundi, Lucas Caccia, Eugene Belilovsky, Massimo Caccia, Min Lin, Laurent Charlin, andTinne Tuytelaars. Online continual learning with maximally interfered retrieval. In NeurIPS 2019. I think this is why the authors only do one edit. Specifically, the authors have responded to my concern about the lack of novelty:    In GEN MIR, the classifier and the generator separately retrieve most forgettable examples for themselves. The approach does not learn a generator that can “generate examples that are more forgettable for the classifier”; instead, feeding more forgettable examples in GEN MIR aims at reducing the forgetting of the generator. It also uses it for itself, see Equation 3. GEN MIR is thus a gradient based memory editing for CL.<BRK>Differentiation with respect to forgetting looks a promising idea that can complement differentiation with respect to current task accuracy and leads the CL algorithm towards a more stable solution by a good tradeoff between stability and plasticity. One major concern about this paper is the empirical validations. The idea of memory editing can also be added as an additional loss to other methods too (not only to ER and MIR). It may suggest that most of improvement is coming from replay (it can be anything) and the regularization effect that it provides to not to overfit to the data. It s good to elaborate on this more. The connection of eq.1 and eq.2 to te final editing formula (stated in eq.3) is a little loose. page 2  >... one online model...page 3  > an categorization ...page 4  > also show that ...Also note that citations are not properly using parentheses.<BRK>The key idea is to modify the individual data points in the replay memory to maximize the one step forgetting at the current time step. Since continual learning aims to learn the data s distribution, I think modifying each data point does not align with this goal. Especially, R1 raised a serious concern about the similarity between GMED and GEN/AE MIR. One possible explanation for the performance improvement is that the editing acts as a kind of data augmentation. From this point of view, I think it is critical to show that GMED is better than other data augmentation tricks. However, there is no much detail about the experiment to verify its validity. Since it is an important experiment, I strongly recommend the authors to provide enough details for reproducing. ### Confusing motivation and writingThe fundamental hypothesis of this paper is:> replaying examples that are likely to be forgotten by the current model helps retain its test performance. With 20% of the dataset, I suspect that even i.i.d.training on the memory would not differ much from i.i.d.training on the whole dataset. At present, most CL papers work on toy problems like CIFAR 100. In my opinion, what the community should look for is an efficient algorithm that can scale to real world problems. For this reason, I think it is more meaningful to constrain the number of examples in memory.<BRK>This paper deals with continual learning. Here we store some training examples in memory from the tasks seen thus far. These are then mixed in with new training data so as to encourage the model to not forget past tasks. Thus, if we take our examples in memory and update them (via gradient ascent) to be "more forgettable", then when replayed these will result in the most benefit to the model. * One comment I have here is that the way this motivation is presented in the paper can be improved; perhaps with a more clearly written paragraph in the preliminaries. Further, the authors propose an online metric for how much the network forgets an example (by comparing the loss at consecutive timesteps). Lastly, we replay the edited example to the network and perform a gradient update based on both this edited example and the current training example. The experiments are performed across standard tasks and show modest to good improvement over the range of datasets. Lastly there is some good visualization and discussion of what editing does. *  I think some ways to strengthen this paper would include addressing some of the comments above, specifically around motivating the "editing" process better and clarifying some of the language around it. While the results here are promising, they do not increase performance uniformly across all tasks   for example on mini Imagenet which is harder than MNIST   this method does not help. In the current form however, this is still a very good submission and I would recommend acceptance.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 5. <BRK>##### 1.SummaryThe paper presents a method to combine graph convolutional neural networks (GCNs) with generative adversarial networks (GANs). Experiments are conducted on standard benchmark datasets and the proposed method, GraphCGAN is compared against several state of the art approaches. The claim that GraphCGAN outperforms GCN and GAT is not supported by all the results in Table 1. This is, in my opinion, the main weakness of this manuscript.<BRK>Pros: 1.This paper proposes the first combination of GNN with GAN for semi supervised learning. 2.The contribution and novelty are limited. This paper deals with semi supervised learning on graphs based on GANs by proposing a framework named GraphCGAN. The proposed framework can be easily extended to include other GNN methods. However, the novelty of the proposed model is limited, and the motivation of this paper is not strong. Also, the only motivation stated by the authors is to improve the performance on semi supervised learning, while the improvement of the performance in the experiments is limited (i.e.~1% improvement). Thus, the significance of the proposed GraphCGAN is not clear compared to the existing GCN and GraphSGAN. Semi supervised learning on graphs with generative adversarial nets.<BRK>This paper combines adversarial learning with graph neural networks to improve the performance of GNN for semi supervised node classification. 3.The proposed method is flexible, which can be used for various GNNs  Negative1. The novelty of the paper is limited. It is simple extension of GAN for semi supervised learning to GNNs. However, it lacks novelty.<BRK>This paper proposes a novel framework to incorporate adversarial learning with convolution based graph neural network, to operate on graph structured data. The proposed method is inspiring. 1.This paper announces that it proposes a novel approach called GraphCGAN to deal with the three challenges of constructing a general framework. 2.More experiments involving different scale networks are needed to prove the effectiveness of the proposed method. This paper is globally well organized and clearly written.
Accept (Poster). rating score: 9. rating score: 7. rating score: 5. rating score: 4. <BRK>Strength:  Achieve robustness performance similar to adversarial training without tampering with the classifier. However, training long run EBM is challenging.<BRK>However, are the reported performance obtained by training them with long run dynamics? 2.An Expectation over Transformation (EOT) defense based on Langevin dynamics. Although Adam was found not useful and switching from Adam to SGD was found to be helpful, the images from SGD alone is not shown.<BRK>VAEs are also capable of generating samples given the corrupted images, and the sampling process is much faster than long run Langevin dynamics! 2) The paper is not very clear in terms of the methods, descriptions, and explanation of the tables.<BRK>It is not convincing enough that the proposed defense will be different. Also, the proposed method is not as robust as a variant of adversarial training (Zhang et al.2019) as shown in Table 2.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 6. <BRK>Quantitative results are not convincing. ### Strengths:  The paper focuses on the important problem of exploiting weakly labeled video data, by exploiting its structure, for example by recovering temporal structure in an autoencoder fashion. ### Weaknesses:  The concept of hierarchy is not well defined or well motivated. While most hierarchical papers refer to hierarhies of concepts, the hierarchy considered in this paper is much weaker as a hierarchy, and it refers to subactions within longer actions (not actions that are specific instances of more abstract actions). While this way of understanding hierarchy can be valid, it is never explained or motivated in the paper, or even compared to the standard way of understanding it. The overall motivation for the method has a lot of gaps. What should we learn from it? What is the relationship between u and v? How are these evaluated? This is, the idea of hierarchy would disappear, but this idea is not used in the experiments anyway. I can understand it can help, but it is not central to the method. Do the authors think that the system has really learned (unsupervisedly) to identify interesting openings?<BRK>This paper addresses the problem of extracting a hierarchy of concepts in an unsupervised way from demonstration data. The two level hierarchy has been seen a lot in relevant fields such as video recognition. So the concept by itself is not new. In fact, I find it somewhat arbitrary that the authors decide to set the number of hierarchies to two. The cooking video dataset does not have such annotations so hypothetically the one can set an arbitrary number for the levels of hierarchy. While the authors form this work as concept learning, the problem nevertheless relates to these research topics. The authors may need to look upon those for a better variety of baselines and also evaluation metrics. Experiment results are weak.<BRK>Specifically, this paper considers 1) unsupervised setting; 2) the hierarchy of concepts, and conducts experiments in two datasets. However, there are some points in the experiment section to be discussed. The pros of this paper include:  The task setting of this paper is important and applicable. The proposed model utilizes multiple regeneration module for the concept learning, which may also help for other hierarchical aware tasks. The cons include (In my opinion, the main weakness is the experiments):  Some design of model (e.g., Traversing across the concept hierarchy, Observation, and Instruction Regeneration, etc) are not fully estimated in this section: are they really useful? I believe more powerful baselines should be proposed, otherwise, the contribution of the paper is hard to be recognized.<BRK>This paper addresses a relatively new topic to learn the hierarchical concepts in videos and commentary in an unsupervised manner. A metric (time wrapped IoU) is also proposed for quantitative evaluation. I rate the paper as weakly accept and here are some questions. How do you define the level of concepts and how did you balance the levels in video and language? 3.You used two different evaluation metrics in two different datasets, which seems that you tend to focus on different aspects in different datasets. Besides, the comparison with other work in the second dataset (Chess Openings) is not provided.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. rating score: 7. <BRK>The authors formulate an assistance game in this setting and show that it can reduce to an equivalent POMDP. The work then describes a communicative assistance problem and shows the equivalence of reward learning to assistance and visa versa. The high level area of human in the learning is a useful and important space. This is important to show in order to support the claims of the paper. What is the main novelty here with respect to more general active learning approaches? A few minor notational confusions: for example, it’s confusing to have R be the robot as it’s often the reward function. The evaluation was also simple and not very convincing with respect to supporting the claims.<BRK>### OverviewThe paper addresses the problem of learning from human feedback. The paper shows that reward learning problems can be converted to assistance problems, turning queries from reward learning to communicative actions in a two phase communicative assistance problem. Conversely, two phase communicative assistance problems can be converted to active reward learning problems. ### CommentsIn my opinion, the paper reads very well, and the discussion in the paper is quite interesting. In spite of an interesting discussion, I am not certain about the contribution of the paper.<BRK>The paper neither provides practical and direct guidance on how to build algorithms to leverage "assistance", nor is a survey that focuses on organizing the area and discussing differences between works. Perhaps the paper would be better placed in a "Blue Sky" track. For example, "Non active reward learning" seems to me equivalent to inverse reinforcement learning. In the case the authors deal with a different problem in this paper, I suggest that the manuscript is rewritten to thoughtfully explain the difference between all those scenarios. Other suggestions  I don t get how the policy decision function will compute the expected reward if the reward function is unknown. do you need the probabilities for taking each action?<BRK>+ The proposed taxonomy (i.e., assistance games) can serve as a useful common ground for discussing the different paradigms, problems, and solutions of ‘agent learning from human feedback.’  + The paper is well written and organized. Some of these threads have strong overlap with the setting proposed in the submission. The key contribution of the submission is unclear (e.g., whether it is a survey, a model, a taxonomy, or all?). For example,     (locally active reward; reward learning paradigm) Michini, Bernard, and Jonathan P. How. This insight is identical to that of planning / control formulations in human robot interaction literature that model the human’s preferences (which in turn influence the reward) as latent states. "Bayesian nonparametric inverse reinforcement learning."<BRK>UPDATE:After extensive discussion with the authors, I m raising my score to a 7. I was also disappointed that the paper didn t discuss reward corruption. For instance, H might fail to tell R that its behavior was intimidating if doing so previously had led R to become more (instead of less) intimidating. I think this is a common criticism of papers that do not adhere to a conventional format or "type", but in this case, it seems unfair. And I found this motivation for the work quite compelling (emphasis mine):> This existing literature is exactly why we wrote this paper: almost all of the existing literature on learning without reward functions can be captured by the reward learning paradigm as we formalize it. END UPDATE Evaluation:This paper is well written and makes a nice point regarding qualitative advantages of assistance over reward learning. For example, the authors state: "Thus, for good performance we need to model π^H.
Reject. rating score: 3. rating score: 5. rating score: 5. rating score: 6. <BRK>Modelling diffeormophisms with neural networks is not verynew. While the introduction of time dependency is interesting, more from a technicalperspective than a conceptual perspective, I believe there are several aspectsof the paper that needs improvement. But by how much? 2.MNIST classification experiments raise some concerns:a. This result is not motivating for the proposedmethod. The method proposed here does not seem to be better than the methodproposed in Lee et al.2019.The lower HD distance can be motivating but valueof this may be better justified. If the proposed innovation was substantial,lower accuracy would have been completely fine.<BRK>3) The authors use the scaling and squaring to solve the ODEs that describe the diffeomorphic transformations, which I have not seen used in other ML studies. My suspicion is the combination of these two assumptions explain some of the behaviours seen in the results. Why is it the case that the supposedly non diffeomorphic models in Table 1 perform better on this metric than the diffeomorphic models? On the whole, it is a solid paper with an interesting and novel contribution, but I think it is hampered by the lack of clarity in the areas listed above. Questions for the rebuttal period:Please refer to the questions in the weaknesses section. If these concerns are addressed, I think the paper will be greatly strengthened in the future.<BRK>The research topic is quite interesting, but the submission is at the preliminary stage and needs more work before publishing. 1) The writing of the paper could be improved in terms of clarification. Most of the time, the authors present the solutions without explaining the motivations or underlying intuitions. For instance, why do we have to have the time dependent vector field? 2) The novelty of the proposed method is limited. Very likely, this approximation is not very efficient, and we also don t know its approximation accuracy. 3) The performance improvement is limited. In the MNIST experimental results, the accuracy improvement is subtle.<BRK>This paper propose a novel method to incorporate shape prior in neural networks based on Diffeomorphic transformation. This is useful as by design it preserves certain desirable properties of output such as smooth boundaries and connected components which are of interest in medical imaging applications. The method is validated on Mnist for data invariance and a medical imaging task for segmentation. Is there a good reason to instead choose breast tissue segmentation task only and not show any experiments on the former one?
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 7. <BRK>I like the paper now a lot, but the unimpressive results still stand. I would not object acceptance, but I would prefer the work to be more complete in this regard first. This is a dramatic and bold idea, but the paper does not explain why this combination would be a good idea, or what’s the benefit. It does not seem to a dimension at all, but instead a state vector of the state space? It seems that this paper takes Raissi2019 method and adds a loss function suitable for image classification. The experiments show that the PDE net, ODE net and ResNet are all equally good at classifying MNIST and SVHN (with no significant differences).<BRK>Summary:The paper proposed the method of neural PDE as an improvement of neural ODE. The experiments show that compared to neural ODE, neural PDE achieves comparable accuracy but with less forward pass inference time; but these experiments are not convincing enough. It is not clear why the authors train in this way. The authors should state this clearly. In fact, I think this part is not the main part of the paper, and can be moved into appendix. Section 3 is not well written and the description is not clear. Is (d, t) the same for all the inputs?<BRK>2.Clearly state your recommendation (accept or reject):I am leaning towards recommending an accept to this paper, as it proposes a coherent way of solving supervised learning tasks with Neural PDE. 4.Ask questions you would like answered by the authors:a) The main question I have is the relationship to prior work, including Ruthotto and Haber (2019), Long et al.(2018) and Raissi et al.(2019).The authors did mention, for example, that the main difference with Long et al.(2018) is that the latter focuses on scientific problems and only uses a set of discretized points of t. What else are the novelties compared to the previous approaches, besides these as well as alternating between the training of the forward problem and that of the backward problem? It may also be reasonable to compare against those models in the experiments.<BRK>The main contribution of this paper is to address the numerical instability issue in neural ODE method when solving the integral problems. I also like the idea of learning governing equation and regression model together by an alternating algorithm, which in theory should be better than training a differential equation based neural network with priori knowledge of governing equations. My main concern is that to avoid the integral problem in neural ODE, the authors paid the price to treat the whole neural network as a fully coupled system and had to solve for all the layers simultaneously, which needs more variables/parameters to tune.
Accept (Oral). rating score: 9. rating score: 8. rating score: 8. rating score: 8. <BRK>The representations are learned offline/passively, but they are learned as to improve the efficiency of the very exploration process that builds that dataset for offline learning. Without this step, the hiding agent may find ways of manipulating objects in a way that makes them simply unretrievable (e.g.the object is pushed into a corner in a way that causes it to glitch out of the room, etc.). The paper is easy to read and convincing. Experiments:  All well done. How does gameplay relate to flexibility? The difference now seems to be the availability of simulators with visual fidelity comparable enough to reality to demonstrate meaningful sim2real transfer. Recommendation: strong accept. How much of the richness of the simulated world comes through in the task feels relevant to the core ideas of the paper, but the paper currently does not address this kind of detail in the design of the Cache game within AI2 THOR. Hopefully it will be defined in the body text. Good that representations of interest (SIRs/DIRs) are named and distinguished. Many other papers, in the interest of highlighting end to end training, would forget to do this. Good explicit list of contributions, excellent that two are specifically centered on representation learning.<BRK>In this paper, the author s propose an embodied adversarial reinforcement learning agent that can play a variation of hide and seek called Cache. This environment is a high fidelity interactive world. The authors argue that the agents are able to learn flexible representations of their observations which encode information such as object permanence, free space and containment. The authors present a concise and well researched literature review which serves to distinguish their work as novel and well built on underlying prior work. First, an adversarial game Cache which permits the study of representation learning in the context of interactive visual gameplay. Finally, they present a study of the static and dynamic representations learned by the agent. It is well researched, well reasoned and presented in a way that is easy to understand and reproduce. I feel like this is a paper that would be of great interest and benefit to the ICLR community.<BRK>This paper proposes embodied game playing with artificial agents as a method to learn better representations of their environment. They describe a game, cache, which is a variant of hide and seek played in a virtual environment and a method for training an agent to play the game. They present results which demonstrate that the static representations learned through game playing perform better than other pre training tasks within the same virtual environment, on both virtual vision and real world vision applications. They also show that the dynamic representations are useful for completing object permanence tests inspired by developmental psychology research. Assuming I understand the methods and results correctly, this seems like a clear accept. My only reservations are the complexity of the task and how the results are presented in the paper, which are relatively minor issues. It seems possible that a simpler task may have comparable results, and it would be an interesting direction for future research to investigate how each of these stages are contributing to learned object permanence. Unfortunately, it is a bit difficult to follow as a result.<BRK>Summary This paper examines the representations learned during adversarial gameplay, specifically a hide and seek game called Cache. The hiding agent must place an object in a room such that the seeker agent cannot find it. The authors argue that the adversarial nature of the game shapes the representations. Inspired by psychology experiments performed on children, the authors examine both static and dynamic representations to probe whether they contain information about properties of the environment such as object permanence. The paper is thorough, detailed, and well written.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 6. <BRK>The singular value decomposition of network activation is used to regularize the embedding space with unit circle embedding assumptions. In addition, a mathematical analysis of the mean singular value boundary is provided to reduce hyperparameter tuning. However, the paper is a bit confusing and lacks clarity. + It is impressive that the corner cases of the proposed algorithm are written in the supplementary material. It seems not enough to simply apply singular value decomposition to the regularizer. I would like to have various experimental results.<BRK>I have reviewed this paper in this year s Neurips. At that time, reviewers and AC have some good points but are not yet addressed (I have compared the two submissions carefully). The idea is to push the distribution of high dim feature embeddings to be a uniform distribution across the embedding space. The idea is implemented by adding a regularize to maximize the averaged singular value computed from a mini batch. Pros:1) A simple but effective regularizer. So the experimental results make me confused about the effectiveness of the proposed method.<BRK>Summary:This paper proposes a new approach to regularize the feature embedding of neural networks. These bounds help to tune the mixing parameter of network’s loss and the singular value loss. model collapse. 2  The proposed regularizer is particularly useful with a larger learning rate. It seems that the number of epochs in the retrieval experiments is the same across methods.<BRK>Pros:This paper is well written and easy to follow. It is also well organized from intuition to method and then to experiments. The background of the proposed method is interesting and useful. And corresponding analysis are also provided. Cons:It seems that the proposed method cannot consistently improve the performances over the baseline method, such as Angular Loss et.al. What is the reason in fact?
Reject. rating score: 4. rating score: 4. rating score: 7. <BRK>In this paper, authors introduce a new approach to content style (C S) disentanglement for multimodal unsupervised image to image translation. In addition, the quantitative metrics used in this paper, in particular the content transfer metric, are very reasonable for evaluation of disentanglement quality. On the other hand, the paper is well written and well structured ans easy to follow; the translation results look promising. There are a few issues in this paper that I would like to be addressed.<BRK>Summary: This paper presents a novel combination of losses to learn representations that disentangle content from style. Separating style and content with bilinear models. Weaknesses:* This paper would be stronger if it dispensed with the incorrect formalism introduced in Section 3, and introduced the training objective as a set of standard loss functions as in Eq.4.* If this paper is about methods for combining embeddings for content/style recombination, I would hope to see a more thorough exploration of the different options. It seems like only two methods (concatenation and the proposed method) were tried. * The proposed method seems to be an ad hoc combination of losses already introduced in the literature. A few detailed questions follow:** Section 3.1: Is Q conditioned on s as well as c?<BRK>The paper addresses the problem of unsupervised content style disentanglement. Pros:The paper is well written and clear, providing a clear formulation of the problem and the proposed architecture. The experiments demonstrate some improvement on state of the art. Overall the experimental evaluation is extensive and show some improvement over a number of baselines as well as a new application in 3D generation. Image2StyleGAN [1] and StyleGANv2 [2] are also able to disentangle content and style. Some concerns regarding the experiments: In table 2, the results of disentanglement are worse that that FactorVAE.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. <BRK>This paper describes an improved way to determine weight quantization bit lengths using reinforcement learning, by injecting model evaluation directly into action selection. Randomly, or in such a way that the initial outputs correspond to different discrete bit sizes?<BRK>As the paper title suggests, the proposed augmentation is “simple” yet practically brings non trivial performance improvement. I am not a real expert on RL. There are a few ad hoc treatments in the method, such as memorizing the accuracy of specific bit value at a layer. The proposed idea is intuitive and well validated by experiments.<BRK> SummaryThis paper studies the DNN quantization using deep reinforcement learning. The paper proposes an augmented DRL which introduces a Q value indicator to refine action selection. In addition, compared to previous methods, the learning speed has been improved by 4.5 64x. It is very clear even for an audience who may not be an expert on DNN quantization. 2.The idea is simple and reasonable to introduce an additional Q value indicator to refine action selection. Given the improvement in performance,  despite the simplicity, the method does provide great performance.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 5. <BRK>(+)+ This paper unifies the above tasks by a tensor formulation and encodes the topology inside the supernet by a tensor network. (+)+ The paper further proposes an efficient algorithm that admits both stochastic and deterministic objectives to solve the search problem. (+)  After reading the related work section, we notice that the authors have known some methods used in the NAS problem, but they are not aware of the current NAS s ineffectiveness problem. I think a NAS method s effectiveness should be sufficiently investigated before NAS can be generalized to a new domain. I think the main idea of this paper is simple. ( )Overall, this paper proposes a method that generalizes the idea from NAS to general tasks.<BRK>Summary:This paper proposes a new and general formulation for supernet, which encodes supernet with tensor network(TN). In this way, the effective of the proposed method can be validated fully. Besides, this paper proposes a corresponding algorithm to solve the search problem.<BRK>The idea of employing a tensor method to generalize NAS seems to be interesting. There are some confusion:  This paper has not clearly described the difference with previous works. It is highly suggested that adding more details about the transforming process. 2.The experimental results are not good enough.<BRK>+) This paper is clear and easy to follow. +) The topic is interesting. It is charming to design a universal method that could solve a number of problems. The notations are not clear. However, I think this is important for the reader to understand the algorithm. ) , what is the number of parameters used for modeling the architectures?  ) For the cell based NAS in Table.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 6. <BRK>One the other hand, there are two main issues that prevent it from being considered in this conference venue. The first concerns the sparse result. With only one 3D shape being presented, it is difficult to understand the capacity of the proposed method; second, no or very weak machine learning and related technical contribution exist in the paper. Overall it seems unfit to ICLR audience. There are a few other comments that I hope would be useful for the authors:1. lack of technical contributions. The could dramatically change the feel of the current work. 2. more empirical studies. The current dataset seems to consist of only one single tree.<BRK>Structure from Motion is then used to recover the larger structures in the tree from these images. 4) I find the name of the "Experiments" section a bit misleading as it mostly discusses parts of the method and does not show much experimental results. Thus, the paper starts from a general purpose method (Structure from Motion) and adds processing steps designed to recover detailed tree branches. I thus recommend to reject the paper. Ideally, the paper would provide run times for the individual stages to show the importance of this task. Unfortunately, the paper does not provide an ablation study here. Can it be readily be applied to other types of trees? Unfortunately, the paper only provides results for a single tree of a single species.<BRK>This paper tackles the problem of geometrical and topological 3D reconstruction of a (botanical) tree using a drone mounted stereo vision system and deep learning based/aided tree branch image annotation procedures. These orientation filters banks have a  Gabor like steerable filter shape. The method is tested on a limited number of real images, and impressive results are obtained. standard reconstruction pipeline, easy to follow. The paper feels more like an experimental report suitable for ICRA/IROS, and less of a scientific paper in the field of Representation Learning or ICLR. It seems only one instance of test is reported/presented in the paper and in the supp.<BRK>The authors describe a processing pipeline for producing 3D models of trees from stereo visual data, which is later extended with a DNN for segmentation. It might suffice, although the authors should be strongly encouraged to find some kind of experiment that could have been done to validate the results more. For example, there could be objective assessments of the models to see if they fit some kind of physical prediction about the trees shape and laws of physics. How do I know these models produced are any good, because I looked at the paper and found them nice? Avoid making a mystery of what is the work done, on the contrary, repeat many times what has been done, completely, in different levels of detail. It s only marginal, though, because the papers still feels a little incomplete an perhaps lacks novelty. How do we know it was achieved? Perhaps cite other examples of research where this has been done, one example that comes to mind is https://arxiv.org/pdf/1801.10130.pdfIt seems the labels were only 2D, always, although 3D information was used to greatly help in the process.
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 6. rating score: 6. <BRK>The authors propose a MARL solution based on the idea of "Local Option Advisor" and "Option based Policy Transfer". Weaknesses  The significance of the proposed solution is unclear given the fact that the additional complexity of the approach proposed by the authors is not fully justified. Moreover, the method is evaluated experimentally, but its actual superiority is unclear. The aspects related to transfer learning are not fully evaluated in the paper.<BRK>This paper proposed an option based framework for multiple agents to share knowledge with each other in the same MARL task. Pros:1.This work models the policy transfer among agents as an option learning problem. How many options are available for each agent for the tasks in the experiment? It is unclear why the authors make this statement. After the rebuttal  After rebuttal, I think the author have addressed most my questions. Given the novelty of the idea and the current status of paper, I maintain my score.<BRK> Post rebuttal I am improving my grade a bit. I recommend the authors to dedicate some time further improving the paper clarity, especially in the matters related to my review and the other reviewers  The authors propose a transfer learning framework for transferring knowledge in multiagent tasks. Their method consists of learning a centralized option based advisor, that will extract advice to provide to all agents in the system based on the options learned. Section 3 should explain only new concepts and equations, making it clear what is a new proposal and what has been proposed before (that ideally should be described in the background section).<BRK>This paper considers the multi agent Reinforcement Learning setting, and proposes a scalable method for the agents to help each other learn, by transferring their policies to each other. Several variants of the proposed approach are introduced, depending on how much information the agents can communicate in a particular setting. This paper is very interesting, as it proposes a method that combines two domains of Reinforcement Learning, namely Multi Agent systems and Options, that are not often combined.<BRK>##########################################################################Summary:The paper proposes a new option based policy transfer framework for multi agent reinforcement learning (MARL) called MAOPT. By framing multi agent transfer as an option learning problem, MAOPT methods are able to learn when to give advice to agents and when to stop it. My concerns regarding the paper are mainly about the experiments. Hopefully, these would be addressed during the rebuttal period. The proposed option based method offers a new prospective for policy transfer in MARL. It is interesting to see how the methods scale with the increase of the number of agents.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 7. <BRK>This paper theoretically analyzes the effect of bias in augmented data on the efficacy of data augmentation approaches, and proposes three new approaches to leveraging data augmentation in ways that purport to limit the negative effects of data bias in the augmented data distribution, and both theoretically and experimentally analyze them. The algorithmic approaches and analyses are interesting, but (A) the theoretical results don t deliver a lot of specific actionable insight beyond the high level intuition they start with, and (B) the experimental section is somewhat weak. Re (A): There is no detailed experimental analysis beyond comparing end performance of models trained with the proposed approaches. Again to be clear: this reviewer is *not* saying that all papers need to compare with SOTA.<BRK>##########################################################################Summary:In this work, the authors first prove a deep model can benefit from augmented data when the data bias is small. If I understand it correctly, the realisation of "AugDrop" is to omit data augmentation and train the model on original data, and the realisation of "MixLoss" is to train the model on both augmented and original data. 2.The proposed method seems to be very easy to implement. Yet another concern is that, the method was tested on CIFAR 10 and CIFAR 100, with "mixup" as the only competitor.<BRK>1.The paper has some typos and grammar mistakes. ", "it is sufficiently to optimize Lc(w)"2. In reality, there might be strong bias in the label preserving augmentations, i.e.Jigsaw.I would like to get a clarification on this. There exists better data augmentation techniques, i.e.auto augment, yielding better results on CIFAR10/100. 6.Similar to my point at 3, do the authors think that their method would work on an another augmentation method, i.e.jigsaw, that preserves the label? 7.What are the disadvantages of the proposed methods?<BRK>This paper illustrated two algorithms that correct bias in data augmentation: AugDrop and MixLoss. The paper tackled the common problem on how to train with augmented data. The concern I have is that if their method could be applied to a large range of applications since their experiment only based on some typical benchmarks, which already shows a good result in this field. But generally, this paper is interesting and acceptable.
Reject. rating score: 3. rating score: 4. rating score: 7. rating score: 7. <BRK>The paper proposes Latent Programmer, an approach that employs an adapted VQ VAE to predict a sequence of latent codes, and then generates the output program conditioned on those codes. ### Strengths ###+ The proposed approach is a first application of VQ VAE to program synthesis. was not answered by the authors. If the authors claim that their approach allows "high level planning", I would expect to see that it works across different models / tasks / datasets / settings. For example, the first few pages contain obscure phrases like "discrete latent codes", "our sketches are comprised of a general latent vocabulary", "the semantics of the latent codes", "stop gradient operator", "sequence of tokens in latent space", "general vocabulary of discrete latent variables". For example, can the VQ VAE be simply VAE or AE? Further, is the focus the VQ VAE really needed, or can a standard VAE work as well (or even an AE)? I think that maybe what the two steps of (1) generating the codes and then (2) generating the program given the codes do is provide more *diversity* of solutions, rather than better search. **VQ VAE**   before reading the paper, I was not familiar with VQ VAE. In this paper, I do not see such strong evidence. Post discussion comments  After reading all reviews, responses, and discussions, I still do not see evidence that the model has learned a "high level plan", which is the main claim of the paper. I agree that most deep learning models are not interpretable, but most papers do provide some qualitative/anecdotal/generality/strong empirical evidence to support their claims. However, it seems that there are not enough baselines to put the results in context.<BRK>Experimental results show that the model outperforms three baseline systems on two tasks. I have major concerns on the nature of the VQ VAE model. The BLEU is computed by the best BLEU score among the output beams, but increasing the beam size may not improve top beam performance. 7) Baseline models are inadequate. However, this paper has very light citation on Kaiser et al.(2018).The authors should be more honest about previous work and make direct comparison on the difference. There has been other efforts on search based program synthesis, for example, Balog et al.(2017, 2020). I d like to see a comparison, and what s the further improvement when they are combined (as claimed by this paper)? 2) The author claims that VQ VAE serves as a discrete bottleneck. However, I strongly disagree with this. In short, the discrete latent space beam search appears to be some interesting idea. There lacks comparison and discussion. 4) The latent codes are generated in an autoregressive fashion. Did you include an EOS token for such autoregressive generation? 5) There is no quantitative analysis on the learned code.<BRK>Edit: I have increased my score to 7. Strengths: The paper is relatively clear. The approach is novel. Weaknesses:I think the baselines/ablations could be more complete. For example, it seems that the gains over the RobustFill baselines could be due to any of 3 factors: 1) use of discrete representations 2) the use of an autoencoding loss, or 3) the ability to search through latent representations at test time. Unless I m mistaken, compared to LP, the transformer RobustFill baseline differs in terms of both (1) and (2): RobustFill does not use discrete latent codes, and it does not use the autoencoding or latent prediction losses. Similarly, how might a continuous latent variable model, such as a VAE, perform on the string editing tasks? I think that disentangling these factors would really strengthen the paper, and could also be of large value to the neural program synthesis community. Summary:I think this is an interesting line of work with promising results. However, I do think that a baseline which uses an autoencoding loss but does not use discrete latent codes is an important ablation to perform. I therefore recommend a weak accept, and I d be willing to raise my score if my concerns about baselines were addressed.<BRK>Summary: This paper proposes a two level hierarchical program synthesizer, Latent Programmer, which first predicts a sequence of latent codes from given input output examples, and then decodes the latent codes into a program. Latent Programer significantly outperforms RobustFill on string manipulation tasks and achieves state of the art results on Python code generation tasks. Quality: The paper presents a novel program synthesis idea and the evaluation is promising and convincing. Clarity: The writing provides enough background and explains the main idea in a very clear manner. Significance: This work shows that a promising hierarchical learning approach for program synthesis. Its effectiveness motivates many future explorations in this direction. Questions: Q1: Why Python Code generation tasks use BLEU as the metric, rather than functional correctness? Q2: Latent codes are motivated as a "high level plan"? The authors do show that varying the length of latent codes could affect performance.
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 7. <BRK>This is reported but the paper doesn t have a section detailing the experiments or showing how this number is derived. The paper could be improved by refining the contributions and detailing what evidence should be observed to support these claims and then providing a significant amount of evidence. The reported difference between the methods is 48.6 vs 48.4.<BRK>After reading this paper and [Ref. 4.The authors conduct many ablation studies to validate the different variants of the proposed methods.<BRK>By detailly formulate the masking based saliency methods and sufficient experimentation, the paper gives a simple formulation and practical training strategies.<BRK>It takes what is considered as the popular choices in generating classifier saliency masks, and conducts quite extensive sets of experiments to dissect the components by their importance.
Reject. rating score: 3. rating score: 4. rating score: 5. <BRK>Summary:This submission works on the neural machine translation problem. The authors propose two alternatives to incorporate prior knowledge, which are the word frequency information for the monolingual data and the prior translation lexicon information for the bilingual data. This work extends the previous work on how to leverage the information from the statistical data knowledge into the NMT model. Overall speaking, there is no big tech flaws or incorrect points, but this paper is not suitable for ICLR conference, better for NLP related conferences. The authors claim that previous works can not explore other prior knowledge in a universal way. Therefore, the contributions of the submission are not important or enough. By the way, in the bilingual lexicon, $L$ is leveraged, but not show in $M_T$, this also makes it to be unclear.<BRK>This paper presents a method for introducing prior knowledge into Transformer models. The details of the proposed approach are not very clear. It seems to me that such prior knowledge can be incorporated into the standard Transformer model in the same manner as positional encoding or expanded input embeddings. The manuscript contains many grammatical errors and is not ready for publication. on WMT14  > on the WMT14?<BRK>This paper proposes a method to introduce **prior knowledge** into Transformer based sentence encoders, here in the context of neural machine translation (NMT). 3.It would also be interesting to qualitatively show some examples where the model outputs differ between the baseline and the proposed approach. Some of the grammatical errors are detailed below. The fact that NMT systems do worse on longer sentences has also been established before in prior work, and is thus not particularly insightful.
Reject. rating score: 3. rating score: 3. rating score: 4. rating score: 4. <BRK>Pros:1.The idea of capturing frequency domain information for collaborative filtering seems interesting to me, although I did not quite understand how this information is obtained in this work. 4.The experimental results are not encouraging. The authors mentioned that their code has been published but no link was provided in the paper. However, there are a lot of recent works building upon state of the art deep learning techniques as far as I know.<BRK>This paper proposes an approach based on Fourier transforms to predict ratings in collaborative filtering problems. How are the unobserved/missing ratings of user u treated in the proposed approach? The key point I am concerned about is that the observed ratings are missing not at random [a].<BRK>The paper has proposed a smooth reconstruction of the preference function in collaborative filtering for recommender system. However, there are several concerns left. 2, The experiments mostly look good. 4, Some procedure of the proposed methods are not intuitive, e.g.In Alg 2, why take log2|C| as the total size of k representations? e.g.page 4,  in the section ??<BRK>However, there are still some technical issues in this article, such as unconvincing experiments and confusing descriptions. As a result, I suggest the paper should be rejected. The key strengths:  The perspective of reconstructing the user preference functions is novel. In the introduction, the authors pointed out that the contemporary deep network in RS is merely limited to shallow networks and they addressed this problem in this work. In addition, the authors did not dive into these results and analyze them. some typos:    “in the Section ??
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 5. <BRK>This paper proposes to use Copula function to model the correlational role in the node level prediction tasks. From my own perspective, I quite enjoy this work. Readers would get a clear understanding on the background information. [Copula Graph Neural Networks]The authors introduce the detail architecture design for the model. What if the precision matrix is without the graph structure and is simply placed with sparse priors? Thus, can these parameterisation method also be used for other copula functions? [Experiments]The experimental part can be sufficient to verify the effectiveness of the authors  proposed model.<BRK>In a nutshell, the idea is to propose a probabilistic model for the joint node level output whose mean can be parametrized as a GNN and whose correlation is determined by a precision matrix with the same support as the graph at hand. Pros:1   The synthetic example in Section 3.2 nicely illustrates the shortcoming of some existing methods. 2   The generic framework is applicable on top of existing GNNs architectures. For example, the “Model Inference” paragraph is key for the implementation of the proposed method. Section 4.1 is too short in the sense that it would be difficult for someone that does not know what a copula is beforehand to actually pick up the necessary notions from there. Please make more emphasis on this.<BRK>The proposed method is nice because, unlike most existing methods, the CopulaGNN enables us to model both the representational and correlational information. Because of this benefit, the proposed CopulaGNN can also be used as a flexible model. However the paper considers only the Gaussian copula. (2) Considering that the GNN has a graph structure, one possible choice is to use the vine copula instead of the Gaussian copula as a copula for the GNN. My major concerns were related to the comments (1) and (2) in Cons, but it is now clear why the authors consider only the Gaussian copula in the paper.<BRK>The idea of introducing copula into GNN is novel and interesting, and this paper is basically well written and well organized. However, I have the following concerns:1. Characterizing representational role and correlational role is the key motivation of the proposed method, but I m not quite clear about their definitions. I think a better way to decompose the model is to say that, the label of a node depends on not only its feature but also the graph topology. 2.The authors decompose the joint distribution of node labels into marginal distributions of node labels as well as a copula density. This is also connected to the above argument that the dependence of node label can be decomposed into its feature and graph topology. Only in this way we can say that we "decompose" the joint distribution of node labels. Although the authors have tried their best to reduce the parameters of the model, it is still prone to overfitting if you maximize the likelihood only on one sample. In addition, as the authors said, multiple sampling rounds may be required to calculate the average.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>In this paper, authors have proposed to use Wasserstein gradient flows to update samples from deep generative models (DGM) to closer to the empirical data distribution which is expected to produce better generated samples. The Wasserstein gradient flows are induced from the entropy regularized f divergence functional between data the empirical data distribution and the to be learned distribution. Moreover, in the experimental sections, there is no result which refines "directly in the data space". In the experimental section:     In Table 2, there is no result of FID for the baseline DDLS. by using auxiliary GDMs as with DGflow? If yes, what are their results in Table 4?<BRK># SummaryThe paper introduces Wasserstein gradient flows for refining the samples of generative models. The authors observe the ratio between the generated distribution and the target distribution is provided by the discriminator and so the dynamic can be simulated via the stochastic Euler scheme. Instead of refining the distribution in the data space the authors suggest and show how to do it in the latent space. In addition, they expand their approach for non GAN generative models, this setting requires to train (or just finetune)  another discriminator. They experimentally show that the proposed method improves the quality of the generated samples for a large panel of experimental settings. 2) I would have liked to see the gradient flow in the 2D space fig 1.<BRK>Summary:  In this paper, the authors present a technique for refining the output of generative models by using gradients from discriminator to update the latent variables from which the generator produces a sample. The technique is more versatile than previous work since it can be applied to vector valued discriminators, and does not restrict itself to scalar discriminators. I think the author’s paper would gain more traction in the community if in the introduction they could come up with some “eye candy” showing how the method allows the gradient to flow all the way from a discriminator to the latent variable, resulting in a better good generated sample.<BRK>Pros:* The proposed framework seems principled and practical. Propagating generated samples to follow the data distribution by simulating the gradient flow of the f divergence to the data distribution is reasonable, and the required quantity for simulation, i.e.the density ratio between the sample and data distributions, is readily given by the discriminator in GAN training. Experiment shows promising results. But the curves cannot be described using tangent vectors $x (t)$ and gradients, as presented in Eq.(1).Eq.(3) is specific to the 2 Wasserstein space. Although asymptotically the method guarantees that the sample distribution will converge to the data distribution, for each sample it may not converge and may traverse non stop in the support of the data distribution.<BRK>This method can be extended to other generative models such as VAE by training a density ratio corrector, similar to a discriminator. However, I have major concerns regarding the actual algorithm and the evaluation. More specifically how are DDLS and DGflow (KL) different in the experiments? With this discrepancy, it is difficult to evaluate the improvement from the proposed method. The problem with uniform prior, e.g., can be handled by projected gradient descent. Consider citing [2], which achieves the state of the art GAN scores from incorporating refining latent in training. Wu et al.2019
Reject. rating score: 3. rating score: 7. rating score: 7. <BRK>This paper investigates if neural network parameters, trained for a standard task such as image recognition, can be used as a cover medium in steganography. Overall, I thought the paper was generally well written and contains some interesting ideas, but these positives are also accompanied by an unclear motivation, lack of positioning with respect to related work, and poor experimental evaluation. My primary concern is the practical motivation for this idea. I wonder how the authors view the practicality of this work? It would have been great to compare this scheme with some standard steganography schemes on images or audio across desideratum such as bandwidth, robustness etc. The authors may not be aware, but there has been work on information hiding in previous work.<BRK>This paper proposed a method to hide information in the parameters of neural network models. Overall the paper is clearly written and the proposed method is well supported by the experiments. It s not clear to me what the criteria is for partitioning the parameters. Is it purely based on sensitivity analysis? Can the standard deviation be added to the results as well?<BRK>This paper highlights and studies the interesting possibility of hiding information within neural network weights, which is a form of steganography. It is shown that it is possible to hide information in the weights of a number of standard baseline neural networks without being easily detectable. Probably should replace  their usefulness  with  the usefulness of DNNs      About two thirds of the way through the first paragraph in Section 4.4, in the sentence beginning  In particular, ... ;  fraction bits an...  that  an  should be an  and . As far as I can see, the sizes of the weights are only mentioned in passing once (on the first page of the paper), where they are said to be  in the hundreds of megabytes  (an understatement these days), and the paper does not say how much information they were able to hide in weights.
Reject. rating score: 4. rating score: 6. rating score: 8. <BRK>This paper proposes a theorem prover based on Proximal Policy Optimization for the connection tableau calculus. This prover is applied on five domain specific datasets, where theorems are relatively simple but their proofs are long and repetitive. I think the assumption of this paper is correct that we need more accurate heuristics but not more searches to find longer proofs. The main issue is that it is unclear what is the novelty of the proposed approach. It seems that the main approach is to train the theorem prover by reinforcement learning following a specific learning curriculum. I think the proposed approach shares the same form of heuristic as the prior work on neural theorem proving, that building a reactive policy to select the next action based on the current proof state. I am not convinced that the proposed approach is a simple form of analogy reasoning.<BRK>Summary:This work introduces a method for learning to prove theorems which can leverage prior proving experience in order to discover very long proofs. IMO this paper should be accepted iff this approach is a new advance for automated theorem proving, but should not be accepted on the basis of it s connection to reasoning by analogy. However, it is possible that I have misunderstood how the algorithm works, and would like to be corrected by the authors if indeed the algorithm works as advertised. However, it does not seem like a large advance for deep learning or for combinatorial search. Pros:+ seems to scale to some extraordinarily long proofs!<BRK>This work introduces a new algorithm FLoP for theorem proving using reinforcement learning, and tests it on a new evaluation dataset. The authors state that this RL technique has not been previously applied to theorem proving. They find that the technique works best on highly structured problems such as proving simple arithmetic statements in unary or binary arithmetic, and less well for problems which benefit from searching through databases of heterogeneous statements. This does not sound like analogy as I defined it, rather it sounds like imitating the prototype. Still, since the comparison with other techniques is encouraging, and since the paper is clearly written and gives a very extensive survey of comparable works, I found it enlightening and would recommend to accept it.
Reject. rating score: 4. rating score: 5. rating score: 6. <BRK>The vast majority of machine learning methods that have been developed for molecules use either 1D or 2D information. Therefore, this resource and the empirical demonstration that using 3D information can improve performance can be very valuable to the community. I am assuming this has not been incorporated to the paper for anonymization reasons, but could the authors confirm that they are indeed planning to make both the data sets and the code used to produce the results (in particular, the three proposed neural networks architecture) available? 2.The paper does not discuss the nature of the 3D information further than "By representing a molecule s atoms and their 3D positions". However, molecules do not have a fixed 3D structure, but rather multiple conformations driven in particular by rotatable bonds. and how does this affect both algorithms (if several conformations are used) and prediction performance? Actually, the abstract (and, more generally, the paper) reads as if neural networks were the only kind of machine learning algorithms that could be applied to molecules and that very little work has been done in the past to incorporate 3D information in chemoinformatics. While it is true that most current techniques rely mostly on 2D (for small molecules) or 1D (for large molecules) representations, it is not for lack of trying to incorporate 3D information, but because 1) this information was either lacking or incomplete (in the sense that a single conformation gives somewhat limited information; for example, you may have the crystal structure of a protein, but that doesn t inform you directly on the pose of its pocket when binding a specific small molecule) and 2) earlier attempts at making use of 3D information have often found that it did not improve performance (see Swamidass et al.(2005) or Azencott et al.(2007)), either because of the aforementioned incompleteness or because the methods were not up to par. I am listing a few examples of such papers below, not because I think they should all be included in this paper, but because in my opinion the paper would benefit from considering this body of work. Mahé, P., et al."Graph kernels for molecular structure− activity relationship analysis with support vector machines."<BRK>This paper is concerned with 3D molecule learning. They propose a collection of existing and new datasets (curated from existing datasets). They show that a lot of existing tasks can do well when 3D structure is considered. The main contribution of the paper is in curating the datasets into a well defined framework with consistent splits and evaluation metrics. This would allow the community members to easily benchmark their approaches against a variety of tasks. I believe this benchmark dataset will be useful to the community. I have some complaints with the experimental evaluations. For SMPs, a variety of unsupervised feature extractions methods have been proposed, which can be applied on 1D (SMILES representations) as well as graphs. It is not clear. Similarly, for binding affinity prediction, the authors compare with DeepDTA. Are 3D models better than this?<BRK>A systematic benchmark with atomistic learning methods is presented, showcasing the value of using 3D atom level data instead of 1D or 2D features. Strong points of the paper  The authors present a systematic evaluation of atomistic learning across multiple tasks and show that 3D data consistently yields better performance than 1D and 2D methods. End to end learning on 3d protein structure for interface prediction. This is a strong point for this paper. Because of this, I feel that there is not much novelty with atomistic learning. (2018).DynaMut: predicting the impact of mutations on protein conformation, flexibility and stability. My overall recommendation is a weak accept. By creating a standardized set of prediction tasks and associated data sets, the authors have presented a resource that may help the community to compare 3D atomistic methods quickly and fairly. Townshend et al., which as stated in the paper is a motivation for the approach, frames the problem as Protein Interface Prediction. If what the authors want is to show that 3D features outperform 2D or 1D for these molecular tasks, then a systematic evaluation of the same methods but with different types of features would be more significant. For instance 3D CNNs vs 2D CNNs (where possible).
Reject. rating score: 5. rating score: 5. rating score: 6. <BRK>Post rebuttal comments:Thanks to the authors for the helpful comments   they indeed help clarify some of the confusions. 4  What was the accuracy/generalizability of this $\delta$ in the experiments? These feature attributions can be used to explain the unfairness of the model. While the paper tackles an interesting and timely problem, it lacks clarity at several important points.<BRK>I think some of this notation could be cleared up a bit. There’s a number of points which could warrant further clarification right now in the paper. The idea of using Shapley values to provide feature attributions for fairness is interesting — particularly the applications for explaining the differences between two models, one of which has been corrected for fairness.<BRK>I think a careful discussion on how to use the insights from the shapley values in practice, or some open questions regarding the interpretation of the shapley values would improve this paper. What is the support of $a$, $y$ and $f(x)$? Originality  There has not been a lot of work on fairness and explainability.
Reject. rating score: 2. rating score: 4. rating score: 4. rating score: 4. <BRK>The discussion of these figures does not lend insight into what type of problems that are challenging for previous methods are addressed by the proposed approach. Finally, the datasets used with the exception of MNIST are not standard ones used in recent semi supervised learning papers. The paper claims previous works do not consider the spatial information provided by unlabeled examples. This is not correct.<BRK>It makes sense and so I believe that the method proposed is correct conceptually. That is what the paper lacks. Second, the comparison studies reported in Fig.4 are not clearly documented and possibly are not fair either. If a different number of the labeled samples is used, it is not a fair comparison either. As a minor comment, I don’t like the acronym MCMC for this proposed method. The authors may want to change to another name.<BRK>The authors employ  ADAM with a learning rate of 0.001 with exponential decay to optimize the novel loss function. In that view, I find the conducted approach very interesting. The loss function is not continuous and in practice, this makes a loss susceptible to local minima. How does that affect the convergence rate of the method on larger datasets? Z are described as the network parameters, but what that entails is unclear.<BRK>[Summary] In this work, authors dealt with the problem of improving classification performance in the setting of semi supervised learning. Authors exploit inherent knowledge on the samples distribution via clustering to improve the latent space. The results are ultimately less informative than one would like and the experimental setting is limited. For example, it is well known that the Davies Bouldin index can report good results, however, it does not imply the best information retrieval. Why then chose Davies Bouldin index?
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>This is paper is an investigation of the potential biological interpretation of attention in five pretrained protein sequence Transformer models (one from TAPE and four from ProtTrans). The paper repeatedly plots the background frequency of contacts between residues in the protein data but that s not the same as determining how often your analysis would yield high values just by chance. I don t know what the null model *should* be but this paper should endeavor to show that finding one calibrated attention head (among the many possibilities) is surprising. If understanding substitution matrices is important to the paper, give them their own bullet point. Otherwise it s a strangely specific technique to discuss while describing amino acids in general. I guess it makes sense that these categories might occur in some secondary structure prediction datasets or tasks, but it seems like the details of some dataset are being presented as representative of biology. It s really nice to actually show it and also show that attention is compatible with other simpler biological notions (like AA substitution matrices).<BRK>### SummaryThis paper offers an in depth analysis of attention in large scale language models including (AL)BERT and XLNet. UPDATE: The authors have removed the attention calibration study. This is a good paper and I heartily recommend its acceptance. very good thank you! Even with my current understanding of the section, I m extremely skeptical about this specific study. Suppose we had a very tightly clustered protein and a high enough radius such that all residues were considered to be in contact. No possible attention map would be well calibrated to the ground truth contacts. We at ICLR care not only about "learning representations", but about assessing what precisely those representations have learned, so the paper is certainly appropriate for ICLR.<BRK>The paper is missing two important baselines: 1) a Transformer with randomly initialized weights and 2) a Transformer trained on randomly shuffled protein sequences (with the same amino acid frequencies.These baselines will show if Transformers actually learn protein features, or if the correlation between network features and protein features is an artifact of the Transformer architecture. 3.The attention analysis (equation 1) depends on a threshold \theta. How was it chosen and how sensitive are results to this threshold? I also doubt the usefulness of analyzing calibration of attention since practitioners are unlikely to use a particular attention map directly for contact prediction, but train a ML model on top of attention maps to predict contacts (see Rives 2020). What matters is if the resulting ML model is calibrated. 7.Section 3, datasets: How similar are test sequences to sequences that were used for training Transformer models? Sequences must not overlap and must have a maximum similarity of, e.g., 80%. 11.Section 4.1; Figure 2: ‘alignment’ can be misunderstood as ‘sequence alignment’ in the context of protein.<BRK>The authors of this manuscript conducted a comprehensive analysis of the interpretability of self attention language models when learning from protein sequences. Specifically, five multi head self attention models from NLP were used to model protein sequences. In general, I think it is a good analysis paper. This paper can be enhanced if consistency and difference between these five attentions models can be further analyzed in the four tasks. What would be the most suitable model for protein sequences? What will happen if 50 or just 5 layers are used for the analysis in this paper? Will the same conclusions be drawn?<BRK>General comments:1) authors claim"We note that all of the above analyses are purely associative and do not attempt to establish a causal link between attention and model behavior (Vig et al., 2020; Grimsley et al., 2020), nor to explain model predictions”. and in the related work section"In our work, we take an interpretability first perspective to focus on the internal model representations, specifically attention and intermediate hidden states, across multiple protein language models. We also explore novel biological properties including binding sites and post translational modifications.”However the structure of evidence is very phenomenological in nature — x (attention at head H, layer L) is observed which coincides with phenomena Y (binding/PTM/). 2) no cross model analysis. Attention as a well Calibrated predictor section. what is it about those amino acids?
Reject. rating score: 3. rating score: 5. rating score: 7. rating score: 7. <BRK>Review: This paper proposes SplitSGD, a novel heuristic for adapting the learning rate. When a stationary phase is detected, the learning rate is decreased of a factor $\gamma$. SplitSGD is then benchmarked and compared against some of the standard learning rate decaying heuristics generally adopted in combination with SGD or other first order stochastic methods. ___________________________________________________________________________________________________________________ + Overall the paper is well written and clear. ___________________________________________________________________________________________________________________ Concerns: 1. The learning rate, also called step size, can be interpreted as a brutal approximation of the local curvature with a scaled identity matrix. All the work on second order methods attempts to refine this brutal approximation with better estimates of the local curvature. Despite being the learning rate and how to adjust it the central topic of this paper, nothing is mentioned regarding thefundamental relation of the learning rate and the local curvature. Specifically, in the case of a relatively small learning rate, we can imagine that, if the number of iterations is fixed, the SGD updates are not too far from the starting point, so the stationary phase has not been reached yet. """ 3.The authors claim to propose an optimization method while what they are proposing is an heuristic to decrease the learning rate in an adaptive fashion. 4.The costs of the stationary phase check are not discussed but the authors just briefly mention that SplitSGD comes with no significant extra costs. This does not seem to be the case though. 5.The literature on increasing the batch size toward the final part of training is not discussed at all, as well as other state of the art heuristics to deal with the noisy gradient estimate. In addition, they could show more solid empirical results by letting an hyperparameter optimizer such as BOHB choose for the best learning schedule. Otherwise it is hard to say that the superior performance of SplitSGD is not a consequence of a wrong hyperparameter tuning of the competitors. 7.I think the deep learning community at this stage needs less work on learning rate heuristics and more attentions on the theoretical analysis of the geometry of the landscape, novel optimization methods with theoretical guarantees and solid work on generalization properties, as there has been enough works on such heuristics which often ends up  adding extra noise  and therefore constitute an obstacle in the process of bringing clarity and understanding in the field.<BRK>This paper proposes a sign based test to determine if a stochastic process is in its stationary state or not. It divides each trajectory into w parts and averages the gradients insides each part. Then measure the similarity of each average of one trajectory to the corresponding average from the other trajectory via dot product and then counting the negative signs and positive signs. Then it shrinks the step size of SGD for future iterations. Comments: 1  Several parts of the paper needs more clarification and explanations: It is not clear what the statistical intuition behind the splitting test is and why we should expect a more robust test. The notion of informativeness is mentioned in the introduction but it is not used in the rest of the paper. Fig 3 just represent eq 3 and gives no extra information. If it means something else such as the inner product for nonstationary is positive with high probability it should be mentioned clearly  In related work: 1  Le Roux et. Why do you mention it? In assumption 4, there is a parameter m which is not clear what it is or there is a limit on it, for example, can m be infinity?. In Thm 3.1. It would be nice to mention how small the \eta should be to guarantee the ineq.in this theorem. For both Thm’s 3.1 and 3.2 it would be helpful to add an interpretation for their result. 2  For the empirical results I think you should compare against  “Lang, H., Zhang, P., and Xiao, L. (2019). Using statistics to automate stochastic optimization” instead of FDR since FDR test has high variance. On convergence diagnostic based step sizes for stochastic gradient descent” which is a distance based test instead of sign based. Fig 4 shows that your test does not work when the step size is too small. However, all your theories hold when the step size is small. For your DNN results, are this results average of several runs?<BRK>**Summary**:The paper focuses on estimating when stochastic gradient dynamics have reached a stationary phase by considering the inner product of pairwise stochastic trajectories referred to as threads. The chosen approach avoids strongly correlated estimates which leads to better mixing and more reliable identification of a stationary phase than previous work for certain problems. The proposed algorithm is specifically adapted for Deep Learning problems and performs well on the presented synthetic and real world problems. These are all constituents of a great paper that I also think provides a valid contribution to the field. **Pros**:  The paper is well written and makes good use of visual media to convey results and intuition. A drawback of previous methods for stationarity detection is the (potentially) correlated samples obtained from successive gradient steps. This did not seem to be a problem in the DL case presented in the experiments but would be for more general problems. **Questions**:  There is also one question regarding the implementation of SplitSGD for the Deep Learning experiments that I might have missed. Is it the case that each epoch of SplitSGD requires 2 passes through the training set or is it alternating between 1 epoch standard training and 1 epoch diagnostic? SplitSGD is generalized to use different optimizers such as SGD with momentum and Adam and I see no reason why other first order methods could not be incorporated as well. If we assume $P_t P$ to be constant during the diagnostic we end up with average gradients of $P\bar{g}_i$ and $Q_i <P\bar{g}_i^{(1)},P\bar{g}_i^{(2)}> <\bar{g}_i^{(1)},\bar{g}_i^{(2)}>_{P^2}$. Do you know how this affects the estimates $Q_i$ and whether it would be necessary to invert the preconditioning for such adaptive methods? How was this handled for SplitAdam? In theorem 3.1 and 3.2 and by extension figure 3 the effects of large learning rates and long time horizons for the diagnostic are compared. Let s assume the horizon $t_1$ is fixed to a "suitable" value. It then looks as if a more general adaptation scheme could be constructed from the condition in Eq.7. If the learning rate is too small, the red histogram of Figure 1 is expected  > $\Sigma$ would be small, which for the chosen $t_1$ has not lead to stationarity. Would it then be possible to slightly increase the learning rate to bring the next diagnostic closer to stationarity? An extreme case of which could be to replace lines 8 12 of the algorithm with:```if S:	decrease eta by factor gamma else:	increase eta by factor gamma/2```Did you consider such an adaptation or see any advantages/disadvantages of such a procedure?<BRK>The paper introduces SplitSGD method that detects the stationary phase in the stochastic optimization process and shrinks the learning rate. The SplitSGD is based on the observation that before reaching the stationary phase, two random batches of data will likely to have the gradient aligned as the noise between different batches is dominated by the shared gradient, whereas after reaching the stationary phase, two random batches should have misaligned gradient as the gradient has become mainly noise. This observation is intuitive, and some theoretical results show that (a) at the beginning, the algorithm determines non stationary with high probability, and (b) more important, the SplitSGD algorithm is guaranteed to converge with probability tending to 1. The experiment reveals the advantage of the SplitSGD method over alternative SGD algorithms for CNN, Resnet, and LSTM. The intuition is neat, and the new approach, as supported by both the theoretical and empirical results, should merit significant values to be widely known to other scholars. The paper has made enough contributions and has high clarity in terms of writing. Here are some concerns that I would suggest the author to consider:1. The theoretical results are of course important, however, the proven results could appear expected and not have surprise, despite the many technical challenges. The proven results are either for the initial steps, or for the final state on whether the algorithm converges. It would make the paper more convincing if the authors can add larger datasets for comparison of methods. 3.The proposed method has gains over a number of alternatives in the simulation. The gap between the new method and the other methods appears not really large. More comparison could be helpful, although I do not think it is fully critical. 4.For the simulation results, as far as I understand, the SplitSGD has better test metrics and results in less overfitting. It would be very helpful if the authors could provide an intuitive explanation of why this is the case. 5.Here is a typo: in Eq (5) and in line 21 of algorithm 1, $\theta^{(k)}_{i\cdot l}$ should have $i\cdot l +1$ rather than $i\cdot l$ in the subscript, to match the definition in Eq (4).
Accept (Spotlight). rating score: 9. rating score: 7. rating score: 7. rating score: 7. <BRK>RIMs exhibit excellent generalization on tasks in which the factors of variation differ systematically between the training and evaluation distributions. The paper is very well organized and well written, with relatively simple and consistently clear explanations of key concepts. The authors state that their central question is" how a gradient based deep learning approach can discover a representation of thigh level variable which favour forming independent but sparsely interacting recurrent mechanisms in order to benefit from the modularity and independent mechanisms assumption." The paper could be improved by more clearly explaining the unique benefits of a gradient based approach to this task. The experiments in section four are particularly thoughtful and well designed.<BRK>The authors argue that the world consists of largely independent causal mechanisms that sparsely interact. RIMs consist of largely independent recurrent modules that are sparsely activated and interact through soft attention. in these tasks. The manuscript is easy to follow, the idea is quite interesting, and the model is empirically tested across a wide diversity of tasks. However, since the authors focused most main figures on performance, it is worth better understanding the cause of that performance gain. In some experiments, RIMs are only compared against LSTMs, and it is not clear whether the gain over LSTMs is due to the use of attention.<BRK>This paper introduces a new architecture composed of semi independent recurrent networks that interact with each other, and with their environment, through attention. The authors present several experiment to demonstrate that this architecture outperforms simple LSTMs on various tasks. The method seems interesting, and the experiments indicate some benefit. Please indicate *total* number of neurons and trainable parameters for both LSTMs and RIMs in all experiments. Section D.4.1 and Figure 7 seem to contain important information about how the system works, but it is totally incomprehensible  e.g.how are the masks generated and moved around? If these explanations are provided (and if the comparisons with LSTMs are fair) I think the paper would be acceptable.<BRK>They mostly clear my concerns and doubt. This proposed recurrent independent mechanism (RIM) includes multi head attention, top k activation section, input attention, and communication modules. The experiments on a range of diverse tasks show that RIMs generalizes better in many tasks than LSTMs. Pros:+ The paper is clearly written. + The related works and the difference with the proposed model are explained in details. + The experiments cover a wide range of scenarios from copying task to reinforcement learning. However, this paper simply combines existing works and thus lacks novelty. I ask authors to clarify it if I missed anything.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK>This paper aims at merging Imitation Learning (IL) and Reinforcement Learning (RL) from high dimensional sensory inputs so that the agent can make use of expert trajectories, even when the experts are suboptimal. This paper aims at addressing this by theoretically using the "Free Energy Principle", which the authors define in the abstract as a unified brain theory that explains perception. The paper tackles an important problem and develops many theoretical functionals. My main concerns are related to the clarity of the paper, which does not allow me to understand some key parts.<BRK>This paper  introduces two different interpretations of free energy minimization as a form of behavior cloning and reinforcement learning. Weaknesses:I found that I was confused by the presentation of section 3.1. Isn t the posterior policy different than the policy prior, leading to the likelihood to the next state this is distinct from the prior probability of the next state?<BRK>This paper extends and explains how to apply the "free energy principle" and active inference to RL and imitation learning. The demonstrations may be suboptimal. It was difficult to evaluate this work well, as I found the approach difficult to follow. It appears that is a partially observed MDP (which makes the use of "observation" even more confusing). I think another view of this paper is that it is applying maximum entropy RL approach to a partially observed MDP, learning a demonstration policy, and then using this demonstration policy as a prior on an RL objective policy.<BRK>It introduces free energy framework that combines ideas of imitation learning and reinforcement learning in a Bayesian probabilistic way. It may be worth explaining q(s_{\tau}) in Eq.14, is it equal to the variational posterior  as an approximation of p(s_{t+1}|o_{t+1})  on page 2.? The expert data provides policy prior. Though it applies existing model, it would enhance the paper if it can show further analysis of the algorithm in this context, particularly from perspective of imitation learning.
Reject. rating score: 4. rating score: 6. rating score: 6. <BRK>SummaryThe manuscript proposes  a distance aware pooling method to use in  graph convolutional neural for predicting whether a subject is infected with Covid 19 (diagnosis) and progression of the disease (prognosis). The proposed model achieved 94.7% accuracy. Although the model trained on the dataset achieved a relatively low error rate (~ 5.3% ),  there is no evidence that this error rate is low enough for the model developed to be really useful, e,g. could actions taken based on a diagnosis method of this level of error rate potentially create a local epidemic? It is unclear how this pooling function is computed exactly. 2.Almost the same can be said about the section of “improved receptive field”. (incidentally, it seems that how authors do not mention in the manuscript how the edges are added to the graph, but I might have missed this). 4.Since the problem that the manuscript is trying to solve is a graph classification and main contribution of the manuscript is a pooling method, the authors should consider comparing their pooling methods with different pooling methods and using different graph classification methods as baseline as well.<BRK>Short summary The authors propose a graph convolutional network (GCN) approach to perform the diagnosis and prognosis of COVID 19 from chest CT scans. They propose a novel pooling that takes into account the edges weights compared to recent methods. Strengths The paper is very well written and easy to follow. Comparison with CNNs would be interesting. While the problem of COVID 19 diagnosis and prognosis is timely, it lacked clinical insights. IoU is not reported across all patients, but rather on cherry picked examples  in Table 2. Given the proposed next layer connectivity, isn’t the next layer A an exponent version of the first layer A for the terms in top k? Is there a reference for this method?<BRK>The paper is an application of GCN with good features on chest CT scan images for Covid 19 diagnosis and prognosis. As a whole, to the representation learning community, it adds limited research values apart from being an application of GCN which is aligned to the application track of ICLR. The paper claims that with less than 1% number of total parameters in the baseline 3D ResNet model, their method achieves 94.7% accuracy for diagnosis, which is marginally better than state of art   however whether the model was over fitted is not clear. It will be better if approach is compared with another concurrent work   "Covid 19 Classification by FGCNet with Deep Feature Fusion from Graph Convolutional Network and Convolutional Neural Network"   https://www.sciencedirect.com/science/article/pii/S1566253520303705.
Reject. rating score: 5. rating score: 5. rating score: 5. <BRK>Does the concatenation match the number of parameters of the proposed method? Or is everything kept the same except for the concatenation replacing the action capsule? Conclusion:This paper presents an interesting task that has the potential of sparking research in this direction. ########################### POST REBUTTAL DECISION###########################After reading other reviews and the rebuttal, I have concerns related to the stochastic trajectories mentioned by R2. The fact that the authors confirmed that all semantic actions have the same number of steps makes me question potential overfitting.<BRK>###Summary###The paper proposes the video prediction model that can handle multiple objects. The proposed model is built from attention based capsule network, and it generates video in a hierarchical feature computation. ###Questions### The explanation of the method is insufficient.<BRK>This paper proposes a new task of semantic action conditioned video prediction. The paper presents a new model for this task, which makes use of the capsule networks to learn hierarchical relationships between objects. 5.It would be nice to have more discussion about the generalization ability of the model.
Accept (Poster). rating score: 7. rating score: 6. rating score: 4. <BRK>The authors propose a DRO based approach which amounts to solving a modified ERM problem. The resulting algorithm is a mirror descent scheme with explicit convergence rates, under Setting : train and test label distribution do not matchThe general approach is:Learn a classifier robust to arbitrary label shifts (from a family)  > doing this by using DRO approach because it allows them to train a model that performs well on all label distributions sufficiently close to the training data label distribution. The paper proposes the DRO approach with KL divergence for the label shift problem, an analysis of the proposed algorithm (optimization techniques and convergence analysis) and experiments on Imagenet (ResNet50)Good points        It paper is well written and asy to follow paper. The problem and the algorithm are clearly described and well analyzed. There are many hyper parameters that were set to a fixed value, and only one parameter is tuned (how?), so how/why do you choose these fixed values? Recommendations:         Contrary to what the authors say, inner problem in (4) does indeed have a closed form solution.<BRK>Weighting each class differently during model training is a common technique to deal with label imbalance and label shift. In this paper, the authors propose AdvShift, a method for learning these weights in a data  and model dependent way through distributionally robust optimization (DRO). I found the paper especially well written and clear, from the problem setup and contextualization within related work, to the description and intuition behind the method and the results, as well as the candid discussion of potential limitations of the method. I’m curious if AdvShift helps on non worst case settings. The paper is clear and proposes a promising optimization method, with moderate comparisons to prior approaches and reasonable results on ImageNet (but not CIFAR 100). [1b] If so, the tricks used in (c) seem to be critical to the performance of AdvShift, so they might perhaps explain the gap between Mohri et al.and AdvShift. This suggests that there could be fundamentally something wrong with the algorithm and its optimization. [2] Compared to Sagawa et al.(2020), the authors write that the method in Sagawa et al.requires the ability to sample data from a given group. I had two points: first, the differences between your proposed algorithm and the standard algorithm might be more stark when the training is not balanced. Specifically, for the fixed model, is it still being evaluated against the fixed distribution? If not, could we plot it as an oracle?<BRK>This paper tackles label shift in supervised learning via distributionally robust optimization. The main idea is to train by solving a min max problem, where the max problem searches for the worst case label shift in an Kullback Leibler divergence ambiguity set. The KL ambiguity set will generate some form of adversarial reweighing of the sample training points, which gives us hope that the learned parameters will perform better in the test data (with shift). The paper proposes to solve the Lagrangian version instead of the constrained version of the DRO problem, and proposes a gradient descent ascent type of algorithm. However, I do not think that contribution (1) about the formulation and contribution (3) about the numerical results can really be justified as the main contributions of this paper.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 9. <BRK>Overall the problem motivation and related work section is well describedNovelty:  The proposed model is created by clubbing together GCN model for node classification and GAE model  for link prediction. The loss function is a weighted combination of the two.<BRK>Results on more datasets, more comparison methods and a different setup will strengthen the paper. In this paper, the authors propose a semi supervised learning for the link prediction for unseen nodes. Reasons for score Overall, the paper is about an interesting problem (cold start for representation learning on dynamic graphs).<BRK>Summary: This paper presents a model called ColdExpand that addresses learning tasks related to attributed graphs. However I think they totally miss part of the recent literature on inductive approaches. Experiments show that ColdExpand is better for both link prediction and node classification, but (again) it is not compared to more appropriate baselines. First, I think the cold start problem presented in this paper can be solved by using inductive approaches that has already been developed. 2.The paper is reasonnably written.<BRK>This is an issue that authors present it as cold start in semi supervised graph learning. The solution, even if simple, is very effective and for this reason even more interesting. After those links are reconstructed they run a node classification step (based on GCN, GraphSAGE, or GAT) on G plus the new node and the predicted links. The results obtained in the experiments are really encouraging with improvements ranging from 15 to 25% over a baseline that does not consider the link prediction phase. There are many applications of Graph Learning where the technique presented in this paper can be of help. 2.The technique presented in the paper is simple, which I consider a plus. In some cases this assumption might not hold true.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. rating score: 5. <BRK>EDIT: The qualitative results help illustrate what the variational bias entails in practice, and indeed the worse coverage constitutes a problem worth overcoming. The Ant experiment was a good attempt at showing scalability, but the deterministic version isn t terribly informative since then the correction term does nothing. Would be a clear accept if you could show that, but as is the paper s contribution is bordering on acceptance. 5 >6  The authors show that implicit VIC is biased in stochastic environment due to its blindness to the effect of its  option  on the state transition dynamics. This is addressed by learning a model of these dynamics to allow for the calculation of the missing terms. Toy experiments are then performed that show the boost in mutual information caused by eliminating this bias. The original VIC paper only really explains things for the explicit case, so even the description of what implicit VIC is trying to do is a novel contribution of this work. Percent of states empirically achievable  should be an easy metric to evaluate in these toy environments and would strengthen the case for your extension. Overall, I like this paper and wish more papers would be like this. I m convinced that you ve found a flaw, but I m not convinced your solution actually improves things.<BRK>This paper studies the problem of maximizing empowerment in the context of RL, where the aim is to maximize the mutual information between some latent variable and future outcomes (e.g., future states). Experiments on a few simple tasks show that the proposed method outperforms prior methods. **Significance**: Empowerment remains one of the main methods for autonomous skill discovery. **Novelty**: To the best of my knowledge, this limitation of VIC has not been discussed in prior work. * It d also be good to include *explicit* VIC as a baseline, even though it requires pre specifying the number of skills. My original concerns were about clarity, high dimensional experiments, and visualizations. Since the paper has been revised to include nice visualizations and improve the clarity, I am increasing my score 5  > 6. I d recommend including a bit more discussion of what implicit VIC is and how it differs from explicit VIC, before continuing with the formal derivation. I d encourage the authors to include some comparisons against baselines for that task. I d recommend moving most of the derivation to the appendix and just stating the final objective as an equation. * The experiments are limited to very simple gridworlds and tree domains. **Questions for discussion:*** How significant is the bias in implicit VIC [Gregor] in more complex tasks? * "This type of option differs..."   Aren t there two differences?<BRK>## SummaryThe paper points out a limitation of the implicit option version of the Variational Intrinsic Control (VIC) [Gregor et al., 2016] algorithm in the form of a bias in stochastic environments. The rigorous mathematical derivations are simply re deriving the VIC mutual information bounds with a new added term and with some extra details on how to do it with a gaussian mixture model. ## Post rebuttal updateHaving read through all reviews and the author s response, I am updating my assessment in light of the responses and new experiments. I agree with the authors that the derivation has theoretical value and is not a simple re derivation of VIC. The new experiments and visualizations have been helpful (I am happy with the author s responses to R3), but the overall clarity of the paper is still lacking due to the dense mathematical notation. In light of this, I am increasing my score from 4  > 6, slightly leaning towards acceptance.<BRK>They point out that, in stochastic environments, the implicit VIC formulation in the original paper is missing a term in the mutual information (involving log likelihood ratios of state transitions). They then compare the empowerment of implicit VIC with and without their corrections in a few toy domains, showing that their corrections do not hurt in deterministic environments, and provide a small increase in empowerment in stochastic environments. Pros:The missing term highlighted, and the derivations of solutions generally looked correct (at least at the level I followed them). This is a potentially interesting contribution. Cons:1) The experiments are a) done only in toy domains and b) even there demonstrate only a 5 10% improvement in empowerment. These experiments are maybe fine as a sanity check, but they are not enough to demonstrate the importance of the authors’ correction term. This is because empowerment is not an important objective in and of itself. It is not clear to me whether the correction term is important in those pursuits. 4) Given the environments aren’t standard and are very simple, they should definitely be introduced in the main text, and not pushed to the appendix.
Reject. rating score: 2. rating score: 3. rating score: 4. rating score: 4. <BRK>* Since DGCN uses two branches of GConv nets, large scale datasets are necessary to evaluate the performance and efficiency. It should be noted why the two methods show such different performance on different datasets. Detailed comments:* The reason why two branches of GConv nets are used is not clear.<BRK>2.The experiment parts are not in a fair comparison too, as the paper does not use the standard way to perform dataset splitting. The new model uses node features to build another graph, uses GCN and GAT on the original graph and the new graph, and also adds a loss term to reduce the similarity of the final node representations. There is very unclear connection showing why this method resolves the problem they proposed.<BRK>The choice of GCN and GAT as the building blocks are not well justified. It is also possible to try other kinds of GNNs.<BRK>ICLR 2020. Finally, the model leverages attention mechanisms on these four types of node representations to produce the final node embeddings. + The intuition in the third paragraph of the introduction is not clear as the paper does not have any ablation study for this intuition.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 6. <BRK>	This paper discussed how data properties (e.g., label noise, label imbalance, data size) affects calibration error. The experimental results show that poor calibration performance accompanies with large noisy label rate, large imbalance ratio and small dataset size. For the reason of small dataset size causing poor calibration error, this paper provided the theoretical proof. Advantages:		○ The idea of considering a softmax cross entropy logit loss to help explain how data size affect the calibration error is interesting. In the future research, the researcher still hard to justify how much calibration error the current dataset whould bring or can t tell whether the current the current classifier whould be robust enough to defense the calibration bring by the current set. ○ There are many typos in this paper, should go over the paper again and correct these small mistakes.<BRK>The authors use several existing benchmark datasets and create synthetic class imbalance for datasets that are initially balanced. At last, the effect of dataset size and data augmentation on calibration error is reported. Although the observations are very informative, the overall contribution of the paper is not sufficient for the ICLR venue. The work is mostly focused on reporting an existing issue with no major theoretical analysis of the problem and guidelines for alleviating the mentioned problems. The paper is in an interesting direction but needs to become more mature. In reality, label noise is rarely random and is structured. It would be more helpful if the authors could extend the experiment to incorporate such scenarios. It would be interesting if authors compared their calibration error performance to their prediction error performance to find out if there is a trade off or the two phenomena are in the same direction. It would be more informative if the general trend of calibration error is compared with the trend in prediction error side by side.<BRK>In particular, the dataset properties that are investigated are:  Balanced/Unbalanced classes. Again, the accuracy of the model should also be shown for context. NLP.The conclusions regarding dataset size also hold with a Transformer on an NLP dataset. Finally, Section 4 provides some theoretical explanation. Figure 1 shows the results. This, however, leads to overconfidence and poor calibration. The authors conclude that overall the imbalance in calibration persists in most cases. The authors should measure, include, and address this, and try to disentangle both aspects, or argue for any correlation / causation relationship among them. The authors tackle the question of how label noise affects calibration. Figure 2 summarizes the calibration error for a number of datasets and noise level. Importantly, the calibration is measured on a test set that is not perturbed with random noise.<BRK>In this work, authors demonstrate that dataset properties can significantly affect calibration and suggest that calibration should be measured during dataset curation. In the field of applied AI to real life problem, we face all the time decision makings on what is the most effective strategy in the pipeline (eg.sampling, noise, labeling) and this paper present some evidence for those decisions. The study is not very novel, but important. Since the conclusions are very important and have key implications, I would suggest to apply this to more datasets, and also some of the existing synthetic datasets. Personally, I would like to see if these observations remain solid with more datasets and more variation of datasets.
Reject. rating score: 5. rating score: 6. rating score: 6. <BRK>The paper Gers et al."Learning to forget continual prediction with LSTM" already proposes a mechanism to remove memories that are not needed anymore. Thus, raising my concerns about the stability and scalability of the approach.<BRK>Given the new information and baselines, I think this is a promising paper that passes the acceptance threshold. Although this presents a neat idea that has the potential to improve an increasingly important model architecture, the experiments fall short of matching the claim that this method provides enables more efficient attention computation over memories _in practice_. Strengths:  The authors focus on an important problem for a very relevant architecture.<BRK>Please consider including a comparison of running time or physical memory usages between your method and other Transformers  It is unclear what are the baselines mentioned in the experiments. The proposed attention is integrated into each layer of Transformer and tested on several synthetic tasks and two language modelling datasets, yielding promising results. Is it possible to improve your performance with more parameters? No experimental result demonstrates that the method can reduce computation complexity.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 5. <BRK>Another limitation of this work is the relatively narrow scope. The paper only focuses on one application of semi supervised keypoint localization. The authors propose to add a keypoint classification branch, and design several well motivated consistency losses. (2) Ablation study of only adding the keypoint classification branch (without the consistency losses). ##################################################################Questions during rebuttal period:1.In Table3, it reads“Our method outperforms the supervised score only by a small margin with 10% of unlabelled data.” It would be more convenient, if the supervised scores are also listed in the Table.<BRK>Images are transformed with simple perspective augmentations. **Cons*** Relatively light on contributions, as the paper is quite simple. The baselines/datasets seem sufficient, but I might have missed relevant works (I do not work in this field). I don t have too much to say about it. My review is positive and, I hope, constructive.<BRK>It can be applied to point heatmaps based network by adding a semantic representation learn by a three loss terms: one supervised and two semi supervised. Experiments are achieved on four public datasets. The main contribution of the paper is the model and losses proposed to train, in a semi supervised way, the network. Experiments have been achieved in order to compare the proposed semi supervised model with other semi supervised models (2 are selected). It should be interesting to add the score of the model without unsupervised loss. It will, for example show if for 100% sample trained, adding unsupervised loss improves the method.<BRK>** Paper Summary **This paper presents semi supervised keypoint localization networks and loss functions to overcome the need for the labeled keypoint data for that task. The proposed method attains the improvement on several benchmarks for human and animal body landmark localization. The authors argue the proposed loss in (2) are different with them in that the previous methods leverage an inverse transformation. For example, if z and z  converge to 0, the loss is going to be minimum, but the networks are not trained well.
Reject. rating score: 3. rating score: 5. rating score: 6. rating score: 6. <BRK>In the theory part, this paper introduces the concepts of early learning timescale and nonlinear timescale, and shows how the learning dynamics depend on the parameter $\beta$. The theory developed in this paper is rudimentary. From the experiment results, the performance is relatively insensitive to the parameter $\beta$ with batch normalization. It is apparent that tuning for the best $\beta$ is always no worse than setting $\beta 1$. The performance with different $\beta$ is not presented in Sections 3.2.2 and 3.2.3. None of the key concepts identified in the paper are formally defined.<BRK>Experiments on image classification and sentiment analysis confirm that tuning temperature improves neural network generalization, even for state of the art models. Weaknesses:* Theoretical contribution seems unclear. Therefore, it is unclear how to use the current theory. The experiments can also be expanded. Currently, there are only two tasks in the experiments. Alternatively, the paper can also be improved by expanding experiments. It is a close decision. While I appreciate the clarification, I think this argument still doesn t fully align with the experiment results.<BRK>The analysis primarily concerns the initial/early phases of the training, or what the authors refer to as short times   note to authors: I think there should be a better phrasing than using "short times"   where $\tau$ is small. Additional comments and suggestions:1. Although some readers can reasonably guess the reason, it is worthwhile to explicitly say why this is the case. 4.It is not mentioned how the first plot of Figure 2 and the two plots in Figure 3 are obtained. The paper deserves an *accept* because it is fundamentally correct and it is one of the "secrets of the trade", and I am glad that it is written.<BRK>The authors introduced the effective time scale, and then provided the linear and non linear time scale based on the output of logits with temperature and stepsizes by using the NTK theory. ## Strong and weak points of the paper### Strong points  Provided the novel time scaling analysis for the cross entropy loss with many empirical validations. ### Weak points   The choice of optimal $\beta$ still remains unclear and left to the future work. Following is just a comment. ).So, I just thought that there might be some connection between this small temperature phonomena.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. rating score: 6. <BRK>The paper proposes to learn task agnostic world models in latent space, using a goal conditioned inverse dynamics formulation and an action priors model, that learn to predict action sequences. The approach is compared against a model free unsupervised control method and an imitation learning method on the subset of Atari games and a continuous control benchmark, and showed to achieve superior performance in most tasks based on goal completion ratio and sample efficiency. The proposed approach learns two networks, the inverse dynamics model and the action prior model, and uses this to plan a sequence of control actions, rather than learning the policy directly as done by the baselines. Although this has shown to achieve more goals in the 2 benchmarks, the authors do not discuss the learning complexity of training two networks compared to one policy network as done in baselines. The authors conduct an ablation study on the computational complexity of planning at test time for their approach. I believe it would help to evaluate the method better if they also show the number of trials used when comparing to baselines. Can the authors provide any real time performance guarantees at test time? The paper claims that the inverse dynamics formulation learns to represent controllabe aspects of the state. And have any empirical studies been done to show that this actually helps model based RL to perform better than when using forward models? It would be interesting to compare against model based RL techniques based on forward models, if avaialble, for this work.<BRK>The paper proposes a model based reinforcement learning method. The method builds a partial model of the environment through learning inverse dynamics, which is the distribution of action sequences that would bring one state to another state. Through training the model with an iterative relabeling scheme, the model is able to learn to reach goals in a subset of DM Control and Atari domains. Comments:+ This MBRL formulation is conceptually simple and does not rely on learning the full model of an environment, potentially enabling long horizon planning while being able to high dimensional input. + The relationship and differences to the closely related work Ghosh et al., 2020 is clearly stated and elaborated. My main concern about the paper is that although the difference between GCSL and the proposed method is exposited in theory (Proposition 3.1), there is no direct empirical experiment explaining how this difference might play out in practice. Specifically, there clearly are performance gaps between the proposed method and GCSL, but I cannot seem to grasp what EXACTLY is causing this gap. Is it ONLY because of the difference stated in proposition 3.1? If there are other differences, I d like to see them addressed in greater details.<BRK>Summary: The author proposes Goal Conditioned Latent Action Models for RL (GLAMOR) a novel approach to learn latent world models by modeling inverse dynamics. The proposed approach learns to track task relevant dynamics for a diverse distribution of tasks and provide a strong heuristic that enables efficient planning. GLAMOR demonstrates good performance against its baselines in terms of achieving accurately goals, sample efficiency and effective planning. Pros: good flow of the paper Correct situation of the problem with respect to current research A good problem to address in Reinforcement Learning area Strong experimental resultsCons: Can the authors talk about how the proposed method perform in a real robotic task? Can you rewrite or provide explanation of how it is possible? In para 3 of Section 3.4, why you sample from Boltzmann distribution and not uniform distribution?<BRK>The authors adopted an interesting idea for learning inverse dynamics which is different from common approaches where oftentimes a state transition model is learned for planning. Though in their scenario this does not make any guarantees on the improvement on generalization, conditioning on the rich semantics could be contained in actions, it might be a justification for learning the inverse dynamics model for tasks that test generalization capabilities. However, the idea of using relabeling to facilitate the learning of goal conditioned policies and world models is not new as discussed by the authors. This makes the learning framework somewhat less significant in novelty. I m curious if this prior distribution is different from a uniform distribution over all available paths starting from $s_1$, it is not well justified why we need to parameterize this with LSTM. There are also several, I believe, typos that might need fixing. In the experiment section, the model was trained with 500k agent steps while in the figure it was 5M, this leads to the problem of sample efficiency (section 4.6). Overall, I feel that though the idea of adopting inverse dynamics for learning action model is interesting, it is still somewhat incremental to prior works with a similar framework. The authors has addressed my concerns well in the discussion period, I have increased the score to 6.
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 7. <BRK>Also, it would be nice to add a column for the mean of the results over 5 subsets in Table 2 for an overall understanding of the performance4. Paper Summary: This paper proposes a deep neural network for online data association. The experiments are extensive, where the proposed method has been evaluated on three toy datasets and two image domain datasetsPaper Weaknesses:1.<BRK>The experiments are conducted on many domains and the proposed approach is compared with many methods from different domains. Ablation study is also conducted to demonstrate the effectiveness of the design decisions.<BRK>The paper proposes DAF Net, an attention based architecture for online filtering, which is used for a flexible set of tasks: online clustering, tracking, and image association. The architecture looks reasonable and handles observation encoding, dynamics updating, and online association altogether, and the presented numbers look good across three experiments. So why is this online approach *supposed* to be better than the batch approach?<BRK>This leads to several questions. The proposed architecture relies on a recurrent structure while also adding knowledge about the data association problem into the architecture design. In the robotic scenarios outlined at the beginning of the paper, one would expect there to be hundreds of objects that need to be tracked.
Reject. rating score: 5. rating score: 5. rating score: 6. <BRK>The threat model is the attacker has full access to the training data and the model training procedure. Overall, the lack of thorough comparisons of the trigger properties to existing attacks is the major weekness. This paper investigated an important problem for secure deep learning. The related work section only mentioned two backdoor attacks Gu et al., 2017 and Liu et al.2019b.To my knowledge, there are many existing works in this field. In terms of the difference to static trigger,  The authors argued that they are different to static trigger and "a static trigger cannot be used as a dynamic trigger", which I don t find it is verified somewhere.<BRK>Summary and contributions:In this work, the authors propose the first set of dynamic backdoor attacks, in terms of trigger pattern and location. The results of empirical evaluation show that the proposed attacks are effective as well as difficult to defend by several defense mechanisms. However, theSec 4.5 state of the art defenses is too brief. Since the motivation of this work is to “break” the assumption in some model based defenses, it would be interesting to compare the reversed triggers and the original triggers (sample from the dynamic trigger distribution). 5.Add a comparison with traditional static backdoor attacks against different defense mechanisms, which would be more convincing.<BRK>Summary:This paper outlines improved backdoor attacks for deep neural networks. This is a grave security concern, and as such there is substantial literature that relates to both systematically training backdoored DNNs, and also detecting when a trained DNN has a backdoor. The authors improve the functinoality of backdooring a DNN by creating a training process where triggers can now appear at random locations in the image and can have random contents. These GAN based techniques are key to both algorithms the authors present: BaN (random location triggers) and c BaN (random location and contents to trigger). I see how this is a valid concern in the community, and I believe that the authors have provided a better attack with respect to the state of the art.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. <BRK>The paper proposes a method of improving the generated samples of differential private synthetic dataset using GANs by boosting them post training. So it significantly improves the quality of GAN generated samples for different experiments under the same privacy budget. The community can definitely benefit from this paper.<BRK>This paper proposes an algorithm to process the sequence of generators and discriminators produced by any differentially private GAN training, in order to produce synthetic data of better quality. The privacy aspect of the algorithm, e.g.which part needs privacy protection and what is the sensitivity of the score function, can be elaborated more.<BRK>Unlike previous DP based GAN models, this paper aims to boost the sample quality of after the training stage. This paper provides a practical private post gan boosting algorithm to improve the sample quality.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 5. <BRK>  Summary of the problems considered and paper contributionThis paper studies an important problem: that differentially private algorithms can have disparate impact on model accuracy for different sub communities. This is an important problem because minority populations often suffer the worst decrease in model accuracy. Through experiments they show that their algorithm does indeed result in an improvement in fairness, according to a variety of fairness metrics. From my reading, the authors never state whether it is differentially private or not, and this definitely needs clarification.<BRK>In particular, the accuracy on the well represented classes is higher than the accuracy in underrepresented classes in an unbalanced dataset. This paper a self adaptive DP mechanism to address the problem of an unbalanced dataset. If FairDP does not satisfy the DP definition or has a very large privacy loss compared to DPSGD, it is not fair to compare FairDP with DPSGD.<BRK>In particular, the constraint set of the problem seems to be all models with optimal risk (absent any fairness, privacy). The paper introduces an algorithm for mitigating disparate impact of private learning (DP SGD) on different groups of a given population. The major concern I have with this work is that it lacks a formal (differential) privacy statement. But are you actually solving this problem?<BRK>How are you calculating the total privacy cost (eps, delta) at the end of the algorithm. Wouldn’t that be a simpler approach than what you are doing but provide the same benefits? Should potentially be considered as a baseline. 3. a) Fig 3 doesn’t really make sense as a line plot to me. b) Also measuring privacy by strength of adversaries does not seem natural here, I would prefer to measure privacy using the parameters eps/delta. b) Why is DPSGD doing so poorly on the total loss?
Accept (Poster). rating score: 9. rating score: 7. rating score: 6. rating score: 5. <BRK>The objective operates by initializing two models with identical weights and then training the models to produce similar sentence embeddings to each other for identical sentences and dissimilar representations for different sentences. It is well established that the final layers are not useful, but I haven t yet seen an analysis looking at all the layers of so many models. The paper demonstrates consistently strong results empirical results on the unsupervised semantic textual similarity (STS) task. The paper provides reasonably good analysis on demonstrating the impact of the proposed technique within different layers of a pre trained model as well as on the effect on the model scores vs. human labels.<BRK>The paper studies the problem of finding effective representation for semantic text similarity (STS). In fact, the last layers are worse than those preceding layers in STS. S2.The proposed method is relatively simple to implement. S3.The experimental results on STS task verifies that the proposed method achieves consistent and significant improvement over the plain BERT and supervised BERT. Although the observation is interesting, the connection between the observation on layer wise performance of BERT on STS and the proposed contrastive training method is not clear.<BRK>This paper proposes Contrastive Tension, a self supervised method to improve sentence representations from pre trained language models for Semantic Textual Similarity tasks. This work is motivated by the observation by previous work that the final layers of pre trained model are often biased towards token level pre training objectives, and perform poorly for sentence similarity tasks. Pros:1) The proposed method only relies on unlabeled data. 2) The experimental results on STS benchmarks are strong. Overall, I find the paper interesting, but the applicability of the method is not clearly demonstrated since the experiment is only done on STS datasets. Thus, I give this paper a weak accept rating.<BRK>The paper describes a method for improving pre trained language representations for sentence similarity tasks. This is shown to improve performance on different sentence similarity benchmarks. The method could potentially be useful and shows positive empirical results. As far as I can tell based on the paper, the only difference then is that negative samples are always passed through one particular model. However, it seems that the proposed CT method is only applied to and compared to representations from the top layer of the models. Why not start with a representation that already performs better? Does CT training happen before, after or parallel with the supervised training?
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 5. <BRK>Summary:In this paper, the authors studied the problem of batch size selection in graph neural networks. Overall, the paper is properly written. However, the results in this paper (e.g.Tables 3 6) show that large batch training generalizes very well for graph neural networks. To be honest, I do not find enough interesting points in the theoretical analysis. Even though the authors only use one GPU, they managed to train almost all of them within one hour.<BRK>Summary: The goal of the paper is to propose a principled strategy to select batch size for training graph neural networks with SGD. you have defined d_min, but nothing about If made more clear and the concerns are addressed, I will be happy to improve my score. This is not clear from the text in the paras (vague)   Can you express S in terms of the k hop neighborhood, is division by $q^l(i)$ well defined in equation (4), etc.<BRK>Please justify the assumptions on sampling. The paper defines the gradient estimator for minibatches constructed by layer wise random node sampling. Pros + Training GNNs to achieve scalability and accuracy at the same time is a relatively underexplored problem. But it is indeed very important. These help the readers better understand the training behavior of existing state of the art methods. However,    * $\phi$ itself is not a constant. The intuitive explanation from variance perspective has also been made in the literature. * Therefore, the analysis on variance gives limited practical insights due to those assumptions.<BRK>Strength:    (1) This paper studies an interesting and important problem on the selection of batch size for GNN training. However, It is not directly clear to me about the derivation of equation (4). I think this assumption is made in order to derive or simplify the analysis. Although I think studying the batch size selection for GNN is very important, I am not convinced or excited by the results in this paper. However, I do not get such points from the current version.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 6. <BRK>Summary:This paper proved that for a certain modified version of sufficiently overparametrized univariate normalizing flows where the underlying neural network has only one hidden layer, with high probability it can learn a distribution that is close enough to the target distribution where the distance can be measured in, e.g., KL divergence. The limitations of this paper are also discussed. The authors only analyzed the univariate case, which is far from the high dimensional case in practice. This setting is far from empirical settings, which makes the conclusions of the experiments not so convincing. Supporting arguments for recommendation:See "Cons", especially points 1 and 2 there.<BRK>*Recent work in supervised learning attempts to provide theoretical justification forwhy overparameterized neural networks can train and generalize efficiently in the above sense* Add a citation. For UNFs, the paper gives finite sample results for UNFs in Theorem 1. For UNFs, Theorem 1 shows that running SGD with a suitable learning rate leads to a neural network with small error. **Negatives**The paper is very difficult to follow because of numerous grammatical issues and lax notations. **Score**I recommend rejection of this paper. The paper is not well written and difficult to follow. 1.As the main result is Theorem 1, UNFs should be discussed earlier and CNFs should be discussed later. 4.** Note that by the initialization, $|w_{r0}|$ and $|b_{r0}$ are O(\sqrt{\log m / m})**What is the initialization distribution, and why can we not change the initialization distribution? 7.In the top right image of Figure 1, I don t see any benefit of large $m$   the training curve is too unstable?<BRK>This paper studies overparameterization over unsupervised learning. In detail, it uses constrained normalizing flows (CNF) and unconstrained normalizing flows (UNF) to learn the underlying unknown one dimensional distribution, which can be parameterized by a two layer neural network. The presentation of theoretical results can be further improved. In Theorem 2, it seems that to derive a finite sample analysis, the second order derivative of $F^*$ should be finite. The authors may want to add such a claim in the statement of Theorem 1. My main concern is the scalability issue. The main theorem suggests that it is possible to use a neural network to approximate the first order derivative of the unknown distribution transformation $f$, and to use the neural network to construct the original function $f$ with sufficient quadrature points. Thus, it seems that for a $d$ dimension case, the number of quadrature points may be the order of $O(1/\epsilon^d)$.<BRK>The paper studies the role of overparameterization in learning normalizing flow models. More specifically, the authors analyze the optimization and generalization of such a model when the transport map f is parameterized by a two layer neural network with potentially many hidden units (or highly over parameterized). Second, the authors prove that unconstrained NFs (UNFs) by modeling the gradient function f’ rather than f itself can learn the data distribution. I definitely think this work makes some interesting contributions in terms of provable results for learning over parameterized NFs. This is given by the fact that the problem is less well understood compared to supervised learning.
Accept (Poster). rating score: 8. rating score: 6. rating score: 5. rating score: 5. <BRK>Consequently, so is both the approach and the baselines it is compared against.<BRK>I recommend an accept, and would be willing to increase the score if my concerns are addressed.<BRK>The models are trained jointly using RL, where the selection module is trained to match the prediction of the main model and the main model minimizes the loss on the target selected by the selection module. How is this defined?<BRK>In my opinion the presentation is clear, the goal of the work, and the proposed solutions are presented cleanly.
Accept (Poster). rating score: 8. rating score: 7. rating score: 5. rating score: 5. <BRK>The paper is easy to read and the authors provide experiments to support the their observations and claims. Overall I think this is a good paper and in the following I suggest some good to have additions. (1) The examples for the supervised learning setting clearly demonstrates the impact of non stationary data. However, given that this is inspired by the problems under DRL setting, it will be interesting to do more analysis of this effect on some DRL tasks. For example, an analysis for offline RL might be a good setting to study this effect. (3) The RL experiment is mainly done in the on policy (PPO) settings. Some experiments with off policy RL setting might be useful, and the effect of the non stationarity might be more pronounced as well.<BRK>The simplified supervised learning setting is a good way of looking at the issue of non stationarity in isolation and it makes a compelling case that neural networks optimized by SGD can have generalization issues in settings where the data distribution changes over time, even after the data distribution converges. My main criticism would be that the hybrid objective feels rather ad hoc and that the proposed method could use a bit more theoretical justification. Regardless, I believe the community would benefit from the inclusion of this paper. * I like that the problem is illustrated in a supervised learning setting, which allows to investigate without the noise of a typical RL setup. * The "legacy feature" effect is an interesting phenomenon and the additional experiments inspecting it are useful. For instance, it would be nice if the teacher student distillation could be supported by some convergence guarantees, e.g.by showing that (under some circumstances) replacing the teacher by the student does not increase the loss. * The set of benchmarks considered in the paper is a good proof of concept, but additional experiments with different environments and base algorithms is necessary to better judge the merits of the approach.<BRK>Some gain of using the proposed method is experimentally presented. My major concern is the limited understanding of the non stationarity issue and ITER. Pros)  The authors propose a new approach to resolve the non stationarity issue in RL. Some gain of ITER is empirically demonstrated in various setting. In particular, the authors design a setup of "supervised" learning to show importance of non stationarity. However, it is a weak evidence on the importance of non stationarity in "reinforcement" learning. However, presuming that the paper reports the best gain possible, it is hard to accept that ITER always improves generalization. I have a concern on the little gain, which is shown by the comparison between PPO vs. PPO+ITER with the same hyperparameters. A fair comparison may use the best hyerparameters for each of PPO and PPO+ITER. Or, at least, there need comparisons with different hyperparameters to claim consistent improvement.<BRK>This paper investigates an interesting problem that transient non stationarity can affect the generalization of the neural network. Strengths: + This paper observes a novel problem that may appear in RL and designs a new algorithm to prevent such a problem. + This paper investigates this problem through experiments on supervised learning tasks. Weaknesses:  The new algorithm ITER doubles the computational costs. Experiments on Page 7 are trying to further illustrate the mechanism for such a phenomenon. Experiment results on Procgen is fair but not significant. Minor comments:  In Section 2, the advantage function is defined as A^\pi(s,a,s ) but later used as A^\pi(s,a) without additional explaination. f in $\mathcal{D}_{f,m}$ is not explained when it first appears. I guess it is the ratio of the modification.
Reject. rating score: 2. rating score: 3. rating score: 4. <BRK>This paper presents a Transformer based model for aspect based sentiment analysis, intended to support the unsupervised induction of constituents within the Transformer forward pass. Pros: The paper begins with an interesting idea and implementation, and the results support the claim that their architecture may replicate some of the contribution of dependency parse information. I would suggest that the authors attempt to change one of these things in a later paper, either by revisiting the model design, or task choice and evaluation (to better motivate the model). 3.Significance results are given (thanks!) The constituent derivation algorithm is not clear. Is something missing from the algorithm presentation?<BRK>And it is somewhat insufficient to limit the application only to aspect based sentiment analysis. What is $L(\hat{y},y)$ in (11) and how to compute it? The idea of modifying the basic transformer to a constituent based transformer by incorporating constituent similarities without any supervision is interesting. However, the paper still lack the following aspects:1.<BRK>The L2 distance between token representations reveals the degree of syntactic dependency; an aspect based sentimental analysis system is designed which does not rely on explicit syntactic information. Over dependence on the pre trained language model: Although the author claims that explicit syntactic information is not used, the system relies on the syntactic information contained in the pre trained language models such as BERT to promote the training of ConsTrans, RelConsTrans, and RelConsTransLG. 3.Inappropriate description: (1) In RelConsTrans, the proposed model encodes the syntactic relation as a feature for constituent scoring based on the role of syntactic relation for sentiment analysis.
Reject. rating score: 3. rating score: 3. rating score: 3. rating score: 4. <BRK>This paper considers a FPS game that can be decomposed into two sub tasks, navigation and shooting. The final policy is simply presented as linear combination of sub policies. These highly related approaches are ignored from the discussion and not considered in the experiments as a baseline.<BRK>Then: The paper mentions that LSTM networks are used, but of what size? Unfortunately, this paper leaves significant open questions about the environment and experiments.<BRK>I ll try to be as unbiased in my scientific evaluation of this paper, but I would request that the language be toned down a bit, and ideally other types of tasks considered down the road. Back to the review:This paper presents a multi task agent architecture with a final mixture component.<BRK>A main shortcoming of the submission is the evaluation section. Overall, the paper proposes an interesting method but the analysis has ethical problems as well as only a single environment which was created purely for this submission and limited baselines.
Reject. rating score: 6. rating score: 7. rating score: 7. rating score: 7. <BRK>*Summary*This papers presents a systematic analysis of clustering validation measures. I would suggest authors to include more comments on the practical usability of this analysis, maybe listing some other scenarios in which it can have a practical impact. What would be the conclusions if the number of clusters is kept fixed (and equal to the true number)? Would the disagreements be so large also in this case? Please consider to remove this property from the list of desirable properties. Actually Section 5 contains a summary of the findings, i.e.a summary of the properties fulfilled by the different validation measures.<BRK>The authors run experiments on 16 real world datasets and 8 well known clustering algorithms and provide a theoretical solution and a list of desirable properties that can help practitioners make informed decisions. Moreover, the authors also discuss the important pros and cons of the similarity indices in the context of the applications. I understand that there is no clear answer to this question, which makes this work interesting.<BRK>The authors propose a comparative review of cluster similarity indices through a theoretical approach prescribing in advance the required mathematical properties for a given application. The problem is clearly stated, as well as motivation and impact, and reference list is rich and up to date. there is no real novel material introduced here, and somehow no real learning involved: however, the analysis is clear and accurate, and, as a review, indeed interesting.<BRK>The authors motivate their problems in two ways. The focus of this paper is to provide a way of comparing different similarity indices. The characterization of CSIs with respect to some set of desired properties is not new, see e.g., [1] (some of the properties are shared). Given the plethora of the clustering algorithms that exist out there it is important to have a systematic way of choosing which clustering algorithm one should choose. I tend to be positive. My main concern is on the comparison with related work and the novelty of the paper, that I don t fully understand.
Reject. rating score: 3. rating score: 3. rating score: 4. rating score: 4. <BRK>This paper developed methods for resampling from the hindsight experience replay buffer. The resampling strategy was developed based on the current policy, and the overall distribution of the relative goals. As the distribution over goals evolves over time, the multi goal agent s replay curriculum is adjusted throughout the learning process. The developed approach, called hindsight curriculum generation (HCG), was applied to DDPG, and evaluated using a set of four robot control problems. The sampling from hindsight experience is partially based on the likelihood of the corresponding state action pair under the current policy. Some more discussions and justifications are needed for "the likelihood of the corresponding state action pair under the current policy." The two baselines of CHER and HER EBP were not mentioned in the experiment section. The conclusion was not supported by evidence or experimental results. The paper mentioned "Appendix" in a few places, but there is no appendix in this submission. It s suggested to experiment with RL methods other than DDPG.<BRK>The authors introduce a wealth of changes to the standard HER agent and obtain a performance improvement in 3 multi goal tasks. In particular they change it to a lower bound based on some nearby real experience minus the distance times Lipschitz constant. they use relative goals (g_original   g_current) instead of absolute goals (g_original). This carries through to the math. As a result, the contributions are a bit unclear. Without these, this paper cannot be reimplemented and is not in a publishable state.<BRK>### SummaryThis paper focuses on the problem of goal conditioned reinforcement learning. To generate goals for hindsight replay, the authors adopted a skew fit type method to the empirical distribution of the K means clusters to sample goals for replay. ### CommentsThe paper is well written and the idea proposed in this paper is easy to follow. Moreover, the proposed method relies on some assumptions that might not hold true for many goal conditioned environments. For example, one assumption lies in the use of L2 distance in equation 3. In many goal conditioned tasks such as maze navigation, the L2 distance might not be a good metric between state goal pairs since two states close in L2 distance could be on two sides of a wall. The paper does not include discussion or empirical evaluations for such tasks. "Search on the replay buffer: Bridging planning and reinforcement learning."<BRK>The paper introduces an extension of Hindsight Experience Replay (HER) called Hindsight Curriculum Generation (HCG) which is demonstrated to learn faster in multi goal RL benchmarks. Second, is the idea of constructing a distribution over the relative goals from the replay buffer using a simple clustering algorithm and defining a sampling distribution over them. This approach is then evaluated on three multi goal RL environments that were previously open sourced by OpenAI. From the experiments, it is not possible to tell whether the improvement in observed performance is due to the idea of using relative goals or the goal sampling strategy.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 5. <BRK>In this paper, the author studies the bias problem in race classification task with face data. Specifically, it first analyses the influence of kernel regularization and batch normalization to categorical cross entropy loss and proposes a maximum categorical cross entropy loss. Experiments on two face datasets colorFERET and UTKFace demonstrate the effectiveness of the proposed method. From the ethical aspect, the topic of this paper is important and interesting. In this paper, the author only compares their approach with the traditional CCE loss which is not convincing. Overall, I think this paper’s topic is important but the approach seems not make sense and less relevant to the racial bias problem. Pros.1.The bias problem that this paper studied is an important problem for image classification, especially for race classification. 2.The results in the experiment section could partially demonstrate the effectiveness of the proposed MCCE loss. 2.Algorithm 1 is not aligned with the paper. The variable \mu is not used in the algorithm but seems to be very important to the method (see Section 3). 3.Accuracy, the key evaluation metric in the experiment part, cannot fully demonstrate the effectiveness of the proposed method. 4.The discussion section (Section 5) seems not clear.<BRK>Summary: the paper proposes a new loss function, called MCCE to reduce the effect of overfitting to noisy examples. This involves calculating the Maximum Entropy (ME) of the input images as well as the filters (?). Specifically, it is unclear what the method is trying to optimize (other than adding some form of regularization term based on entropy). 1 and no justification is provided for the design choices (such as: how is mu   ME(w) used in the algorithm? What does convolutional reconstruction loss amount to? ...).The general discussion up to Section 2 can be shortened significantly and devoted to the development of the method. Overall, the paper is poorly written on the technical side. Additionally, it is not clear why the authors attribute the bias in the predictions to noisy examples. For instance, a poorly trained model or a model which overfits to certain examples can produce biased predictions. A more recent work, called the bi tempered loss (Amid et al.2019b) extends these methods by introducing a proper (unbiased) generalization of the CE loss and is shown to be extremely effective in reducing the effect of noisy examples. "Peer Loss Functions: Learning from Noisy Labels without Knowing Noise Rates."<BRK>Pros:1.The authors propose an extension of the CE loss to reduce classification bias that occurs in present methods and datasets. They calculate Maximum Entropy (ME) for images on the entire training dataset and then calculate the reconstruction loss between this and the ME for convolutional kernels during training. 2.The paper is thoroughly written with minor typos and is easy to follow. Cons:1.How does minimizing Maximum Entropy in the form of reconstruction error help to improve the weights learned by the model more suited for unbiased performance ? 2.How is the ME entropy calculated? It is good to briefly discuss the method/formula to calculate that. 3.In Algorithm 1, is there an error in line 5, it should be mu instead of gamma? The 1D interpolation seems like a good way to normalize, but is it the only way or the best way? Some evaluation on this either comparing feature activations or particularly which category improves more compared to CE might give a better intuition. 5.Please discuss some related work  or compare against as baselines with papers also trying to reduce classification bias.<BRK>The paper proposes a new extension to the categorical cross entropy using maximum entropy (MCCE) loss function to reduce model overfitting. The goal is to stabilize the training with respect to overfitting and generalizability. It is theoretically well founded and easily implemented. +The paper provides good initial results, and the experiments are conducted on the various dataset: colorFERET and UTKFace. Weakness:+ The proposed method is not novel and just a combining of maximum entropy with cross entropy. + The authors claimed to upload the supplementary material, but it s missing. And should report the computation cost for each experiment. +  More generalization analysis would be beneficial.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 6. <BRK>This paper studied a simplified image classification task with orthogonal non overlapping patches and is learned by a 3 layer CNN. They proved that if a learning algorithm satisfies PSI, the sample complexity is nearly quadratic in the filter dimension; while the VC dimension of the network is at least exponential in the filter dimension. The authors also verified PSI in some task based on MNIST that has non orthogonal patches. My major concern about this paper is that the theory seems to only work under orthogonal patterns and non overlapping filters, unfortunately, neither of which is true in practice. The theory only considers orthogonal patterns and non overlapping filters.<BRK>This paper studies a new theoretical framework to understand the ability of ConvNets to deal with pattern recognition tasks. It is assumed that there are two unique patterns whose occurrence in an image determines if the image is classified by 1 or 0. The setting is very specific (one type of architecture, realizability assumption, a single positive pattern and a single negative one). We also prove that SGD indeed satisfies PSI in a simple setup of two points in the training set" I find this somewhat unsatisfying. It seems very unlikely that this has not been observed before (at least empirically). While obtaining rigorous proofs of properties of NN is hard, one would expect that for the simple setting studied by the authors there would be a simpler explanation for the so called PSI. This paper makes numerous restrictions and assumptions. Some examples: "a natural model in this context is a 3 layer network with a convolutional layer, followed by ReLU, max pooling and a fully connected layer." Also in the classification task why is it assumed that n<d?<BRK>This paper is concerned with the question of generalization of convolutional neural networks. All patterns are assumed to be orthogonal to each other. Those images should be learned with a 3 layer neural network. The authors show that the networks, which are analyzed in this paper, have large VC dimension. In particular, the authors prove that PSI implies good generalization. They provide empirical as well as some theoretical evidence. I think this is an interesting paper, which develops a new point of view and contains interesting ideas. It would be interesting to see a more general version of this theorem.<BRK>In this manuscript the authors derive theoretical analysis for the generalization guarantees of a naïve CNN (3 layers) where the task is a simplified binary classification task, under the assumption that the images contain orthogonal patches (a naïve assumption). Informally, this means that the magnitude of the dot product between the learned pattern detectors and their detected patterns is correlated with the distribution of the patterns in the data. They prove that if a learning algorithm  satisfies PSI then its sample complexity is O(d^2 log (d)), where d is the dimension of the filter. According to their empirical derivation SGD satisfies this property. In contrast there exist learning algorithms that have exponential sample complexity. Pros.1.Addressing the problem of generalization guarantees is important and interesting. 2.Novelty.In some sense, this works shows theoretical results which are less restrictive than Yu at el, circumventing the dependence of the sample complexity on the network size.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 7. <BRK>This paper presents tailoring and meta tailoring to eliminate the generalization gap by optimizing at test time. Strengths:  The idea of using meta learning to improve tailoring is interesting. Some theoretical justification for the proposed method is provided. There is also no theoretical or empirical evidence to show that tailoring/meta tailoring is better than TTT. The descriptions of the experiment settings are unclear. Therefore, I lean to recommend rejection for this paper.<BRK>The authors propose a learning method called tailoring, inspiredby transductive learning, which worksby fine tuning a model on an unsupervised loss given a test time exampleinput. The authors also propose meta tailor, which includes the tailoring processas an inner optimization loop during training. In a sense, meta tailoringis like meta learning but with  each training example considered a separatetask. As this claim is included in the list of experimental results,I find that somewhat disingenuous. I lean to reject this paper. Miscellaneous note:In the last paragraph of page 4, it is mentioned that parts of definition 1 and theorem 1 are in bold green, but I see no bold green inthis copy of the paper.<BRK> Summary This paper proposes a meta tailoring method where the auxiliary tailor loss is used to adapt the model parameters at test time. The paper also provides a theoretical analysis of the advantages of the proposed tailoring/meta tailoring. In the last sentence of the abstract, it says "..., and using contrastive losses on the query image to improve generalization". However, I didn t find a discussion on it in the main paper. Given that previous work (e.g., Sun et al.) In general, this paper proposes an interesting approach and insightful analysis of the proposed method.<BRK>The paper proposes tailoring and meta tailoring, learning processes that fine tune the model parameters during test time using unsupervised objectives. This allows for designing and integrating powerful inductive biases into the model, leading to an improved test time performance in two example tasks. Strengths:* The proposed approaches are well motivated, and the paper is written clearly. * The experimental results show the benefits of the proposed approach convincingly. Additionally, building on these theoretical results, it would be interesting to see how the contrastive loss might be used for tailoring in an experimental setting.
Accept (Spotlight). rating score: 8. rating score: 7. rating score: 7. rating score: 7. <BRK>Consider adding a couple of lines pointing out this relation. It is easy to follow and the points are clearly stated.<BRK>####################Pros:$\bullet$ The proposed graph convolution method is tested on different problems (object recognition on the spherical mesh, facial expression recognition, action recognition on the face and body landmarks), and real world datasets.<BRK>The authors present a new definition of graph convolution that is also shown to generalize well known existing ones. The experiments are well conducted and I appreciate the indication of the standard deviation in the results.<BRK>I feel that three is a really shallow network. It is also robust to graph noise. Can the authors kindly comment/validate on this please?
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 6. <BRK>In this paper, an improvement of BERT model is proposed. * The authors claim that they evaluate their results on generation task but it rather seems that they evaluate language modeling using perplexity. Strengths:* The paper is well written, the positioning to the state of the art is clear and the method is rigorously described. *They are some redundancy in the text (second paragraph of 3.2 and fourth paragraph of the introduction) that is not necessary.<BRK>Pros:1.Good empirical results are demonstrated across an extensive suite of benchmarks. Hence I am willing to give a score of 6 despite of the following concerns. Cons:1.My major concern is about the novelty of this paper. I think the author should be honest and compare with relative positional information introduced in transformer XL in the forefront. Hence I am worried if the baseline such as XLNet was well tuned. The work does not make use of any disentangled techniques, or have disentanglement representation/architectures.<BRK>The main contribution is to tackle issues with the relative position embeddings used on standard transformer architectures. Strengths  The proposed model tackles a known issue in transformer architectures. The authors perform a comprehensive comparison on standard text benchmarks as well as an ablation study. The findings show that disentangle attention improves results on some text benchmarks. Weaknesses  Related work on disentangle representations for text, and the further motivation for using disentanglement into the attention model are not discussed. Missing results of the variance in metrics with multiple runs on the downstream tasks.<BRK>The authors run the standard suite of GLUE benchmark experiments, on both “large” and “base” setups, as well as a generation setup (Wikitext 103). One thing that I find disingenuous is fact that their disentangled approach does introduce additional parameters, which is not quantified (or even mentioned) in the main paper. I’m listing below the most important issues in this section:“RoBERTa and XLNet are trained for 500K steps with 8K samples in a step, which amounts to four billion passes over training samples”.
Reject. rating score: 4. rating score: 5. rating score: 7. <BRK>Overview: The paper proposes a novel loss function using Sobolev norms to decrease the computational costs when solving PDEs using neural networks. Would you have a reference for this? I understand that higher order could be more complex, and perhaps the 1D equations are sufficient to convey the intuition, however in that case at least a comment is needed on how these results could be extended to higher orders. In Figure 1, what is the reason for H2 loss not speeding up convergence with the ReLU? Is it the differentiability? I think that if theoretically you do not prove the results for high dimensional PDEs, the value of the proposed methodology for high dimensional PDEs should at least be shown in extensive numerical experiments. I do think the example in 5.4 is in the right direction, but a more rigorous analysis would be needed. I give two examples of papers which could be of interest below. The authors also discuss how certain norms cannot guarantee convergence of the derivatives of the numerical solutions. "Optimally weighted loss functions for solving PDEs with Neural Networks.”. The authors discuss the choice of loss functions to also speed up / improve convergence and the solution accuracy.<BRK>The idea of using neural networks to approximate the solutions of the pdes is very interesting, specially in high dimensional setting where classical approaches fail to scale. One of the most important aspect is the choice of loss function to guide the training of the neural network. And the paper s aim is to address this issue by proposing Sobolev norm as the loss function instead of the commonly used $L^2$ norm. The Sobolev norm includes additional term about derivatives of the error. The main claim is that with the inclusion of the additional term, the convergence of the neural network training becomes faster. It would be interesting to have a negative result about the L^2 loss function that motivates the application of the Sobolev norm. Subsection A1 does not really include a proof. If the result is already known, it seems better to cite the reference (with exact pointer to the result) in the main body of the paper and do not include it as the contribution. A4 is not written with care. The bounds in A19 and A20 are obtained without explaining the steps. It will be good to include a definition of the norms and Poincare inequality for the reader unfamiliar with pde analysis.<BRK>Sobolev training of neural networks, which augments the standard loss function with terms that penalize discrepancies between the derivatives of the network and target functions, has been shown empirically to improve data efficiency. Intuitively, one would expect that it also aids generalization in settings where the target function is sufficiently smooth. This manuscript proposes augmenting the loss functions used to represent the solutions of partial differential equations with terms penalizing the Sobolev norm of the solution, its initial condition, and the boundary condition. The motivation for this approach is clear because data  efficiency is of the utmost importance in PDE learning problems where the data could be very difficult to access. The experiments in this paper clearly show that a target accuracy can be achieved with fewer overall training points when using Sobolev training, very much consistent with the established understanding of the effect of penalizing the Sobolev norms in typical supervised machine learning problems. Some of the examples are high dimensional and non trivial. The theoretical results are not particularly compelling, but they serve a reasonable justification for the proposed scheme.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 6. <BRK>The authors propose a multi stage layerwise training (MSLT) approach to reduce the training time of BERT. Overall the idea of multi stage layerwise training is reasonable and the results look promising. My major concern about the work is the empirical comparisons. This paper claims that "ALBERT has almost the same computational complexity as BERT, training an ALBERT model is still very time consuming." "An ALBERT configuration similar to BERT large has 18x fewer parameters and can be trained about 1.7x faster. Seems the proposed method in this work (with 1.1x speedup) is not as good as ALBERT. Besides, it is better to also test on more complex downstream tasks, such as SQuAD1.1/2.0 and RACE.<BRK>This paper presents a training strategy to progressively adding top transformer layers, which results in training time speedup. Usually when training transformers, all layers are updated simultaneously. The idea is conveyed with sufficient background and related work. It looks to me the approach that the paper proposed is a very straightforward extension from the work of (Gong et al., 2019). Instead of updating all parameters in all transformer layers, this paper freezes bottom layers and only update top transformer layer (newly added). With experiments performed on base and large BERT models on GLUE dataset, this training strategy is approved to have slightly drop of quality but faster convergence speed. I would argue that this is a great investigation and experimentation but it does not meet the criteria of acceptance. I would be more than willing to re evaluate my ratings if that happens.<BRK>### SummaryThis paper proposes a simple method, ie multi stage layerwise training (MSLT), to speedup BERT training. Specifically, the authors progressively stack layers. The bottom layers are fixed and only the new added top layers are trained. Although compared with other speedup training methods like ELECTRA [1], the idea of this paper is not novel. However, the proposed method is simple and effective to some extend. ### Strengths* The proposed training method is simple and easy to implement. ### Weaknesses and Questions* The most related method to this paper is [2]. If training with MSLT using the same time as baselines (like train 2M steps), how about the performance compared with baseline? [3] shows that training with more steps can help improve performance. Does this phenomenon still remain in your method?<BRK>The work proposes a simple enough idea to speed up the training of BERT by progressively stacking new layers while fixing older layers. Empirically, with the same number of training steps (and less time), the proposed method can achieve a comparable performance to the original BERT. One problem with the current paper is the empirical evaluation is only conducted on the GLUE benchmark, which is sequence level and relatively simple. Another question is what would happen or what the performance would be if the entire model is not jointly trained for the last 20% steps. This information will help to better understand this method. In addition, the original motivation of the work comes from the fact that the attention patterns in the bottom layers do not change much after jointly trained with more higher layers. However, this does not mean the lower layer attention patterns don t change much if the entire network is jointly trained from scratch. To truly establish the validity of motivation, it would be good to monitor and evaluate how much the lower layer attention patterns change when jointly trained.
Accept (Poster). rating score: 9. rating score: 7. rating score: 6. rating score: 6. rating score: 6. <BRK>The paper includes visualisation of the learned components, extensive appendix with additional experiments and ablation studies, and a systematic overview of the prior work in learning compositional structures and lifelong learning. The algorithm is loosely inspired by biological learning and consists of two main steps. The topics of learning compositional structures and lifelong/continual learning are of high interest to the community. Adaptation step relies on existing methods for adapting the knowledge state given a new task in continual learning (component parameters are updated).<BRK>** Strengths **The paper addresses a highly relevant problem. ** Weaknesses **While extremely thorough, the experiments are somewhat repetitive. I.e.the authors say "Intermittently bypassing the new component ensures that existing components can compensate for it if it is discarded", but at the same time it seems that for compositionality, we want different components to be unique and independent of each other. After reading the paper I felt genuinely grateful to authors for putting this work together in such a thorough and thoughtful manner. Overall, in my opinion, the contribution is above the acceptance threshold.<BRK>### SummaryThe paper attempts to solve lifelong/continual learning (CL) by building reusable components and learning both the way of combining them and the components themselves. To do this, the authors present a framework of algorithms which is based on three abstract elements which are iterated:a. Updating the components itselfb. Updating the way the components are combined (structure) for a given taskc. Adding new componentsThe framework is instantiated as a concrete algorithm in several different ways whose performance is evaluated through extensive experiments. The work under review does neither.<BRK>This paper addresses lifelong learning of compositional structures by proposing a general purpose framework, which separates the learning process to two stages: combine existing components for assimilation; adapt existing components for accommodation and optionally add new components. The strong point is that the approach is general for model architectures and can be combined with different catastrophic forgetting mechanisms. My recommendation is that this paper is below the acceptance bar. The reasons are mainly from concerns for compositional structure learning and catastrophic forgetting evaluation. 3.If this paper emphasizes ability as a pre training algorithm, it should compare with other pre training algorithms.<BRK>The authors propose a new framework for compositional lifelong learning. The paper is pleasing to read, each choice is discussed and justifiedMy main concern is about the scalability and the resilience to harder streams of tasks. I would also be interested in seeing how sharp the component selection is on each task, is the assimilation step selecting only one or two components per layer or is it blending them all?
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK>In order to make the model robust to variable input lengths, the method stochastically reduces the length of a sequence at each layer during training. The paper looks from an interesting angle to build adaptive Transformers for inference   reducing the input sequence at each Transformer layer. However, there are a few concerns. First, the paper proposes to use a series of techniques to make LengthDrop work but lacks the ablation studies to show how those techniques help to make Transformer length adaptive. However, there are no ablation studies or comparison results on LengthDrop vs. LayerDrop in terms of the accuracy vs latency trade off. In addition to LayerDrop, it appears that the paper also incorporates several other fixes, such as the sandwidth rule and inplace distillation, which are borrowed from prior work. However, how these fixes contribute to LengthDrop is not clearly explained, and there are no studies nor experimental results to explain how each technique contributes to the final accuracy vs latency results. Second, the comparison with related work is weak. In particular, LengthDrop is built on top of PoWER BERT, yet the evaluation does not compare with PoWER BERT. For example, it is unclear what s the batch size used in the evaluation. For example, it is unclear what s the maximum sequence length is used in training. [1] Fan et.<BRK>The authors extend PowerBERT, a method introduced recently to perform  efficient inference by dynamically reducing input tokens as the model goes deeper. The authors address two limitations of PowerBERT: the need to pre set the required computational budget during training (which makes the model inflexible), and the inability to tackle span level tasks (which require the full sentence at the final layer). The authors present a simple solution to both problems (though one that requires heavy engineering to work, see below), and show promising results compared to several BERT baselines. Second, how does this approach compare to the original PowerBERT model (despite its limitations)? Third, given that the evolutionary search is performed on the validation set, how do the authors evaluate it during model development? I am curious to read the authors  response, and could be convinced to increase my score, but currently this paper does not meet the ICLR bar. The authors present simple solutions to both: for the former, they train a model that randomly selects the number of dropped tokens during training, which makes the model more resilient to different levels of pruning, and perform an evolutionary search process to match the exact pruning levels for a given computational budget. The authors address this concern in section 3.2, saying that "it only require a single pass through the relatively small validation set for each length configuration", but later say they "repeat this iteration G times" (with G 30) and "evolutionary search converges after about fifteen iterations". Although they (supposedly) provide important benefits compared to this model, it is important to understand at what (accuracy and efficiency) cost. More comments: 1. This paper would strongly benefit from a proof read. all around the paper.<BRK>The model can be trained once and directly applied to different inference scenarios. To achieve this goal, the author proposed the LengthDrop method, which randomly samples the length at each layer. In addition, the author used the sandwich rule to train the model. At each step, the sandwich rule will train the largest model, the smallest model, and another bunch of randomly sampled models. In the inference phase, the paper proposed to search for the best length configuration that balances the accuracy and latency tradeoff via evolutionary search. It is not so clear how different techniques impact the final performance and the author has not reported the training time. ##########################################################################Pros:  1.<BRK>The work targets an interesting direction of improving the efficiency of Transformers by reducing the sequence length. The main contributions of the work are (1) proposing LengthDrop as the way to achieve length reduction; (2) utilizing techniques developed in NAS, namely one shot NAS, to enable proper training and allow adaptive drop ratio search after training. Empirically, the authors show that the proposed method is able to match or even outperform BERT base model with 1/3   1/2 FLOPs during inference (not training). This adds another layer of complication to judge how much the gain/loss comes from distillation and the length reduction. Secondly, authors do not mention much about the training (finetuning + ES) cost compared to the standard BERT or Power BERT. In many real world cases, this cost is also non trivial. This may be part of the reason why only 3 datasets are considered in this paper.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>This paper put forward two Creative datasets   Creative Birds and Creative Creatures, which have corresponding part annotations. The authors demonstrate a multi stage sketch generation approach   DoodlerGAN, and achieve the STOA compared with other methods in several metrics. The topic of creative generation is interesting and gradually becomes the future research direction in the image generation field. The reviewer thinks this contribution is meaningful and solid. Although the idea of multi stage generation has been used in many synthesis tasks, it s mainly for different resolutions or granularities (local and global). The authors reimplement in deep learning way. The reviewer has to say that it may have some limitations on other fields, it s appreciated if the authors can share new possibilities, but that s not the point. Quantitative evaluation and human evaluation both outperform other baseline methods. Some concerns are also proposed:  How are the predicted labels added to the generator? It doesn t seem to show up on Figure 3(a). The reviewer has some confusion about the noise range in Figure 3(b), which is N(0,1). Which one is the true sampling distribution? The full loss functions are lack in the mainly body, which should be depicted more clearly.<BRK>The contribution of this paper is twofold. First, the authors introduce a pair of manually collected (via AMT) datasets of creative sketches (birds and general creatures) each along with part annotations. Second, they propose a part based Generative Adversarial Network for the generation of unseen compositions/configurations of novel parts (legs, body, etc.) for creative sketches. They also provide easy to understand evaluation metrics allowing the community to keep on working on the task at hand. The illustrative experimental setting shows the usefulness of the datasets, the potential of their generative approach and its easiness of use. Rarely is an immediately surprising insight offered in the same paper. The value of such datasets/tools is often clear only in hindsight with the benefit of time. If they are useful, they see organic adoption. Furthermore, one can argue that there is no particularly novel insights in this paper (no science behind the collection of the datasets and the proposed GAN is a logic incremental evolution of the SOTA). However, the paper is very well written and structured, the methodology followed is correct and the experimental setting is comprehensive (with promising results), so I m happy to let the noisy process of science (and reviewing process) figure out the value here.<BRK>In this paper, the authors collect a large dataset of sketches of imaginative birds and other creatures, then train a GAN to produce similarly creative sketches. They use both quantitative and qualitative methods to evaluate their work and find that the generated sketches are of higher quality than both other models and human sketches, and are novel. This topic is delightful and your paper is quite well written. My only concern is fit for the venue. There are plenty of papers at ICLR that aren t about representation learning these days, so I m still inclined to accept, but I d like to hear an argument. More specifically, I appreciate the level of detail regarding the dataset collection study and your model training process. I would also like to see more discussion of your "conditioning perturbation" trick. Can you characterize the effect the perturbations have on the generated sketches? I also find it hard to believe that the human drawings perform significantly worse than the GAN drawings across the board. Does that indicate a problem with the dataset you constructed or some bias among your participants? Very minor issue, but I think you also have a typo in Table 1, the DS column should probably be labeled GD to match the table caption.<BRK>### SummaryThis paper introduces two creative sketch datasets of birds and creatures, segmented into parts, each with ~10k doodles collected from Amazon MTurk workers. Additionally, the authors propose a GAN architecture for generating novel sketches in an incremental fashion, one part at a time. They provide many qualitative results as well as human studies to validate their approach. ### Explanation of RatingWhile the new datasets look nice, I m not sure that they sufficiently different from or better than existing sketch datasets. With respect to the proposed generative model, I think a user in the loop interface is a reasonable approach, but though the model seems compatible with such a system, the authors do not actually implement it. It would be nice to see this interface in practice, since without it, the results do look a bit better but are not particularly more useful than those from other GAN models. The paper is well written and contains many qualitative results and figures. The labeling of sketches by semantic parts presents an advantage over previous datasets. For this reason, it would be useful to have information about the order of individual strokes, which the dataset does not appear to include. It would be useful to provide a citation for the doodling process used for data collection. The paper claims the importance of certain architecture choices, e..g, that part channels help the model better predict the part locations.
Accept (Spotlight). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>The authors hypothesize plausible reasons, but I think the findings only show that BN and dropout does not work well for on policy algorithms. Many different RL algorithm implementations have used regularization with and without acknowledging its use in the paper, and this paper sheds light that using regularization in deep RL algorithms does have significant impact and warrants further study.<BRK>The results presented in the paper are certainly suggestive, but do not I think meet the level of rigour required at ICLR. I do very much like the motivation of the study, and would have liked to have seen results of greater statistical significance. It could be that regularisation worsens performance, for all we know.<BRK>The authors clearly state the scope of the paper and its placement with respect to RL literature. Overall, I find this paper to be a good empirical study and I am leaning to accept it.<BRK>2.The authors conducted substantive experiments, which I appreciate. 3.The work is presented clearly, and the paper is well written. (1a) The DL regularizers studied in this paper have proved to help training neural networks. (1b) The main reason the authors claimed for why some DL regularizers work is from the generalization perspective, which makes sense in DL. Second, DL regularizers help generalization as claimed in the paper.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 6. rating score: 7. <BRK>This paper studies an interesting problem, i.e., point sampling, in 3D point cloud generation through GAN. On the contrary, the sampling sensitive point cloud CNNs are not suitable for acting as a discriminator according to the authors’ experiments that all of them fail to generate reasonable point clouds. The rationality of the proposed method is also unconvincing. Unfortunately, the results shown in the paper (e.g., table 4) demonstrate that using avg also does not perform well. 3.To show the proposed mix strategy is general enough, it is required to show with other baselines. In other words, it actually heavily relies on the used extractor, i.e., with a sampling insensitive extractor, the metric is tend to be sampling insensitive. More details about the setup and network architectures about this experiment is helpful for reader to understand.<BRK>This paper experimentally examine a number of generator and discriminator network choices for point cloud GAN. It is shown that the best generator choice would be a PointNet with a mixture between max pooling and average pooling, and that an attention based PointNet framework performs the best in terms of discriminators. Hence, it also makes sense that they could perform well in a generative setting. Besides, using a convolutional discriminator may not be able to generate gradients good enough for a non convolutional generator. But if the generator and discriminator match in terms of architecture presumably the performance could be better. Is WGAN GP type cost term and penalty used in this experiment? Hence this detail can be important in deciding whether the conclusions from the paper would be credible. The main reason I want to nitpick on these seemingly small items is that the conclusion of the paper might change significantly from those details. This is very unclear in the current paper.<BRK>The paper conducts experiments to examine the effects of point sampling patterns in point cloud GANs. The paper proposes a new improvement direction for Point Cloud GAN, which might have a strong impact in the community. The paper is written concisely and the illustrations are clear. However, the paper does not provide the exact definition of “Sampling Spectrum”. The paper mainly divides thes spectrum into three regimes: sampling insensitive/ aware/oversensitive, which is oversimplified, and too broad. The authors should give more justification and background intuition on why the “Sampling Spectrum” should be in this form. In Figure 3, the authors show some visualizations of the point clouds generated by different discriminators. To make the experiment results convincing, it is better that the authors should provide more details on the experiments of KPConv and PointConv, and provide more explanation on why KPConv and PointConv give almost random point clouds.<BRK>Summary of the paper:This studies how different discriminator architectures are sensitive to sampling strategy of point clouds. The paper proposes three benchmarks that measures how sensitive the architecture to the sampling strategy of point clouds, which provides insights to future research about how to choose architectures for both discriminator and for feature extraction. The paper also tried to verify such results with a wide ranges of architectures and the results seemed rather self consistent. Discreteness of the “sampling spectrum”. While the idea to study how sensitive the network is to the sampling schema is certainly good, but the way to characterize the spectrum in this paper is too empirical and discrete. It would be nice if the author could provide a way to quantify the difference between different sampling methods (e.g.expectation of EMD on the same surface area), and verify how different architecture falls into such continuous spectrum. I agree with the authors that the point clouds’ quality can be thoughts as outputs of the geometries and the sampling, and those two factors goes hand in hand. Maybe the authors could point us to evidence that show that the networks used in the paper all have about the same capacity in telling apart geometries, which is an alternative to address this concern. Justifications:The paper is the first in my knowledge that studies how sampling strategy affects point cloud generation and evaluation.<BRK>The same is performed with the existing metrics evaluating performance for point cloud generation. Presented results suggest that existing PointNet based discriminators with max pooling point feature aggregation mechanism are insensitive to severe artifacts in point density, which leads to non uniform samples from the generators. On the other hand, existing improvements to PointNet, which perform better in discriminative tasks are over sensitive to sampling and manage to capture even the smallest deviations from the uniform sampling which leads to poor training signals (in terms of gradients) from those discriminators. ___After reading other reviews, authors  comments, and checking the revised manuscript I decided to slightly improve my rating for two reasons. Firstly, my concerns were answered during discussions, secondly, I do not agree that the concerns raised by other reviewers could justify a rejection. 2) Overall rigorousness of the experiments in the paper is impressive, and the conclusions drawn are logical and insightful. 3) Additional inputs on the sampling sensitivity spectrum for evaluation metrics are also valuable. Thus, it is not clear, where the obtained improvements for point cloud GANs put them on the performance quality list, considering all the generative models for point clouds. Overall, this is an impressive work, which I think may be accepted even as it is.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 7. <BRK>The paper proposed an algorithm to learn a policy when provided with natural language constraints. My main point is that I feel that the authors haven t exploited enough the uniqueness of this "safe reinforcement learning with natural language constraints" problem, and the proposed solution is rather straight forward. The three constraints are described in natural language. The authors proposed a two step solution. 2.The way the authors categorize the constraints and the collected dataset by the authors are contributions to the research community. Cons:The scientific contribution of the proposed algorithm is rather limited. I think the paper would be stronger (in terms of scientific contribution) if it makes some efforts on the following direction:1. How would we learn a policy without the labels for the constraints interpreter. Currently, the natural language does not play an important role in the algorithm. However, natural language is compositional.<BRK>In this problem, the goal of the agent is to find an optimal policy that maximizes the cumulative rewards while satisfying the constraints given in natural language. This paper is well organized overall, but I have several concerns and questions about the paper. I would like to receive detailed answers from the authors on this part. Also, if this is because $C(s_t,a_t;x)$ is provided from the environment as in the first question, I think that it is not necessary to use LSTM for sequential constraints as well. Also, for the results of baseline algorithms, are they also the results of using data equally? (including the pre training process)The main difference between the problem proposed in this paper and the existing constraints reinforcement learning problem is that constraints are given in the natural language. Therefore, although I acknowledge the considerable work for introducing a new safe RL environment with natural language constraints, I think that the contribution is insufficient unless consideration of natural language constraints is added.<BRK>This paper present an experiment of safe reinforcement on a 2D grid word where the safety constraints are specified in natural language instead of being specified formally. According to the authors: "The key challenge lies in training the agent to interpret natural language and naturally adhere to the constraints during exploration and execution". The proposed system is made of two parts: a constraint interpreter that is (mostly) trained in a supervised way with Amazon Mechanical Turk to translate natural language orders into grid world ad hoc constraints and a policy that is trained through PCPO, a TRPO like constraint aware policy optimization algorithm. If the natural language understanding (NLU) task has to be handled before the agent s exploration/training phase, we are facing a concatenation of two problems: NLU then Safe RL, not a new problem involving tightly NLU and safe RL.<BRK>Summary:The paper addresses how to learn policies for tasks in which constraints are specified in natural language. Towards this, the paper proposes a model that encodes the different types of natural language constraints into intermediate representations that model both spatial and temporal information between states. They also propose a new benchmark (Hazard World) which is inspired by the 2D MiniGrid environment. The authors address an important problem of modeling natural language constraints for safe RL. Even though the environment is built on top of 2D grid world with simple action spaces, constraints are still specified in free form natural language. Please let me know if I missed it in the appendix. How can a model handle such constraints?
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. rating score: 5. <BRK>In particular, the authors show that, as vector spaces, the RKHS on the unit sphere of the NTK for a ReLU network of any fixed depth are the same and in fact coincides with that of the Laplace kernel. 3.Proving that the RKHS of the Laplace and ReLU kernels coincides as vectors spaces is a substantial result. Perhaps the closes prior work used free probability to study the spectrum of the NTK. 5.The paper is clearly written. Overall, this article is a solid theoretical contribution to our understanding of the NTK.<BRK>This paper compares the reproducing kernel Hilbert spaces (RKHSes) generated by a deep neural tangent kernel (NTK), a Laplacian kernel, and an exponential power kernel. Aronszajn’s lemma is then used to establish the inclusion of the RKHS. The proof technique using analytic combinatorics seems novel, and the main results are useful for theoretical understanding of neural networks.<BRK>This paper proves that the reproducing kernel Hilbert spaces of a deep neural tangent kernel and the Laplace kernel have the same set of functions when they restricted to the sphere $S^{d 1}$, which improves the results established in Geifman et al., 2020. Moreover, the paper proves that more non smooth of the exponential power kernel leads to a larger RKHS with restriction on the sphere $S^{d 1}$ and the entire $R^d$. In summary, the paper is well written and organized logically. The contribution of this paper includes two parts. Now I would like to give some comments, 1.<BRK>The goal of this paper is to complete the theoretical subset inclusion relationships given in (Geifman et al., 2020). The authors proved that the RKHS of the the neural tangent kernel (NTK) with any number of layersis a subset of the RKHS of the Laplace kernel. In Section 2.1,the original Laplace kernels and exponential power kernels are all classic shift invariant kernels.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 6. <BRK>It categorizes ways to define losses according to whether they depend on the data value, on the data index but not its value, or are independent of the data. The paper should make connections to maximum likelihood and maximum a posteriori optimization more apparent. Does sec5.3 amount to MAP optimization with some suitably defined prior? I found it difficult to settle on a rating for this paper. Yet the paper on its own is lacking in detail so much that the experiments are impossible to understand fullyAs a conference paper, it seems to be "bursting at the seams"; maybe it would be better suited as a journal paper, where the important aspects can be developed.<BRK>The three applications mentioned in this paper are also typical and important. However, given that the main point of this paper is to optimize full likelihoods for learning problems, it is not clear to me what are the real novel contributions made by the paper. I m expecting more comments in this regard. I would suggest that the authors should at least provide several examples by pointing out which is which. 4.In my opinion, the presentation of the paper could be further improved.<BRK>Weak accept: optimization of hyperparameters of $\ell$ is theoretically well founded from a Bayesian point of view, and should be more explored, as the authors do. However: 1) there is no consideration for this approach, while the preceding works of Barron (cited in the paper) had a word about it. 3) The generalization of this approach to the penalty is not well founded (at least, the authors do not justify it). Edit:### RebuttalI had read the rebuttal and the other reviews. It seems that there are some clarity issues, which are independent from my knowledge of the area chosen for the experiments. However, I consider that the Bayesian point of view (which comes from preceding papers) has been well highlighted in the revised version.<BRK>Overall, I think that the paper would shine as a journal paper while it is only a borderline submission in its current form. Hence, the novelty mainly lies in the application of these ideas in deep learning and the employment of some likelihoods better suited for the respective problems (i.e.softmax and rho estimators). # Rating and comments after the rebuttalI think that in the revised version the paper has addressed many of the weaknesses pointed out in our reviews, hence I increase my rating to 6. In fact, there are too many cross references to the supplemental material, to the point that it seems that most of the paper is described in the supplemental material.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 5. <BRK>**Summary**  This paper presents a method of knowledge distillation, which showed better results than KD and RCO in experimental results. Aside from everything, the performance improvement that ProKT alone brings seems limited compared to CRD, and it is difficult to trust because there is no information such as the number of runs or standard deviation in the results of the experiment. In this method, the snapshot interval of the teacher model targeted by the student is more frequent (every training step) than the RCO (authors called this property as *continuous*), and the parameters of the teacher model are also updated during *student* training (*dynamic*). **Strong points**  The proposed method was evaluated for two modalities (visual and textual).<BRK>This paper follows the work of RCO[1], where knowledge distillation is conducted by learning from the optimization trajectories of the teacher rather than the converged teacher solely. The authors argue that smoothly changed targets can help the student out of poor local optima. 2.The idea of making the teacher aware of the student for distillation is interesting. Cons:1.My main concern is that the proposed ProKT is highly similar to some prior works such as deep mutual learning [2], KDCL [3], and PCL [4] from the perspective of methodology, although their motivations are somewhat different. How to make the proposed method more efficient, or how to make more fair comparisons should be carefully considered. All the reviewers are leaning to reject this paper due to the limited novelty and unfair and incomplete experimental comparisons.<BRK>Overview summary: This submission studies the knowledge distillation approach for deep learning models. 3.As for the experiments, in Table 1, the gap between RCO and ProKT is really limited, which makes it hard to say that ProKT is stronger or more effective than RCO. General comments:Overall speaking, the motivation of this submission is clearly described and the idea is easy to follow. The performances are similar, only slightly better from ProKT. 2.In figure 2(a), the bleu line fo the student model training is interesting, it first increases the loss, do the authors have more explanations? To be honest, the proposed method mainly do a step wise distillation for the student model.<BRK>The proposed method is simple yet effective and achieves state of the art results on both image and text classification tasks. The computational cost of ProKT is much higher than RCO as it requires training the teacher and student simultaneously. Moreover, this paper only provides an empirical evaluation on the CIFAR 100 dataset. It will be useful to see whether ProKT can achieve higher student accuracy on a more complex dataset such as ImageNet. In the proposed ProKT algorithm, both the student and teacher models are trained with the same number of iterations, and also the student model learns from an updated teacher model in each training step. Minor detail:In Algorithm 1, it shows that the student model is updated before the teacher model.
Reject. rating score: 4. rating score: 4. rating score: 7. rating score: 7. <BRK>This paper studies off policy reinforcement learning for sequential recommendation. The basic idea is to summarize each possible environment dynamic (i.e., the state transition) into an environment context vector, and optimize policy with respect to this context vector accordingly. The problem setup is a mix, which affects the generalization of the proposed solution. The key idea behind the proposed solution is to exhaust possible environment models from offline data, and estimate the policy for each estimated environment model. Hence, how to partition the offline data and how to estimate the environment model become vital; but there were not enough discussion or experiments on these two important aspects. However, my major concern about the generalizability of the proposed solution still remines, as there are too many design choices depending on domain knowledge.<BRK>The idea is that the learned policy is adaptable to diverse simulators defined using offline data. >> solve an offline problem that policies can applied to real world applications without any additional online sample. Taking expectation with respect to \rho is not considered robust. This misunderstanding decreases confidence in the validity of the results presented. 4.The paper is generally dense and hard to understand at several places. >> the environments include a two level structure: ... Unclear (what is the special structure?how is the environment made context agnostic? If it is relevant to differentiate the proposed solution with respect to this weakness of prior works, more information and clarity is needed. >> Since the environment has stochasticity, learning by consistency penalties will obstruct the policy into a small region for exploration.<BRK>This paper discusses the problem of RL based sequential recommendation system and proposes a model based learning method to solve the offline RL in real world applications. Experiments demonstrate the effectiveness of the proposed model against state of the art baselines on one dataset. Implementation code is not released. 3.Only a few baseline methods are introduced with the proposed model.<BRK>The novelty in the work is that it extends on other model based and adaptive algorithms to suit the needs of SRS where stochasticity and non stationarity are a part of the problem this any learned model has to be robust to distortion because of off policy setting. The background and notations in Section 3 and 4 are well written and rigorous, however, a little more grounding to a sequential recommender system setting would help in two ways: make the paper more self sufficient for a Recommender systems audience, and also motivate the distinction between other model based and other adaptive policy learning methods. In the experiments section, it is worth discussing what is the trade off between choosing an application specific dynamics set has with models that do not have access to this knowledge in a different application. On a minor note, the authors should make the setup in section 3 a little more self sufficient and motivate the problem from a recommender system perspective i.e.how stochasticity and non stationarity come into play.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK>However, they were only demonstrated in a few settings. 3.Except for Figure 1, PHATE is not compared against more commonly used reduction algorithms such as tSNE and UMAP. While the use of better visualization techniques may lead to better understanding of neural network dynamics, I am not convinced that the paper succeeds in making this case:1. The generalization results (Figure 2) are interpreted using known results about the generalization properties of flat vs. sharp minima.<BRK>d. What about fully connected nets? However, I think that the scope of the experiments you provide is limited, the claims you make a bit stronger than would be justified by the results shown, and there are some worrisome features of some of the embedded trajectories that make my question the validity of your overall conclusions. * The paper is well motivated.<BRK>However, the rationale to do this is not quite clear and the interpretation of the visualization is hard. Is there a smooth transition between the “good” and “bad”? WeaknessMy major concern is that the insights observed from the methods are not quite straightforward for interpretation and some conclusions are not clear or novel.<BRK>Insights are not significant, but the visualization of them is improved. The jump and retrain experiments are effective and well chosen. The demonstrations of learning algorithm are not as effective, and I m not sure what to take away from them.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK>This paper presents a recurrent network for long term video prediction, which learns hierarchical latent states in space time. While I understand that they have many differences in technical details, such as the number of layers and the dependencies between the latent states, the key insight behind these approaches is similar. 2.Hierarchical latent variables have long been used for video prediction, as in Wichers et al.(2018) and Xu et al.(2018).I think the authors should make more comparisons with these similar works both in terms of methods and experimental results. For both Moving MNIST and KTH datasets, the proposed model tasks as input 36 context frames, which is not a common practice of most previous models, including those from Wichers et al.(2018) and Denton et al.(2018).Given that the temporal dynamics is relatively simple in these datasets, it may not be necessary to use so much context information.<BRK>The KTH action dataset is the closest to a real world dataset, but is still simplistic with just one moving object and a fixed background. The authors should consider experimenting with more challenging datasets. 3.Experiments showing different amounts of information present at different levels of the latent space are interesting and show that desired hierarchical structure in the latent space is achieved. Cons:1.In light of the paper "Improved conditional VRNNs for video prediction" by Castrejon et al., I do not believe there is enough novelty in this paper in terms of the latent space architecture, as this paper also applies a hierarchical latent space model for video prediction.<BRK>This paper tackles the problem of long term video prediction. Empirically, the authors validate that the higher levels of the hierarchy do indeed encode for slower changing features in the videos. They also show that using temporal abstraction allows TALD to produce better predictions of long sequences (hundreds of frames) and outperform prior models in that setting. ################################################Strong points:  The paper is clear and easy to follow. Thorough experimental results validate that the model behaves as expected. They are trained to provide plausible sequence, and not retrieve the exact ground truth sequence. Why weren t stochastic variants of PSNR, LPIPS and SSIM (as used in the original publication for SVG LP), or Frechet Video Distance (maybe on subsampled frames of the sequences) considered instead? ################################################Score motivation:The paper tackle an important problem with a well motivated novel model. However, I have concerns about the quantitative evaluation methods.<BRK>This paper proposes a method called Temporal Abstract Latent Dynamics (TALD). Results are qualitatively better than other methods in term of maintaining long term consistent prediction. Quantitative comparison is reported only on KTH dataset (Figure 5). Written presentation is clear and easy to understand. Pros:  The idea of modeling hierarchical latent variables for long term video prediction is novel and interesting. The experiments of removing one level of hierarchical latent (by replacing the posterior) is particularly interesting which proved that TALD can abstract different semantic information at different level its hierarchical representation. The supplementary provides two more sequences (1 from moving MINIST and 1 from GQN Mazes) with qualitative results, but not quantitative results on these benchmarks.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK>The paper proposes K PLUG, a procedure for pre training an encoder decoder transformer with domain knowledge. More concretely, the authors propose to apply multiple pre training objectives adapted to the knowledge source:1. PROS:  Integrating knowledge in transformer based model is a topic of interest  The experimental evaluation is solid. The results may therefore not be reproducible. (see comments)  The paper is not self contained (see comments)  It is not clear if and how the approach extends to other types of knowledge, outside of the e commerce and products domain. there seems to be enough space. Perhaps examples can be given? with some explanation of how the proposed approach helps?<BRK>Summary: This paper proposes pretraining language model for e commerce domain. Specifically, the authors design five pretraining objectives to incorporate various domain knowledge into the the models with an encoder decoder architecture. The paper is generally easy to follow. Experiments are solid and convincing. The paper is generally lack of novelty and inspiration, however. Most of the pretraining objectives have been tackled more or less in previous work. It will be more inspiring if the authors can give some high level principles. For example, from the notation it is unclear which training objectives are defined on encoder and which are on both encoder and decoder. How the five objectives are weighted in the pretraining?<BRK>They evaluate their method in e commerce scenarios. They use five pre training objectives, including domain specific knowledge bases, entities aspects, entities categories, and entities selling propositions. Reasons to reject:* The paper is not easy to follow with too many abbreviations. * The main concern of this work is: the comparison to existing works is too weak. However, the authors do not compare their training objectives/strategies to theirs, it is very hard for us to make a conclusion that which is better. It is not an apple to apple comparison.<BRK>#### SummaryThis paper introduces K PLUG, an in domain transformer style pretrained language model. They design specific pretraining objectives based on each type of knowledge. Experiments on several downstream tasks exhibited the usefulness of the proposed model. Specifically, in the e commerce field people often have heavy needs for automated text processing and also a large amount of data, making this line of work more useful in practice. The experiment results also justified the design. #### Weakness  The proposed method (objectives) is hard to transfer to other domains. More importantly, it requires highly structured and large scale in domain data for pretraining, which might not be available in other domains. In other words, when the tasks are too similar, it becomes less clear whether the performance gain comes from the pretraining method or merely a larger training set.
Reject. rating score: 6. rating score: 7. rating score: 7. rating score: 7. <BRK>2.This paper incorporates conservation laws into the RNN as an inductive bias. 4.MC LSTM utilizes a positive left stochastic matrix to redistribute mass. 5.This paper validates the MC LSTM on arithmetic tasks, traffic forecasting, pendulum, rainfall tasks. Incorporating mass conservation laws into the neural network is important. 2.This paper proposes MC LSTM, a simple and effective mass conserving model. 3.The motivation of the research and the proposed methods are straightforward. 4.Related works section provides a comparison between MC LSTM and others such as the Markov chain, and it is also very interesting. This paper shows extensive experimental results, but there are less qualitative results. 2.This paper missing some important related works, such as a physics guided recurrent neural network model (PGRNN)<Questions and Additional Feedback>1. 2.I wonder how MC LSTM actually works. 3.What is the reason that Eq.(5) and Eq.(6) utilize L1 norm? 4.Does MC LSTM can be extended to represent (2*mass) or (mass^2) conserving properties? Discovering physical concepts with neural networks<Typos>1. I will keep my positive score because my concerns are resolved partially.<BRK>In this paper the authors propose a novel architecture, called Mass Conserving LSTM (MC LSTM) based on LSTM. Thus, they propose that also the quantities involved in deep learning models should be conserved. To do so, they aim at exploiting the memory cells of the LSTM as mass accumulators and then force the conservation laws via the model equations. The authors finally show successfully the potential of this novel network into three experimental settings where several types of “conservation” are required (e.g.mass conservation, energy conservation, etc). Pros:+ they deal with a problem which can arise in non laboratory scenarios in a novel way. Moreover, I found really interesting how they deal with the related work and special cases. Cons:  (minor) the authors focus their experimental section to settings where mass (or energy, etc) conservation is required. It would be interesting to see how it performs also in settings where it is not required as well, thus showing whether this method also generalizes to different settings.<BRK>#### SummaryThe authors proposed a new family of LSTMs (i.e.MC LSTMS) which can be shown to have a mass conservation property for the LSTM memory cells. They have shown in various applications that these models lead to on par or better performance compared to state of art approaches. I have listed those concerns in the question sections as authors might have precise answers. #### ProsThe authors in this paper have proposed a new family of LSTMs (i.e.MC LSTMs) which can be shown to have a conservation property for the mass held by the LSTM memory cells. The authors have applied their models in three different prediction tasks involving quantity conservation, namingly summation(and subtraction), traffic prediction and rainfall runoff modelling. The proposed models (with some variants) achieve better or at least on par with current state of art models. These different applications illustrate the usefulness of the models; the authors also reference appropriately to current state of art approaches, helping reviewers to well situate the paper’s contribution. #### Questions:Well I understand there are differences for each application (motivating to choose different neural architectures), it would be very interesting to know the performance of one quite general architecture (compared to say standard LSTM implemented in torch). In the paper, we have seen in the experiments:  time   independent R^t used in arithmetics  time   dependent R^t used in hydrology experiments  Hypernetwork based R^t used in pendulum experimentsAnd also they vary on the auxiliary/mass inputs choices. What would be the performance if the authors use a quite general architecture (for example the second setting) for all the experiments please? Meanwhile the r value for the traditional r value is different from what is chosen for the MC LSTM. My main concern was about the generalisation of the proposed architecture and the results at the current revision are convincing to me.<BRK>The paper provides an interesting and novel LSTM structure named MC LSTM, which extends the inductive bias of LSTM to deal with some real world problems limited by conservation laws. I deem that the novel architecture based on LSTM that conserves quantites is useful and interesting. My major concern is about some explanation about definitions and some additional ablation models (see cons below). 2.This paper provides comprehensive experiments, including both qualitative analysis and quantitative results, to show the effectiveness of the proposed LSTM. The entire structure is organized well and the formulas are very detailed. Because in basic LSTM, there is sigmoid. It would be better to provide more details about it. It would be more convincing if the authors can provide more cases in the rebuttal period. ##########################################################################Questions during rebuttal period: Please address and clarify the cons above
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 5. rating score: 7. <BRK>They empirically show that in a variety of cases, simultaneously learning action value estimates with varying multi step TD targets can lead to improvements over existing methods. ### ReviewThe proposed algorithm appears novel, and the results are interesting. 2) The motivating example compares learning with a 1 step TD target with that of a 5 step TD target. The paper suggests that due to the larger bias in a 1 step TD target (and consequently, slower convergence), the 1 step learner ends up exploring more. Such a high frequency of this suboptimal path seems suggestive of the method actually *not* exploring enough. As such, I m not sure the conclusions drawn from the example are convincing, and would appreciate if the authors could comment on the above or clarify any misunderstanding. Can the authors clarify what this precisely means, and elaborate on why this is a desirable property? 4) It appears a fixed learning rate was used for each algorithm. Could the authors comment on how hyper parameters were chosen in the empirical evaluation? The use of 3 seems insufficient for central limit theorem arguments, but the mention of 68% suggests an assumption that the results are normally distributed. Can the authors comment on the statistical significance of their results, and whether the number of runs are sufficient for the claims being made? Post Discussion  I appreciate the clarifications made in the discussion regarding additional experimental details and whether the methods were fairly compared. There are good recommendations by Henderson et al.(2017) and Colas et al.(2018) for the empirical evaluation, and I think it would additionally strengthen the paper to formalize the notion of heterogeneity (e.g., be able to approximately measure it, and convincingly argue that this is what s underlying any differences in performance).<BRK>This paper utilizes different step return targets for different heads in bootstrapped DQN, such that a more heterogeneous posterior estimation of policy value may be obtained. However, my main concern is the novelty. The step size can be viewed as a tunable hyperparameter, and the posterior computation is mainly credited to the usage of different randomized DQNs. It seems more natural to me to include the comparison with this baseline, since the performance of mixed 1 3 may be more close to all 2. I m not sure if Figure 4 does do you a favor, since again we can always tune the hyperparameter of step size, and it does not need to be the same for different environments. This paper s method seems to not peak either of them. I m not sure about the point of comparing an agent trained with its own data versus one trained with other data, since we are just learning the policy value but not the policy itself, and there is no issue like off policy correction, etc.<BRK>###################################Summary: This paper combines the idea of multi step returns for value based reinforcement learning with Bootstrapped DQN. Updating value functions with different step returns has potential benefits, and was elegantly integrated with the Bootstrapped DQN structure, where different heads have different backup lengths. The strength of having different backup length in value based RL is that multi step returns have lower bias (despite the high variance) and sensitivity to future rewards. I like the originality of this idea presented by this paper. Separating data generation agent and learning only agent is valid; MB DQN is compared with DQN lambda and DQN ensemble and shows better performance; and more experimental results with different numbers of heads and different configurations of return lengths are compared. The choice of (step length) n_k is limited: this paper presented main empirical results using five heads of 1 step and five heads of 3 step, and also compared with mixed 1 2 3, 1 2, 1 3, 2 3 steps. In the supplementary materials, there is a result on 1 3 5 step configurations. Could you add some experiments on 10 or 20 steps or at least add insights on how it would perform? However, I don’t see comprehensive analysis in the perspective of explorations. To be specific, as authors mentioned in section 3.1., shorter step return is better at exploring wider areas in the state spaces than longer step returns, although longer step returns might be faster at convergence. In terms of exploration perspective, is there an evidence that mixture of shorter & longer n_k’s is better than single fixed 1 step returns?<BRK>Summary:This paper proposes Mixture Bootstrapped DQN (MB DQN). It shows better performance in several Atari environments, compared to bootstrapped DQN. This work also attempts to analyze the source of empirical benefits in terms of attention areas of the agents, the quality of data sampling, and the way of utilizing different backup lengths. Reasons for score: (+): The proposed method is tested considering several different aspects, including the agent’s attention area, data quality, and the sensitivity of parameters. These experiments provide a deeper view of the MB DQN and try to explain why this new method helps with learning. I find this assertion rather flawed as it suggests that inefficient learning is an avenue for better exploration. Deliberately slowing the learning process such that even exploitation appears exploratory is a defeatist approach towards the exploration problem. Can we see the benefits clearly in the tabular/linear function approximation setting, or make a theoretical statement about the benefits? Section 4.2 suggests that MB DQN helps by changing the data distribution in the replay buffer, can we see the same effect in the tabular/linear function approximation setting, perhaps in the domain used in Figure 1? ( ): The experimental results are averaged over only 3 different random seeds. More importantly, 3 runs are not sufficient to establish the statistical significance of the results.<BRK>Multi step RL has been around for a while now, and this work attempts to extend it further to a mixture setting by exhibiting a performance boost when the proposed technique is used with Bootstrapped DQN. Why Bootstrapped DQN:Provides easy incorporation of the multi steps since it has multiple Q heads, and the authors could assign different multi steps to them. The different sections have a “natural” flow to them and the reasoning is clear. To prove the advantage of using the algorithm, it has been tested on 8 of the Atari games and it exhibited better performance in several of them. A qualitative comparison was also performed based on the attention map visualization for 2 Atari games and it showed that MB DQN infers more relevant sate space regions. “However” points:The critical point is that the performance of the baseline Bootstrapped DQN is different from the one reported in the original paper. (Slight indentation problem in the first hyperparameters table that pushed the head return value to the first row)Verdict: This is an interesting and powerful method. More extensive experiments would have made the proposed work more convincing.
Accept (Poster). rating score: 6. rating score: 6. rating score: 5. <BRK>The key theoretic result is the derivation of the sufficient conditions for decentralized attribution and the design of keys following these conditions. Results are validated on two datasets, MNIST and CelebA. Overall, I am more positive. I am willing to raise my score to 6. However, the paper is still somewhat borderline. The derivation is on the sufficient conditions. DecisionThe paper seems to present interesting results on decentralized attribution of generative models.<BRK>Given the rebuttal, I am willing to raise my score to a 6 due to the added StyleGAN, PGAN, and other experiments and improved paper layout / clarity. The added experiments are welcome addition to the paper and demonstrate this technique. The lip and eyestaining are interesting results and I do hope this direction gets explored in the future.<BRK>This paper proposes a decentralized attribution to the generative model trained on the same dataset. Furthermore, this paper provided theoretic insights into the proposed method. The author tempts to distinguish the generative models trained on the sample dataset. This problem is interesting and inspiring to the researchers in the related field. However, the presented work is not related to such decentralization.
Accept (Poster). rating score: 8. rating score: 6. rating score: 5. rating score: 4. <BRK>The way, the pipeline is described, the processing, including imputation and feature selection are performed on the full dataset, which is then passed to the modeling phase. This has to be stated in the limitation of the study design. Having a team science approach with clinician scientists as part of the team is integral part of the study, which seems that this paper is all about. ex: “While issues such as data cleaning, algorithmic fairness, and privacy and heterogeneity have import, they are beyond the scope of our software.”  revise the sentence “have import”. Finally, I am not a software engineer and will leave that level of evaluation to my colleagues.<BRK>The manuscript introduces and illustrates an end to end software pipeline, called Clairvoyance, for medical machine learning on time series data. The authors must be congratulated for having designed and developed this wonderful resource to accelerate the adoption of these computational techniques in clinical practice as a way to support people’s judgement and decision making. The manuscript excels in describing and relating its contributions with related work. It has also included a convincing set of experimentation on datasets from three medical environments that are supplementary to each other. However, the submission is excellent, and the program committee should discuss this case further.<BRK>In this paper, the authors showcase a pipeline intended to standardize and industrialize AI model development and testing for medical time series. It is very clear that the authors have put in a tremendous amount of work in building their pipeline. As far as the *paper* is concerned, however, I’m not sure if the paper (as it is written) and the venue (ICLR) are a good fit. 2. benefits of using this pipeline over other pipelines or no pipeline: the authors benchmarked their pipeline’s performance over off the shelf ML models for some tasks, which is great!<BRK>##########################################################################Summary:The authors present a new package aimed at improving the design and validation of pipelines using medical time series data. The package, as depicted in the paper, appears to be very comprehensive and well motivated. ##########################################################################Rationale for score: The primary reason for my recommendation of reject is this paper is ill suited for a venue such as ICLR. The paper is a descriptive paper for a new software package and it is aimed at the healthcare/informatics sub community. Moreover, the authors do an excellent job at motivating the need for such a pipeline not only as a tool but as a means of standardization and benchmarking, something that is sorely needed in many healthcare applications of machine learning.
Reject. rating score: 6. rating score: 6. rating score: 6. rating score: 7. <BRK>Summary:This paper tackles robust RL under the multi agent setting. They formulate the multi agent adversarial robustness problem as a nested optimization problem and propose a practical algorithm (ERMAS) to solve it. Theoretical proof and empirical study on two environments are provided to demonstrate the effectiveness of the proposed framework. ##########################################################################pros: + The paper is well written with clear and interesting motivation. Bridging the gap between simulation and reality for multi agent environments is an important topic. Although I m not an expert in this direction, it feels to me the proposed framework is technically and theoretically sound with comprehensive empirical evaluations. ########################################################################## cons:   It would be better to include more baselines from the MARL field, especially the ones you cited in related work that also considers robust MARL. Only a vanilla MARL baseline cannot actually tell how much you improve empirically from SOTA. It would be better to include literature from the nested optimization field in the related work. Since you formulate the robust MARL as a nested optimization problem, have you considered or compared ERMAS with any other existing nested optimization solvers?<BRK> ##########################################################################Summary:This paper purposes a robust optimization framework for training robust policies, such that policies learned in imperfectsimulations can be robust against reality gaps in order to be effective in the real world. The multi agent adversarial robustness problem is derived using \eqsilon Nash equilibria; the paper purposes ERMAS, and present how ERMAS solves the nested robust optimization problem using dualization, trustregions, and first order meta learning techniques; the authors  empirically validate ERMAS by training robust policies in two multi agent problems: sequential bimatrix games and economic simulations. Overall I think the reality gap is a very important problem, and the paper is well organized and presented clearly. I have several concerns about this formulation (stated below)2. The robust optimization formulation requires an input of the size of the uncertainty set. However I did not find discussion in the paper on how such a bound on the uncertainty set can be obtained or estimated in practice. 3.I would like to see more discussions on the approximate feasibility and optimality for solving the robust optimization problem eq(7). 4.In the experiments (figure 2), it is unclear that if the average per time step regret experienced by the agents increases as the reward slack \epsilon increases (the trend becomes decreasing as \epsilon is large) ##########################################################################Questions during rebuttal period:  Please address and clarify the cons above  ######################################################################### post discussion  The authors  response are helpful in addressing the concerns in the original review.<BRK>This paper proposes an interesting method for being able to act and plan robustly in a multiagent simulation and be robust to the reality gap between training time and testing time for agents in a marl setting. the method does show improvements in terms of being able to train the policy for this use case and being more robust to some out of distribution configuration of the environment however these improvements appear to be rather limited. in addition, the organization and writing for the paper is very technical and could be improved with additional background information on the uses of metrics and environments as well as better flow between the content in the paper to understand the importance of the different aspects of the method. The paper appears to be very technical. The impact of this paper on a more general audience is going to be limited due to the number of details left out of the paper that would help motivate the reasoning for using certain metrics for learning why you would want bounded regret and Nash equilibria. For example, in figure 2 all of the plots have different quantities on the x and y axis that are only briefly described and not very well motivated why these are good metrics for analyzing this type of algorithm. Additional Comments:  The list of contributions at the end of the introduction seems to be fairly repetitive from the content in the last paragraph in the introduction. The part the connects the work to TRPO is not discussed carefully paper. Is the method just using TRPO for multi agent RL? The paper has also been edited to improve the motivation and experimental explanation.<BRK>Unlike previous work which allows for perturbations in other agents’ policies, this work allows for perturbations in the reward functions that those agents optimize, allowing for more relevant robustness. With this, it is no longer necessarily the case that a Nash equilibrium exists: Nash equilibria are only guaranteed to exist when the utility functions are of the _game outcomes_; they need not exist when the utility functions can also apply to the _chosen policies_. It is easy to see that there is no Nash equilibrium where either player plays a stochastic policy, as they could then switch to a deterministic policy. I was confused by the fact that ERMAS outperformed MARL even when there is no change in agent policies. The authors note this fact as well:> In fact, ERMAS outperforms the baseline AI Economist for the original setting of η   0.23. It is straightforward to show that the optimal policy in the MARL setting does at least as well as the optimal policy in the robust setting. This could happen in one of two ways:1. If it’s the second case (especially hyperparameter tuning), then it calls into question whether any of the improvements in the experiments come from the design of ERMAS, rather than coming from something unrelated like better hyperparameter tuning. I liked the algorithm derivation (though I did not carefully check the math), and the evaluation does show the benefits of the approach, though it would have been nice to test the approach on more environments (there is just one toy environment and one more complex environment). Clarity: I found the paper to be quite clear. Originality: I believe this is a novel formulation, though I am not very familiar with the robust RL literature.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 7. rating score: 6. rating score: 4. <BRK>In this paper, the authors propose a general approach for image completion with large scale missing regions. The presented approach has demonstrated strong performance in the image painting with large scale missing pixels and some image to image translation tasks. A new metric P IDS/U IDS is proposed to evaluate the perceptual fidelity of inpainted images. Strength:  The idea of co modulation is quite interesting and has demonstrated strong results in various tasks. Other image inpainting methods such as gated convolution, partial convolution can be also evaluated.<BRK>This paper proposes an image completion method that can deal with large scale missing regions. The paper also proposes new image quality metrics, Paired/Unpaired Inception Discriminative Scores (P IDS/U IDS), for measuring the image quality of the inpainted images. Overall, I like the idea and the paper’s results. The idea of extending modulation to co modulation is interesting. **Weaknesses**  Although the experiments show a good correlation of the proposed P IDS/U IDS metrics to the user study results, it is not clear how well the metrics reflect perceptual fidelity. Do these metrics work best for measuring the quality of inpainted images? Although the main target is image completion, the paper claims that the proposed co modulated GANs also works well on image to image translation.<BRK>In this article, authors proposes to bridge the gap between image conditional and recent modulated unconditional generative architectures with a generic co modulated gan architecture. They also proposes a new inception score based on linear SVM in order to measure linear separability in a pre trained feature space. The paper is easy to read and the experimentation part supports the claims made in the theoritical sections. I have one question though: Why using a SVM in the computation of Paired/Unpaired Inception Discriminative Score (P IDS/U IDS) and not a simpler linear model ? How the training of the SVM goes especially on large datasets ? (not in term of accuracy but in term of computation power)<BRK>This paper proposes generator architecture for image inpainting using co modulation, which is similar to the weight modulation in StyleGAN2 but is conditioned on both the input image and the stochastic variable instead of only the stochastic variable. Update: Thanks for the response from the authors. Cons:   The novelty of this paper is a bit limited. It seems to me that the proposed co modulation is a straightforward extension of conditional modulation with stochasticity.<BRK>The paper identifies the following problem: current inpainting methods are suitable for small missing regions, but do not do well for large missing regions. I think the exposition is outdated and does not consider new work published at CVPR 2020, ECCV 2020, and possibly other venues. "Image2StyleGAN++ ..." show the inpainting of large regions as application and specifically mention the large scale inpainting problem. The paper proposes an architecture modification the authors call co modulation. The idea is to have the normalization of the generator layers not only controlled by either a random vector, or an input image, but by both. The paper also proposes a new way to evaluate GANs using the proposed P IDS and U IDS score. The main idea here is to use a pre trained feature transformation (the inception network) on real and fake images and then to evaluate the images using a linear classifier. The user study is more meaningful and gives some indication that the new metric is better.
Accept (Oral). rating score: 9. rating score: 8. rating score: 7. <BRK>This paper presented a novel kernel decomposition for nonsymmetric determinantal point processes, which enables linear time of inference and learning w.r.t.the cardinality of the ground set M. This is a significant improvement over previous arts and makes NDPP practical in relatively large datasets. This paper is well written and easy to follow.<BRK>This paper propose a decomposition for non symmetric determinantal point process (NDPP) kernels (M*M) which reduces the requirements of storage and running to linear in cardinality (M). Additionally, they derive a NDPP maximum a posteriori inference algorithm that applies to both their proposed kernel and the previous work (NDPP). Pros:	○ This paper is well written and easy to follow. ○ The author provide sufficient calculation process and relevant proofs of scalable NDPP. ○ For the existing problems of traditional DPP method is time consuming, need large memory and could not apply to large set, the scalable NDPP really solves them (e.g., this method could run on Instacart and Million Song datasets).<BRK>Nonsymmetric determinantal point processes (NDPPs) received some attention recently because they allow modeling of both negative and positive correlations between items. This paper developed scalable learning and MAP inference algorithms with space and time complexity linear in ground set size, which is a huge improvement compared to previous approaches. The inverse of $C$ appears in the gradient of $Z$.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 6. <BRK>By leveraging the monotone operator theory, the authors propose to use the forward backward forward method so as to avoids the notorious limit cycling problem. The classical FBF method requires two gradient evaluations per step, the authors introduce a new algorithm which reuses the past gradient in the same way as OGDA. In the setting of convex concave minimax optimization, the authors claim to prove novel convergence rates for both methods. Review: This paper is well written, though I think the difference with extra gradient can be made much clearer. Could the authors comment on that and explain why FBF could achieve the rate of O(1/sqrt(k)) in the stochastic setting (while EG fails)? I m willing to increase my rating if the authors could resolve some of my concerns, especially my concern on the novelty of the analysis. I m still concerned with the novelty of the paper given there are similar results for EG/OGDA.<BRK>The main results are an asymptotic convergence results and a non asymptotic convergence results using a restricted merit function. This splitting, called FBFp, is indeed new, and has the potentially of being of practical relevance. Cons:   The paper takes a long time until it becomes clear what actually the monotone inclusion looks like. The stochastic FBF has been studied in Bot et al.Mini batch Forward Backward Forward Methods for solving Stochastic Variational inequalities, forthcoming in Stochastic Systems. Note that the Arxive version of that paper is available since 2019.<BRK>Since ICLR is a highly selective conference,the originality and significance of one submission will always be in the first priority. I cannot accept this paper in current state. Later the author restrict Equation(1) to a deterministic version,which means that the noise input of GANs will no longer be considered. The noise input is an important ingredient of GANs. Except Algorithm 3.2 and Theorem 3.2 which suddenly provide stochastic versions,all the following results are on this deterministic version. In my opinion, the authors should provide more explanations on this point. Before Equation (2), the authors didn t explain what is monotone and what is monotone inclusion. In Section 3.2,the authors provided a generalized FBF algorithm. If we intend to do theoretical contributions,we should try to prove the theoretical properties or convergence bounds for the existing useful optimization methods.<BRK>2) This lack of connection is reflected in the experimental section as well. The main contribution of the paper is proving novel convergence results for Forward Backward Forward (FBF) algorithms as well as Optimistic Gradient Descent Ascent (OGDA) based on tools from monotone inclusion problems. 3) It is not clear that the connection of OGDA to FBF and monotone inclusion provides any new insights about the convergence properties of either method.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 5. <BRK>This work proposes Distribution Embedding Network (DEN), a meta learning model for classification. DEN is designed for the setting where data distribution and the number of features can vary across tasks. However, the experiments do not sufficiently show how much each of these components contributes to performance. I feel that both the claim and proof are obvious.<BRK>This paper tackles the problem of meta learning, namely learning from labelled datasets across related tasks, aiming for adaptation to unseen tasks, with little data. The proposed architecture (DEN) contains three building blocks:1. A classification module that aggregates the input features and the distribution embedding, taking the form of a Deep Sets (DS) network. The writing of the paper is very clear overall. Authors do not mention why the set representation and related topology are adapted to pairs of features and labels. I recommend a reject for this paper, on the grounds that the architecture design needs further analysis (as explained above).<BRK>############################################ Summary   This paper provides an interesting model that can be used for a meta learning situation where both data distribution and the number of features vary across tasks. is written. My main concerns are the clarity of the paper including the objective of each module design and that experiments are not enough to demonstrate the effectiveness of the proposed method. Also I think the baseline models used in the experiments are not enough strong.<BRK>Summary:The authors proposed a method for meta learning that produces a distributional embedding of the task, and then uses this information to perform few shot classification. Weaknesses:* The motivation for this method is not very well explained. Does the proposed method have any inductive biases or other desirable properties? * The experiments are hard to interpret. Several nontrivial details seem to be omitted   for example, where do the features that are input to the proposed method come from? Overall, I would lean toward rejection based on my current understanding of this paper, but I would be willing to revise my score based on the authors  response & the other reviews.
Reject. rating score: 4. rating score: 4. rating score: 6. rating score: 7. <BRK>The Table 1 is an incomplete definition. is given a representation F(X) E as input. c)	There is a simple alternate fix for the entire problem:The space of problems that authors want to fix abstractly stated are when the environment label (domain label) itself is so strongly correlated with the label that the IRM is encouraged to use environment as a representation. and conditional independence made by the authors: Conditional independence (Y \perp E | F(X)) is a necessary condition but not sufficient for the theory of IRM to work. Originality: The authors have proposed a new CMNIST dataset and a new algorithm to fix it. The right claim to make is that CMNIST+ does not satisfy the assumptions IRM makes for the method to be successful. However, this is easy to fix as I explain soon. Before moving to the next section, I would like to also make another important remark. Say for the two environments, the support of the feature distributions do not intersect, the support of the label distributions do not intersect.<BRK>And it does not clearly depict what s different compared to the standard scenarios amendable to IRM. ** The author(s) have identified a major weakness of IRM that I also find concerning: while developed from the notation of invariant representations, based on which invariant predictors are defined, IRM does not explicitly regularize the representation in its formulation. The discussion needs to be substantially enriched. What I have expected is some theoretical discussion, instead, the author(s) have provided a numerical table computed from "theoretical computations". I do not consider these as theoretical as they do not generalize beyond this particular example. **The term out of distribution (OOD) is a bit misleading, as this phrase is usually associated with the task of anomaly detection, where novel samples that are very different from the training examples are identified. I would suggest the author(s) replace OOD to avoid confusion.<BRK>The core idea is to address the existence of spuriousness correlation by introducing the MMD and KL divergence based conditional distribution matching constraint to the IRM learning process. In general, the paper introduces an in depth discussion of the limitation of the IRM learning mechanism and points out the root cause of failure of IRM (spuriousness correlation). This is interesting and potentially impactful for practical OOD learning tasks. Still, our concerns are as follows:1. How would this method perform in a domain transfer learning task? 2.Following the first question, we would expect some discussion about the relation between the proposed method and other transfer learning methods, such as meta learning methods. Could domain invariant casual feature learning be considered as a way of conducting meta learning? How are they defined?<BRK>Then, consider a test domain where you ve an equal proportion of Y 0 and Y 1, but all Y 0 are colored G and Y 1 are colored by  B. 2.The paper provides an intuitive argument for why IRM does not work on this dataset: the IRM constraint is equivalent to "class label independent of domain label given feature representation", such a constraint does not preclude the classifier from learning "feature representation   domain label". Such a classifier however would not generalize well to an unseen domain where the domain label is not correlated with the class label. However:    I d strongly encourage that you consider trying similar experiments on a dataset like say Rotated MNIST (or Rotated MNIST+ to be more precise, if at all possible). The text says "Theoretical analysis shows that this [label balancing] is an invalid solution". Good luck to the authors. **Further updates**: I d like to elaborate on my thoughts a bit more with the hope that the authors may find it useful for future versions of the paper. I wish to emphasize that, after a long discussion with R3, I ve some strong disagreements with their review regarding the "simple fix":       I personally think EIIL is out of scope as it is a recent algorithm.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 6. rating score: 6. <BRK>In this paper, the authors mainly show that the adversary can force the GNN to classify any target node to a chosen label by perturbing another single arbitrary node’s feature in the graph. However, there are several concerns about the paper:1. The novelty of the paper is rather limited. The paper simply uses the gradient attacker method to add continuous perturbations to the node attributes. The authors should also consider add more baselines as the attack methods. "Certifiable robustness and robust training for graph convolutional networks."<BRK>The paper studies the problem of adversarial attacks in graph neural networks. 5.Since the authors claim a single node attack is as effective as a multiple node attack, multiple node attacks, i.e setting the number of attacker nodes to a larger value, are suggested to be included as baselines. The studied problem (adversarial attack on graphs) is important and the single node attack setting is interesting.<BRK># summary #This paper studies a problem of attacking graph neural networks. Especially, the paper focuses on the single node attack while most of the attack studies focusing on edges. The paper claims that attacks on multiple edges (and nodes) are difficult to conduct in practice. Considering the single node attack is important since injecting a new single node, or a manipulation on one vulnerable node is less difficult. The studied problem is an evasion attack on a node feature vector of a single node. I find no obvious concerns about the methodology. An average degree (number of edges a node has) will be a good measure to check this issue. This conflicts with a naive guess: multi node attacks is stronger than single node attacks. ( ) No explanation in why two nodes attacks are not superior to the one node attacks.<BRK>Just some minor comments, I sometimes got confused about  attacker node  and  victim node . This paper studies how to attack a specific attacker node by perturbing only one single node s feature. Compared with other attacking strategies, the attacking scenario in this paper is more realistic, e.g., the access to a single node (hacking the account), etc. In the paper, the authors claim that it can attacking by perturbing single arbitrary node. What would happen if the number of layers in imitation model is different from the attacked model?<BRK>This submission shows that a single node attack for GNNs can be surprisingly effective. It can potentially complement our understanding of GNNs and bring novel ideas in the future. Friendly to even the readers who don’t follow the field of adversarial attacks closely. W3: The submission is mostly about new insights. It, however, barely invent any novel techniques.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 6. <BRK>In this paper, the authors derived a quantitative measure of transference among tasks in the setting of multi task learning. Based on this measure, two methods were subsequently proposed, one to find group of tasks that may benefit from collaborative training, the other to combine task gradients at each training step. Despite there are typos in a few places, the paper is good written and easy to follow. No comparison with any baselines was done to show the advantage of the proposed method to identify task groups. One such baseline can be first training each task separately, then for each trained network fixing the shared parameters, resetting and adapting the task specific parameters for other tasks, and cross comparing the performance of all obtained models to identify task group for which collaborative training could be beneficial for individual tasks in the group. I was not able to find any guidelines in the paper for how to construct the candidate set J. I guess because of this, as the authors mentioned, their method is not scalable, only can handle problems containing small number of tasks. The empirical results are very weak. Such weak results do not justify the significantly elevated computational cost.<BRK>This paper proposes some interesting observations and proposes a novel measure of transference. However, there are some flaws in the proposed method and the proposed methods seems not technically sound. My concerns are listed as follows. 1.The definition of the transference measure is problematic. However, in MTL, the negative transfer or positive transfer is proposed with respect to the generalization loss. This paper should give theoretical support to show that the proposed measure can indicate the influence on generalization loss. For example, this paper is closely related to the previous work [1]. However, this paper has not compared with it. The authors should add the comparison with [1]. 3.The paper is hard to follow. For example, how to do task grouping based on the measure of transference are not clearly written. [1].Yu T, Kumar S, Gupta A, et al.Gradient surgery for multi task learning.<BRK>This paper studies the transferability in multi task learning. They propose a metric, transference, to evaluate how tasks affect each other during multi task training, and a method called IT MTL which utilizes this metric to compute and improve lookahead loss changes. Although the proposed metric and method are interesting from a scientific point of view, there are a few key downsides (as the author themselves summarized in the conclusion) that require further investigation/improvements. Cons:1.The biggest concern is efficiency. The proposed method requires calculating lookahead loss for a set of combinations of tasks. While the author claim it only require O(m) possibilities empirically, it is not guaranteed for other datasets/settings. In fact, even with O(m) complexity could still be prohibitive for large models. To reveal the full picture, I recommend the author to demonstrate the actual time required for training for each method, and conduct an ablation analysis on the efficiency performance trade off. 2.From Table 2, I can only observe marginal improvements over prior methods. Considering the extra computational cost, it is hard to justify the effectiveness of the method itself. The idea of using lookahead method on shared layers in multi task learning has been recently explored in [1]. Although they use a different optimization process, the high level idea of improving transference/validation loss is shared. On the other hand, dropping gradients is also recently proposed [2]. 2.I would also recommend exploring some greedy strategies for the proposed method to be efficient. Wang et al., EMNLP 2020.<BRK>[Summary] This paper studies the problem of task relationship/transference in multi task learning, by introducing a quantifiable measurement based on relative loss updates. A (nonsymmetric) task transference between task $i$ and task $j$ then can be computed by measuring the relative change of training loss of task $j$, with the updated shared parameters from the training loss of task $i$. The proposed solution based on the relative loss update is intuitive, clean, and simple to implement. **Task selection. ** To achieve a maximal performance improvement, it seems that we have to compute task transference from all possible combinations of task grouping, which is exponential based on the number of tasks. Besides, in the experiment section, the number of tasks evaluated is no more than 3, and the improvements are quite marginal (mostly within 0.5%). ** In Fig.1, task transference is visualised by the pairing of all possible combination of two different tasks. So how can we know the transference for each individual task just by the transference for all tasks? The authors should elaborate on these details. So why introduce another notation $\mathcal{J}$ in Algorithm 1 to include all possible task groupings, this looks quite redundant to me. I hope the authors could elaborate on these questions and include more relevant details in the paper.
Reject. rating score: 4. rating score: 5. rating score: 7. <BRK>The authors apply algebraic geometry to program synthesis, by identifying programs with points of analytic varieties. They construct a smooth relaxation to the synthesis problem by considering the space of probability distributions over codes for universal turning machines (which is a smooth/continuous manifold), and then translate this probability into corresponding probabilities of generating a correct program. While the ideas are interesting, it seems like more work needs to be done before they could be of interest to practical program synthesis (admittedly, the authors do not claim that it is practical, but I am unable to judge the paper by quality of the theoretical work)<BRK>This paper is very densely written with a lot of heavy mathematical formalism and not enough schematics to explain how it works. I spent many hours and still don t understand the point that the authors are trying to make. They support their thesis with some theorems and assumptions around the geometry of program synthesis. The paper is based on this work https://arxiv.org/pdf/1805.11813.pdf which is outside of my research area. I would suggest a revision and a lighter submission of the paper, I am not sure it is appropriate for this venue<BRK>  quality : good  clarity : very good. I was worried about reading this due to all the math symbols, but it turns out the big pictures are clearly explained. The main claim of the paper is that when doing relaxations and attempt to view satisfying programs as the variety W0 of some mathematical function, W0 is typically larger than W0 \intersect Wcode. This is important because one may easily find a point of solution in the variety W0, yet it cannot be realized as a working program in Wcode. As I am confident that my expertise in this area is not high, I will not be providing an explicit pro/con list. I would instead ask a few questions to the authors and hope I can get a good response:  A natural thing to make program synthesis more amendable is by changing the set Wcode.
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 6. <BRK>##########################################################################Summary:The authors propose a new method for an RL agent based , interactive medical dialogue. As the authors argue, other factors are certainly important (reliability and robustness), but all are secondary to diagnostic accuracy in my opinion. The authors also propose two new metrics by which to evaluate medical dialogue systems. The proposed approach is novel and interesting and addresses an important problem. 3.The introduction of new metrics other than diagnostic accuracy is new and somewhat well motivated. From equation (3), it appears to be the average probability from k 100 bootstrapped models.<BRK>In this manuscript, authors proposed a novel dialogue system for medical automatic diagnosis (DSMAD) called INS DS. There are three components in the general DSMADs, which are NLU, DM and NLG. Is this due to the inconsistency between the ground truth diagnosis and the diagnosis made by students? Authors have evaluated the robustness and reliability of DSMAD agents based on proposed metrics. This is a general and widely accepted approach to demonstrate the agents  robustness and reliability. The performances regarding I.D. may be different across these two sets but there should be a reasonable overlap.<BRK>This paper proposes an interesting medical diagnosis dialogue system. Furthermore, the core modules of the paper are essentially same as Xu et al.2019 as also acknowledged by the authors in Section 3.1. The paper also proposes two evaluation metrics based on reliability and robustness of the models. Weaknesses:This paper appears to be an incremental piece of work based on the prior work on the same task.<BRK>The paper proposes a method for automatic medical diagnosis in a dialog system which is comprised of two modules: one which proposes symptoms to inquire about, and another which decides whether to go ahead with the inquiry or inform a disease. Are they accuracy results on the noisy test sets? Overall, the paper is well motivated and written, and the authors show equal or better results compared to other state of the art methods.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 5. <BRK>The paper proposes BaNNER, a new transfer learning approach to accelerate hyperparameter optimization based on related tasks. 2.**Non challenging problems**. A number of experiments against standard BO and random search, as well as against ABLR, the most closely related transfer learning baseline, indicate that the proposed approach tends to find a good hyperparameter configuration more quickly. Also, it is not clear how robust BaNNER is to noisy or unrelated tasks. c. The method comes with a number of hyper hyperparameters, which the authors learn on meta data by running an additional BO loop powered by BOHB. Without these comparisons, it is not clear if BaNNER should be the transfer learning method of choice, considering the many alternatives. Overall, I vote for rejection for the current version of the paper. There are also some more recent closely related baselines that are not compared against nor discussed in the related work. In the considered experiments this is not necessarily unfeasible.<BRK>Minor quibblesPage 2: The *most* data efficient class of algorithms for this setting are Bayesian optimization (BO) algorithms... If the aim of this paper is to set up an informative prior, it would be necessary to produce empirical comparisons with the state of the art probabilistic meta learning algorithms in terms of probabilistic predictions, which is lacking in this paper. There are a number of major concerns about the technical formulation and the experimental setup and results, as detailed below. Furthermore, g_theta(x) is not defined. The authors have discussed this only briefly in the second last paragraph of Section 5.2. The authors  justification in the second last paragraph of Section 5.1 was not able to allay my above concern. In meta learning, this seems fine.<BRK>In the paper, a probabilistic method is proposed for Bayesian optimization transfer learning (or meta learning). My biggest concern is the novelty of the proposed probabilistic approach. I think a very clear distinction between this method and Neural processes needs to be made. Therefore, NP should serve as the most relevant baseline to compare with in terms of experiments and methodology. Typo: Sec 5.1, strongly "form" any meta learning model  > from After author s response As the other reviewers also pointed out, some baselines are missing and ablation studies on meta data are missing as well. Meanwhile, my concern is that the presented idea in the paper looks very similar to that of the neural process, except the paper is optimizing over a point estimate of z while NP is optimizing over q(z|D). This remains unexplained in the author s response. Therefore I am keeping my original evaluation at the moment.<BRK>In this paper, the authors introduce a technique for multitask Bayesian optimization based on meta learning. This paper falls in to the latter approach, learning a meta learning style task embedding (eqn.1) to be able to make predictions over a new task given auxiliary task data (eqn.2).Overall, my biggest concern is whether the empirical comparison is as well set up as it could be. The total dataset size here is significantly smaller, which (1) does not match the setting ABLR was originally used for, and more importantly (2) is well within reach of multitask BO. Does this refer just to the mechanism for setting BaNNER hyperparameters and not to how test tasks / data sets were selected, or were training tasks actually reused as test tasks? If my assumptions about the empirical set up of the paper are correct (e.g., 4096 and 16384 meta examples), then my biggest issue is whether the playing field that ABLR was compared to was level. Some of the standard error plotting (Forrester ensemble, GLMNET1471 in the main text, much of figure 8 in the supplementary materials) is clearly broken for some methods.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 6. <BRK>Expressions are represented by trees and equivalence is proven by a sequence of axioms applied to the first expression. In any case, 5 and 10 steps are very few and claims of robustness and generalization are weak with such few steps in the required proofs and such a large overlap in the steps required for pains in both datasets. * There is no motivation for using a deep model over another path finding method on the constructed graph that connects the input programs. The idea of GNN based path finding for expression/program equivalence proving is promising, but the methods are not clearly and rigorously explained and the experimental settings are too weak to support the claims of generalizability and applicability to realistic scenarios. If this is not the case, could you please clarify the training set up? 3.What are the evaluation metrics used and reported? If the work is not referenced as part of the text of the sentence, it should be in parentheses for clarity. Typos:Pg 5, “production rulse" should read “production rules”Pg 5, there is a sentence that begins with “E.g.” which should be replaced with the grammatical “For example,”.<BRK>The authors introduce a new synthetic dataset of equational proofs over the basic axioms of linear algebra. The appendix refers to supplementary material including code and data, but no supplementary material was submitted. It is a fine paper, but ultimately I do not think the work is sufficiently interesting to merit publication at ICLR. As far as I can tell, the entire synthetic dataset is easily decidable by traditional approaches, e.g.by simplifying and distributing modulo AC then checking for syntactic equality. Other miscellaneous comments:  The novelty claim in the abstract seems overstated. Later in the intro: "[the authors  new method] can deterministically prove equivalence". Page 2: I think `P1 ≡ A1(a, P1)` should be `P2 ≡ A1(a, P1)`  Page 4: Why is it necessary for datapoints not to share `P1`s? What are the exact differences between this architecture and the pe graph2axiom model? It seems odd that they do not compare against transformers.<BRK>The prediction can be validated by a simple checker so that any false positives can be eliminated. I like this work in general but hesitate to recommend acceptance before a few concerns can be addressed. Detailed comments:  Quality: The authors present a comprehensive discussion of related work. Q6: Can the system prove equivalence between "1 + 1" and "2", or a slightly more complicated example, "a + a" and "2a"? However, comparison with baseline approaches seems arbitrary. It is not clear what data is used for the mini ablation study shown in Table 3. The claim of "generate axiomatic proofs of equivalence between program pairs" is a bit over claimed and misleading, as programs are way complicated than algebraic expressions. How is it related to the "incremental/non incremental" thing? Significance: This paper investigates a very important research topic, i.e.deep learning for symbolic reasoning, and proposes an interesting dataset, which could motivate many future works in this direction. Questions:Q0: How is the model trained?<BRK>The authors present an algorithm for proving equivalence between algebraic expressions. I think it is very positive that the authors implemented it in OpenNMP py. Comments* On page (2) the claim (ii) is not novel in the sense that it produces a sequence of the proof, but because it uses the OpenNMT py framework. This is actually the way they generated data. I think there are other competing methods that are much more successful in theorem proving. This one seems to be more appropriate for compiler optimization. Also, the authors should give a direction of how they can solve this limitation. I think the paper needs more graphics, given the complexity of the algorithm* I don’t understand the use of the Graph Neural Network. * The related work section is sufficient* On the positive side of the paper, I liked the comparative study between incremental and non incremental proof.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 6. <BRK>This paper could be presented better, as the motivations of the work and the description of the method lack clarity and effectiveness. Indeed, the agent is learning to explore states under a certain criterion, i.e.minimizing the accuracy of the observation, closely reminding all the literature about intrinsically motivated exploration, that in this paper is only cited in the related works. Pros * The paper addresses an interesting problem that can potentially improve sample efficiency in deep RL problems. Cons * Poor description of the methodology, in particular explaining its connection with intrinsic motivation;* No deep RL problems considered;* No comparisons with methods in literature. I agree with the authors that intrinsic motivation is different, and perhaps in my review I expressed this concern too strongly.<BRK>I found the paper fun, and well written/edited. So, it s essentially a kind of "ML for scientific visualization" paper. The ML novelty appears small standard algorithms and test problems are used. are nice, but not really impactful. The earlier work isn t able to solve these problems as well as the current paper, but the model is very related. I m confused about the state used in the experiments.<BRK>Comments   I believe that the problem considered in the paper is interesting and follows some recent work on "tuning" MDPs (see ref[a] below).   Overview   The paper proposes a reinforcement learning algorithm that enables an agent to "fine tune" the quality/accuracy of its sensors to its current task. Now I may be missing something, but it seems to me that, from the agent s perspective, this is equivalent to adding noise to the dynamics of the environment, since the agent treats the observations as state.<BRK>In contrast to standard reinforcement learning (RL), the paper investigates the variant where the observation made by the agent about its state has a cost. The authors propose to model the problem as a POMDP with an augmented action space (normal action + observation accuracy) and a new reward function that is defined as the original one penalized by the observation cost. PROSI find the research questions asked in the paper interesting.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 8. <BRK>In conclusion, I think that at the moment this is a borderline paper. The approach is definitely interesting but right now the experimental evaluation is not solid enough to recommend this paper for acceptance. I highly recommend the authors to revise the paper and I m looking forward to authors  response. Comments:1) How many additional interactions from the environments does this approach require? Is it the same entropy term as in SAC? I think that the paper could significantly benefit from additional experiments that demonstrate that the proposed formulation is better than simply using (7) for state action distributions. 5) Table 2 demonstrates that smaller values of $\lambda_f$ lead to better performance (with an exception of Ant but the difference is within the standard deviation). The figure table 2 can benefit from including results for $\lambda_f 0$.<BRK>* __Section 5.1: Although this does not seem to be a central claim of contribution in your work, I am wondering if your claims of sample efficiency are well founded. * __Section 2.2, Theorem 1: Assumption 1 should definitely be included in the main text. This is quite a strong assumption (even if it holds in many physical/robotics problems), and the reader should not have to look into the appendix to realize this. In a non deterministic transition setting, the mutual information term could seek out states where the dynamics are more determinstic, but I don t see why that would be a good thing necessarily. * __Table 2: $\lambda\_f$ is tuning the knob for the stregth of the mutual information maximization term. * Section 3:  * You are using SAC as your RL algorithm. Is there any concerns one should be aware of?<BRK>This work proposes a novel density matching method for learning from demonstration, which achieves state of the art demonstration efficiency. The experimental results are mostly complete and convincing. The improvement in the final performance over state of the art is demonstrated. Although the sample efficiency is not the focus of this work, it would be helpful to see the averaged learning curves as in the resultsin related works, which can give a straight forward intuition on therobustness of the proposed methods. of the reported results in Table 1 and other tables. Does the improvement come from the better density estimation of the expert s policy? I didn t see too much difference in terms of final performance between 1 and 25 expert trajectories. 2.Could the author explain the reason why one expert trajectory can provide a good density estimation of expert policy?<BRK>Most of my concerns were clarified and I still think the paper should be accepted. However, I agree with Reviewer 4 that  additional experiments would be good to better tease out the reasons for this method working. The proposed approach involves density estimation to learn a surrogate reward function that can be optimized via RL. ##########################################################################Reasons for score:  This paper provides strong theory and strong empirical results validating the theory. GAIL like methods are notorious for their instability so having non adversarial IL methods is a significant improvement. 2.Significant improvement over state of the art IL approaches. It would be nice to show that the authors method qualitatively imitates a variety of behaviors rather than just being able to go really fast without falling down. 3.Quite a few knobs that need to be tuned in terms of hyperparameters.
Reject. rating score: 4. rating score: 5. rating score: 6. <BRK>I understand that new experiments broadly are out of the scope of this paper with length considerations, but it would help the paper flow if there are intuitions for why this matters broadly. 2) This paper heavily relies on previous knowledge of Pan et. This paper is well constructed, but these unclear boundaries make it difficult to accept for publication. HC Dyna is very recent work with only one small difference. Comments:1) The authors make some loose claims about model based reinforcement learning. 2) It should be said, this paper is well written at a low level. It would be interesting to include other baselines in such a well known task (such as cartpole). I think the paper needs a bigger revision and more results to be above the acceptance threshold.<BRK>Summary:This paper investigates the search control problem in Dyna style reinforcement learning algorithms. Strength:(+) The suggested method is based on the theoretical justification. (+) The analysis of the out dated priority is interesting and provides a better understanding of priority based RL algorithms. ( ) Related to the concern above, it is unclear how well Theorem 2 holds with parametric function approximation. More thorough experiment results can alleviate this concern; how does the average return for the roundabout v0 look like? ; how well does other Dyna variants algorithm work for the roundabout v0 and MazeGW? While the theoretical analysis is interesting, I am not fully convinced about the utility of the proposed algorithm. It would be useful to have more experiments in the widely used benchmark such as atari or have better motivation explaining why faster convergence would help in the RL context as it would in supervised learning.<BRK>The authors motivate the approach by a theoretical/empirical observation that the prioritized optimization of L2 loss is equivalent to the direct optimization of cubic loss. ##########################################################################Reasons for score: Overall, I vote for borderline acceptance. However, experiments fail to show how exactly insufficient coverage of the sample space is being tackled. Moreover, the empirical results in real domains fail to show a significant performance increase over Dyna freq when a learned model is used. Assuming access to the real model for updating the priorities may not be fair to other approaches when the metric in question is the sample efficiency. Hopefully, the authors can address my concern in the rebuttal period. The paper tackles one of the fundamental issues of training any RL agent   sampling mechanisms from the experience replays. For me, the problem itself is real and practical. 2.The proposed Dyna TD prioritization scheme is novel such that it actively searches for states with a high expected error. Moreover, it also provides a sampling method for sampling imaginary transitions from the model using the same method. Although the proposed method provides several ablation studies, I still suggest the authors conduct the following ablation studies to enhance the quality of the paper: (a) How does Dyna TD compare with Full update variants of Dyna Value and Dyna Freq prioritization mechanisms.
Reject. rating score: 10. rating score: 2. rating score: 4. rating score: 5. <BRK>This article proposes a novel approach integrating language throughout the visual pathway for segmenting objects according to referring expressions. The limitations of the currently dominant top down approach are well argued. The task of segmenting by referring expression is important and well chosen. The proposed model is sound, and well described in the article, and the experimental results demonstrate that the model outperforms clearly the state of the art in all metrics. The qualitative examples provided are quite impressive and demonstrate the success of the approach. In sum, I feel this is a well written paper addressing a very timely and important problem in computer vision and AI research and should be of broad interest in the community.<BRK>This paper presents a model for image segmentation from referring expressions which integrates linguistic representations of the referring expressions both at low level and high level stages of visual processing. I vote for rejection, mainly on grounds of significance and quality, expanded below. The paper does not provide a clear motivation for their model. Here are some arguments I looked for but did not find:  1. Cognitively: There are some references to relevant cognitive science papers, but there seems to be little concrete inspiration taken from this or other cognitive work in the particular model design. 2.A priori based on the task: What would we expect to gain from using language in early stage visual representations? What sort of correlations might exist between particular types of linguistic input and low level visual representations? 1.The quantitative results don t seem to constitute an enormous improvement over past work. The variability in table 2 across evaluation sets makes me doubt the statistical significance of the claims. No statistical significance tests (across resamples of test data or across random training restarts) are provided. What are the contents of the linguistic representations, and how exactly do they modulate low level visual features? For reference, Hu et al.(2020, Figure 4) [1] and Hui et al.(2020, Figure 5) [2] both do some of what I m looking for here, showing the influence of language on the behavior of the model. OriginalityI don t closely follow the relevant literature and can t speak confidently on the originality of the model. I look forward to the results of the analyses the authors mention in response to R3 Q2, to better understand what exact interaction between language and low level visual input is being modeled. Along with R4 I remain unconvinced of the strength of the empirical results. Significance tests would not take too much time   it s not absolutely critical that you retrain the models for this.<BRK>This paper concerns the problem of image segmentation from referring expressions. The paper proposes to use language to modulate the image encoding and decoding process intensively, by applying auxiliary convolutional connections between the two branches and further condition the convolution kernel on the language embedding. Despite that the introduction of top down and bottom up language modulation significantly boost the baseline performance (Tab.1), the full model struggles to match existing works on certain metrics such as UNC testA/testB, UNC+ testB, and ReferIt, which put a question mark on the effectiveness of the work. The results on the validation set are promising but not as good on the test set, which indicates a possible over tuning of the model. A minor comment on the model part. However, what is the rationale for splitting the representation since each split does not attach to any particular abstract of the image feature (low level, mid level, and high level)? Besides, some numbers from Tab. The authors mentioned "We made this decision based on Mei et al (2018) which proposed our baseline model (the top down approach)", where the reference of Mei et al (2018) cannot be found in the paper, as a critical baseline. Therefore, I am lowering my rating to 4.<BRK>This paper proposes to integrate visual and linguistic features in both top down and bottom up modulation of the visual input. This is done by fusing two modalities while doing convolution and deconvolution operations over the visual input. Experiments on image segmentation from referring expressions in standard datasets show that the proposed approach achieves state of the art or competitive results. I believe the novelty and contribution are rather thin because many ways of the modeling language are not explored at all. S4 Section 4.2: It is not clear how each of these ablations was performed. al."Modeling context between objects for referring expression understanding." [2] Cirik et. Please either bring the figure from A.1 or add a comparison with a model from the literature where the other model is successful where yours is not to do a contrastive analysis on how your model can be improved. Q1: Section3: What s the effect of the number of layers for the model?
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 6. <BRK>The authors introduce a differentiable approximation for the optimal myopic adversarial formulation that leads to a better direction for the adversarial perturbation and increases the attack impact for bounded perturbations. Empirically, the authors show with experiments in various games in the Atari environment that the attack formulation achieves significantly larger impact as compared to the current state of the art. The conclusions drew from this paper are completely based on the experimental results. No major theoretical results have been reported to provide stronger support for the bounded myopic adversary. Also, the evaluation metric in the paper is Impact instead of returns, which is not popular in literature. I suggest the authors to include this in the appendix.<BRK>The approach is simple and straightforward with good numerical performance. 2.The writing is easy to follow and experiments are thorough. For example, the sensitivity analysis of the return / Q value with regard to the states might help. 2.How does the proposed approach compare to other more recent baselines such as Gleave et al.(2020)?3.Is the metric in (16) a common one? I have read the authors  response and the associated discussions, and based on that  raised my evaluation by 1<BRK>The paper could do a better job of communicating the assumptions and limitations, but assuming this is addressed then it seems worth disseminating to the ICLR community. + What happens with attacks that are non myopic, e.g.the "enchanting attack" of Lin et al (2017)? However, I am inclined to agree with other reviewers that the paper s contribution is incremental. My best guess is the method could be made to work, given that e.g.Pattanaik s method is not all that different and worked on MuJoCo. This section should really be supported by experiment, or have the claim reduced in scope to there being no *obvious theoretical hurdles* to its application. Paper should be more up front about assumptions being made. It s a good summary of some of the history of adversarial examples – but is this that relevant? Most readers (and certainly your reviewers) will be familiar with adversarial examples. This will make your method suboptimal (even in the myopic case). I think the paper is a good submission without fixing these limitations (though it would be much stronger if they were addressed), but it s important they re clearly signposted so that they can be addressed in future work. I consider this paper borderline but overall am leaning towards accept.<BRK>Pros:  The idea is effective, finding the right attack objective is interesting and surprising that was not considered earlier. A comparison to non myopic attack would make paper stronger (https://arxiv.org/pdf/1907.09470.pdf)  The legends in the figure are just too small to be readable  Would have been good to show attacks on more complex problems. Questions:  This attack is for a particular state, which state is chosen for this attack? Is it towards the start of the game or end of the game?
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 5. <BRK>The relevant work, introduction and main text allows the user to understand the problem easily. The paper introduces graph aligning the mediating layer to the ground truth labels and propose an algorithm for the process. QualityThe experimentation showing the effectiveness of using a mediating layer compared to the base seems adequate for the 2 datasets. However there doesn t seem to be a good comparison to previous work. Is this valid only during training and alignment? Why is the use of doing this? And why would we expect better results than not using domain adaptation? Is it an empty set in the beginning? In algo 1, input seems to mention there are already m strong labels before the start, does that need some correction? Do iteration 0 always correspond to the Oldenhof model or were they different models, not clear from the text other than mention that it was pre trained? Goal of the paper is to learn a function that maps an input (that can represent a graph on a 2D image) to the graph structure. Paper has experiments showing importance of mediating layer but doesn t make similar comparisions to previous work making it harder to understand if the mediating layer is really useful or not.<BRK>##########################################################################Summary:The paper describes a method to convert 2D molecular images to molecular graph structures, with applications in extracting raw chemical structures from journal articles and other publications. The paper proposes some domain adaptation techniques to reduce the amount of expensive ‘pixel wise’ labels required for training the segmentation network##########################################################################Reasons for score: Overall, I currently vote for rejection. How does the model actually construct the graph structure? Also, there is a lot of analysis on the image segmentation part of the model (eg figure 5, 6). **Information about the Indigo and Maybridge dataset could be provided in a more accessible way. In terms of pure performance, it’s not clear to me that option 1 is superior, for example, [ref 30] which directly predicts the molecular smiles from the 2D molecular image [option 2] can attain ~80% in the indigo dataset, while this proposed approach seems to attain only ~40%*Figures 2 and 3 show various performance metrics over multiple iterations of the re training.<BRK>The particular advance of this work is to include such intermediate representation based on a graph alignment approach that generates “strong” labels. Nevertheless, my major concern is about the clarity of the paper, but I think that beyond clarity, the lack of the full or at least partial code should be available from the beginning for reviewers. A specific motivation for the use of the mediating representation is missing. The authors repeatedly use “strong labels are expensive”, just for the sake of completeness, would be good to be explicit in this fact instead of assuming that the reader will infer what the meaning is. “We also assume the map E(v) which gives all allowed graph edits for the graph v” it is not clear. What is the origin on the such a different performance on the Indigo and Maybridge datasets. Fig.5 and 6 nicely summarise the good performance of the model, but in order to understand better the method and its limitations or type of graphs that struggle with, it would be good to present out layers where the system doesn t work.<BRK>The authors propose a domain adaption technique for self labeling for strong/expensive planer graph labels given normal labels. For the application of molecular graphs, a graph alignment method on the planer graph level is proposed to find an isomorphism with a minimal edit distance of the predicted strong labels. The results show the proposed method can gradually correct the strong labels and improve the prediction performance with interpretable explanations. However, there are several concerns about the paper:1. 3.The cost for the optimization problem argmin|e| is not discussed. 5.The generalization of the model is not discussed. For example, the background of chemical structure recognition and the settings could be introduced at the beginning of the paper. 5.The segmentation network uses 134K  images.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>The proposed method based on the observations result in significant experimental benefit in multiple classification datasets in GLUE. Overall: I think the paper nicely touches on positional encoding, an important component of Transformer based architectures, and the experimental results are encouraging.<BRK>This paper introduces an approach for the positional encoding of input tokens into pretrained contextual embedding models. They then introduce in equation 9 their modification to the positional encoding of the CLS symbol. Again, this is fairly straightforward, and likely leaves room for improvement but is a reasonable first step.<BRK>The paper takes a stab at fixing the correlation problem between the positional encoding and word embedding when they are added together in a transformer. The authors fix this by resetting the positional correlations related to CLS from others and to others from [CLS]. Original BERT model was evaluated on SQuAD and SWAG as well. The proposed method is called TUPE short for Transformers with Untied Positional Encoding.<BRK>This paper studies the positional encoding in BERT. The experiments are solid and well executed too. The proposed approach is built on top of BERT and evaluated on  the GLUE benchmark.
Reject. rating score: 4. rating score: 5. rating score: 8. <BRK>The problem is to learn a time series model from this, and models considered here are in the form of an SDE. In my first reading of the paper, I thought the topic was very interesting but I struggled to follow some parts of the manuscript, which I thought was overly unclear. From this I got more serious concerns. The authors write: "There are many existing models to learn dynamics of full trajectory data. Based on these strong similarities I do not think that the proposed manuscript is suitable for publication.<BRK>For the purpose of learning, the paper proposes to use a generator, in the setting of WGAN, to learn the drift term in the dynamics. The problem is interesting, but I found several issues about the proposed approach that need to be addressed. It is assumed that the diffusion coefficient is known and the objective is to learn the drift function. I think this is a strong assumption and learning diffusion coefficient is also very important and need to be included in this work. 2  Learning the drift function from aggregate data may have fundamental limitation that the paper needs to address. 4  Theorem 1 is not stated correctly and it is missleading. However, eq (7) is just an approximation. Also, Lemma 1 is well known and a reference should be given.<BRK>This paper proposes a new approach to learn the dynamics of density evolution of objects from aggregated data. The paper is overall well written and solves an important problem. I have only a few minor questions. 1.How prediction is made based on the estimated model? 2.Some notations are unclear in Theorems 1 1. In Theorem 1, what are the definitions of $N$ and $x^{(k)}$? Do you generate data for N times? In Theorem 2, I  assume that $n$ stands for $n$ steps ahead from t_{m_0}?
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. rating score: 5. <BRK>Weak points•	The proposed SAD metric seems to work just on the same network during its own training process, as it does not consider the fact that at different training runs the hidden neurons may develop differently. 4) The last phrase of the above mentioned paragraph and the next one are, up to my knowledge, accurate just for CNNs.<BRK>The proposed method supports to increase the accuracy of N:M fine grained structured sparse networks, and it can be applied in many diverse tasks regardless of pruning method like general unstructured pruning.<BRK>2.To validate outstanding performance of the proposed method, the authors did experiments on five different types of applications: image classification, object detection, instance segmentation, optical flow, and machine translation.<BRK>The novelty of the proposed SR STE is not enough, and the defined SAD lacks relevant theoretical support.
Reject. rating score: 3. rating score: 3. rating score: 5. rating score: 6. <BRK>This paper claims that decentralized parallel SGD (DPSGD) performs better than synchronous SGD (SSGD) and noisy version of synchronous SGD (SSGD*) in large batch setting. Experimental results on CV and ASR tasks show that DPSGD can outperform baselines when batch size is very large. Besides, there are many writing issues in the paper. (3) Some claims are ambiguous.<BRK>This paper gives some empirical comparison of centralized sgd and decentralized sgd for relatively large batch size. Furthermore, for vision tasks, evaluation on a single dataset (CIFAR10) is certainly not enough. The paper writing is problematic too. "While there was anecdotal evidence that DPSGD outperforms SSGD in the large batch setting". Which paper claims this?<BRK>Previous literature has also demonstrated that this gap can be bridged with longer training. The paper first sets out to intuitively describe this phenomenon by decomposing the noise in the gradient based updates. I ask because the parameters are 100 dimensional, and so it may not be the case that the level sets are entirely characterized by these 2 random vectors. However, I would be interested to see if this observation is particular to CIFAR10 on vision tasks, or whether similar results hold on ImageNet, which is much more stable to large batch sizes.<BRK>The paper did comprehensive experimental study on how DPSGD and SSGD converge in different tasks.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 6. <BRK>Also, the author need to explicitly argue the main advantages of the proposed method in related work. Second, this method was evaluated on facial image manipulation only. Eq(2) is not intuitive to understand.<BRK>This allows fine grained manipulation of the attribute of interest. 3.English: "evoke" verb is used several times in the paper, but I guess the intended meaning by the authors was not the more common understood definition. Overall I vote for rejecting this paper.<BRK>“Interpreting the Latent Space of GANs for Semantic Face Editing”, In CVPR, 2020. https://dblp.org/rec/conf/cvpr/ShenGTZ20 Could you elaborate the difference between your work and these papers?<BRK>Experiment results exhibit that the proposed method achieves state of the art results on several image translation tasks. How do you choose the weighting factor for the networks?
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 7. <BRK>The benchmark consists of three datasets, multi object tracking metrics and of the evaluation of four methods. The benchmark is comprehensive, in that it contains both data and evaluation measures. The comparison between different models is interesting, and authors have also added a custom designed novel method (Video MONet). They investigate a set of challenging scenarios and carry out out of distribution tests. **Paper Weaknesses**My main concern about the paper is the lack of novelty. In particular:  the dataset associated with the benchmark are mostly based on existing works   they might be appropriate for evaluating this task, but the level of contribution is a bit limited;  on the metrics, the only contribution is to suggest using MOT metrics, which are again pre existing;  the experimental and the insights it gives, again, can foster the community towards better model, but it s not a sufficient contribution for ICLR in my view. I would therefore suggest to reject the paper.<BRK>The paper needs to clearly set the contributions. The video datasets are from self made simulations or taken from other sources because the appendix cites many references for each part of the dataset. 2.The appendices are linked in the paper but given in the supplementary. 3.Apart from these, the paper is well written and useful in the community. But the empirical evaluation is not convincing. There could be many baseline approaches for this, for example, paper [1] and the methods it is comparing with. This is my major concern on the paper and if stated well, the recommendation can change.<BRK>The positives:1. Compare a single image model and four video models (total five) 3. Since, the core goal of the paper is not novelty but a better understanding of various models I would have liked for the discussion to have some clear conclusions and better structure (use X in scenario Y, Model Z needs to be extended for scenarios Y etc.). Instead of focusing on such a comprehensive set of things   3 datasets, five models and multiple metrics it would have been better if authors did some minor extensions of the models and showcase novel directions. But, instead the paper is mostly an understand only work and i worry that the conclusions don t necessarily give clear future directions for other researchers to build on. Given the cons and specifically on not a clear actionable suggestion on how to improve models and no analysis beyond synthetic datasets I am leaning towards a rating of below acceptance threshold.<BRK>The paper represents a much needed comparison of several related models which have previously not been evaluated on common benchmarks. Given the rapidly increasing number of competing models in this space, I believe analysis papers like this one serve an important role. This paper rectifies this, and also provides guidance as to the relative vulnerability of the different methods. 2.The results are not entirely conclusive, in that there is no clear best model, and their relative quality varies with datasets and metrics. Overall, the paper serves an important role in consolidating the ecosystem of unsupervised object representations. Given the increasing need for such analysis papers, and the competent execution, I recommend acceptance.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 6. <BRK># Synopsis of the paperThis paper proposes a novel embedding or representation algorithm forpersistence diagrams, i.e.topological descriptors. This method, by contrast,presents a trainable embedding to the Poincaré ball, thus representingpersistence diagrams in a hyperbolic space. This has the advantage of being more appropriate for representing*essential features*, i.e.features of infinite persistence. Experiments with graph classification and image classification tasksdemonstrate the utility of the proposed method. The paper is technicallysound, apart from some minor inconsistencies, which I shall discussbelow.<BRK>The authors propose to learn a representation for the persistence diagram (PD) in the hyperbolic space to incorporate the essential features (i.e., infinite persistence). The authors show that the hyperbolic representation has stability. The authors give some details about the background (e.g., persistence diagrams, Poincare ball). However, the main part of the framework and how to learn the parameters of the embedding are missing. (It seems the authors combine projection with some transformation here?).<BRK>The authors propose to learn representations of topological persistence diagrams in hyperbolic spaces (Poincare balls). They provide a step by step methodology, and an algorithm for creating the representation. They also compare their approach with various graph classification and other ways of representing persistence diagrams. This is the first work that learns hyperbolic representations of persistence diagrams. 3.Applications to graph data as well as image data show some promise. They have mentioned that essential features have infinite persistence and truncating the persistence is not the best thing to do.<BRK>In this paper, the authors proposed a new representation of persistence diagrams that can include ` essential features . The authors further proposed a classifier that learns the parameterization of the embedding of a diagram in the Poincare ball. On the positive side, I think the proposed representation well unified essential and non essential topological structures. I think this approach is a natural and necessary baseline to be compared with. This way the paper can potentially have a bigger impact. So there is some discrepancy in the experiments.
Reject. rating score: 5. rating score: 5. rating score: 6. <BRK>One issue is that the main text of the paper intensely refer to contents in the appendix, including the actual testing procedure pseudocode, the discussions on many aspects of their study, and many additional results. The appendix is not required to be read by the reviewers, but the current text without these appendix is not self contained. Another issue is that intuitive explanation and discussion are not enough. There is lack of conceptual explanation of this lemma. Through this test, the authors demonstrate that the standard t test fails to discover the carry over effect and cannot distinguish the two policies, while the test proposed by the authors could detect the long term effect and show the advantage of the new policy. Should it be Q(a , a, s )?<BRK>Summary: the paper is motivated by the problem of sequential experimentation as found in online experiments; more precisely the authors propose to use an RL framework to measure performance while optimizing each treatment policy during experiment. The proposal gives rises to a sequential statistical test procedure to decide at the earliest which treatment policy is significantly better than the others. Theoretically, the power and precision of the sequential decision process are studied and proven to be non trivially efficient. I would have expected more discussion of early stopping methods for sequential testing, e.g.https://dl.acm.org/doi/abs/10.1145/2766462.2767729 .<BRK>*Summary*This paper introduces a reinforcement learning framework for testing the difference in long term treatment effects between treatment and control in online experiments. *Assessment*I m not too familiar with literature but slightly leaning towards acceptance since I find the subject somewhat interesting. *Pros*  The problem considered in the paper seems relevant in practice. The algorithm is generally clear and intuitive. Empirical performance of the algorithm looks overall good. Several key notations are delayed to the appendix, which makes the reading experience not very smooth.
Accept (Oral). rating score: 9. rating score: 8. rating score: 7. rating score: 7. <BRK>The work proposes a hybrid architecture that has: (1) language specific (LS) components; (2) as well as the components that are shared across all the languages   a trade off between specificity and generality. In terms of exposition of the ideas, it s a well written paper for the most part.<BRK>In terms of modelling, the work follows in the line of recent work on language specific parameters for multilingual NMT. The deviation from existing work is mixing elements of conditional computation with language specific computation. I find the analysis presented in the paper very interesting and insightful   and distinguishes it from previous work in this area.<BRK>In this work, the authors present a conditional language specific routing (CLSR) scheme for transformer based multilingual NMT systems. This is nice work.<BRK>The idea of learning which parameters to share across languages in multilingual transformer models is original and potentially useful for designing and analyzing multilingual models in the context of NMT. **Strengths**:The paper is well written and easy to follow. How important is this difference? Do you think the conclusion would be still the same if a language specific hyper parameter p_l was used instead?
Reject. rating score: 4. rating score: 4. rating score: 7. rating score: 7. <BRK>This work studies the finite time convergence for neural networks. In particular, it tries to recast the problem of training neural networks as a control problem. Finally, convergence results are obtained with standard theory from non linear systems. However, results presented in this paper seems limited, and it is not clear what contributions the current work really bring to the community. The authors simply try to mimic the theory by having a candidate Lyapunov loss and continuous weight update equations. It is not clear why these are used for neural networks at the first place; rather, it seems that these are only applied for the sake of proving some technical results.<BRK>The authors in this paper make an attempt in providing finite time convergence guarantees of the training process of neural networks, using ideas from control theory. The convergence of which can then be analyzed using standard control theoretic techniques. This  is the novelty in the paper. Though the above is an interesting contribution in itself,  I am not convinced that the results for a fixed input case would generalize well to the batched input case.<BRK>This paper presents a Lyapunov based analysis of the loss function in neural network training and derives a priori upper bounds on the settling time of the training, which somewhat complements existing studies. The supervised neural network learning problem is formulated as a control problem with the weight parameters being the control input, and the learning problem as a tracking problem. Hopefully, this can further motivate exploration and application of more control theoretic tools to understanding of neural network training. Although this paper is fairly readable, the presentation and organization can be improved. Several detailed comments are provided below. Nonetheless, here in the discussion the y^\ast I guess is determined by the loss function, training method, data, as well as the neural network architecture altogether, right? In that case, what would be the y^\ast? In the context learning, one is more interested in the neural network parameters that not only capture the training data but also predict well the unseen ones? So it would also be interesting to present the corresponding testing results?<BRK>The paper aims to make strides towards a theoretical understanding of Deep neural networks, which remains elusive to date. This paper uses a control theoretic formulation to analyze the convergence rate of deep neural networks. In other words, do these results easily extend to classification tasks? And what effect does the new loss have on overfitting? I m a bit confused by the theoretical upper bound. What does this mean?
Reject. rating score: 6. rating score: 6. rating score: 6. rating score: 6. <BRK>The paper considers open world SSL settings  where the model recognizes previously seen classes, and detects novel classes which are not present in the labeled dataset. The method contains three losses to train a model in this setting: a) supervised loss on labeled data, b) unsupervised loss on unlabeled data from pseudo labels obtained  from confident pairwise similarities, and c)  regularization term that avoids assigning all the unlabeled samples to the same class. First, I think this set up does not contain all the scenarios for a real world SSL. However, in the real world scenario, we may have images of the same class but different domains. Does the regularization towards uniform distribution consider unbalanced novel classes which is common in open world SSL?<BRK>The objective function to be minimized in the proposed method comprises three terms: unsupervised loss, supervised loss with uncertainty based adaptive margin, and entropy regularization. Review summary This study is well motivated and tackles an important problem that would occur in real world applications. Details Strength  The setting of the open world semi supervised learning is interesting and should be practically important. The paper is well written and well organized. Two concerns on uncertainty based adaptive margin. Since this adaptive margin is adopted to improve the accuracy of pseudo labels, how much it is improved should be reported in Fig.3.How did the authors conduct validation? Due to the existence of unseen classes in unlabeled data, how to conduct validation is not trivial.<BRK>Summary: the authors propose Open World Semi Supervised Learning (ORCA), a semi supervised method that learns to classify previously seen classes in the labeled data and novel class in the unlabeled data. Pros:The paper takes one of the most important issue of semi supervised learning: recognizing novel classes. For me, the problem itself is real and practical. This would make sense for text embeddings but the experiments are on image datasets<BRK>Pros  The paper explores an interesting semi supervised learning (SSL) setting in which the unlabeled data contain not only the seen class but also novel classes. The problem is interesting in that it is more practical than the classic SSL setting in the real world and has been seldomly researched. The supervised loss, which is the main contribution of the method, overcomes the imbalance problem caused by BCE with a novel adaptive margin based loss. The authors validate this loss with empirical results. Cons  The comparison methods (such as pseudo labeling, DS3L) are not strong enough.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 5. rating score: 6. <BRK>3) The only case where “bias resetting” seems to help nontrivially is for CIFAR 10, but at which point this “resetting” is applied doesn’t seem to matter; why is that the case? Here, the authors present a clean case for “negative pretraining effect” on images, and propose three ways to mitigate it. However, the investigation is limited to supervised image classification on four small scale datasets with blurring as the only deficit studied. It’s not convincing to me that the proposed approaches are fundamental fixes that can be generally applied to other learning scenarios where one wishes to get rid of the “negative pretraining effect”.<BRK>The paper claims that it has proposed three distinct ways to **remove** the negative pretraining effect. In particular, a network trained on a sequence of tasks performs inferior to a network trained from scratch on the intended target task. The main idea of the paper is to study this phenomenon by formulating and intervening on different constituents of the sequential learning process   (1) changing the learning rate across tasks, (2) number and type of tasks encountered in the learning process, and (3) resetting the model biases when going from one task to another. If the current experimentation setup does not render the phenomena (except for CIFAR 10), this raises the question of whether the paper is analyzing the right setup (datasets and model)?<BRK>Adding datasets with a different number of classes than 10. The fact that it s a question of generalization and not the training loss going to zero does not mean it s not a question of optimization   optimization includes the generalization properties of whatever optimum it found, so it definitely does deal with this. It would be good to try others. It might be worth typesetting those values in a more aesthetic way. ## 5.ConclusionI think the question asked is interesting and the approach the authors took promising. I don t think Figure 3 shows that clearly, or possibly at all.<BRK>1.Summarize what the paper claims to contribute. 2.List strong and weak points of the paper. iii.The results of resetting network biases are not consistent across different datasets; need further analysis to make a more clear conclusion. 3.Clearly state your recommendation (accept or reject) with one or two key reasons for this choice. (1) Could you provide further analyses to refute i iv in 2.(2).a? Make it clear that these points are here to help, and not necessarily part of your decision assessment.<BRK>In this paper, the authors formalized the sequential learning problem and the negative transferring in this type of learning, and conducted empirical study on three interventions that can help to remove the negative transferring. Out of the three datasets, only one clearly shows the impact of the intervention.
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 6. <BRK>##########################################################################Summary:This paper studies the problem of designing adversarial attacks (on GNN models) that perturb the feature to maximize the misclassified instances. The paper identifies several conditions on the threshold that can make the influence maximization problem submodular, thereby making it easy to optimize. ##########################################################################I find the paper interesting but lean towards rejection at this point. The main reason is that the assumptions are not realistic, and the contributions are incremental.<BRK>* The connection to influence maximization is interesting. * The problem of GNN robustness under (realistic) adversarial attacks is important. Does this mean that the results in Table 1 are also reported on the whole dataset? If not, why is there a mismatch between the reported misclassification rate and the optimization objective? * Sec.4.4: the authors mention that the first approximation leads to the problem becoming "likely to be submodular". Why is it only likely to be submodular and not guaranteed? * The paper just briefly mentions that Eqs.<BRK>Paper summary: The paper studies the problem of attacking GNNs in a restricted black box setup, i.e., by perturbing the features of a small set of nodes, with no access to model parameters and model predictions. The authors draw a connection between the restricted attack problem and the influence maximization problem, and then propose several approximation techniques to solve the reformulated attack problem. In this case, how do you select the feature indexes j as there are many ties in epsilon_j? I think one reason could be that the selected simple distribution for theta largely deviates from the true distribution.<BRK>This paper introduces a novel connection between adversarial attack on graph neural networks in a restricted black box setup via node feature perturbation, on the one hand, and the influence maximization problem under the linear threshold model on the same graph, on the other hand. This paper would be stronger if it addressed the question of whether this property also extends to the analogy, apart from the submodularity property. The algorithms are called efficient, but no evidence is provided to that effect.
Reject. rating score: 6. rating score: 6. rating score: 7. <BRK>This may cause the global feature $z$ to encode unwanted local information or vice versa. However, the experiments presented in this paper are not very comprehensive, particularly the baselines and the ablation/alternative studies. The EER reported in Hsu et al.(2017) is much lower than the results in this paper, and as mentioned by the authors, that work regularizes the representations with a discriminative objective that approximates $H(x|z)$ and therefore can be seen as a form of MI VAE. The authors argue that previous work regularizing the representation $z$ by maximizing its mutual information $I(x; z)$ with the data $x$ has the side effect of simultaneously maximizing the mutual information between $z$ and local features $s$.<BRK>I want to strongly encourage the authors to be as precise as possible when describing the novelty   maximizing the mutual information that a representation carries w.r.t.some relevance variable while simultaneously minimizing information that it carries w.r.t.to another variable is NOT novel. Overall I think the ingredients for a good paper are there, but they are not quite coming together yet. 3) beta VAE and MI VAE are ok baselines, but are not sufficient to show that the method performs very well. In this paper the idea is to add an explicit penalty for statistical dependence (mutual information) between the global and the local random variable. This intractable objective is simplified with a series of approximations, leading to a novel training objective. Results are shown for speech data, and MNIST/FashionMNIST, where the proposed training objective outperforms a beta VAE objective and an objective that explicitly aims to maximize information on the global variable. There is also some indication that local features capture less global information with the proposed method compared to a beta VAE. Currently this is entangled in the derivation of the method.<BRK>Relation between the proposed method and Conditional MI, beta VAE, and domain adversarial training is discussed. The authors designed experiments that show that the proposed approach can improve global representation learning. Quality*Pros: The paper is overall of good quality: the context of the problem is well explained with adequate diagrams/plots to aide understanding. Experiments are well designed and details are provided. Instead of proposing a completely new architecture/method, the authors spot a gap in the current literature, i.e., that local and global feature representations can be disentangled using the current learning objective. The paper addresses exactly this gap.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. rating score: 6. <BRK>As a note, in the paper [Monti et al.2018] (available only on https://arxiv.org/pdf/1806.00770.pdf) also presents line graph formulation with GAT applied to the line graph. The benefit of using Incidence Matrix based updates is clear in terms of space and time complexity. The paper claims performance improvement over graph U net which demonstrates the benefit of weighted approach which does unbiased node feature updates. # Recommendation Overall, I vote to reject the paper.<BRK>The paper introduces a GNN architecture that is based on a weighted line graph transformation (in conjunction with a node based architecture). I feel that some kind of stronger theoretical justification is missing here, given the computational costs of expanding the representation to edges. Unfortunately, I feel that both the theory part and the experiments are not fully satisfactory. Additional comments:The general idea of the paper sounds appealing.<BRK>The paper proposed a GNN model based on a weighted line graph, which adds weights to the line graph for the original input graph in a node/graph property prediction task. 3.The experiments, like Table 1, compare with some existing GNNs methods. Experiments compared the performance of the proposed model with existing GNN methods on graph classification tasks and computational complexity with other methods.<BRK>Then it performs message passing on both the original graph and the weighted line graph. The experimental results demonstrate the effectiveness and efficiency of the proposed method. Overall, this paper is well written and easy to follow. Does it work on large graphs with thousands of nodes?<BRK>The authors suggest a model that simultaneously propagates information in both graphs, coupling the two propagations at each step. They perform a series of experiments using this combined propagation scheme through the graph and the weighted line graph.
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 6. <BRK>There are lots of other places in the paper where clarity needs to be improved. The method is evaluated on the Procgen and achieves superior performance relative to Rainbow, a standard RL algorithm. However, I think the paper needs more work to be ready for publication for the reasons laid out below. Thus, I cannot recommend it for publication at this stage but I encourage the authors to continue working on it. Additional experiments and discussion to motivate this choice would be useful.<BRK>In particular: As noted by the authors, prior work has shown that "standard" regularization techniques like L2 (weight decay), Batchnorm or Dropout can improve performance on both training and testing. Minor point: A second set of other procedurally generated environments, other than ProcGen, would help show that the method is not overfit to ProcGen. Other remarks (no need to answer, didn t have impact on evaluation):  I found section 4.2 hard to understand, maybe it could be reformulated?<BRK>The notion of similarity is called in the paper "cross state self constraint"   CSSC. My understanding of the paper is that the standard IMPALA architecture was equipped with an additional "embedding head" (I am inferring it from Figure 1). It would be useful if the authors include pseudocode how the loss is exactly computed for a given batch of samples. Though I cannot find a precise description of this architecture change, in particular how large are the embedding vectors.<BRK>The method developped in this paper seems to provide interesting empirical results. In that case, what happens with the loss? What does n corresponds to in practice in the experiments? In table 1 and 2: How many seeds are used? Why is the standard deviation not given? No information on the neural network architecture seems to be given.
Reject. rating score: 4. rating score: 5. rating score: 6. <BRK>The paper proposes a knowledge distillation method for face recognition, which inherits the teacher’s classifier as the student’s classifier and then optimizes the student model with advanced loss functions. Strength:  The proposed method is simple and easy to implement. The paper is well organized and well written. Weakness:  The novelty is limited. Directly inheriting the teacher’s classifier is a common strategy in the face recognition community, which can be found in Ref.1. The experiment lacks comparison with the general knowledge distillation methods (Ref.2) in image classification and the specific used methods (Ref.3) in face recognition.<BRK>This paper proposes ProxylessKD method from a novel perspective of knowledge distillation. However, I still have some concerns. Second, ProxylessKD can be interpreted as initializing classifier of student model by the classifier of teacher model. Third, experimental comparisons with more advanced KD methods are necessary, e.g.[1], [2], [3] etcs. [1] Park, Wonpyo, et al."Relational knowledge distillation." 2019.[3] Karlekar, Jayashree, et al."Deep face recognition model compression via knowledge transfer and distillation."<BRK>This paper proposes a new KD method to inherit classifier from teacher models and utilize it to train the student model feature representation, where previous KD methods are mostly focusing on the proxy task other than the target task itself. The methodology illustration is simple yet clear. It can provide a direct comparison to other methods, i.e.ArcFace trained using ResNet18 compared to the student model with ResNet18. Current experiments lack the comparison to the state of the art methods, i.e.ArcFace and CosFace. It does not consider the knowledge distillation itself. It needs sufficient analysis to justify the authors  choice of only applying the teacher model s classifier as distillation. 3.In ablation, how would the number of teachers influence the student performance?
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 5. <BRK>Theoretical analysis is also conducted to understand the performance of the proposed method. Although the idea of the proposed method is somewhat interesting, I do have many concerns for the paper. 1) The writing is not good, so it makes it hard to understand the work. 2) Section 4.2 is too ambiguous. And how to set it in practice? I went through the 2nd paragraph in Section 6.2 and still feel unclear how to set this hyperparameter in practice. 4) Regarding the theoretical analysis, unless I miss something, it is just the standard theorem as in Srinivas et al.(2010), but replace the assumption of the objective function f being a sample path from the GP, by the assumption of the latent space function h being a sample path from the GP? Besides, what does it mean when the neural networks are pretrained on a number of data points?<BRK># cons1.The general idea presented in this work is very interesting. 2.Although not a new technique, e.g.siamese network, triplet loss, I like    the idea of penalizing close points in latent space combined with a GP. It    implicitly incorporate prior knowledge in modeling the GP, which usually    boost the performance of a GP. # pros1.I am doubtful about the correctness of eq(1). This means the batched update    of GP in eq(1) will not produce a correct GP model, if I understand eq(1)    correctly. Although this work focus    on GP based BO, empirical results of SMAC and TPE **without** considering    collisions will make this work more convincing. 3.The experimental settings used in this work are not detailed, e.g.how many    units in each layer in the neural network, etc. It is not clear to me why the retrain interval $\tilde{T}$ is set to be 100    for 3c, 3a and 3d in Figure 3. Can the authors comment    on this? Overall speaking, I am afraid this paper doesn t contain necessary details andthe theoretical results are not strong enough.<BRK>It’s a natural idea to add a Lipchitz like regularization loss to mitigate “collision”. The theoretical result seems a straightforward derivative of the Srinivas et al.(2010), but I don’t really see the novelty of the theoretical result, since Lipchitz continuity is implicitly determined by the kernel function? It’s not clear to me what it means by “pre train”. 3. there are several parameters, such as $\lambda$, $\gamma$, how are they chosen exactly? It would be great if you could plot the function on latent space. 6.To me seems the work could be more motivated by input domains such as graphs or other discrete structures, at least for the benchmarks in the experiments I don’t see why they need this method despite the claimed superior performance.<BRK>The paper proposes (1) a new regularization strategy for the latent space based BO, (2) an optimization aware dynamic weighting for adjusting the collison penalty to improve BO, (3) theoretical analysis for the BO on the latent space. The idea for the regularization is to take in pairs of data points and penalizes those too close in the latent space compared to their target space distanceThe paper makes an interesting observation that the learned representation (for BO to deal with complex object or high dimension) often leads to collision in the latent space: two points with significant different observations get too close in the learned latent space. The idea of using constraint in the latent space has also been studied in [1]. Despite of the good motivation, the paper execution is not yet demonstrated the effectiveness of the proposed approach for three main reasons:(1) The experiments using 4 settings are quite simple and havenot yet satistisfactorily convinced why the proposed approach performs intuitively better. It can be improved further by demonstrating the collison effect in more challenging task, such as automatic chemical design [2]. (2) The theoretical analysis follows and extends from Srinivas et al 2010. It will be useful if you can add another figure in the same setting using the regularized latent space.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 9. <BRK>##########################################################################Summary:This paper proposes to use deep learning neural network to model eigenfunction while solving a generic eigenvalue problems. Since such problems is very generic this application of such framework are huge and the experiments show some promising results. This paper proposes a new framework for generic eigenvalue problem which seems promising. The notation is confusing on some parts. Perhaps using a function which gives the associated weights could help. 2.When dealing with multiple eigenfunction, there is en additional term is the equation 6 for the orthogonality constraints. 5.Again on the implementation details, the loss function of equation 6 is highly non convex and I assume that there is several way to try solving it.<BRK>This work is a natural follow up of the work by Bar and Sochen (2019) for solving PDE based problems. The unsupervised loss resembles the loss suggested in Bar and Sochen (2019), with addition of mainly two terms: (a) The Rayleigh Quotient term and (b) the orthogonality constraint. The proposed framework has the potential to handle eigenvalue problem of self adjoint differential operators over irregular domains. Moreover, in the 2D case, M networks are optimized simultaneously. 4.Limited experimental work. However, as far as I can see the experiments are done over regular domains for a special instance of differential operator, namely the Laplace operator.<BRK>The main idea is to parameterize the eigenfunction with a neural network and minimize a loss function which enforces the definition of eigenfunctions along with auxiliary terms enforcing boundary conditions, smoothness and getting rid of de generate solutions. My main concern is the lack of novelty/understanding what the contribution is. Going through the existing literature (section 2.1 in related work), it seems that the standard way to solve a differential equation using deep networks is to parameterize the solution using a deep network and optimize a loss which captures the physical relation (that is the PDE itself) along with auxiliary terms for smoothness, boundary condition etc. The main loss in the paper is definition of eigenfunction since that is the problem they re interested in and in the literature for solving PDEs, the main loss is the definition of the PDE itself. In addition, it s unclear to me why we are particularly interested in the lowest M eigenpairs. I don t think this is motivated in the paper.<BRK>The authors frame the decomposition of the Laplacian equation as an unsupervised regression problem that is using a 5 level (and fully connected?) neural network as regression function. A comparison with a classical method indicates that the proposed approach is comparable to or better than the former for the given task. I like the overall approach and the idea of framing the solution of the eigenvalue decomposition as an ANN regression problem, and I would see that this is of interest to ICLR. In real world problem critical information is contained in the boundary conditions.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 7. <BRK>This paper proposed three simple algorithms for sparse principal component analysis (SPCA): a) randomized matrix multiplication; b) deterministic thresholding scheme; and c) semidefinite programming relaxation. However, it is somewhat interesting to have novel theoretical guarantees for these simple strategies whose error bounds depend on the properties of the input matrix and the target sparsity. This paper is mathematically sound and the theoretical bound of existing SPCA tricks is also interesting to know. It will be nice to know what is the running time for the proposed algorithm on a moderately large dataset. I recommended a reject as it has marginal novelty and the claims of the proposed method are not experimentally well supported in the paper.<BRK>The paper studies the sparse PCA problem. While no assumptions is imposed on the input covariance matrix, the authors give theoretical guarantees on the optimality gap (i.e.the gap between the optimal value and the objective value achieved by the solution) as well as the sparsity level of the solution. 2.The authors provide numerical experiments to illustrate their theoretical findings for the three presented algorithms. This paper imposes no constraint on the input covariance matrix. If this is the case, I think it is not reasonable to seek for a sparse estimate of the leading eigenvector. (ii) In the case when the top eigenvector is sparse, the theoretical findings in this paper does not provide guarantees that the output of the three algorithms are reliable approximation of the top eigenvector.<BRK>This paper presents three approximation algorithms for the sparse PCA problem. All the three algorithms have provable theoretical guarantees and low degree polynomial time. I have the following comments. 2.The algorithm only output a solution with expected k sparsity. This significantly limits it practical use since we often need a exactly k sparse solution. 3.The experimental comparisons are not sufficient. In addition, the dimension of the data set is too small to show the effectiveness of the algorithm.<BRK>2.It is prefer to test the proposed algorithms on large scale datasets, which could make the paper be more convincing to machine learning community. However, it is somewhat surprising that such simple strategies have reasonable theoretical guarantees whose error bounds depend on the properties of input matrix and the target sparsity. I have some comments as follows:1. Hence, I think it is necessary to provide the theoretical analysis for the proposed frameworks with inexact SVD and establish the error bounds contain the error from approximation of SVD.
Accept (Oral). rating score: 8. rating score: 7. rating score: 7. rating score: 7. <BRK>The paper proposes a new optimization method named FedDyn to handle data heterogeneity inherent in FL via a dynamic regularization. Such a dynamic regularization modifies each local objective by adding a linear and quadratic term that makes the local stationary point is asymptotically consistent with that of global objectives. I have read the authors  responses and almost all my concerns have been well addressed. Pros:1.The paper is well written and easy to follow. All proof seems correct. Besides, a lot of details are given, which helps reproduction. 3.The proposed method has great superiority over other baselines in communication efficiency, shown by a lot of experiment results. However, from the main theorem, I couldn t figure out the reason. However, again, no theoretical analysis is given. The main theorem is about the convergence of balance data in terms of communication rounds not about the effect of unbalance data. The experiment indeed did well, however, there is still some shortcomings. But I can understand what the author wants to convey: their affine function saves communication costs.<BRK>This paper proposed a new federated learning algorithm called FedDyn, which was motivated by the observation that the local objectives for each device might lead to inconsistent models between devices for heterogeneous data. Such observation is already observed in SCAFFOLD, and this paper further improves over SCAFFOLD with better communication efficiency. Both theoretical convergence rates and empirical  results about communication efficiency are presented to support the advantages of FedDyn methods. The choice of alpha: it seems alpha is an important hyperparameter which can largely affects the theoretical convergence and empirical efficiency of the proposed FedDyn method. Unfortunately, I could not find much discussion in the paper about alpha: I think how alpha affect the convergence rates, and how alpha is chosen in the empirical results should be discussed, it would be better if the authors could provide an empirical study about the parameter sensitivity in alpha. 3.Related work: there is a related paper (to appear in NeurIPS 2020) which proposes to solve a similar local objective, it would be good to discuss the differences: FedSplit: an algorithmic framework for fast federated optimization https://arxiv.org/abs/2005.05238.<BRK>* Context: the authors propose a new distributed optimization algorithm, called FedDyn, to minimize a sum of smooth functions. Their motivation is the context of federated learning, in which each function is the loss of one user corresponding to its data stored locally. It would be very interesting to compare them. Indeed, in case of full participation, it is not clear that the algorithm has any advantage in comparison with classical (S)GD type methods. This is not the case of the proposed method, which is much closer in spirit to the class of SGD methods. No regularizer at the master in the objective function. However, I view the theoretical analysis as a preliminary one, and several aspects would deserve to be investigated more in depth. There should be a discussion about the literature of SGD type methods. 4.When mentioning prior work on methods using local steps of SGD, which is "inconsistent with minimizing the global loss" (or later when mentioning that "performance degrades in non IID scenarios"), you can cite the paper "From Local SGD to Local Fixed Point Methods for Federated Learning", ICML 2020, which gives a precise characterization of this "inconsistency" in the strongly convex case (Theorem 2.14).<BRK>In FedDyn, the objective function of each active device in each round is dynamically updated, so that the device optimum is asymptotically consistent with the global optimum. The main contribution of the paper includes:  Proposing the dynamic regularization method to tackle the inconsistency issue in federated learning. I appreciate the authors  discussion in the introduction. The experimental results seem comprehensive. Also the comparison between the proposed algorithm and SCAAFFOLD makes the claim (saving communication costs) much clearer. Cons:  What can be said about the computational time of each device during each iteration, compared to that of existing algorithms (say, SCAFFOLD)? This would be interesting, although I understand the authors focus on a communication point of view. I did not check other proofs though.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 7. <BRK>The paper finds that a small learning rate is beneficial in finding a performant pruning mask, although the resulting network s performance may be worse, while a large learning rate is better suited to then optimize the sparsified network in the subsequent lottery ticket style training procedure. * The study on the effect of separate hyperparameters for sparse training and discovering sparse masks could be helpful in guiding future research on pruning. * The part about LPR being the main driven factor for the improved performance is really interesting. Most pruning work considers CIFAR and ImageNet. This paper provides additional experimental evidence for this phenomenon by separating the effects of training hyperparameter on the mask finding procedure and the sparse training procedure. Specifically, the LT hypothesis in the more general setting states that if the experiment is repeated _exactly the same_ starting from that early iteration in training but with the applied pruning mask, it does not harm the performance of the resulting network. Consequently, the conclusions that are drawn in this paper are somewhat confusing, it is really hard to discern the generality of the results, and to understand what is actually observed. Are these observations related to LTs or are the authors proposing another method, i.e.small learning rate IMP, to find performant pruning masks at initialization? Specifically, I can see two avenues for the paper to improve its framing of the results: 1. Show that the conclusions hold for a more _general_ LT setting. As a result that would naturally require the authors to repeat the experiments. [3, 8], where it has been previously shown that valid LTs can be found using standard IMP. It will be easier to read and jump back and forth between the main body and the appendix. I have additional feedback in the "Weaknesses" section as discussed below. ## Strengths* I commend the authors for a very clean and well written paper.<BRK>Finally, Footnote 1 is quite unnecessary. They then further show this phenomenon on other LT setups (with late rewinding, LR warm up, LR rewinding), architecture (ResNet18), and dataset (MiniPlaces), and for structured pruning as well. ### Strengths:+ The paper tries to provide better understanding of pruning methods, rather than introducing a new heuristic, which is an extremely important research direction in network pruning. "_      _"Common practice rests on the assumption that models with the best performance will generate the best masks, such that the optimal hyper parameters for mask generation and mask evaluation should be identical. "_    _"If we had to use the same LR for everything, we would have to make a trade off between a better unpruned performance and a better pruned model performance. However Renda et al.(2020) showed that using the same ones at each iteration is a good heuristic, and this is precisely what the experiments of this paper seems to show (the best overall results are: in Figure (2) LR_eval   LR_find   0.2, in Figure (3) LR_find   LR_eval   0.2, in Figure (A1) LR_find   LR_eval   0.1, etc...). I do agree however that one should not first tune the hyper parameters to obtain the best network, and then reuse these same hyper parameters for the pruning stages, and your experiments indeed show that, but I don t think this is what practitioners do in practice. Are other pruning criteria also affected? On that note, how are the networks initialized (this information is missing from the document)? ### Questions:  At each iteration of LT, what is the model selected to compute the mask on? Is it the model obtained at the last iteration, or the model that was best performing on the validation set? How many seeds were used? This should be precised in the document.<BRK>##########################################################################Summary:In the lottery ticket hypothesis s general framework, network pruning s two phases of training (before and after the network is actually pruned) use the same hyperparameters   in particular, the learning rate schedule is identical in both phases. Using different learning rates for these two phases, and, unintuitively, preferring a learning rate in phase 1 (finding the mask) that results in a *worse* dense model, can result in a better final model (after training with the mask in place). It is also shown that the layerwise pruning ratios may be the key to understanding this behavior: finding the proper LPR is best done with a small learning rate, but given an LPR, a large learning rate is better able to determine the specific mask. (This hyperparameter is also not new in all contexts, only in the LTH.) ##########################################################################Pros:+ This paper is well written and fairly easy to follow with a straightforward organization. ##########################################################################Cons:  It s hard to tell what the practical benefit of these findings are. The discussion suggests practitioners incorporate "simple additional sweeps of decoupled learning rates." Useful guidance for limiting costly hyperparameter sweeps, either from a theoretical grounding or wider empirical studies, would make this requirement less painful. That said, applying it to the LTH framework is new, and it uncovered the dependence of a good model on finding a good LPR set. (Also, as I argued *for* the submission above, it s new that the goal of the mask finding stage isn t necessarily to find the best performing dense model; a worse dense model can lead to a better pruned model.)<BRK># SummaryThis paper evaluates explicitly decoupling the hyperparameters (specifically, learning rate) used to find pruning masks from those used to train pruned networks, finding that for global magnitude pruning lower learning rates (relative to the learning rate that results in the most accurate full size network) result in masks that can train to higher accuracies; on the other hand, higher learning rates tend to train the pruned networks better. The paper then shows that these differences in performance are primarily due to differences in layerwise pruning rates that come from training with different learning rates. # Strengths  The paper proposes an interesting experiment, decoupling mask finding hyperparameters from mask training hyperparameters, with the potential to change how people think about network pruning, challenging the assumption that a higher accuracy full network results in a higher accuracy pruned network  The findings are well evaluated, convincingly showing that lower learning rates than those that result in the highest accuracy full network result in better masks  The analysis is also strong, showing that the primary factor in the performance delta is the layerwise pruning rates# Weaknesses  Given that many of the findings are centered around performance when resetting the weights to the beginning of training, which is known to not work with large scale networks, a significant amount more discussion of or replications of experiments of [1] is warranted. Presentation of figures could also be significantly improved: for instance, by moving figures closer to text that references them, by collating high level takeaways into tables, and otherwise making it easy to process the large amount of data in this paper  The early pruning subsection seems to refer to results that are not presented in the paper; given that the data is presented in the appendix, it d be better to include this text in the appendix.
Reject. rating score: 3. rating score: 5. rating score: 7. <BRK>Questions:  There are multiple references to "creation", e.g., "create novel compositions" which makes the method sound like it s performing generation. ), and so does "create novel compositions". I think the claims and presentation of the paper need to be carefully re scoped. There is a lot of focus on "learned arithmetic operations" but no analysis as to what exactly this component ends up doing or learning. Why use language pretraining and not vision pretraining (e.g., topline)? "The results show that ARTNET achieves significant performance improvements in terms of new composition accuracy, over a large scalevideo dataset." What is the intuition for this not working, if it did not? It s a big advantage/relaxation on the test set to have no OOV tokens. This is poorly worded, since compositional generalization is well and commonly studied, to the point that even in this paper there is a section in the related work about it.<BRK>Is $k$ the cardinality of the set of all analogy pairs? Choice of K in Top K              The Appendix (Implementation details) mentions that you pick K as 3. Why?What happens when you increase or decrease the value. Is there something to be learned here? What is the distribution of number of object and words in the dataset for an image sentence pair? What does the distribution look like? Minor concerns (suggestions, typos, etc.) Section 3.4: grammatical errorsPreliminary Rating and its justificationThe current version of the paper seems to have a couple of loose ends in terms of clarity and completeness of experiments.<BRK>The paper introduces a new dataset based off of EPIC Kitchens (Damen et al.2018) which masks out verbs and nouns and splits the evaluation data into seen combinations of verb/noun pairs and unseen combinations of verb/noun pairs, challenging a model to generate captions for pairs which were not seen during training. The paper also proposed a model, ARTNet, to address this task. However, I have some concerns about the dataset s construction and suggestions for additional experiments. The proposed model performs well on the proposed task. There seems there would be a tradeoff between accuracy and computing resources necessary to run inference, but what exactly is that tradeoff? Similarly, would be good to see a similar analysis for the number of top candidate examples that are passed to the ARN module from the AMM module. 1.Many terms in the model section should be defined more formally, e.g., "candidate analogy composition", "ordered constituents in a composition", "pars(ing) each sample into candidate analogy compositions", "multimodal resulting set of pairs / analogy pairs", "linguistic clues".
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 5. <BRK>However, I would like to see some experiments regarding this model. So the algorithms that are polynomial in the number of states are not efficient. The paper considers the factored block Markov decision process (MDP) model and adds a few more assumptions. Moreover, the paper is very well written and clear.<BRK>This paper proposes a new result for provably efficient exploration in rich observation RL with assuming that the underlying structure is a factored block MDP. I appreciate the novelty of this result in the direction of combining factored MDP with rich observation RL. My major concerns are:1. And the sample complexity can still be pretty large if \eta_{min} and \beta_{min} are very small even with a small d. 2. I appreciate this work as a theoretical result. I would like to change my score if the authors can address my concerns.<BRK>This paper studies reinforcement learning in spaces with a large number of states by modeling the states using a factored / latent representation. This problem has been studied in the non factored setting by Du et al (2019), and this paper extends to factored settings. Overall, I found the paper difficult to follow since it is not presented well.<BRK>##########################################################################Summary: This paper studies reinforcement learning under the setting of factored block MDP, where the observations are generated from latent factors. The performance of the proposed framework is theoretically analyzed. ##########################################################################I find the paper theoretically sound but lean towards rejection at this point. ##########################################################################Major comments:It is appreciated that the authors provide the complete theoretical analysis of the proposed framework, and the analysis is reasonable and sound to me. Please be consistent. “An agent is responsible for mapping each observation x ∈ X to individual atom …” What is the difficulty in having such a mapping? There are almost 30 pages proof in the appendix.
Reject. rating score: 4. rating score: 4. rating score: 4. <BRK>The paper proposes QUAIL, an ensemble of a generative model and a classifier, where both are trained with differential privacy, in order to generate a differentially private dataset (from the generative model) and a label vector (from the classifier). It is hard to judge whether the main claim is that the paper presents a survey of the current differentially private generative models, in which case, the survey part is very short and not in depth. My some other concerns are detailed below:  There is no mention of \delta used for (\epsilon,\delta)   differential privacy  Please use something else than \delta for difference between performance measures as it can get confusing  As the version of GAN used in the paper is conditional, shouldn t the generator be differentially private as well? There is a mention of PATECTGAN performing better even compared to the non noisy model trained on real data, how is this possible, please add some explanation.<BRK>The authors proposed QUAIL, an algorithm that uses a supervised model and a synthetic data model to generate synthetic data that is good for downstream tasks. The technical part, especially the experiments, might need some improvement. The paper can be better organized, for example,  The notations in the algorithm should be clearly explained/defined before the algorithm (for example, N and X), and some intuition can be added after the algorithm description. So it might be important to report the standard deviation of the algorithm for readers to better understand what was going on. The paper called the algorithm "ensemble method".<BRK>The proposed method works as follows. Given samples are partitioned into two parts; one is for classifier training and the other is for data synthesizer training. Both are trained in a differentially private manner. One limitation of this manuscript is that the reason why the proposed scheme can give better classification accuracy is not discussed. The main claim should be constructed with the contents in the main body.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>The paper considers distributed mean estimation in two variations (mean estimation and variance reduction), applicable for instance in distributed learning where several machines needs to figure out the mean of their locally computed gradients. The paper measures the quality of an estimator in terms of the input variance, where earlier work has implicitly assumed that the input across the machines had mean zero, and instead measured quality in terms of the inputsIn that sense the approach takes in this paper generalizes previous work. The cons i have listed i think are all small and overall i think this is a good paper as it provides a clean practically applicable version of the problem, the bounds shown are tight and an actual new algorithm is provided and shown to have good practical qualities.<BRK>The paper studies the distributed mean estimation problem where $N$ machines (each holding 1 value $x_u$) wish to compute the mean of all $N$ values. The parameter $y^2$ is called the input variance. One crucial contribution of the paper is that it provides guarantees with respect to input variance instead of input norm (which can be large if the inputs do not have 0 mean). The paper is well written. This work studies a basic and important problem in distributed computation. Is it accounted for anywhere? Given that this is mainly a theoretical work, having such experiments is not a major downside, but they are also not very expressive.<BRK>Summary: This paper studies the problem of mean estimation of n vectors in R^d in a distributed setting. There are n machines. Their main contribution is using a new quantization method by exploiting the structure of Lattices. Furthermore, we need to ensure that other machines could decode the bit string and obtain the same lattice point again and use this point to compute the average. The writeup could be improved as well. Then, we can run the algorithm again.<BRK>The paper considers a particular setting of distributed mean estimation problem, where each party has a vector of potentially large $l_2$ norm, yet this vectors are fairly close to each other. Overall, I think the result is fairly interesting. This allows to recover the points we rounded to for each party and thus estimate the mean.
Accept (Poster). rating score: 6. rating score: 6. rating score: 5. <BRK>The authors of this paper propose an compression technique for GNNs that was inspired by lifted inference. The approach seem quite novel, pretty interesting and it could interest a wide audience that works on graph models. However, I think the presentation could be improved. In particular, it is not clear the evalutation settings that they are using. Do they use golden triples?<BRK>The paper proposes an approach to make learning deep neural networks more efficient using ideas from lifted inference for relational probabilistic models. Specifically, symmetries in the computation graph are identified and then the neural network is compressed into an equivalent model. Therefore, it seems to be a nice direction for improving scalability in neural networks such as GCNs. Regarding the experiments, they show that the proposed techniques can help speed up different types of deep models based on GNNs. In general, I like the idea of using symmetries to compress graph based neural networks.<BRK>##########################################################################Summary: The paper provides an interesting work in the scale/speed up of structured convolutional models. In particular, it proposes an idea using a technique named lifting which is used in scaling up of graphical models to detect the symmetries and compress the neural model such as Graph Neural Network. Authors show that this compression can lead to speedups of the models in many tasks. It clearly shows the effectiveness of the algorithm over other methods. There are no comparisons between these two methods are shown in the result. 4.It would also be interesting to see an analysis on the effect of the algorithms on different graph types based on their characteristics (degree distribution, density, veracity etc).
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK>This paper addresses the problem of obtaining more compact CNNs by a parameter sharing method. The authors propose to represent a weight filter in a low rank subspace (represented as a linear combination of low rank filter basis) plus a set of non shared low rank filter basis (per layer). In this way, the shared low rank filter basis is reused across several layers, and the non shared ones per layer are used to enhance model generalization ability. This paper suffers from following issues. Compared with existing low rank approximation related works, the main contribution of this paper is sharing low rank filter basis across several layers (in the same group or block of a CNN), while retaining a non shared low rank filter basis per layer. To me, the motivation is not clear enough, e.g., such kind of recursive design may easily lead to more high computational cost; it is usually not useful to get obvious benefits. Furthermore, from the definition in the method part, sharing low rank filter basis across layers needs weight filters have the same shape size. The authors claim that parameter sharing in this way can address “… a shared filter basis can cause vanishing gradients and exploding gradients problems”. What kind of modifications did the authors make?<BRK>Experiments on ImageNet show that the method reduces 10.01M parameters of ResNet 34 with almost the same error rate, i.e., the method is more effective for DNN architectures without using separable convolution. pros.1.In Equations (1)~(3), the authors clearly and formally explain one of their motivations that a convolution layer with high computing complexity can be replaced by a convolution layer with lower computing complexity and a liner layer. Moreover, in Section 4.1.3, experiments without orthogonality and with orthogonality are qualitatively and quantitatively compared. The main novely of this paper is to simultaneously train convolution decomposing and weight sharing. For weight sharing, the authors propose to share weight across layers which is also similar with previous weight sharing papers. 2.The experiments are not sufficient. In Section Experiments, the proposed method is only compared with classic ResNet 34 and MobileNet V2.<BRK>The authors proposed a parameter sharing method among repetitive convolution layers, where typical filters are decomposed into a set of resuable filter bases and coefficients. The experimental results show some improvement about the number of parameters and FLOPs. Generally, the paper is well written and the method is presented clearly. The authors also claimed that orthogonality regularization can reduce the potential vanishing/exploding gradients problem in weight sharing training. (e.g., Jastrzebski et al.,2018; Köpüklü et al., 2019 mentioned in the paper). Therefore, it s not easy to judge the novelty of this incremental work. (3) The author should, I suggest, focus more on the actual performance such as memory and time overheads during network training and deployment inference, rather than theoretical MACs and FLOPs, (also discussed in ShuffleNet_v2 paper ) which can contribute more to the compact NN community.<BRK>To regularise against vanishing/exploding gradients and promoting more useful representations, the authors seek orthogonal filter basis . This builds in nicely with a lot of work in multi task learning about learning which weights to share and so I wonder about how your method might generalise to other problems   could the authors comment on this? I would suggest reworking this section to make it easier to appreciate the results. In particular, the nomenclature for ResNetL SsUu is confusing. 2.There is a typo in in the sentence above Equation (5) ; reusrive  > recursive3. Was it possible to analyse empirically the gradient magnitude to show that the orthogonality regularisation helped?
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 7. <BRK>This could be made clearer in the paper. The novelty is the extension to a new distribution: multinomial logistic normal distribution. This is achieved by using the Isometric log ratio (ILR) transform. Based on my understanding of the used metrics, I am not sure why this would provide empirical support of  the claim in this paper that the principal components can be recovered.<BRK>However, since there are no comparison in the paper, I fail to understand the benefit of one with respect to the others. In particular, they use the probabilistic PCA framework, extending it to multinomial distributed data. ### Reasons for scoreI fail to grasps the novelty with respect to the state of the art. The authors mention that such techniques rely on Dirichlet distributions, but that is not always the case (such as in [1] and [2]), where they show it can be approximated with a Logistic Normal.<BRK>The authors demonstrate a VAE model and estimation framework with which the PPCA subspace is recovered for data with multinomial observations. The authors claim that existing solutions are not scalable. The paper is clearly written with a few minor typos. There appear to be a number of scalable PCA techniques available where the full covariance matrix need not be formed and decomposed in memory. Could the authors comment in the rebuttal on how this approach<BRK>This paper is well written, presenting a great interdisciplinary work on covariance estimation. It relies on recent techniques such as connection between VAE and PPCA, ILR transformation, and presents an augmented VAE to obtain MAP estimates on multinomially distributed data. 2.Clarity can be improved on the part elaborating Algorithm 1, to clarify conclusive statements, eg justification on complexity, and put aside technical details. 3.Experiments part should have more clear intro on machine learning abstracted versions of the problems, which better fit general audience in ICLR.
Accept (Spotlight). rating score: 8. rating score: 7. rating score: 6. rating score: 6. <BRK>Summary Taking multilingual NMT (MNMT) into account, this work, investigates better model optimization alternative, that is in part can be attributed as a multi task optimization problem. Experimental results shows the proposed GradVac to contribute for the improvement of model performance. please re arrange the figures, I see discussion about Figure 5 while there is Figure 3 and 4 beforehand   if possible.<BRK>The paper proposes a novel method, GradientVaccine, to improve multi task optimization on a massive multilingual translation and named entity recognition model. The presented idea is very elegant and has been convinced by the theoretical foundation. Weaknesses:  The results of the proposed method seems consistent. In summary, this is a good paper. It presents a straightforward and useful idea for multi task learning.<BRK>This paper conducts comprehensive analyses and a method to the multi task training in multilingual models. Furthermore, this paper proposes a method called GradVac to improve the multi task training over standard monolithic training. Experiment results show GradVac achieves better accuracy than other multi task optimization methods. The motivation is clear, the analyses on the gradient similarity are interesting, and method proposed is effective according to experiments comparison. How about first choose a not good gradient than alter it instead of alter by random?<BRK>The paper studies the behaviour of gradient similarities across languages in multilingual NMT models. Hence, they look at method to gradient based methods for multilingual NMT. **Strengths**  I find the analysis of gradients and language similarities interesting and adds to the understanding of multilingual NMT models. Particularly, the study of hyper parameters to discover the best settings is very useful. So, it is not clear if there is a major benefit to the GradVac procedure over PCGrad.
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. rating score: 7. <BRK>Comment: Summary: This paper presents a technique for more expressive neural ordinary differential equations (NODE) flows. The authors propose two variants of the methods: $\textit{open loop}$ and $\textit{closed loop}$. Model performance is demonstrated on several tasks. The model is also shown to outperform the vanilla Augmented NODE method. The results are very impressive.<BRK>The paper introduces a novel approach N CODE, based on Neural Ordinary Differential Equations (NODE), that increases the expressivity of  continuous time neural nets by using approaches from Control theory. The paper is well written and clear. The proposed approach is original, and the results suggest that the performance of the approach leads to significant improvements compared to the state of the art. Also, while performance seems generally higher, the limitations of the proposed method are not clear.<BRK>The figures are also nice. However, reconstruction doesn’t really make sense as a task (since the identity function would be optimal in this case). It is interesting if the latent space learned is very low dimensional (i.e.the model compresses the dataset well), however there seems to be no discussion of this. The motivation for the paper is clear and fairly important: alleviating some of the representational weaknesses of neural ODEs and generally improving and extending neural ODE models by using time varying weights.<BRK>However, this paper uses a typical neural network for image reconstruction and generation. In the supervised part, has good visualizations of the decision boundaries induced. This issue needs to be addressed sufficiently. The images and figures are explained well.
Reject. rating score: 5. rating score: 6. rating score: 7. rating score: 7. <BRK>It proposes a primal dual optimization method and proves its optimality. The theoretical analysis in this paper assumes $P_a(s^\prime, s)$ is perfectly known whenever needed. In fact, in this case formulations (3), (4), (5), (6) are simple deterministic convex (linear) optimization (regardless if density constraints are introduced or not). The developed algorithm, Algorithm 1, is also simple application of (primal) dual subgradient method for convex programs. Overall, I think the technical contribution and novelty of this are marginal.<BRK>The duality between the density function and value function is well known in the community, and has drawn great attention recently in the policy evaluation community [1, 2, 3, 4], which I think the authors should have some discussion on this in the related work,  and to see if the techniques can be used to estimate the density functions. Overall I think the paper proposed a new perspective for density constrained rl, which I think is interesting and is publishable if my above concerns are well addressed. The empirical results demonstrate that the proposed algorithm  is effective in several mujoco benchmark and  autonomous electric vehicle controlling. Followings are my detailed questions and comments:  I have a major concern about estimating the marginal (or stationary) state function of the policy $\pi$.<BRK>I m leaning toward accept, but have several suggestions regarding the empirical evaluation, that are detailed below. Pros:* The paper is clearly written and well motivated. * It s not clear that the $[0, \infty]$ for the density function $\rho$ corresponds to $t$ (in the first sentence of the Density Functions section). * Why are there two $s$ s in $\rho_s^\pi(s)$ (e.g., in Equations 1 and 2). Should the subscript be a $\gamma$ instead?<BRK>The submission introduces the dual formulation of a Constrained MDP optimization. I rate my confidence low because I have some doubts about the novelty of Theorems 1 and 2. My main regret is the lack of discussion of the results, both on the theoretical and empirical side. Proposition 1: I m surprised the failure modes of Proposition 1 are not more discussed. What are their failure modes? Isn t it simply because the constraint is defined on the density level and that Algorithm 1 is the only one designed to solve this? What is it used for? Shouldn t it be simply S >R_>0?
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 5. <BRK>The overall impression I have of the paper is still similar to the initial one. Pros1.The proposed method achieves in the reported experiments better results than existing methods, for both single and multi step adversarial training. Cons1.The final loss used looks like a straightforward combination of TRADES and a regularization term on the norm of the Jacobian matrix at the adversarial points. Overall, I think the novelty is very limited, which combined with some concerns about the experiments makes me lean towards giving a negative score.<BRK>The proposed method improves the model robustness at each local patch and combines these patches through a global term, achieves overall robustness. The authors need to reorganize the presentation of this part to make everything clear. And equation (5)/(6) is actually useless since finally, the authors only rely on Proposition 1 to promote the local patch robustness term. My major concern is that the proposed method still does not have a very convincing intuition that why the local patch robustness term or combining the two terms helps.<BRK>In this paper, the authors introduce a novel technique (called LEAP) that improves model robustness at local patches (around an adversarial example) and combines "local patches" to get global robustness. Overall, the paper is well structured. However, it is a bit confusing at points, and sometimes not clearly motivated. The experiments seem fair and demonstrate that the proposed approach is better than other state of the art techniques. This paper reminds me a lot of [1] and the loss presented could be justified through other means.<BRK>**Summary:**The paper proposes a new way to combine existing techniques for improving adversarial robustness: adversarial training, Jacobian regularization and TRADES consistency loss. The obtained results on three datasets are better than the baselines which rely on adversarial training, Jacobian regularization or TRADES separately. **Cons:**  My main concern is the novelty of the proposed approach. “we **take care** of local balls at various points”   imprecise (what it means to take care in this context?) **Score:**5/10 because of the concerns about the novelty and clarity of the provided justifications of the method.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 4. <BRK>This paper presents a self training algorithm based on GCN to improve the semi supervised node classification on graphs. Second, the novelty of the presented approach is limited, as adding unlabeled samples with high confidence is not a novel idea.<BRK>The paper proposes an algorithm combining SVM and GCN to solve the node classification problem in label less scenarios. The proposed model uses a self training mechanism to generate labels and features and integrates SVM to improve the confidence level of the labels.<BRK>### SummaryThis paper proposes a self training based semi supervised framework for node classification using Graph Neural Networks when the amount of labelled data is very limited. 3.Choice of "t" seemed adhoc, incorporation of how "t" impacted the performance of the model would be interesting to see.<BRK>This manuscript proposes FASG, a self training model with GCN to improve node classification in graph. In my opinion, it is a general choice and the novelty is limited.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 5. <BRK>**Summary:** The authors propose a new NAS method that comes with a space of operations generalizing the convolution. The proposed method makes it possible for users to design their own search spaces according to the training data. The authors only conduct experiments on small datasets, e.g., CIFAR. Moreover, it is also not clear how different search spaces would affect the search performance? However, this method ignores the topology of architectures. In my opinion, architecture topology should be an important factor to search in NAS. 4.The authors argue that the proposed method “enables the discovery of good design patterns with limited human specification from data in under explored domains”. 5.What is FFT in Page 4? There is no definition before it appears, which makes the paper hard to follow. 7.It is not clear what the proposed search space is. 8.Several state of the art NAS methods should be compared in the experiments, such as PDARTS, PCDARTS, etc. 9.The writing of this paper should be improved. The paper is hard to follow and there are many grammatical errors:(1) In Page 3, “the main challenge … is intractability” should be “The main challenge … is intractable”.<BRK>Combining with differently architecture search method, # StrengthThe proposed Kaleidoscope Op is new to the field and seems to be an interesting aspect, searching for an operation similar to convolution but on different tasks. # Major weakness## 1. The paper introduces a simple operation set but with many unnecessary descriptions and definitions. This is confusing. It occupies a significant space in the main text but is not related to the methodology section 3. Will the author show some ablation study what happened if these properties are violated for some other operations? Why they are good properties in the NAS domain? It is strange to see the authors spend a large space to describe the common NAS algorithms in section 2 but do not clearly state how this K Op is combined with the search algorithms. On page 6, there is one sentence saying `K op comprise a continuous search space... , but what exactly is that? What are the settings of your experiments? Does it mean you generate the architecture after search and train from scratch on CIFAR 10? ## Weak baselinesThis paper essentially extends the original paper to make it searchable. Does it seem to be reasonable to compare against it at least over one task in the Kaleidoscope paper? The baselines, e.g.LeNet, in this work are too weak compared to the recent NAS approach. I understand this paper does not claim to be state of the art but showing the potential, yet with this baseline on CIFAR 10, it is hard to compare with the literature.<BRK>The paper claims to perform neural operator search on a search space defined by a family of Kaleidoscope operations. Here the author deliberately selects a family of operations that contains FFT and convolution. It is unclear how you constrain the search space of K matrices; I get you searched K operations, but I did not find it s structure and how does that different from FFT. There are a lot of ablation studies show that the searched K operations are better than convolution. However, none of them show they actually pushed SoTA results. Considering the fact that there are so many tricks (https://github.com/facebookresearch/LaMCTS/tree/master/LaNAS/LaNet) in boosting the performance of a CNN, it is more convincing to see the searched K operators can actually push the boundary. The ablation studies in the current experiments are not enough to convince me, especially they are focusing on relatively simple tasks, e.g.CIFAR 10, MNIST. c) Another thought, Tensorized Neural Network has also tried to replace of current operators, and their hyper parameters can also formulate a search space; My main concern about this line of work is "are we really making progress here"? Here we re building something based one prior knowledge; if we will end up someting similar to convolution, so what s the point of doing it? However, the paper lacks a strong evidence that they invented a new operators that actually work. This is my main concern of this paper.<BRK>Summary:The paper introduces Kaleidoscope operations to reprameterize convolutions. The reparameterization results in a more general search space of convolutions. Pros:The paper seems a good reading material to teach readers about a big picture of convolution search problem or even neural architecture search problem. The paper also shows a huge ambitious motivation to touch the boundary of the current main stream NAS methodologies. The idea of introducing repratermeterized convolutions (i.e., K operations that was originally proposed by Dao et al., 2020) to convolution search seems novel and promising to me. The evaluation is comprehensively conducted on several novel search spaces over vision and text data, and the results show the effectiveness of the proposed method. When being evaluated on permuted CIFAR and spherical MNIST, the new method shows some superiorities. Compared to regular NAS algorithms like DARTS that search for a much larger architecture space including convolutions, poolings, skip connections, this paper’s search space is merely on reprameterized convolutions. This makes me disappointed as both the title and the beginning parts somehow mislead readers that the paper aims at making a good innovation in the big scope of NAS. However, I finally realize that it actually searches for better convolutions rather than an entire neural architecture, after I went to the last paragraph of Page 5. This also reminds me that there exists one work [Stamoulis et al., 2019] which shares a similar motivation with this submission. This strategy also enables one level optimization and has a potential for supernet optimization as suggested by the submission. In Table 1, I find the best performances on CIFAR 10 and the transferring to CIFAR 100 are still far away from the state of the art. For a fair comparison, I see the results of the proposed K op with supernet SGD/SGDR are worse than Conv (fixed operation baselines, offline), while they are better when warm starting with convolution. Table 2 is not self contained. It should clarify the meaning of CR, MPQA, …, TREC in the caption. The last column of Table 4 seems confusing to me whether it corresponds to the case that uses warm start or from scratch.
Reject. rating score: 3. rating score: 5. rating score: 7. rating score: 7. <BRK>This paper concerns the problem of learning from single label supervision, when this label is known not to be the truth. The research agenda of the paper looks reasonable, even if it can be seen as a very specific instance of partial label learning (where one just considers the complement of the complementary label and tries to learn from it). A positioning with this latter approach therefore seems necessary. Most mentioned papers do not appear to have actually applied the setting. * Definition 2: I do not really follow definition 2. For instance in the first two pages only:  "supper"  > super  "A complementary label is only specific that the pattern"  "in some questions refer to private."<BRK>Summary: This paper deals with the problem of complementary label learning, that is, when we know the set of labels which a given observation does not belong to. There are many typos and other writing issues in the paper. The experiments are also weak. However, the definition of robustness of loss function is different in this paper. I am unaware of this definition of robustness of a loss function as it seems very specific to the complementary label learning problem. 2).The results in Table 2 are not an apples to apples comparison. The numbers for GA, PC, Fwd are copied directly from other papers.<BRK>The paper presents (two) simple yet insightful sufficient conditions for a usual loss to work well as a complementary label loss. Based on this, a simple training procedure, which minimally differs from usual training, is presented. Empirically it is shown that the proposal outperforms state of the art. Also the analysis 4.2, though simple, is elegant and insightful. For example, it helps to identify that MSE is not an ideal CL loss. 3.Overall, the write up is well organised; however at places I felt the presentation can be simplified a lot.<BRK>On the basis of the ordinary label learning, the authors defined "robust loss functions" for complementary label learning:  a a loss function is called robust  loss function if minimizer of risk with complementary labels would be the same as with ordinary  labels. Experimental results show that the proposed method outperforms other methods in several datasets. Overall, the problem is interesting and important, and the proposed algorithm seems reasonable and effective. However, I think the paper could be improved from the following two aspects. So, it is important to specify the exact form of complementary label to show the necessity of complementary label learning.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>This paper examines the problem of learning controllable embedding (LCE), with the goal of learning good representations (usually achieved using variational inference algorithms) such that the maximum cumulative reward can be achieved. One of the main strengths of this paper is found in Theorem 1. The authors devise a simple policy iteration approach in the low dimensional learned space. While I appreciate the quality of the theoretical work, the paper had some drawbacks that brought me to my current score :1.<BRK>This paper proposes a new representation learning + RL algorithm called CARL, with a specific objective for learning a latent representation and dynamics model coupled with SAC policy learning in the latent space. + The experimental results succinctly demonstrate the promise of the proposed approach. The experimental results are not sufficient for this largely empirical work.<BRK>This paper aims to address an important question in reinforcement learning: policy learning from high dimensional sensory observations. In the experiments, the proposed algorithm CARL shows improved performance when compared with other LCE baseline algorithms. The readability of this paper is also pretty good, which can be difficult to get right because the of the correspondence between the original space and the latent space.
Accept (Poster). rating score: 7. rating score: 7. rating score: 5. <BRK>The authors propose a modification of the well known actor critic algorithm, give a intuition for how it works ("adding an adversary") and present experiments showing state of the art performance on certain tasks on the VizDoom and MiniGrid environment, beating several recent baselines. That is, the actor is not optimized for it, confirming that the agent exhaustively covers the environment." In combination with the lack of apparent ICM, RIDE, etc lines in Fig.2 this makes for a confusing impression. Certain other claims in the paper seem overblown or indeed irrelevant, to a greater extent yet than usual in the field of reinforcement learning.<BRK>The paper presents AGAC, an architecture for efficient, and generalisable, exploration in RL in settings with very sparse rewards. Section 5.4 exploration with no reward. Section 5.5, diversity. The actor critic objective functions are adjusted as follows. The adversary itself is trained to minimize the KL divergence from the actor. Under such a setting the paper shows that the resulting objective, in addition to maximizing the return, it keeps the next update of the actor policy close to the previous actor and far from the adversary policy. I would like to see some more discussion on why is this so? I guess this refers to the exploration bonus, but it would have been useful to clarify that.<BRK>This paper proposed a new actor critic framework with adversary guide for deep reinforcement learning (RL), and introduced new Kullback Leiblier divergence bonus term based on the difference between actor network and adversary network to deal with the exploration in RL. The experimental results showed the merit of this method for exploration. Some comments are provided as follows. RL algorithms generally exploit more at the early stage and then explore at the later stage. But, this work fixed the exploration reward hyperparameter in learning procedure.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 6. <BRK>This paper introduces HyperSAGE, a method to address the problem of representation learning for hypergraphs. The two aggregation functions are not defined. In eq.(1) and (2), the reader can understand that the input of an aggregation functionis a subset of features. In eq.(3), the input is a node. Finally, an additional condition seems to be assumed in the proof while not mentioned in the main document (see sentence under eq.(11)).For these reasons, Sections 2 and 3 (as well as Appendix B) are difficult to read and understand.<BRK>(1),  it seems the reported performance of hypergraph based methods is not better than the results of GAT, GCN. cons The novelty of the paper seems limited. The proposed method defined in Equ. (1) and (2) is a straightforward extension of the message passing method for ordinary graphs.<BRK>Cons:         The novelty of the proposed method is limited to that of the aggregation functions. The relevant baselines and related methods are discussed appropriately. It would be better to include that in the main paper, since there is a discussion on train test already present.<BRK>[2] The proposed HyperSAGE framework is inductive. However, the intuition of the proposed framework is not very clear, and the model efficiency needs to be analyzed and evaluated compared to other baselines. Specifically, the pros and cons of this paper are summarized as follows. [5] There is a typo in Eq.(3).Should it be |N(v_i, e)|/|N(v_i)|?
Reject. rating score: 3. rating score: 3. rating score: 5. <BRK>This paper focuses on the problem of generating sparse l2 adversarial examples in a white box and surrogate/transfer setting. Having identified this region, the author use SGD to find the adversarial perturbations. The experimental results show that a high attack success rate can be achieved with this method. 2)	Also, the idea is not new implying that there exist several related approaches achieving sparse adversarial examples (see [1,2]). 4)	The robustness analysis of adversarial examples to image transformations or adversarial example detection methods can be interesting. [1] Kaidi Xu et al., STRUCTURED ADVERSARIAL ATTACK: TOWARDS GENERAL IMPLEMENTATION AND BETTER INTERPRETABILITY, ICLR 2019[2] Hai Shu et.<BRK>In this paper, the authors proposed a new adversarial attack method based on feature contributive regions. The authors firstly utilize an off the shelf method to extract FCR of the image and then utilize the FCR as constraint for the perturbation. 2) Although I am not familiar with the adversarial attack literature,  I think the method proposed in this paper is quite straightforward and the contribution of this paper is not significant enough for ICLR.<BRK>The paper proposes a method for adversarial attacking, which focuses on the contributive region of the input image. However, there are several weaknesses:1. The interested region based adversarial attacking is not a novel idea, it has been proposed earlier: Yao et al.Trust Region Based Adversarial Attack on Neural Networks, CVPR 2019. The feature contributive regions are generated by Grad CAM, as the authors mentioned in Section 3.2. The perturbation generation is also similar to general gradient based methods except for the mask of the feature contributive regions.
Reject. rating score: 2. rating score: 3. rating score: 3. rating score: 4. <BRK>##### Impact:The submission claims that other works that investigation compositionality in representation learning do not actually test compositional generalization ("because all combinations have positive joint probabilities in training"). However, I disagree that this is the case in prior work; here are some examples of prior works that correctly hold out novel combinations (of underlying components) for test time:  https://openreview.net/forum?id HJz05o0qK7  http://papers.nips.cc/paper/8825 learning by abstraction the neural state machine  https://arxiv.org/abs/1910.09113  https://arxiv.org/abs/1912.09713  https://arxiv.org/abs/1912.12179There are many such examples; they are too numerous to list here. ##### Quality: The algorithmic components in Section 4 are not adequately motivated, and the relationship of the algorithm to prior work in compositional representation learning is not discussed. The evaluation tasks are extremely simple (overlayed MNIST digits and conjoined word token) and are, as such, far from the complexity of existing work on compositionality (which can deal with, for example, naturalistic image data; see the references above for examples of such works). The "extraction network" is referred to several times in the introduction and methods section prior to its introduction/explanation. "compositional generalization is a type of out of distribution (o.o.d.) transferring or generalization, which is also called domain adaptation" This is inconsistent with the previously discussed definition of compositional generalization i.e., that it is not just domain adaptation. "We propose to obtain compositional representations not from the extractor but reversely from an auxiliary network." "These networks can be some existing networks for compositionality learning" If so, what are examples of "existing networks for compositionality learning"?<BRK>This paper aims to contribute on this topic and proposes some interesting datasets. None of the three papers cited in the introduction to motivate the work actually has the word "compositionality" in the paper  The authors claim that previous work has focused just on whether models can extract compositional representations in the training distribution while ignoring the test distribution, while actually most recent papers they cite in related work test compositionality by considering very specific train/test splits  The author s definition of compositional generalisation does not seem to take into account that compositionality is traditionally a property of mapping between input and output, not of a model itself. For this reason, much previous work on compositionality in neural networks has created datasets where the training and testing data were distributionally different (as also the authors of this paper do). In particular, their section on _localism_ is particularly important for the author s definition of compositionality. I do have a few comments/questions:  If the main motivation for wanting compositional generalisation is that this is an important capacity of humans, isn t it a problem that humans perform very poorly on the dataset (much worse than the best deep neural network)?<BRK>## SummaryThis paper studies "compositionality" and in particular the way in which it "transfers" on test data. ## AnalysisThe authors frequently say that *compositionality may not transfer to test distribution* but I have a hard time understanding exactly what they mean by this. As I understand it, "compositionality" is a property of a representation. Do authors mean that, on the test data, the representation of an input is able to separate multiple components, yet the same network does not separate the components on the test data? It may be true for the models they trained here, but I would have appreciated a comparison with other methods. I would have appreciated a thorough study of the "compositionaly" limitations of previous techniques. Given their claim that this is the *first work for the transferability problem of compositionality* the experiments presented on section 5 are on a new dataset and are not compared to previous work. Ex: *This work is orthogonal to many efforts of learning compositionality in training distribution.<BRK>The paper introduces a “transferability of compositionality” problem and proposes an approach to alleviate it. The said problem may arise when one trains neural models to produce “compositional” representations of the input. The manifold is estimated by saving representations of individual object representations from the training time. The problem that the paper considers is an interesting one. There have been a lot of papers on learning object oriented representations recently [1, 2], and an implicit assumption in all these works is that there is no statistical dependency between which objects that occur in the scenes. There is also the literature on disentangled representations that the paper extensively cites, where the independence assumption is also common. My concerns regarding the paper are as follows:   Positioning with the respect to the prior work. The literature on learning object oriented representations is not cited. The experiments appear to be technically sound.
Reject. rating score: 3. rating score: 3. rating score: 5. rating score: 6. <BRK>Studying the effect of the predictor is an interesting direction and will make the paper much stronger, as the authors also point out in the conclusion.<BRK>According to the authors, this decomposition allows for a better understanding of the training dynamics. From my perspective, there are too many unjustified claims, and I cannot recommend paper acceptance.<BRK>3.The implementation of BYOL in this paper regarding Cifar10 is not convincing. [2] also use the resnet18 as the encoder and it achieves the accuracy with 91+ in Cifar10.<BRK>*Quality*I really like the analysis of the paper.
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 6. <BRK>Overall, the general idea seems interesting and useful, but the novelty is unclear given the prior work; the clarity of the presentation can be improved which makes an accurate assessment of the significance harder. ### Detailed Comments* In the introduction, authors motivate the idea of amortization by making a distinction between optimizing the distribution versus a network whose parameters output the distribution. Can the authors provide an example of an algorithm that does the former for clarity (i.e.an example of a non amortized policy   what would that look like hypothetically?) For instance, $p_\theta$ has a dependence on $p_{env}$ in this figure, but it s not clear what that means. * The value overestimation section discussion seems to suggest that the iterative scheme does. Not clear what the take away from this section is in the context of the rest of the paper.<BRK>**Contributions**: The authors propose to use iterative amortization for policy optimization to help reduce suboptimality in policy optimization. On the other hand, in the Ant environment, the amortization gap of the direct method is much higher, yet the learning curves of both methods are practically identical. Some additional questions:To what extent is the amortization gap alleviated by direct amortization but larger, more expressive networks? Does doing so provide similar improvements in performance? In Figure 3b, it appears that with the iterative schemes, the new policy consistently differs the old one (what exactly is pi_old in this case?) Should we be concerned here that the learned policy (and presumably also the Q functions) are not really coming close to converging? Overall, I like the method since it provides a very simple way to improve learning by plugging in iterative amortization, but think it needs a to provide a better understanding of why it helps.<BRK>This paper draws an interesting connection to variational inference, categorizing current policy optimization methods with KL regularization as direct amortized optimizers. I think the contribution is novel and introduces a different type of optimization that is shown to improve current policy optimization methods. The results look convincing overall but I think more runs should be conducted. These two environments in particular were environments that showed significant difference between the two methods, and makes the reader wonder whether this improvement was due to optimization method difference or architectural change. I think at least a justification in this particular design choice is needed or perhaps all results for both models can be shown to help readers understand the general performance. Lastly, it would also be helpful to mention how many runs were conducted for model based value estimate experiment in Figure 8a.<BRK>The reason is that SAC uses “direct amortization” (a feed forward network from state to Gaussian params for the action distribution), and hopes that simple gradient descent (eqn (13) in the SAC paper) can sufficiently minimize loss (eqn (10) in SAC) over time, which is not true as Fig 6 in this paper suggests. In Fig 1 and 2, which state and which training iteration is being studied? RecommendationOverall, I’m slightly inclined to accept this paper. Could you explain why? The amortization gap is an interesting and often ignored phenomenon. I’d hope the authors understand my confusion and restructure the paper correspondingly. Consequently, the experiments are not comprehensive enough. Do they work? (b) If the amortization gap is an issue, do you see a problem in the expressivity of Gaussian? Does a more general form of policy (e.g.in the original SQL paper) further reduce the gap and improve performance? 2.The presentation is very confusing. If the paper pointed out SAC’s amortization gap earlier and how it seriously impacts performance, the paper would be easier to understand. “policy distribution parameters”: Easily confused with “policy network parameters”.
Accept (Oral). rating score: 8. rating score: 8. rating score: 7. <BRK>Therefore, each eigenvector serves as a player in a game and they will achieve strict Nash Equilibrium at the end, which enables a decentralized algorithm for large scale PCA problems. In the experiment, the authors conduct experiments on synthetic data, moderate scale data, and large scale data by resnet activation maps. The first two experiments demonstrate that the proposed algorithm is competitive with Oja s algorithm and even better under some conditions. The large scale experiment on resnet activation maps is only feasible by the proposed algorithm and demonstrate that it is a powerful tool to achieve interpretable representation. This paper is well organized and easy to follow. So it would be good if we can see some comparison of the lower dimensional features in some downstream applications. So I vote to accept this paper.<BRK>3.The experimental plots in the paper is too hard to see clearly. While there are a plethora of algorithms for PCA, along with accompanying analysis, a majority of these works have been developed from an optimization perspective. ***Post discussion period comments***The authors have satisfactorily addressed all of my comments as well as, in my opinion, comments of other reviewers. The main contributions of the paper in this regard are the following:  Setting up the PCA problem as a competitive game between $k$ players and showing that the Nash equilibrium corresponds to the PCA solution (Theorem 2.1)  Development of two games (algorithms), with one a sequential algorithm and the other a decentralized algorithm, for solving the PCA problem (Algorithms 1 and 2)  Convergence analysis of the sequential algorithm under a restrictive set of assumptions (Theorem 4.1)  Establishment of the equivalence between the decentralized algorithm and the Generalized Hebbian Algorithm (GHA) of Sanger (Proposition H.1)Overall, this is a novel paper in that it offers an alternate view of the PCA problem, which might lead to further advances in our understanding of PCA type algorithms in the future. 2.While Theorem 4.1 for the sequential game does not explicitly state it, it appears that it also requires the eigenvalues to be distinct (Theorem L.4, e.g.). This, once again, is a major assumption that is neither discussed clearly in the paper, nor compared to other works that do not seem to have this limitation. 3.Majority of the works in the PCA literature require the initialization subspace to not be orthogonal to the $k$ PCA subspace. Not only is this a strict probabilistic assumption in the case of random initialization, but it also becomes harder to satisfy as $k$ increases (as the authors also discuss). In light of this strict condition, this reviewer is confused by the claim in the paper that "these theoretical findings are strong relative to other claims." I would also have liked the authors to discuss this assumption up front in the paper. There is however no discussion of the connections between such approaches and the proposed sequential game. It would be helpful for the authors to comment on the differences between their decentralized algorithm and this distributed GHA work.<BRK>The authors present new insights on PCA analysis by reconceiving it in terms of a Nash equilibrium among different players, related to the different components. The insights lead to parallel algorithms and are demonstrated on large scale problems, which is nice. However, this statement is unclear and possibly incorrect: for coming to (2) the authors start from the solution i.e.the eigenvalue problem, while this should be the result of the derivation. This part should be clarified. Probably it is better to replace this part of the paper by a standard formulation and derivation of PCA analysis as given in standard textbooks. Also one can find it component per component and add orthogonality constraints in each step. it is nice that a parallel algorithm is obtained.
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 7. <BRK>This paper focuses on designing more effective ways for contrastive learning. Different from directly applying the stronger augmentations to minimize the contrastive loss, the author proposes to minimize the distribution divergence between the weakly and strongly augmented images. The experimental evaluations are conducted on ImageNet classification and related downstream tasks, and the results are promising. My main concern is the experimental comparisons. As we all know, contrastive learning algorithms like MOCO and SimCLR benefits from longer training epochs a lot (for example, training with 800 epochs is much better than with 400 epochs). Thus I think the comparisons in Table 2 are not convincing. From algorithm 1, we can find that the equivalent batch size of the proposed CLSA method is two times as classical MOCO method. It would be nice if some ablation results are provided.<BRK>This paper presents a method to incorporate stronger augmentations into the visual representation contrastive learning framework. Specifically, three correlated views of an image are first generated by using two weak and one strong augmentation operations on the same image. The method is evaluated on several visual tasks including classification,  transfer learning, and object detection, with the standard evaluation protocol for self supervised learning, and the results are promising. Cons:1.The motivation about using stronger augmentations is not well justified. Specifically, the authors propose to use stronger augmentations based on two reasons: (1) stronger augmentations can expose some novel useful patterns; (2) the effectiveness of stronger augmentations is proved in the semi supervised learning and supervised learning field. (Chen et al.(2020a)) even demonstrate that when training supervised models, stronger color augmentation hurts their performance. E.g.,  how does the performance change as the magnitude or usage times of stronger augmentations changes? 3.The proposed DDM loss seems general for different contrastive learning frameworks. I would like to see if it still works when applied to other frameworks, e.g., SimCLR, InfoMin? Overall, given the novelty and strong results of the proposed framework, I remain positive towards this paper.<BRK>Summary:\This work investigate the recent popular direction of unsupervised representation learning using contrastive loss between augmented images. Authors propose to minimize the divergence between the distributions of strongly augmented vs. weakly augmented images. It is not clear if this is will result in stable learning for the unsupervised setting, and what effect that may have on the performance and quality of the representations.\ Evaluation only focus on final result and lacks analysis of the proposed method, especially when compared to recent paper of similar nature published in top conferences. For example, strong augmentation is a focus of this paper, but there are no ablation regarding the augmentations. It is novel and interesting and seems to achieve good results. However the lack of both theoretical and empirical analysis beyond results on performance raises many questions.<BRK>This paper proposes the better utilization of strong data augmentations for contrastive loss functions in unsupervised learning. In Moco set up, typically, weaker augmentations such as color jittering, cropping is applied to construct positive pairs from the same image. In this study, by proposing a modified objective, the authors leverage stronger data augmentations to construct more challenging positives and negatives pairs to improve the quality of the representations. 3.The numerator in Equation 3 should be z_i  vs. z_i not z_k. As a baseline, it would be nice to directly use the stronger augmentations in MoCo v2 objective and perform comparison to their method. It helps to understand the full picture for the proposed method.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 5. <BRK>[Summary]This paper studies the training of multi branch networks, i.e.networks formed by linearly combining multiple disjoint branches of the same architecture.<BRK>This paper studies the multi branch neural networks. The paper claims that concatenation is equivalent to sum aggregation for ResNeXt. 7.The aggregation form that the paper has studied is the weighted sum of multiple branches.<BRK>This study focuses on the stability of multi branch networks. The analyses are based on the assumption that all branches have the same structure without normalization layers. Pros:The paper is well written. 2.How to quantize stability?<BRK>##### SummaryThis paper proposes to scale down the features of multiple branch networks by $1/\sqrt{C}$ during aggregation where $C$ is the number of branches. It is very suspicious that the authors do not report the BLEU (+STAM) of the first row in Tables 2 & 3. This is a false claim.
Reject. rating score: 4. rating score: 5. rating score: 7. rating score: 7. <BRK>This paper presents a framework for adversarial robustness via incorporating local and global structure of the data manifold. This is very confusing. Experiments are provided on several datasets and demonstrate significant performance improvements. Pros:1.The key idea of using the global data manifold into the robustifying framework is quite interesting. Or for that matter, how will the proposed approach achieve adversarial robustness ?<BRK>Summary: This paper considers the local and global information in adversarial attacks for adversarial training, where the authors design an adversarial framework containing a discriminator and a classifier. The idea is interesting and the paper is easy to follow. However, I have still some concerns below:   The novelty of this work combines the idea of PGD (local information) and Feature Scatter (global information) .<BRK>The paper proposes a new method of improving model robustness by generating adversarial samples that are regularized by their latent distribution through f divergence, whereas existing literature only uses local manifold property such as smoothness. The method is well motivated and the clarity of the paper is good. The experimental results are compared with several competitive baselines and the improvement looks significant (Although I am not familiar with the state of the art experimental results).<BRK>The paper analyzes the property of local and global data manifold for adversarial training. In particular, they used a discriminator classifier model, where the discriminator tries to differentiate between the natural and adversarial space, and the classifier aims to classify between them while maintaining the constraints between local and global distributions. The authors implemented the proposed method on several datasets and achieved good performance. This paper was, in general, well written.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 7. <BRK>The paper proposes replacing each linear layer with a linear ensemble (average) of m (m>1) linear layers. Authors argue that this helps to reduce network internal variance and achieve better accuracy. I think it is possible to see that the proposed model is equivalent to the baseline model with a different hyper parameter setting (specifically the random initialization and learning rate), and therefore the observed variance reduction and improved performance are a result of better hyper parameter choice in the proposed model.<BRK>**Summary**This paper proposes "Inner Ensemble Network" that ensembles intermediate layers in a deep neural network, which would be trained simultaneously during the training. Similar to the classic ensemble methods, this method can reduce the variance of the training and achieve better than the models without the ensemble. **Originality and significance aspect**This is probably the most weak aspect of this paper. Authors claim this paper is simpler to apply than [2], but they do not have any empirical evidence that their proposed method is better than [2]. "Simultaneous training of negatively correlated neural networks in an ensemble." For the experiments, I think authors should actually use the classic version of ensemble (call outer ensemble in Section 5.2 or in Table 2) as their baseline instead of a single model. If authors are concerned about the inference computation time, they can consider a distilled model from the classic ensemble method, which is a fairly standard technique. **Recommendation**Overall, the paper is interesting; however, I cannot recommend this paper mainly due to weak novelty and weak empirical support. The main concern of this paper, the weak novelty, still remains (also pointed out by Reviewer#1/4).<BRK>Summary:In this paper, the authors proposed Inner Ensembe Networks (IEN), which trains an ensemble of layers and therefore reduces model predictive variance. Therefore, the improvement of the proposed method (IEN) on variance reduction over dropout and maxout is theoretically justified. The authors also show the boundary case where IEN achieves the same variance reduction as dropout and maxout. The empirical evaluation matches the theoretical analysis. Figure 3 demonstrates how ensemble size affects the ranking of all methods and all architectures considered in this work. Overall, the paper proposed an interesting method to efficiently reduce model variance, which leads to an improved performance. But the cons outweight the pros in its current version.<BRK>For inference, the weights are averaged themselves, so the params count in inference is the same as without the inner ensembles. They also perform experiments with various image models and finally show theoretically and empirically that their method leads to a greater decrease in variance and error rate than similar methods (maxout, dropout). However, the presented theoretical analysis lacks rigor and has to be updated to be formally correct. The proposed method being simple means a lot of related works used methods similar to the proposed one. Better move it to the main part of the paper. Better keep only one of them for clarity. The authors should have a more extensive explanation of what is new in their paper and what distinguishes it from that existing one. * (Section 2) (Opitz et al., 2017) does not force the use of a special loss function instead of a main loss but instead suggests using additional loss function to achieve better performance through making weights more diverse. Adding those additional losses may be an interesting new set of experiments to run. * (Table 2) It is unclear why the outer ensemble outperforms single model IEN (not the $IEN_{\widetilde{w}}$). * (G) The dropout results are very poor. It suggests the models were not operating in a normal state.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 5. <BRK># Justification of ratingThe zero cost proxies used in this paper are mostly inspired by recent pruning literature. I generally feel positive about this work due to its extensive analysis and the nice idea of zero cost warmup/move. Most of the weak points mentioned above are for clarification. Designing zero cost proxies for NAS is a promising direction to reduce its cost. This paper evaluates a series zero cost proxies and demonstrates their usefulness.<BRK>Overall, this is a solid work with interesting ingredients. The main contributions are mostly in terms of empirical results rather than a new search method. Strengths* The paper is very well written and easy to follow. Weaknesses* The technical novelty is limited in the sense that (1) the idea of zero shot NAS is not new (Mellor et al., 2020), and (2) the best zero shot metric (synflow) is a straightforward application of the existing work.<BRK>In particular, it proposes a series of zero cost proxies. On the other hand, the experiments in this paper are very sufficient and strong. Cons:1.In Eq.(1), it seems that you use a Hessian Matrix to compute grasp metric. However, the computation of a Hessian Matrix is very expensive. Or you have some techniques to bypass the computation of the Hessian matrix? INTRODUCTION： operate at at > operate at2.<BRK>This paper provides zero cost proxies that can estimate the performance of the given neural architectures at the initialization step saving time and resources. 2.The idea of this paper is simple yet effective, where this paper is inspired by pruning at initialization methods and adopt those methods as proxies for NAS methods. ############################################################Reasons for score  Overall, I vote for weak rejecting. 3.I recommend the results of cifar10 and imagenet120 of NAS Bench 201 move to the original paper from the appendix.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>#########################################################################Pros: 1 Compared with existing weight manipulation AT methods, this paper first utilizes stochastic weight averaging (SWA) (averaging multiple checkpoints along the training trajectory) without incurring computational overhead. #########################################################################Cons: 1 The paper’s novelty is marginal. For example, to my knowledge, three papers got accepted with the shared philosophy but slightly different techniques/decorations [1, 2, 3] Second, as the authors mentioned, manipulating model weights is also shown effective [4]. [1] Metric Learning for Adversarial Robustness, NeurIPS 2019\[2] Rethinking Softmax Cross Entropy Loss for Adversarial Robustness, ICLR 2020\[3] Boosting Adversarial Training with Hypersphere Embedding, NeurIPS 2020\[4] Revisiting loss landscape for adversarial robustness, NeurIPS 20202 This paper hypothesizes “one source of robust overfitting might lie in that the model ‘overfits’ the attacks generated in the early stage of AT and fails to generalize or adapt to the attacks in the late stage.”  It is not clear to me why this hypothesis is valid. Would you explain more justifications? More adversarial training on different network structures are needed, e.g., Wide ResNet.<BRK>Summary The paper leverages two methods for improving generalization in standard training, logit smoothing and stochastic weight averaging, and show that these results can mitigate robust overfitting and improve generalization for adversarial training methods. To be clear, since a number of these approaches on the benchmark are quite recent, I am not requesting that the authors directly compare to these new methods in their work. This should alleviate most concerns on the validity of the result 2. Update I have looked through the response and edited version.<BRK>The paper studies a method for mitigating robust overfitting. is quite brittle relative to choices of hyperparameters such as slight differences in weight decay. The paper proposes an alternative to early stopping: smoothing the logits and smoothing the weights, by using  two existing techniques, namely self training and stochastic weight averaging.
Reject. rating score: 3. rating score: 6. rating score: 7. <BRK>2.An expansive path, symmetrically constructed, with upsampling by NN interpolation, and a final softmax activation. The paper proposes an architecture tailored for very specific setting of 2d images. 3.The fact that only one dataset is considered undermines the paper, which focuses on numerics. 5.The paper does not provide other metrics (such as inception scores) to assess the performance of the methods other than visually. I recommend a reject on the aforementioned grounds.<BRK>Results are compared with other approximation methods. Reason for my score:This paper is ell written, and the idea is interesting. However, the elephant in the room is: How close are these learned barycenters from the actual barycenter? This limits the impact that the paper can have on a broader audience. 2.It is a less exciting if the loss function used is not OT itself but KL. 6.For some basic distributions it would have been more useful to compare the learned barycenter with the output of the linear program.<BRK>The paper is clearly written and presents a fairly objective view on their work. The benefit of the proposed architecture is that it can be adapted to compute the barycenters for any given number N of inputs while only trained on pairs (N  2) of input measures. The experiments are compared against and trained with the GeomLoss code which seems to be state of the art for computing entropic regularity. They also compare with Wasserstein embeddings methods favorably. — References to literature in using deep learning for solving inverse problems would be welcome. If I understand correctly, the training data is an approximation of Wasserstein barycenters between two measures. I do not understand this statement.
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 6. <BRK>The paper focuses on the cold start problem where few samples with a new feature observed is available. Pros:  Methods straightforward and easy to understand, and does not have much ad hoc design choices that are hard to validate. The experiments are relatively thorough. Experiments does not do external comparisons with other recommender systems that deal with cold starting other than its own baselines. Despite the large amount of baselines, ablation still lacks: (1) Train from random, but instead of training for a fixed number of epochs, just train an SVM or linear regression until convergence. Not clear if the set of datasets is persuasive. One is a synthetic dataset. One recommendation system and one grading dataset. Post rebuttal:I appreciate the additional ablation study, but unfortunately the results did not strengthen the paper s distinction from related work.<BRK>The CHN is applied to P VAE and some experimental results are provided to demonstrate its effectiveness in some application, i.e., recommender system, e learning and healthcare tasks. The motivation is clear and reasonable, and the proposed method with hyper network is pretty interesting and attractive. * The logic of paper is clear and easy to follow. * The paper only shows the advantage of prediction time, but it doesn’t discuss a lot on the training latency. * The experiment setup is not strong enough to demonstrate the effectiveness of the proposed methods, for example,    * Lack of major important baselines and studies. In the paper, only several extension on how to handle new features on top of P VAE is given in the comparison. However, it is unclear how it performs with other methods which target at solving cold start problem. * Limited evaluation metics.<BRK>This submission focuses on the cold start problem of new entities (new items in a recommender system, new treatments in a medical application, etc.). Pros:  The writing is clear. Good reproducibility. Details, including hyperparameters, are listed in the appendix. What’s new when compared to Vartak 2017, and how well the proposed method outperforms Vartak 2017 empirically?<BRK>This work proposes CHN, a framework to extend an existing model to incorporate new features as they become available. The benefits of CHN are demonstrated by utilizing it to extend a P VAE. This $\hat{\theta}_n$ can then be used with the base model s representations (encoded vector for the P VAE setup) for downstream tasks. However, in section 2.3 it is mentioned that $k_n$ is sampled from Uniform[0,32]. How is the new feature incorporated in the model for future use i.e., in a continual learning type of setup. This seems to be a very practical requirement. How will the CHN be used for classification or other types of supervised learning tasks.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>This paper proposes a cross modal (audio and video) contrastive learning scheme to pretrain on one of the modalities. They also propose a sampling scheme for the negative examples. The experiments and the corresponding ablations suggest that the proposed methods is helping overall. Also, the samples in the appendix seem to suggest that the proposed ` active negative sampling``   scheme seems to provide more relevant negative samples.<BRK>It builds on the Momentum Contrast (MoCo) paper (2020), which aims for a moving average dictionary per modality along with cross modal contrastive learning for self supervised tasks, evaluated on audio visual datasets. While emphasis is good, I found the repetitiveness to affect my interest in reading the paper. SummaryThis paper proposes an approach to actively sample negative examples for audio visual representation learning and combines this with an audio visual approach for contrastive learning. The paper suggests that an active sampling approach that samples datapoints that are are both diverse and uncertain would result in a much more informative dictionary of negative samples, and uses gradient information and the k MEANS++ algorithm to create such a dictionary of negatives. StrengthsThe paper presents novel, well motivated results in the field of cross modal representation learning.<BRK>#### SummaryIn this paper, the authors propose a cross modal (audio video) self supervised representation learning method with a contrastive learning framework. To overcome the high redundancy in the negative samples, they propose an active negative sampling method. Audio visual self supervised learning could be interesting for many researchers in the computer vision community. #### WeaknessesHowever, I have a few concerns about this work. * The comparison of the proposed method with the fully supervised method is unfair. I cannot find a 7.2p improvement on the UCF 101 and the 14.1p improvement on the HMDB 51 compared to the fully supervised method. It is okay that the proposed self supervised method is weaker than the fully supervised method. #### Minor comments:* 6.<BRK>The goal of the paper is audio visual self supervised learning using an active sampling technique to mine hard negatives during training. The strategy for selecting negatives based on diversity seems well reasoned and experimentally outperforms random sampling and OHEM. Weaknesses:   It would be nice to see some discussion of the other self supervised contrastive works that also focus on the optimal selection of “negatives”: eg. In a similar vein, the comparison to OHEM in the supplementary is quite nice and I believe should be in the main paper.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 5. rating score: 5. <BRK> SummaryThis paper studies automatic learning rate tuning in Multi Agent Reinforcement Learning (MARL). It proposes AdaMa, an algorithm which balances the learning rates of actors and the critic, and can also make use of the second order information. Why is this helpful? The derivation of $\Delta Q$ can be deferred to appendix.<BRK>This paper proposes an algorithm called AdaMa for multi agent reinforcement learning (MARL). Based on the contribution of the critic and actors, the algorithm adopts adaptive learning rates. Numerical experiments are provided in four cooperation scenarios to show the performance of AdaMa. The contents in Section 3 is largely based on heuristic approximations. In addition, there is no comparison with state of the art algorithms, which limits the contribution. For example, there is no formal description of the proposed algorithm.<BRK>The paper studies adaptive learning rate for Actor Critic style MARL algorithm. Some optimization algorithms with adaptive learning ate are mentioned in the related work section. However, it is not clear why these methods cannot solve the problem and we need new techniques. Is this possible for, say single agent setting? Otherwise, we will need to work with the significantly larger observation space instead of the state space?<BRK>I think more experiments on larger scale tasks are needed to make the effectiveness of the proposed method convincing. This paper proposed AdaMa, which can automatically use adaptive learning rates for each agent in cooperative Multi Agent Reinforcement Learning (MARL). The proposed method is intuitively reasonable and verified by small scale experiments.<BRK>Summary:The paper proposes a new algorithm for multi agent reinforcement learning (MARL) that adaptively picks learning rates for actor and critic. Specifically, the learning rates are updated to directions maximally affecting the Q function, and the algorithm dynamically balances the learning rates between actor and critic. Pros:+ The choice of learning rates in MARL is an interesting and important issue. The learning rate balance between actor and critic is well motivated. More experiments are needed for more reliable results.
Accept (Spotlight). rating score: 7. rating score: 7. rating score: 7. rating score: 7. <BRK>#### SummaryThe authors present a method for online policy adaptation during domain transfer in the case no reward is available in the target domain. They achieve this by adding an auxiliary self supervised task, such as inverse dynamics prediction, that helps shape a set of features shared with the policy during training. #### Pros  The authors present a novel, intuitive method for domain transfer in the case of absence of reward signal. This is very relevant for e.g.sim2real transfer, where reward computation often require privileged information that is not easily measurable in a real world setup. The authors perform a large sweep of experiments on a variety of different tasks, incl.sim2real transfer on a robot arm, and provide significant detail of the experimental setup. The results are largely in favour of the proposed method in comparison to fair baselines. The paper is very well and clearly written, with significant attention to related work. #### Cons  My main concern with this submission is the lack of any discussion about the assumptions and resulting limitations of the proposed method. It is only in Section 3 that it becomes clear that this method is aimed at _perceptual_ adaptation, but otherwise assumes the transition dynamics of the source and target environment, and hence the resulting optimal action, to be the same. It s unclear how this method fairs in the case of a change in dynamics.<BRK>This paper presents the Policy Adaptation during Deployment (PAD) method, which allows a policy trained on a particular visual input distribution to be transferred to a new visual input distribution, as long as the underlying system dynamics remain the same. PAD works by structuring the policy into two parts, a state embedding network pi_e and an action selection network pi_a. Additionally, PAD learns an auxiliary prediction task with a network pi_s that predicts the inverse dynamics (necessary action to transition from s >s ). I think the more general question is how to adapt in MDPs when part of the MDP changes but another part stays constant. The  trick  in my opinion in this paper is that the dynamics are consistent across task variations, so building a predictor for this consistent dimension works. "), could you clarify?<BRK>The method uses both a self supervised representation learning objective and an RL objective during training, then at test time (when RL supervision may not be available) adapts the agent to a new environment using only the self supervision objective. The self supervision objective is to learn an inverse dynamics model $p(s_t|a_{t 1}, s_{t 1})$ using an IDM network which shares its convolutional layers with the policy network $p(a_t|s_t)$. The authors provide experiments using a variety of simulation environments augmented with adaptation target tasks, then demonstrate the effectiveness of the method using real robot experiments. ### Originality4/5This work s originality stems primarily from its simplicity. ### Significance4/5Addressing test time adaptation without costly pre train methods (e.g.meta learning) is an important challenge for RL in the real world, and this work provides a step in this direction. but this text contains no evidence that the proposed method could achieve that kind of adaptation. Adaptation to new reward functions and dynamics likely requires a different family of methods than those addressed by this work.<BRK>This paper studies an important problem in vision based RL: how to adapt a pre trained policy to an unseen environment in a self supervised manner. To do this, the authors introduce an auxiliary task branch that can be used to tune the intermediate representation of the policy network on the fly in a self supervised manner(e.g.inverse dynamic prediction). The experiments in the DeepMind Control suite, CRLMaze, and robot manipulation tasks show the generalization and effectiveness of the proposed method in various vision based RL problems. Strength:  The paper is well written and easy to follow. The proposed method is simple yet effective. The author conducts experiments in various environments and provides a comprehensive analysis of the results. The implementation details are also detailed for reproduction. Weakness:  Lack of novelty. The proposed method can be viewed as a simple application of the self supervised auxiliary task in the domain adaptation. How does this method perform in this situation?
Accept (Poster). rating score: 7. rating score: 6. rating score: 4. rating score: 4. <BRK>By using a local momentum, this method can be extended to all other existing robust algorithms. The authors also provide some theoretical analysis of the effect of their algorithm. 3.This paper has comprehensive experiments that compare the combination of different attacks, defenses and datasets. # UpdateThough the theoretical analysis is a bit weak, I think the experiments are quite good. The code can also run without any issue, which is a significant contribution in my opinion.<BRK>The results are done in a rigorous and reproducible manner. Their inclusion does not seem within the scenarios considered. In this vein, I don t find much realism of the addressed attack scenarios. Conclusion:I think the paper is  well written contribution to the literature on Byzantine attacks for stochastic gradient descent. I think it will be significant to researchers in the area.<BRK>I am having trouble identifying the contribution of this paper. A number of defenses augmented with this approach is studied experimentally against two recent attacks. There are no clear guarantees of the proposed mechanism that I could find as the paper seems to be focused exclusively on experimental results. Overall, combined with the fact the Byzantine setting itself is already a stretch in terms of realism, this paper doesn t seem to have much lasting value. I would suggest identifying and studying broader classes of attacks/defenses of which presented ones are special cases and giving at least some guarantees in terms of what the proposed approach provides.<BRK>The paper s method is quite simple and it argues that the latest distributed stochastic gradient descent (SGD) state of the  based on the Byzantine model can be tackled using momentum based versions of (SGD). There is not much contribution by the authors in this area. More insight and analysis are required and I am not sure how it may behave other attacks that are not studied in this paper.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 7. <BRK>It is interesting to see that so many graph networks can also be unified in the same framework. A unified framework does aid in both theoretical analysis and implementation of GCNs. However, the claims and derivation do not seem to account for the non linear activation in the networks, and hence, significance of the work seems limited. Since the proposed method combines multiple types of regularisation, it is expected to perform better than other networks. Clarity and orginality: The paper is otherwise well written / organised, and the theoretical contributions (although technically straightforward) seem original and somewhat interesting.<BRK>Experiments on the standard settings of node classification on Citeseer, Cora, and Pubmed prove the effectiveness of the proposed regularization techniques. Overall, this is a very interesting paper, proposing a unified framework for different variants of convolution based graph neural networks. What if we have to consider the nonlinear transformation? (2) In the case of linear GNNs (without nonlinear transformation matrix), it is actually not surprising formulating GNNs as a regularized optimization problem. Such a regularization framework has already been discussed in the original GCN paper (Kipf et al.2016).(3) In the case of linear GNNs, the overall framework is also very similar to the traditional label propagation framework (Zhou et al.Learning with Local and Global Consistency).<BRK>This is similar to the idea of batch normalization. This is reasonable because GCN tends to correlate the learned features with the graph Laplacian embedding (the optimal solution of the 2nd term in the authors  framework). This is interesting but empirical. As in any regularization framework, there is an additional parameter involved that is the regularization strength (\alpha_3 in 21). Based on the novelty, a more proper venue for publishing this work could be relevant journals. Overall this submission presents a borderline case and I recommend weak acceptance. It is not non trivial enough to combine several linear operators into a unified optimization framework. As you mentioned instability, it is worth to have some toy example to demonstrate the instability and study the cause of such instability and show how to avoid such instability using the proposed regularizer.<BRK>Overall, the empirical evaluation feels a bit shallow by only evaluating on small benchmark datasets, but might be sufficient for a work that has mostly theoretical contributions. The paper is mostly well written, although though to understand on first read. Sadly, I cannot find the newly added section regarding the non linearity analysis in the revised manuscript and therefore cannot judge the findings of the authors. On the other hand, it is not exactly clear to me how the proposed regularization technique differs from PairNorm (which is build upon similar insights by preventing node embeddings from becoming too similar).
Reject. rating score: 4. rating score: 4. rating score: 4. <BRK>The sequence of figures are modelled as a Marlov chain, which work in  a generative+discriminative fashion to output the final prediction of the model. ContributionsThe paper addresses an application of HAR which I see can be of interest for the community due to its novelty. The state of art is adequate and gives enough background on the domain Points to improveThe principal flaw of the work, and main reason why I do not recommend its acceptance in the proceddings is its weak experimental setup and overall evaluation.<BRK>The paper describes and motivates an application, describes data collection and explores a number of (standard) methods. The authors envision a scenario where the audience is informed about the individual dance moves through automated recognition. * Automatic segmentation of dance moves is not explored.<BRK>The paper is fairly clearly described, but the work uses fairly standard model structures and a very limited training set, so I m not sure the reader really learns anything novel from the paper.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>SummaryThe paper studies offline policy evaluation (OPE) and optimization in the model based setting. The paper is well written, the proposed extension is simple to implement and convincingly outperforms baselines on a variety of environments. Strengths  The paper is written clearly and generally easy to follow. Using autoregressive models does not address aspects that are specific to the offline setting.<BRK>#### SummaryThe authors consider the usage of autoregressive dynamics models for batch model based RL, where state variable/reward predictions are performed sequentially conditioned on previously predicted variables. Extensive numerical results are provided in several continuous domains for both policy evaluation and optimization problems. #### Pros  The paper is very well written and easy to follow.<BRK>The authors study the impact of better models on OPE and on policy optimization. The paper is generally well written. The default ordering may be completely arbitrary, how is the new dynamics model able to cope with this in the experiments? In other words, it is unclear if P(s_i|s_{j<i},...) is appropriate at all without knowing the ordering. >> We ultimately care not about the log likelihood numbers, but whether or not the dynamics models are faithful in policy evaluation and optimization.
Accept (Poster). rating score: 9. rating score: 8. rating score: 6. rating score: 5. <BRK>In this paper, the authors propose MSR, a parametrization of convolutional kernels that allows for meta learning symmetries shared between several tasks. The paper is interesting and is easy to read. 3.The related work as well as the experimental part allow for a clear positioning of the proposed approach. I did not find any major weaknesses in the presented paper. I enjoyed reading the paper.<BRK>Within the meta learning framework, they learn both $U$ and $v$ as part of the outer and inner steps respectively. This is very interesting and exciting with the ability to "learn" rather than "hope" that equivariance is learned from appropriately augmented data. They are visual evidence of achieving what is expected from the proposed approach. CONS:  The biggest concern for me are that the experiments are largely on synthetic data which has been randomly generated (Sections 5.1 and 5.2). I don t expect better results but some discussion of comparable performance without the handcrafted design or the ability to learn equivariance for groups without a handcrafted filter available will be a huge plus. REASON FOR RATING:I like the paper and it makes a very good contribution in an important area.<BRK>+ The work learns partial translational symmetry, euqivariance to rotation and flips+ The work performs important ablation studies, such as testing reparameterization with and without meta learning by using multi task learning instead. Adding a reference to this line of work may improve the introduction. Typos on page 14 may be fixed.<BRK>### Summary:The paper presents a meta learning algorithm to learn/encode equivariance into deep nets. They authors motivated this approach stating that data augmentation may not be practical for robotics application which requires training in the real world. Overall, I think the paper is interesting and relevant to the community. However, there are some questions that should be addressed. 3.For the experiments, I think a necessary to have a baseline that treats both U,v as trainable parameters. I wonder if it is necessary to train U and v separately on train and val; Maybe the performance gain comes just from the fact that U is trainable, i.e., this model architecture benefits learning. How are these hyperparameters searched? Maybe also mention the issue of  data augmentation with real world data in the introduction?
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 5. <BRK>This work presents a generative model for multimodal learning. Using the experiments, the paper demonstrates that the proposed approach improves multimodal learning, data efficient learning, and label propagation. The paper is well written and easy to follow. The experimental results across all three tasks demonstrate the efficacy of the proposed approach. Can the authors do such a comparison?<BRK> The paper proposes a contrastive objective that (1) minimizes the distance between "related" samples while (2) maximizing the distance between randomly paired samples. The novelty lies in the optimization of (2) which can further benefit from unimodal samples for which no "related" samples of the other modality are available this can be viewed as a semi supervised approach for weakly supervised multimodal data. Do the results in Figure 8 depend on this threshold and would it be reasonable to show an ablation acrossdifferent threshold values? The related work section could benefit from a short paragraph onweakly supervised learning, which seems to be an important theme in the presentpaper. If the authorsrefer to generative tasks in particular, this should be stated more clearly.<BRK>SummaryThe paper proposes a contrastive multimodal generative model framework for including unrelated datapoints as well as related datapoints in the learning of the multimodal model. Using such a contrastive formulation, the paper shows that the proposed model outperforms previous work on four desiderata for multimodal generative models. Also, intuitively, why is it that using unrelated data makes us believe that the models should be more data efficient? Essentially, any insight on when one should and should not do contrastive training would be useful to add to the paper. I thank the authors for addressing all of my concerns in the rebuttal.<BRK>It is kind of strange because we make a hypothesis about the model after training and optimize the model to reach the hypothesis. Experiments:I m not an expert in multimodal learning. The evaluation metrics are not commonly used but chosen following a related work. The paper claims that the main empirical contribution is its data efficiency, which seems to be verified. **after rebuttal**Some of the issues are clarified.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. <BRK>The authors are working in the context of a denoising process that runs in the reverse direction to a sequence of steps that each add a small amount of Gaussian noise to the original data. The proposal is to introduce an auxiliary function that breaks the Markov assumption by leaking some information in a controlled way about the training points x0, and then use this auxiliary function as scaffolding to train the actual Markov chain of denoising functions. This paper builds on other works in the recent literature and proposes something useful and novel. They propose something relatively down to earth and then they methodically analyze the consequences and derive all the mathematical formulas that follow. I feel that the dose of mathematical content is just appropriate for what they set out to do. Less would be too vague, more would be excessive. The direction that they are proposing is relevant, contrary to certain other papers that involve a lot of correct equations but don t take us anywhere interesting. In my mind, this is a very good clean paper. The only bad things that I could possibly say about this paper would involve comparing it to certain other wildly creative papers, and to say that it s not as innovative or throught provoking as those papers. And that s not a fair thing to say. I would like to ask the authors if, with their framework, there is a need to train a completely different epsilon_t for every t 1..T ? I presume that the thing that makes this reasonable is the fact that S < T, and only S different models need to be trained?<BRK>This is accomplished by changing the “forward” process which adds noise to the data. This paper proposes to replace this Markov forward process with a non markovian process that is designed to have the same marginals. The generative model, in this case, changes such that to predict the next step in the process, the model must first predict the “clean” sample at the end of the chain which is then used to give an estimate for the next step in the chain. Thus, the models differ only at sampling time. Inside this family, there also exists an implicit generative model which can be sampled from in a more deterministic fashion than the other members   hence the name of the paper. The authors present a number of image generation experiments and study the impact of the sampling parameters on sample quality. Strong areas:I quite enjoyed this paper. I find that this work provides more insight into how these diffusion models work and which choices in the original presentation are important for their performance and which (like the inference process) can be changed. These are important insights and should be impactful for further research on this class of models. Weaknesses:While this is a strong paper, there are a few issues in my opinion. I found some of the experimental details difficult to understand. In section 3.2 you claim some equivalence between the original DDPM objective and the variational bound in your model. This looks like re weighting the various terms in the objective. In the DDPM work, they discuss the impact of this re weighting. In reading the abstract, it was not clear to me that using a non markovian forward process would make sampling faster. This became clear as I read the paper. My recommendation:I think this paper was clearly written and proposed a strong contribution to the field of generative modeling. I think the insights presented here give us more understanding of diffusion models and while also improving their sampling speed. I will recommend accepting this paper. Some questions:Do you think these same insights could be applied to discrete diffusion models proposed in the original work on non equilibrium thermodynamics?<BRK>#### DESCRIPTION This paper consider tweaks to denoising diffusion models, exploring non Markovian inference models, as well as shorter and possibly deterministic generative trajectories. #### DISCUSSIONI m having difficulty placing this work in the correct context, and disentangling exactly what the contributions are. I m also a little confused by the choice of language in the paper. Traditionally, a diffusion process describes a continuous time stochastic process which satisfies the Markov property. If you further make the inference process non Markovian, so that you have a non continuous, non Markovian process, does it make sense to persist in using the word diffusion to describe the model? This may be a nitpick, but I m also not sure about the sense in which this model can be described as  implicit . from this paper could equally describe a standard normalizing flow, which is certainly not an implicit model. As far as the experiments are concerned:  It seems the takeaway message from section 5.1 is that the proposed model better adapts to evaluation with shorter trajectories when eta is small, and that the original DDPM doesn t fare well with shorter trajectories. In section 5.3, I don t really understand the purpose of interpolations for demonstrating the capabilities of generative models. What exactly do they demonstrate? "The magnitude of σ controls the how stochastic the forward process is. ", "...the stochasticity of the process", "...compared to its less stochastic counterparts" What does it mean for something to be more or less  stochastic  than something else? How can  stochasticity  be used as a quantitative measure? "Fenchel Inception Distance"  > "Frechet Inception Distance"#### CONCLUSION Overall, I think fundamental idea of improving the long sampling trajectories of DDPMs is certainly interesting.
Accept (Poster). rating score: 9. rating score: 7. rating score: 6. rating score: 5. <BRK>That being said, the improvements are quite stark and I don’t doubt the validity of the claims   I am merely suggesting that the authors should include this information for clarity. The authors derive the optional regularization strength in the one dimensional setting, to set ground for the proposed approach and its application in higher dimensional settings. There could also be a correspondence with Figure 1, if constraining the comparison to freq,rare ; noisy, clean input types. The paper is well structured, clearly written   and was a pleasure to read.<BRK>I very much enjoyed reading this paper. ** This paper is well organized and clearly written. The experiment section is also well executed, covering convincing synthetic and real world examples to demonstrate the effectiveness of this proposal, comprehensively compared to SOTA alternatives. One of the main novelty that I appreciate is the fact the author(s) have scaled the gradient penalty for model complexity to deep neural nets, which in the final solution is replaced by the Lipschitz estimate. Eqn (6) should perhaps be more explicit on the dependence for $f$.<BRK>This paper proposed an adaptive regularization method to handle heteroskedastic and imbalanced datasets, which are closer to real world large scale settings. The authors first theoretically study the optimal regularization strength on a one dimensional binary classification task. The experiments show great improvement. 1.Assuming the pre trained model is sufficiently accurate is not reasonable, especially in your complicated setting. 4.The imbalanced (long tail) experiments did not compare with current SOTAs, i.e., BBN (CVPR20).<BRK>The results in Table 3 are certainly impressive, however, additional results in the Appendix are less so. The whole section seems distracting from the main message of the paper. What is the definition of rare classes in the experiment in Table 1? it seems accuracy for the rare and noisy classes is calculated on all examples, including the 40% for which labels were exchanged.
Accept (Oral). rating score: 7. rating score: 7. rating score: 7. <BRK>This paper presents a simple and intuitive data augmentation method for few shot image classification. I have a few more requests/ablations that I am curious to see. This method makes an important assumption that the feature distribution is gaussian. The proposed method is novel and can inspire future augmentation based methods in few shot image classification.<BRK>What are the results without the self supervised loss? The paper proposes a distribution calibration algorithm that makes use of the meta train class distributions to calibrate the few shot class distributions. Is there a reason for that? Pros:1.This paper identifies and tries to tackle an important problem in few shot learning   estimation of the class distribution.<BRK>SummaryThe paper proposes a method to calibrate the underlying distribution of a few samples in the few shot classification scenario. Based on the observation that the mean and variance of the distribution with respect to each class are correlated to the semantic similarity of each class, base class distribution can be transferred to the novel class distribution. The empirical evidence is not sufficient to claim the approach applies to general network architectures and few shot learning approaches. Is the approach effective for more variety of backbone networks and losses?
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 7. <BRK>First of all, I believe the paper is looking into a very important question that attracts lots of attention recently. The set of techniques proposed in the work are also reasonable and practical, where the proposed progressive space pruning seems to work very well. As for the comparison with previous methods, since there are too many implementation details that can affect the fairness of the comparison (e.g.length of pretraining, batch size, teacher performances, etc), it s hard to judge the actual scale of the gain. Firstly, when the block wise search is used, it feels like the NAS BERT is trained in a way that is more similar to a variant of distillation that additionally utilizes intermediate hidden states. As this signal is not used in the standard BERT baseline, some improvement could actually come from this factor besides a better model (architecture+param). A better baseline could be a Transformer trained in a similar way. So, whether this is really a good way to obtain the desired task agnostic compressed models is still questionable.<BRK>This paper presents an effective NAS method for pre trained language models at the pre training stage, so the selected models can be applied to various downstream tasks with fine tuning. The main contribution of this work lies in the designed search space and the proposed three strategies (block wise search, progressive shrinking and performance approximation) for improving search efficiency and accuracy. Although the novelty of this work is quite limited, training a big supernet for BERT at the pre training stage is not trivial, which is useful for industry applications. However, I have several concerns:1)  In the Table 1 under the KD setting, “two stage distillation” is conducted on the selected models from supernet to further improve the performances, it would be interesting to add another two settings: a) only conducting the distillation at the pre training stage, b) continuing to pre training on large scale unlabeled data to finally obtain better task agnostic models. 2) The models are evaluated on the GLUE dataset, more experiments on challenging QA tasks should be added. 3) In the Table 1, the comparison to MobileBERT and TinyBERT should be added, and the FLOPs or the inference time on CPU/GPU can be provided.<BRK>Or they find out the best configuration matching the model size and latency requirements and then do the pre training again with those architectural choices. For lower model size ( < 20M ) it can be observed that the NAS BERT ends up choosing SepConv layers most of the times. How does the network perform if it is composed of all SepConv layers? In terms of original ideas, although the concepts of block wise architecture search, using SepConv layer for NLP tasks and using block wise knowledge distillation are not novel by themselves but this paper has efficiently made use of the available techniques (along with efficient engineering work like progressively reducing the search space) to develop a method that gives good performance on NLP tasks.<BRK>The paper (together with the appendix) is clearly presented, and the idea is new and interesting to me. It seems that the SpeConv operation is particularly effective when the model size is small. The search algorithm including the block wise training, progressive shrinking can remove less optimal structures quickly and significantly reduce the search space. The performance of NAS BERT models are generally better than those of the compressed BERT models with similar model size, although the comparisons may not be completely fair. The paper may not be easy to follow if the appendix is skipped, especially for the readers who are not familiar with NAS or related work. 2.The novelty of the paper is unclear to me. Please the author clarify the main novelties and technical contribution of this work, especially to the field of neural architecture search or more broadly, AutoML . Some of the well noted models such as MobileBERT and TinyBERT are not included in comparison. AdaBERT, which adopts NAS for each specific task, should also be included if possible. Again, since there are of many models with different size and latency, it may be better to have a plot for clear comparison.
Accept (Poster). rating score: 6. rating score: 6. rating score: 5. <BRK>Originality:The paper presents an interesting generative model termed generalized energy based model (GEBM), which is essentially an exponential tilting of a plain generator model P_G by an energy based model exp( E). Also, in the experiment section, the authors should reveal more detail on how the energy function (class) is chosen. The purpose of introducing such a model, as stated by the authors, is to make use of the fact that target distribution may concentrate in a low dimensional manifold in the target domain, which can be captured by a low dimensional generator model P_G. Cons:It would be better if there is some analytical comparison between GEBM and GAN.<BRK>This paper proposes a framework called GEBM that combine an implicit generator and an EBM to define a probabilistic model on low dimensional manifold. This paper mainly has two contributions: the generalized ebm framework and the KALE objective function. When the base distribution is an implicit model, GEBM is still a GAN like model: cannot estimate density. If the base distribution is an implicit model, which means that the support is only a subset of the data space. Some minor concerns about this paper are provided as follows:1. The experimental results are chaotic. Second, Figure 1 needs more explanation. Besides, in the experiments, the energy model is also an NVP. Further, how NVP is trained with CD?<BRK>Summary: In this work, a generalized energy based model (GEBM) is proposed. +ves:  1.This paper has proposed a framework so that the energy function can be used to refine the probability mass on the learned base distribution. The framework is trained by alternating between learning the energy and the base. Empirically, the framework outperforms GAN with the same complexity. There is a lot of detail on this derivation. In this work, a new framework is proposed for training. This seems easier. Could you explain the more possible benefits of your method? MCMC is also used for generations.
Reject. rating score: 3. rating score: 5. rating score: 5. rating score: 5. <BRK>I wonder how feasible the upper bound on $\alpha$ is to guarantee convergence. In fact, GTD methods canonically use IS for their off policy variants. I will summarize below. In many cases, the proposed algorithm does not clearly outperform baselines. This is a strange choice that is not used in either the empirical or analytical section as far as I can tell. * The primary motivation of the paper was around off policy learning, yet only one of the tested domains was off policy (Baird s Counterexample star MDP). There a few other issues with the proof that concern me. There are a few other papers in the literature that do this and derive the corresponding GTD2/TDC algorithms.<BRK>### ReviewI like the simplicity of the proposed method, and its intuitive interpretation as a value based trust region. This sort of ties into what was described in 2), where what was presented seems to fall a bit short, and how the space could have showcased a bit more. 4) While the paper s focus was on the case of linear function approximation, can the authors comment on how readily the approach can be extended to the non linear case? GTD methods have not seen as much adoption as their approximate dynamic programming counterparts when combining TD methods with non linear function approximation, that it can raise questions as to how the methods scale to more complicated settings. I think 1) is a rather significant issue that needs to be addressed, and I m willing to raise my score if that, and my other concerns, can be sufficiently addressed.<BRK>This paper proposes a variant of the GTD2 algorithm by adding an additional regularization term to the objective function, and the new algorithm is named as Gradient DD (GDD). The idea of extra regularization on the distance between two value functions sounds reasonable to me since it resembles the constraint in trust region optimization for policy gradient methods. It would help the readers to understand the improved convergence if the authors could complete the analysis and show the convergence rate. It seems that the proposed algorithm may not be as robust as the conventional TD algorithm? After reading them and the discussion with other reviewers, I still think the current contribution of this paper is marginal and I keep my score as 5.<BRK>This seems like a serious issue to me; the experiments may need to be re run with different parameter settings that better match the claims the paper is making about learning speed. Consider motivating Gradient DD more along the lines of TRPO, REPS, and other algorithms that penalize large changes to the weights being learned instead of motivating it as accelerating GTD2. This regularized objective function is used to derive a GTD2 like algorithm where updates to the value function weights are penalized. If not, it would provide more support for the proposed method. (2018).Online off policy prediction. The analysis seems like it was based on an existing analysis, but nothing is cited.
Reject. rating score: 4. rating score: 4. rating score: 7. rating score: 9. <BRK>Response to authors:The authors have largely responded well to my original concerns. However, after reading through the discussions with other reviewers, I agree with reviewer 2 that more work is required to make this publishable. In particular, this should include comparisons to the other methods suggested and justification of the use of the UMAP loss function. ProsThe parametric version presented here appears to work well in the experiments given. The incorporation of the UMAP loss directly in a neural network as a regularization is also interesting. UMAP tends to inherit some of the weaknesses of t SNE as it tends to overemphasize local structure at the expense of global structure. If so, that should be mentioned as a potential drawback. While this allows for more results, I think it would be a better paper if some of the figures were included in the main paper and some results were moved to the appendix.<BRK>**Update following discussion:**Following the revision by the authors and the discussion with them, I am updating my score from 3 (Clear rejection) to 4 (OK, but not good enough   rejection). However, I still maintain the paper is not ready for publication in its current form. Moreover, there is significant related work that is either ignored by the authors, or just mentioned in passing in the appendix without providing proper discussion and comparison with the proposed method. Therefore, an important question has to be asked here for whether the UMAP loss is indeed a good choice for a loss term to impose on networks, for example, to enable visualization or improve various tasks. A discussion about the difference between these two approaches should be added to the main paper here, and it seems some comparison between them should also be presented to establish the advantages of the proposed approach here. Now, beyond the described lack of relevant comparisons for autoencoding and semi supervised classification, even simply as a parametric implementation of UMAP (which would be a rather narrow scope, which is not very enticing as a motivation on its own),  I am not sure this work is sufficient to establish the presented approach. As such, even if one insists on only comparing to UMAP based methods, there are multiple OOS methods that can be used, such as Nystrom, geometric harmonics, etc. However, as it currently stands, I find it is not mature enough for publication and would need nonnegligible amount of work to properly position the contribution provided by this work compared to previous and related ones. These include not only methodological illustrations, but also all results establishing the method. If, on the other hand, we include the result figures as integral parts of the main paper (as they should be), then it clearly has significantly more than eight pages.<BRK>The authors propose a parametric version of UMAP by replacing sampling embeddings in the optimization of UMAP with directly learning weights of a neural network. The paper is very well and clearly written, but I have several significant concerns:1. Replacing embeddings with neural networks learning seems to be quite basic and straightforward. The simplicity of methodology could be neglected, if the authors demonstrated significant improvement in their experiments, especially on downstream tasks. 2.A large part of the experiments is devoted to the comparison with tSNE, however it is not very clear why there is a lack of comparison with other parametric methods, such as Topological Autoencoders. 3.The performance of parametric UMAP achieves similar results to non parametric UMAP, which is certainly nice, but also quite expected. However, experiments on this part are not convincing at all (especially on CIFAR10 dataset). Would be interesting to see the performance on some other datasets. Also, it would be very interesting to see confidence intervals for Figures 15, 16, 18. 4.In terms of speed I also don t see an improvement compared to non parametric UMAP (TF). I see clear improvement compared to UMAP learn version, but this as far as I understood due to a different implementation of the original UMAP and not in particular novelty of this paper.<BRK>In the manuscript, the authors introduce a parametric version of UMAP, replacing the original embedding optimization step with a deep learning solution detecting a parametric relationship between data and embedding. The novel approach compares favourably with the standard algorithm and, as a major contribution, defines a loss function that can be employed for other important applications such as constraining the latent distribution of autoencoders, and improving classifier accuracy for semi supervised learning. The paper is well written, complete and thoroughly detailed, both in the theoretical and the experimental section. The introduced material represents a significant advancement in the field, becoming a valuable resource for researchers in several areas. Fig.3 in the Appendix is extremely useful to graphically explain the algorithm to a broader audience   I understand the page length limit, but I would strongly recommend to fit it in the main text. I would also suggest to include (maybe in the Appendix) a kind of “how to” fully worked example to help researchers in optimising the use of novel algorithm in a data exploration pipeline  I would point out (within the limitation of the anonimity requirement) the availability of the code for the algorithm
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 7. <BRK>Coordination is a problem in MARL. More motivation is needed for the approach. The details of GameDistill are unclear and it also isn t clear how general it is. The paper should make it clear how general the method is. The experiments show the method outperforms an independent learning and LOLA, but more extensive comparisons are needed. As such, methods that promote cooperation should also be discussed and compared to.<BRK>The GameDistill algorithm is an interesting way to reduce general environments to matrix games, but seems highly instrumented for the current environments, and unlikely to work in more general environments (discussion below). Nevertheless, it is hard for me to recommend acceptance, given the number of unseen changes that still need to be made to the paper. It is not immediately clear to me that there is a simple objective function for which (7) is the gradient.<BRK>This paper focuses on the problem of multi agent cooperation in social dilemmas, in which mutual defection is individually rational but collectively suboptimal. Hence it is not correct to say that "SQLoss encourages an agent to stick to the action taken previously". However, precisely because the method in this work seems to be so effective, there is a glaring need for improvement in many areas, and there are significant open questions for further research. The introduction and motivation for SQLoss needs improvement.<BRK>#########################################################################Other questions/suggestions:It is already quite interesting enough to only consider two agents scenario, but it seems to me that the method can be potentially extended to multi agent (n>2) cases. The authors further proposed a method to generalize this loss to non matrix games and showed its effectiveness. I’m not sure what are the major differences between the three matrix games tested in the paper.
Reject. rating score: 4. rating score: 6. rating score: 7. rating score: 8. <BRK>I found it difficult to evaluate this paper because the paper does not say which continual learning problem it is solving in the supervised learning case. Without knowing that, it is hard to make an assessment because the two problems are usually solved in very different ways and their evaluation protocols are different too. I tried to guess but get confused. How do you do (i)? So, I am guessing that you are doing task incremental learning. If you are doing class incremental learning, more recent baselines should be compared. The baselines used in your experiments are old. ICLR, 2020In the experiment, varying the number of tasks for each dataset is also desired to show the generality of the proposed approach.<BRK>Summary:  The paper proposes a continual learning framework based on Bayesian non parametric approach. The variational inference is done with Bayes by backprop on a common ELBO setup. The experiments show less diminishing accuracy on the increment of tasks on five datasets for the discriminative problem, and for generation the methods learn one digit or character at a time on MNIST and notMNIST datasets. The paper is easy to follow. Cons:   The structure of the network is rather shallow. The authors also mentioned the challenge in their conclusion. Minor comments:  Typo P6 par2 "tries to adapts"  Typo P7 par 3 "comapred"Overall rating:The idea of applying Bayesian non parametric for continuous learning is interesting, and the authors show a simple implementation on simple datasets. The evaluated tasks are extremely related and in a general continuous learning setup this method may not work. Due to all these issues I rate the paper as a 5.<BRK>The authors present a new structure learning approach to Continual Learning, by modelling each hidden layer using a nonparametric Bayesian prior, a technique inspired by recent work on learning sparse NNs. This makes for an overall very convincing submission, reflected by my minor criticism. Both discriminative and generative modelling are naturally supported in the same framework. Analysis covers most of the interesting questions that can naturally be asked about this method. I was pleased to see experiments in an application other than supervised image classification (here unsupervised learning). A simple heuristic for dynamic expansion is introduced, a worthwhile direction for future research. Cons (in no particular order):  This method requires storage of a binary matrix for task and each layer in the network. However, the authors show that space complexity grows logarithmically with the number of tasks, which is likely to make this an acceptable trade off. Author feedback:  IMHO the main contribution of this work is its ideas on structure learning in the context of Continual Learning. This is however not reflected in the title. The paper would benefit from clearly stating why structure learning for CL is a worthwhile direction to pursue. Well written papers clearly state why the proposed direction is a worthwhile method of investigation.<BRK>This work proposes an online variational Bayesian (VB) approach to continual learning. The prior over neural network functions is both over the neural network structure and parameter values, where the structure is modelled by an Indian Buffet process (IBP) and the weights are drawn from a Gaussian. The approach is similar to VCL in that it uses online VB for learning, however, the prior and approximate posterior is more general in that it also considers the neural network structure as a random variable (prior and posterior). Theory:The approach is theoretically sound and well motivated; the paper is presented well and easy to follow. It could be possible to extend this work to similar adaptation mechanisms, although for multi task learning such adaptation/forgetting may not be desirable. I would appreciate a few comments on this and I think it should also be discussed shortly in the paper. Experimental evaluation:The experimental section considers scenarios that are very common in the CL literature. I find the results on classification from the latent space of the unsupervised learning approach especially convincing and interesting (Table 1). What exactly is the difference if the IPB is put on the activations rather than weights? What are pros and cons? I am aware that there are additional experiments in the supplementary material comparing to Kessler. Do you think this is because the performance is already high?
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. <BRK>Theoretical and empirical study on how to combine two approaches using auxiliary information for out of distribution samples**Quality:**_Pros._ The authors propose the method to combine two representative methods (Aux Inputs and Aux Outputs) to exploit auxiliary information which is usually available in real world scenarios. Beginning with theoretical analysis on Aux Inputs and Aux Outputs models, they show that the proposed method, In N Out is effective in minimizing the risk under a distributional change in a linear regression setting. In particular, the in domain risk only depends on the dimension but not on the conditioning of the data."<BRK>This paper investigates how to use auxiliary information to improve classification performance when few labeled examples are available. The authors present three intuitively plausible baselines/ablations, two of which use auxiliary information, and explain the benefits and downsides of each. For instance, regarding the aux inputs baseline: "the relationship between the aux inputs and the target can shift significantly OOD, worsening the OOD error". These claims are later supported with theory using linear models. The only difference is that they fine tune on pseudo labeled in distribution examples in their method.<BRK>This paper introduces a new method for leveraging auxiliary information and unlabelled data to improve out of distribution model performance. Theoretically, in a linear model with latent variables, they demonstrate using auxiliary data as inputs helps in distribution test error, but can hurt out of distribution error, while using auxiliary data to pretrain a "good" representation always improve out of distribution error. I m not intimately familiar with all of the papers in this area, but I think this emphasis (as opposed to transfer learning) is new. This may be of interest more broadly. The paper reports experimental numbers on two real remote sensing datasets rather than solely evaluating on synthetic data.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 5. <BRK>Unfortunately, I am not able to judge the proposed idea (which might be interesting) and provide a reasonable review, because I could not understand it after trying to read the paper couple of times. In general, I find the clarity and coherence of the paper lacking. In my opinion, the paper should be improved significantly along these lines, such that to be accessible from the reader. There is too much compact information, but with low clarity. In general, I could not understand the main story of the paper. Many terms are used without a definition e.g.what is the definition of isometric embedding? what is the definition of rate distortion? In general, the mathematical notation that is used is a bit unclear. I think is better to avoid this tactic and directly show the result in place. How the VAE model is perceived? In my opinion the Figure 1 is hard to understand. There is a lot of context and connections to related works, which the authors presume is already known from the reader. However, I think that this is not the case for a general reader. ​Overall: Unfortunately, I am not able to provide a constructive feedback. The main idea of the paper might be interesting, but the writing style does not help. After rebuttal  I appreciate the fact that the authors update their manuscript, taking into account some of our comments (+1 for this). However, I still feel that the content is quite hard to follow.<BRK>The paper builds on a branch of recent works that consider and analyse Variational autoencoders (VAE) from the view point of data compression. This started from Alemi et al.2018, where the authors consider and analyse the mutual information between data and latent codes and culminates in Kato et al.2020, where the authors consider models in which both the encoder and the decoder are assumed as deterministic (isometric) mappings. The submitted paper aims at reconciling this type of models with standard VAEs by claiming to prove that VAEs can be obtained from the former by an non linear component wise scaling of the latent space. Unfortunately, the paper is not well written. Moreover the authors do not explain precisely enough the concepts used in the main building blocks of their proof and their final composition. Altogether, this makes it impossible (at least for me) to follow the steps of the construction and the related proof. In my opinion, the paper in its present state is not publishable at ICLR because it requires a major revision. I have tried to read the revised version submitted by the authors. Unfortunately, it is still very hard for me to fully grasp the proposed concept and to follow the derivation steps and the proofs.<BRK>My main negative impression of this paper is that it is hard to read. The paper is written in a way which is hard to understand. It is information dense, with a lack of intuitive explanation for many of the statements. For example, the introduction of the rate distortion theory is not motivated at all. This parallel allows one to use knowledge from the compression literature to aid our understanding of these autoencoder like models. However, this is not mentioned anywhere in the text, which makes it difficult for the reader to follow the narrative of the paper. I will say that I am not very familiar with the literature on isometric embeddings, which has made it harder for me to understand. At a high level, this is very similar to the contribution of this paper. There are certainly differences, primarily in the theoretical analysis given to the VAE here, and the focus on examining the contributions of each latent dimension through the variances, in a PCA like fashion. If the differences between this work and RaDOGAGA were made, and the paper itself was made more clear then I would be willing to upgrade my score.<BRK>Based mainly on the rate distortion theory, the authors propose to interpret VAE as a non linearly scaled isometric embedding. However, I find it challenging to fully understand the main contributions of the paper, even though I use VAE a lot. The current manuscript might not be self contained. It relies heavily on the prior work RaDOGAGA (Kato et al., 2020), with which I am not familiar. I would suggest the authors elaborate on that. So, is it possible that the proposed interpretation/discussions also generalize to other VAE variants (like one exploiting a flow on top of the variational arm)? Discussions are necessary. How to address related concerns in practice?
Reject. rating score: 4. rating score: 6. rating score: 7. rating score: 7. <BRK>The key idea of the Selfish RNN training algorithm is a non uniform redistribution across cell weights for better regularization. The authors mentioned previous sparse training techniques mainly focus on Multilayer Perceptron Networks (MLPs) and Convolutional Neural Networks (CNNs) rather than RNNs. Overall, this paper is carefully written and provides some interesting empirical results. The authors mentioned that they picked Adam optimizer for SET, DSR, SNFS, and RigL. For example, the way of learning rate decay has a big impact on the performance of training Penn TreeBank dataset. For future work, an analysis of computation (flops) to communication (memory access frequency) ratio seems to be necessary.<BRK>In this paper, the authors propose an approach to train sparse recurrent models, and a sparse variant of the NT ASGD. The proposed method mixes some interesting novel methodologies and achieves interesting empirical results on Penn Treebank and WikiText 2 language modeling tasks. In general, the paper is well written and interesting, but in section 3 many explanations about the rationale behind some architectural choices of the selfish RNN methodology are only partially explained, and sometimes they are just related to empirical results (e.g.in the cell weight redistribution). Reading the results reported in table 5 of the appendix, I found it interesting that the performance of the DSR improves significantly by using SNT ASGD instead of Adam  (it outperforms the Selfish RNN).<BRK>Strong points:  very rigorous experimental setup and analysis  Solid evidence for many new insights into some sparse training phenomena. The fixed FLOPS only seems to be a by product of the algorithm and particular network structure but not necessarily an algorithmic contribution. Recommendation (short):This is a very solid paper with exemplary experimentation and analysis. It provides many unique insights that are very valuable for anyone who wants to work in the field of sparse training. Recommendation (long):I think this paper is one of these papers, which is a very solid all around. Findings of different initialization schemes and performance of other sparse training methods are precious and make the overall literature on sparse training robust. I can see that this paper may seem a bit boring and less impactful to some reviewers, but good science like this is not about being exciting but about providing rigorous results for a small problem. The cell weight redistribution algorithm description is unclear.<BRK>The paper claims that the previous sparse training methods mainly focus on MLP and CNN, and fail to perform very well in RNNs. Hence, the authors proposed an approach to train sparse RNNs with a fixed FLOPs budget. Moreover, a variant of the averaged stochastic gradient optimizer (SNT ASGD) is developed for the training of sparse RNN to account for the effect of weight masks during training. Strengths:By adding some refinements and tweaks to the existing techniques (masking for sparse training and adapting the NT ASGD), the authors were able to achieve good performance to train sparse RNNs. Weaknesses and questions:Compared to the existing methods, the technical novelty of the paper is minor. It can be seen as some tweaks and improvements to the existing ones (although I admit that those changes are essential for the method to work for RNN.). In the paper, it is stated that "magnitude weight removal" is applied to non RNN layers.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>The new analysis improved the existing bounds of FedAvg. Besides, the analysis is also extended to the non stationary network, where the number of workers participating in the optimization may vary. As FedAvg is the most important optimization algorithm in federated learning, I think such theoretical progress is good. I suggest the authors demonstrate this fact in the paper, e.g., by adding a column concerning communication complexity in Table 1 and appending some discussion about communication cost. One drawback of the theoretical results is that the model for the non stationary network is too strong. One typo: I guess $p_i \frac{1}{m}$ should be $p_i \frac{n}{m}$ in the last line of page 5.<BRK>Added after reading author response: I believe authors have addressed the issues I raised about clarity in certain parts of the paper in their response sufficiently. I encourage authors to investigate $m$ s effects on the convergence rate more to see whether there is a structural limitation in federated learning settings, perhaps better left for future work.<BRK>I m glad to see that some of my concerns have been addressed so I decide to change my score. Like the other reviers  comments, I think some part of the paper like notations should be with clear explanation. The analysis is featured at client sampling and non iid distributions among clients, which are two well known challenges in federated learning. First, the metric for convergence rate is slightly less common in the methods shown in Table 1. I would like to see if there is a way of showing the linear speedup in the same fashion.
Accept (Poster). rating score: 8. rating score: 8. rating score: 6. rating score: 3. <BRK>The main contributions of the paper are the following ones (informal): 1. The authors also emphasize the difference between their results and the previous results. The memory growth rate is polynomial with respect to approximation error. The role of input data is significantly ignored in the given analysis.<BRK>The paper provides a theoretical examination of the challenge of fitting recurrent neural networks (RNNs) to fit processes with long memory (or long range dependence). Sufficient details for reproducing the experiments are provided. The analysis presented here is rigorous and comprehensive, and while the discussion is limited to linear RNNs, it provides a good starting point for further studies regarding long range dependence with RNNs. 3.I was disappointed with the “informal” presentation here. However, the general argument appears sound. But, even in its current form, I recommend this paper for acceptance to ICLR. "On matrix exponential distributions."<BRK>This paper studies approximation and optimization of linear RNNs for learning linear functions, from the perspective of the memory properties of the temporal sequence. It shows that linear functionals can be approximated by a linear RNN, with the rate of approximation depending on the long term memory of the process. The problem being studied in the paper is interesting and well motivated. The underlying phenomenon are simple and elegant, and I think they can be explained effectively to the ML community.<BRK>This paper reports a mathematical study of approximation properties of linear RNNs. The first part  reports a universal approximation theorem, and presents an analysis of how efficient the approximation is. the fact that a diverging number of exponentials are needed to approximate a power law function is also well known; from that perspective the "curse of memory" is not very surprising.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. <BRK>This may range from pure sampling, top k, top p to temperature modulated sampling. While filling the theoretical gaps, this work proposes an adaptive top k decoding mechanism   Mirostat. This is based on the understanding that cross entropy is a useful measure of the quality of the generated text. Theoretical analysis of the previously proposed sampling schemes in terms of surprise, cross entropy, perplexity. 3.Clarity of the paper. 3.Human evaluation missing. Overall:a) The theoretical analysis helps in understanding why top k and top p perform the way they do. Lowest perplexity does not always mean good quality generation.<BRK>The paper proposes a feedback based adaptive top k decoding algorithm, named mirostat, to solve the repetition and incoherence problems in text generation. Experimental results demonstrate the effectiveness of the proposed algorithm. The proposed method aims to address this issue by controlling the perplexity of the generated text, which is novel and interesting. According to the examples shown in the experimental results, the algorithm can improve the quality of the generated text. 3)	The experiments are clearly organized. 3)	The proposed decoding algorithm is only experimentally verified on the GPT 2 model and has not verified its universality under other generative models or other generative tasks.<BRK>In the context of neural text generation, the authors study how perplexity varies with top $k$ and top $p$ sampling and propose a sampling algorithm that uses Zipf s law to dynamically adjust $k$ in order to control per sequence perplexity. (edit: see discussion below, I have adjusted the score to above the acceptance threshold)#### Pros  The theoretical analysis of cross entropy growth with top $k$ versus top $p$ was interesting (e.g.summarized in Figure 1). Nice empirical demonstration of repetition correlating with log probability. **Human evaluation. *Surprise rate* is not used again in the text. However it requires setting a target value ($\tau$). Could the authors comment on this issue? The underlying assumption of the method is that it is a good idea to have a fixed perplexity for all continuations.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 6. <BRK>## SummaryThe paper studies the problem of high probability mean estimation for heavy tailed distributions, i.e., constructing a high probability confidence intervals for the mean, when the underlying distribution has only finite low degree moments. Perhaps authors can focus more on the experiments in a resubmission. I liked the experiments in the paper but they are currently not extensive for an acceptance on its own. + The paper derives theoretical bounds for their mean estimation algorithm, and then applies it to study generalized models via gradient aggregation. This is a standard procedure in this literature. + This difference is considerable, especially, in high dimensions, which is the focus of this work. A spectral algorithm for robust regression with subgaussian rates. The current theoretical results (Theorem 1 and Theorem 2) have significantly large sample complexity, and do not explain the experimental results of the paper ($n   O(p)$ and $ r(\Sigma)   p$ in experiments).<BRK>I thank the authors for their detailed reviews. I have updated my score The submission presents a robust estimator for the mean of heavy tailed distributions for application to neural network training. Understanding and dealing with the distribution of the noise in stochastic optimization for machine learning is relevant to both practical and theoretical aspects of machine learning and relevant to the ICLR community. The main weakness of the submission is a lack of clarity in the contributions and presentation. Key statements are vague, basic notation is not defined and the writing and supporting figures would benefit from an additional pass. More details below. But I am open to increase my score depending on the results of the discussion period if the issues below are addressed. **Minor comments:*** The figures need additional work as they are currently unreadable when printed.<BRK>In summary, this paper proposes a novel estimator for the mean estimation of heavy tailed distribution. In my view, the authors *must* report the running time of their algorithm and others to make a fair comparison. It would also be helpful to clearly state that $\theta \in R^p$ at an early stage. In the line just above Algorithm 1, it is claimed that other methods are impractical because "they have several hyperparameters". This is not convincing.<BRK>The author(s) propose a computationally efficient mean estimator for generative distribution that are "heavy tailed" in nature. The phenomenon of heavy tailed distributions for gradients in the training stage of generative models are common in nature and the proposed method aims to alleviate this problem by constructing a robust gradient estimator in such situation. The topic is interesting and the proposed methodology is novel.
Reject. rating score: 3. rating score: 3. rating score: 3. rating score: 5. <BRK>This paper presents a GCN based solution for multi step spatio temporal data forecasting. The proposed spatio temporal joint graph convolution is very similar to the concept of 3D GCN [1]. 7) Many typos and grammatical errors.<BRK>This paper proposes a spatial temporal graph neural network, which is designed to adaptively capture the complex spatial temporal dependency. Further, the authors design a spatial temporal attention module, which aims to capture multi scale correlations. But the description of the method is unclear and the writing of this paper still needs improvements.<BRK>[3] Geng, Xu, et al."Spatiotemporal multi graph convolution network for ride hailing demand forecasting." The model uses a convolutional block to model the spatial temporal correlations and an inception attention based module to capture the graph heterogeneity. Proceedings of the AAAI Conference on Artificial Intelligence. Especially, the multi scale motivations in [3] [4] are almost the same in the paper.<BRK>This paper investigates the important problem of spatial temporal forecasting, and proposes a multi scale spatial temporal joint graph convolution that jointly model the heterogeneous spatial temporal correlations.
Reject. rating score: 2. rating score: 4. rating score: 5. rating score: 6. <BRK>D that to cover whole images, they are split into X by X patches and then aggregated into four masks. Literature is poorly conducted with many relevant recent papers missing (furthermore, the literature comes late in the paper). Experiments need to be significantly extended. First, authors merely include two methods in their evaluation, while there exist more than those used in the comparisons. Second, authors report results in terms of AUC, while I strongly suggest that they use the accuracy for individual classes, and the AUC as average of the classes. The reason for this is to better compare to related work (See for example Table 6 in [4]).<BRK>The iterative masked approach does provide a principled way to increase the signal to noise ratio in the reconstructed images. The authors reproduced the numbers but they appear to be much lower than those published. Only fig 4 shows the progression of the AUC during iterations but no discussion are provided. Assuming the complexity is higher than other SOT methods due to the iterative aspect, and given that the performance is similar to SOT (see above point), this approach now looks a lot less convincing. However, I think the paper is not ready for publication.<BRK>This paper presents an impainting based method for anomaly localization on images. In the training time, a conditional GAN based generative modeling approach is adopted. The idea is very intuitive and experiments demonstrate improved performance (especially on textures) over two recent baselines methods. Strong points:   This paper improves the test time in unsupervised anomaly detection with an iteration scheme. Another novelty is in the training time, the proposed approach using a conditional and GAN based approach. Weak points:    The experiment evaluation is not quite convincing. Some of these numbers look worse than what is reported by Bergmann et al., 2018 and Dehaene et al., 2020. The qualitative comparison with Dehaene et al.2020 is missing in Figure 3. Figure 4 clearly shows the AUC improves with iterations. Would it be better than other iterative schemes such as iterative projection with the same trained model?<BRK>The majority of the obtained resultsare better than the state of the art has been reported in this paper. The most important works on the topics are not cited. The proposed method is not comprehensively compared with the other solution. Strengths:This paper is well written so that the structure is easy to follow.
Reject. rating score: 4. rating score: 5. rating score: 5. <BRK>The paper proposes LISR, a method for learning symbolic intrinsic reward functions from interaction using using symbolic regression via "symbolic trees". Testing on multiple environments with wildly different tasks, control systems, and agent settings is excellent, and shows that the method is potentially applicable to a wide variety of settings. algorithm. This raises multiple questions, such as:  a. d. Why was all of this not achievable simply through backpropagation? My guess is that much of these details are contained in the CERL paper, however considering that the proposed pipeline largely seems to resemble it, it would probably be best to provide an overview of it. It would be good to add details about how the 2 agent setting was constructed (e.g.is there a centralised policy? Is the reward function decomposed? ), to understand exactly how LISR is operating in this setting. 3.Considering the significant amount of moving parts in the method, experimental evaluation section could use more ablations of the system and some more analysis. This is particularly important for the narrative of the paper, since the method aims to improve interpretability of reward functions in RL agents for real life tasks.<BRK>Description:This paper proposes a mechanism to learn reward functions that is based on a symbolic reward generator (in the form of a tree of pre defined simple math operators). The generated reward is used to optimize a policy, and an evolutionary approach is used over a set of these policies to select the final “champion” policy. Good use of images to illustrate the method. Though the authors do not make a very convincing case for when this is absolutely necessary. The reality is that for most real world tasks in RL (and I have worked on many!), the reward is specified by a human, and thus is interpretable from the start. I can imagine some cases in imitation learning or inverse RL, where one has expert trajectories (e.g.driving a car in dense traffic) where it might be useful to infer an interpretable reward, but this is not the setting here. The paper does not describe the formal setting. I am not sure I understand exactly what each of the alternative methods considered in the empirical results are doing. Which case would this be?<BRK>This paper presents a method to learn symbolic regression (tree) to make analysis of reward function more interpretable and tractable. Authors conduct the training of policy in the shared replay buffer for off policy RL. Strong points  An interpretable way of policy learning using symbolic reward functions  New interpretable reward as shown in Figure 9. Weak points  Symbolic reward functions are not clearly written (or not self contained)  It is hard to find algorithmic novelty in the paper  Empirical evaluations are not extensive. It would be better to include examples of new discovery of interpretable reward functionThe ideas in the presented paper looks reasonable. However, the symbolic reward function and the procedure to learn such symbolic representation is not clearly written. Also, it is hard to see that the proposed algorithm present new qualitative (interpretable) discovery of reward function.
Reject. rating score: 5. rating score: 6. rating score: 6. <BRK>1.One of the major problems I consider in this work is how to make sure the positive samples follow a unique concept. The authors mention that the actual concept space is unbounded and uncertain. These models are also missing. See the following references for examples.<BRK>I think the toy example provided in the author s response is certainly helpful, and should improve the paper in this area somewhat. It is not yet clear to me that the analysis has yielded any particularly groundbreaking insights, though I would love for the authors to correct me on that. It s not clear to me what the  benefit of the audio modality is.<BRK>Why I am not convinced this is a good (useful?) UPDATE after reading author response:  The authors  example answered my question about the nature of the "uncertainty" in their setting. In terms of writing: the intro/abstract are very clearly written and are easy to follow.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 7. <BRK>Based on the framework proposed by Pang et al.(2020), this paper unifies adversarial examples and Trojan backdoors into a synergistic attack. The inference results are dominated by the Trojan trigger and the adversarial perturbations. Incorporated with adversarial perturbations, the desired results of the adversary could be multiple classes. However, this paper involves a too strong assumption: the model parameter needs to be modified by the adversary. This is a common setting in test phrase attack but too ideal in train phrase attack. So bypassing existing defenses is not surprised.<BRK>This paper presents a new attack against neural networks that combine Adversarial inputs and trojans. Strengths    The attack presented by the authors beat most existing defensesWeaknesses   The contribution seems very incremental given that there exist a substantial number of works in both adversarial inputs and trojaning. The core training algorithm (Alg.1) also seems very straightforward. The threat model seems to be somewhat unrealistic. It is not clear to me what is achieved by putting these two different types of attacker models together that cannot be achieved by a regular trojan attack with larger triggers.<BRK>Summary: This paper proposes a new type of attack: AdvTrojan. This new attack is activated only when the test examples contain two things: backdoor trigger pattern and adversarial perturbation. This is more like to intentionally leave a loophole in the model. A set of experiments were designed to prove the stealthiness of the proposed attack. 2.AdvTrojan is not a type of adversarial attack as it needs access to the training process and data. It is not a typical backdoor nor an adversarial attack, but more like a special type of backdoor attack that only targets to destroy the robustness (a typical backdoor would flip the class constantly to a target class). But the main purpose is to destroy (or fake) the adversarial robustness. The motivation of this paper is poorly presented. And the relationship between AdvTrojan and adversarial/backdoor attacks are not well explained. This can be improved by adding a comparison table. 3.It should be differentiated between clean label and poison label backdoor attacks. The threat model should be clearly defined somewhere. 3.Fig.2 is a bit confusing. Are the two patterns also part of the training? 4.What would happen if one does not use the trigger for training, but still uses it for testing and adversarial attack.<BRK>This paper presents a very strong combined attack method, where infectedtraining examples are crafted such the the trojan backdoor becomes verydifficult to detect. The paper focuses on the mechanisms of cleverly disguising the Trojan trainingdata, and does an excellent evaluation. The attack is particularly importantin some online training scenarios, where one might wish to use non trustedtraining data. Readbility, and organization between main paper and appendix material was good. Appendix A addresses this for 2 cases that *fail* to defend, but a question Ifound important was what steps would make the attack harder? A case (2) make the adversarial component more difficult if somelayers have parameters unknown, perhaps on a secure compute platform? p.7: Anomaly Index was actually *not* defined in appendix F, but in Wang et al.This is a well presented paper, with extensive experimental investigationand should be published. I was the only reviewer who happened to imagine their threat scenario had some importance.
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>In particular, it considers language models which compute a probability distribution over the next word in a text, given the previous context. **Strengths**  The paper is generally quite clearly written and the claims are well validated. The analysis which builds on these definitions/models/assumptions provides meaningful theoretical insight into why language model pre training may be so beneficial for downstream training. Also, can you add a column where a dense linear model over p_f(s) is used? * The paper doesn’t explain why learning a linear model directly on the context embeddings f(s) performs better than using the contextual mean embeddings. * There are some points in the paper that could be made clearer. * I think there should be more discussion about the implications of Proposition 2.2. Overall, I really enjoyed reading this paper, and found it to be quite insightful. As a result, I recommend acceptance for this paper.<BRK>**Summary**This work relates a pre training performance with a downstream performance for tasks that _can_ be reformulated as next word prediction tasks. The paper is well written: it gives an appropriate context, presents the main theoretical results, and verifies _some_ of the claims experimentally. I don t see why your theory does not generalize to a _masked_ language modeling (MLM). Please clarify/fix this. What is the "margin of task $\mathcal{T}$" mentioned on p.5? **Limitations**  The authors admit that their work is limited to a particular type of downstream tasks.<BRK>1.It seems to me that the central question lacks strong practical motivation. As you have argued, many classification tasks can be framed as predicting the next token (perhaps in the presence of a prompt). In particular, some proposed concepts such as the refined transferability coefficient, conditional mean features, substitutability matrix might be useful for future studies. 2.The article is precise, well written and cautious in its tone. The accompanying experiments are informative and supportive of the main theoretical claims. I enjoy the overall journey the authors presented and would love to see more well reasoned articles like this in ICLR. 1.The presentation can be improved by allocating more space to ideas in the Extensions section. Perhaps boldface for when it is viewed as a vector. 1.I think a moderate revision reducing some (parallel) elaboration on “unconstrained language model” should provide the space needed for Extensions (and other novel ideas). One trick I found useful is to follow the notational convention of a textbook or a classic paper.<BRK>This seems to be a novel finding and may help inspire future work in this important direction. This paper provides a mathematical framework to understand this question. One novel finding of this paper is that the distribution of the next word, conditional on the context, can provide a strong discriminative signal for the downstream task. It is unclear to me how the authors are going to justify this "assumption".<BRK>Summary: This paper presents an explanation of why pretraining on language modeling (LM) helps performance on downstream text classification tasks. Overall, this work contributes an interesting framework for analysis. However, I have one large conceptual concern about the framework. Though, to the authors’ credit, they do have to do additional work to extend an LM that is eps optimal in next word cross entropy (i.e.on average) to optimality on the specific task formulation. This strategy introduces the potential for spurious correlations: The heuristics might be strongly correlated with the task in general, but might be off due to other factors like sarcasm. I am not sure what it means for a task to “lie in the row span of word embeddings”.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 6. <BRK>Take aways from the paper are intuitively reasonable. THe points covered in related   work are valid and relevant### Cons/Questions1. I disagree that Lyapunov analysis is always impossible. Impossible  > implies there is no such function. The paper in general would benefit from a language review. This does not   factor in the review decision, however, authors may prefer to ease readers   burden by getting the work reviewed for language alone.<BRK>Your technique seems like an overkill for the cartpole. It would be important to see where your results stand with respect to these related works. What are the limit of these approaches for learning and verification when the system is stochastic and how does your approach overcome them if it does? You have that in the later definition for your special case but this is also true in the general case. I think it would then make sense to try your approach also on such a system and not just a cartpole.<BRK>While the paper contains one experiment, I think it is not enough to fully support the overall theoretical results. This seems to suggest that "proper" policy gradient methods can lead to stability. High probability bound is derived in terms of the number of trajectories and the length of them. Post rebuttal:I thank the authors for addressing my comments and updating the manuscript.<BRK>Some numerical experiments are also presented comparing the results to a recent off policy RL method. **Cons:**  Although, the general objective of the paper (namely, to have finite sample stochastic stability guarantees in RL) is nice and should be further studied, the paper in its current form seems premature. The paper should present some specific examples to show how should the conditions be checked in practice. The authors mention that an optimal controller might not be stable. Nevertheless, it would make much more sense finding the optimal stable controller, namely, the best controller (for example, with respect to the total expected reward criterion) which is also stable, in the MSS sense. Some assumptions are important, but not emphasized.
Reject. rating score: 2. rating score: 2. rating score: 3. rating score: 5. <BRK>I would encourage the authors to develop that strategy before publishing their method. Setting these parameters to strict values would be very unfair given that the authors method is not required to produce such breakpoints de novo. The authors should consider the actual range of the various input data and encode with a simple and interpretable strategy. Doing so involves critical parameters, such as the allowed distance between the method prediction and true specified variant break points.<BRK>The paper is only 4.5 pages. First, the authors did not explain or investigate many decisions in their model design. I have many questions and concerns. 3) Why are the negative samples not balanced with the positive samples? Does the model distinguish these two types?<BRK>The core issue I have with this paper is that I do not think the experiment settings are realistic. I’d recommend authors properly cite and compare previous works with the proposed method.<BRK>Below is a breakdown of strengths and weaknesses of this submission. 3.The authors have considered baselines that do not take as input already known break points, which makes the comparison somewhat unfair.
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 7. <BRK>Hence in general, I think the paper is still not ready for publication and needs substantial fixing and improvement. This paper proposes Trust Entropy Actor Critic (TEAC), a novel algorithm for reinforcement learning (RL) combining the idea of TRPO/PPO and max entropy RL, together with the corresponding critic, actor and dual updates. To achieve this goal, the authors propose to augment the original trust region subproblem in TRPO with an additional constraint on the lower bound of the policy entropy (together with two other trivial constraints corresponding to the validity of the policy in the MDP framework). In general, all the terms appearing in the optimization problem should either be some constants or a function of $\pi$, but this is not made clear by the authors.<BRK>It uses a modified objective for the critic that incorporates the trust region and entropy terms   It uses a first (rather than second) order method to optimize the dual variables   It uses these to provide a proof of policy improvement. TEAC is evaluated on a number of MuJoCo tasks. At least I don’t see a definition of it. 5) The proof of Lemma 2 should point to A.3 not A.2. It doesn’t seem to be otherwise referenced and seems to be a repetition of material in the main text. (3) I m still bothered that to demonstrate improvement the algorithm is tuned on a per example basis but the baselines are not.<BRK>The authors propose to add three more constraints to the well known trust region policy optimization model. The first one of these constraints aims at keeping the entropy level higher than a given threshold. In general, I find the paper difficult to follow and hence cannot properly assess its novelty. Likewise, if we obtain a solution with tackling the Lagrangian function, then is that solution feasible for (2)? Why \lambda is in (5)?<BRK>The paper addresses the problem of reinforcement learning in continuous spaces by formulating the problem as a constrained optimization problem. The paper then derives closed form solutions for the Lagrangian, which are used for obtaining variable update rules. However, since I m not familiar with the topic, I cannot verify the details of those claims and results. I base my recommendation on a high level idea of the paper.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 6. <BRK>However, many statements are entangled and mixed, the paper has many typos and inconsistency, making it hard to follow. The use of $S_r$ is not rigorous, for Eq.3, it should be $s_r$({$X$},$Y)$ instead of $s_r(X,Y)$. I did not find how the model answers logical queries (task 3) in the inference step, i.e., given a new $(${$X_j$}$,R_{cpx})$, how to find $Y$.<BRK>The significance or usefulness of graph like rules could be established more convincingly. The Figure 1 running example seems a little contrived, and the experimental evaluation for these rules had to rely synthetic data. The actual mechanics of what gets computed and how are not very clear. The einsum notation is nice, but it is not rigorously shown why or how the operations cannot be expressed as matrix operations, or what actual computational graph PyTorch would generate under the hood to run learning and inference. #### Recommendation (accept or reject) with one or two key reasons I would argue to reject primarily on clarity and rigor concerns, and somewhat less so on significance of graph like rules. This notation doesn t seem common in the related works.<BRK>* The experiments are a bit limited in scope, but they are convincing. I realize that for the complexity of the algorithm it might be hard to scale to more datasets. * What I find odd is that the authors didn’t try to model the 3 dimensional tensor A with a deep network. I would also like to point that from equation (3) it is implied that their algorithm cannot learn arbitrarily long recursive relations.<BRK>This paper proposes techniques that generate logical rules out of knowledge graphs; the idea is to produce more complex rules than usual by exploiting a differentiable formulation of the associated learning process. In fact, it would be nice to indicate how important this notation is in the whole system. One question: looking at Figure 1 I see some questions on knowledge graphs and their possible answers, I cannot see exactly where are the "rules" there.
Reject. rating score: 3. rating score: 6. rating score: 6. rating score: 7. <BRK>The paper considers the sequential recommendation problem. (ii) two time scale: using 2 RNNs to model active users and inactive users respectively. Although the model design makes sense, it seems too ad hoc and not novel. For example, the idea (i) actually learn the model with pretrained embeddings, which achieves faster training speed. But this has been widely adopted in various domains. The comparing baselines certainly could easily adapt to use the same pretrained embeddings for faster speed. Specifically I have the following concerns:  Encoding long range sequence: the proposed method doesn t model long sequences, while seeks to capture user history with a simple MF. This idea: (i) ignores sequential patterns in user previous history; (ii) has been widely adopted in the literature[1][2], especially before we have powerful deep sequential models. Such an acceleration, again, could be adopted by other methods. SASRec performed better than GRU4Rec and also showed much faster training speed. The recall of 2TS on lastfm 1K is worse than GRU4Rec, but the improvement shows 2.2%. The proposed mode design, though, makes sense, but lacks novelty as it s a combination of various commonly used tricks. Several strong baselines are missing, which further weaken the paper. For example, in Q1 "We propose that NOT ALL users (but only active users) should use transductive embeddings to memorize their history, which is different from existing methods that treat all users in the same way. ", however, comparison against them is missing, especially for methods like [1] and [3] that uses both inductive and sequential embeddings; in Q2 "Trivially cutting off the sequence could result in worse accuracy", however, this is not verified: not clear what s the effect of cutting off on baseline models. I believe these are the core research questions that need to be verified in the paper, which are unfortunately missing.<BRK>The main idea in this work is to separately model users with a lot of activity in the systems and users with little activity and data. To this end, two RNN s are trained on the items that the users have interacted with one for active users and one for less active users. User and item embeddings are generated with a matrix factorization process and these are used as input to the RNN s and to initialize its hidden state. For inactive users, a common initialization is used. While this is an interesting idea it is somewhat unclear what the real benefits of this method are. The split between active and inactive users is rather unclear, how many interactions are then left in the inactive users split? It is also unclear if the improved results are due to inactive or active users. I also think that it would be interesting if the two models would be linked somehow explicitly rather than  just  from the joint factorization. Overall an interesting idea but the experimental results I think are not convincing and the split modeling could be justified quantitavly. minor: In table 1 last column the second best performing method is GRU4Rec. Figure 2, is not very informative for large timeframes as most methods converge in a few hours (or minutes) on this data After reading rebuttal, I think the paper is somewhat improved hence I increase the score.<BRK>Challenges  For active users, common models such as RNNs have limitations when dealing with long range sequences due to the difficulty in gradient propagation. For inactive users (or new users), there exists the cold start problem. Main idea  They partition the users into two sets: active and inactive users. Two independent RNNs are trained for the active and the inactive users, respectively. When training the RNNs, user and item embedding vectors are fixed. For all the inactive users, a common embedding initialization is shared. Strengths  They tried to explain the intuition of the proposed model in detail. The proposed method outperforms some previous methods on real world datasets. The ablation study shows the effectiveness of the proposed training algorithm. Weaknesses  The idea is very incremental compared to previous methods. There is no theoretical analysis of the proposed method. The presentation of the figures is not good (Colors in Figures 2 and 3). Summary  They suggested the need to separate learning for active and inactive users. The intuition behind the method makes sense, and it has been shown through experiments that it performs well. Authors will be able to increase their contribution through thorough theoretical analysis or developed ideas. Authors also need to improve the presentation of the paper.<BRK>Authors propose an interesting application of Recurrent Neural Networks to build a sequential recommender system with a different model for active and inactive users. In a real world recommender system, handling cold start users separately is a big challenge, and this paper proposes an interesting approach to solve that. Authors propose a two staged method, wherein the first stage, the aggregated user item interaction matrix is factorized to generate initial transductive embeddings for users and items. These are later re used in the second stage and for active users, their embeddings are fixed. Some strong points of the paper:1) A modular approach is taken to handle active users and inactive users separately. This would be helpful in designing real world recommender systems, where the cold start problem is critical. 3) The solution is well motivated with proper justification in Section 3 and in the introduction. I assume cold start items is an equally critical problem faced in designing a RecSys system. 2) Why are item embeddings kept constant in the second stage? For frequent items, learning/updating embeddings during RNN training would enhance their quality.
Accept (Oral). rating score: 7. rating score: 7. rating score: 7. rating score: 10. <BRK>Pros  The work analyzes the differential NAS methods from a new perspective that the value alpha is not suitable for selecting edges. Based on this observation, the author proposes a new method to evaluate the edge strength. Some descriptions and figures are not very clear. The paper is interesting and has found some values for the NAS community by rethinking the representation ability of the important factor $\alpha$. I tend to accept this paper.<BRK>  Short Summary  This paper proposes a new policy for selecting the optimal architecture in neural architecture search (NAS), in particular for methods that involve a one shot model and that deploy gradient based methods for the search. I would recommend to do a detailed proof read of the paper. In ICLR 2019<BRK># SummaryThis paper identifies an interesting phenomenon that on DARTS based method, the operation can not be simply chosen based on the maximum value of trained weights. The authors propose a new selection paradigm. This is the first time I have seen a simple and reasonable explanation of this phenomenon, and by itself is a great contribution to the NAS community. Training for longer simply destroys the search and without proper training, the network is essentially in a random search state, i.e.it goes back to the game that DARTS do not out perform random search. They are too short. Grammar issues	  Abstract: one of the most ... methods (s)<BRK>(In DARTS the finalization step actually orders incoming edges by the max of the architecture weight magnitudes at each edge and selects the top two edges and the corresponding maximum architecture weight in them as the final operators.). This paper examines this quite ad hoc step very closely. It finds that the magnitude of architecture weights (alphas commonly in this niche literature) are misleading. The paper proposes a much more intuitive finalization step which just picks the operator at each edge which if removed from the supergraph results in the largest drop in validation accuracy. Comments:  The paper is wonderfully written! This paper is actually throwing a big wrench in one shot differentiable NAS literature.
Reject. rating score: 6. rating score: 6. rating score: 6. <BRK>## Summary ##This paper considers the relevant problem of group fairness in ML when there are no predefined groups. It would help to discuss it for the readers who have not read Martinez et al (2020)## After Rebuttal ##I thank the authors for their clarifications and efforts to improve their work. * There is no discussion about the result in Lemma 3.3. Furthermore, being robust to any subgroup allows for the algorithm to be fair to groups who have not yet been declared as protected groups.<BRK>The description is now easier to follow and the convergence results now follow directly from Chen et al.I m raising my score to 6, but still find the novelty in the formulation to be somewhat limited. This paper s proposal is to formulate a min max optimization problem: like Martinez et al.the minimization is over the space of pareto efficient classifiers, whereas the maximization is over all (soft) partitions of the data into two groups. They then propose an algorithm inspired from Chen at al. I like the problem formulation and find the problem setup to be practically relevant (given that there s an increasing emphasis in the fairness community on addressing scenarios where the protected group information is noisy or unknown). However, I m unable to verify correctness of algorithm and guarantee  The authors say they adapt Algorithm 3 from Chen at al., but I find the proposed algorithm to be different from the one in Chen et al.Unfortunately, the textual description doesn t delve into any of these differences.<BRK>##########################################################################Reasons for score: The idea for establishing performance for unknown groups is interesting, simply because the set of protected attributes (e.g., nationality, race, genetic information) is evolving with time. In this regard, I find the premise of the paper interesting. Their setting is a bit incremental compared to multiple recent papers on minimax Pareto fairness guarantees, and not motivated beyond unknown groups (see cons). ##########################################################################Cons: Comparisons from related work are weak. in comparison to Martinez et. Why are subgroups based on outcomes relevant? al 2020, and Hashimoto et a. For which applications do the authors argue that distribution independent results are more useful, compared to these recent distributionally robust results? ##########################################################################Questions during rebuttal period:  Please address and clarify the cons above  #########################################################################Typos:
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 4. <BRK>In particular, it proposes to introduce high dimensional and high entropy label representations for group truth, to improve image classification performance from two practical matters   Robustness and data efficiency, while achieving comparable accuracy to text labels as the standard representation. I like the idea of approaching image classification problem from a new angle that are not well explored yet. The experiment design is also quite comprehensive as it covers all potential perspectives and variations. ##########################################################################Cons:  Although the proposed representations have shown better performance in image classification problem, with evidence to support its out performed robustness to adversarial attack and data efficiency   achieving comparable accuracy with less data in training, I would still suggest the authors to conduct the following studies to enhance the quality of the paper: 1. 2.What’s the performance with high dimensional and high entropy label representations, comparing to text label, for other kind of the classification problems, such as NLP problems.<BRK>The authors study the effect of data labels on the quality of trained models. The results show that high dimensional and high entropy label representations are more useful, which is observed in the experiments related to robustness and a limited amount of training data. Such a result is very interesting and suggests that the label representation can be further explored and potentially plan an important role. This paper starts an interesting direction and conducts nice experiments.<BRK>On the other hand, zero shot learning has characteristics in that it uses an additional information to learn the embedding space. Also, the author argues that audio labels is special, but in the paper, other type of high dimensional representation of labels that uses external information are not explored, such as word2vec. The results showed that the proposed approach are more robust in adversarial attack and feature effectiveness. Second, to verify the specialness of the audio label, other external data can also be compared, such as general word2vec, ...<BRK>Ratings:Although the paper shows some interesting research direction, the way the authors show the effectiveness of the proposed method is mostly built on empirical results, which is not enough to claim such a bold argument. The experiment results show that the proposed approach is effective in terms of adversarial robustness and data efficiency. Strength: The idea of giving speech signal as a proxy for categorical label is interesting. The authors hypothesize the labels with high dimensionality & high entropy will help the network learn better feature representation.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. rating score: 6. <BRK>This paper aims to improve the robustness of DARTS, and proposes to add an auxiliary skip connection branch to the “mixOp” in cells. The authors also analyze the effect of auxiliary branch on residual block from the view of gradient flow. Moreover, extensive experiments on multiple search spaces and datasets are conducted, showing the effectiveness of this method. In general, I think this paper propose a simple but efficient method to alleviate the performance collapse of DARTS, the strengths and weaknesses are listed as follows:Strengths:1). I am sort of concerned that the auxiliary skip connection may suppress the weight of original skip connection.<BRK>The paper focuses on improving the robustness of differentiable architecture search models. I think this is a strong paper from an empirical perspective. I tried to look at the prior work (mostly those mentioned in the paper and frequently cited) and based on this quick research, it seems that the approach is novel.<BRK>Therefore, the authors propose to add an auxiliary skip connection to play that role. The paper provides an interesting theoretical analysis that this can help prevent the gradient vanishing problem. In the experiments, this method   DART , is compared with DART and several other approaches, and show that the proposed method can outperform the others, however, by small margins. The paper provides some interesting theoretical analysis to show the potential impact of the auxiliary skip connection on the gradient vanishing problem. cons:While the basic idea is well motivated, one could question about the specific architecture to add the auxiliary skip connection.<BRK>This paper presents an interesting method to alleviate the mode collapse of DARTS (all operations degenerate to skip connect). This is done by simply adding a skip connect operation to complement the output of the cell function and making the coefficient of the auxiliary operation decay with time. The method is tested on a few benchmarks and settings.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. rating score: 5. <BRK>* In practice, even when using the proposed method, one still has to choose a relatively small set of possible augmentations. Overall:The paper is proposing a well empirically motivated modification to current contrastive learning methods, the methods and results are presented clearly, the experiments are extensively described. The authors show empirically how making a model invariant to specific augmentations is  detrimental to some tasks (e.g.adding rotation invariance degrades 100 category ImageNet accuracy). * Extensive evaluation on several different tasks, showing consistent improvement on all and highlighting the flexibility of the proposed approach.<BRK>The paper addresses potential information loss in contrastive learning, when the invariance to a variety of augmentations may be suboptimal for other downstream tasks that require the model to learn those augmentations as discriminative features (e.g: colour in fine grained bird classification). Strength:+ Simple, yet elegant approach. + Paper is well written and easy to follow. I read authors response to my question and as well as other reviewers feedback. I will keep my rating as it is.<BRK>The current mainstream way of doing contrastive learning in Instance discrimination is to train the network to associate  two independently augmented versions of the same image. The authors argue that some augmentation (such as rotation and texture) are bad when used in a general augmentation pipeline. They propose to use embedding sup spaces during Instance discrimination learning that would effectively learn from the previously hard to learn augmentation. But I wonder if the method can generalize to more subspaces and achieve gain over MoCo on larger datasets like full ImageNet?<BRK>Given this, and the fact that the paper is well written and motivated, I am increasing my score. The rationale is that different downstream tasks may require different types of invariances (e.g.we may want to be rotation invariant for pictures of flowers, but not for pictures of animals), and one does not know a priori which kind of invariances will be required. Reasons for score  I believe that the authors aim to tackle a very important question in self supervised learning. I appreciate the effort of the authors running these extra experiments.
Reject. rating score: 3. rating score: 3. rating score: 5. rating score: 6. <BRK>The authors introduce a log barrier extension loss term enforcing soft constraints on the range of values to enable fully end to end quantization aware training. Strengths of the paper:   The paper addresses an important topic, because there are increasing concerns in performing fully end to end low precision training to deploy on low precision hardware. Survey on the related work is not sufficient. It is lack of convincing evaluation results to support the proposed scheme. You should provide how critical the problem is on the low precision hardware with the other SOTA quantization schemes. Accuracy loss is intrinsic in fully end to end low precision training. (3) The quantization range is fixed in the proposed scheme. For example, assuming that the weight distribution is Gaussian is too strong to be practical.<BRK>This paper introduced a log barrier based regularization method to reduce the dynamic range of data types (activation, weight, error, gradient, and input) in neural networks. However, the reviewer is afraid to think that there are serious technical issues with this claim. It is not clear how the proposed method is evaluated.<BRK>I would like to point out that there have been works that have analytically determined suitable fixed quantization ranges in the context of backprop. The proposed method method is very nice, Lagrangian optimization techniques are used to ensure weights do not fall into the tails of the distribution. This reduces the probability of overflows. It is not a very serious issue, but the clarity in that regard could be improved. The experimental results are good. I wonder if the authors could spare a few sentence to discuss how they simulated accumulation quantization. Comments Post Rebuttal:I still find the technical contribution of this paper to be a good one. However, As stated in my original review, there were some clarity issues with the manuscript.<BRK>It further indicates that the accuracy of MobileNet using the proposed method is quite significantly lower (66% vs. 72% for Sun) compared to previous works. 3) The results in Table 2 are sparse. 6)The paper is missing comparisons with a couple of highly relevant papers on fixed point training listed below:[1] Zhang et al., Fixed point Back Propagation Training, CVPR 2020.
Reject. rating score: 4. rating score: 4. rating score: 6. rating score: 7. <BRK>1."In recent years, plenty of research work has been published in the area of NAS." 3.The definition of SPD matrix is wrong, this definition is for positive definite matrix, not for SPD. 4.In Eq.(1), log should be matrix logarithm, and need to be mentioned. 18.Given that the operations are already defined, definition of SPD cell is NOT novel! 23.The experimental setup is weak, e.g., SPDNet and ManifoldNet need to be compared with same model complexity, otherwise the comparison is not fair!<BRK>The work focus on finding better SPD Manifold Networks from the view of neural architecture search. Then bi level optimization problem is solved using the same method with DARTS. 2.My main concern is novelty problem. The authors also treat SPD Network as a graph in Figure 1 such that the DARTS method can be applied directly.<BRK>Then, the first attempt for NAS problem of SPD manifold networks is not an appropriate motivation for such a combination. The authors should clarify why it is necessary to search neural architecture for SPD inputs. The experiments are conducted on three datasets. +: The paper is clear written.<BRK>This paper proposes a neural architecture search problem of SPD manifold networks. However, the comparisons of the experiments results are limited to the SPD methods.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 4. <BRK>Transformer based bidirectional LMs pre trained using Masked Language Model loss typically share input and output token embeddings. This paper makes an interesting investigation about decoupling input and output embeddings and gains which can be obtained out of this decoupling. In particular, this paper shows that the pre training performance of transformers and the transferability of the learned representations can be improved by increasing the dimension of output embeddings while reducing the dimension of input embeddings. E.g.>  the model pretrained with a larger output embedding size slightly outperforms the comparison method on average despite     having 77M fewer parameters during fine tuning    > Reducing the input embedding dimension saves a significant number of parameters at a noticeably smaller cost to accuracy     than reducing the output embedding size.<BRK>Summary: This work investigated the strategy of reallocating parameters of multilingual language models for improving their cross lingual transferability. Then,  they proposed a Rebalanced mBERT (RemBERT) model that reallocates the input embedding parameters of mBERT model to the output embedding and additional layers. Experimental results on XTREME benchmark showed that RemBERT significantly outperformed XLM R with similar model size. Pros:   The paper is well written and easy to follow. Shouldn’t decoupling the input and output embeddings double the parameters of the embeddings?<BRK>This paper systematically studies the impact of embedding coupling with multilingual language models. The authors observe that while na¨ıvely decoupling the input and output embedding parameters does not consistently improve downstream evaluation metrics, decoupling their shapes comes with a host of benefits. The idea of decoupling embedding is novel, and the evaluation results are strong. Strength:+ The systematical study of the impact of embedding coupling on state of the art pre trained language models. + The paper proposes a method to reinvest saved parameters to the width and depth of the Transformer layers and achieve significantly improved performance on the XTREME benchmark over a strong mBERT.<BRK>This work studied the impact of embedding coupling in pre trained language models, by taking a multilingual model as backbone. The major finding is that decoupling the input and output embedding shapes can bring benefits, and the output embedding plays an important role in the transferability of pre trained representation. A rebalanced mBERT is designed by combing and scaling up the investigated techniques, achieving strong results on the XTREME benchmark. The authors must conclude with optimal values of E_{in} and E_{out}, otherwise the paper is merely a series of experiments with different values of E_{in}, E_{out}, and # of layers in the baseline.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 6. <BRK>This paper proposes an improvement of Binary Neural Networks from the work of Bi RealNet (Liu et al.2018) by utilizing the linearity of modules. In addition, similar to recent works in BNNs, the paper also utilize group convolution layers to reduce the number of parameters and save the computation s cost. The experiments are tested with image classification tasks on the large scale dataset of ImageNet. The paper is well written and straightforward. Although the results in ImageNet are promising and the computation is less, It raises concerns about the novelty of the work. The paper used novel FPReLU but missed some comparisons with a similar idea work using PReLU[1,2]. For results in ImageNet, it would be nice if we can do more comparison and analysis with the state of the art recent work [3]. In conclusion, with current manuscripts, the paper is not sufficient enough to present at the conference.<BRK> paper summary :  This paper proposes to improve the BNN’s discriminative ability by introducing additional non linearities. In addition, the paper exploits the group convolution to enable a wider network, which can strengthen BNN’s representational ability, while keeping the total overhead unchanged. Pros :  This paper introduces some practical methods to improve the performance of BNNs. In particular, the additional FPReLU is convincing. Moreover, the paper shows that grouped convolutions can be applied to wider BNNs to increase the representational capability while keeping the same complexity. Cons :1:  This paper is incremental with limited new technical insights. The original idea comes from EfficientNet. c)  Some other tricks such as replacing the 1x1 downsampling shortcut with pooling have been widely used in the community. For example, the authors argue that “despite the big quantization error made by quantization, the binary model can achieve much higher accuracy than the real valued model with no quantization error”. In other words, minimizing the quantization error can hinder the discriminative ability of BNNs, which is the main point of this paper. This observation is interesting, but needs further theorems to further explore whether the quantization error has some relations with the predicted accuracy under some constraints. If zero quantization error cannot lead to a good result, then what should be the best trade off? I encourage the authors to further explore this issue. At the current stage, it is far from enough. 3: The experimental results in Table 4 may have mistakes. However, the formulations in Eq.(3) and Eq.(4) are not equivalent if both values are binarized to { 1, 1}. 5: More experimental results on deeper networks (e.g., ResNet 50,  101) on ImageNet are needed to justify the effectiveness of the method.<BRK>The author proposed an improved binary neural network (BNN) model. FPReLU which increases the discriminative ability of BNN and group convolution make the proposed BNN architecture achieve better accuracy than prior works under similar operation budget condition. The paper was clearly well written, but I still have a concern or questions about the parts described below. 1.According to the sign function in Appendix E, its output value is either 0 or 1 if its input is from ReLU and is either  1 or 1 if its input is from PReLU, FPReLU or not passed through a non linear function. According to [1],  better performance can be achieved by using [0,1] activation in BNN, so my major concern is that the accuracy improvement of Bi Real Net V2 compared to Bi Real Net may come from the fact that both [0,1] and [ 1,1] activation are used in a network. 3.One of the factor which can decide the model accuracy is the number (or total bits) of parameters, so I think the performance comparison among BNN models will be more clear if the authors add that information of each BNN models. Minor comment:In Appendix C, it is stated that original ResNet 18 model is used as a teacher model in all stages, which is different from [2] where the model trained at the previous stage is used as a teacher. This is the difference between using a teacher which has a good classification ability and using a teacher which resembles the student. I question if the authors had any reason to choose the former method and how two training cases affect to BNN performance. Reference[1] Peisong Wang, et al.Sparsity Inducing Binarized Neural Networks. [2] Brais Martinez, et al.Training binary neural networks with real to binary convolutions.<BRK>summary:Summary:The authors note that the poor performance of BNN may be due to the disappearing of the amount of non linearity in the input output transformation. They propose a new Bi Real architecture where they replace the scaling factor with extra non linearities. Strengths:Analyzing the role of non linearity is a very nice idea. Studying BNNs is a clean setup for understanding the well known predictive power of NN, even in real valued settings. Weaknesses:The comparison is between binarized but rescaled networks and binary networks without rescaling but added non linearity. It is not clear if the good performance of the proposed architecture is due to i) the BN step making the scaling redundant or ii) the additional nonlinearities. In some sense, noting that the inclusion of non linear transformations can boost the predictive power of a network is not surprising. The extra computational cost associated with the introduction of non linear activation is not fully discussed. The comparison does not include the case where non linearities are added to the rescaled version (the one on the left of Figure 1). how does the introduction of extra non linearity affect the computational budget in the proposed method? is there a figure presenting the performance of the reduced budget model shown in figure 3? is the benefit associated with extra non linearities be expected to extend to other architecture than Bi Real?
Reject. rating score: 4. rating score: 7. rating score: 7. <BRK>The authors propose adaptive self training that uses self training + meta learning for few shot training of neural sequence taggers. Specifically the authors focus on reducing noisy training data for student models and reweighting them. * Overall, It would ve been nice to see a deeper discussion around whether this is needed in the first place, what benefits does it give, and if so what are all the ways of solving this problem and why the particular approach is the right one. RE WEIGHTING PSEUDO LABELED DATA* I could not find sufficient novelty about the "token" aspect. * Have the authors compared with an explicit distillation step of a teacher (E.g.BERT+finetune + distill)?<BRK>SUMMARYThe paper presents a series of strategies for self supervised learning for sequence labeling tasks. The proposed model is a teacher student network. The student model is trained on the pseudo labeled data. This paper introduces a strategy to select informative labeled examples to use as dev set for the student model, and adapts an existing re weighting mechanism for pseudo labeled examples to the sequence labeling setting. It would be interesting to see an experiment where the entire dataset is used for training the teacher model and an external unlabeled data is pseudo labeled. The impact of adaptive label data acquisition is not tested. do you have same instances and same tokens in different batches? why do you need that? Minor, regarding the ablation study in A.1, with S 3 you get the best results, why not try with more?<BRK>SummaryThis paper proposes an adaptive self training framework, called MetaST, for tackling few shot sequence labeling tasks. The framework consists of several components: a teacher model that finetunes with the few shot training data and generates noisy labels for the unlabeled examples; a student model that learns from re weighted noisy labels (at the token level), and an iterative process to update the teacher with the trained student. This subset is sampled based on the student model’s uncertainty to improve learning efficiency. It outperforms previous semi supervised learning systems across all the evaluated tasks. The comparison against previous systems (including ones using BERT, similar to the proposed model) seems quite thorough. Ablation studies that showcase the effectiveness of each model component. It is nice that the paper contains a fairly complete ablation study to analyze the effectiveness of each of these components, though.
Reject. rating score: 5. rating score: 5. rating score: 5. <BRK>This paper proposes model splitting as a method to perform private inference from an edge device to a cloud provider. The authors achieve this by finding representations that are approximately equivalent in the following layer (i+1), but have much smaller norms than the original feature vectors. Experiments on four datasets point to the effectiveness of this idea, and that simple baselines such as adversarial training or pruning perform worse. I thought the core experiments were sound, but I am not totally convinced by the motivation of the approach. The paper would have been much improved if these kinds of questions were captured by a formal threat model. 5.In Figure 4, why is there a large drop at the final Conv layer? > "..do not contribute to the public accuracy."?<BRK>The proposed method does not need to identify the private attributes. Pros:1.The idea of removing irrelevant information instead of private attributes is an interesting idea. 2.The paper is well organized and well written. Cons:1.The key concern about the paper is the feasibility of the proposed methods in deep neural networks. However, the adversary can choose to use a more complex model to extract the privacy attributes in the evaluation. It would be nice if more adversarial models can be evaluated in the paper. It is hard to tell if the better tradeoffs are due to the deeper layers or fully connected layers. From Figure 4, it seems the proposed methods do not perform well on the convolutional layers. Page 6 Figure 4 shows that the information leakage can be controlled using the following factors “factors”<BRK>The paper is well organized with sufficient background discussion and the related works. This paper lies in an interesting setting of client server, by sending the shared representation z while decoding the public and private feature at the server side. The private information is not necessarily to be orthogonal to the public information. Thus, by the proposed method, without the semantic labels of the private attribute, it is not clear why the orthogonal to public feature would necessarily to be private feature. While in the conclusion, the authors mentioned they do not require the knowledge of private attributes, which is a contradictory. If the public and private information are with some correlation, would the proposed framework still work under this situation?
Reject. rating score: 4. rating score: 4. rating score: 4. <BRK>This paper develops EXP4 style algorithms for Gaussian bandit and RL, proving upper and lower bounds and give empirical evaluations. However, a simple way to do that is to train them and evaluate the policies separately. How to use function approximation with EXP4 is an interesting yet challenging question.<BRK>One is on the theoretical aspect: It analyzes the lower and upper bounds of EXP 3 for Gaussian multi bandit setting for which the reward can be unbounded. What if you consider a baseline that involve multiple different DQNs and randomizes/explores among them? This paper is overall clearly written. Other than both are algorithms in the EXP family, I did not find much connection between them.<BRK>The authors consider analyzing the EXP3.P algorithm for the case of unbounded reward functions, in the sense that the rewards are governed by a Gaussian distribution. I am not sure how to interpret these regret lower bounds, since they require the horizon length to be bounded from above. In general, there are quite a few typos, and some parts of the writing are  a bit ambiguous in the way they are phrased.
Reject. rating score: 3. rating score: 3. rating score: 3. rating score: 6. <BRK>The proposed method is a federated method allowing to have a certain amount of data shared between all the learners and some data specific to each learner. The method consists in learning a global classifier (with the shared data) as well as local classifiers (one per learner, using the local data). The first objection I will make to this paper is the confusion it causes when talking about privacy. In this case efficiency is not privacy is the motivation and the gain should be evaluated in terms of efficiency. In this case, the domain adaptation methods apply and should have been investigated by the authors.<BRK>The paper proposes a federated learning framework using a mixture of experts to trade off the local model and the global model in a federated learning setting. A three step pipeline is designed to train personalized FL with a mixture of global model and local model. Pros:1.The proposed setting is a new scenario that considers both opt in and opt out devices. 6.Convergence of the gating function is not discussed.<BRK>The paper proposed a novel personalized federated learning method using a mixture of global and local models. Step 3 is to train a personalized local model by mixing local and global models. The mixed use of global and local models (equation 6) is not a novel way of federated learning. The paper is an integration of the mixture of experts method with existing personalized federated learning. The paper s contribution is incremental.<BRK>2.WeaknessThe proposed method is not novel. The third step which fine tunes in the local and global models using a gate network is essentially fusing the global and local models. Overall ScoreGiven the above concerns, I recommend reject this paper in the current stage. Most importantly, such an empirical method lacks analysis or convincing experimental results. Using the same learning rate for all baselines are wrong experimental settings. The dataset CIFAR10 and CIFAR100 are not difficult enough to demonstrate the concept of the proposed algorithms. The opt in and opt out strategy is totally empirical without any intuition about why it works.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 7. <BRK>Existing works tend to learn either global representations or local representations, while this work aims at learning versatile representations that generalize well to both scenarios. Without comparing in an apples to apples manner with other self supervised contrastive learning methods (e.g., GDT, AVID, etc.),  it would be unconvincing that the proposed global local audio visual contrastive learning method indeed learns better representations that have the suggested properties. Justification of rating: The paper proposes a decent idea to learin global local reprentations and evaluate on various tasks/datasets. Post rebuttal:Thanks for the clarifications in the rebuttal. It addresses some of my concerns.<BRK>The idea of designing new network architectures that can take advantage of local and global information is an interesting direction. The paper relies exclusively on pretraining + finetuning experiments. While it is encouraging that the method outperforms other self supervised pretraining methods, these comparisons are not completely apples to apples, since they use different network backbones. The paper should explain what the novelty of their approach is, and should compare with previous methods. I didn t find the paper to be particularly well written. I found it challenging to understand the motivation for the method and the description of it.<BRK>Another interesting direction would be to compare to this work in the speech domain (https://ieeexplore.ieee.org/stamp/stamp.jsp?tp &arnumber 9054057), since it also trains with two self supervised contrastive losses, one trying to capture global representations for identity, and one trying to capture local representations (in time) for content. Strengths:   The method and experiments are described clearly. What is the performance on the DeepFakes downstream task with only the local or the global contrastive loss? Weaknesses   The main motivation for the paper appears to be a self supervised approach that can capture both global and local information, and hence the “views” of the data do not need to be carefully selected for every downstream task.<BRK>**Summary**This paper presents a new method of using self supervised contrastive learning to learn global local audio visual representations, resulting in strong results on a variety of tasks. The proposed method leverages simultaneously captured information in audio and visual modalities at different time scales to generate different "views" of the data for contrastive learning. + Strong empirical results on a variety of downstream tasks indicate that the learned representations do indeed generalize well to both global and local downstream tasks. **Weaknesses**  A better explanation about the motivation behind using the non standard MIL NCE training objective would be helpful. I would like to see it accepted to ICLR, provided the above concerns are addressed.
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 5. <BRK>Update: I really appreciate the response from the authors. Some of my original concerns have been addressed, and additional experiments help to show the benefits of CAM HD, so I have increased my score to 5. Most reviewers have pointed out the presentation and insufficient experiments, so it s better to submit the improved version to one of upcoming conferences. **Summary**This work proposes an optimizer that adaptively determines a learning rates from different levels (global, layer wise, parameter wise) based on the hypergradient framework. **Detailed comments**The proposed method adaptively adjusts learning rates at different levels (parameter wise, layer wise, and global).<BRK>Considering that it needs some additional works to solve all these concerns, I suggest the authors to improve the paper and submit it to one following conference. By setting the constraints on the learning rates at multiple scales, the paper derived a hierarchical learning rate setting approach, which is the combination of adaptive learning rates at different levels. 4.The comparisons with more network optimizers should be given in the experiments. I think that the initialization of hyper parameters of combination weights seem to be heuristic, and it is unclear on the effects/robustness of its initialization on the optimization performance.<BRK>### Summary of the paperThe paper investigates the setting of hyper gradient descent in the context of adapting learning rates at different levels in a neural network (e.g.per layer). The paper derives an equivalence between regularization of such learning rates and a weighted combination of non regularized adapted learning rates. As the experiments go as far as ResNet 34 on CIFAR10, it would be interesting if a comparison to existing results from the literature could be made.<BRK>The results on their metrics are poor and ambiguous. The analysis is limited and there are few generalizable insights to be gleaned from the paper. There’s no transfer analysis or generality analysis, implying that each task will have to have its hyper parameters tuned independently. ClarityTheir combination ratios plot is unclear. Hypergradients are known and this is an extension to more parameters which will interact with one another. Multiple levels of tree based interacting hyperparameters is novel, to my knowledge. The potential upside of a working method here is high, as the learning rate dramatically impacts model performance. There are proofs backing convergence claims made by the authors.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 6. <BRK>In Summary: I like some of the ideas presented in this work, but I found it difficult to read. I think the biggest problem is that the authors  motivation for various choices is not well described. Some of these bells and whistles may not even be needed, so simplifying the model and streamlining the text would go a long way for me.<BRK>The authors setup F to be decomposable into two types of embeddings, one which characterizes the group activity and the other which characterizes the individual activity. Pros:The authors highlight a very interesting application. Cons: It is not clear what is the form of the embeddings e_\phi. It is not clear what is the baseline for the interpretable individual embeddings. Is there a way to setup this framework to check that true F and learned F in the synthetic dataset match more directly? Do the authors consider the model results robust given this high variance in the synthetic dataset? The way the paper is written it seems like these terms could be used interchangeably, but isn’t this is what the authors are trying to show in the paper?<BRK># SummaryThe authors introduce a novel method for non negative matrix factorization for timeseries and apply it to longitudinal honey bee interaction data. These temporal basis functions are functions of the bee’s age. However, the AC’s are in a better position than I am to judge the latter point, so I am not certain about my recommendation and could be convinced otherwise. The authors also offer some interpretability of the temporal basis functions as corresponding to different social roles in the bee colony. # Pros* The paper is clear and well written. # ConsI have a few primary concerns, listed here roughly in order of significance:* I am concerned that this paper might have too narrow a scope in too niche a field for it to be suitable for ICLR. Perhaps it is better suited for an animal behavior or computational biology journal/conference. * While the paper focuses on interpretable representations of honey bee behavior, it lacks evidence/support for the usefulness of these representations.<BRK>The authors present a matrix factorization model to jointly characterize the lifetime interactions of thousands of bees over generations. However, these features are ostensibly things they already know about the individuals in the population. The paper requires more principled motivation for the choices the authors made as well as cleaning up the notation.
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 6. <BRK>Additionally, adversarial samples constructed on traditional few shot algorithms are empirically shown to transfer to the fine tuning few shot algorithm (when using similar feature extractors). Pros:1.This is the first paper that looks at support set poisoning as an adversarial attack in few shot learning. Is the support set used as the seed query set in this case? Since it is one of the baselines, a better explanation for the method should be included. This is unrealistic. Few shot learning deals with scarce amounts of data. 4.As pointed out by the author s the transfer of adversarial samples in supervised learning is not new. It is not surprising that it happens in the case of few shot learning as well, so that should not be counted as a major contribution.<BRK>**Pros:**+ The paper considers the construction of adversarial examples for a new learning paradigm which has practical relevance. + A number of possible threat models under the few shot learning paradigm are considered. **Cons:**  The attack methodology is not particularly novel, as it is just a simple extension of standard PGD attacks. The writing in the paper lacks clarity. The descriptions of meta learning algorithms are not clear enough for a reader with knowledge of supervised learning but limited background on few shot learning. In particular, there should be a dedicated section explaining the differences between attacks on traditional supervised learning and few shot learning. For example, the defender may add steps of training on the poisoned task to  undo  the effects of support set attacks  The attack success metrics are not clearly defined.<BRK>This work proposes adversarial attacks on few shot learning systems. The authors then apply evasion attacks to a support dataset, such that the n shot classifier s loss is maximised on a query set. I am not meta learning expert, so I have assumed the authors choice of meta learning algorithms and datasets is fair. The downside is that this work introduces attacks, which as far as I can tell, are quite simple extensions or applications of gradient based evasion attacks such as PGD. Is this a fair characterisation of the paper s contributions? For example, to (Goldblum et al., 2019; Yin et al., 2018) on the evasion attack side?<BRK>It performs an experimental evaluation of proposed attacks on various state of the art few shot learning / meta learning classifiers. In particular paper does not clearly describe goals of adversary (it’s implied that adversary wants to make model always misclassify entire test set), does not discuss capabilities of adversary (white box vs black box). * The main contribution of the paper is a poisoning attack, however authors talk quite a bit about adversarial examples (evasion attack) which could be distracting from the main point of the paper given that paper does not really add any new technique specifically related to evasion attack. If feasible, consider adaptation of the attack for different goals (i.e.change only a subset of predictions of the classifier on test set) and capabilities of the adversary (i.e.black box). Authors may refer to https://arxiv.org/pdf/1804.00308.pdf which discuss most of the necessary terminology and which is already cited by this paper. Thus I would recommend to change wording and call it “considered attacks” or something similar instead of “taxonomy”.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 7. <BRK>The authors discuss several ideas aimed at improved semi supervised learning by adopting an appropriate "plate model" with probabilistic content, and then examining various techniques and variants. Also the connection with neuro symbolic learning is interesting but it feels a bit too much; why exactly is it needed in this framework? Or is it just an optional add on? (Besides, for the proposed approach to work I believe more testing is needed.) I could not find it.<BRK>#### Pros+ This paper is motivated by a very important problem. Proceedings of the twenty first international conference on Machine learning. Although this is a theoretical paper, I think the authors should improve the notations and provide some simple examples for helping readers understand this framework. The authors have surveyed a wide range of related works in the area of semi supervised learning and neuro symbolic learning. However, this paper should include more discussion about related works in statistical learning. The first $\theta^x$ should be $\tilde{\theta^x}$.<BRK>The paper aims at proposing a theoretical rationale for discriminative semi supervised learning that is comparable with that of generative models. Moreover,the paper aims at theoretically justifying a family of neuro symbolic SSL approaches. As such, the paper does not provide "a theoretically principled understanding of integrating ‘connectionist’ and ‘symbolic’ methods." So overall I think that the paper claims more than it effectively provides. In Figure 1 the left and center subfigures are equal, I guess in the left one x and y should be exchanged.<BRK>The authors introduce a discriminative model for semi supervised learning for which several existing methods are special cases. This is a neat direction and in keeping with recent trends to integrate statistical and logical reasoning, although the paper would be strengthened if the authors gave concrete examples of a real world dataset for which their innovation would be helpful.
Accept (Poster). rating score: 7. rating score: 7. rating score: 5. rating score: 4. <BRK>The paper uses an optimal transport approach to ensure that two similar items that differ only on a sensitive feature (e.g., gender) are ranked similarly. Minor note: References: Some of the references should be cleaned up and made more uniform. The regularization term is weighted by rho, a parameter than increases fairness as it increases. In contrast, Fair PG Rank design for group fairness does poorly on individual fairness. I d like the authors to be more careful about using the word "sufficient". What is the running time of Algorithm 1 both in theory and in the experiments (seconds). How does the running time of SenSTIR compare with Fair PG Rank?<BRK>The authors extended individual fairness approach to the domain of learning to rank domain. This paper proposes a method for training individually fair learning to rank models by making use of optimal transport based regularizer. While that papers focused on training individually fair ML models by casting the problem as training supervised ML models that are robust to sensitive perturbations, this paper extended the idea to individual fair ranking that are invariant to sensitive perturbations of features of ranked items. The code and the datasets used for the experimentation have been provided. I vote to accept this paper. Question:My only question for the authors is how easy would it be to learn fair metric in a typical application for LTR models?<BRK>The paper address the problem of fair ranking. The authors use a notion of individual fairness, meaning that two similar inputs should receive similar outputs. I didn t find an explanation of how the counterfactuals are computed. Finally most of the paper is based on work done in Yurochkin &Sun 2020 and Yurochkin et al 2020. This is unfair as SenSTIR is the only algorithm to use the same kind of counterfactual data than the one used for the evaluation.<BRK>The paper tackles the problem of providing rankings that are "fair" to individual items being ranked as opposed to groups of items as in previous work on fair ranking. In other words, fairness is only an issue is there is a lack of resources, which means that randomization doesn t really solve the issue, but just gives one the illusion that it is resolved. I should point out that there are multiple implementations of LambdaMART out there and it s important to used the best one, which is the one included in the LightGBM package. You can find some results for the MSLR dataset in this paper among others: https://dl.acm.org/doi/pdf/10.1145/3336191.3371844Given these issues, I don t think the paper is ready to be published and I encourage the authors to think carefully about their problem definition and experimental setup before resubmitting the paper.
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 9. <BRK>A brief summary:This paper shows that the model trained to restore sequential data from images in a supervised manner tends to capture more informative latent representations of the data. Strengths:+ Demonstrates that training a decoder model to reconstruct sequential sketches leads for an encoder to better represent the input image. + Achieves the SOTA result in Omniglot recognition task, compared against existing unsupervised methods. The main contribution of this paper, I believe, is mainly the idea to utilize the sequential sketch data during the supervised training time. However, I found most of the implementation details quite unclear, and experimental results were often misleading. Will it improve the test result? May have been much more interesting if this paper explores the unsupervised disentanglement in latent space, to support their claims for what they note as structured embeddings. Conv VAE alone may not be the best baseline.<BRK>This paper proposes a generalized sketch drawing model named SketchEmbedNet for producing sketches and visual summaries of open domain natural images. The idea is interesting and the experimental results show SketchEmbedNet is able to do not only few shot classification but also one shot generation. Many sketch synthesis methods, such as [8] in the reference, can reconstruct the sketch with a sketch image input. (4)	Figure 5 shows SketchEmbedNet outperforms VAE on latent space organization. I would like to see the comparison with sketch pix2seq, which is the reference [8], as both SketchEmbedNet and sketch pix2seq do not use KL term in training.<BRK>The authors evaluate their learned embeddings on few shot classification tasks and explore the the quality of the latent space. The performance on natural images in Figure 4, especially on unseen classes, is not great. In particular, most of the experiments are done on datasets that have ground truth sketch strokes but not their ordering (e.g., SVG files). In this case, it seems like imposing an order on the strokes (and asking the decoder to replicate it) is a counterintuitive constraint for the model. I m not sure that it is fair to compare to fully unsupervised few shot classification methods.<BRK>Authors investigate the possibility to learn a generalized embedding function that captures salient and compositional features of sketches by directly imitating human sketches. The manuscript is written clearly and concisely. Methods have been presented with enough detail and seem accurate. Particularly, the results from the Quickdraw and Omniglot datasets showing generated sketches are rather impressive, and the ones for the natural images seem promising. Overall, I very much enjoyed reading the paper and suggest it for publication without any major changes. These results deserve more space in the manuscript. Could the authors elaborate on why or why not this may be the case?
Reject. rating score: 3. rating score: 3. rating score: 3. rating score: 5. rating score: 5. <BRK>**Update**: Thank you to the authors for addressing the comments and updating the paper. So, it is possible to improve the robustness of ResNets by reducing its Lipschitz constant. I believe that more experiments are needed to demonstrate that Neural ODEs offer robustness advantages.<BRK>I think this is a fatal flaw in the derivation process, and the authors should address it. Keeping strong assumptions and weak empirical evaluations in mind, I feel this paper needs a lot more work before it can be considered for acceptance.<BRK>So, this statement is unrelated to the main claims of the paper. So I believe the paper is incomplete and will need substantially more effort at explaining the phenomenon.<BRK>I have read the authors  comments. might work outjust as well. I feel the paper is well written and clear.<BRK>Other than this main issue, there are also some limitations of the current work. They also try to support their claims that Neural ODEs are more robust due to their continuity (small step sizes) through experiments. As an aside, can the authors supply more details on how they generate adversarial examples for the neural ODE model? Based on the above observations, I believe the current paper is not ready for publication in ICLR yet.
Accept (Oral). rating score: 9. rating score: 8. rating score: 7. rating score: 7. rating score: 7. <BRK>The experiments are thorough and well executed, comparing with listening tests to many state of the art neural vocoders for the conditional task. It describes a thorough evaluation of the unconditional generation task, which is in general difficult, but in this case was constrained in such a way as to make it feasible and informative, using reasonable metrics that clearly show the advantages of the proposed approach. The literature review is thorough and comes at a point in the paper where the reader understands the proposed approach and can appreciate the nuances of the differences between the approaches. The paper has two minor weaknesses. First is that it does not make a clearer distinction between the concurrent work from Chen et al (2020) along similar lines, although presumably this paper was not released prior to submission of the current paper. An extended comparison would be welcome in a camera ready version of this paper. Second, that it doesn t explicitly state the real time factor of WaveNet generation in the results discussion on page 6, which is presumably much smaller than 1. Overall, this paper makes a strong contribution to the field of neural vocoding and to the field of representation learning more generally for long duration intricately structured signals (i.e., speech).<BRK>Well written paper with strong, well presented results. I was a bit confused by the core model description at first. Rather, it s just a step in the diffusion process? At first I thought it was a time index, and so the model seemed very much like an AR model, leaving me very confused. Use of multiple models as reference models, WaveNet, WaveGlow, WaveFlow, Clarinet, WaveGAN, in addition to the proposed model. Focus on unconditional generation, which AIU has not received that much attention in the communityWhere I am unsure is the originality of the work. I personally am not aware of the diffusion approach having been applied to TTS, but this is not my primary area of expertise. Obviously, if there is related work in TTS with diffusion models, this should be cited. The paper would be strengthened by having a more balanced conclusion. I have a few more specific comments. Throughout the paper, "... audios ..." : "audio" is not usually used as a plural noun. "Notably, the quality of audios ... "  > "Notably, the quality of audio ... ""Note that, the quality of ground truth ...": nit, no comma after "that".<BRK>Summary:The authors adapt the recent trend of work on denoising diffusion probablistic models to the task of conditional or unconditional waveform generation. Using the same principles as in (Ho et al 2020), as well as a Wavenet like non causal model, the authors provide state of the art results for both tasks, as evaluated on spoken digits dataset (for unconditional and conditional generation) and on the LJ speech dataset (for deep vocoding). The generation speed is comparable to previous methods. ## ReviewThe paper is clear, well structured and the authors provide many experiments to validate their approach. While serious, the paper does lack novelty, as the method is completely taken from (Ho et al.2020).The architecture is similar to Wavenet, but non causal. Note that this exact same non causal wavenet architecture has already been used for source separation [Rethage et al.2018, Lluis et al.2019].One limitation is that unconditional, or weakly conditioned generation (i.e.not conditioned on a mel spectrogram) is only evaluated on single digit generation, which is relatively limite. It would be interesting to add WaveGlow or WaveFlow to the SC09 comparison. Overall I recommend acceptance as the paper show that denoising diffusion process can be used for waveform generation, even though the paper does not bring further novelty. References:Rethage et al.2018: A Wavenet for Speech DenoisingLluis et al.2019: End to end music source separation: is it possible in the waveform domain?<BRK>The paper develops a speech synthesis model using denoising diffusion processes, a generative model framework recently demonstrated in image generation (Ho et al.2020).The application is straightforward and there is little if any theoretical difference from the Ho et al.paper.I didn t check the proofs included in the appendix, but they along with the learning and sampling procedure seem to be already developed in (Ho et al.2020).The authors should take care to be very clear about the mathematical developments that are directly taken from the prior literature, and what developments are introduced in this paper. The experiments and demos are convincing, and the results could be considered highly competitive in conditional generation and  state of the art for class conditional and non conditional generation. The writing at times could use improvement. If you want to introduce an abbreviation derived from a term or phrase, a widely accepted conventional method is to italicize the phrase and define the acronym the first time it is used, as in "\emph{diffusion waveform} (DiffWave) model". DiffWave has already been defined relative to "diffusion waveform". If this is supposed to be cute, it s not. Also "audios" is not a word. Please use "audio signals". This is repeated throughout the paper.<BRK>The Diffusion Probabilistic model is gaining popularity as a generative model. Diffwave explores the same for speech synthesis tasks. They show very good results i.e.matching autoregressive Wavenet on the conditional and outperforming baselines on the unconditional audio waveform synthesis tasks. However, the work is significant for speech synthesis applications since it shows great results, with a small foot print network with a very new method. This work can be expected to spark a plethora of follow up works for speech synthesis and other real valued time series modeling tasks. Lack of novelty in terms of insights/approach2. would help the readers get a lot more value out of the paper.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 5. <BRK># Summary This paper proposes an alternative data augmentation method for robust image classification. Training on augmented data has been shown to improve robustness. For example, a traditional auto encoder, as proposed in 2006 [2]. * The empirical results are not comparable to other reports in literature. In AugMix, the task loss penalizes the prediction on clean data, in DJMix, the task loss penalizes the prediction on augmented data. How do the methods differ when using clean or augmented data for the task loss? From equation (2), I understand that $\hat{x}$ is a convex combination of the discretized image and the original image. Overall, this method proposes to augment images using discretization.<BRK>This paper proposes DJMix, a data augmentation method to improve the robustness by mixing each training image and its discretized one, and analyzes DJMix theoretically from the Information Bottleneck perspective. In addition, it creates datasets, Segmentation C and Detection C, to benchmark the robustness of CNN models on semantic segmentation and detection tasks. Although it achieves good results in some settings, e.g.CIFAR 10 in Table 1, when combined with Random Augmentation (RA), it looks like the overall results are just comparable even slightly worse than AugMix. Therefore, I prefer to reject this paper until it has more solid experiments.<BRK>In this paper, the authors propose to linearly combine the following: the original image, and a discretized version thereof, to form an augmented image. Using the established pairing of a distance based loss (for pushing logits from augmented/unaugmented data to be similar) and a classification loss, they show improved performance on a range of benchmarks that aim to test robustness to natural noise. Overall, linearly combining an image with a discretized representation of itself is a neat trick to produce an augmentation where the contours and lines of an object are identical. The authors frame the aim of the work as needing  robust methods that are agnostic to test time noise a priori . I would recommend more discussion of results and a more thorough examination of the trends in the results. For instance, the task loss is different to the augmix baseline — but this is not really explored except by experiment. Are there reasons why the different loss might be theorized to improve results? In general, the robustness of the proposed method is slightly overstated. This is especially true when the corruptions are applied one at a time.<BRK>#### SummaryThis paper proposes an image augmentation strategy DJMix for helping the network robustness in image recognition tasks over the corrupted images. The experiments show it performs better than AugMix on segmentation and detection over pascal dataset. #### NoveltyThe method is simple while effective as demonstrated with experiments, though it is similiar with Augmix. The idea could related to some robust training methods proposed. for example, Improving Adversarial Robustness by Data Specific Discretization, which should also be discussed. It would be more strong by comparing with these networks.
Reject. rating score: 4. rating score: 6. rating score: 6. <BRK>The paper introduces a first order algorithm for nonconvex nonconcave min max optimization problems. I think that this is a major shortcoming of the solution concept. As the paper itself presents in the related work there have been a lot of recent developments on variations to the standard GDA techniques such as extra gradient, optimistic methods, different types of averaging, etc which are known to significantly and robustly outperform GDA both theoretically and experimentally across numerous datasets. On the negative side neither the theoretical nor the experimental results seem particularly strong.<BRK>This paper proposes a new stochastic gradient descent ascent based method to approximate a stationary point (or local min max solution) of a nonconvex nonconcave minimax problem with application in GANs. The method is similar to the one in the GAN original paper, but the authors incorporate it with an acceptance rule and use a different model for the max problem. Unfortunately, the reviewer was unable to verify the proof due to the time limit. The reviewer finds that it is really hard to understand the proof techniques as well as the meaning of local min max points defined in this paper especially via a neighborhood D_{x*,y*}. Many places are explained in words which are also hard to verify some of the statements.<BRK>This paper treats non convex/non concave min/max problems motivated by the respective problems that arise in GAN training. The main contribution is that they develop an ADAM based algorithm that converges to \eps  local min/max points. The paper seems to be well written and easy to follow. Moreover the proofs seem correct and sound. Therefore, a reasonable question would be how this work is related with this kind of results.
Accept (Poster). rating score: 6. rating score: 5. rating score: 4. rating score: 4. rating score: 10. <BRK>As the proposed method should also work for fixed point quantization and even binarization. The effectiveness of the proposed approach can be better justified by comparing the results with other SOTA fix point quantization and binarization schemes in [1 4]. The current scheme selects a random subset of the weights to add the proposed noise, which means the weights in different layers are chosen with the same probability. For pruning, existing work [5] finds that it is desired to prune different fractions of weights for each layer. I would consider raising my score if the authors could address the aforementioned concerns.<BRK>This paper introduces Quant Noise that quantizes a random fraction of the network at each step instead of quantizing the entire network. Pros:1.The proposed technique is simple and easy to use. But I do not think this is one of the contributions of this paper. Besides, why Quant Noise works for int4 quantization is not clear. If so, I think the authors should focus on low bit quantization in the experiment section and test the proposed method on more low bit quantization settings  (e.g., 2bits quantization, 3bits quantization, mix precision quantization, etc).<BRK>During training, only a different random subset of weights is quantized for forward propagation such that controlling the amount of noise is a way to improve model accuracy when extreme compression is applied. The training graph with various learning rates or regularization parameters could have elaborated on the contributions of this paper. The authors show poor results trained with QAT and claim that the proposed method can improve the accuracy significantly. Also, it should be clarified whether int4 quantization and int8 quantization are post processing quantization or not. This reviewer is wondering why the authors do not choose the models for easy and fair comparisons.<BRK>The proposed method is validated on multiple models like Transformer, ConvNet, and more challenging EfficientNet B3. Experiential results show that the proposed method is practical. One of important novel points in this paper is subset quantization. A potential advantage of subset quantization is to mitigate the Gradient Mismatch problem caused by STE. There are some technical differences among these methods.<BRK>Reasons for Score:The work proposed in this paper is novel and very well presented. The claims are supported by experimental results on different neural network models and applications, showing a good trade off between accuracy and neural network compression. Which is the training time overhead of having quantization noise during training?
Accept (Poster). rating score: 8. rating score: 7. rating score: 5. rating score: 4. <BRK>Why isn’t FedAvg reported here? Parametric experiments on a simple heterogeneous dataset (built as the union of digit classification tasks) show that the proposed method yields a better performance, and a more stable one, than standard FedAvg. The text « we started with including… then, we simultaneously added n clients » is a bit ambiguous. The use of both m and M in Sec 4 can be confusing10. In Lemma 4.3, assumption 1 should be replaced by assumption 4.111. Although the underlying model is a simplification of the experimental reality, the problem tackled is very complex, so this result is still a significant contribution for the community. D/ WritingAlthough dense, the paper is well written and easy to follow. 2.There is a minor error in the proof of Corollary 4.6, which does not invalidate the results.<BRK>Such as batch norm and some optimisers ? To alleviate this issue, the authors propose to exclude the BN parameters from the aggregation (i.e.each client has its own BN excluded from FL). This paper is self contained in the sense that the initial problem is well defined with some theoretical background and visualisations. The proposed FedBN convincingly outperformed all the other approaches in all the setups. I think the content should be a bit more structured. + Finally validating the use of local BN that was certainly used by many researchers but for "unexplained" reasons. It s great to see one. What happen at testing time ? Let s say we want to distribute the trained model to a new client that does not have already computed statistics ?<BRK>This paper develops a modified version of FedAvg by local batch normalization that is tailored for federated learning with non i.i.d.data.Different from most of existing work that consider the unbalanced labels, this paper uses unbalanced features to motivate the non i.i.d.federated settings. 5.The simulations are not convincing. Merits:1.The paper tackles an important problem in federated learning, which may have practical impact. The condition under which this holds true needs to be explicitly mentioned in Theorem 4.4 and Corollary 4.5.<BRK>This paper proposes a minor modification to the existing FL learning framework, which can be easily implemented. The major contribution of this work comes from a few theoretical analyses. The theoretical analysis seems pretty solid and intuitive. Also, please add the comparison with the non iid scenario with the one proposed in FedAvg. Without these comparisons, I believe the experiment is not complete.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 7. rating score: 7. <BRK>The paper is well written and the background work is adequately discussed. One may say that the problem of density estimation is harder than estimating the size of the support of a distribution, and therefore assuming access to such oracle is not natural.<BRK>The paper studies the following problem: suppose there is a distribution $P$ over $n$ class {$0, 1, \dots n 1$},  and one has access to a set of samples drawn from $P$. I am not convinced by such argument.<BRK>The paper considers the problem of estimating the support of a discrete distribution, when provided access to samples and an oracle that approximately predicts the probability of the observed sample. They propose an algorithm based on Chebyshev polynomials  and also show that the proposed algorithm is optimal. The results are interesting and I recommend acceptance.<BRK>This paper considers the support size estimation problem using a random sample from the unknown distribution and access to some predictor of the element frequency. The algorithm is empirically evaluated by both real and synthetic datasets. I will recommend for acceptance after the concerns below are addressed. This seems to be not justified in the paper. It seems that the proposed algorithm reduces to WY once $b$ is moderately large such that one interval suffices.
Accept (Spotlight). rating score: 8. rating score: 7. rating score: 7. rating score: 6. <BRK>The paper proposes a neural topic model derived from the perspective of optimal transport (OT). The paper is well written. The experimental results are convincing. The paper gives an intuition for the transport matrix P in section 2.2. It may be better if it can also give more explanation on the role of M and what the meaning of the distance between two probability vector is. It would be interesting to see how the proposed method performs compared to those models. It is suggested to do so for reproducibility and for the use of practitioners.<BRK>This paper builds off existing neural topic model work that s based on variational autoencoders and specifically introduces a novel loss function based on optimal transport (more specifically, the Sinkhorn distance). The "weight" of this paper seems to really be carried by the experimental results though, which I found to be quite compelling. Supervised versions of topic models I think also are worth exploring.<BRK>Summary: The paper proposes a neural topic model which log likelihood is regularized by Sinkhorn distance, instead of following Variational AutoEncoder (VAE) approach. It s good to see how a well established technique can be applied to topic modeling and show encouraging empirical results, but the lack of justifications on the modeling approach would make it difficult for the further development and adoption of the method, especially because the proposed method is not a probabilistic generative model anymore. Maybe this is related to some sort of variational approximation which disentangles the usual constraint that topic distribution of all words in the same document are exactly the same.<BRK>This paper proposes a new variant of neural topic model leveraging optimaltransport in order to incorporate information from pre trained word vectors. Basically this paper is interesting, but still leaves some questions. K means results are only auxiliary evaluation of the former, thusthe reader would like to know whether the proposed model could yield better perplexity on documents or not. It seems that the choice of dimensionality and the number of topics seemstoo low and arbitrary. The proposed OT regularization seem to work better, but I cannot see why thebaseline of word vector based topic models like (Dieng+ 2020) is inferior.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 6. <BRK>For this to be more useful, a clearer understanding of the use cases is needed. The authors compare both random convolutions as images and mixing them with original training images.<BRK>The use of random convolutions is interesting. 2.I notice that in [1], the performance of PAR can be further improved with strong data augmentation. Significance: I think this paper proposes a simple yet effective method.<BRK>I think this is where authors could capitalize more on. * The word `texture’ in the paper is a mis nomer.<BRK>I am also strongly concerned by the way the paper confuses texture and colour. The authors posit that this approach will lead to increased robustness and generalisation of the network as if forces a bias towards shape and away from texture (and colour), and demonstrate this with a wide range of experimental results.
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 7. <BRK>#### Summary:In this paper, the authors propose to replace commonly used shooting based methods for action sequence planning in learned latent space dynamics models by a collocation based method. The authors propose a collocation method based on Levenberg Marquard optimization with a scheduled Lagrange multiplier which outperforms two shooting methods (CEM and gradient based) on a set of robotic tasks. I wonder about the discretization of the colors in Fig 8. The paper still does not detail the update rule for \lambda_actOverall, the author response has addressed some of my technical concerns, but the main challenges are only addressed partially. It is unclear why collocation should find goals better than CEM or gradient descent for sparse rewards. If the reward function network learns this sparse reward, there is no meaningful gradient towards the goal for an optimization based method. If not reward shaping has been used, why is the learned reward by the PlaNet network useful for collocation? Hence experimental results are questionable. The idea is evaluated in a sufficient range of experiments,  although further experiments on standardized benchmarks (DeepMind control suite) would significantly improve the paper. The points raised in weaknesses above should be addressed. How do the terms in the Lagrangian relate to the residual terms? In 6.3, you write "To provide a fair comparison that isolates the effects of different planning methods, we use the same dynamics model architecture for all agents". The paper should explain how this was implemented.<BRK>Collocation approaches are effective in settings with difficult path constraints, and thus exploited by this work to dramatically improve model based reinforcement learning. Solving a collocation problem in the latent space is a sensible approach, and a much better idea than using CEM planning or shooting. ### Cons  It s a reasonably straightforward application of collocation in a learned latent space. While I have not seen this done previously, it is a relatively obvious improvement. However, none of the tasks (pushing and reaching in free space) considered in this work are long horizon tasks, or particularly challenging. I would like to see a comparison against trajectory optimisation using a dynamical system with learned locally linear models, which arguably allows for simpler planning and control. I  believe that more discussion on the contrasts between these ideas would be a useful addition to this paper. The algorithm seems to indicate that dynamics model learning and planning happen jointly,  which doesn t really make sense   we shouldn t need to re learn a dynamics model at planning time. I assume that this is not the case, as experimental methods seem to indicate that dynamics and reward models are pre trained, separately from trajectory optimisation using collocation.<BRK>## summaryThe paper proposes to transpose colloction methods to solve planning problems in a learned latent state space. This can then be used as a replacement for shooting methods in model based RL, particularly suitablefor image based tasks, where planning in the observation space is impractical. ## pros  Basic shooting methods are a primitive planning technique; we should be able to do much better. Using collocation methods in learned latent state spaces makes sense. ## cons  The problem is only difficult because of the attempt to learn the task directly from visual inputs. In this sense, the tasks are "straw man" problems that are uninspiring. The tradeoffs are not as simple as portrayed. ## recommendationsI currently lean marginally in favor of acceptance, purely on the grounds that transposing collocationmethods to latent spaces does havae future potential. However, the given examples are uninteresting. ## questions  How would the results compare to simply using the latent state to estimate a traditional compact state descriptor  and then using that with a classical motion planner? What are the general constraints or restrictions, if any, on transposing the many known planning methods into   the latent space? What is stochastic about the dynamics, if anything, for the chosen experimental tasks? ## feedbackThe output is a trajectory, not a policy. To make it actionable would require using the optimizedtrajectories to learn a policy or to use MPC. The title could more directly address the contribution, i.e., motion planning vialatent space collocation. "To this, collocation methods" (sic)Figure 2: the text refers to a decoder, but this is missing in the figure. The dynamics model is left unlabeled. Section 5, Constrained optimization: "balance between the strength of the dynamics constraint."<BRK>Summary: The paper studies the problem of planning in domains with sparse rewards where observations are in the form of images. It focuses on solving this problem using model based RL with emphasis on better trajectory optimization. The experimental results show improvements over a) zeroth order CEM optimization, b)  PlaNet (Hafner et al., 2019) and c)  gradient based method that optimizes the objective in Eq.1.Strengths:i) The motivation, organization and the overall writing of the paper are clear. Weaknesses:i) Discussion of literature on planning in latent spaces [1,2,3,4,5] is left out and should be included. Here, space can be saved by removing Figure 4 since all of its subfigures look identical given their (visual) quality. iii) How do you reason about the length of the horizon T? iv) There does not seem to be any presentation of hyperparameter selection/optimization, runtime results or quality of solutions. Similarly, Figure 5 is very hard to read and not clear what each axis represents. Overall, I would say this is the weakest part of the paper. ** Post Rebuttal **To best of my understanding, the authors have addressed all my questions and suggestions with the appropriate revision of their paper. Specifically, the necessary discussion of hyperparameter selection is added and presentation of the runtime&solution quality results (i.e., raised in point iv)) have been improved with the inclusion of important details, additional discussion of related work is added (i.e., raised in point i)) and questions are addressed (i.e., raised in point ii) and iii)).
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 7. <BRK>### After rebuttal ###Thanks a lot for the authors  extensive experiments and good explanation to my questions. I would increase my score. The basic idea of this paper is promising and useful. However, there are still several problems after reading the rebuttal. Figure 2 shows that GD can distinguish different noisy levels, however, it is not a very realistic setup when training with a noisy dataset, to be specific, the dataset only has one noisy lever rather than multiple. Furthermore, GD is not able to give an early stopping criterion on noisy datasets from Figure 2 where test error keeps decreasing but GD is increasing. ### Original comment ###This paper aims to provide an early stopping criterion measured by gradient disparity when training with limited data. The authors also provide theoretical insights on inducing the gradient disparity and empirically show the proposed method is robust to label noise. 2.Regarding Figure 9 in the appendix, the figure failed to explain the correlation between GD (2nd column) and test accuracy (4th column) where GD goes up. Still, accuracy has various behaviors. Is the method *only* work with a small dataset? What is the difference between Test error and Generalization error? In Figure2.<BRK>Early stopping is achieved by monitoring such a metric without needing a validation set. Some theoretical results are also given to motivate the use of gradient disparity as a generalization metric. The use of a generalization metric to replace the validation set is an interesting idea. From the experiments in the paper, it seems the proposed metric indeed shares a similar trend as the test error. Cons:1.It looks like gradient disparity can be more correlated to generalization error instead of test error on some datasets (e.g., in Figure 13 for MNIST). This could make the algorithm stop too early since in some cases, the generalization error is increasing, but the training error decreases even faster and overall the test error is decreasing. However, gradient disparity has a similar trend with generalization error and when it increases for 5 epochs, we will terminate the algorithm while the test error is still decreasing. In addition, using test error + gradient disparity as a proxy for test error is not valid since gradient disparity has a different scale instead of while test error is between 0 and 1.<BRK>The paper proposes using gradient disparity between two batches for early stopping, and explains the reason by showing its connection with the generalization error. Finally, the experimental setting uses k fold CV instead of a fixed validation set, and it is not clear what the standard deviation of the experimental results for k CV means, e.g., the standard deviation describes (i) randomness due to data splitting; or (ii) randomness due to training algorithm? My main concern is related to the experimental setting: The experiments in Section 5 compares (a) k CV with (1 1/k)N training samples + N/k validation samples; with(b) Gradient Disparity (GD) using  N training samples. Experiments on limited size dataset and noisy dataset are presented.<BRK>This paper proposes a novel early stopping criterion called gradient disparity, which is the the l2 norm of the difference between two gradient vectors on two different batches from the training set. In contrast to typical early stopping techniques that require validation error on a held out dataset, the proposed criterion fully operates on the training set. In practice, the algorithm take, say, five batches and compute the average pair wise GD as the early stopping criterion, where the GD is computed with the loss values of each point rescaled by the standard deviation of the loss values in the batch. Overall I vote for accept. Pros: The method appears to be well motivated both theoretically and intuitively; empirically results show that the proposed gradient disparity strongly correlates with generalization error. The experiments are quite extensive. What exactly is your early stopping algorithm? Do you early stop when the GD metric increases, or there is some kind of threshold (I suppose this metric would be noisy)?
Accept (Poster). rating score: 7. rating score: 7. rating score: 5. <BRK>This paper presents a way to learn from demonstrations with weak or no labels. The premise behind this paper is that even when humans provide labels during a demonstration, those labels often do not fully describe the data (e.g., the human may say "soft" when "fast" would also apply). The variables are modeled such that the observation is conditionally independent of the human provided labels given the latent variables. This allows the human provided labels to be decoupled (or disentangled as the paper calls it) from the observations. This model was applied to a task where a human would teleoperate a robot arm and apply a dabbing motion in relation to an object in the scene. This is an interesting contribution. The paper does a good job of comparing the methodology with 3 different off the shelf models for implementing the inference networks (GS, AAE, and VAE), as well as comparing each with and without the weak labels (baseline). Furthermore the extremeness of some of the results are a bit concerning. Further experiments should be done with other, more standard tasks and potentially user provided labels to better determine the performance of the weak label models as compared to the baselines. Overall, though, this paper does present a novel solution to the problem of user provided labels.<BRK>*Summary* Under the context of learning from demonstrations, the paper studies the problem of leaning interpretable low dimensional representations from high dimensional multimodal inputs using weak supervision. Paper argues that since robots and humans have different levels of abstractions and mechanisms, observation+action spaces between them are greatly misaligned which complicates learning by directly observing humans. However, the underlying concepts essential for tasks lie in a much lower dimensional manifold. Towards this end, the paper proposes to learn probabilistic generative models capturing high level notions from demonstrations using variational inference. The strength of the paper is in demonstrating that conditional latent variable models can learn disentangled low dimensional represented using weak supervision; which authors effectively demonstrated using real world experiments. My main reservations are in terms of the technical novelty of the paper and the narrow scope of experimental evaluation. However, the idea of learning "interpretable representations" using "weak supervision" is interesting and probably the most significant bit in this work.<BRK>  SUMMARY  This is a well written paper that discusses how to learn disentangled representations for the learning from demonstrations (LfD) task in robotics. It is shown that using weak supervision on top of unsupervised learning frameworks (that use the variational autoencoder for instance) can work well in this case. QUALITY & CLARITY  The paper is written very well, in fact most papers contain a lot of spelling mistakes but this paper was a joy to read in this regard :) The concepts are also explained clearly. From a robotics point of view, I would have accepted it as a very good application paper, if the authors had also presented real robot results that show the learned model in action (generating actual trajectories for the robot). The paper needs to present real robot results and cover related work in more detail. Why?* Related Work: ProMPs are not represented as dynamical systems* "To fully close the loop, the trajectories which we sample from themodel could further be executed on the physical robot through a hybridposition/force controller (Reibert,1981). However, such evaluation isbeyond the scope of the paper."
Accept (Poster). rating score: 9. rating score: 9. rating score: 8. rating score: 6. rating score: 5. <BRK>The paper also proposes three protein specific pooling operations to cope with the large input size when representing all atoms in a protein. Overall, this is an extremely clear paper, and the core ideas appear to be sound. Some positive points are that the authors consider several different tasks, and numerous state of the art methods are included in the comparison. One drawback to this work is its focus on recent literature. The manuscript is up front about the fact that a drawback of the method is its requirement that the input proteins have known 3D structure. This information is typically represented as a PSSM column for each observed amino acid. I would like to have seen this acknowledged, since it seems like a potentially valuable source of additional information.<BRK>Their method introduces a convolution operation that considers both the intrinsic distances between atoms as defined by their bond structure and the extrinsic distances as defined by 3D proximity. They also introduce interpretable pooling operations developed using known biology of the amino acids. Overall, the method is effective and straightforward to follow due to having avoided unnecessary complexity. The authors’ method outperforms a variety of competitive alternatives on protein fold and function classification tasks. These are important problems for which the authors’ model has achieved a significant performance boost. I don’t see why this model wouldn’t work well for any 3D protein structure labels that can be collected.<BRK>Pros:  I think the paper is exceptionally well written and the figures are very carefully designed. I very much appreciate proper comparison to other methods. Less important, but the model also performs better at these two tasks than any other approach. ( I say this because I believe the field shouldn t always require SoA if there is a significant technical advancement.) Cons:  The authors site "over smoothing" for why their convolution operator performs better, but provide no direct evidence that this is the case. Are there any replicates for standard error and ablation studies? Table 3 BLAST comparison is weak. This may be good to clarify. In Table 2, does the modification of the architecture change the number of parameters? Are there any sequences with post translational modifications in the dataset?<BRK>The authors have also included two additional baselines: Bepler, et al.and MSAs.More analysis is needed to compare this with SOTA in representation learning. This paper presents new convolutional and pooling operators for protein structures. These components are used to design an architecture that shows strong performance on several downstream tasks. The authors compare to Bepler, et al.(2019) which is a great baseline since it uses both sequence and structural information. Relatedly, the authors have not demonstrated that the models can generalize to novel folds. Without demonstrating this, the model cannot be used for important tasks, such as protein design. The paper would be much more compelling if the authors could show that their architecture generalizes better than prior work. The names of the test splits on the fold classification task is non standard. Overall, this paper is a great start and the proposed model architecture could be interesting to ML researchers and practitioners in the biology space. However, no baselines are included, so it is difficult to understand the result in context and understand how this method generalizes compared to existing methods.<BRK>This paper proposes a graph neural network architecture that operates on the atoms in a protein structure. ## Strenghts:* The two key model choices feel like a powerful choices for a graph neural network with an interesting domain motivated set of architectural choices:    construction of the hypergraph with shortest distance edges of 3 types    custom pooling of the graph from atom level nodes to groups of amino acids* The paper has helpful visualizations and well written, however see below for concerns around framing. Furthermore I believe the method still fits in the "neural message passing" framework (see bullet below). * Framing as (a) representation learning: improved in the updated paper, (b) convolution: still stands   the point cloud convs are not a very good comparison, since there is no graph structure there. However re: "the message passing function is learned": this is still very much within the default MPNN framework from Gilmer et al.Altogether, the whole method would still be much better framed as a graph based network, rather than shoehorning this into a description of a single "convolutional operator". There is a lot of good work in this paper, and I would consider the paper a clear accept with the same method and same results, if it were thoroughly rewritten based on graph neural networks.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 6. <BRK>Pros:1. this paper studies an interesting problem, "imitation gap" in imitation learning. 2. the paper is easy to follow. Especially, the example part is easy to understand. 3. the intuition for this idea is well explained. Cons:1. from my perspective, the basic idea is dynamically using imitation learning and reinforcement learning for agent learning by a weighting function. 2. the main contribution of this paper is proposing an advisor and integrating it with the imitation and reinforcement learning process. 3. in experiment PD, adding advisor from the beginning seems not fair enough, since at the beginning the reward is much higher than baselines. Maybe the experiments need a comparison before the advisor is added and after the advisor added. In summary, in this paper, the motivation is clear and easy to understand, and the problem is worthy to study.<BRK>[Strength]+) Personally, I like the research problem presented here. +) The paper is overall clear and well written. +) The experiments sufficient enough to validate the proposed method. [Weakness]The main concern I have with this submission lies in some missing factors and their potentials to the rationale of the main approach presented here. ) However, I find this assumption could be unrealistic as there are several factors that could affect the imitation performances as well, to name a few: the number of demonstrations (major), different imitation learning algorithms (major), and even the training strategy (epochs, etc, but minor though). number of demonstrations and also imitation learning methods other than BC to verify how these factors could affect their approach. ) I  do believe some of my concerns have been addressed while the main issue on the lack of some necessary evaluations (e.g.num.of demonstrations) remains.<BRK>They are interesting to show the intuition behind the algorithm. This is the imitation gap phenomenon. It is only careless application of those algorithms in inappropriate settings such as POMDPs with asymmetric privileged information between expert and apprentice where this problem can arise. Unlike the method proposed by the authors that consists in reducing the importance of demonstrations where there is an imitation gap, they directly try to improve the amount of information of the state provided to the apprentice. Using an LSTM is a first step towards that direction but more could be done (see papers provided in the previous section). weighting demonstrations depending on the imitation gap may not have been studied but methods that try to fill the gap have.<BRK>### SummaryThis paper identifies a problem in imitation learning when an expert has access to privileged information that is not available to the learner. This paper proposes to tackle this imitation gap by following the expert only when the learner can reproduce the expert s behaviors only with partial observations. The tasks tested in the paper are limited to simple 2D examples, which are tailored to the proposed method. In theory, ADV should learn as much as BC demo+PPO and ADV demo+PPO can. Attach the appendix at the end of the main paper.
Reject. rating score: 3. rating score: 3. rating score: 4. rating score: 5. <BRK>But it is not stationary, at least in the experiments presented in the paper. I understand that the authors simplified the assumption to handle the non stationarity of the noise. 2) The experimental study of gradient noise non stationarity shows interesting behavior of the gradient noise that can be useful for future research in this area. This part should be clarified. It is an important assumption and should be stated explicitly in the statements of these theorems. Next, what are the assumptions on $p$? In the current form, this description is misleading.<BRK>This is an unconditional exceptation: it means in particular that the expectation is taken wrt to the distribution of the initial value  of the parameter (I guess therefore that the expectation is also taken on the initial condition, which can be concentrated on a point). Even if we take a  this is very difficult to check, except in the situation in which the "noisy gradient" used in the procedure is the true gradient affected by some additive noise, independent from the current value of the parameter. The paper presents two results. The Definition 1 of the non stationary noise oracle is more far fetched: "The stochasticity of the problem is  governed by a a sequence of second moment $\{m_k\}$ and variance $\sigma_k^2$".<BRK>This paper studies the problem of stochastic optimization where the gradient noise process is non stationary. I would encourage the authors to dig deeper in this direction under a suitable set of assumptions, and show some real acceleration effect.<BRK>This paper studies gradient based stochastic optimization algorithms which incorporate (estimates of) the noise statistics in the adaptive stepsize design. 2) We all know very well that the standard analysis of non convex gradient descent (which the authors adapt here) does not lead to tight upper bounds. The paper is well written, and I think Thm3 is novel, correct and interesting.
Reject. rating score: 4. rating score: 5. rating score: 8. <BRK>This paper proposes an idea to use NCE for videos where positive/negative training pairs are created by temporally sampling different frames in the video. The idea is fairly straightforward, there is not anything particularly novel about the approach, other than how the training pairs are created. Experimentally, there aren t really any comparisons to previous self supervised learning methods. This is a pretty major weakness as it makes it difficult to understand how well this task is doing. Methods like SimCLR provide >70% accuracy on ImageNet and others do well on video tasks (see missing related works below). Overall, I think the proposed idea is not especially novel and the experiments aren t strong enough to show that the simple idea is good. I think there is some potential in the paper, but needs more to be convincing.<BRK>This paper incorporate the popular contrastive with unsupervised learning from video. Specifically, multiple frames from the same video is used as positive pairs and frames from different videos is viewed as negative pair. The author also proposed a simple and effective ways to collect class balanced and diverse video frame dataset from Youtube. The author conducted extensive evaluation experiments on both video recognition and image recognition downstream tasks. Lack of novelty. (Mentioned in the related work section of your paper too)2. Overall, I think this paper is interesting but its contribution is limited.<BRK>The idea of learning representations from video rather than single images is an appealing one with many favorable properties to allow a system to get direct signal on appearance of objects under various natural transformations (occlusion, lighting, etc). The authors create a dataset based on video with positive pairs for noise contrastive estimation, conduct fairly comprehensive experiments and promise to make their newly constructed dataset available. The experiments showcase this type of learned representation outperform alternatives not based on videos on a variety of tasks. Clarity : the paper is quite clearly written for the most part. Using this procedure,..." it almost seems like a few sentences were dropped between the first and second sentences. While the information exists in the appendix, a sentence or two seem to be warranted in the main test. This might also make this representation useful for other types of tasks that are not looked at in the paper that I would encourage the authors to explore.
Reject. rating score: 3. rating score: 5. rating score: 6. rating score: 6. rating score: 7. <BRK>If I understand correctly, the paper seems to be based on a mischaracterization of the arguments in [3]**Clearly state your recommendation (accept or reject) with one or two key reasons for this choice. **While the authors tackle an interesting problem and propose an interesting solution, the arguments on which the paper is based seem flawed. The more flexible definition the authors propose seems quite close to *disentangled representation* in [3], please clarify the difference. Perhaps my intuition is incorrect, please clarify. While the direction the authors pursue is of unquestionable merit, I remain unconvinced that the work as it stands is sufficiently impactful for this venue.<BRK>I would further note that what is done in practice in the paper is different from this definition, because we have one latent space per operator, not multiple operators acting on the same space. This paper studies the notion of disentanglement in a group representation theoretic setting. Although equivariance is a good property for various reasons, it does not seem to me to be reasonable definition of disentangling by itself. The definition of disentangling still seems a bit vague to me, and I m not convinced of practical applicability of the proposed method.<BRK>+ Using operators on the entire latent space is a new direction for the study of disentanglement. The authors’ viewpoint that “isolating factors of variation” is different from “mapping these factors into distinct subspaces”, and how they propose a new definition based on this viewpoint is interesting. The MSE in the appendix is not enough for quantifying disentanglement. Since this paper focuses on disentanglement, at least Factor VAE, one of the other representative disentanglement VAE models should be considered when doing the model evaluation. Rotations may be more challenging to learn.<BRK>The key strengths of this paper are the examples that showcase the lack of ability to learn independent latent factors. Figure 2 is even more convincing in that it shows that the orbits of the different factors cannot be mapped to one another and thus cannot be truly independent. Definition 1 is trying to state the same idea but is much less clear to the average ML readerMore generally, the authors should work harder to communicate this to the ML audience. The key weakness is that their new formulation of disentanglement is that it is definitional does not give a plan of how this should be done. A set of equivariant operations on the data (that are perhaps cyclic generators of an orbit).<BRK>The authors then propose a relaxed definition of disentanglement and show that it can be realized by means of a shift operator in latent space. This is a very interesting idea that represents significant progress in an important problem. Unfortunately, the current organization of the paper does not work well: the authors devote too much space (half of the paper!) Please add appropriate references to the introduction or related work. Why is this the case?
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 6. <BRK>Summary:  This paper proposes to consider the importance of each parameter in the post training quantization of weights. The authors propose to use weight magnitude as the importance indicator and to minimize the weighted distance between the full precision weights and quantized weights. Strengths:   The paper is structured clearly and the proposed method is simple. It is not clear to me why the change in the loss in (7) and (8) is necessarily related to the magnitude of the weights. 2.The second concern comes from the novelty of the proposed method. Tuning these parameters separately for each task can be inefficient.<BRK>Based on a previous classic binary coding scheme, this paper proposed to introduce a modification $m_i$ on the binarization scaling factor $\alpha$, by considering the weight magnitude. It further use 3 hyperparamters to refine $m_i$ by constraining its upper/lower bound and exponent. This paper contains the following drawbacks:1. The proposed post training quantization method has no connection to language model. All we can say is that: Post training weighted quantization of neural network *in* (or applied in) language models. 2.There is a confusion on experimental setting: The quantization should be applied to the *fine tuned* model. 3.Author spent much effort on how to determine the hyperparameters, which is also one defect of the method: it requires exquisite tuning on hyperparameters, which are sensitive as shown in Table 2. How does the work deal with parameters in batch normalization layers?<BRK>The paper employs the binary coding based post training quantization (without retraining) for language modeling. Two methods, Greedy and Alternating, are also modified to use the importance. Binary coding based quantization is recently introduced but seems to be quite a promising direction, so this paper has some significance. The choice of E, C, P as controllable parameters seems reasonable. However, it looks like these parameters are sensitive and differs much task by task and model by model, that the robustness to the hyper parameter is not ensured. Major questions:1)	I am not sure that in Section 4.1, Equation (7) and (8) properly support the state “weight magnitude can be a dominating factor for the loss function perturbation”.<BRK>This paper proposes a weighted quantization framework that could be applied to general neural networks for language models. Thus it would make sense to take use of importance metrics to do weighted quantization across parameters. With a good introduction of related work and a pretty self inclusive references to experiment setup. Originality:Originality in this paper is mostly from introducing the concept of parameter (weight) importance and how it is defined and applied to the quantization problem. This paper is based on an earlier paper (also new), e.g., BiQGEMM (Jeon et al., 2020), which paved the foundation  to support binary coding based quantization techniques to accelerate quantized neural networks. A magnitude based importance metric is proposed and approved effective in the form of the binary codes. To fine tune model accuracy, three hyper parameters are explored and empirically investigated to discover best performance (regarding model quality). If we combine the effort of quantization with distillation, as shown with the metrics in DistillBert, we could achieve great ability of inference speed, a very smaller model size but still maintain reasonable model quality.
Accept (Poster). rating score: 7. rating score: 7. rating score: 5. <BRK>The work builds on top of the BERT models, with the adjusted contrastive learning objective goal. The authors work to clearly identify shortcomings in the current literature. Kiswahili is the smallest parallel data that you have, and I wondered what would happen with a smaller language? I think the contrastive learning approach is very interesting for this use case and made my reading and evaluation of this paper more interesting. I look forward to the responses from the authors.<BRK>Summary:The paper presents HICTL which enables models to learn sentence level representations and uses contrastive learning to force better language agnostic representations for large multilingual encoders. Reasons for score:I score this paper a 6. The constrastive losses introduced in the paper are interesting. The improvements on XTREME over XLM R is pretty strong. I would have also liked to see more ablations. Finally, the authors initialize from XLM R and fine tune on 15 languages with both monolingual and parallel data. Can you present the breakdown of results in different tasks by language as an appendix? Why did you not make an official submission to the XTREME leaderboard?<BRK>The paper proposes a pre trained language model variant which extends XLM R (multilingual masked model) with two new objectives. Given that the main contribution of the paper is empirical (none of the ideas are new), better and more comprehensive experimental results would have strengthened this work. Contrastive losses are promising and the paper shows positive results when adding them to the previously proposed XLM R model.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>The authors presents how to learn optimal adversary following the state adversarial Markov decision process (SA MDP), and also proposes alternating training with learned attacks (ATLA) framework that trains the optimal adversary online together with the agent to improve the robustness of the DRL agent. The contribution is clear. I still have a few questions and concerns below:1) It is a natural and interesting idea to use alternating training to optimize both the adversary and the agent online. 3) Are the adversary and the agent both getting stronger over time? The paper only showed final results and did not show the running time result.<BRK>Building on the results of (Zhang et al., 2020), the paper proposes a new learning framework (ATLA), that simultaneously trains a (strong) adversary and a (robust) deep RL agent. Clarity: The paper is overall enjoyable to read, but some parts are not clearly/precisely written. Reasons for score: Overall, I find the paper to be an interesting read and its contribution relevant to the line of work on adversarial attacks in RL. As shown by the experiments, the proposed solution leads to significant increase in performance compared to state of the art baselines.<BRK>The paper is very well written and the considered problem of training an adversary along with the agent is very interesting. Within the proposed concept, the parameterized adversary can be trained by viewing the agent as a part of the environment, so it avoids to access the parameters of the agent policy. From the perspective of the agent, with an unknown adversary, the MDP becomes a POMDP with uncertainty hidden in the adversary, and hence the fact of using LSTM policy is much better for the agent is reasonable.<BRK>The learning of an optimal policy under a fixed adversary is done by solving a POMDP problem. The paper is well organized and easy to follow. In addition, the proposed LSTM based policy is shown to be more robust than regular feedforward NNs. However, there are a lot of works that attack the observations of a fixed policy [3,4,5]. And more importantly, [6] also proposes to train a robust agent under adversarial attacks. 4.The computational complexity / sample complexity of the proposed ATLA might be problematic, as for each iteration of learning, the adversary needs to solve a new MDP, which makes the proposed robust training less practical to use.
Reject. rating score: 4. rating score: 4. rating score: 6. rating score: 6. <BRK>The main usable result of Path NTK is a proxy for computing the speed of convergence of the neural network. However, the graphs are only qualitative, and not quantitative. This is very different from "Piecewise *affine* NNs", which are actually widely used (biases can be non zero). I assert that this limitation is not discussed in the paper, and my attempt to discuss it here has been eluded in the rebuttal. Despite the new and more fashionable figures the authors presented in the rebuttal, I am disappointed by its lack of accuracy and vagueness, especially when discussing the "piecewise linear NNs".<BRK>__summary.__The paper studies the problem of neural network pruning at initialization through the lens of neural tangent kernels (NTK). As a result, the paper delivers a unified perspective on SNIP, GRASP, and SynFlow. The paper also motivates a SynFlow variant from the theoretical framework. Given the limited novelty and theoretical significance, I do not think this paper makes enough contribution to be shared at ICLR. The fact that the considered pruning methods (which are being unified under the given framework) are all based on the first order approximations is not very surprising; I would rather say that the fact has been quite well known. Also, the combination with the NTK theory for the case of pruning at initialization was already outlined with details in the GRASP paper. __clarity.__The paper is clearly written.<BRK>In this paper, the authors propose a new kernel named Path Kernel to understand deep neural network training. However, in deep neural network training, the weights could be quite far from the initialization. The authors provide some new understandings to the existing pruning methods instead of unify them into a single framework. 3.This paper is well written and easy to read.<BRK>This paper proposes the path kernel which decomposes NTK in terms of path kernel. Could the authors provide a comparison between the proposed algorithm and keep ratio pruning? The experiment of the proposed variation of  SynFlow also coincides with the theory. Pros: a good theoretical foundation on the proposed algorithm which separates the data independent part to Path Kernel.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 5. <BRK>The sizes of the gradients are the same as the model parameters, so they are also 100 MB. So OLCO3 s communication scheme and compression compensation techniques are not really novel. Also, whether the communication is pipelined with the computation or not is orthogonal to the communication scheme.<BRK>Experiments on real datasets are used for evaluation. 2019.After discussion:The authors do not provide rebuttal. Hence, I keep the original opinion to give this paper a weak reject. Furthermore, there has appeared one similar paper[A] which combines sparsification, quantization and local SGD into the same framework for communication reduction.<BRK>The major obstacles in distributed training are communication costs and communication delays. In the literature there exists different methods which attempt to overcome these two issues but, as far as the authors claim, none of the existing algorithms succeeds in dealing with both these aspects at the same time. + The authors makes a comprehensive review of the literature and the major techniques used in distributed training of NNs.<BRK>I would like to encourage authors to provide comprehensive empirical results to justify the pros and cons of the proposed scheme, as well as some practical guidelines. ### pros.* the paper is well written; the arguments are supported by both empirical and theoretical results.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 6. <BRK>It is not surprise that partial updating outperforms pruning by a large margin, as the inference of small updating still uses the whole weights of the network. So I suggest rejecting this paper. The paper is well written. 3.Experiments are conducted on various datasets with various net structures to support the proposed method.<BRK>In this paper, the authors have proposed a new approach to determine the optimized subset of weights instead of simply conduct full weights updating. In order to better update the weights, they measure each weight s contribution to the analytical upper bound on the loss reduction from two sides (global and locally). While they report the accuracy in the evaluation part as one evaluation results.<BRK>This paper proposes a deep partial updating paradigm that can reduces the computational and communication cost on the edge devices by only updating most important weights in each round instead of a full update. The experiment results show that the proposed method can obtain similar performance with the full updating but costs much less communication overhead. It seems a very practical method in this area and the paper provides an interesting empirical study. So what s the justification of the definition of global and local contributions  when the loss is non convex which is the most common case in the experiments?<BRK>I d assume all the weights. cons    The overall presentation is difficult to parse. The technique owes much to pruning methods and methodologies. It would be great to have that discussion in related work, moving it out of Section 3.1 and Section 4. Equally important, does the technique work well on pruned networks? This section also refers to the second step as an "optimization" step. It would be useful to introduce a metric that combines update size with accuracy loss at the beginning of the paper. The evaluation does this, but consider pulling it forward and defining it explicitly.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 6. <BRK>The similar idea was adopted in existing work HAHE [4] as well, which also injects high order (long range) structural information as node features. In summary, the originality and significance are limited (please see detailed comments in the following). 2.The experimental results seem promising.<BRK>Experimental results show that the proposed models perform well on the tasks of node classification, graph classification, and counting triangles. Utilizing long range dependency is not new in graph neural networks. Moreover, some details of the method are not very clear.<BRK>3.Results in Figure 1: The results reported in this table need more explanation. Why one of the figures has a start point of 5% on the x axis and the other has 10%? However, it s already pointed out in the literature that long range dependencies help the improvement. 2.Results in Table 1: Results reported for GCN and GAT are not consistent with the original papers.<BRK>SummarizationThe authors formalize four levels of injection of graph structural information, and use them to analyze the importance of long range dependencies. 1 are not so convinced. Experiments1)The experimental results in Table.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. rating score: 4. <BRK>The authors propose a new semi supervised federated learning algorithm (FedMatch). The paper is sometime difficult and its clarity could be improved. I would consider the novelty of the method, from machine learning prospective, to be somewhat borderline as it uses known elements and adapt them to the introduced learning scenarios but overall the proposed solution is interesting as it seems to work well in practice.<BRK>This paper introduced a novel learning paradigm, Federated Semi Supervised Learning (FSSL), to handle the federated learning from distributed and partially labeled data within the clients. Basically, the proposed method is intuitively and technically sound.<BRK>In terms of the empirical validation, it is disappointing to see  that you are using synthetic datasets. There are far too many papers in which a novel approach does great on synthetic data without impacting the state of the art results on real world domains.<BRK>This paper studies the problem of learning a joint model for different local data sets. My impression of the method proposed in this work is pretraining the model with labeled data and fine tuning it with unlabeled data with constraints to make the fine tuned model as close as the pretrained model.
Reject. rating score: 4. rating score: 4. rating score: 7. rating score: 7. <BRK>#####################################################Summary: The paper focuses on the geometry distortion problem of unsupervised image to image translation. In practice, performing estimation and maximization of MGC is challenge, then the paper provides an approximate representation of mutual information: relative Squared loss Mutual Information (rSMI). + The paper conducts extensive experiment on the various kind of datasets. +This paper is well written and easy to understand. The improved result (e.g.KID, Table 3) is weird. I think the proposed method is added directly to current frameworks. For example, using CycleGAN + MGC makes the model less freedom to generate the target domain, which probably results in less performance than CycleGAN. I am not sure how authors select baselines. Besides, some datasets are performed on GcGAN rot/vf (Table 1), but other one only contains GcGAN rot (Table 2), and Table 3 show GcGAN (w/o rot/vf), which is weird for me.<BRK>This paper presents an image to image translation framework that addresses the problem of preserving geometric details. Specifically, the goal is for the framework to translate the color details, however preserve the structural cues. To this end, the main idea is to derive a mutual information constraint between the pixel colors of the input and translations, which is added as a regularization in the standard adversarial GAN loss. Experiments are provided on several examples and show some promise. Cons:1.It is unclear to me how precisely is the paper tying the geometry preservation with mutual information? 2.The quantitative results show only very marginal improvements over other methods. The technical presentation does not seem to match what the contribution is claimed to be.<BRK>This work introduces the geometry distortion constraint (MGC) for mitigating undesired geometric distortions that may often occur in the current unsupervised image to image (I2I) translation approaches. * Cons1) The proposed MGC is applicable only for some limited tasks, where geometric layouts are strongly preserved. Namely, deforming objects in a geometric fashion does not belong to such cases. 2) For general scenes with multiple similar objects, this constraint may lead to severe distortions. Failure cases and analysis on the limitation of the proposed constraint should be provided carefully.<BRK>This paper presents a geometry distortion constraint for the unsupervised image to image translation for a better structural similarity between the source and the target, which is deducted from the pixel correlation. The experiments on multiple GAN frameworks and datasets show its effectiveness in reducing the shape distortions in generated images. The experiments are quite thorough. The sensitivity study clearly shows the influence of mutual information by changing its weights. ## Cons  In some datasets, simply maintaining the shape similarity doesn t always mean visual appealing results.
Accept (Poster). rating score: 7. rating score: 6. rating score: 5. rating score: 4. <BRK>This paper presents a compounding strategy for constraining the search spaces for once for all (OFA) network training framework. The paper is generally well written and includes sufficient references to prior work. CompOFA s search space is supposedly a subset of OFA s search space.<BRK>Deploying models on multiple target end points with different hardware spec requires training model variants of an underlying architecture that meet the latency and other resource constraints on the device. Due to the large state space of parameter sizes for subcomponents, this search can take a while. This paper proposes a heuristic to bring down the size. The result is that the scope of this paper is narrow.<BRK>#### SummaryThis paper proposes to reduce the search space of OFA (a training scheme for obtaining networks for various deployment requirements) by scaling or shrinking the depth and width dimensions in NAS search space together. For examples, the authors argued the model dimensions are not orthogonal, and the latency requirements only need to be satisfied in a way (granularity) that are below many thresholds.<BRK>Paper Overview:This paper is aiming at optimizing the OFA method in neural network model searching and training. However, the OFA still have some problems. Thus this paper addressed a new solution, CompOFA, to speed up the training and try to get a balance between the accuracy and latency by building constraints between dimensions of model searching space and "progressive shrinking" approach. This paper gives out obvious evidence of their basic insight. The design idea and motivation of this paper is not well addressed. However, it still costs 50% training times compared to the original OFA method. However, the current design of the solution is not mature enough.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>The key innovation of the paper is to learn to set audio visual waypoints, which decomposes a final goal to useful subgoals. Pros:(1)  A deep reinforcement learning approach for AudioGoal navigation with audio visual waypoints is proposed. (2) The experiments are thorough and can well validate the effectiveness of the proposed audio visual waypoint based approach. (3) The paper is easy to follow and the provided demo can nicely illustrate the problem and demonstrate the superiority of the proposed method. Cons:(1) Rather than only current audio, the authors propose to use the acoustic memory, which aggregates the audio intensity over time in a structured manner. Overall, this is an interesting paper and extensive experiments are conducted.<BRK>This work presents an approach for audio visual navigation, in which an agent receives both an RGBD observation of the world and an audio signal emitted from the goal. The proposed approach leverages a structured memory via an occupancy grid and an acoustic map. My issue with claiming this as the first use of end to end learned subgoals in navigation is that there have been many recent works from goal conditioned hierarchical RL that use end to end learned subgoals, e.g., https://arxiv.org/pdf/1712.00948.pdf, https://arxiv.org/pdf/1805.08296.pdf, https://arxiv.org/pdf/1909.10618.pdf. The paper is well written and clear. The baselines and results are thorough and show clear benefit of the method and design choices. I appreciate both the comparison to state of the art methods for audio visual and the baseline comparisons. Is directionality from the audio signal used at all within the acoustic memory? The paper is somewhat limited by the impactfulness of the setting, audio visual navigation. My other concern is that at times the paper is unclear or overstates contributions.<BRK>It introduces the idea of an acoustic memory, which maps and aggregates acoustic intensity over time. An agent’s acoustic memory, in tandem with its egocentric depth view, is then used to select navigation waypoints in an end to end manner. Their method beats SoTA in AudioGoal for two environments: Replica and Matterport3D. There’s a very nice symmetry between the occupancy map (used for waypoint selection) and the acoustic memory map — backed, of course, by experimental results and convincing ablations. Overall, the paper is extremely well written. Namely, in the exposition of the AudioGoal task. The paper provides a comprehensive set of experiments, baselines, and ablations. This affects how SR and SPL are interpreted.<BRK>This paper studies the problem of navigating to the sound source in a virtual environment such as Replica. The main contribution is a new formulation that learns a policy on the next "waypoint" and uses the predicted waypoints as intermediate goals for path planning. The results are promising, much better than recently published baselines especially Gan et al and Chen et al.The paper is complete, well written, and the reference is thorough. The authors have also included some interesting analyses to better understand how the model works. The authors don t have to respond to this point. What I do want to hear from the authors is why waypoints are useful. If the authors restrict the action space to be the space of rooms (i.e.actions are "go to the room on the right", instead of "moving right by 1 foot"), unless the agent believes it s already in the same room as the audio source, then such a policy learning method may work quite well, maybe even comparable with the waypoint based method?
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. rating score: 6. <BRK>Using this result, they propose schemes for gradient quantization and gradient pruning that are theoretically principled and outperform existing methods in the literature. I was very impressed by the principled nature of the authors  approach. Can the authors provide any intuition as to why a log normal might be better? * Clearly for pruning, logNormal distribution seems to work better than logLaplace.<BRK>This reasoning does not directly apply to intermediate gradients. The key contribution is likely to be of interest to the ICLR community and the paper is well executed. The paper proposes a principled approach to the compression of intermediate gradients by fitting a log normal distribution to the individual derivatives, which they show is a good fit for the studied models. Those benefits are more obvious in a distributed setting, where the main bottleneck to training is the communication of gradients.<BRK>The authors suggest an interesting finding where the gradient distribution in each layer is close to log normal distribution instead of normal distribution. There are many previous studies on quantizing gradients for efficient distributed training, such as QSGD [1] and signSGD [2]. Although most of those studies do not reduce the computational complexity for backpropagation, they still serve as well performed gradient quantitation methods. I think it s better to distinguish from those previous studies explicitly in the paper. How does weight update work with the quantized gradient?<BRK>This work proposed a very interesting idea that the back propagated errors have log normal distributions. The authors could extend this intriguing observation into computation efficient algorithms; reduced precision floating point quantization or the pruning of the back prop error, which are very interesting. Table 1 seems to be a good start, but it is a little disappointing to see that a very limited set of image classification models are tested with the proposed method in Table 3 and Fig 5.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 5. <BRK>The authors studies an episodic MDP learning problem, where they propose to study an Optimistic Closure assumption which allows the Q function to be expressed as a generalized linear function plus a positive semi definite quadratic form. The authors propose a regret bound for the algorithm. The proposed work is an interesting development to the line of research on RL with function approximation, and is large well written.<BRK>The first contribution here is to establish that it is enough to assume that the function approximation class (for Q functions) is closed under an optimistic (~inverse covariance bonus) version of Bellman update. The paper establishes that this is strictly more general the linear MDP assumption, where the above discussed closure holds for backups of all functions (and not just linear Q functions).<BRK>### Summary  This paper analyses an existing algorithm (LSVI UCB) with generalized linear function approximation instead of conventional linear function approximation. Under this generalized linear setting, they propose a so called “optimistic closure” assumption which is shown to be strictly weaker than the expressivity assumption in the conventional linear setting. The paper also derives a general error propagation through steps that do not require a closed form expression of the empirical dynamic and reward functions as in the linear case; this could be applicable to general function approximations.<BRK>Key to their method is assuming the Q function takes the form of a generalized linear function plus an optimism term. Once this assumption has been made, they demonstrate that their algorithm, The LSVI UCB algorithm provably finds a policy with bounded regret.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK>The paper introduces and analyses the possibility that the effectiveness of PGD based adversarial attacks might be reduced by imbalanced gradients between the terms of the margin losses commonly used. Pros1.Understanding why some adversarial defenses make some versions of PGD (e.g.with cross entropy or margin loss) overestimate, sometimes even largely, robustness is an interesting direction, and the proposed explanation of imbalanced gradients is, as far as I know, novel. 2.The proposed attacks are effective against many defenses. Cons1.It is not clear what the proposed metric Gradient Imbalance Ratio (GIR) captures and how it is connected the scheme of MD. Overall, the hypothesis the imbalanced gradients might be a cause of overestimation of robustness is reasonable and worth exploration, but the metric proposed by the authors does not clearly capture this, and the attacks which should overcome that issue are not clearly better than existing methods. Update after rebuttalI thank the authors for the response.<BRK>This paper explores constructing adversarial examples in classification, in order to create better robustness metrics for general classifiers. It is also not clear why, when one term dominates the other, that it is a form of nonrobustness. The premise of this paper is to use gradient imbalance as a way of creating perturbation targets, which are claimed (and shown numerically) to better fool networks that are trained to withstand more traditional attacks, and can be used to create more robust models in general.<BRK>This work highlights the existence of imbalanced gradients as a phenomenon that may hinder optimization of gradient based adversarial attacks and, thus, give a false sense of robustness. The paper addresses an important issue in adversarial machine learning, i.e., the security evaluation of a defense. The paper provides an exhaustive analysis and formulation of the problem of imbalanced gradients, for which also a heuristic metric is provided. The attack formulation and algorithm are clear. It is important to remark that rather than proposing a new attack framework, the authors actually only propose an optimization variant of the existing PGD and MT attacks and run these attacks with the proposed changes.<BRK>Motivated by such an issue, a marginal decomposition (MD) attack is proposed to offer a stronger robustness measure. In general, the paper is well written, and the studied problem is interesting. The MD perspective explains why label smoothing may provide insufficient robustness. 2) Does the proposed stronger attack offer a stronger min max defense? Suppose that the ordinary PGD attack is replaced by an MD attack during min max training, will it offer better overall robustness? The general question to ask is: In addition to root cause analysis on the ineffectiveness of some existing defense methods, what are the additional benefits of the newly proposed MD attack? In the black box setting, will the MD attack be more query efficient than a commonly used PGD black box attack? After reading other reviewers  comments, I shared a similar concern on the marginal contribution.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 6. <BRK>Note that this trivial solution does not depend on the model $m(\cdot; \theta)$. ### OriginalityThe use of the adjustable parameters for fitting noisy data is studied in the literature of robust learning. ### SignificanceBecause of the flaw I raised above, I think the contribution of this paper is not significant. This fact also implies that the analysis of the objective function alone (without the regularization, in Section3) is no longer meaningful. Moreover, because the L2 regularization (or weight decay) is an essential factor, the tuning of its weight should have a major impact to the resulting model.<BRK>The approach is based on a base network and a category dependent constant. The paper is well written and the ideas are explained clearly. Overall, I like the simplicity of the approach but I believe with the current state of the paper, it’s hard to judge the value of the approach over other methods for learning with noisy labels.<BRK>This framework can automatically capture the effect of incorrect labels and mitigate it without removing mislabeled samples. There is some improvement in performance, but not much difference. Section 3 for the proposed method AutoCleansing. The authors may want to consider adding more descriptive text (and possibly figures) that would make the paper more accessible to readers without and extensive mathematics background. The authors should be explicit about the difference between the model proposed here and the models implemented by previous studies and how their model works compared to other methods.<BRK>This paper seems to be a useful contribution to the literature on deep learning with noisy datasets, showing a good improvement over the state of the art. This work presents a theoretical model formulation to capture the biased effects of incorrect labels automatically The paper is generally well written and structured clearly. Please add additional details. •	In addition to the learning rate, weight decay, Epoch etc.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>Post rebuttal:I acknowledge reading the rebuttal as well as other reviewers comments. Experiments: > Could the authors comment why FID is a good metric for semantic image synthesis? Justifying the choice of metrics would make the evaluation section stronger. The qualitative and quantitative results highlight good image quality of the proposed pipeline. I would expect that the lack of perceptual loss would lead also to increased diversity in synthesized images. Detailed review and comments:Abstract: > "semantic image synthesis GAN"   I m not convinced with this wording used in the paper. This model seems to have high diversity scores and it would be interesting to see model samples. e.g.why semantic image synthesis is of interest to ICLR community? The reduction of diversity is rather an effect of perceptual loss overall. Adding comparison to this work would make the paper even stronger.<BRK># Post Rebuttal:Thank you for the detailed rebuttal. Ultimately, I am keeping my score at 7, accept. The authors evaluate their architecture on 3 datasets: ADE20K, Cityscapes and COCO stuff. The technique seems to improve upon prior work qualitatively, quantitatively, and performance wise. 1.The paper does provide a very thorough comparison with prior works. One of the main claims of the paper, (that this technique does better without VGG), is not evaluated on COCO stuff.<BRK>They demonstrate substantial quantitative and qualitative performance over baselines and perform an ablation analysis. 2.The paper is well written and easy to understand. 3.Performance is impressive and experimental evaluation is thorough. The authors perform an ablation analysis. There is still a dependence on strong semantic constraints. However, the technical novelty is limited.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 6. <BRK>This paper is about representing functions $\psi : (\mathbb{R}^d)^n \rightarrow \mathbb{R}$ that are symmetric or asymmetric with respect to the permutation group $S_n$. The aim is to consider neural networks giving only functions that symmetric or asymmetric, and to establish universality results. The content here is that any asymmetric function is divisible by the Vandermonde determinant. Page 6, Theorem 3, theorem statement: Thus you have reduced the representation of an antisymmetric function in $n$ variables to that of $n$ symmetric functions in $n 1$ variables, at the expense of an $n \times n$ determinant.<BRK>In both cases, the authors emphasize the fact that the theorems deal with (i) vector inputs rather than scalar inputs, and that (ii) the approximation results are based on smooth polynomials rather than discontinuous functions as done in previous works. In its current form, The paper is not ready for publication. “Universal approximations of invariant maps by neural networks” 2018 that discusses symmetrization and approximation of symmetric functions (and function invariant to many other compact groups).<BRK>In this paper the authors study the representability of symmetric or antisymmetric functions using neural networks. In fact the author doesn t even talk of the main results for the first 5 pages and the main results are simply swept under the rag, and it s not clear to me what is the novelty of the results in that case.<BRK>Update:I have read the authors  responses and other comments. After all, no approximation rates and numerical results are provided. Summary:This paper studies the representation of symmetric and anti symmetric functions as well as the parameterization with neural networks. Pros:This paper is well organized, in particular the connection to the previous work.
Accept (Poster). rating score: 7. rating score: 6. rating score: 5. <BRK>This paper studies how to improve the worst case subgroup error in overparameterized models using two simple post hoc processing techniques: (1) learning a new linear classification layer of a network, or (2) learning new per group threshold on the logits. Pros:  The paper studies a timely topic generalization in overparameterized models  with some applications to fairness.<BRK>Specifically, the paper demonstrates that this result is not necessarily due to overparameterized learning poor representations for rare subgroups, but rather mis calibration in the classification layer that can be addressed with two simple correct techniques: thresholding and re training the classification layer. They show improvements over ERM in worst case subgroup error. The scope of the work seems largely limited to the set up from Sagawa a, b. Could the authors better explain why overparameterization reduces our expectations on the effectiveness of these post hoc procedures? Recommendation:  I recommend acceptance.<BRK>The paper shows that post hoc correctionsmay improve the worst subgroup scores similar to an earlierstate of the art system that modifies the learning objective. This topic is interesting. That being said, it feels the study/paper builds on a few earlierstudies heavily, and I am not fully convinced that there is enough newfindings in the present paper to warrant publication in ICLR.
Accept (Spotlight). rating score: 8. rating score: 7. rating score: 7. rating score: 6. <BRK>The proposed approach is, as far as I know, original and contributes to the state of the art. It then contributes an incremental algorithm for this problem. Overall, the ideas in the paper are presented in a very clear and elegant manner and the results strike me as technically sound.<BRK>I am wondering if this is a fair evaluation for the baselines, given that the policies are always evaluated on $w_t^{SMP}$, or whether a new set of tasks (a proper "test" set) sampled from B (the standard ball) should be used to fairly compare (8) with the baselines?<BRK>the authors never theoretically consider combining the policy, apart for stating that a good combination of policy should achieve higher performance than the best of the policy set. For clarity, I would recommend to either stick to the simplest setting of choosing the best policy given a reward function, or to consider policy election that take into account the way the policies are going to be used/combined. For all these reasons, I recommend to reject the submission.<BRK>What are some practical setting where this approach would be beneficial? 3   On the experimental section, I think there s a baseline that should be included that s missing.
Accept (Poster). rating score: 7. rating score: 7. rating score: 5. <BRK>The module consists of two main components: 1) Hamiltonian Engine (HE), and 2) Fingerprint Generator (FG). These ps and qs are fed into a discrete Hamiltonian system with dissipation, and produces (generalized) "conformations" of molecules. The FG also takes a molecular graph with atom and bond features + the generalized positions and momentums from 1) as inputs to generate the final vector embedding of the input molecule. This should be explicitly included in the paper. I understand that the module is quite complicated and not easy to depict it in one figure, but I ll appreciate if the authors can include all learnable parameters in Figure 1. In the first place, the HE used 32 dimensional vector for ps and qs.<BRK>It is based on two components: a "Hamiltonian Engine" that runs a brief simulation, predicting the structure of the small molecule by minimizing a learned potential energy, and 2) a message passing algorithm that uses the predicted structure as input. The reported experimental results demonstrate state of the art performance. **Recommendation**I recommend this paper be accepted.<BRK>This paper proposes to use 3d conformations for learning molecular fingerprints by 1) training a generative model to predict the 3d coordinates and 2) use those to train a "fingerprint generator" to obtain fingerprints by learning to predict molecular properties. The paper is well structured and clearly written. Update:The responses cleared up some aspects of the Hamiltonian engine and I adjusted my score accordingly. This would be interesting to analyze.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK>Pros:1.Interesting idea on trying to define and apply low pass filter on feature domain2. Moreover, the authors propose to learn the feature graph $L_2$ in the actual implementation. Compare to node domain where the graph topology is given by data, this asymmetric is very weird to me. I think the authors need to explain better on why the proposed design is necessary under their motivation. Doesn’t the weight matrix $W^{(l)}$ in each layer work as the feature transformation already? As a final comment, I think the experimental results seems interesting. The paper would be greatly improved if the concerns above can be addressed. Detailed comments:The main weakness of the paper would be its motivation. Also, even if we are given with some feature graph, why the low pass filtering defined with respect to this feature graph is reasonable? However, it is not clear that the same conclusion will hold for the newly defined feature graph. Consider the case were we only have $1$ node with $2$ features $Y   [y_1,y_2]$. In node domain, since we are given the graph topology directly from data, we are able to define “frequency” based on the spectrum of graph Laplacian.<BRK>This paper proposed a new graph convolutional network. It considers not only the original graph structure information but also the latent correlations between features, resulting in a graph neural network as a bi directional low pass filter. This technique seems to improve the model’s denoising performance. Cons:  The ADMM algorithm is applied to solve the optimization problem (4). However, the convergence of this algorithm with Taylor approximations is not provided in this paper. In fact, there are simpler algorithms with convergence guarantees that can be applied to solve the Sylvester equation (5), e.g., the gradient based iterative algorithm in "Gradient based iterative algorithms for solving a class of matrix equations" by Feng Ding and Tongwen Chen. However, in the new model, $L_2$ is a learnable symmetric matrix, which makes the new model more complex compared with other models. It is better to give the definition of the normalized Laplacian matrix for completeness.<BRK>Pros:  In addition to encouraging the node embeddings to be smooth over the graph space, the paper further regularize the embeddings to be smooth among different features. The paper is well organized and clearly written. Cons:  Compared with previous methods, the proposed method only achieves comparable accuracy performance on the real world datasets. The Gaussian noise is too weak to evaluate the robustness of the method. It is better to give an analysis of the computational cost of Equ.<BRK>The manuscript introduces a graph convolutional layer based on the optimal solution of the minimization problem of recovering the true graph signal given a noisy observation. The proposed solution is interesting and the paper is well written. Another point that it is not clear to me, is related to the fact that the authors highlight several times the connection between the proposed convolution and the low pass filter concept. For what concerns the experimental results it is interesting to notice that the BiGCN is significantly more robust than the models considered in the comparison it is not clear how the proposed model performs on clean data. Is there any difference in the data that justify this behavior? In my opinion, in the set of models considered in the comparison, the authors should also consider ARMA, which has the advantage to be more robust than the other models in the literature (as reported in section 2.2).
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 6. <BRK>This paper proposes an iterative PBD solver which uses a neural network to guessthe constraint force and position updates before polishing with a conjugategradient solver. The method utilizes a graph network architecture which makes itagnostic to the particular discretization allowing generalization (in theory)across scenarios. The choice of methodology makes it mildly ofinterest to the ICLR community. It is an application (without adaptation) ofgraph networks. The result is mildly positive (though not earth shattering),indicating success of graph networks to some degree. I have not seen graphnetworks applied to rod simulation or elasticity simulation, although they havebeen applied to meshes in geometry processing (similar and more challengingscenario) and neural networks have been applied more generally to speeding upelasticity simulations (with what would be a trivial extension to rods). However, this is a really straight forward idea and the gainsare again small. 3) the method shows a small number of smallexamples (low resolution rods with one or two rods in a scene). So the gains by taking an aggressive initial guessare tempered a bit, though still resulting in a (quite modest) overall speed up<1.5x. Or other momentum based strategies? Results of this method at test time? This video did not really help supplement this submission. For example, the paper mentions many times that it is *not* replacing the timeintegration/constraint projection with an end to end trained network forrobustness reasons. Let s see it fail then! It d be great to see a video where we also see an evolving plot per frame (likeFigure 6) showing the performance gains of applying this method rather thannaive initialization methods. Are only neighboring segments connected withthese edges? (e.g., the dual graph of the polyline representing the central axisof the rod). Figure 4 is very confusing. This is surprising to the point ofindicating a bug/overfitting. Then wouldn t its speed up be 1x? The paper does not accurately categorize past works when it writes "Existingmethods enable learning these systems often in an end to end manner and with afocus on replacing the entire integration procedure." It is potentially rude to continuous reviewers to have tohunt for small changes (and then see that their reviews were largely ignored).<BRK>The paper proposes an algorithm to accelerate simulations of deformable rods based on position based dynamics. Update:The paper has considerably improved:the title is now accurately describing the papercomplex scenes have been added (and the method works there too)comparison with knn has been added and it shows a clear improvement for the proposed methodI think the paper has improved, but I still find the comparison with direct solvers problematic. The title and abstract should be tuned down and made more specific. The key idea is novel: instead of replacing the entire time integrator with a neural network, which is typical of previous approaches, the authors propose to use the network to accelerate the constraint project step, and in particular to still rely on the standard projection used in PDB, but using the network to generate the initial guess for the nonlinear optimization. Overall, I am still mildly positive, but not willing to champion this paper given the many issues raised in the reviews. For small scenes like the one shown in this paper, a direct solver is fine, there is no need to use an iterative one. The abstract makes claims of generality, while the approach is very specific to rod simulation with a very specific solver. The comparison should be done with the state of the art, which is not CG. While I agree with the author s conclusions that “We discovered that applying GNs for replacing the initial guess has fundamental advantages over end to end approaches.” this is by itself not a surprising result.<BRK>Summary: the authors present a Graph Network (GN) architecture to speed up the running time of rod physical simulations by predicting the initial guesses to reduce the number of iterations of an iterative position based solver. Strengths: The paper is well written and motivated, methods are very clear. Therefore what one would like to see from an alternative approach like the proposed here, is a system that scales well with time. Figure 6 shows that s not the case here yet. 2) It would have been interesting to see how the GN compares to a different neural network predicting the initial guesses (say a simple LSTM or   more ambitiously   a transformer). I understand that many of the successful neural approaches for physics simulations are GN based, so the choice is well motivated   but those systems are generally trained end to end, which isn t the case in the work from this paper. This could be integrated in the end to end approach tested on the paper. All in all, despite its weaknesses, I still think the paper takes a meaningful step towards the efficient use of neural networks for physical simulations.<BRK>This paper proposes a graph network(GN) called COPINGNet (“COnstraint Projection INitial Guess Network”), which learns to compute a good initialization for the traditional PBD method. To simulate a physical system, PBD first computes updated locations of vertices then corrects the estimates of the initial position by constraint projection. The projection step is computationally expensive, and that is where the proposed COPINGNet is applied to generate a good initial guess for the built in linear system solver (e.g., CG). on the graph. According to Figure 4 and Figure 6,  the total speedup of the entire simulations is approximately 1.4 compared to vanilla CG solver. Can the initialization guess network accelerate other iterative solvers? Can this be explained? Generally speaking, this article provides a new idea. By combining graph neural networks with traditional methods, the speed of solving physical systems is improved. Due to some concerns about the speedup ratio and generalization, the practicability of this method needs further investigation.
Accept (Poster). rating score: 7. rating score: 6. rating score: 5. rating score: 5. <BRK># SummaryInspired by the effectiveness of hard negative mining in deep metric learning, this papers focuses on the problem of negative mining in unsupervised learning under the contrastive setting. Finally, within this family of conditional distributions, the paper proposes the Ring model, which takes inspiration from semi hard negative mining approaches, and that can be applied to state of the art contrastive algorithms in order to sample harder negatives, resulting in better representations. It shows the importance of selecting difficult negatives to obtain stronger unsupervised representations. Not less important is the proof that even though CNCE is a lower bound on NCE, it has however lower variance, leading to better local optima. The Ring model results in a simple method that can be applied to any contrastive algorithm resulting in a better representation that outperforms existing approaches by a significant margin.<BRK>This paper propose to sample effective hard negative samples in contrastive learning, conditioned on given anchor point. Experiments on several datasets as well as transferring datasets show some promising results. Pros:  Conditional negative sampling in contrastive learning is currently less studied, and this paper starts on this direction. The relative improvement of the new objective function upon multiple popular contrastive learning methods have been verified. The strategy in final realization is easy to implemented (though this realization is not very closely coherent to their theoretical analysis). E.g., how would MoCoRing works with MoCo v2 baseline with 800 epochs training of R 50 model on ImageNet? Specifically for MoCo which maintains a FIFO queue, I am not convinced that the improvement mainly comes from hard negatives. It might because it mainly comes from removing "too close" negatives which are false negatives. Overall, I have a mixed feeling over this paper, and would appreciate authors response.<BRK>Briefing:This is an interesting paper that discusses the negative sample mining in visual representation learning. Their theory shows that the NCE with negative examples sampling from a conditional distribution q is lower bounded with mutual information, and the object has higher bias and lower variance. #######################################################Pros:The topic of this paper is popular and interesting. The negative sample mining strategy in unsupervised representation learning is well discussed and found effective in recent research. The experiments show the effectiveness of their method compared with the original NCE methods. #######################################################Cons:This paper s writing quality is limited, and this makes some points that are not easy to understand. It seems the sample selection strategy is not well supported by the theory the authors claim.<BRK>This paper adopts semi hard negative mining, a sampling strategy widely used for metric learning, for contrastive self supervised learning. Specifically, the paper chooses the negative samples in the range of $[w_l, w_u]$ percentiles (close, but not too close) in terms of the normalized feature distance. has some logical/empirical supports? Empirically validate the proposed method improves the contrastive learning methods. For a single sample of $x_i$, it is easy to find the semi hard negative samples. However, how to construct the batch $\{x_i\}$ such that each sample is effective negatives for the other samples?
Reject. rating score: 5. rating score: 5. rating score: 7. rating score: 9. <BRK>The latter is an optimization problem whose optimality condition reveals constraints on the weight matrix W of the neural net. The authors provide details on solving such problem as well as numerical experiments that leads to similar results than competitors. The proposed algorithm does not significantly improves the accuracy of estimators (eg convNet in Cifar) when compared with actual methods. I did not find any such convergence proof in the paper. Should be nice if the authors can provide a pseudo code of the overall SIS strategy in a practical deep neural net (not only one layer).<BRK>The key idea is to make the outputs of the neurons in the sparse and dense networks at the same input close enough. They conduct a series of experiments to evaluate the performance of their proposed methods. 2.The experiment results show that the proposed method can achieve better performance than the baselines under this paper’s experimental setting. My concerns are:1. I understand that as reported by the authors in this paper, only in few experiments, they need to search for a good $\eta$. I mean that different layers could have different tolerances on accuracy. In other words, as it is challenging to find  near optimal $\eta$ for each layer, we could not reduce the network into very small size. For example, in RigL, the size of ResNet50 on ImageNet is compressed by more than 97%.<BRK>The discrepancy with some baseline numbers has been resolved and the authors added clarifying information to the paper regarding the counting of FLOPs. Cons:Some of the details of the authors experiments are not clear or potentially misleading:1. Some results presented for existing techniques are from those techniques’ original papers, but some results were re run by the authors. The RigL authors did not present results at 60% sparsity, and Appendix D does not include details on how this number was generated beyond the authors using the released code with RigL. They also use non uniform distributions of sparsity across the layers of the network which affects the number of FLOPs in the resultant network. I focused here on RigL because it appears to be the most commonly used baseline by the authors of this paper, but it seems likely that these observations apply to other techniques as well. Sparse training (i.e., sparse to sparse training) is known to be a more difficult problem than dense to sparse training [1] or post training sparsification. All results should be reported as accuracy with a given parameter count and accuracy with a given FLOP count. 2.Make it clear that some algorithms under comparison have additional capabilities compared to the proposed approach (e.g., RigL with sparse training). Many of those working in model compression who would be interested in this work will not be familiar with these topics.<BRK>## SummaryThe authors pose sparsification as a subdifferential inclusion problem, a novel formulation that results in quite meaningful results on established benchmarks/tasks. The paper overall is very well written with a detailed overview of current sparsification techniques and how the proposed method differs. ## Pros* Very comprehensive analysis and proofs (which seem correct, although not thoroughly verified)* Empirical results justify this novel approach across the board## SuggestionsThe computational characteristics of using SIS has not been characterized in the manuscript; it is no very clear what the complexity of training a large model is using the proposed approach. The authors suggest their training approach is efficient, but do not provide any empirical results or further justification. For example, all of the results in Table 3 and Table 4 can have an additional column that characterizes the time to train.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 5. <BRK>The paper proposed a new method, called AutoLabel, to assign labels to synthetic samples. The label assignment is calculated by the distance between the clean data and the augmented data, and updated by performing a label smoothing based on the calibration performance over a hold out validation data set. But the current form of the paper is weak due to the insufficient experimental studies and the minor performance improvement obtained with additional computational cost, compared to the baseline models.<BRK>Summary:This paper proposes to use label smoothing to determine the labels of augmented samples. The distance between the original data distribution and the augmented data distribution is used to determine the amount of smoothing in the smoothed label. Is this a contradiction to the main motivation of using a smoothed label for augmented data? For example, TRADES[2] substitutes the original label with the model s output on clean data when training with corrupted data. Hence the proposed method may not be very useful for adversarial training.<BRK>The motivation of the work is good. [Pros]  Several strong statements are not supported, and therefore, the level of novelty is hard to appreciate. Therefore, it is hard to appreciate the significance of the proposed method. Moreover, central observations are unsupported: > A major observation of the authors is: “Our key insight is that the confidence in the labels associated with the augmented data likely depends on how significant the transformation is.”  This is the definitional concept behind when generating pseudo labels ( examples). ICLR 2017. [Experimental Results]  There is a lack of strong discussion and findings that somehow weakens the paper.<BRK>This paper proposes a method to adaptively smooth the augmentation data labels based on the augmentation strength. The motivation is that the label preserving assumption may not hold for data with substantial augmentations. Pros1.Automatically tuning the labels for augmented data is interesting, as most previous works mainly focus on new augmentation methods. The paper does not discuss this critical factor and provides an ablation study on it. 5.According to the experiments, the accuracy improvements are small. My concerns lie in the technical novelty and lack of clarifications on the method design and experimental setup.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 6. <BRK>I would regard the experiments as incomplete. These discussions are lacking in the current paper. However, there lack enough discussions with existing multi layer GNNs.<BRK>The paper proposes a new definition of GNNs designed to cope with bi directional message passing processes. The paper proposes an interesting approach, but personally, I found the document a bit foggy in some parts. Code and datasets are not available.<BRK>b) more connections between its proposed architecture and the empirical experiment section (e.g.how the proposed theorem could explain the performance gain connections)There are also grammar mistakes in the paper which may hinder the understanding of the readers (e.g.last sentence in the abstract). Summary:This work proposes a new graph neural network architecture with modified rules for message passing, Iterated Graph Neural Network System (IGNNS).<BRK>This paper proposes a new framework of GNN which can deal with undirected and directed graphs in a unified way. The main idea looks reasonable and interesting.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. rating score: 6. <BRK>This paper presents a question answering dataset that needs up to reasoning over three distinct modalities — text, wikipedia tables, and images of wikipedia entities. The dataset comes in both an open setting as well as closed setting, in which distractors are chosen carefully (e.g.distractor images for other entities in the table or using a state of the art retriever to get paragraphs). The paper is very clearly written. The analysis were also helpful. However, ability to reason over multiple modalities is important and this dataset is important wrt that goal.<BRK>Additionally, the authors could have provided some analysis of their proposed model’s effectiveness on other datasets   does the model still work well for single modality QA datasets when compared to other pre existing models? ### DecisionI think this paper is marginally above the acceptance threshold. ### Positives  The dataset is interesting and comprehensive, covering multiple modalities with a significant portion covering compositional questions as well. ### Negatives  The paper could benefit from further comparison and exploration of how the methods and data relates to others in the literature.<BRK>This paper builds a new large scale QA dataset for multiple modality reasoning including table, text and images. The difference between the proposed dataset and this one is that HYBRIDQA does not require visual inference. I think almost all our cases of QA have not have the table in our daily life. This paper also introduces one model to deal with the problem of multiple modality reasoning defined by this new dataset. It seems that these models are not new, which are deeply related to several prior works.<BRK>The paper introduces MultiModalQA, a dataset that requires joint reasoning over table, text and images. The dataset would be a useful resource for multimodal QA advancement but the manuscript in the current form disregards all of the prior work that has happened in the space including the work on VQA [1], TextVQA [2] and PlotQA[3]. Have you done other ablations on 2 hop setting by using the TextQA module twice, TableQA module twice etc. Overall, this is an interesting and useful dataset. I would like to keep my rating as it is. I recommend authors to tone down the claims around being first MultimodalQA dataset and position themself properly with respect to previous related work if accepted.
Reject. rating score: 5. rating score: 6. rating score: 7. rating score: 7. <BRK>There’s two phases, and both phases are independent:1. Extract keywords, z,  using a BERT classifier/sequence tagger trained to predict keywords2. One can use automatic keywords using (1) and get uncontrolled generation, for which they present SOTA results on some summarization tasks. 3.Results provided across multiple summarization datasets. Interesting zero shot/transfer results from summarization to Question answering. Contribution in methods over Fan et al + BART (Lewis et al) is minimal. 2.Since the focus of the paper is on controlling generation, more results on this would be informative. It is unclear how well control works when not using oracle words or automatically extracted keywords, i.e.user controlled. An MTurk experiment evaluating how well control works would be useful in assessing this.<BRK>Paper Summary:* This paper proposes a framework for controllable summarization, CTRLsum. It is different from standard summarization models that CTRLsum uses a set of keywords extracted from the source text automatically or descriptive prompts to control the summary. Strengthes:* The authors investigated the effectiveness of the proposed model through extensive experiments. The control tokens "the main contributions of this paper are : ( 1 )" is far from the keywords used during training, and so I think that the keywords are not effective for contribution summarization. In fact, BART that uses prompt worked well for contribution summarization. * Can CTRLsum control the generation with multiple aspects (length and entity control, length and QA control, etc.) Update:Thank you for the answers to my questions and additional experiments.<BRK>The keywords and prompt can also be guessed automatically by a BERT base model, which seems to improve automatic metrics on CNN/daily mail. This paper studies a few different ways that the summaries can be controlled: through prompts, entities, or one sentence summaries of summaries (contribution and purpose summarization). These seem novel at least to this reviewer and could be helpful for future work. * The idea of using a two stage approach (with a BERT Base extractor to guess keywords to guide the summary) seems novel to this reviewer, and it seems to enable this approach to perform well even in an unconditional setting. * (minor) one possible reason why the two stage approach might perform better on unconditional summarization is because there are more parameters when ensembling BERT Base and BART. Update: thanks for the additional human evaluation results! These help and the results on excluding unimportant entities seem strong to this reviewer. Perhaps it might be more helpful for the annotators themselves to try to interact with the summarizer in some way, but that s a more minor point. Anyways, I bumped up my score from 5 >7.<BRK>The authors propose a straightforwardway of obtaining keywords from an article similar in spirit to Gerhmann et al.2018.Alternatively, "ground truth" keywords can be found using a referencesummary. Theuse of prompts to obtain question answering and more focusedcontributions/purpose summarization was also very interesting. In general, I would like to seemore work like this exploring methods of controlling pretrained languagemodels. There several areas where the paper could improve. The explanation of howlength control is achieved was not very clear. Comparisons are to the standard BART model or to Fan et al.(2018) whichsimilarly prepend important control information to the input. Additionally, the authors should say more about the differences in entitycontrol of their method and Fan et al.2018, which seem on their face to besimilar.
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 7. <BRK>The proposed method is evaluated on digit classification, letter classification, and check processing systems. The baselines compared are mostly methods that were originally developed for color images. However, the evaluation of the proposed method is flawed. The authors provide a clear explanation of how attacks on the binary images are different from those on color images. The authors provide theoretical analysis about the upper limit of pixels that need to be flipped in order to confuse the classifier. This is measured by D_x(x ). However, D_x is parameterized as L0 distance which is not convincing. The results in Section 6.5 do not have baseline algorithms to compare with. Also, it would be helpful if a reference to the check processing system is provided. This paper lacks a conclusion. Please address the comments in the cons.<BRK>A score based black box attack algorithm SCAR is designed to fool Tesseract and US banks’ commercial check processing systems. ##################################################################Pros:(1) This paper introduces a challenging task of attacking binary image classifiers. (3) The paper reads smooth and is mostly well written. ##################################################################Cons:(1)	The task of attacking binary image classifiers is a special case of L0 attacks, which also aim at perturbing a small number of pixels. The reviewer thinks that most L0 attack algorithms can be adapted to this binary attack task. (2)	The novelty of the proposed method is somewhat limited. The proposed SCAR greedily selects the modified pixels, by considering spatial and temporal correlations. However, this method is not new enough. Similar heuristic algorithms have also been developed in previous black box attack algorithms.<BRK>Summary: The paper describes adversarial attacks on binary (black/white) image classifiers, flipping a classifier result by changing just a few pixels. I believe it is worth opening a discussion on whether this raises ethical issues for publication. The SCAR algorithm is different, but it seems important to compare with this work. The existence of other sparse methods also calls into question whether this represents a fundamentally new advance. Putting them in a table would be helpful. If L_0 distance isn t the goal, why not?<BRK>This is an important question because of the application of such binary image classifications for check processing, invoice processing, and license plate registration. One also would think that such systems are less vulnerable to adversarial attacks given the simplicity of their inputs and the fact that most adversarial attacks are based on color or grey scale images. The authors also provided some general theories on the existence of binary image classifier provably robust to any attack that modifies large, bounded number of pixels. However, it is not clear to me how this is related to the rest of paper’s discussion. The connection is missing in the paper.
Accept (Poster). rating score: 9. rating score: 6. rating score: 5. rating score: 5. <BRK>#### Summary:The authors leveraged and repurposed Noise Conditioned Score Network (NCSN) that was originally introduced by Song & Ermon (2019) for generative modeling to be used for detection out of distribution (OOD) images. Are there other potential next steps that can be done on top of the proposed method? ############################################################################## Major comment: While the paper is overall very well written, structured and communicated, I found the final discussion and conclusion quite lacking. The paper addresses a relevant issue of OOD images detection using norms of score estimates and is highly relevant to the ICLR community.<BRK>How sensitive is the method to the choice of L and other hyperparameters? The paper addresses the problem of detecting out of distribution (OOD) samples at test time, i.e.samples which belong to a class for which there was no training data. Therefore, it would be helpful to include some experimental/theoretic analysis of why the method works, and when it does not work.<BRK>[Lee] and many works after does study this. Application on medical images is novel, could potentially benefit the ICLR audience if the dataset is released. Concerns:#1 Robustness of method (i.e.sensitivity to hyperparameters) MSMA introduces an auxiliary model, which introduces extra hyperparameters, e.g.number of components in GMMs. For the MRI task, the author  did not compare  to relevant  baselines. Lastly, the authors show no intention in open sourcing their code/dataset, which undermines the value of an empirical study. "Generative modeling by estimating gradients of the data distribution." They dismiss comparison to density methods by saying they cannot be used with their high resolution images. [Lee] Lee, Kimin, et al."A simple unified framework for detecting out of distribution samples and adversarial attacks." Providing a new and meaningful application of OOD detection such as the MRI dataset provided here is a good contribution, but it seems to me that the authors did not attempt to compare to other methods, but only tried to show MSMA somewhat works. However, the analysis is quite brief. #3 Incomplete understanding of the methodSection 2 tries to provide some intuition about the effectiveness of the method.<BRK>The proposed method is evaluated on two different settings and is effective for out of distribution detection. The writing of the paper needs further improvement. 2.The novelty of the method is marginal. Such application is trivial. The method doesn t compare with previous works when applying on brain scan images. 4.Important theoretically analysis is missing. The proposed method has several important hyper parameters: number of scales, sigma value for each scale, etc. Real data distribution could be very complex, in this case, how to select these parameters? Discussions about how to the effect of these parameters are missing. Update after rebuttal:I appreciate the efforts of providing a hyper parameter study. I would like to increase my rating from 4 to 5.
Reject. rating score: 3. rating score: 5. rating score: 8. <BRK>The approach is interesting due to applying updates to the hidden state of the past observation. The idea sounds interesting, however the lack of comparisons with other approaches and theoretical justification of why this approach is superior makes it hard to convince reader. However, paper will benefit from a better explanation of the method, simpler diagram and equations to remove uncertainty on implementation. Originality: I believe the idea is novel and interesting for community. For example image denosing task could be relevant (works like Noise2Noise, Noise2Self etc)<BRK>This paper introduces a propagation method to estimate RNN dynamic parameters during the learning process. Otherwise, if the focus of the paper is supposed to be only on noise robustness, I think the motivation in abstract and introduction needs to be clearer.<BRK>This is an interesting paper on an idea introduced by the authors as active tuning. However, it is not clear if steps were in place to make sure that no bias was introduced during this sample generation. I read the paper carefully multiple times, and feel that a few inclusions will help the readers better understand the proposed method. To demonstrate the effectiveness of active tuning the authors trained a distributed graph recurrent neural network (DISTANA) on three datasets with increasing complexity.
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 5. rating score: 5. <BRK>For example, the way the training copes with distribution shift or alleviate forgetting is not clear or elaborated on. Beyond the abstract and before the empirical validation no theory or justification is provided to substantiate this claim. The experiments look convincing. Writing and presentation are a good start point for improving the paper.<BRK>(4).The authors claim that the proposed training scheme is  good for non stationary, high dimensional streaming data, but there are not much analytic results with the dimenionality and non stationary streaming data. In fact, this paper proposes an annealing mechanism for the SGD algorithm and makes the experiments to compare the proposed training algorithm with sEM algorithm on several real world datasets. However, I have following major concerns: (1). The proposed training scheme does not require data driven parameter initialization (e.g., k means) , but the data driven parameter initialization can improve the efficiency.<BRK>The paper proposes a new approach to train GMMs using SGD under a variety of settings (streaming, concept drift, etc) addressing the issues of catastrophic forgetting, problem of parameter initialization and numerical instability. However I feel that paper is not presented with sufficient clarity to pass the ICLR bar and the exposition could be greatly improved. It is hard for me to grasp the key findings or takeaways in the paper wrt to other existing baseline methods. One of my main questions here is: how did the authors measure the value addition of annealing? Did they compare the final solutions obtained by the proposed approach with a baseline (without any annealing) that uses random initialization to deal with local optima?<BRK>This paper presented a stochastic gradient descent approach to learn a non stationary high dimensional Gaussian mixture model from online data. The proposed approach is demonstrated with several vision/non vision tasks. Overall, I feel that the paper is slightly below the borderline. Pros:+ Interesting combination of new research trends (continual learning) and old models (GMM). Cons:  Lack of an approach to identify the number of mixture components  Lack of theoretical justification about the max component approximation and soft max component approximation.<BRK>The authors propose a technique to training GMM using SGD instead of (s)EM. However, section 4.1 has shown that with or without random initialization does not impact the perf too much, then why not just use k means initialization? I believe k means initialization is also a randomized process. A second question is: is there any theory or bounds to support the convergence assumption? random?if fed data of one sample, is the SGD stilling running for 2 epochs? or halting and wait?
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. rating score: 5. <BRK>The authors experiment with the GPT 3 models of various sizes and UnifiedQA, and the results suggest that the size of the model may be one of the important factors in achieving higher performance on all the tasks. This is an interesting work that tries to tackle a very important problem of how successful are large language models across multiple tasks. It would help the paper to have a more thorough discussion of the results.<BRK>3.Can you elaborate on the results from table 1 which shows that an 11B UnifiedQA model outperforms 175B GPT 3 model? The main claims of this paper are to demonstrate these large scale models still struggle to use the knowledge it has learned during the pretraining phase and these models struggle to on calculation intensive tasks. 1.This study shows how far off these language models are when compared to how humans use knowledge and commonsense reasoning to solve tasks. After reading the rebuttal  I thank the authors for providing further information and answering the question raised by the reviewers.<BRK>Performance of these models is well below their performance on other benchmarks: not above chance for the smaller GPT 3 models, and under 50% average accuracy for the best models. The paper has a lot of good features: it’s obviously great to have a broad, challenging, large scale dataset that aims to separate human from model performance. Is the plan to release this dataset publicly? The calibration results for GPT 3 are very interesting.<BRK>Then, the paper provides results of experiments with the latest (GPT 3 and T5 based) models along with some quantitative and qualitative observations. Most of the tasks were taken from different human examination sets. Despite the fact that the UnifiedQA model is superior in quality to GPT 3, most of the results are devoted to the GPT 3 model. + The authors show that in the case of GPT 3, as with earlier similar models, the model s confidence is not a good estimate of the actual probability of the prediction is correct.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 8. rating score: 8. <BRK>The authors  claim that they are not scalable leads the reviewer to anticipate that LToS naturally is scalable, but there appears to be no evidence whatsoever presented in the latter sections of the paper to show, let alone prove, the superior scalability of LToS, with, for example, growing numbers of agents and training times. What if the network were sparser? + Niche is well identified, and the contribution is clear.<BRK>Indeed, some of the points I raised were addressed well and the paper updated accordingly. There is no way to know if any of the results are going to hold up. This is not clear to me. I also agree with the concern raised by other reviewers that the paper is currently not positioned clearly. The "traffic" and "ROUTING" experiments seem more interesting. Also, centralized baselines are missing. Please work on the clarity of the writing. "However, they are learned in a centralized way and hence not scalable." I don t follow this.<BRK>The paper aims at optimizing a global objective and assumes (also in the propositions) that this objective has additive connection with the decentralized rewards. If the method is designed for continuous action space, it is expected to have the notations to be continuous as well. The authors claim that the problem with the related work is that they can not scale up with the number of agents. However, I have some comments/questions for improving the paper that are summarized below.<BRK>SummaryThe paper considers the cooperative MARL setting where agents get local rewards and they are interconnected as a graph where neighbors can communicate. The paper’s flow motivates such a framework well. It seems that the proposed method does not support multi hop sharing because rewards can only be shared to neighbors. Why is this single hop sharing effective in the experiments? Could the authors explain it with more details? Specifically, does “phi can be learned in a decentralized manner” mean that the *optimal* phi can be based on only the local observation for each agent, instead of based on global state?<BRK>The paper present a new method, called LToS which enables agents to share rewards in MARL. In the second scenario, authors also show the need for high level policy by introduction fixed LToS. At the end of Introduction, the sentence ‘LToS is easy to implement and currently realized by DDPG…’ can be misleading because of the word ‘realized’ and the fact that authors argue that LToS is a newly proposed method. Does this mean LToS simply combines DDPG and DGN?
Reject. rating score: 2. rating score: 4. rating score: 4. rating score: 5. <BRK>### Summary The paper claims to exhibit a connection between differential privacy and neural network pruning. This is proved by bounding the sensitivity of the network and adding Laplace noise so that the resulting function satisfies differential privacy and approximates the function defined by the pruned network. According to the authors, these results mean that “ network pruning can be an effective method to achieve differential privacy”### EvaluationThere are multiple serious conceptual issues with this work. For example, if $x$ is an image, are we preserving “pixel privacy”?<BRK>Overview: This paper aims to establish a theoretical connection between differential privacy and magnitude based pruning. The authors instead use 4.2 to draw similarities between adding noise and applying pruning, which I think is not well supported by their findings. This would be analogous to protecting the value of one pixel in a database of a single image, which is a rather useless notion of privacy. Yet, I am unconvinced about the relation between pruning and differential privacy claimed by this paper due to the concerns that listed below.<BRK>My summary of the papers main goal:The paper aims at drawing a connection between neural network pruning, and privacy of the trained model. More specifically, the authors try to draw similarities between differentially private training of DNNs and pruning. Visualizing two sets of examples does not really prove anything, especially not DP. This cannot be asserted or evaluated with two image reconstructions. Maybe there is one example that is actually completely reconstructable.<BRK>This paper seeks to protect privacy of the dataset involved in learning process. Specifically, it tries to establish some connection between privacy obtained from pruning neural networks and differential privacy. They seem to overestimate the power of their results. The connection is an L2 norm connection with the output of a possible DP algorithm. That is just not true. It drops by 30%. Or any intuition to why they would generalise? I feel that the negatives outweigh the positives here.
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 5. <BRK>The main idea behind the approach is to use unsupervised approaches such as GANs to model the natural variation. 2.There is a bit of a lack of relevant methods to compare against, but the experiments still show that the proposed approach does well. This paper is essentially defining a new adversary model, given by natural variations. is quite straightforward for unsupervised models such as GANs, this is not too much to ask for. Since there is a lack of baselines to compare against, the paper could benefit from a few ablation studies. 3.It would be nice to have representative images of the models of variation, perhaps in an appendix. Updates after author response I thank the authors for the response and it helps clarify some points.<BRK>Strength:+ The paper addresses a very important topic of adversarial robustness of DNN models and is accompanied by diligent evaluation over different datasets. The key difference seems to be adoption of the idea of using auxiliary transformations (called natural perturbations) which is also very well studied in literature, for e.g.see https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Auxiliary_Training_Towards_Accurate_and_Robust_Models_CVPR_2020_paper.pdf  So, the approach presented here is quite incremental for a premier venue such as ICLR. If the defense approach uses some background/auxiliary knowledge, one must consider the attacker with this knowledge if it is accessible to the attacker. Questions to authors:  Can authors explain why an attacker can t build similar natural model to defeat the proposed defense? al.(and references therein) would be also useful. After author s rebuttal:"The other paper that the review points to ([4]) addresses a similar setting to our paper, but the approach is completely different.<BRK>The paper compares baselines to different versions of this general model based framework with experiments on several datasets. Typo: "ImageNet c" should be "ImageNet C"___________________________________________________________________________________Update after author feedback:I thank the authors for improving my understanding of the paper. Using this phenomenon to improve robustness is a good idea, and the MRT/MDA/MAT methods explored in this work are nice choices for this investigation. However, I agree with the other reviewers that it would be nice to include more baselines from other work where appropriate. Overall, I think this paper has some strong experiments and investigates a good idea, but the claims of a "paradigm shift" are overly grandiose, and some of the most interesting experiments could use more analysis.<BRK>However, I wonder if this model preserves accuracy on the clean data. * The method is presented in clear pseudo code, and detailed in the 36 page counting appendix. We know from existing literature [1] that a trade off exists for adversarial training between accuracy on perturbed and clean data. # Minor feedbackThis minor feedback is not part of the assessment. However, the comparison is unbalanced, as ERM does not benefit from the prior knowledge. If Table 2 would be about comparing methods under the assumption that the test time perturbation would be known at training time, then the ERM should be trained with the same data augmentation. * The paper misses comparisons against previous published work. Although Table 2 and 4 compare against another method, these results were not obtained by the respective authors, thus introducing a bias. If the proposed method requires significantly more compute, the comparison should be made with a method that has access to the same amount of compute (for example a larger model, or longer trained model).
Reject. rating score: 3. rating score: 3. rating score: 4. rating score: 4. <BRK>This paper describes zero shot learning for image classification. I appreciate that the authors have revised their responses. My final evaluation about the paper is on the negative side in that its technical novelty is moderate and the experimental results are not convincing. After author response: My main concerns about this paper are technical novelty and weak experimental results.<BRK>*Summary*The authors tackle the problem of zero shot learning, that is, the recognition of classes and categories for which no visual data are available, but only semantic embedding, providing a description of the classes in terms of auxiliary textual descriptions.<BRK>This paper proposes a visual semantic embedding model useful for generalized zero shot learning. Post Rebuttal Evaluation [FINAL] I would like to thank the authors for their response and for updating their paper based on the reviewers feedback. The paper is well constructed and easy to read.<BRK> Summary: This paper proposes a simple yet effective method for zero shot learning. In the method, a network is learned to predict the compatibility function weight given the input of the image. Hopefully the authors can address my concerns in the rebuttal period. Cons:1.The paper fails to validate its motivation. Based on the points above, I find the paper less significant and lack contributions, thus giving my score.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>Summary:  Proposes new method to improve transformation invariance in contrastive representation learning and demonstrates utility on downstream tasks  Proposes using feature averaging from multiple transformations at test time leading to further improvements   Introduces Spirograph dataset to explore the importance of learning feature invariances in the context of contrastive learningClarity  Paper very well written and easy to follow. Novelty/Significance  New contrastive objective with gradient regularizer term to encourage transformation invariance and the Spirograph dataset are well motivated   Results over contrastive baselines suggest the contributions are important improvements to the contrastive training recipe Questions/Comments/Clarifications  “Unfortunately, directly changing the similarity measure hampers the algorithm.”   Please add a citation to validate this claim. Should t not be β? “It may be beneficial, however, to aggregate information from differently transformed versions of inputs to enforce invariance more directly”  > it is unclear why invariances need to enforced at test time if the learned representation is already invariant  The authors point this out but the gradient regularization term is unfortunately encouraging invariance only to differentiable  transforms and this is a key limitation  Worth pointing out for certain tasks likely near OOD detection, you may want to be transformation covariant rather than invariant. Would be interesting to see results of the proposed method on OOD detection benchmarks following https://arxiv.org/abs/2007.05566  One major limitation is lack of baselines beyond vanilla contrastive training. For example, it would have been good to compare test time feature averaging with test time augmentation ensembling. Similarly instead of gradient regularization, the model could directly predict augmentation parameters and have the gradients of that loss penalized/ reversed. Adding such additional baselines, would solidify the improvements as best in class. It is also unclear how much train/test time compute model adds. Overall, this is a very nicely written paper and very solid contribution. If the authors address my concerns, would be happy to increase my score.<BRK>The paper focuses on a subtle different yet seemingly novel approach to learn invariance, i.e.instead of forcing the representations of two transformed inputs to be similar, it minimizes the gradient of the representation w.r.t to the transformation. This is indeed closer if not exactly the definition of tolerance or invariance to transformations is. The paper uses trick to to use a different representation of the conditional variance and first order approximation to arrive at a more computationally viable formulation. The paper does provide some relations to adversarial robustness as well, indeed it is quite intuitive to find connections between robustness and invariance. The paper also provides validation experiments to show that indeed the conditional variance is reduced using this regularization. This is an important property in practice when there might be multiple transformations available that one needs to optimally utilize. This is one solution for such a case.<BRK>1.SummaryGiven one image, the paper first generates different views which are controlled by differentiable parameter \alpha, and then minimizes the additional "conditional variance" term~(expectation of these views  squared differences). A testing strategy is further proposed by voting features with different augmentations. * Results in Cifar and the new proposed dataset validate the effectiveness of the methods. The claimed contribution of "feature averaging" during testing looks more like a testing time augmentation for me, although the paper has some differences in the augmentation details. I also think that the claim of SOTA is not rigorous if it has benefited from testing strategies. * It seems the proposed methods can be plugged into many recent studies. After reading the authors’ feedback and comments from other reviewers, I raised my score from 5 to 6. I agree the feature averaging is another testing strategy.
Reject. rating score: 3. rating score: 4. rating score: 5. <BRK>Summary: The authors propose a new framing of the matrix local low rank representation technique, identifying 3 different subproblems and claiming most existing approaches sonly solve some, then proceed to propose a novel method that solves all three, evaluating on synthetic data. Review:* The paper is confused and hard to follow. what is $K_\Omega^h$?). The proposed algorithm is complex and made of several different components that are not appropriately justified. The experimental setup is not described appropriately(e.g.you mention a feed forward neural network in section 3.4 but don t report any parameter such as number of layers or size). * The evaluation is limited and not very supportive of the author claims, especially for real data more experiments would be necessary. Summary: While the idea could potentially be promising, a more clear explanation and more attentive evaluation are fundamental to assess the methods applicability and value.<BRK>1  The section of mathematical formulation does not do a good job of providing a clear definition of the target problem. 2  The presented algorithm is composed of many steps and the paper does not provide sufficient study/analysis to guarantee their performance. The paper also lacks a theoretical study of the proposed method. 3  The experiments on real data are not convincing and I suggest the author to include further experiments with real data.<BRK>They three sub problems, namely, LLR 1C, LLR 1, and LLR k. Based on extensive simulations, the authors claim that the proposed method is more general and outperforms previous methods on this problem. 1.As the authors analyzed, this problem is tackling topically coherent sub matrix discovery in a given matrix. To tackle this problem, the proposed method is composed of 6 steps described in Figure 2. Some of these steps, however, are not fully justified to be present. This is also not clear in this manuscript. 1 4) It is not clear how the matrix is sliced in the first step. In anchor based methods, for example, a kernel is used to gather similar rows and columns. How do we recover X from these? This is not clearly described in the paper. 2.The proposed algorithm is quite arbitrary, especially with the local low rank filters. Previous anchor based methods like LLORMA learn those filters (called kernels there) from the data. Effectiveness of these filters should be different from dataset to dataset, so it is hard to justify generalizability. 3.Experiments can be significantly improved in many aspects. We expect reproducible details on these datasets for a successful ICLR submission. The authors used a machine with 16GB ram, which even does not fit for a large scale dataset to be loaded. These are far larger than the scale used in this paper, such as 4000 * 2000. The authors are encouraged to play with more hyperparameters on a machine with higher capacity. As LLORMA already has shown it is scalable to at least a few times larger matrices than what the authors tried, so the authors should be able to reproduce it. 3 4) The JMLR version of LLORMA paper describes some primitive anchor point selection methods. (I am not sure if they are good enough to be a strong baseline though; those methods are really primitive, something like using k means clustering.) E.g., what is E, u_k,  in Eq.(2.1)?Overall, this paper is tackling an important problem and we encourage the authors to keep working on this, but the current manuscript is not fully ready to be published in ICLR this year.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 6. <BRK>A study about tuning the local and global learning rate is also presented. Pros:(1) The paper is well written and easier to understand for most parts. It would be helpful to provide a discussion and comparison of the two strategies and their advantages/disadvantages on the two issues. Experimental comparison to FedAvg, FedAvgM, and SCAFFOLD on seven benchmark FL tasks show that the proposed three adaptive FL methods are better or comparable on early stage convergence and final validation set performance.<BRK>This paper proposes several federated variants of adaptive stochastic gradient methods. Moreover,  the convergence rates of the proposed algorithms are also provided. The marginal value of adaptive gradient methods in machine learning. 2.The second question is about the generalization ability of federated adaptive stochastic gradient methods.<BRK>Summary: This paper presents an adaptive federated optimization framework that induces three different adaptive federated learning algorithms, which are proposed to address the issues of client drift due to data heterogeneity and lack of adaptivity. ***********************************After carefully considering the rebuttal from the authors, I think I am more positive about the paper so I raised my score. The authors analyzed mathematically the convergence rates of the proposed framework and showed extensive experimental results on different benchmark datasets to validate the efficacy of the developed algorithms. The authors need to give more discussion.<BRK>This paper studies the convergence of well known adaptive methods, ADAM, ADAGRAD, and YOGI, for the federated learning problem. Post rebuttal comment: I appreciate the authors  responses, especially on highlighting the theoretical challenges. My main questions are regarding the theoretical analysis of the paper.
Reject. rating score: 3. rating score: 5. rating score: 6. rating score: 7. <BRK>The paper discuss how to detect erroneous steps in gradient descent on a non trusted GPU using a separate slower trusted execution environment,by randomly deciding in each step whether to check the values returned by the GPU, as well as using small learning rates and clipping the gradients to ensure all updates are small. In my opinion this paper is an implementation of a straight forward idea and the theorems for setting the probability parameters  are basic probably computations. I have questions about the model.<BRK>It would add to the novelty and merit if there were some evaluations too for this. While F.3 seems to demonstrate this on CIFAR10, this cannot fully verify that such gradient clipping wont hurt the accuracy of the models.<BRK>GINN combines random verification and gradient clipping to achieve good performance. 2.The paper is well structured, and the assumptions are clear. 3.The random verification strategy is based on the observation that it is unnecessary to verify all of the computation steps.<BRK>First, they clip the gradients during training to force the attacker to insert multiple deviations to influence the model. They then randomly verify the integrity a subset of the gradient updates to check for tampering.
Reject. rating score: 5. rating score: 6. rating score: 7. <BRK>This paper formulates the learning of three tasks, including graph classification, node classification and link prediction, as a multi task learning problem and adopts a meta learning approach to learn the three tasks together in the spirit of the Model Agnostic Meta Learning (MAML) method. The proposed meta learning approach seems a direct application of the MAML method. I cannot see much difference with the MAML method. In the meta objective, how to set different \lambda’s? This is more important to the performance.<BRK>The manuscript proposes SAME, a model based on GNN and meta learning for learning multi task node embeddings. Two model variants iSAME and eSAME are proposed base on different settings in inner/outer loop of parameter update. Despite the new problem and different task settings, the model framework adopts the similar procedure as general meta learning procedure. I would like to see more discussion about the contribution and novelty of this work as well as the potential future study. 2.This work follows the meta learning setting. 3.Reference format is not consistent, typos, etc.<BRK>This paper presents a multi task framework to represent the node embedding for transferred knowledge. The methodology is based on the meta learning, which is capable of producing multi task node embedding. This paper is well motivated and well written. The experimental results illustrate the effectiveness of the model. I would like to recommend to accept this paper. The related work can be strengthened. 2.Sec 3 seems redundant to most related readers.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. rating score: 5. <BRK>Summary and Contributions: Inspired by the mixture of experts, authors propose an image clustering algorithm using a mixture of contrastive experts where, each of the conditional models is an expert in discriminating a subset of instances based on contrastive learning. Additional Feedback and Suggestions:  Since the goal of the paper is image clustering, providing some visual results is appreciated. Correctness and Clarity: The paper is well written, with informative figures and tables. Thanks also for sharing the code.<BRK>Summary: Authors present “mixture of experts” type of method to solve a clustering with unsupervised learning problem. Recommendation: I am tending towards accepting the paper (rating 6). Reason for the acceptance is the novel method supported by empirical evidence. Strengths: 1) Authors address an image clustering algorithm given number of clusters. 2) The proposed approach is well motivated. Weakness/Questions:1) All recent papers have ImageNet 10 as one of the five common datasets [1 2]. Why was it omitted in the current paper? 4) Almost all prior methods and proposed method MiCE assume that a number of clusters are known (which shouldn’t ideally be the case). Don’t they have to be in the range of [0,1]?<BRK>The paper presents an image clustering methodology based on Mixture of Experts (MoE) for image clustering. Although MoE has been proposed for supervised learning problems, the authors exploit the instance discrimination framwork to apply the MoE idea for image clustering. This is a novel aspect of the proposed method. The MoCo framework (unsupervised) for contrastive learning of image representations is employed to define a mixture of MoCo experts model where each expert additionally includes a cluster prototype vector to facilitate clustering. Comments to be addressed:1) It would be easier to understand the contribution of the paper, if the MoCo approach were initially described and then the proposed method was presented as a mixture of MoCo experts. 2) In section 3 that describes the method, there is no reference about image augmentation, although it is a critical aspect of the approach.<BRK>The paper proposes to use mixture of experts for image clustering. The proposed method has shown superior clustering performance compared to an extensive number of clustering methods on a reasonable collection of data sets. Though the overall idea of the proposed method is clear, the paper does not seem to explain some technical details clear enough. Overall, the proposed method appears to be an interesting combination of some existing methods.
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 7. <BRK>The authors propose a new version of the regularized autoencoder where they explicitly regularizes its decoder to be locally isometric and its encoder to be the decoder s pseudo inverse. Regarding the motivation and the math, I like the idea of isometric regularizer preserving the geometric properties in the learned manifold. The math formulation primarily sticks with a linear version of the autoencoder. It would be great to get some insights for a non linear counterpart. Regarding the experiments, indeed the authors successfully show the IAE converges its decoder to be an isometry and the proposed regularizer promotes more favoured manifold. However, the experiments mainly rely on visualization but fail to give some numeric results. How can we practically make use of the isometry property in applications other than data visualization?<BRK>This paper provided a novel method to train a local isometric autoencoder, which can preserve the local Euclidean distances well between the original space and the latent space. 2.The theories are well presented and explained pretty well. Also, Isometry is a very important property in several cases including manifold learning, etc. The authors do mention that "a local isometry which is also a diffeomorphism is a global isometry" on page 3 bottom paragraph. However, there s no discussion about the "diffeomorphism" in the following sections. Due to the fact that the distance is computed only based on the triangular meshes between edges. This would strongly support the global argument. 4.The experiment of the data visualization is somewhat weak. The benefit of the Isometry Autoencoder is not well addressed. The t SNE is well used for visualization with almost nothing wrong. The only benefit comes when arguing the "even" sampling. Some other minor comments: 1. This would be more clear than just mentioning differential.<BRK>The paper suggests a novel auto encoder based method for manifold learning, by encouraging the decoder to be an isometry and the encoder to locally be a pseudo inverse of the decoder. It is noted that for a linear architecture, this gives PCA, therefore, this can be seen as a nonlinear PCA approach. This loss function is claimed to be the main technical novelty of the paper. In the experimental part, the authors compare the merits of this approach on synthetically generated low dimensional manifolds in high dimensional ambient spaces, against other standard manifold learning algorithms, and show that the paper s method outperforms other method using a measure of distortion of triangle edges on a grid. They also experiment with "real data" (e.g.MNIST), show the merits of the proposed algorithm when visualizing the 2 dimensional bottleneck of the autoencoder. The overall idea and theory seem interesting. The experiments are a bit disappointing. Maybe I am missing something, but would it be impossible to generate, say, a 50 dimensional manifold in 100 dimensions? The editorial level of the paper is not very high, due to grammatical English mistakes.<BRK>Original Review:This paper describes a new type of regularization for the parameters of an autoencoder   one that forces the decoder to be an isometry. The authors present conditions that need to be satisfied by the encoder and decoder parameters, and show empirically that the regularization terms that they propose ensure that the resulting autoencoder has an isometric decoder. The paper is well written and easy to follow. Distances between points on a data manifold are not usually measured through L2 distances in a latent dimension, and it is not clear why one should require that L2 distances in the high dimensional space are the same as distances in the latent space. The numerical results on reconstruction error that the authors present in the appendix do not indicate any reason to prefer isometric AEs over other baselines that are considered. In case there is a setting where isometric AEs can be shown to model the data manifold better than regular AEs, that is not highlighted in the current draft. The authors claim that isometric autoencoders would "evenly sample the manifold" which is a little confusing, since the sampling of the data manifold is separate from the technique used to model the data (regular AEs vs isometric AEs). The experimental results also do not indicate how the embeddings learned using the proposed method perform on downstream classification tasks, for instance.
Reject. rating score: 4. rating score: 6. rating score: 6. <BRK>The paper proposed a learned variant of the well known iterative Hessian sketch (IHS) method of Pilanci and Wainwright, for efficiently solving least squares regression. While getting a learned variant for IHS is an interesting direction, the current theoretical contribution of this paper is only incremental, and most importantly, the reviewer is unconvinced for the practicality of the current approach. The iteration count does not reflect such overhead, and the reviewer believe that the wall clock time comparision would be more sensible. In order to truly demonstrate the benefits of learned IHS (which seems to be robust to small sketch sizes), the authors should choose for each algorithm the best sketch size and then compare them in run time, at least between learned IHS and Count sketch IHS. The authors have avoided such type of analysis and the main results are "safe guarded" by the concentration of the random sketch, which are easy to derive. Overall, the idea of combining IHS with learned sparse sketch is interesting, but the reviewer believes that the current version needs significant amount of rework to be publishable in top conferences.<BRK>The present paper considers a particular kind of sketch for which the sketch matrix is learned from data. I think the idea of using learned sketches is interesting, and it seems like it has not been applied to the problems considered in this paper before. Is it the case that the $\min$ may be large with some small probability? This is different from the $m   O(d^2)$ required for CountSketch to be a subspace embedding. Good balance of algorithms, theory and experiments. It seems like learned sketching has not been used for Hessian sketching and regression before. In Sections 5 and 6, these quantities are used to represent the optimal solution to the unsketched problems, but here they seem to represent the solution for the sketched problem using one of the random sketches. 3, and the upper bound on the nuclear norm in Section 5.3. In particular, my main concerns with the theoretical results and proofs have been fixed.<BRK>Summary:Previous work has shown that sketching the Hessian can improve the running time of second order optimization methods. However, these prior sketching techniques were data oblivious. This paper leverages learned sketches to improve the convergence rate of second order optimization methods. Weaknesses:+ The technical novelty of the paper seems incremental. The learned sketch algorithm from (Liu et al., 2020) serves as a replacement for the data oblivious sketch in these second order optimization methods. However, this question was not thoroughly investigated in the paper. In the experiments, the sketching matrix is learned in each iteration. On learned sketches for randomized numerical linear algebra.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 6. <BRK>This paper proposes a programming language, RASP, as a computational model for transformer encoders, and discusses how analysis in terms of this language could be used to understand the behavior of transformer models. Results such as this would be of high value to the ICLR community. However, in order to justify using the RASP language to reason about transformers, I think it is necessary to demonstrate experimentally that insights from RASP can translate to new empirical findings. The idea of finding a computational model for transformers is interesting, and (as discussed in section 4) could lead to insights in terms of how to build better models.<BRK>The authors introduce a DSL, the Restricted Access Sequence Processing (RASP) language, that they claim can serve as a computational model for the transformer encoder. The ideas in the paper open up many new questions and directions. There is essentially no discussion of related work. The main weakness of the current version is that it only glosses over the connection between RASP and the transformer encoder. Are there RASP programs that cannot be realized by transformers, or vice versa? I would be particularly interested to see how sharp these curves are.<BRK>This paper proposes a restricted programming language containing rough analogues of operations used in transformers. Using this language, the authors show how some algorithms can be implemented, which gives some insights about the limitations of transformers. Overall, the paper is badly written, with many typos and many hard to understand areas. Some things I would have expected an analysis on:* How well do the individual RASP operations map to the relevant transformer operations? * What are "useful" algorithms that cannot be represented in RASP? * Sec2: "the base sequences": unbalanced parenthesis* In the discussion of `aggregate` a selector `s` is an input, but `s` is never used.<BRK>This paper proposes a computational model for the transformer in the form of a sequence processing programming language named Restricted Access Sequence Processing Language (RASP). The paper shows how RASP can be used to program solutions to tasks that could conceivably be learned by a transformer. 2.It is not clear whether there exists other forms of programming language that can also "explain" how transformer works, and if so, how the presented one (RASP) is a better abstraction of the transformer model. 3.Although the presented RASP language can help analyze the number of layers and heads required theoretically, there is limited value and insights for improving existing transformers models.
Reject. rating score: 3. rating score: 5. rating score: 5. rating score: 6. <BRK>Summary: The paper proposes a technique for assessing the uncertainty of a Transformer based NMT model on a given input $x$. The technique relies on computing a variance like estimate over a collection of translation candidates for $x$, where these candidates are obtained by perturbing the decoding mechanism through the use of dropout at test time. Experiments compare this technique with other ways of measuring the "epistemic uncertainty" of the NMT model. Doing that in a way that only relies on intrinsic properties of the NMT model without requiring a specific quality estimation training setup, which additionally might have difficulty covering all of the possible out of domain cases. The clever way in which, in equation (9), a proxy for variance over symbolic target sequences (which is not well defined) is obtained in terms of the BLEU distances between *pairs* of target sequences. Two issues here: (1) no details at all are provided concerning the training of Transformer with the MC Dropout technique; (2) it is also not clear how fundamental for the approach it actually is to train according to this technique (presumably according to some variational principle ?), as opposed to randomly perturbing the weights of the model at test time in any simpler way   which would also produce a sample of outputs on which the proposed BLEUVar technique could be applied. Related to this point, BLEUVar could also be applied to simpler techniques for producing the target samples than sampling the parameter space, namely simply sampling the output space (hierarchical sampling over the NMT outputs, for the standard fixed parameter value). This would appear to be a rather obvious and relevant baseline. In other words: what is the relative importance in practice of a Bayesian approach to producing different outputs   as compared to the effect of using BLEUVar to measure the variance of candidate outputs ? If this is the case, it looks counter intuitive, because detecting that $x$ is out of domain would seem to be very correlated to the structure of $X$, and not only on the structure of $Y$ given $X$ ? The experiments do show some superiority of your technique BLEUVar over the non bayesian BS and the bayesian SP uncertainty measure, in terms of their "correlation" with BLEU, but BLEUVar use BLEU internally, while the other two measures don t, so could this explain its superiority? Overall, I did not find the experiments strong. I do understand that your point is of course not to claim that your technique can be used as a practical language guesser, but to show that your technique is able to compute high uncertainty on Dutch sentences (for an NMT system trained on German). However, this experiment can only be seen as a toy experiment (the translations from Dutch can be immediately detected as being ridiculous, on very simple criteria), and more realistic experiments would be needed to convince the reader of the practical applicability of the technique. After rebuttal: I am lowering my score for the paper. Also, I am disappointed that the authors did not update in any way their submission to reflect the reviewers’ comments (contrarily to misleading expressions in the rebuttals).<BRK>This paper adapts MC dropout to neural machine translation in order to measure prediction uncertainty to detect OOD samples. While the proposed method shows good results when used on out of domain data (or when the input data comes from a different language) the paper lacks comparison with previous work. Moreover, the paper s organization relies too much on the appendix. Pros:  The paper is clearly written and well presented  Good results on out of domain data and mixed language dataCons:   Insufficient comparison with previous work. In particular Fomicheva et al.2020 and Wang et al.2019 propose similar methods based on the variance of the token level log probabilities (instead of BLEU here). Without these results, it is hard to tell whether the proposed use of BLEU variance is responsible for improvements over the simple "SP" baseline. BLEUVar seems to help more when the data is from a different domain. However, it would be more interesting to see how it fares on a mixture of in domain and out of domain data (after all, this is a more realistic OOD detection setting). Results in the appendix are heavily referenced in the main text, which makes it feel like the paper is not "self contained". The reader has to jump back and forth between main text and appendix to get the whole story. ": Having parts of the title in a different language is fine, especially for an MT paper, but consider adding an English translation in the introduction or as a footnote, as most of the audience does not read Dutch. This valuable space could be used to move more relevant content from the appendix to the main text (such as related work or the experiments in appendix B)  In section 3, it might be worth mentioning that BLEU (or rather 1 BLEU) is not a proper distance metric. Seems like a relevant baseline for this specific scenario (and more practical than running a neural model 10 times). I suggest the authors focus on discussing mainly the results they can include in the main text, and only refer to the appendix shortly at the end (eg."see appendix [..] for examples of other cases") Post rebuttal: The authors have partially addressed my concerns with regards to experiments on actual domains. I think this is a central part of the paper and these experiments could be improved, however I am willing to augment my score to 5. I am still ambivalent about the paper but I wouldn t fight against it being accepted.<BRK>The paper proposed a Baysian method for detecting out of distribution (OOD) in machine translation. To this end, the paper introduces BLEU variance (BLEUVar) that is computed based on a number of samples from Transformer with MC Dropout. The advantage of BLEUVar is that it doesn’t require reference, instead it’s computed based on pairwise comparison of the decoded sentences. The proposed BLEUVar requires decoding $n 10$ translations of an input sentence. This is not desirable for MT especially each target sentence is decoded conditioning on a particular dropout mask, so it’s not parallelable. For OOD experiments, I find that the experiment in section 4.4.1 is a bit unrealistic. For MT, there are many datasets for different domains that can be used as testset (e.g., [biomedical](http://www.statmt.org/wmt20/biomedical translation task.html), [chat](http://www.statmt.org/wmt20/chat task.html), ...). Moreover, the training data in 4.4.1 is too small compared to typical MT training data. Similarly, I also felt that the experiment on different language domains in 4.4.2 is unrealistic. While the authors applied their method for an extreme case for demonstration. I think there are many extreme domains in translation which are more realistic. It would be nice to see the proposed method applied for those domains (i.e., biomedical, law, IT,...)<BRK>This paper describes a method for estimating a neural machine translation (NMT) system s uncertainty about its translation of a sentence that has two parts: (1) use MC Dropout as a proxy for integrating out parameters; (2) two uncertainty metrics (probability of translation summing over randomly sampled parameters and variance in BLEU using randomly sampled parameters). The baseline method is just to use the probability of the 1 best translation under the MLE parameters. For in domain sentences, BLEUVar does the best, and SP is the same as the baseline. The authors also tried treating Dutch sentences as "out of domain" and got similar results (BLEUVar > SP > baseline). I don’t think I understand the authors’ differentiation of the present task from confidence estimation (e.g., Blatz et al., 2004, http://www.alexkulesza.com/pubs/confest_report04.pdf; Ueffing and Ney, 2007, https://www.aclweb.org/anthology/J07 1003.pdf). That’s fine, of course, but I’m not seeing that the distinction between the present work and previous work on confidence estimation is that sharp. The paper uses a lot of space (about 1.5 pages) presenting fairly technical (to me) background on Bayesian inference, which turns out to not be needed for understanding the rest of the paper. I would suggest cutting this section down. On the other hand, the paper spends very little space explaining MC Dropout. Sections 4.2 reports some experiments/analysis on the relationship between uncertainty and sentence length. Section 4.3 shows one example sentence; again, I’m not sure what question this example is meant to answer. Would it be valuable to study the relationship between certainty (as measured by your measures) and quality? What would a scatterplot of BLEU vs. BLEUVar or SP look like? In any case, I think it could be interesting to show more than a few examples, maybe 10+ examples, to give a sense for what makes the model more or less certain. Overall, I like this method and think that it has value for the MT community. I’m not totally sure that ICLR is the best place for this work to be published; the WMT conference would have been a better fit. Other comments:  Why is the baseline method called "Beam Score" when all three methods use beam search?
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>Nevertheless, I think the kind of analysis presented in the paper very useful for the community and for further development of learned data structures. Thus, it is important to separate the impact made by a learned model from an algorithmic improvement.<BRK>Update (Nov 30th) In light of the author s responses and the other reviews I increase my score for this paper to 7: Good paper, accept.<BRK>Second, it is possible that the proposed technique needs to be reoptimized upon insertions. In this case, it will be important to understand the overhead of constructing the Bloom filters proposed in this technique compared with the baselines. And the writing of the paper is clear and easy to follow.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 6. <BRK>Paper Summary:This paper aims at discovering transferability of perturbations across different environments in RL. The authors did not propose any novel algorithms or modifications based on existing algorithms, and provide only preliminary experiment results. As such, I suggest a clear rejection. "Such a model... of the deep reinforcement learning agent". I am hardly convinced by this statement and led by this statement, the proposed approach in this work. My first thought when I read the title is that it would be a transfer learning method for adversarial attacks. But the authors seem to prefer a universal offset on the raw pixel input. Besides, all experiments are done only on DDQN, hence the claims are hardly validated. It is also not explained how the environments are chosen.<BRK>They defined different types of perturbations, like different perturbations for each state, or apply the perturbations on the initial state on every state. The conclusions of this submission are unclear and questionable. The authors showed many results in their submission, but all the conclusions are plausible. The A_M^random is more like a fixed random noise, so we cannot draw conclusions about transferability from it. Equations 1 and 2 do not make sense. I agree with other reviewers that the task is interesting and the submission has great potential, but might need another round of editing. However, I agree with R4 that applying a single perturbation from a random state of another MDP does not make sense unless the two MDP and the two states have some similarities.<BRK>— idea:A framework composed of 6 different adversaries is proposed using which it is claimed that the transferability properties among Atari environments can be studied. The only novelty is about using which states as adversarially perturbed input. In the experiments section, authors have used a pre trained DDQN agent which originally does not show significant generalizability compared to methods like A3C. Thus, the impact would not be this high (according to tables 1 and 2). I believe the paper in its current form requires substantial changes. Due to the aforementioned reasons, I don t feel comfortable supporting this version of the paper.<BRK>### Summary of ContributionsThe paper explores adversarial perturbations in deep RL, providing a new thread model where the perturbation is computed based on a single state. The paper explores the impact of various types of perturbations between states in the same environment (state transferability), and between states in different environments (environment transferability). ### ReviewI think the paper does a good job at capturing the extent/prevalence of the issue of adversarial perturbations. Do the authors have any insight as to how the trends might change should a smooth policy be used? Can the authors comment on whether 2) How were hyper parameters chosen for the DDQN agent, and can the authors comment on whether such choices can have reasonably strong interplay with the transferability of perturbations? I do think the paper falls a little short in that there wasn t a representative sample of deep RL methods, as well as not commenting on design choices made and how/whether they might interplay with the transferability measured.
Reject. rating score: 4. rating score: 5. rating score: 5. <BRK>This paper proposes to learn representations for graphic design layouts unsupervised using a Masked Language Model objective as in BERT with additional domain specific input embeddings. It also presents a large dataset of slides along with manual annotations used for their evaluation. The idea of using BERT like training to pre train features for graphic design layouts is sound and the proposed approach modifies the input token representation to represent different layout elements   using positional embeddings for continuous attributes (position, rotation etc.) Moreover, it would be useful to make it clear if the authors plan to release the collected slides dataset since that would count as a positive contribution in my books. would also help with the reproducibility aspect of the paper. Overall, I believe this is an exciting direction and I m looking forward to hear how the authors think their paper could be evaluated on harder tasks, against stronger baselines and against existing work.<BRK>To train their model, named CANVASEMB, the paper contributes a dataset of Powerpoint Slides, with more than 1 million slides (the paper promises to make the dataset public). Doing such an evaluation on the same datasets as CANVASEMB will demonstrate the strength and weaknesses of the Transformer based architecture. However, the paper makes use of a Transformer based model for Layout embedding, which is different and was not explored/employed before. As I said earlier, the RW section could (and should) be improved. The reader should not have to refer to the cited papers to know/learn what the Transformer is made up of. The first author naming convention cannot be superseded at the writer s discretion. Ex: A Transformer is first trained using the Slides crawled from the internet, which we term as CanvasEmb, and can be used as a pre trained model for other downstream tasks. 4) "mechanism" typo in the last paragraph of introduction (it appears twice continuously)5) In Equation 5, the tasks for each loss formulation should be written (in the equation itself)6) I would like to know the maximum number of layout elements in a Slide in the entire dataset. Needs to be corrected. There has been no mention of this in the entire paper, but this is crucial to understand if there is really a need for Transformer based architecture. 1) READ: Recursive Autoencoders for Document Layout Generation, CVPR 2020Quality:1) The paper claims that CanvasEmb is the first work to provide pre trained models for layouts in graphic design. What is special about CANVASEMB, that can not be done by the 2019 work when given such a large dataset?<BRK>This paper proposes to pre train the network on this large scale dataset without masked reconstruction strategy and verifies it with several subtasks including element role labeling, image captioning, auto completion and layout retrieval, with a comparison to a decision tree based method as baseline. This paper looks at slide layout data and constructs a large scale (>1m) dataset with parsed element properties. The proposed evaluation tasks all seem to be sub tasks of pre training and it doesn’t look falling into the classic scheme of unsupervised pretraining + supervised fine tuning. And is the only difference that the training loss? Evaluating graphic layout can be a hard problem and this paper tried to propose several small tasks as probes into the learned network. However, it will be more convincing to have a systematic design of experiments. Especially for layout design, visualization is very important. Layout retrieval is an interesting experiment, but manual scoring seems to be arbitrary. Neural design network by Lee et al.in ECCV2020 and LayoutGAN by Li et al.in ICLR 2019 seem to be good baseline network architectures to compare, although they are trained in different ways.
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 6. <BRK>The exact circuit routing problem is NP hard in general, and author proposed to an approximate method. At last, evolution strategy is deployed to find the best parameter that results in the most optimal greedy solution. ### Weakness:I am concerning on the novelty of the paper. The paper uses parameter to control the execution of a sequential A* algorithm, and the parameters are updated using neural strategies. As the circuit routing is a well known problem, author should include some baselines using traditional methods to demonstrate the significance of deploying RC on the routing problems. These problems can also be solved using traditional methods. Author can consider to evaluate RC on problems whose design constraint cannot be handled using traditional methods, as promised by the introduction. *During training, whether the learned ranking can be decoupled with the learned cost map?<BRK>Due to physical constraints no two routes may intersect, and the routes may not pass through obstacles if there are any. The paper proposes a ranking cost algorithm that combines A* search with an evolution optimization technique for learning efficient routes. Routing in circuit boards is an important problem for the vlsi community. Moreover, the problem has a combinatorial optimization flavor to it, which have been considered and is of broad interest to the iclr community as well. The main issue I have with the paper is the lack of sufficient motivation given for the proposed solution. The other issue I have is regarding generalization. If this is indeed the case, then how is the learning approach justified over well known black box optimization (e.g., simulated annealing, genetic algorithms etc.)<BRK>2.Strengths & weaknessesThe paper is well written and generally easy to follow and uses a principled and elegant approach to solving an interesting problem. The major limitation of this work is that the proposed method is only compared in terms of solution quality, and not by computational cost. 4.Arguments for recommendationThe paper is of high quality, well written and well motivated, but the experiments lack a comparison of the algorithm and baselines with respect to computational cost, which is relevant to support the claim that this method outperforms baselines and has better scalability. Maybe  objective function  could be an alternative? Why does A* use Euclidean distance as heuristic and not Manhattan distance which seems more sensible in a grid?<BRK>Kindly address the concerns regarding time complexity of the proposed and past routing approaches as described in the Cons section. Along with that, the paper is also missing comparisons with the exact solutions proposed in past papers targeting this problem. Updated: The authors added the requested timing complexity data and additional experiments with better baselines to compare the proposed RC algorithm against. 3.Section 4 presents a well written incremental description of the proposed routing approach starting from ranking learning to cost map learning to ranking cost. Which are missing in the cited learning based previous approaches. It would be useful if the authors could present an analysis of the limitations of this algorithm. Are there any cases when it fails to converge to the optimal solution?
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>The paper proposes to use graph based networks for evaluations of PDEs with continuous time formulations. Despite the simplicity, the authors make an effort to illustrate the behavior of their method with an ablation study, and to compare to previous work. For the PDE net comparison, I was wondering how the 3 time step sizes were incorporated into the PDE net. The larger time steps of Fig.3 seem quite trivial, but what I instead hoped to see here was a comparison to a model that simply receives the chosen tilmestep dt as an additional input, and is trained in a supervised fashion with data from a few different time step sizes (i.e.non continuous, but varying dt). This is maybe what was done for the PDE net, but the paper is not clear here.<BRK>Review: This papers proposes an algorithm to learn a model for spatio temporal data assumed to be described by a stationary spatio temporal PDE. The continuous time ODE can be solved using classing ODE solver. The obtained sparsity of the ODE coupling makes the method scalable. The methods cite other approaches that take a different route to the problem (e.g.for example learning the parameters of PDE directly) such approaches have their scaling issues but inherit from half a century of research on approximate solutions to pdes, and come with guarantees. For this reason, I am not a big proponent of the paper but do not oppose acceptance.<BRK>This submission proposes extensions of PDE net that relax some constraints that could help extend the range of applications of this approach. First, rather than fixing a spatial discretization in the form of a grid, the authors use a Delaunay triangulation to represent the domain. The updates to the nodes of this triangulation are performed using a message passing GNN framework which couples neighboring nodes. For small time steps, however, PDE Net is multiple orders of magnitude better than the present approach, a fact that should have been thoroughly discussed in the text rather than just mentioned, because the discrepancy is so large. However, the fact that the method could in principle be used on sparse data is not a demonstration that it works on sparse data. How does PDE Net perform in the 2 and 4 time point cases?<BRK>  SummaryThis paper presents a graph neural network model for learning to model PDE dynamical systems. Previously proposed methods either would not work on continuous time, or unstructured grids, or would not be applicable to settings with unknown governing PDEs. This work combines all these features. ConsSimilar graph based methods that used continuous time to model differential equation dynamics had been previously presented, e.g.GNODE.The novelty of the proposed method might be limited. On the positive side, the method seems to perform favorably when compared to other baselines, in comparisons that are actually favorable to the other methods (e.g., using regular grids). Moreover, the proposed method centers around using message passing neural networks to model the differential equation dynamisc. It seems strange that errors would spike up initially and then not compound significantly over time. Do the authors have any intuition as to why this is the case? Is it maybe a consequence of the data samples reaching a sort of steady state after some time? If so, wouldn t this weaken the case being made for a continuous time model? What integrator is used for the experiments?
Reject. rating score: 4. rating score: 4. rating score: 6. rating score: 7. <BRK>This paper presents four methods for creating benchmarking datasets, each focusing on a particular discourse phenomenon which is difficult/hard to solve with existing context aware neural machine translation (NMT) models. It also evaluates several existing NMT models using the created data and makes a remark about their current status, such as the superiority of one method in one task. This work has the originality and I am happy to know that some researchers devote their efforts to address these issues. However, I am very skeptical that the contribution of this paper fits for ICLR, since the methodology presented in this paper itself mainly consists of a pile of human efforts. In other words, if I understand the contents properly, only the technical advancement presented in this paper is TgtCon, a variation of the anaphora centric method proposed by Voita et al.(2018b).However, it cannot be a substantial merit to the community, since it does not necessarily perform better than existing methods and the reason of its deterioration is not analyzed. Considering the main focus of the paper, i.e., creating benchmarking datasets, the paper should rather fit for a journal article in the field of natural language processing, where the authors can give more details in creating the datasets, such as procedure, tool, and attributes of annotators, not in appendices, and more careful analyses of the results, including WHY each method does (not) perform well on a particular discourse phenomenon. For instance, the proposed TgtCon is not explained in Section 2 but the readers are advised to see an appendix. Other information that is indispensable in data creation but missing in this paper is the detail of human judgment, such as the proficiency of the evaluators, protocol, and judgment criteria. With these reasons, I am not completely convinced of the quality of the resulted datasets and the portability of the proposed methods. Q1.Is there a reason to choose these four particular discourse phenomena? Are they exclusive to each other? Q2.Test sets for coherence/readability and discourse connectives are significantly smaller than those for the other two phenomena. I am not sure that a test set with 200 or less examples is enough. However, in many error classification schemes, terminology errors are, irrespective of incorrect translations and inconsistency, distinguished from coherence errors. (*1) http://www.qt21.eu/mqm definition/Q4.<BRK>This paper presents a benchmark for discourse phenomena in machine translation. Its main novelty lies in the relatively large scale, spanning three translation directions, four discourse phenomena, and 150 5000 data points per language and phenomenon. positives:  clearly, a lot of thought and effort has gone into the creation of the benchmarks, and this is the most diverse set of benchmarks for discourse phenomena in MT so far. negatives:  two of the four benchmarks, anaphora and coherence, are evaluated by neural models trained on WMT outputs, so the interpretation of scores is opaque, and their validity is unclear. Specifically, Jwalapuram et al.(2019) train a neural network to distinguish references from MT output based on the ELMo representations of pronouns, but in principle, this model can use signals other than the correctness of pronouns translation to make this distinction. also, I m not fully convinced about the validity of the automatic discourse connective evaluation. According to the manual analysis, there is a large proportion of false negatives (synonymous translations flagged as errors), and rankings would change if synonyms were counted as correct. I was also not satisfied with the evidence that the omission of connectives is generally an error. The human study was a bit simplistic in that it just deleted connectives (although more changes might be needed to ensure grammaticality) or used noisy MT output rather than alternative human references. I was surprised by the low results (already in terms of BLEU) of some of the tested variants. Authors describe in great care their efforts to fairly reproduce previous work, including the use of original code and hyperparameters where possible, but I can t help but think that the models are suboptimally trained, and that statements about whether context aware models consistently improve discourse phenomena are tainted by this. recommendation:I m leaning negative on the current version of the paper and benchmark. I think the test sets have been carefully assembled, and along with the various types of models evaluated on them, this work has value. But before I d recommend that the benchmarks actually be used in the field, authors would need to improve upon the evaluation scores used for anaphora, coherence, and discourse connectives and make sure they really are targeted towards the phenomena they claim to measure, and do not have large blind spots.<BRK>This paper presents a dataset, a trained evaluation metric and a leaderboard for evaluating discourse phenomena for machine translation. They test this on a range of discourse level translation models and develop metrics which evaluate the models according to their performance on four discourse phenomena: anaphora, lexical consistency, coherence and readability, and discourse connective translation. They release data for three language pairs and using their method one could extend it relatively easily into others. I like the thoughtful way that the authors find examples of hard discourse phenomena and each phenomena requires distinct handling. Weaknesses:They rely on previous work to create the Anaphora test set and evaluation model (Jwalapuram et al.(2019)).They should have explained at a high level how the Jwalapuram evaluation model works and they should have given a general idea of the rules used to filter the anaphora test set: how many rules, an example, would this be possible to do for other languages or does it only work well for English? I would have liked more discussion about the extensibility of their approach into languages other than English. They should have used sacrebleu or at least specified if the BLEU scores were tokenised and true cased. I think this paper should be accepted because of combined strength of the dataset/metric/leaderboard. I think fine grained evaluation of hard phenomenon is the way forward for improving already very good MT models.<BRK>In this paper, the authors propose specific test sets for document level NMT. They target anaphora, coherence/readability, lexical consistency and discourse connectives, and are available for multiple language pairs. The first two challenge sets rely on model based evaluation. Strengths:The test sets, which target various discourse phenomena, directly evaluate the output of the models (contrarily to some existing multiple choice challenge sets). The authors validate the quality of their metrics by comparing against human judgements (although the number of samples is arguably small). Weaknesses:The evaluated NMT models all date from 2018 or earlier. The anaphora challenge sets are only a minor update over previous work. All language pairs use English as the target language. Other remarks and questions:For the anaphora and coherence/readability test sets, future work may "cheat" by using the evaluation models as part of the NMT systems. The reference scores should be the same across all systems, so it only shifts all results by a constant and doesn t affect relative performance.
Reject. rating score: 4. rating score: 6. rating score: 7. <BRK>Augmented Sliced Wasserstein distance between random vectors X&Y is defined as SWD between g(X) and g(Y) for an injective g. It is claimed that this generalization solves the problem with SWD, i.e.a small distance for most of the projection directions. Since g_omega is also searched for by maximizing (16), then it becomes hard to interpret the value of metrics (also taking into account the role of lambda). What it measures now? The code is given, results seem reproducible. Since the paper is experimental, maybe it is natural to expect a comparison with SOTA generators that do not deal with generalizations of Wasserstein distance.<BRK>The paper provides a notion of generalisation for sliced Wasserstein distances, that allows to explore nonlinear projections in arbitrary subspaces in a suitable and efficient way. I feel this idea, although simple can be quite powerful, and can be extended to wide domains especially identifying objects based on arbitrary feature selections although I do not think the authors have explored that direction in this piece of work. Comments:(1) Numerical results show the distance computed by ASWD is smaller than other measures. For eg, if the sliced Wasserstein distances are computed along orthogonal directions to the primary features the Wasserstein distance obtained in that regard will also be very small, although this does not in anyway validate the superiority of the distance metric. (2) Can I use this method to construct arbitrary nonlinear projections of choice instead of the projection that gives the best distinction? For eg, suppose two pictures have several objects among which cars in the two pictures are the most distinguishing features. In that broader sense I suppose my question deals with trying to identify barycenters of objects via sliced Wasserstein distances. Any thoughts in that regard could be useful for the reader. Overall I find the paper an interesting read although it requires some clarifications as outlined above.<BRK>To this end, the architecture first maps inputs to higher dimensional hypersurfaces, before projecting. The designed ASWD is shown to be a metric as long as the initial mapping is injective. The loss aims at capturing a hypersurface that differentiates measures the most, and is regularized to control the initial mapping. The first task considers the minimization of sliced Wasserstein flows for different settings of synthetic data. The other task consists of generating images with GANs, using a sliced Wasserstein loss to learn the generator network. Strong points of the paper include:1. 2.Both theoretically and numerically, the method is well described, well compared to existing notions and yields convincing results. It seems that a lot of projections are needed to retrieve visually satisfactory generated examples. 2.Have you tried other types of injective maps than Eq 15? 3.Can you comment on the impact of regularization strength lambda in practice?
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 7. <BRK>I am mildly familiar with the "canonical discriminative" formulation. %%%%%%%%%%%%%%%The paper discusses identifiability of learnt representations in supervised settings and under "canonical discriminative" modelling. It provides conditions under which two learnt representations are equivalent up to a linear transform. A longer journal version would be more appropriate.<BRK>In this paper, the authors address the model identifiability in a general setting that can be adapted to several recent deep learning models (DNN supervised learning, CPC, BERT and GPT). Although the purpose of the work is appealing, there are some issues related to the current structure of the paper, proposed theory and its relationship to the provided experiments. Theoretical claims: This figure clearly illustrate a strong relationship between f’ and f*, which is close to a linear mapping, but it is not exactly linear. It would be good that the authors add some theory about it. Agreement between theory and experimental results.<BRK>This paper claims that, through Canonical Correlation Analysis on the representations learnt by popular deep models, we can show that the representations learnt on the same dataset, with same model using different sets of parameters, the representations are linearly identifiable. This would be interesting to add in my opinion. As is, this contribution in my opinion remains as a cute thing to know. This seems like a nice thing to know.<BRK>This paper investigated the identifiability of the learned representations in pre trained DNN models that fall into a general class of function space,  defined by the canonical mathematical form. From the theoretical aspect, this paper proved the linear identifiability of a large class of DNN models in an ideal setting. At last, the authors conducted experiments to empirically investigate the linear identifiability of DNN models in a practical setting: finite data and partial optimization.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>The story and presentation of the paper are clear.<BRK>There is generally a lack of references in the description of the evolutionary algorithm (EA) used in the paper. Based on the author response, the rating has been updated (see comments below). The introduced space of morphologies is quite general and can capture some very interesting designs.<BRK>While the paper is interesting there are a few issues that should be addressed:  The approach relies on randomly sampled actions. Additionally, most of the morphologies are relatively simple, compared to some of the other work in evolving virtual creatures.<BRK>The paper introduces an algorithm for optimizing the robot morphology in a simulated environment. In general I think the paper introduces an interesting idea. The proposed algorithm is demonstrated on a few simulated locomotion and manipulation tasks. The idea of optimizing morphology in a task agnostic way is interesting.
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 8. <BRK>To verify this condition may take exponential time, but it gives quantitative expansivity results for random nets. 1.The research problem on when a multilayer ReLU network is injective is mathematically interesting and sound. The paper has some though understanding of a single layer case followed by some discussions of multi layered settings. However, I also have some concerns about the paper. 2.The considered problem seems not very useful to the ML community. It is not exactly true that global injectivity is important to the studies of inverse problems with generative models. With leaky ReLU or flow model, global injectivity is automatically satisfied for non degenerate weight matrices, and in most applications, we don t see much difference. It is unclear to me how to interpret the results and how to make them useful in general.<BRK>Summary This paper studies injectivity of fully connected and convolutional ReLU networks. Given all the theoretical results I have some fundamental questions on how to apply these ideas to practice. However, given that the DSS condition takes exponential time to check, how do you check for injectivity of a given network? Also, a good portion of the paper deals with randomly sampled or generic matrices, but after you train a network using some training dataset, these matrices are no longer random or generic. How do you ensure that the network is still injective? I see that the experiments in the paper used a convnet counterpart of Corollary 2, but Corollary 2 only holds if the expansion factor is exactly two. In addition, I think the paper has several typos in notation as well as unclear points in theorem statements. The term “shift” in Eq (8) is not defined precisely. But two lines below that we have $n_L   m$, suggesting that the indices are off by one or two.<BRK>Summary: Inverting a deep generative model with the ReLU activation function is very important. The paper studies injectivity of neural networks, which improves inference in GANs. Notably, the paper presents the directed spanning set (DSS), which is the core of all theoretical results. + Some conditions for injective networks are presented. Weaknesses:   The paper is not clearly written, making it is hard to follow. When Wx y has multiple solutions or Wx<0 has solutions, ReLU(Wx) will precludes injectivity, where y> 0 is a constant. I is an identity matrix and is also an inference network. It seems that the result in (Lei et al., 2019) (i.e., m> 2.1n) is tighter than the presented one (m> 5.7n). Also, how to use Corollary 2 and Theorem 2 to construct W in practice? It is better to test with some real problems, e.g., denoising. In the first paragraph of subsection 2.3, ||ReLU(x) ReLU(y)|| should be ||ReLU(Wx) ReLU(Wy)||.<BRK>This paper studies injective ReLU networks, motivated by various applications such as generative modeling, inverse problems and compressed sensing with generative priors. They provide layerwise and multilayer results, characterizing the stability of inverting an injective network, and using tools from differential topology to study injectivity of deep networks. They also prove a sufficient condition for injectivity, which is an end to end doubling of the dimension. The overall writing is clear and the use of pictorial illustration is helpful for less mathematically mature readers. Despite not being an expert in this area, I acknowledge the signifcance of exploring the properties and theories of injective (ReLU) neural networks, particularly for the study of inverse problems and generative modeling. Cons:   Would like to see an experiment for inverse problem applications, though this is rather minor. **, do you mean $ \mathcal{NN}(n, m, L, \mathbf{n}) $ instead of $ \mathcal{NN}(n, m, L, \mathbf{m}) $? In (2), $ b_L $ instead of $ b_\ell $? Should there be $ b_{L+1} $ as well?
Accept (Poster). rating score: 8. rating score: 7. rating score: 3. rating score: 2. <BRK># Summary The paper introduces the Graph Information Bottleneck (GIB) which aims to learn the most informative compressed representation $Z$ given graph $G$ with associated label $Y$. The procedure retrains the graph subgraph mutual information estimator in the inner loop for each step (eqn.10) before updating the parameters of the backbone GNN and the subgraph selection MLP and finally updating the subgraph label MI estimator ($L_{cls}$). Further, on graph interpretation task, the authors show that the GIP objective improves the similarity of the retrieved subgraphs using domain specific metrics. The authors also evaluate on graph denoising on the MUTAG dataset. This paper is well written, makes a clear theoretical contribution to the field as well as provides sufficient empirical evaluation. I wonder does the initialization have an influence on the final chosen subgraph nodes. saturate  as mentioned on page 5? It would have been good to see plots showing the convergence of the different losses as part of the bi level optimization iterations.<BRK>Summary:I think this is a nice paper that successfully used information theoretic objective functions for graph representation learning. The authors leveraged the DONSKER approximation of mutual information for a global information bottleneck loss used on the input space instead of learned latent space. To help stabilise optimisation, the authors also use bi level optimisation with more iterations on the inner $I(G, G_{sub})$ as well as automatic masks learning through their $L_{con}$ loss. I hope the authors could spend a bit more time to add the relevant methods and possibly compare against them. Pro:  Quite interesting information theoretic objective functions that actually work on multiple graph learning tasks. Figure 1 is quite helpful for understanding the story quickly. Con / Questions:  In Eq.13.<BRK>The submission proposed to use Graph Information Bottleneck (GIB) for the subgraph recognition problem in deep graph learning. Basically, it makes use of bi level optimization to find a subgraph that well encode the information for graph classification task. Typos:In 4.1, “a informative representation” >”an informative representation”In 4.3, “S is 2 dimensional vector” >”S is a 2 dimensional vector”6). The authors does not report or discuss the running time complexity of the algorithm. The submission is lack of novelty. First, the Graph Information Bottleneck (GIB) is used to learn a robust GNN against adverbial attack in the paper accepted in this year NeurIPS. 1).My major concern is on the novelty part. Since the framework needs bi level optimization, it is supposed to discuss how fast the algorithm will converge. As for the subgraph selection, much work can be found. The major claim is that the resulting subgraph is more robust for the learning. 3).The experiments are not sufficient.<BRK>Authors propose to apply information bottleneck to network structured data which is represented by graphs whose nodes are assigned features. The idea seems promising but the authors need to improve their manuscript considerably. In particular, the probabilistic model underlying the IB framework needs to be made clear right from the start. Which random graphs do you consider ? "...the GIB seeks for the most informative yet compressed representation Z by optimizing the following objective .. " what is the domain of the optimization problem here ? and what do you mean precisely by "compresse representation"
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 6. <BRK>My point is that   why not push the broader goal to the extreme   can one combine all these engineering tricks in one piece, and design a way faster contrastive learning pipeline that can train with way less resources and still perform as well in much less wall clock time.. Reducing time to accuracy could be done in a host of other ways   eg   make everything run in fp16 or bfloat16 on the right kind of hardware   you would see a speedup of ~1.3x already. A useful consquence of this paper is making contrastive unsupervised learning on ImageNet more accessible to people with less computation resources, ex PhD students in academic labs who may not have a DGX 1 or v3 TPU pods. I also encourage the authors to push further on making their pipeline even more efficient.<BRK>In this submission the authors suggest modifications to reduce the computational cost of contrastive learning. Pros:1) Earlier contrastive losses are an interesting idea! 2) While the authors present a few analyses which parts of their method and network are important I don’t think these are particularly informative. 3) Having losses for the individual parts of the network, the immediate idea would be to train the parts separately which would remove many more backpropagation operations and would allow parallelization of the network parts onto separate machines. This possibility is not explored here. 4) Given that the stated aim is to reduce training times it seems that there is very little analysis on what actually takes time in this approach.<BRK>This paper proposes techniques to speedup contrastive self supervised pre training. The two key ideas are to back prop only through a subset of layers in the network (from a random layer, back to the input), and to drop training instances which are not "hard" (in a way defined more precisely in the paper). Although the acceleration approach works well early in training, it is not clear whether the same benefits play out when one aims to achieve performance on par with that reported in the literature. It would be useful to know at what point in training (or at what scale) this becomes apparent. Anyways, I guess the main message is about reduction in training time, and memory savings could be down played.<BRK>**Summary**: This paper proposes a new pipeline to speed up the training of contrastive learning. These two strategies can significantly accelerate contrastive learning while matching the performance of the recent methods. **Pros**:+ The whole idea makes sense. + Overall, the paper is well written and well organized. As mentioned in [1], the low  and mid level representations, not high level presentations, make the instance discrimination good for the detection task. Hope the authors could address my concerns or questions in the rebuttal period.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 5. <BRK>Review: This paper studies how to improve contrastive divergence (CD) training of energy based models (EBMs) by revisiting the gradient term neglected in the traditional CD learning. This paper also introduces some useful techniques, such as data augmentation, multi scale energy design, and reservoir sampling to improve the training of energy based model. + The paper is well written. Even though the motivation of the current paper is clear, which is to improve the CD learning. However, the CD learning (in equation 2) is biased compared with MLE (in equation 1). Even though the narrative of the development of EBMs is quite comprehensive, it is not complete and even a little bit misleading.<BRK>This paper proposes several techniques to improve contrastive divergence training of energy based models (EBMs). Other techniques include: using data augmentation, defining the energy function as a sum of energies over multi scales, and using reservoir sampling. The performance of the trained EBMs on image generation, OOD detection, and compositional generation are tested. In generally, the paper is well written and addresses the important problem of improving EBM training. But I have some concerns. [a] achieved much better results than the proposed method in CIFAR 10 (Table 1). But it is this non negligible gradient term that stabilize the EBM training. 2) I can see the benefit such as compositionality from the proposed method of training EBMs.<BRK>The paper proposes a series of new techniques to enhance the training of an energy based model, and the proposed techniques include: adding the often neglected KL term to the training scope/data augmentation + multi scale energy function/an experience replay buffer for training. The experiments demonstrate the proposed method could generate high quality images, compositional tasks and perform out of distribution detection. The main idea and motivation are well and clearly conveyed by the writing. Figure 9 would need to compare against other methods. An important argument in the paper is that the added KL term enhances the mode coverage. Could the authors provide some more evidence on this point?<BRK>This paper proposed an improved version of contrastive divergence learning of energy based models by combining a bag of techniques: (1) add back a KL term that is neglected by previous methods (2) data augmentation (3) multi scale processing (4) reservoir sampling. Pro:The paper is well written and easy to follow. Cons:1.The advantage of adding the KL term is not quite obvious given the current experiments. 3.The multi scale processing of EBMs has been explored in [1], which should be discussed and compared. Besides, [2][3][4] are relevant references of training EBMs that should be discussed. However, I am concerned about the correctness and the necessity of adding the gradient term (L_KL), which is one of the major contributions that the authors claim.
Reject. rating score: 4. rating score: 4. rating score: 7. <BRK>*Summary of the paper:The paper discusses new measures to evaluating the quality of representations. They present two new measures namely SDL and $\epsilon$ sample complexity, describe their benefits and apply experimental study, comparing these measures to standard baselines. The authors emphasize that their measures are independent of dataset size $n$, but this only applies when $n$ is large enough and when $\epsilon$ is a good estimate of the conditional entropy. The authors also mention two issues regarding existing measures (MDL,MI)  They are insensitive to statistical complexity  They are insensitive to computational complexitywhile these are true for the theoretical measures, I don t see what is the benefit of the current measures in this context. This is since old and new measures are leaning on an approximate  computation of $L(A,i)$.<BRK>The authors address the issue of evaluating the quality of representations based on performance of a classifier for a downstream task. The premise is that conventional metrics for the downstream classifier such as Validation Accuracy, MDL or MI are flawed due to dependence on the size of the dataset (VA, MDL) or because they ignore statistical/computational complexity of the classifier (MI). The alternatives proposed in the paper SDL/\epsilon SC, which attempt to measure the complexity of learning the classifier to \epsilon tolerance. The authors do say that the they want an evaluation measure which depends on the data distribution, but seem to not pay sufficient attention to the "distribution" part (measurements are noisy, and we should understand what is noise, and what is a significant improvement, possibly by computing confidence intervals). The authors specifically mention a downside of MI is that it ignores computational complexity of the classifiers, but that seems to be a valid complaint of their approach as well (presumably more (computationally) complex classifiers can achieve a loss threshold), though their approach does address the issue with statistical complexity. There are other issues   there are typically many tasks that we might want to use a common representation for. Further, we typically have a fixed amount of data for each downstream task   it would seem that the approach proposed here would require an arbitrary amount of data for each task, which may not be feasible.<BRK>The submission addresses the problem of representation evaluation from the perspective of efficient learning of downstream predictors. Leveraging the introduced loss data curve framework, the paper studies and demonstrates the limitations of the existing methods in terms of their implicit dependency on evaluation dataset size. Motivated by practicality and interpretability of the measures for choosing the best representations, the paper introduces two novel methods, $\epsilon$ sample complexity ($\epsilon$SC) and surplus description length (SDL), which are well motivated and supported both theoretically and empirically. The paper has excellent motivation and discussion about how existing methods are inconsistent with representation quality, computational complexity and are not robust when evaluation dataset size changes—thus deeming them inapt to answer practical questions. For example, how hard is it to learn a predictor on the given representation? How many samples with given the representation are needed to achieve a specified performance? The proposed methods are designed to handle these questions and are robust to the evaluation dataset size secured against precipitate decisions on which representation is best. The paper considers the best representation to be the one that allows the most efficient learning of a downstream predictor. Although very plausible, the assumption might not hold when there is a sufficient shift between training and validation data.
Reject. rating score: 3. rating score: 5. rating score: 5. rating score: 6. <BRK>## SummaryThis paper attempts to unify the three most prominent regularization based continual learning methods: EWC, MAS, and SI. However, I could not find any description of why the applicability is restricted and to what extent it is applicable. I think these are vital parts of a proper theory. The paper is well organized and easy to follow. The authors should have claimed, "SI and MAS work, **and** they are similar to AF," instead of "SI and MAS work **because** they are similar to AF." Even after the discussion with other reviewers, my concerns are not resolved. The similarity alone is not enough to be a theoretical explanation for effectiveness. ### Weak connection between the diagonal Fisher Information and the Absolute FisherDespite searching the web, I could not find any proper material on the Absolute Fisher that explains its connection to the diagonal Fisher. The only similarity that I find between the Absolute Fisher and the original Fisher is that they can be computed with the gradient. Similarly, I think other methods, such as MAS and SI, can also be optimal under different assumptions. I suspect that the latter case is not even possible. The experiments also support my claim: AF does not necessarily outperform MAS or SI. Since I could not see other utility in this paper, I recommend rejection.<BRK>**************************************************************************************** POST DISCUSSION UPDATE ****************************************************************************************Thank you to the authors for the discussion. However, since the connection mainly hinges on the empirical correlation between the two, I still don t think that the paper quite establishes the theoretical connection between the three continual learning methods that it aims for. By this the authors claim to establish a relationship between these two approaches and Elastic Weight Consolidation, which approximates the diagonal of the Fisher. While the paper is well written and  structured, and SI and MAS are popular methods in the literature, I m not convinced by the link through the "Absolute Fisher". I think the authors really need to either provide some references to existing work or establish themselves that this variant makes sense theoretically. Further, I m not really sure what the takeaway from the proposed unification of these methods is even assuming that it can be put on a more solid foundation. Is it that through the relationship to EWC they are all approximately Bayesian? The part on SI relying on its  bias  seems potentially interesting, but since   if I understand things correctly   this is an unexpected empirical result from the theoretical point the paper is trying to establish, it would be necessary to go a bit more in depth.<BRK>Disclaimer: I am not an expert in continual learning even if I have already experimented with EWC, but rather my main expertise is related to FIM in other contexts. This works aims at unifying 3 popular regularisation type continual learning methods, namely EWC, SI and MAS, by showing that under some assumptions they all relate to the Fisher Information Matrix. These assumptions are shown to hold on 2 different tasks trained using Adam. ### Limitations regarding SIYour argument regarding SI (sec 5.2) seems to be valid only when using Adam, while SGD+momentum is the standard for image classification nowadays. In my opinion you can come up with a more general argument even using standard stochastic gradient algorithm, e.g.the relationship between the FIM and the covariance of the minibatch gradients as already been studied e.g.in [1] and [2]. Moreover the denominator $\theta\left(T\right)   \theta\left(0\right)$ of eq.4 is not discussed later in the text, or in other words, if $\tilde\omega\left(\text{SI}\right)$ is similar to $\omega\left(\text{EWC}\right)$, what about $\omega\left(\text{SI}\right)$? ### ConclusionIn conclusion, I really appreciate the effort of trying to unify which is certainly more useful than inventing countless slightly different variants of the very same technique. I would however like to see these points addressed before publication.<BRK>**Summary of paper**This paper draws links between three common regularisation methods for continual learning: EWC, MAS, and SI. It shows that MAS and SI approximate the Absolute Fisher matrix. Can they say anything about the links between the Absolute Fisher and the empirical Fisher? **Review summary**I really like the majority of this paper. I believe this paper is a good paper. However, I have an issue with the proposed quicker/cheaper update for calculating the (diagonal empirical) Fisher Information Matrix for Online EWC ("Batch EF"), as I detail later. If it were not for this, this paper would be a clear accept for me. I hope to resolve this issue with the authors during the discussion period, depending on which I can raise (or lower) my score. The authors already changed these claims somewhat in the updated paper. One of the biggest reason I find this paper is interesting is not mentioned (enough) by the authors. The paper is written well, with good detail and very good experiments. Some reviewers are arguing for further changes. Such works provide evidence that different approximations of the EF can work well. I think some claims can still be reduced, particularly, the link between AF and EF (and hence the link to EWC). At the end of training, when we have converged to a low loss, this will be very small. However, I expect Batch EF to have smaller values than EF because of the reduced noise on average.
Accept (Oral). rating score: 9. rating score: 9. rating score: 8. rating score: 6. <BRK>Each variable of a logical query (involving existential quantifiers, conjunctions and disjunctions) is mapped to an embedding. Why are they important? The paper proposes an elegant and effective method.<BRK>The fact that queries are not embedded (and so learning does not need large numbers of queries) is a strong point of CQD,with competing methods (Hamilton et al., 2018;Daza & Cochez, 2020; Ren et al., 2020) requiring many queries for tuning the query embeddings. The experiments are sufficiently extensive to support the claim of the paper that CQD is also outperforming competitors in terms of the quality of solutions. ": for rank do the authors mean the embedding size? ": this sentence is not clear.<BRK>The paper attempt to answer conjunctive queries that are in the form of a chain of facts bound together with unobserved variables. In general, I think it is a very practical paper<BRK>I think the paper is clear and easy to follow. 2.Also, the notation $e_i^j$ is abused, in Eq.2, it represents a logic formula, however, in Eq.3, it represents the output of $\phi_p$, which is a scalar.
Reject. rating score: 3. rating score: 5. rating score: 7. <BRK>1.SummaryThis paper proposes an augmentation of the Ladder VAE model (LVAE [1]) using 1. more flexible variational distributions using normalizing flows (denoted $f$) 2. an autoregressive component (because of the generative dependency $p( x_{l} | z_{l}, x_{l 1}) $ (*data layers*), although this is not directly stated as a contribution. 2.[a] Strong Points  the authors introduce a very flexible architecture for DGMs by combining recent improvements from the literature. This prevents the community from understanding the effect of each of the architecture choices and does not guarantee that such an architecture could be effectively adapted to other contexts. I think your work could greatly benefit from an ablation study and from defining the model in more minimal terms. "Nvae: A deep hierarchical variational autoencoder."<BRK>**GENERAL**The paper proposes a new hierarchical architecture for VAEs. The main idea is to use invertible components that could be shared by the encoder and the decoder. However, the paper is very confusing in many parts, and the experimental studies are not too strong. D6: I appreciate the experiments provided by the authors. S2: The idea is to use bijective layers shared by the generative and the variational parts of the VAE.<BRK>**Contributions & Significance**:  The proposed paper introduces an encoder decoder architecture for hierarchical VAEs based on bijective transformations, which preserves the true factorization of the posterior in its variational approximation (which is formally shown). The general method seems widely applicable for VAE like models and as such can be combined with other architectural improvements or potential future work. **Style**In general the paper is well written and reasoned for. Typo Figure 2: Prior Layers: I assume $p(\epsilon|z_2) $should be $p(\epsilon|z_1)$**Summary**The paper is well written, has a promising and sound approach and seems generally applicable. However, it could still benefit from some improvement in terms of clarity and number of experiments (i.e.other data sets).
Reject. rating score: 3. rating score: 5. rating score: 6. rating score: 6. <BRK>My major concern is not about the two contributions mentioned above. Instead, I think that the first and main contribution of this paper is a subset of previous works  contributions. The contribution of this paper seems to be the first part of [1]. Frans Oliehoek and other researchers also did lots of excellent works on this topic [2,3,4,5]. Additionally, the difference between the proposed method and Jaques s paper (social influence) is not significant. The only difference is whether the action of other agents, $a_j$, is $a_j^{t}$ or $a_j^{t+1}$ when calculating the mutual information (In Jaques s paper, they prove that their formulation is equivalent to a mutual information formulation). I do not think the author s definition is an improvement of that of Jaques. The problem is that SSD used by Jaques is a more challenging task than those used in this paper.<BRK>I agree that the latent variable $z$ should be an unobserved variable which reflects some information of coordination, e.g., such a variable can be implicitly induced during the learning process of policies of agents. Especially, the authors use a common and pre determined sequence of $z$ is generated before execution for agents. In my opinion, this violates the CTDE paradigm which is claimed by the authors, since such a common sequence of $z$ is more like explicit signals of how to perform coordination (not truly decentralized). Moreover, it seems that the two of three environments, i.e., Predator prey and Cooperative Navigation, are not partially observable (PO). It would be better to provide more evaluation under PO environments. As in this paper, the experiments contain environments with up to 4 agents. A development towards more agents should be considered.<BRK>This paper can be seen as a modification of SAC, in a multiagent setup by adding the conditional entropy $H(\pi_i|\pi_j)$ as a second set of regularization on top of $H(\pi_i)$. The overall idea and intuition appear to be interesting. Mutual means two $a$, how about more, e.g., $H(a^i|a^j, a^k)$. Is that true? 2.The motivation for inducing mutual information using latent variable is not well motivated. 3.Would the new regularization benefit other RL algorithms? Say, would the regularization combined with the baseline algorithms shown in the experiment be better compared with those without regularization? Essentially, would decentralized training be more reasonable?<BRK>Summary:The authors propose to include the mutual information between agents  simultaneous actions in the objective to encourage coordinated behaviour. Weak points:The decentralised execution relying on having random generators with the exact same seed is not a robust solution. Could be some adversarial example of an environment where the optimal policy requires lack of coordination? Wouldn t the current method suffer in such case? Since the environment has stochastic transitions, aren t more assumptions needed in order to allow gamma   1 and still ensure existence of optimal policy? The authors use the term causal diagram, but it seems to refer a Bayesian network, which indicates correlation rather than causality, is that right?
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. rating score: 5. <BRK>Objective of the paper:  The objective of the paper is to show that the side channel information (based on timing, etc.) Is there any way to combine the multiple side channels (into a mega side channel?) 4)  This reviewer does not have good insight into why this type of side information would be useful in determining how to reconstruct images in the manner done here. and do better?<BRK>This paper describes a  new side channel analysis attack framework to reconstruct images from system side channel. This discussion deserves to be included in the paper. The idea of reconstructing images from system side channels is interesting and the proposed approach shows good performance.<BRK>A VAE LP model first reconstructs a coarse image from side channel information which is reshaped and processed using a convolutional network. in section 3.2 #### Justification of rating:Although the authors present an interesting application of the VAE GAN framework for recovery of private information from SCA, the formulation of the problem is remarkably similar to conditional VAE. Having said that, the paper is very well written and thorough in its analysis. 4.Although the formulation is interesting. The novelty is somewhat limited as it is straightforward application of the VAE GAN framework to SCA data.<BRK>The authors proposed a representation and generation model to reconstruct the image signals from system side channel signals. Otherwise, purely talking about the attacks strategies may raise some ethics concerns of the paper. Instead, it is a general framework for any reconstruction problems. The task itself is interesting and novel, demonstrating the first efforts and impressive performance on recovering noisy side channel signals.
Reject. rating score: 3. rating score: 5. rating score: 5. rating score: 5. <BRK>The paper proposes a novel approach for solving MORL problems while considering uncertainty in the Pareto frontier. The contributions are interesting and novel, but the paper has several flaws which make it not ready for acceptance, especially regarding experiments. And why these environments out of all Mujoco ones? (these three are known to be the easiest). How did you turn them into multi objective? Finally, there is no evaluation against any of the MORL algorithms mentioned in related work. Overall, the experiments section feels rushed and incomplete, and the paper is not ready for publication. The changes are too substantial and the paper looks like a completely new one.<BRK>This paper proposes a framework to tackle uncertainty in multi objective optimization of reinforcement learning problems. While this is quite beneficial, the paper raised many questions, some of which may need further treatment (for instance, increasing the number of objectives has an effect on the number of Pareto optimal solutions that is is not trivial). For instance, it is worth adding a detailed algorithmic description of the approach. They are not mentioned in the experimental part, hence the results cannot be reproduced. Multi attribute Bayesian optimization with interactive preference learning.<BRK>The paper proposes a robust multi objective RL approach and a non linear utility metric to enforce an accurate and evenly distributed representation of the Pareto frontier. The goal of the main agent is thus to learn the policies on the Pareto frontier under attacks from the adversary. Could you give more motivations why a robust approach is needed for MORL? The motivation now seems simply to be that the literature didn t take into account robustness. I think the idea of the paper is quite interesting but it is not well written/explained. I think a few details are missing. In figure 4 what is $\alpha(\Omega)$? Why there is an approximation in the definition of the gradients? This is to say that I think the example in figure 3 has a major drawback.<BRK>* Lack of experimental details for reproducibility, e.g., network architetures and DDPG hyperparameters. Recommendation:This paper connects two seemingly orthogonal problems, multi objective RL and robustness. This is an interesting topic, but there are several issues regarding clarity and the motivation (as detailed in the cons list below). * The empirical evaluation shows that the approach outperforms ablations and an existing state of the art MORL approach (Xu et al.2020) on continuous control tasks. Does Bayesian optimization only suggest one preference at a time (in red)? In addition, existing methods are stated to only be able to find solutions on the convex portions of a Pareto front.
Reject. rating score: 6. rating score: 6. rating score: 6. <BRK>In this work generative models using a GP as prior and a deep network as likelihood (GP DGMs) are considered. In the VAE formalism for inference, the novelty of this paper is located in the encoder: It is sparse and the posterior can be computed even when part of the observations are missing. However, there are many unclear details in the experiments. How does the model compare with prior similar work, such as the GPPVAE? Here we see results based on amortized inference, but what if we weren t using it? In the other experiments, no strong baselines for that specific  data seem to be provided.<BRK>The model proposed by the paper is a multi task, multi output GP, with likelihoods driven by a decoder network. Since this plays a central part in the contribution of the paper, I hope the authors can expand on this. For example, in what way is it "partial" and how is it different from normal inference networks, and what are the challenges. From a GP and Bayesian inference perspective, missing data is no problem, as acknowledge by the authors in section 2.3. Although section 2.3 is section dedicated to this, it is insufficient. 5.The section title for section 2 is "spatial temporal". For example, I do not really find the "temporal" dimension in Jura. One way around this is to have a more general section title. The paper is contributes to the community and the work is correct, so *accept*.<BRK>This paper attempts to enhance the inference in multi output GPs on previously unobserved data using amortised variational inference. This model is proposed to overcome the shortcomings in current GP DGMs. The major concerns regarding this paper are summarized as below. The NN parameters are shared across tasks, which however might be a too strong assumption and thus could deteriorate the performance of multi task learning. The comparative results in Table 1 cannot showcase the superiority of GP VAE in comparison to the state of the art multi output GP. It performs worse than GPAR in terms of NLL on the EEG dataset, and is comparable to GPAR on the two datasets in terms of SMSE.
Reject. rating score: 2. rating score: 5. rating score: 5. rating score: 5. <BRK>Overview : I found this paper very confusing with crucial mathematical definitions and exposition often inconsistent or imprecise. The Riemannian geometry in the paper needs to be explained more for the average deep learning researcher, e.g.its not clear why the "geodesic interpolation" is doing at or why its algorithm makes sense. Finally the lack of experimental code makes this paper a very clear reject in my opinion. The main algorithm needs to be explained with more details as to _why_ we are doing the steps.<BRK>It would be useful if the authors could give the intuition to make the reader understand what the true solution means and the need of the geodesic interpolation. The paper would benefit from merging both explanations. I think this should be better explained. We do not know which hyperparameters are chosen (for the AE and for the new development) and how they were chosen. (“wrapped on S^2” a bit above the figure is not clear). is more sensible”. The structure of the paper does not allow a simple identification of the prior work from the method as Section 3 contains prior work and new developments. However, I believe that the paper would benefit from experiments on a larger number of datasets, in order to better understand on which type of datasets their method shows better performance. However, the authors do not define what they mean by well behaved.<BRK>The paper proposes a different kind of generative model that is composed of autoencoder in the bottom, and a standard distribution p(z), and a conditional kernel mean embedding defined by a collection of sample pairs (z_i, l_i). The distribution of the latent codes p(l) is modeled by the kernel sum rule that corresponds to the marginalization \int p(l|z)p(z) dz (ultimately defined by the collection of sample pairs). ##### ClarityThe clarity is low. In every aspect I find it more close to a latent variable model (there is a latent prior over the z space). Why is this a sensible baseline?<BRK>I found the geodesic interpolation part a bit difficult to follow and not clearly motivated. Could you go into more detail on its component parts and why they were included (e.g., what appears to be the importance sampling part)? What about the effect of the pretrained AE? The method appears to work, which in itself is very interesting as this means one should be able to construct generative models (a la VAEs) starting only from a simple autoencoder.
Accept (Spotlight). rating score: 9. rating score: 7. rating score: 7. rating score: 7. <BRK>The authors here proposed message passing via edge transporters. This step includes also an alignment of the tangent planes. The second crucial point is that equivariance imposes a linear constraint on the kernels $K_{Self}$ and $K_{Neigh}$. The related work section is comprehensive. The paper is unreadable without the appendix and somehow it would be better to make it self contained and move the experiments in the appendix.<BRK>This paper presents Gauge Equivariant Mesh CNNs. The method is motivated by the fact that graph convolutions can be modified for meshes to take into account the angular arrangement of local neighborhoods. The result is a Mesh CNN that is equivalent to GCNs with anisotropic gauge equivariant kernels. The authors identify the issues related to existing networks and devise a sensible approach. In general it would be great to have an understanding of the proposed transport operator. I thank the authors for conveying an analysis on this front.<BRK>The work presents a novel message passing GNN operator for meshes that is equivariant under gauge transformations. I would welcome, though, if computational efficiency would be analyzed in the work. The paper is well written. The line of reasoning needs to be gathered from the main text, appendix, and referenced related work. How does this interfere with the equivariance property?<BRK>The architecture is an elegant way to incorporate gauge symmetry on meshes and RegularNonlinearity addresses an important issue for equivariant neural nets. Though I would prefer more interesting experiments, they are sufficient to validate the design. It seems both this method and the other method contain different complexities and subtleties. **Minor Points:**  All of the citations in the paper use \citet, but it would be more readable to use \citep. How would this compare to the proposed method in the paper? if could be if and only if, correct? But then why only parameterize $K$ by $\theta_q$; why not also include the distance $r_q$? The authors are correct that non linearities have been a bottleneck for using equivariant neural networks with representations other than the regular representation.
Reject. rating score: 6. rating score: 6. rating score: 6. <BRK>We appreciate authors  efforts to add additional experiments results in Table 1 and Table2. However, the performance improvements are marginal (more or less 0.7) and speed of Graph Transformer is slower than transformer. how layer iterations are used in graph transformer? Authors should refine its main context to increase understanding instead of adding lengthy Appendix for us. 1.It provides a multi ordered graph MoG explanation for the representation generation of Transformer encoder. This such paper presentation and organization are not clear to understand. 2.The selection of the baseline seems unfair. As stated in Section A.5.1, position encoding can be viewed as a specific MoG because it builds edges between nodes. Therefore, methods that focus on position encoding should also be included for comparison. For your reference, Transformer XL[1] and [2] are two tailored solutions for this purpose. 3.It is unclear about the model’s efficiency, i.e., memory efficiency and learning efficiency. The authors should provide more discussions about the model complexity to improve the quality further. 4.In Figure 3, why position encoding is needed? More explanation is welcomed for this configuration. However, the readability can be improved by addressing the following suggestions. 1.The layer level iteration in Figure 2 is difficult to understand; it would be better to give a toy example with a real sentence for a better explanation. [2] Wang Benyou, et al.“Encoding word order in complex embeddings”, ICLR, 2020.<BRK>The paper has propose the layer aware graph transformer to enhance the ability of capturing heterogeneous information of the current models. The multi order graph structure is innovative and contribute to the expression of generating subgraphs. 1, it is not clear how the "unified explanation" is defined and delivered in the graph transformer model? MoG seems doesn t make a well defined explainability to reflect the quantitative model explanation. 4, It is not cleared that in the 2.2, whether it could be overlaps between SN and TN? Also the performance of the proposed model may highly be dependent over the quality of MoG construction, which may limit the generalization ability of the model.<BRK>Summary:The paper proposes a new multigraph architecture called Multi Order Graph to explain the representation generation process in neural sequence encoders (Self Attention or SAN based models). + Proposed Graph Transformer uses self attention and also attends over different order of subgraphs (low order, middle order and high order). This paper claims that with the proposed MoG explanation it is possible to model high order relationships such as Syntax and Semantics. However, there is no example provided for this. Overall, I would like to see the paper at the conference. The idea of modelling the relationships as a Multi Order Graph is certainly novel and can fuel further research. However, due to the current state of experimentation I am voting for **rejecting** the paper. I am willing to increase the score if the concerns are addressed by authors in the rebuttal period. I am sorry but I am not aware of this variation. It would be good if you could provide a reference for this particular variation of the model. It would be good to see some discussion on this in the paper. 7.I would encourage the authors to provide the implementation for the model proposed in this paper.
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. rating score: 6. <BRK>Summary The paper presents the first GPU capable library implementing the _"signature"_ and _"log signature"_ functions as well as their gradients. It introduces these transformations to a machine learning audience, as well as their recent uses in ML, then proposes algorithmic improvements that reduce the necessary computation. Arguments * A fast, differentiable implementation of an operation of interest, especially integrated in a major framework, is *highly significant* for the community, as it enables quick exploration of research ideas to incorporate it. * The paper is *clearly written*, and does a good job exposing the concepts of interest to an unfamiliar audience. Relevant literature is clearly cited with context. Clarifications The main missing part is the derivation and implementation of the gradients of these transformations for reverse mode automatic differentiation. The text glosses over it in section 4.4, but I would have appreciated if its implementation had been explained, similarly to the original transforms. Do the implementations benefit from the same algorithmic improvements, or other ones? What are the opportunities for parallelism? Additional feedback It would have been particularly nice to have an empirical evaluation of the removal of the new basis of the log signature function. I understand it is not the focus of this work, though.<BRK>Does it remain  pythonic ? Library papers are difficult to review, and in general reviewing them is a highly subjective process (far more subjective than reviewing in general). In the current form the paper doesn t do a good job advertising the signature and logsignature transforms as a must have in a researchers toolkit. As this is the first time many readers will learn about the existence of the signature/logsignature transform, I d like a more significant portion of the paper s content to motivation for why these transforms are important in machine learning, as well as useful applications. I think that this deserves more real estate than say comparisons to competing libraries, as well as the precise mathematical details, which could be moved to the appendix. Could the authors give a usecase of this? Indeed, usage in a wider deep learning application of these transforms with good results would make this library a bit of a  must have . Plenty of space is wasted in Sec.4 highlighting the contributions of the library.<BRK>Signatory is a library for calculating path  signatures . The performance seems to be a significant improvement on previous work. This seems to be stem from a mix of algorithmic modifications and engineering work. * How does performance compare with esig/iisignature in the batch size   1 case, where using AVX is harder? Although batching is common during training, the  batch size   1  case is important in applications. Is the difference that Signatory does kernel fusion for the signature and log signature, but iisignature only does it for the signature? Is Signatory  fit for purpose  for machine learning in the sense that either(i) the calculated signatures elements are within 5% of the true values, or(ii) when used in a machine learning context, test accuracy is equivalent to previous libraries? Other reviewers have questioned if ICLR is the right venue for a paper about a library.<BRK>The authors added a more application oriented benchmark, which makes the more convincing case for practical speedup of 210x. The NT library is an interesting counterpoint on the library front. I remain somewhat skeptical that the signature transform can enjoy wide applicability given the exponential scaling behavior, unless first and perhaps second order terms suffice for practical use cases. The main contribution is a software library for computing the signature and logsignature transforms taking advantage of CPU parallelism and GPU acceleration, and providing a reverse mode derivative. Some algorithmic improvements which yield substantial speedups to the computation are described. The paper provides a background of the signature and related transforms, motivating these with selected applications including continuous differential equations (relevant to continuous normalizing flows), sepsis prediction, handwriting ID, GANs, etc. Or perhaps it fits into the "implementation issues, parallelization, software platforms, hardware" CFP. The transform can be useful in neural differential equations, an area of some interest to the community, though exploring how is beyond the scope of the paper. This is "just" about the software. It would be feel more compelling to me if the software were offered as the artifact of an application paper, or at least if the benchmarking portion were on an application instead of the pure forward/backward passes of the signature transform.
Reject. rating score: 4. rating score: 5. rating score: 5. <BRK>This paper proposes a contrastive autoencoder approach that only requires small data to perform a multi label classification on the long tail problem. Strengths:  A straightforward method for self supervised contrastive label embedding prediction using in task data. This paper focuses on a challenging, noisy long tail, low resource multi label text prediction taskWeaknesses:  The key concern about the paper is the lack of comparison to the baselines. However, I still have issues with the evaluation and the clarity of the paper.<BRK>The proposed, dataset internal contrastive self supervised learning benefits in zero  or/and few shot learning settings. ##########################################################################Reasons for score:  Overall, my score is marginally below the acceptance threshold. 1.The paper provides a very in depth survey of recent work on pretraining research and a clear scoping of the problem to be addressed in this work against the other work. I only find that (5) in Figure 1 is a simple aggregation of binary classifiers, rather than contrastive optimization. ##########################################################################Questions during the rebuttal period:  Please address and clarify the cons above. Each training technique may have different levels of optimality.<BRK>Therefore, the proposed approach seems to be quite limited to the areas of multi class classification. The paper at least need to have these models as baselines for comparison. However, the prevalent self supervised training in many work (BERT, RoBerTa, to name a few) do not require labels, so I don t see why we really need to avoid that. However the paper does show that it is possible to do well on long tail zero  and few shot learning with only small data pretraining, so that is a nice conclusion derived from the experiments.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 6. <BRK>This paper investigates incorporating shape information in deep neural networks to improve their adversarial robustness. The second idea is to train a conditional GAN to reconstruct images from edge maps and use the reconstructed image as input to a standard classifier. If this is a separate paper, given the previous work [1] that already proposed this idea, the contribution of this work seems to be limited. This is not an excuse for not doing quantitative study; the authors already considered adversarial perturbations with magnitude as large as 64.<BRK>However, I have still some concerns below:  In summary, the paper is hard to follow and the writting is not clear, such as the detailed motivation of the proposed methods and the structure of this paper.<BRK>The two baseline methods do not have this under determination because their input is rgb images. 1.The assumption of this paper is that edge map does not change much under adversarial attacks. Since it is deep net and so it is fragile. So I am not sure how the proposed method work with deep net based edge detectors, on a somewhat more realistic image datasets.<BRK>However, the essence of robustness in this work lies in the BINARIZATION of the input (i.e., binarized edge maps) which is shown in the previous work [1] and need not necessarily attribute to the shape information obtained through edge maps. Summary: This paper aims to improve adversarial robustness considering the information about the object shape details with the means of edge maps. Final thoughts:The proposed method is clearly motivated.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. <BRK>The authors also extend this setup to work on computer vision tasks, where parameters are shared between different input patches. This does not, as far as I can tell, change the core results of the paper, but it is important context (e.g.helpful to know when evaluating  the change in accuracy due to regularization in Table 2). EXPLANATION OF RATINGWhile I think the core idea of ShapNets is very interesting and promising, I think the empirical results are currently somewhat lacking   in particular, I hope to see a clear use case where one would prefer ShapNets over alternative methods. I would recommend that the authors instead either explore: (a)  the "remove least k salient pixels" method from the FullGrad paper: https://arxiv.org/pdf/1905.00780.pdf (b)  the "digit flipping" experiments used in the DeepSHAP/DeepLIFT papers, or (c) (best) train their models on the BAM dataset and quantify interpretation quality accordingly: https://github.com/google research datasets/bam. I would also be curious to see how the contributions estimated by ShapNets compare to the true Shapely values for the computer vision datasets (Table 4 in the supplement seems to report this for tabular data only).<BRK>##########################################################################Summary:The paper proposes to incorporate Shapley values as latent representations in deep models. Overall, it seems to be a good idea to incorporate Shapley values into deep models and the proposed method seems to be reasonable. The empirical results have demonstrated the usefulness of the proposed method. The paper is also well written and technically sound. I have some comments as detailed below. The paper overcomes this challenge by forcing the active set of all Shapley modules to be size 2. Therefore, I am not quite convinced that the proposed SHAPNETs have satisfactory representation power. Have the authors considered a comparison of SHAPNETs with different active set sizes?<BRK>I ll start with questions about the method. Table 2, Figure 4 and Figure 6 show that the regularization works as intended, but this is a lot of space to prove this point; other results might be more important to include in the main text (mentioned earlier). However, I ll mention a couple lingering concerns. Is the ShapNet method unable to work on models that are not the linear combination of intermediate values (e.g., the classification probability after softmax activation rather than logit)? If the authors disagree, they may consider improving this aspect of the paper. Most research advocates for either the "interventional" [2] or "observational conditional" approach [1]. Perhaps the most important question about Deep ShapNet explanations is how similar they are to SHAP explanations (e.g., from KernelSHAP). To call these "SHAP values" is almost misleading because it silently changes one of the core aspects of SHAP. There s a lot of verbosity used to describe some straightforward ideas involving SHAP values. The "Shapley transform," the "Shapley representation" and the "Shapley module" are all based on the simple idea of concatenating SHAP values into a matrix/tensor.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 6. <BRK>The neural tangent kernel can be computed by a similar root finding problem as that in the deep equilibrium problem itself. Some experiments have been performed to compare the performance of deep equilibrium neural tangent kernel with that of finite depth neural tangent kernel. Overall I vote for rejecting. First, the motivation of considering deep equilibrium models is unclear to me. The authors should provide some further literature review. The theorems in the paper are lack of explanation. The paper has some grammar mistakes and misuse of words. Section 3, 1st paragraph: we simplify fully connected DEQs as DEQs. Section 3, 1st paragraph: In section 3.1, we show the NTK of the approximated DEQ using finite depth iteration...<BRK>This paper studies the neural tangent kernel (NTK) of fully connected neural networks with input injection (defined in the first set of display in Section 3.1), and the infinite depth limit of the NTK. The theorems derived in this paper are incremental given existing studies on NTKs. This paper also lacks a proper introduction to many concepts. For example, it is hard to understand what does DEQ NTK really means in the introduction. Given the above concerns, I don t think this paper is suitable for publication in ICLR.<BRK>2) By using the rooting finding ability of the DEQ models, these derived NTK kernels can be computed for both fully connected and convolutional variants. It would be natural to also include comparisons with DEQ models, as DEQ NTK (or CDEQ NTK) can be viewed as augmented model of DEQ (or CDEQ). Otherwise, it would be no point to use NTK on DEQs. This is also what the paper tries to claim. 4)  The DEQ NTK in the experiment is derived for FCNN.<BRK>This paper studies the double infinite width + infinite depth limit of fully connected and convolutional neural nets from an NTK angle, when input injections enter the picture. The techniques mix NTK techniques with Deep Equilibrium (DEQ) model techniques to directly compute the infinite depth limit of the infinite width limit of such neural nets. Overall, I think that these results are mathematically interesting and could lead in principle to practically useful insights, although this is not realized at this point.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 5. <BRK>This paper proposes a method of compressing neural radience fields (NeRF s) by learning mappings from latent codes to model parameters such that both distortion/reconstruction quality and the rate get minimized. The strong aspects of this paper include:(1) it addresses the valid problem of compressing NeRF models, which is particularly valid given that one usually trains one NeRF per scene. In terms of drawbacks, this paper can be made stronger by addressing the following points:(1) while model sizes are compressed significantly at almost no cost of rendering quality, there is no improvement in rendering speed. In fact, because of the additional decompression, rendering novel views might very well take longer than the original NeRF. Unfortunately, Fig.5 shows having separate NeRF s have higher rendering quality at comparable sizes as the compressed models;(3) it would be interesting to explain how this is related to meta learning. This also achieves similar compression effects, and may work better for the multi scene setup.<BRK>**Paper Summary**:Compressing 3D scene is an interesting problem to explore. The paper proposes to add entropy penalized reparametrization (Oktay et al.(2020)) technique into Nerf (Mildenhall et al.(2020)) and compress the neural network in Nerf. Experiments also show some compressing rates improvement with the proposed baseline, which I have some concerns in the weakness below. However, the multiple scenes experiments are not performed well to demonstrate the effectiveness of the proposed method (see below), so the overall novelty is not enough. A more valid baseline should be: receiver receives the training images compressed by HEVC  > decode the images  > receiver train a new Nerf on the decoded images  > run the trained model on the test set. In literature, Nerf model takes more than one day to converge, while HEVC is very fast for both encoding and decoding.<BRK>My concerns with the paper lie in two area. However, given the lack of this prior work, the solution feels far more like an engineering solution to allow NeRF to work well on cellphones then machine learning research. My second concern is the lack of comparison to other approaches to scene compression. From the way I view the problem you have proposed, you are assuming practically explicit 3D scene information as input and attempting to transfer this information between devices in as small a size as possible such that it images of the scene can be sampled on the new device in as high a quality as possible. The experiment I really want to see here is if this method is better then if the explicit scene information is compressed and rendered on the second device. My main concern here is that for simple scenes I am very sure that compressing or even transferring directly the 3D scene explicitly is a far superior, though there is probably some level of scene complexity at which your method overtakes it.<BRK>Summary:This paper proposes to compress nerf models with entropy loss, where instead of directly training nerf model parameters, it trains a new function F which takes some compressed information and decodes to the nerf models. Comments:The paper combines network compression and neural renderings, which is pretty interesting. Though it introduces compression to neural rendering, such a combination seems to be not very creative. While one can train such a network for nerf scenes, the network may get overfit to these scenes since the whole dataset has no more than 20 scenes. Since HEVS is a traditional video compression method while LIFF is also a traditional blending view rendering method. The authors are encouraged to compare with some neural network compression methods + neural rendering methods. Conclusion: Overall, I think this paper proposes an interesting idea and shows good results.
Reject. rating score: 2. rating score: 3. rating score: 6. <BRK>Pg3, describing CFR. This is not an ablation study   it is a hyperparameter sweep. That would have been a very important ablation study to have in this paper. Ideally, the authors would use the standard limit game, so that the results can be compared against other work. Recommendation and Justification:I suggest rejection.<BRK>This paper presents a new algorithm for regret matching that can be dropped into existing CFR variants. I would encourage the authors to run these algorithms for more iterations.<BRK>I have doubts that the experimental evaluation is done properly given stochastic nature of evaluation (see the 3rd question)Main questions: 	In Sec.4, the authors write “.. we propose Temperature Regret Matching (TRM), an efficient RM method ..”. In fact, I read the paper and did not find a proof of this claim.
Reject. rating score: 4. rating score: 5. rating score: 7. <BRK>The paper develops a method to learn a binary classifier based only on pairwise comparison data. The authors test their methods on four standard data sets (three MNIST variants and one more). Or which of two laptop computers is "more cat like" than the other? The authors must get some small amount of pointwise data, or they have some assumption that I missed, like the base label frequencies are known.<BRK>b) Another bigger problem is that the method ignores the PAIR nature of the problem; the method basically ignores the pairs and just treat list of $x, x $ as from different distributions. I have some doubts though:	a) The paper assumes a generative process of the pairs: Generally, it assumes a rejecting sampling process. The authors considers two kinds of methods. Minor Comments:	a) Second paragraph in Sec 3   the paragraph is a bit unclear to me.<BRK>I have a slight concern about the practicability of the assumed comparison model (Sec 3) which only provides examples like (+1+1), (+1, 1), ( 1, 1)? The theoretical proof does not go through without unbiased risk estimators, which in this is obtainable because of Thm 1 and 2, but these are very specific to the above assumption on the generative model? Also, can you comment on the tightness of the current bound say Thm. Overall I found the paper is easy to understand and well structured. Two interesting follow up questions:  It would interesting if the authors can add some comments about the possibility of extending the work to a multiclass classification setting.
Reject. rating score: 4. rating score: 6. rating score: 6. <BRK>There are 4 tasks proposed by the paper, inspired by Animal AI Olympics. The proposed method outperforms those baselines. ##########################################################################Reasons for score:From the text it seems like BC baseline doesn t have access to rewards. If that is the case, the baselines are very weak   they either have access to demonstrations only or sparse rewards only. Nice research direction: combining demonstrations with sparse rewards. and warrants more extensive comparisons. Introducing new tasks requires some discussion on why the established tasks are unsuitable for the goals of the paper.<BRK>It s always helpful to define the origins of the chosen nomenclature for cases where it is not obvious. ## SummaryThis paper looks at problems that have sparse rewards, are partially observable, and havevariable initial conditions. Instead, this work proposes to use a PPO+D, an on policy method (PPO)for a policy with memory (GRUs).<BRK>There are a few missing commas, like "In our approach,", bottom of the first page. Overall, the paper is very "short & sweet" in that it s not a groundbreaking new technique, but a small change to PPO but it s well explained, and the results that _are_ in the paper are good. #### Weaknesses  The main problem I have with this is actually the lack of further experiments. On the flip side of that, Figure 1 is a bit redundant.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>### SummaryThis paper proposes a novel top down program synthesis for programming by example which combines concrete evaluation with neural embeddings. The paper is well written and was a pleasure to read. I do think that the authors should include some experiments with simpler tasks and more baselines, to give a better sense of how their method compares to prior work in the settings where that prior work is applicable. But overall, I think this paper is good and deserves to be accepted. Is there still value in using blended semantics in that domain, or is PCCoder better in the cases where it can be applied? It s interesting that neural semantics can still be used even when there are concrete values. It would be interesting to see some example programs that the method is able to produce, perhaps as part of the appendix. I m also curious whether you have any intuition for what the neural semantics "mean" for a program with holes; have you done any analysis of this?<BRK>Sometimes you need to choose a code idiom (a loop) before you choose what goes into the loop. * (+2) Performance seems to be good for interesting and non trivial DSLs. 1.I like this work and the main ideas are intuitive and well described. I see some of that detail in the appendix, but information such as dataset size is essential in understanding results. It can t be in the appendix only. How many are there? 1.Page 6, how was test construction exactly? Your introduction would be much more effective if you explained briefly the semantics for your DSL (even though it s relegated to the appendix). Is `1`  the height from the bottom? REPL could do just fine in such a setting (in principle) but it would have to build the program bottom up, with all of the arguments of a higher order function, before it composed them into an application of the higher order function. "We do not expect that our approach would outperform the REPL..." Why not?<BRK>As such, I am increasing my score. I don’t feel the problem domain was properly motivated (or maybe I just missed it). I think if the authors perform a more rigorous analysis of their system, this will result in a strong tier 1 publication. In its current form, I cannot argue for its acceptance at ICLR. Let’s take Hoppity (ICLR 2020), which looked something like 300k JavaScript bugs and was able to synthesize solutions to around 10,000 of them out of something like 36k. In this paper, the authors note that the recent prior work in program synthesis using traditional formal methods as well as non traditional techniques such as machine learning are still in the early stages of exploration and can be improved upon. However, he’s not the only one publishing in the space. Based on the current knowledge I possess, it seems like this is an interesting and novel way to approach the problem. However, it’s a bit hard to tell because the authors have overlaid the legend on *top* of the REPL curve in Figure 7, so one can only speculate about certain aspects there. I discuss some of these below.<BRK>I am not clear about the scope of this work, and whether it will generalize beyond these specific domains and bechmarks. The examples and (some) benchmarks include loops. A  loop  is probably treated as any other function. A more convincing evaluation would be to have benchmarks where training is restricted up to loop bounds, say k, and the test set contains programs with loop bounds m > k.The functional list processing benchmark uses higher order functions. The other problem is that of potential non termination. It is easy to synthesis a non terminating program even with finite loop bounds, for instance, if the loop counter is decremented in the loop body and never hits the loop termination condition. The paper should talk about this issue and how it can be mitigated. The semantics are not fully formalized. What is the initial state of a program? But doesn t it implicitly need the position of the cursor where the column should be built?
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 5. <BRK>The presented study tackled the problem of evaluating different metrics for generating similarity based explanations. To this end, the authors used three tests to evaluate four types of metrics on several different datasets. Overall, the methods adopted by the study are technically sound and the paper is well written (though with a few typo errors).<BRK>The authors investigate which relevance metrics are desirable for explanations that are based on extracting similar instances as evidence to support a model prediction. While the approach presented and the two additional principles are quite simple, it is nice to see a comparative study on explanation metrics. The authors conduct an empirical evaluation on two image datasets, two text datasets and one tabular dataset. Why this data specifically?<BRK>This works focuses on similarity based explanations, which means providing similar training examples along with the predicted class. One test is taken from the literature and the two others are proposed in the paper. I really like that the authors find an explanation for those results, based on geometrical properties of the measured objects. Besides, they use this finding to correct existing measures. Third, the randomization test relies on a unique random model that is confronted to the learned model. Finally, I was puzzled by Fig.6.<BRK>#### Summary: This work provides an empirical evaluation of similarity metrics used in example based explanations methods, where the goal is to provide decision support examples in the training set for a black box model s prediction. #### Recommendation with reasonsWhile I like to motivation behind this work and it s potential usefulness, the validity of the proposed tests is not clearly addressed.
Reject. rating score: 2. rating score: 3. rating score: 3. rating score: 4. <BRK>The paper claims to introduce a new quantum machine learning framework called GenQu. In fact, the same basic ideas are so well known in the community that they are described in detail as usage examples for popular quantum computing platforms such as Qiskit and IBM Q. The only remotely nontrivial part of the paper is contained in Section 4.2 about Quantum Deep Learning, where the authors consider the MNIST data set.<BRK>##########################################################################Reasons for score: This framework is not new and has been widely adopted in the quantum machine learning community. It is unclear to me what is being proposed by this work. It is not scientifically correct to claim the proposal of a new framework "GenQu" when this has already been widely adopted in the quantum machine learning community.<BRK>The coding scheme proposed in the paper is not novel and not even state of the art. [2] Broughton, Michael, et al."Tensorflow quantum: A software framework for quantum machine learning." This paper is more like an entry level tutorial, rather than a technical paper. Could you refer to the paper from which you get this baseline?<BRK>I think this points to a strong parallel between neural networks and quantum systems but isn’t really a new concept. The results of the paper really are proof that the basic functionality is there, but not really a proof of concept in a general way.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 7. <BRK>### StrengthsThe paper and the topic are both interesting. In both cases, the experiments should include comparison to state of the art approaches in those learning problems. How can the proposed work be used in practice for that goal, since we do not know in advance what is the quality of each dataset, or how well does an estimator perform? If there exist work on that domain that is specific to the problem of combining data from different sources, please provide a more specific reference. ### Overall evaluation: The paper is interesting and tackles an important problem. However, the answer provided on my comment about the distinction between excess loss and absolute loss is not sufficient.<BRK>This work studies the problem of predicting model performance with more training data when the data are collected from different sources. Update after discussion: The difference between excess loss and absolute loss is an issue I overlooked. This is a key step in the proposed algorithm when fitting the predictor. The paper has not explained it. In the experiments, if the authors use the estimator trained with full data as the oracle to compute the excess loss, that would invalidate the practical usability of the approach because the goal was to predict the performance without having full data to begin with.<BRK>In this paper, the authors proposed a model to predict the model performance given the sample size n and the composition of the source of the data, q. They argued that the excess test loss should be separable in n and q. The paper is interesting and well presented. The proposal could be useful for some practitioners. The baselines in the experiments are all relatively weak. How many lambda s should we use?<BRK>Concretely, given a model trained on $n$ samples from a mixture of $K$ distributions, can we predict the excess risk of the model on data drawn from a different mixture? The paper and proofs read well and don t appear to have correctness issues. Cons:  My main complaints with this paper is the lack of limited baselines and the limited scope of the experimental results. Experimental scope: Scaling laws like the ones presented in this paper are useful and interesting in so far as they capture some "universal" phenomenon. In each of the settings considered, the experiments focus on one model, e.g.pretrained BERT for question answering. Overall, I very much enjoyed reading this paper, and it offers several avenues for further inquiry.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 5. <BRK>The method proposes a selection based approach to address one of the weaknesses of template free approaches   the predicted reactants may be commercially unavailable. However, I am afraid the proposed approach is an overkill. If we only select reactants appeared in the USPTO database, the model cannot generalize to new reactions which involves new reactants not in the USPTO database. 2.Moreover, in multi step retrosynthesis, you are allowed to make new intermediate compounds from commercially available compounds in order to make your final product. I am concerned that the neural network based ranking will run very slowly and cannot scale to larger sets of compounds. 4.The proposed method is quite straightforward, with limited technical novelty in my opinion. 5.The result looks very strong on the USPTO 50K test set. 2.The approach has limited novelty to ICLR audience. ### Post rebuttalI would like to thank the authors for their valuable response. I believe this paper can make great impact if submitted to a chemistry journal.<BRK>This submission describes an approach to single step retrosynthesis based on contrastive learning that selects reactants that can be used to synthesize a target product in a single step. The stated contributions are (1) an approach to retrosynthesis that is constrained to only select “available” starting materials; and (2) a novel contrastive learning scheme with hard negative mining. I take some issue with the premise of constraining retrosynthetic recommendations to an enumerated list. That is why in the multi step evaluations, the authors rely on the Transformer model to propose intermediate structures. Appendix D also suggests that in the pathway search experiments, knowledge of the routes in advance was required to construct the set of all starting materials to select from. The model has access to a restricted list of possible starting materials of which very few are likely to be plausible precursors for a given product. While the contrastive learning approach is clever, this work uses a contrived formulation for retrosynthesis that is not applicable to multi step planning and the experiments do not support the conclusions drawn.<BRK>RETCL enumerates all of the candidate molecules (all US patent dataset, all 671k) based on selection scores computed by graph neural networks. The cosine similarity between products and reactants are used to design scores, which is later used for training. “Selection based algorithm” tends to achieve overly optimistic results due to this reason. It is great the paper shows that RETCL generalizes well to unseen templatesHowever, if we select upon an existing dataset (though very large). How does RETCL possibly yield totally unseen reactants (i.e.not in any existing dataset)? How does these probabilities be approximated by a mini batch of reactions? The batch statistics are very different from the C (all US patent dataset, all 671k)<BRK>This paper poses an approach to retrosynthesis that addresses the challenges of (i) availability of reactants and (ii) generalization to unseen templates. Their reactant selection framework RetCL uses GNNs to calculate selection scores for all candidate molecules. The authors provide a good summary of related work in this area. However, the model is never challenged by reactions for which the reactants are not present in the candidate set, and it is completely unclear how the model would perform in this scenario. This is important, because to tackle the chemically relevant retrosynthesis problem it is exactly necessary to solve reactions where the reactants may not be present in the 671,518 reactants present in USPTO 50k. There are other arguments that the authors could also make to defend this point in addition to these experiments   but the current state in which the issue is ignored is not satisfactory. The description of the contrastive training approach is clear and coherent. The addition of hard negatives to the batches to improve learning is interesting, and the results of the ablation study speak to the important role that it plays.
Reject. rating score: 2. rating score: 3. rating score: 4. rating score: 4. rating score: 6. <BRK>The paper proposes to use a set of input examples, x1 to xn, having a common label y, and use them together for better classification. And use these shared label examples as additional information during model pruning. This is insufficient. Either the whole motivation on IR aspects can be removed or relevant experiments and approach be proposed. At multiple places, I am either lost or confused on what is the problem that the paper is trying to solve. If you have 10 instances to classify, instead of classifying them independently, the paper is trying to classify them together. 3.Incorrect or Insufficient assumptions:The paper makes a lot of strong assumptions, which are not practical:a. What if multiple instance, x1 to xn of the same class label is not available ? I do not agree to this assumption.<BRK>The experiments section shows the proposed method performs well and achieves a high compression rate. The data model used in this study is different from the common classification problem. This paper assumes that n data points give side information with the shared label batch, referred to as "n tuple." In general, classification problems have labels independent of other data points. The shared label model should be motivated very well. It would be more convincing if the authors can add results for the proposed CNN LSTM network. Graphs are too small to understand<BRK>SummaryThis paper introduces the problem of shared label prediction   the problem of classifying the (common) label of a set of points conditioned on the knowledge that they all share the same label    and suggests various methods that take advantage of side information to solve it. This is then somewhat more rigorously defined as the “shared prediction problem” in Sec.1.2 and set as the target problem that this paper tackles. However, given the lack of clarity in the exposition   e.g., in motivating and defining the problem (and its connections to pruning) and conveying & contextualizing the paper s contributions  , I am unable to fully understand and appreciate the significance of this work. The definitions for the variables used are ambiguous and defined way later after being used.<BRK>The current set of experiments do not address this key factor that would occur in any practical setting. This is especially confusing given that LSTM based solution is found to be the best empirically. Each proposed method is compared under various compression ratios. I would encourage the authors to make their future work stronger by grounding the work in a real problem. If this problem is solved as is, this work can be very impactful. The paper is missing a clear description of real world applications. The simplifying assumptions made at the moment weaken the problem statement. However, the actual task the rest of the paper focuses on is materially different wherein n examples are known to share the same label. This is not clearly described at all and required many readings.<BRK>This paper discusses empirically a workflow about how to compress network architectures with side information. The side information defined in this study is provied by the training data instances that share the same class labels, aka the problem of shared label prediction. This work defines a set of benchmarks measuring efficiency of shared label prediction, including relative information and information gain. These two metrics are used to measure how much information can be learned by the compressed neural networks over the data instances sharing the same label, given the compression ratio and the name of training instances 2. Thus the average same label loss should be noisy.
Reject. rating score: 4. rating score: 4. rating score: 6. rating score: 6. <BRK>This paper aims at novel policy seeking which incorporates curiosity driven exploration for better reinforcement learning. If is,  why use t? Can the author provide some insight on how to tune this.<BRK>Summary:This paper attempts to solve the problem of seeking novel policies in reinforcement learning from a constrained optimization perspective. Details:The idea of formulating the problem from a constrained optimization perspective is interesting.<BRK>This paper proposes a novel constrained optimization based method, to optimize the expected return as well as encourage novelty of a new policy in contrast to existing policies. The first contribution of this paper, is proposing a novel metric to measure the novelty of current policy in contrast to existing policies.<BRK>* It can be more interesting if some visualization of hopper policy diversity is included. Based on a diversity metric defined on policy divergences, the paper employs two constrained optimization techniques for this problem with some modifications. (2018).Diversity driven exploration strategy for deep reinforcement learning.
Reject. rating score: 4. rating score: 5. rating score: 9. <BRK>I like the idea of applying the siamese paradigm to authentication however the paper offers limited novelty for a representation learning conference. There are zero baselines (no siamese) compared. The proposed architecture seems to be very accurate, as reported in the experimental section. 2.This paper provides comprehensive experiments, including both qualitative analysis and quantitative results across different datasets, to show the effectiveness of the proposed framework. There seems to be no new component proposed that is tailored to biometric data. 2.I would suggest thinking about the contribution to the ML community.<BRK>The authors argue that the latter requires more specialised hardware and is thus not easily scalable. Related work is well covered to the best of my knowledge, making reference to one of the oldest instantiations of such model for signature verification. This disposes them to problems such as limited scalability, lower performance and high latency. The aim perhaps of this paper is to present a baseline NN based approach for user verification/ biometrics. As a main track ICLR submission I find it lacking in novelty, reach and research contribution.<BRK>The proposal is evaluated on three different dataset, collected ad hoc for this work, showing a performance up to >0.9 acc. RecommendationsAlthough it is covered in the section 3.1, given the scope of the conference and its relevance for the reproducibility of the results, I’d ask authors to include a clear and detailed explanation of the feature vector employed. ContributionsMy recommendation for this manuscript is a strong accept. The paper is well structured, with a clean format, very well written and easy to read.
Accept (Oral). rating score: 7. rating score: 7. rating score: 7. rating score: 7. <BRK>This paper introduces a Transformer based image recognition model that is fully built on the Transformer layers (multi head self attention + point wise MLP) without any standard convolution layers. The results are impressive and interesting. In this regard, further analysis of this issue would improve this work. This needs to be clarified. As I noted above, since the Transformer in this work is permutation equivariant, it can also be seen as translation equivariant.<BRK>This paper explores the Transformer architecture for image recognition. Some of the visualizations are also insightful. My major concern is that the pre training conducted in this paper is fully supervised. It would be great to explore self supervised pre training in the future as also pointed out by the authors. A hybrid approach seems to be a good work around to handle this case.<BRK>## SummaryThis paper studies to adapt transformer model for image classification task. All it needs is to cut input images into patches, and reshape it as an input sequence. 2.The experimental section has clearly demonstrated the pros and cons of the proposed model. 2.$z_0^L$ on page 2 bottom seems to be a typo. Does this mean that the images get resized to fixed resolution before cutting into patches?<BRK>Clearly, the paper has the potential to re ignite another wave of excitement for exploring this great learning model on different computer vision tasks. The paper delivers a strong message " transformers can be a more powerful, yet efficient,  compared to the SOTA CNN backbones for image recognition tasks if there is a large enough dataset available " and the authors prove this claim by performing comprehensive experiments on several large scale image recognition benchmarks. Otherwise, a similar message (replacing CNN with transformers/attention) has been attempted to be verified in a few earlier works (as acknowledged in this paper as well).
Reject. rating score: 4. rating score: 6. rating score: 7. rating score: 7. <BRK>This paper studies the potential bias in deep semi supervised anomaly detection. The empirical results also show the potential impact of this bias on several anomaly detectors. Overall, the paper is well written and studies an important problem in anomaly detection. A number of theoretical and empirical results are presented to justify the arguments. This is a fundamental assumption in supervised learning like classification. By contrast, there can be novel types of anomalies that can be very different from the known anomalies.<BRK>This paper establishes the finite sample rates for estimating the relative scoring bias for semi supervised anomaly detection. The paper only evaluated the convergence of the relative scoring bias/FPRs on the simple synthetic dataset. Specifically, this paper proposes the relative scoring bias, which is the difference of TPRs of two anomaly detectors when the FPR is below a certain value, to model the effects of a biased training anomaly dataset. Pros:  Investigating the effect of training anomaly datasets on anomaly detection is important and useful in anomaly detection studies.<BRK>+ The paper has a clear structure and is easy to follow. Deep anomaly detection with outlier exposure. Using self supervised learning can improve model robustness and uncertainty. To my knowledge, this is also the first instance of a finite sample complexity bound on the scoring bias for this setting. The Clever Hans effect in anomaly detection. In this regard, I think the paper also covers important ground for future analysis and towards building semi supervised models that are unbiased.<BRK>#### Problem StatementThis paper considers the effect of bias in anomaly detection. The anomaly detection setup is this: A model is trained on a $1  \alpha, \alpha$ mixture of normal and anomalous examples . 2.Bias from the data, where the set of anomalies available at training is not representative of the test distribution of anomalies. This is more of a domain adaptation problem. The paper is easy to read and scholarly. 2.The experiments related to bias in scoring resulting from bias in the training data are interesting.
Reject. rating score: 5. rating score: 6. rating score: 7. rating score: 7. <BRK>Linguistic information is incorporated by biasing attention heads to line up with dependency (or other) structures. Positives about the paper: it s an interesting experiment to try, and an important direction of work. As it stands the paper presents the approach and results, with little inspection of why improvements are seen. I would like the authors to go much deeper with the analysis. Is the new model much more sensitive to long range dependencies, as found in syntactic structures? Answering these questions will be challenging but would add a lot to the paper. * Most importantly, the evaluation metrics are unclear. This is a terse description of a critical part of the approach, and I can t make sense of it. The matrix $D_{fr}$ is the output from BERT. In the original Gauthier and Levy paper they appear to use metrics in addition to MSE.<BRK>A linear classifier from the final layer of BERT s embedding (mean pooled) is then learned to the fMRI data. Assessment: this is a nice paper that investigates an intuitive method of incorporating syntax based, structural soft attention constraints into Transformer encoder models for language. The authors also evaluate the effect of fine tuning on targeted syntactic evaluations from Marvin & Linzen; the results here are not particularly conclusive. However, there are a number of technical questions that are left unclear in the submission, and some of the results are cause for some concern. I would guess that the high perplexity comes from poor prediction of the proper nouns in the Harry Potter book chapter. Maybe there needs to be some amount of fine tuning of the models to the domain of the test set corpus. Overall, the paper needs more clarity on why it is only the Wehbe2014 dataset where the perplexity is so high and the fine tuning affects decoding performance so much. 4) What words are pooled over for the Wehbe2014 analyses   the four words in the 2 second window? And the difference in mean decoding performance for DM finetuning is barely visible. For Wehbe2014, UCCA data yield the worst decoding accuracy but yield better perplexity than DM data, which yield decoding accuracy only slightly worse than the UD data.<BRK>This paper tests whether fine tuning large pre trained language models(LMs) with structural information can increase the correlation betweenthese representations and the representations of brain activitymeasured while processing the same stimuli. The injection of thestructural information is done through fine tuning of the pre trainedmodel by "guided attention", which makes use of binary relationsbetween the words according to three different syntactic or semanticformalisms. Authors predict themodel representations from the "brain representation" (this seems tobe based on earlier studies, but I did not verify). Hence, an alteration of the model representations thatsimplifies them may result in better predictions, and hence, highercorrelationsExcept the above, I have some (mostly minor) comments:  I would be happier with a bit more explicit discussion of the main  results. The effect on two different  data sets (also means representations at different levels/units) are  quite different   not allowing a clear conclusion. A few language/typography issues/suggestions:       I am not sure about the ICLR guidelines, but avoiding citations      in the abstract is a good idea (abstracts should stand alone).<BRK>The authors conduct experiments with the BERT model and two fMRI datasets and show that including linguistic structure through fine tuning can improve brain decoding performance. The paper would be improved by experimenting with language models other than BERT, as it is not clear at the moment whether the produced results are generalizable to different language models or are BERT specific. For example, additional experiments with AlBert, distilBert and RoBerta would provide additional insights on the effect of size of the model, in terms of the number of parameters. It would also be interesting to read a discussion of semantic analysis, as currently the paper concentrates the most on syntactic formalism as represented in both BERT and fMRI data.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 7. <BRK>Pros:1.The paper performs the first security analysis of defenses and designs adaptive attacks, which demonstrate current defense designs are vulnerable. 2.The paper proves that sorting based parametric pooling operations can improve the model’s robustness. 3.Based on the existing problems, a deep symmetric pooling operation, DeepSym, is proposed. Cons:1.DeepSym seems to be simple and slightly lacking in novelty. Therefore, the computational cost (both time and space consuming) of the DeepSym should be discussed in the paper.<BRK>This paper first points out the drawback of state of the art defenses for 3D point cloud classification, via designing adaptive attack. Based on such analysis, the authors propose a deep symmetric pooling operation which can enhance the adversarial robustness for adversarial training. The presentation of the paper is clear and easy to follow. However, there are still some concerns that need to be solved.<BRK>This paper studies adversarial robustness of point cloud classification models. In addition, this paper proposes a DeepSym operation, which is built on top of both the sorting based pooling and the parameterized pooling. Pros:1.Robustness of point cloud classification is an important problem to study. 3.Empirically, this paper shows improvements under different attacks. Cons:1.My biggest concern of this paper is its novelty. The sorting based pooling and parameterized pooling layers are not new and have been studied extensively in existing papers. Can the authors state the novelty/difference between this proposed method and existing ones?<BRK>The paper addresses the problem of adversarial robustness in 3D point cloud representations. The authors then propose to use adversarial training (AT) to improve the robustness. The paper claims contributions in:  Demonstration that current defenses do not provide real robustness. A new sorting based pooling operation, DeepSym, that works the best with AT against adaptive attacks. I agree with the authors  assessment of the paper. The proposal of DeepSym is somewhat novel, but not sure if sufficient for a publication in this venue. Clarification:  I imagine that the adaptive attacks are white box attacks? Conclusion:The paper provides interesting insights and strong experimental results. Although the novelty is slightly lackluster, I would be happy to see this paper at the conference.
Reject. rating score: 4. rating score: 4. rating score: 6. <BRK>Short summary:The paper introduces a promising new method, hierarchical nonnegative CP decomposition (HNCPD), as well as a training method for the HNCPD, neural NCPD, for topic modeling problems. Further, it is not shown how the derivative of the argmin function with respect to $\boldsymbol{A}^{(l)}$ is formed, which is later on used for the backpropagtion. However, the present experiments only utilise rather small uni modal data sets (for instance a selection of 37 forest images). In section 3: presidential election  > United States presidential election  In section 3: combines the the rank 8  > combines the rank 8   In section 3: we display the the results  > we display the resultsOverall Evaluation:The paper is well and clearly written, the significance of this work, however, needs to be further proven by experiments. The originality seems moderate, as already existing concepts of neural NMF and Hierarchical NMF are applied to NCPD.<BRK>Summary: In this paper, an extension of nonnegative CP decomposition called hierarchical nonnegative CP decomposition (HNCPD) is proposed. A new algorithm is proposed. S2.Experimental results with real data are reported. W1.The formulation of HNCPD is not new. Cichocki et al.(2007) proposed a more general form of HNCPD; see Eq.(13) of the following paper. So the technical contribution of this paper is summarized to the development of neural NCPD, but it is a direct extension of Gao et al.(2019).````Cichocki A., Zdunek R., Amari S. (2007) Hierarchical ALS Algorithms for Nonnegative Matrix and 3D Tensor Factorization. For example, the pictures in e.g.Fig.4 seem to tell us some interesting information, but how can we use that? A similar inconsistency is observed in Fig4, Fig6, etc. The interpretation of experimental results is unclear.<BRK>SUMMARY:This paper presents a hierarchical nonnegative CP tensor decomposition method. In the experiments, do you use a combination of the loss functions in Equations (11) and (13) (e.g., C+E), or just one of them? The topic is interesting and should be of interest to the ICLR community. While this may be somewhat incremental, it seems like the authors are doing something that hasn t been done before. For the discussion in Section 2.2 about the derivatives, it would be a good idea to let the reader know that there is a more in depth explanation available in the appendix. In particular, in the discussion on approximation in Section 2.1, it is not clear if this idea is used in the implementation, which might make it difficult to replicate the results. Could you perhaps add an explanation in the appendix or point to a relevant reference? My main issue with the paper is that there are a few portions that are currently difficult to follow. What does $z$ here mean? It seems like the authors combine existing ideas (hierarchical and neural NMF, NCPD) into a new method. Could they be made brighter? #######################Update:I thank the reviewers for their responses. What do you mean by "relationship"? However, I still think Section 2.1 in particular is difficult to follow.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 5. <BRK>There has recently been an interesting line of work of using learning to optimize policies related to placement and scheduling of the neural network computation graph outperforming tediously hand crafted heuristics. The proposed paper would be a nice extension along this line. The paper overall is clear and easy to follow.<BRK>The paper indeed tackles an important problem that can affect the overall performance and efficiency of the hardware. Another related question comes from how this work relates to the optimizations of the dataflows [1,2]. Last question comes from the baselines. Overall, I have enjoyed reading the paper and I find the ideas in the paper interesting. How are the results affected by these dataflows?<BRK>This is important since many of the efforts in this domain remain fragemented and difficult to reproduce. The paper is well written. Overall:I felt that the paper has some interesting ideas but needs more experiments. Placeto: Efficient progressive device placement optimization. A hierarchical model for device placement.<BRK>The paper describes a machine learning method for mapping computational nodes in a neural network onto different levels of the memory hierarchy (DRAM, LLC, and SRAM) to minimize latency of inference. Overall, the paper was well written, targets an impactful problem, and the reported improvements (28 78% over native compiler) are impressive. It’s also good that they compare against PG and EA alone as a form of ablation, given that their method is effectively a combination of these two.
Reject. rating score: 5. rating score: 6. rating score: 6. <BRK>############## Summary ##############This submission draws a connection between deep representation learning and continual learning. This is not mentioned at all at any point in the paper up to this point. The authors use very simple toy experiments and theorems on linear models to motivate their claim that learned representations should be very different across different classes. Updates during discussion  I have updated my score from a 4 to a 5 based on the authors  response. What s the point of it? In Theorem 1, the authors claim that points close to the decision boundary are likely to yield negative inner product of their gradients.<BRK>This paper analyzes the relationship of gradient diversity to the performance of continual learning systems, the analysis inspires a novel loss which shows some improvement in the continual learning setting. The authors should either greatly expand the empirical analysis (in the non linear setting) of their claims on intra and inter class variability in CL  and/or make the experiments of the DRL method more convincing and varied in scope The observations regarding gradient diversity might be the basis of future more effective methods.<BRK>This paper presents a novel way of making full use of compact episodic memory to alleviate catastrophic forgetting in continual learning. Authors gave insightful analysis on the influence of gradient diversity to the performance of continual learning, and proposed a regularization that connects metric learning and continual learning. However, there are still some issues to be addressed as below.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 5. <BRK>The paper proposes a parameterized graph shift operator (PGSO) as a replacement of fixed, hand picked GSOs for application to GNN architectures and the experiments illustrate that PGSOs automatically adapt to the regularized GSOs for different settings. Pros: The paper is well written and easy to follow. The theoretical and empirical spectral analysis of PGSO reveals its applicability to many existing GNN architectures. The experiments are sufficient to illustrate the utility of using PGSO over a constant GSO for different settings. Do the authors mean  presence  and not  absence  of an edge in Remark 1? The evolution of accuracy with the number of epochs for some of these results will help evaluate the cost of learning the parameters of PGSOs.<BRK>They consider a family of GSO (that they name PGSO) based on seven scalar parameters, and show that it includes most commonly used operators such as the adjacency matrix or the laplacian. Finally, some empirical results are provided demonstrating the PGSO as a drop in replacement for standard GSO in GNN architectures. The paper is overall well written, and proposes a method which could be of interest to practitioners. On the other hand, the paper has some minor weaknesses, in that the proposed parametrization is redundant with some existing techniques (e.g.$m_3$ corresponds to residual connections, which are widely used), and, given the simplicity of the proposed method, a more comprehensive empirical evaluation could be beneficial. Additionally, the proposed operator is still restricted to 1 neighborhoods, and hence cannot by itself solve the expressiveness problems encountered by graph neural networks. Perhaps an informative experiment could be to initialize the PGSO at different commonly used GSO parameters (e.g.adjacency, Laplacian, normalized Laplacian), and observe how such initialization affects performance. Overall I feel that this paper presents a neat idea that could be of interest to some people in the community, and I have modified my score from 6 to 7. It would be great for the authors to discuss the importance of initialization, as in particular, it seems to me that the sign of $m_2$ can never change  (from its initial value), indicating perhaps that practitioners should try initialization at either and select the better performing model.<BRK>##########################################################################Summary:The paper proposes a parametric form for a matrix representation of a graph to be used as a building block within graph neural networks (GNNs). In essence, people use different normalized versions of the adjacency and Laplacian matrices within GNNs. This, of course, can be obtained with other conceivable parametrizations. The authors try to justify this by saying that (1) is the "most general affine form of the adjacency matrix", but this does not seem to be a rigorous statement. What is the space of all affine forms of matrices? The authors mention "without having to worry about complex values", worrying in what sense? Why would it be difficult to implement the examples at the end of page 4 if the eigenvalues would be complex? Why are these bounds useful? #########################################################################Some typos: (1) In Remark 1, "absence of an edge" should be "existence of an edge". #########################################################################Edit after author response: We thank the authors for their response to my concerns and those of other reviewers. I have updated my score from 6 to 7 based on their changes.<BRK>This paper describes a parameterized family of "graph shift operators", defined for any graph on n vertices as an n x n matrix where the i,j th entry is 0 whenever edge (i,j) does not appear in the graph. The paper studies some spectral properties of the parameterized family, and experiments with using them as components of graph neural networks. This paper appears to me to be a hammer in search of a nail. I suspect much of that relates to the lack of clarity in the problem. However, the writing is also unfocused at the paragraph level. In short: this paper needs substantial work before it s ready for publication.
Reject. rating score: 4. rating score: 4. rating score: 7. rating score: 7. <BRK>##########################################################################Summary:The paper presents a framework for deep reinforcement learning that is motivated by causal inference and with the central objective of being resilient to observational interferences. This intervention on the mechanism of $x_t$ happens whether we obtain $i_t$ and train the DQN with it or not. To be clear, I m not questioning the significance of using the interference labels in the training, but rather the causal story and formulation behind CIQ. The proposed framework appears to be sound and the experimental results show superior performance in comparison to other baseline RL methods. ##########################################################################Cons:The causal component of the proposed framework is not well explained. Such knowledge of a variable in the CGM does not account for an intervention. ##########################################################################Questions during the rebuttal period:It would help if the authors can clarify the issue raised in the "Cons" above regarding the clarity of the causal component and its central role, as claimed, in the proposed framework. However, the additional discussions in the paper are still confusing and raise soundness concerns. It is not clear why this alternation between the two approaches is employed.<BRK>The CIQ agent is shown to learn faster and more effectively when compared to a number of baselines on a selection of OpenAI Gym tasks that are modified to include various types of observational interferences. While this paper presents a method that is shown to perform well empirically in the setting is aimed to tackle, I cannot recommend it for acceptance because (i) almost no motivation or intuition is given for the architectural choices that seem to be key to the performance of the agent (in particular the switching mechanism between Q networks), and (ii) the characterisation of the agent as performing causal inference, which is the key message of the paper, is confusing in a number of ways. Concerns:	•	There is little to no motivation given for the architectural choice of having the interference label modulate a switching mechanism between Q networks, which based on the comparison to baselines and the ablation studies in Appendix E.3 seem to be key to the agent’s performance. •	The causal graph drawn in Figure 2a seems to contradict the experimental setup described in the text: 	◦	(i) The interference label i_t is stated to be sampled from a Bernoulli process but in the causal graph drawn in Figure 2a, there is an arrow from the latent state z_t towards i_t; in what way does the latent state affect the probability of interference? This is directly contradictory. •	Section 3.3 describes a toy example that is used to show how being provided interference labels during training can lead to better sample efficiency in learning the reward distributions for each of two states (one of which is occasionally subject to interference that switches the observation to the other state). It’s not clear here whether the message is that the CIQ agent performs better due to better sample efficiency or due to its ability to infer the latent state at test time. In a real world setting, this could be useful.<BRK>Paper summary:The paper makes two main contribution: 1). The idea of adding interference type as label is quite novel, and the authors provide extensive experiment results to show that CIQ achieves better resilience against interferences. Comments:  The proposed network architecture does not seem to differ from a normal DQN other than an additional interference type output. As such, I am not very convinced by the causal inference insight. Is it possible to have multiple interference type during training? For real life application, I believe that it is possible that different types of interference could happen simultaneously. If I understand correctly, the difference between DQN CF and CIQ is just that the inteference loss does not propogate to the Q network parameters in DQN CF? 2.Depending on whether there are interference, how different are the outputs of f_2 and f_3?<BRK>Overview: The paper introduces a causal mechanism that both creates and explains away noise interventions into observational data fed into RL agents. The authors propose a form of resilient agent, that based on training data containing labeled interventions, learns both Q function and the causal impact of interventions on the Q function. I find that the authors introduce the causal coneepts in an informal, intuitive way, but that should be followed up by a clear formalism. I find that the biggest downside of the method is that it needs to be trained with the type of invervention that the agent will be resilient to. It would be interesting to create an intervention detector that is fully unsupervised and that is based more on a state anomaly detection. On the downside, the novelty of the paper does not seem major and the predefined nature of interventions might make it unrealistic in a lot of RW scenarios.
Reject. rating score: 2. rating score: 5. rating score: 5. rating score: 5. <BRK>The findings of this work are that for contrastive learning, most of the negatives deemed easily separable are unnecessary, the most important negatives are somewhere in the top 5% closest to the positive sample, and that some of the exceedingly hard examples are detrimental. In general, I felt the main findings of this work to be roughly in line with what we already know about contrastive learning. We can easily look at this work s findings with respect to the soft SVM margin, in that only the examples close to the decision boundary should matter (max margin), but some difficult examples  (the aforementioned exceedingly difficult ones) make the data inseparable, so we allow some violation (slack terms). Validity of WordNet as a measure of semantic similarity: Section 4 uses WordNet distances to estimate the semantic similarities between classes by finding their shared subtree root. While I do not dispute the claim of the hardest negatives being from semantically similar classes. A 2 hop distance in one subtree could easily be more of a semantic jump than a 3 hop distance in another. An example is [1]. I have to imagine this statement is poorly phrased, as [2] (also cited in this paragraph) very explicitly mines for  face like non face patterns. There is a very long list of hard negative mining works in object detection. Overall, I value the empirical impact of this work, in that the rather detailed analysis may lead to improvements to future versions of the contrastive feature learning task.<BRK>This paper argues that in contrastive self supervised learning, different negative instances have different importance. On ImageNet and MoCo2, the authors show that using the most difficult 5% negative instances can achieve similar performance compared with using all negative instances. I recommend to reject this paper due to the following major concerns: 1) study is performed on a single dataset, which is not convincing; 2) study is performed on a single method, which casts doubts on whether the conclusions hold for other methods; 3) this study does not seem to have practical value. While this study is interesting, it lacks rigor, in the following aspects. It is unclear whether the conclusions hold for other contrastive SSL methods, such as BYOL and many others. It is unclear whether the conclusions hold for other datasets. 4.In the author s measure of difficulty, the difficulty is a function of network weights. In early stage of the training, the network weights are random, which implies that the calculated difficulty may be meaningless. However, the paper does have a few strong points. 2.In Figure 3, only three temperature values were considered, which may not be very convincing. Update: I read the authors  rebuttal. The authors didn t address my concern "The study is conducted on a single dataset: ImageNet. sufficiently. I would like to keep my original rating.<BRK>In this paper, the authors carried out a series of experiments to analyze the impact of negative samples in contrastive learning (instance discrimination   CID). Of the many recent self supervised learning approaches, they chose MOCO V2 as the testbed. They trained the MOCO model from an ImageNet pre trained one. Various settings, which correspond to various ways of filtering our hard or easy negatives, were used. Hardness of samples are measured based on embedding distance to the query. The author provided experiments and convincing evidences for a number of insights. My main reservations with this paper are: 1) most of the points are not new and are elaborations of what were pointed out before elsewhere, for example, in semi hard mining for distance metric learning. 2) the empirical results are only within the context of MOCO2 and for a linear classification task. 3) The sample hardness is measured based on embedding distance, which would be evolved during the training process itself. It is not clear how accurate it is especially in the early stage of training. My suggestion for improvements is that either to empirically show that their findings (numbers) are consistent across a number of frameworks and downstream tasks, or to provide some theoretical justification for their findings if only MOCO v2 is used.<BRK>This paper mainly studied how the negative samples can affect the model performance in supervised learning CIO works. Through the experiments, this work has a few interesting findings, including the majority of negative samples are not important for the model learning, only a small subset of hard samples determine the model importance. These hard examples are also closely related with positive samples (more semantically similar). We can see from experiments that it s very important  to fairly treat negative samples in supervised learning tasks. However, there is no frameworks proposed to help improve the learning representation or speed up the training task. In general, the readers are more interested in the solutions after realizing the importance of negative samples treatment during the experiments. It would be necessary to include the corresponding solutions by automatically setup these negatives samples in CID related task.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. <BRK>In this work, the authors aim to approach the non iid data issue in FL by allowing for mean of the local client data to be transmitted in addition to the model parameters. I find this work very interesting and the paper well executed. The authors mention this explicitly, but do not go into more detail. Ideally, the $LocalUpdate$ would receive the same arguments as those that it is being called with on the server side for example.<BRK>Thus, it would be helpful to run the experiments for more FL rounds. The proposed FedMix scheme is inspired by Taylor’s expansion of the global Mixup formulation. 7.The approach to simulate data heterogeneity in the current paper can be generalized by the method proposed in [3 4]. The main concern on the proposed FedMix method is communication and computation efficiency.<BRK>This paper studies an interesting idea that applies Mixup to Federated Learning (FL) for addressing some challenges such as non iid data. It would be better if the authors can provides results on big data such as ImageNet. In that case, NaiveMix and FedMix is very close when $\lambda$ is very small. Please write it explicitly.
Accept (Poster). rating score: 7. rating score: 6. rating score: 5. <BRK>From the coordinates, an “isometric adjacency matrix” is created, which is such a tensor. This combination seems a powerful approach to point cloud neural networks. The core object, the isometric adjacency matrix G, is ill defined. In Eq 1 it is defined trough the embedding coordinates and “the transformation invariant rank 2 tensor” T. This object is not defined in the paper, which makes section 3 very confusing to read. Expand the related work section 	Compare to the strong baselines that use the coordinates. Are these the dimensionalities of the tensor product representation? Or do they denote the number of copies of the representation?<BRK>The paper proposes a graph convolutional network that can be in /equivariant to isometric transformation. This contradicts their statement from above, that GCNs are message passing neural networks. It should be clarified what exactly makes these approaches less efficient. Regarding the timings in Table 4, it is not clear to me, whether the timing include the preprocessing of the adjacency matrix for the inference. Does this refer to both the neural network and the FEA? The additional baselines improved the paper.<BRK>This paper gives a comprehensive derivation of propositions used in the construction of isoGCN. 5.The paper is too theoretical and does not fit to ICLR. The idea to realize the purpose of being isometric transformation invariant and equivariant is good. 2.How to interpret the adjacency matrix up to m hops, what is the construction of the adjacency matrix mentioned in the paper?
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>Summary Of Contributions: This paper proposes to automate the design of auxiliary network and its allocation under decoupled neural network scheme, a design that speeds up network training and potentially boost model accuracy. The approach is validated with leading performance on ResNet and VGGNet under various datasets. Strengths:  The idea to search auxiliary network for decoupled neural network is novel, and the proposed method is verified on multiple widely used datasets. The details should be provided on how the ensemble works, are ensembled auxiliary heads kept during the testing stage? If so, Table 3 cannot tell if the gain is from the ensembled aux heads or decoupled neural network scheme. Yet the authors set K 4 in Table 3 and got a contrary result. The search cost should be indicated and the overall time cost should be discussed.<BRK>The paper proposes a method for decoupled training of neural networks called SEDONA. In the spirit of recent trends in greedy layer wise and indirect training, SEDONA allows gradient information to flow either from the next layer as in backpropagation or from an auxiliary head, trying to make a prediction using the current layer s output. The proposed search algorithm is novel to my knowledge and may be found useful in the community. The major points are the following:1. One way of assessing this would be to also provide training loss or accuracy values. $\theta$ does not appear on the RHS of (1).<BRK>### SummaryThis paper proposes a differentiable architecture search approach for splitting a deep network into locally trained blocks to achieve training speedup. The approach achieves better performance than using backprop on small datasets (CIFAR10 and TinyImageNet), and comparable or slightly improved performance on ImageNet with 2x claimed training speedup. Learned network architecture choices seem to transfer between datasets. S3: On ImageNet, the method achieves slightly improved performance with claimed 2x training speedup. W3: Complicated optimization tricks seem necessary to get the bilevel optimization to work (Section 3.3: "In bilevel optimization, meta variables (α, β) depend on the learning trajectory of layer and auxiliary weights (θ, φ) (i.e.a sequence of values of (θ, φ) during inner optimization). If these issues were clarified I would be inclined to increase my score. Or at least describe the confidence in terms of the log scale. Some rewording of this sentence should help.
Accept (Poster). rating score: 7. rating score: 5. rating score: 5. rating score: 5. <BRK>The authors demonstrate that using a modification of the LiRPA method during the branch and bound process for solving the neural network verification problem can lead to significant speed ups. The authors convincingly show that the their method outperforms the existing state of the art method by Lu & Kumar (2020) on an experimental setup similar to that work. Some questions/requests:  The experimental setup details should be provided in the final version. And if so by how much? I have kept my ratings as my score was for primarily for the strong experiment results (and the score was also conditional on the paper being more polished). I am happy to support this paper for acceptance, but I am a little concerned about the degree of changes in the final version versus the initial submission, given the number of concerns the other reviewers had.<BRK>The paper focuses on verifying simple properties of neural networks onaccelerator hardware. My main problem with the paper, is that is claims a completeverification procedure without proper proofs. However, to claim completeverification a general soundness of the procedure should be proved. On the other hand, the experiments show that the approach is fast andas such makes it more feasible for verification and the paper mostlyreads well. An interesting alternative to discuss in related work, could be proofassistants.<BRK>### SummaryThis paper describes a branch and bound (BaB) process for neural network verification that uses linear relaxation based perturbation analysis (LiRPA). ### StrengthsThe biggest strength of the paper is the impressive experimental results in section 5: the method described in the paper is several times faster than previous methods. I had trouble understanding the problem, the setup, and the proposed algorithm. It does not seem like that s something too difficult to prove (given that BaB + LP is complete), but it should still be clearly stated. I encourage you to either move it later in the paper, or even better: introduce more background/examples in the introduction, as well as the notion of completeness, so that the related work is easier to understand. I was impressed with how much the paper improved in this second version.<BRK>The work proposes a new algorithm that can be used for the complete verification of neural networks (NNs). In fact, the problem is not even properly and formally defined in the paper. Hence, regardless of the experimental results, I do not think that the submission is ready for publication at this stage. Post Rebuttal Comment:I thank the authors for responding to my comments.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 4. <BRK>I trust that the suggestions of all reviewers, taken together, provide substantial avenues for improving the work. Original Review  Summary:The paper aims to formalize “task similarity” in meta learning settings by making use of nonparametric kernel regression techniques; such similarity information is then proposed as a means to alleviate some of the current issues with meta learning algorithms such as MAML/Meta SGD, namely reliance on large sets of similar meta training tasks. Meta training data reuse across tasks at test time has been proposed previously, e.g.[7], so it is also not novel to this paper. Inaccurate claims of novelty are made; they must be made more specific and put into context.<BRK>The paper introduced a meta learning framework in which a kernel describing similarity between the tasks is used to construct an RKHS which is used to perform kernel regression. The experiments on two regression tasks are presented to analyse the efficacy of the proposed method. This is further exacerbated by the fact that authors need to settle with optimization tricks to learn their TANML model. The novelty of the authors  work is limited and the experiments are not convincing. In Sec.4.2.<BRK>  Summary  The paper proposes a meta learning method based on a notion of task similarity/dissimilarity. The proposed method TANML closely resembles gradient based meta learners in the outer update but replaces the inner update by the matrix vector product of kernel regression coefficient matrix and task similarity vector based on a kernel function. In that, the kernel function effectively quantifies the similarity of the loss gradients of the different tasks, evaluated at a learnable parameter initialization. Reviewer’s main argument  Overall, the idea of incorporating a notion of task similarity into the meta learner and the particular proposal to use the kernel between the task loss gradients to quantify such similarity is sound and is a valuable contribution in itself. Thus, it is not surprising that MAML/Meta SGD perform worse than TANML. Overall, TANML has scientific merit   when introduced with a convincing storyline and properly supported by realistic experiments and relevant baseline comparisons, this would be a clear accept.<BRK>This paper proposes a theoretical formulation for meta learning that uses task similarity based on task gradients, which helps learning in the presence of outlier tasks. The inner loop parameter update is given by linear kernel regression, where the kernel function computes similarity between gradients of different tasks. Tasks with similar gradients have a similar update in the meta learning inner loop. 2.The paper has no ablations or analysis for particular parts of their method, such as removing the gradient from the kernel function or removing the regularization term from the outer loop. Thus even on the simplistic datasets considered, it is hard to judge which aspects of the method make it work better.
Accept (Oral). rating score: 8. rating score: 8. rating score: 7. <BRK>This includes investigating the phenomenon of implicit curricula, showing if the examples are learned in a consistent order across different architectures, and exploring the influences of explicit curricula in the standard and emulation settings. The paper empirically shows that curriculum learning has marginal benefits for standard training, but is helpful when the training time is limited or the training data is noisy. It is a thoroughly enjoyable experience to read the paper. I believe the analyses presented in the paper can be valuable for the community.<BRK>Summary: The paper conducts a large scale evaluation of the impact of curriculum learning (CL) in image classification. In particular, the notion of "implicit curriculum" is shown to exist. Prior findings around when CL is helpful (limited training, label noise) are confirmed, which is nice. Strengths:  + Extremely well written and easy to read. I agree that Fig 3 right shows that the difficulty ordering (e2d, rand, d2e) can change the order in which examples are learned when using the step pacing function.<BRK>The authors designed extensive experiments and obtained some interesting results: 1. the models with a similar architecture learn samples in a consistent order; 2. with enough iterations curriculum learning has no gain on performance comparing with random ordering; 3. curriculum learning outperforms others when the training time is limited; 4. curriculum learning is more robust with noisy samples. In general, I think this is a quite practical work that could be beneficial to the community. The experiments are carefully designed and the results are sound, and the paper is well structured. Isn t it more similar with bootstrapping? It would be interesting if the authors can have some discussion in this point of view.
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 7. <BRK>This work proposed a transfer learning method for GANs in the limited data domain. It borrow ideas from IMLE (to overcome mode collapse) and conditional GAN (to improve training stability and generation quality), by introducing data instance prior (plays a similar role to that of  label information in conditional GAN) and knowledge distillation techniques, the model is claimed to be effective in preventing mode collapse and discriminator overfitting. Though the main idea makes sense to some extent, the writing is a little weak, especially the equations and some statements that are not correctly verified in the experiments, making it difficult to go through the paper. 1.Eq 3 is not correct, the sample process should be placed under “Expectation” rather than “minimize”; also, there is no information about which parameter is going to be optimized here. What’s the relation ship between x and x~, are they independently sampled from the target dataset? Also, a detailed training process is necessary, e.g.how to sample, when to optimize the generator, discriminator, and the embedding networks? 5.When the data is large scale, doing clustering is inhibitive. 6.On Table 3, the results are not state of the art on CIFAR10 and fall behand the recent works on generation with limited data, e.g.discriminator augmentations (DA) [1],  DiffAugment [2], then what’s the advantage of the proposed method when compare to these methods? 8.It claims that the proposed method can prevent discriminator overfitting, but in the experiments, it is not shown. It is not clear how to obtain the results shown in Figure 4/9 on custom editing, e.g.there is only one input image, how to compute and exchange the C(x)? [1] Karras T, Aittala M, Hellsten J, et al.Training generative adversarial networks with limited data[J]. arXiv preprint arXiv:2006.06676, 2020.<BRK>##########################################################################Summary:The paper focuses on  improving the performance of training generative adversarial networks (GANs) with limited target data. With the low diversity and quality when traing GANs with few data,  the paper proposes to use data instance prior to reduce the overfitting. Specially, taking the target sample as input, the data prior is extracted by a pre trained network /  self supervised model  , and then mapped into the embedding by  both G_emb and D_emb. The former acts as the class embedding, and the latter is the image embedding combined with the discriminator. Authors also extend the proposed method into the large dataset, and provide the cluster method or a Gaussian Mixture Model. [1] Transferring gans:  generating images from limited data. Authors leverage the pre trained model to extract data prior, and combine it with conditional GANs. + For experiment, the paper uses current SOTA methods (BigGAN, SNGAN and StyleGAN2) and a series of datasets to evaluate the proposed method. Besides, all latest methods (to my best knowledge)  is compared to the proposed method, even the unpublished papers (DiffAugment),  which indicates that the proposed method is effective and convincing . ########################################################################## Cons:  For me, the paper is so clear to understand, and miss a few information. I fail to connect the inference section to the whole paper. In this paper, authors  additionally consider the adversarial loss, and improve the reality of the synthesized image. To be honest, I am not sure it works well when combining StyleGAN2 with project loss. I think it is the one [1], which is first paper to perform transfer learning for GANs with limited data. (2) What is the x and hat of x in E.q 3.<BRK>This submission deals with transfer learning for training GANs with limited label data. The challenge is that training with limited data can result in mode collapse. This submission proposes to use data priors for each instance of the target distribution, transformed through knowledge from a source domain, as conditional information in GAN to ensure mode coverage of the target data distribution. A pre trained feature extractor is used to provide the information to condition the GAN. A range of experiments is performed with the features extracted from VGG16, SIMCLR. They show consistent improvements in the image quality and diversity, measured via FID and precision recall,  for few shot, limited data, and even large scale data settings. The experiments are extensive and they show improved quality and diversity for a wide range of settings. It seems that a gaussian sample z is taken, but on the other hand, for a given network G, z is optimized to match x. It seems that hinge loss is used for GAN based on (1)? It would be important if the authors could comment on the choice of the divergence/loss in this setting. One may wonder that the limited data and mode collapse could be better handled with Wasserstein distance.<BRK>This paper illustrates how they train GANs with small sample sizes with the help of Transfer Learning. The paper tackled a very specific problem: what should we do with a small sample training size if we want to train a GAN. The authors have supported their arguments by a proof in Data In Prior and experiment results. They illustrated well in both aspects. Here are my point of views:Transfer learning is a good way to help GANs when sample size is limited. but I have two concerns over this paper:1. The datasets are very popular in the filed of GAN, however, for the Anime one, I am just curious how the VGG pretrained network can also help. 2.As for the experiment, it lacks of comparison with results that transfer learning is not applied. Generally, the paper is good and it can help data augmentation in other applications.
Reject. rating score: 5. rating score: 6. rating score: 6. <BRK>This paper studied the two time scale A3C in discounted MDP based on recent development in the finite sample analysis of A2C. Otherwise, the value function learned by the critic would be different from the pure TD(0). In fact, it is very difficult to design a two time scale algorithm for a discount MDP, as the transition kernel required by actor and critic are different. I suggest the author to study "averaged MDP" instead. Overall I think the paper is well written.<BRK>This paper revisits the A3C algorithm with TD(0) for the critic update to provide better theoretical analysis of A3C. To show the empirical results, the authors provide convergence results of A3C TD(0) with Markovian sampling in synthetic environments and speedup of A3C TD(0) in CartPole and Seaquest. I think it is a valuable research direction. Moreover, the authors use only two gym environments, which seems insufficient.<BRK>This paper studied the convergence rate of the asynchronous actor critic algorithm (with linear value function approximation)  for RL. In general, this paper is well written and easy to follow. The sample complexity results are insightful. Could the authors address my following comments/questions? It is reasonable that the critic convergence (Theorem 1) uses  a drifting $\theta_k$. If so, is it necessary to define $\epsilon_{app}$ in (13) as the error regarding to the worst \theta?
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 7. <BRK>**Summary**The paper proposes OT GNN, which incorporates optimal transport distance to message passing of GNN. The message passing is aggregated by using a Wasserstein discrepancy for a point cloud. The contrastive regularization is utilized to overcome extreme clustering of nodes of the same class. Also, in theory, the author shows that the Wasserstein kernel is universal while the "agg" kernel is not. The proposed model is tested on several molecular property prediction tasks. The proposed OT GNN has good performance for molecular property prediction tasks. It shows the Wasserstein metric induced message passing is superior to the Euclidean metric case in terms of universality. The paper also shows the $L_2$ Wasserstein kernel is not conditionally negative positive. 3.The computational complexity is analyzed for Wasserstein optimal transport in the OT GNN. For Wasserstein discrepancy in (4), there seem many pairs of $H$ and $Q_i$, where $Q_i$ contains a set of free parameters. Will it bring about overparameterization? 2.Using the Wasserstein metric, the computational cost will increase much. Besides, the contrastive regularization will add more training time. In particular, the discussion of positiveness of the kernel is of no significance or use. 4.The authors also need to try more public graph datasets, such as Open Graph Benchmark, https://ogb.stanford.edu/.<BRK>This paper introduces OT GNN, a combination of optimal transport and graph neural network, for graph level tasks. In addition, the noise contrastive regularizer is added to the model so that the optimal transport plan is discriminative, and the new model could outperform its Euclidean counterpart. However, I do have some concerns regarding the presentation and experiments, which, at this stage, prevent me from recommending the paper confidently to the broader community. The construction of prototypes is not very clear to me. I would expect the authors to provide more discussion before the experiment section regarding its structure, requirements, or formulation details. The authors, if my understanding is correct, named the same subject to different terms as ‘prototypes’ and ‘free parameters’. The design of Figure 3 is confusing. What is the difference between Prototype 1 5 and Prototype 6 10? As they are designed with different shapes, I would expect them to represent different kinds of prototypes. What is the partition function you mentioned in the 3rd line after Eq.6? Some figures could be redesigned for better presentation.<BRK>This paper combines OT  with parametric graph neural network. It replace the inner product between the graph embedding and the first layer weights of MLP by the Wasserstein distance between the node embeddings and some point clouds. Then the GNN, point clouds and the downstream MLP are trained in an end to end way. The authors then theoretically show that the Wasserstein kernel is universal. 2.The idea of using prototypes is interesting. That being said, I think the word "prototype" here is actually misleading. In Snell 2017, their prototypes is a representation of the data. In contrast, here Q_i is NOT the representation of H, since we are not minimizing the Wasserstein distance between them. Instead, Q_i just helps to keep some information that is useful for the task. My major concern is about computation time. Could you please report the training time of each method? 2.Since the model has significantly more parameters, e.g., in Q_i, it would be better to also compare it to a baseline model with comparable number of parameters and the node embedding info. For example, \mathfrak{agg} first appears in section 4.1, but its formal definition is in section 4.2. 4.Could you please provide some intuition that different variants of the proposed model performs the best for different dataset (table 1)?<BRK>The aggregation is performed by comparing the node embeddings of a graph to learned, prototypical node embeddings via the Wasserstein distance. This approach is proven to be strictly more powerful than just adding up the node embeddings, which is the most common state of the art. Conversely, if the aim is just to prevent collapsing of the prototypes, would there not be simpler ways to prevent such a collapse, such as regularizing with the negative log of pairwise distances between points inside a prototype? As such, it would be good to move the appendix into the main paper. But again: Such an argument is not trivial and is currently missing. * It would help to qualify Theorem 1 a little. ## JudgmentOverall, I believe that this paper is strong enough to warrant acceptance. This can be done, but is unusual. It is still a limitation to how much Theorem 1 actually tells us about the proposed model. * It is not clear to me why the distances to the prototypes are plugged into another MLP instead of using them directly, at least for classification. Would the more general case not be interesting as well, where, say, prototypes receive more weight if they are more important for representing the data? Are these supposed to be the weight vectors of neurons in the MLP? * Page 3: It is not clear to me why the negative inner product and the Euclidean distance should yield the same transport plan. The paper currently gives no guidance on how to choose these hyperparameters. In particular, the heart of the paper, namely the proposed model architecture, the main theoretical result, and the experiments, are strong and my issues   namely related work discussions, some theoretical fine points, and ablation studies   are aspects that can be addressed during a revision without affecting the core paper too much. As such, I am confident that the paper, as it stands, is worthy of publication and, with some additional work during revision, can become even better.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 7. <BRK>In summary, the work has good potential, but it is not yet ready, hence my decision to stay on the reject side. To me, the introduction of subspace clustering information is somehow pushing some spectral information into the spatial convolution based approach by GAT. I would have liked to read in the paper a discussion placing the proposed approach in this recent interesting literature, e.g.leveraging diffusion information (as in the paper above or https://ecai2020.eu/papers/1339_paper.pdf) or very related subspace clustering methods (https://ieeexplore.ieee.org/document/9181470). Instead, the discussion of GNN background (Section 1) and related methods (Section 2.) The manuscript should be very clear in stating the limitations of the proposed approach, if any, and what bit of the model is possibly introducing such constraints. In the empirical analysis, Figure 2.c should also report the confidence intervals, as I am not convinced that the difference “with and without ad” is statistically significant. POST REBUTTALI have really appreciated the effort placed by the Authors in their rebuttal.<BRK>The authors present two ways of incorporating the structural coefficients, namely Implicit direction and Explicit direction. Pros* The paper presents an interesting approach to introduce structural information when computing the attention coefficients in an attention based GNN. * The authors don’t mention the complexity of their approach. What if structural coefficients are all constant? Then the model should be empirically equivalent to GAT. Reason for scoreThe paper proposes an interesting technique to use the graph structure as well as the nodes features to compute attention coefficients, however the experiments are a bit limited, using only three small datasets and limited ablation studies. For these reasons, I consider the paper to be marginally below the acceptance threshold. Post rebuttal updateI appreciate the rebuttal made by the authors, as they have clarified my questions about the paper. It is also good that a new dataset and additional ablation experiments have been added to the paper, however, the empirical section still needs to be improved in my opinion.<BRK>However I cannot recommend acceptance in the current form based on the theory alone, and in particular, the following three major points should be addressed by the authors before the paper is ready:  The authors only evaluate their work on the Cora/Citeseer/Pubmed citation networks, which are known to be oversaturated and unreliable as graph representation learning benchmarks (see e.g."Pitfalls of Graph Neural Network Evaluation" from Shchur et al.) and should be avoided as centerpieces of evaluation. The proposed structural learning work is one way of injecting structural information into the attentive mechanism, but is not the only one. Could the authors compare with some of the above, either theoretically or empirically? I m a bit concerned about the method s scalability. Could the authors comment on this? Post rebuttal update:I would like to thank the authors for providing a detailed reply, which partially addresses some of my concerns. The discussion of alternate structural learning approaches is not sufficiently detailed, and has no empirical backing to the authors  proposal. Lastly, I do not find that the authors have appropriately toned down their claims in the Introduction.<BRK>However, from complete view of graph structure, this neighbor might be a noisy node. From this point of view, it s interesting to study whether the attention and structure weights can be complementary to each other. Theoretical analysis shows the expressive power of  the proposed strategy. The experimental results for node classification also demonstrates its superiority comparing with state of the art baselines. Presentation:The presentation of this work is very clear and organized very well. Can we replace it with another correlation matrix, such as Jaccard similarity matrix where each element stands for the node similarity measured by Jaccard equation or Personalized PageRank diffusion matrix? Maybe more justification about the notations are needed to show its connection to next section. In Advances in neural information processing systems, pp. In Proceedings of the AAAI Conference on Artificial Intelligence, vol.
Reject. rating score: 5. rating score: 6. rating score: 7. <BRK>summary:This paper introduces distributional robust learning (DRL) for the unsupervised domain adaptation task. However, I have the following concerns,1.The generalization error (both theoretically and empirically) of the gradient approximation is unclear. It is necessary to analyze how effective and under what conditions the proposed approximation can work for the expected target loss optimization. 2.It needs elaboration why the density ratios can be directly replaced as discriminator predictions, which seems not straight forward and is the main difference to the conventional DRL. 3.There are some related work not covered in the paper.<BRK>Self labeling has proven to be a useful approach for unsupervised domain adaptation. This is done through the framework of distributionally robust learning. Second, the relaxation of the predictor into a formula involving a ratio of densities (Eq.2, Eq.4 and Eq.5) and third the estimation of the gradients of the target loss through the use of source data. For instance, how does figure 1(a) related to figure 1(b)? How can source data be used to approximate gradients on the target? In addition, while Alg.1 is shown to be a relaxation of the original formulation, Alg.2 seems to be an adhoc attempt to combine the DRL framework with self training. However, some experimental evaluation is missing in my view. For instance, in Alg.1, viewing the density ratios as an uncertainty estimation, what is the effect of using other approximations for the uncertainty, either by the adjusting this ratio, or using existing uncertainty estimation methods. Further, the paper claims to better captures the shape features, but this is not shown. 1 are well explained and shown theoretically. The overall approach well evaluated on 3 datasets and some of the components evaluated. On the negative side, clarity of the paper could be improved, particularly with regards the the connection between the main components.<BRK>**Summary**The paper proposes to use the distributionally robust learning (DRL) for unsupervised domain adaptation. First, the authors demonstrate how differentiable density ratio estimation can be done for source and target domains in an end to end manner. In terms of results, the authors show that the proposed approach — Distributionally Robust Self Training (DRSL) — can provide competitive performance on t he VisDA 2017 benchmark. **Strengths**  With the exception of a few points raised in the weaknesses section, the paper is generally well written and easy to follow. This is perhaps one of the strongest positive points in support of the proposed approach. **Weakesses**  One of the minor weaknesses with the current draft is readability. For instance, it was unclear as to how gradient expressions for the classification network from the expected target loss for DRL from equation 8 were aligned with the proposed reduction in equation (6) — specifically, what data are the gradients estimated on?
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 7. <BRK>The training method of the overall model is not presented. However, there are several things that are not clear. However, the quality of the paper is degraded by the unclear claims and some questionable experimental results. Not able to understand what it says.<BRK>It is very interesting to see how experimental results support the theorem nicely (Figure 3). The experimental results are a plus. The two components are already used. what is the complexity of the whole model?<BRK>The paper also proved the bond dimension and showed experiment results to justify their theory. In the experiments in table 1, was there a reason why you don t use the pretrained BERT embeddings? In your BERT + TextTN experiment in table 2, do you finetune the pretrained BERT embeddings? Could you please analysis on the efficiency of your model? It s possible that the tasks are not very hard so there are not too much headroom.<BRK>This model and different variants are assessed on multiple text classification datasets, with decent performance shown against a range of benchmarks. In that sense, the experimental results given are important for assessing the usefulness of tensor network models for real world ML challenges, a question which has seen limited study so far. Recommendation:I would recommend acceptance, owing to the new experimental evidence presented for the performance of tensor networks in NLP.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK>This paper attempts to explain why UNMT using back translation (BT) and denoising autoencoding (DAE) has been successful. I generally found this paper difficult to follow. Section 2 presents some theoretical arguments, or analogies. Section 3 presents some experiments to support the arguments made in Section 2. Overall, it seems to me that these findings agree with the intuitive understanding of UNMT that was already present in the UNMT literature, with the exception of the finding in 3.4.1 that the BT term should be given higher weight than the DAE term. While I think that explaining why UNMT works is an excellent research goal and there are some interesting ideas here, I do not think that this paper is ready for publication yet.<BRK>This paper takes a closer look at the inner workings of unsupervised MT training. The authors provide two alternate views on the backtranslation+DAE objectives used in unsupervised MT. However, I find that the paper falls short by being purely descriptive. The paper would greatly benefit from another round of revision to clear up these points and clarify the presentation if it is to be useful to the research community. While the many observations made by the authors are interesting, the reader is left hanging trying to figure out what to do with them. No need for denoising if the autoencoder is trained less? The paper is very hard to read. In addition, the progression of the experimental section is hard to follow. Right now, it reads like a list of facts about UNMT (this is also related to my first point)  I am not 100% convinced that the mutual information between model prediction and input is a good measure of MT success. Wouldn t that make a model that just copies its input successful (if H(x)>c )?<BRK>It links the reconstruction loss to ELBO (where the q distribution is a back translation model). It shows that the original loss with both the components is important for unsupervised MT and ELBO needs to be augmented with denoising autoencoding loss to be effective at training unsupervised MT models. Many findings in the paper are unsurprising and add little to our current understanding of unsupervised MT systems. This can be followed from the proposed conceptualization of mutual information in the paper. Therefore, unregularized training (with just one of the loss components) is expected to result in degenerate behavior. The reasons could range from optimization issues to poor Monte Carlo sample based approximation of expectations in ELBO (reparametrization trick or score matching is not explored in this paper for better approximation).<BRK>Pros1.This paper attempts to explain UNMT theoretically, which is a significant step for understanding UNMT inspiring the future direction of UNMT. 2.The experiments are properly aligned with the theoretical arguments made in the paper and give good empirical explanations. 2.Although this paper provides propositions that what is a success UNMT training, it could ve been better if showed how we can create a better UNMT system based on these definitions. Some of the analogy and propositions are not well supported by the empirical results. This paper could have given some future directions based on their theoretical understanding.
Reject. rating score: 4. rating score: 4. rating score: 4. <BRK>They evaluated the proposed approach in audio, video, and image classification tasks. Overall, the contribution of the paper is limited in applying the soft ranking loss (or perturbed FY) on permutation based unsupervised learning framework.<BRK>The paper should contrast against this work both technically and in experiments. More recent baselines should be included in the comparisons and the advantages of the specific approach needs to be convincingly established. Overall, while the idea is interesting, the paper does not place its contributions within recent prior works.<BRK>Strengths:  The idea is interesting and general to apply to different modalities. Concerns:  Lack of comparison with state of the art methods.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>In this paper, the authors propose a model based approach with representation balancing (RepB SDE)to cope with the distribution shift of offline reinforcement learning. RepB SDE learns a robust representation for the model learning process, which regularizes the distance between the data distribution and the discount stationary distribution of the target policy in the representation space. The experiments demonstrate the effectiveness of RepB SDE over almost 10 baselines in the popular offline RL benchmark D4RL. But this experiment part can be improved, e.g., including the state of the art offline model free baseline (Kumar et al., 2020; CQL). This paper is well written, especially its methodology and experiment parts are clear. But I have some concerns about the motivation of RepB SDE in the introduction part, where this paper says that "However, recent offline RL studies mainly focus on how to improve the policy conservatively while using a common policy evaluation technique without much considerations for the distribution shift." In order to better presentation, the introduction part needs to be well motivated and justify recent offline RL algorithms fairly.<BRK>Summary:The paper deals with batch RL (aka offline RL). The experiments are extensive. Weak points:The work is on the one hand very specialized, on the other hand just an incremental modification of existing methods. The presentation is very dense and quite hard to grasp, even with the Appendix. Additional feedback with the aim to improve the paper:Please be more explicit to make the text more understandable. Actually lines in such a plot are reserved for a fit to the points or a theoretical curve.<BRK>Compared with the existing works, this paper gives a tractable method to explicitly learn the model representation w.r.t the stationary distributions of two policies. This method is pretty general and could be paired with other pessimistic model based RL methods. Significance:The paper studies offline RL, which is an important topic in high risk domains.<BRK>Strength: Model learning is an important component for offline RL, which is usually done independently from policy evaluation / optimization. The authors propose a new model learning method for offline RL that takes policy evaluation error into consideration / as regularization. In terms of control, the authors show empirical advantages over existing model based and model free algorithms in the challenging D4RL dataset. If B_\phi and \bar{k} are very large, I feel the bound in theorem 4.3 can be very loose. I think the paper may benefit from clarifying more on this assumption. I would suggest to add more model free baselines, e.g., Fitted Q Evaluation [1] and DICEs, to motivate the necessity for learning a model. [1] Voloshin, Cameron, et al."Empirical Study of Off Policy Policy Evaluation for Reinforcement Learning."
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 5. <BRK>The authors argue that their proposed few shot imbalanced setting is different from the long tailed recognition problem and generalized few shot learning setting. Pros This paper introduces a new benchmark to evaluate the imbalance problem in few shot learning. [1] introduced a long tail recognition benchmark where the ImageNet classes are divided into many shot, medium shot and few shot classes based on the number of training examples.<BRK>The paper analyses the effect of class imbalance on few shot learning problems. + The paper considers class imbalance in both the base training and finetuning on the support set. + Overall, the paper presents a thorough and detailed analysis of the class imbalance problem in FSL. Cons:   An approach to deal with the imbalance in FSL settings could have made the paper even more stronger.<BRK>The authors present a detailed study of few shot class imbalance along three axes: dataset vs. support set imbalance, effect of different imbalance distributions (linear, step, random), and effect of rebalancing techniques. Pros: 1) the paper covers the state of the art few shot learning methods, over 10 methods are compared in the paper;2) the work reveals some interesting insights in few shot learning, such as the three analysis summarized in Abstract. The codes to reproduce the experiments is released under an open source license.<BRK>This paper conducts extensive comparison experiments to study the effect of class imbalance for many few shot approaches. 1.Though eleven few shot approaches are considered, some strong baselines are missing, such as [1];2. In the contribution part, this paper declares "compare over 10 state of the art few shot learning methods using backbones of different depths on multiple datasets", however, "backbones of different depths" is commonly used in few shot learning literature. Overall, the contribution of this paper is somewhat limited. Apart from conducting extensive experiments, more informative observations and conclusions should be made. [3] Learning to Stop While Learning to Predict.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. rating score: 6. <BRK>The is another contribution to the rather extensive literature on disentanglement of latent representation of VAEs. Usually, generative models for molecules are evaluated using the validity, unicity and novelty scores. I do have a few comments as to the clarity of the statements in the paper which I hope will be addressed by the authors during the rebuttal period and in the final version. Is this the same as $y \in R^K$?<BRK>If authors wanted a dataset with ground truth generative factors in the dataset, they could consider 3DShapes (Burgess & Kim, 2018), which would have more generative factors and is more challenging than dSprites. Towards this, the paper proposes group wise and property wise disentanglement terms. The group wise disentanglement term separates two subsets of the latent representation. The paper is clear to understand.<BRK>To encourage disentanglement in the latent space of a variational autoencoder (VAE), the authors propose to learn two sets of latent z and w: the dimensions of w are independent of each other and each dimension w_i maps to a known ground truth generating factor y_i. In such cases how does the choice of dimensions of z and w impact the learning and their independence structure? The well studied Total Correlation regularisation is used to enforce the independence of z and w, and the same is used to enforce the independence of the dimensions of w. Each dimension is learned to predict a corresponding ground truth factor. Latent z captures all the other factors.<BRK>**Update**I appreciate the effort by the authors to clarify some of the issues, most of which are addressed in the rebuttal, so I will raise my score to 6. How do you evaluate $I(w, y)$, and for which distribution is this defined? **Summary**The paper proposes property controllable VAE, with the aim to learn certain latent variables correlated to the property and are disentangled. Why do we use avgMI as a metric?
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. rating score: 6. <BRK>They empirically demonstrate that group equivariant self attention networks have an improvement over their non equivariant counterparts. The abstract, the intro, and the related work are well written and give a clear understanding of the content of the paper. 3.The experiments demonstrate the improvements achieved by the added group equivariance in image classification on several datasets. This issue can be especially important if a group of dilations (scaling group) is considered. The text is easy to follow. ### After rebuttal commentsThe authors andwered my questions.<BRK>Authors first prove permutation equivariance of global self attention without positional encoding. Instead, one performs a group correlation where the output is a function of the group action (not of the quotient space) and after this step one applies a group convolution (also appearing in the spherical CNNs (Cohen)as well as in the icosahedral multi view networks (Esteves)). The paper concludes with the claim that linear mappings whose positional encoding is G invariant are G equivariant. To summarize the paper is interesting but quite difficult to follow. One can somehow see how the positional encoding implies it but a section would be worth to be dedicated to it.<BRK>The submission outlines a general framework to make self attention equivariant under the action of ‘arbitrary’ groups. This is on account of the improvements to the submission by the authors, a detailed rebuttal, and somewhat to align with the recommendations of the other reviewers. I thank the authors for an interesting read and thank you for a good rebuttal. I also think that the conclusion the authors come to makes sense and in someways (with hindsight, having read the paper) is to be expected. I like it how the authors explain that the networks are steerable by design because the group can act on the positional encodings. The results are quite disappointing. I couldn’t find it in the submission.<BRK>This paper develops self attention mechanisms for group equivariant networks. Page 6: "an space". The paper is based on the following series of observations. to the set of feature vectors, used as keys in the attention layer, functions which are invariant to this subgroup but not to any larger group. However, I find this work not fully ready in terms of exposition, ideas delivery and elaboration. I think the paper keeps reintroducing positional encoding and self attention in several formulations and there is much space to save there. What happens when the group action cannot be represented as subgroup of permutations?
Reject. rating score: 3. rating score: 4. rating score: 5. <BRK>Besides this, the description of the method and the discussion of related work is given, SMT methods are briefly mentioned but the usage of the idea in previous work, also SMT literature is necessary.<BRK>The core of the method is in the word substitution model (Sec 3.2) and how it is integrated in the decoder (Sec 3.3). Also the variability of performance (e.g.due to sampling) is not assessed.<BRK>This work simply tweak sampling rate to vary the ratio of using word alignment links, not distorting links, and thus, it is not a direct measure to verify the quality tradeoff.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 6. <BRK>The paper attempts to analyze the softmax cross entropy loss and discuss useful properties it has in an intuitive and accessible way. The paper mainly presents comparisons between this loss function and two other (very standard as well) loss functions that are the "sigmoid cross entropy" and the "squared error" loss functions. It is very interesting to read the paper and realize that the topics discussed here have been written in well cited and well established statistics and ML books for decades. The softmax cross entropy loss is the multinomial regression loss while the sigmoid cross entropy loss is no other than the standard binary  logistic regression loss and finally the "squared error" is the least squared loss. Authors compare the performance of these three on a few benchmarks. I think it s good to read the "Elements of Statistical Learning" and get a clear exposition to what science has been up to for decades before getting too deep into deep learning.<BRK>The paper benchmarks various  training objectives, including loss functions and regularization schemes and measures accuracy, calibration and out of distribution robustness. The paper also studies how these objectives affect representations in various layers of the network. Cons  Unfortunately, novelty is very limited and this is mostly a restating/confirmation of results of earlier work. If the losses produced the same predictions, then they would all have the same accuracy. "label smoothing improves calibration" : already known The only possibly novel contribution is that losses behave similarly in the earlier layers, with most of the separation happening at the final layers. Suggestions on improving novelty:  For a conference devoted to learning representations, the authors should investigated  into why, for example,  different random seeds of squared error loss disagree with each other so much, while some other losses are more self consistent. but given that top 1 accuracies are nearly the same (table 1), the difference in figure 2 must  mostly be contained in the predictions where the classifiers were wrong (respect to ground truth). in other words, there shouldn’t be a significant difference in the cases where the classifiers agree with ground truth   can the authors look into this? At the start of the paper it is stated that  "our goal is instead to understand when one might want to useone loss function or regularizer over another" : however, I do not see the paper providing any additional insights into this beyond what is already known.<BRK>The authors give a pretty thorough investigation of several existed losses of training deep networks in the task of supervised classification. Several discoveries are presented from the experiments:1) Regularizers and alternative losses did improve over vanilla softmax loss. 2) Different objectives mainly affect the last few layers while the layers not close to the output seems similar. 3) All objectives that improve over vanilla softmax loss produce greater class separation in the penultimate layer of the network. Pros:+ A comprehensive analysis of different behaviors of different objectives. Cons:  The authors claimed  Our goal is instead to understand when one might want to use one loss function or regularizer over another and, more broadly, to understand the extent to which neural network performance and representations can be manipulated through the choice of objective alone. I wonder if the study in this paper realizes the goal that the authors claimed. It seems it is hard to make a conclusion to decide which single objective performs better than others, even in the task of classification, with the existing methods and optimization methods. It seems to have little insight for future works that using novel methods or optimization frameworks. I respect the efforts that the authors made to give a thorough analysis of existing common used objectives. But since little novel technical contribution is made and the results seem not surprising, I am not sure whether such work is above the threshold of ICLR.<BRK>The paper presents a study on different loss functions and regularizers that apply on the penultimate layer and show how the choice of the loss function/regularizer can affect the performance of classifiers. The authors experiment with image datasets and study multiple parameters and side effects of the loss functions/regularizers. This is an interesting paper with multiple experiments. In the same spirit what it misses is an extension on a large scale scenario. Many practical applications have very large output space. Also, the authors could extend to other objectives like negative sampling etc. Some analysis on the individual classes is missing also. As most of the impact is on the penultimate layers would it make sense to have a couple of experiments where you tune a model on another task. If I am not mistaken you trained from scratch. It would be nice for example to have some results from another domain also like an NLP task. Overall I think is a good paper.
Reject. rating score: 2. rating score: 3. rating score: 3. rating score: 4. <BRK>The literary style of the paper is not suitable for a scientific publication. There are many errors in spelling and the clarity of presentation is poor. As a detailed note, there are no paragraph breaks, which makes the reading hard.<BRK>There s also mention of nodes in Section 4.2.1, are those the same? While RL is very flexible, it is not always the right solution. Overall I find this submission too hard to follow and agree with to vote for it to be accepted.<BRK>However, it is not clear if this is necessary, or optimal. Overall, I think this paper is not well written and should be further polished. For example, in the abstract and the beginning of the intro, the paper aims at optimizing the model loading latency, but in the later part of the intro, it also aims at optimizing on device training? The paper decomposes this process into steps, and formulate the search process as a markov decision process and use RL to search for the solution.<BRK>2.This paper also deployed their method on mobile platform. The experimental setting is unclear and only few comparisons are available. It is hard to judge the advantage of the proposed algorithm. 1.3 Why the number of "available" threads is not considered as a factor in deciding how to split? 2.2 Why authors do not compare to the heuristic baseline, e.g.split the model based on the number of parameters or number of layers? 3.2.What is s in Algorithm 1?
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. <BRK>Unlike most decentralized methods with compression that are inspired by primal methods (DGD type methods), this paper introduces a new primal dual algorithm with compression. The proposed method s main idea is borrowed from the NIDS algorithm, which converges linearly when the local loss functions are smooth and strongly convex. Overall, the paper is well written, and the authors explain the intuition behind each step of their proposed method very well. Although the main proposed method and its convergence analysis are similar to the ones in the paper that introduce the NIDS algorithm, the final theoretical result is strong. A few minor comments:It is acceptable that the authors provide a linear convergence rate for the proposed method, but it would be better to characterize the overall complexity bound for their proposed method to achieve an $\epsilon$ accurate solution. In particular, it would be great if they could study the dependency of the overall complexity (number of communication rounds) on the graph connectivity  parameter and the objective function parameters (strong convexity and smoothness constants.) The current result for the stochastic setting is a bit trivial, considering the result for the deterministic setting.<BRK>The paper introduces a novel decentralized algorithm (LEAD) incorporated with compression that achieves linear convergence rate in strongly convex setting. Convergence analysis is provided for both deterministic and stochastic variants. The paper is well written and the results are presented clearly. I only have a few questions regarding the presentation of the algorithm. Providing more intuitions on these points will be helpful for further understanding. d) Does the result transfer to the convex but non strongly convex setting?<BRK>I really enjoyed reading the paper. The paper builds on top of the recent successes in distributed optimization and two particularly popular approaches: quantization and decentralized topology of the network. Its design consists of multiple pieces: a consensus algorithm based on NIDS/D^2, a compression scheme based on Diana, and the gradient updates of SGD, all of which are given together immediately. Firstly, the meaning of the variables that are presented in the algorithm could help to understand it. Do the authors think that it s possible to relax the assumptions for LEAD as well? I think it should be something O((C*kappa+beta)*log(1/eps) + kappa*sigma^2/eps) and I d hope to see a comparison to the bound of (Koloskova et al., 2019). 5.The authors should provide a reference for the claims around Assumption 1, such as the eigenvalue bounds. 6.The convergence rate recovers that of NIDS as mentioned in Remark 3. Does it also recover the right rate in other special cases, for instance, when the algorithm is fully centralized (W I)?
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 7. <BRK>The complexity accuracy tradeoff of the algorithm seems to be appealing according to the experiments. If the authors think the contributions lie in reformulating the CCA problem as a PCA problem, then the reformulating part is perhaps the contribution. It is unclear why the reformulation in (3) is a good approximation for CCA. “SO(n): group/manifold of nxn special orthogonal matrices.” what is the definition of “special” here? From the current writing, it is very hard to see how the complexity of the algorithm is calculated. It is also unclear how the convergence and convergence rate analyses come together.<BRK>This paper aims to reduce the computational complexity of canonical correlation analysis.By decomposing the CCA projection matrices into a product of several structured matrices, a stochastic gradient based optimization on a Riemannian manifold is provided reducing the computational complexity from $d^3$ to $d^2k$. Weakness:Although the authors claim that their proposed method captures more correlation than MSG, two of the three datasets in which their method is superior (MNIST and CIFAR) are not realistic setting (i.e., correlation between left/right half of images).<BRK>The convergence of the training process is proven. This makes it not applicable to data in high dimensional spaces, e.g.representations learnt by deep networks. To resolve the issue, the authors propose a reformulation of linear CCA which decomposes the transformation matrix U (V) into a product of three matrices. They allow for Riemannian stochastic gradient descent which ensures their updated values lie on the same manifold.<BRK>This solution to that task is not obvious, because the objective function of CCA, together with whitening constraints, does not allow simple additive decomposition. First, the optimization task is reformulated as a task over certain Riemannian manifolds, and a natural initialization is suggested. Then, a natural minimization algorithm is presented, which is based on stochastic gradient descent on a Riemannian manifold. The key aspect of the algorithm is a combination of 2 types of gradient, the gradient for top k principal vectors and standard gradient.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 7. <BRK>**Summary:**This paper presents a new unsupervised learning method for learning latent representations for visual RL control domains. The method, Augmented Temporal Contrast (ATC), can be used alone to learn a representation to be combined with an RL algorithm, or as an auxiliary task in an end to end system. The paper provides an extensive experimental study to support its claims. **Strengths:**The paper is clearly written, and all of the main points are well articulated. Included is a thorough experimental study that effectively demonstrates the performance of the method. This information should be included so that the reader can better evaluate the variance of each method, and make more confident conclusions. **Recommendation:**Overall I vote to accept. The method presented in the paper is not revolutionary, but it appears to be novel and significant enough to be of interest to deep RL practitioners. **Questions:**How many independent runs (seeds) were used in each of the domains? Can this information be included in the main text? **After Author Response and Discussion:**Thanks to the authors for their responses. After reading the other reviews and the author responses, I am lowering my score to 5. I think that the number of independent runs used (especially on the smaller domains), and the way the results are presented with the min max extent makes me less convinced of the results than I was in the initial review. Overall I think the paper is of interest to the community, but the experiments and their analysis could be improved.<BRK>Summary of the paper:The paper aims to define an unsupervised pretraining architecture that can be used to pretrain representations for a reinforcement learning agent. It consists of an encoder compressor predictor architecture for an (augmented) observation that is trained in a small latent space by minimizing the InfoNCE loss between that prediction, and the encoding of some future state. The representation used (the pretrained encoder) is then frozen, and a policy network is trained consuming these representations. The authors then evaluate their approach on several different tasks: a control domain (DMcontrol), Mazenagivation (DMLab) and Atari. They show that their approach   without finetuning the representations   achieves comparable results to end to end reinforcement learning on most of the different domains. They also show that adding their defined loss as a regularizer always helps learning. They finally show that their representations can generalize out of domain by running several multi task experiments, while only having pretrained on one domain. Commentary on the goal of the paper:The goal of the paper is extremely important: separating learning representations from learning policies would enable better transfer, possible more sample efficiency and lower variance in outcome. The paper is extremely well written  The paper has an extensive empirical section. Results are generally very good. As I said above, I agree and understand the desire for the separation of representation learning and reinforcement learning. However, the paper would have been stronger if it had concentrated on a single benefit of this separation, and evaluated their approach on that. While they say that their goal is to investigate "how to learn representations which are agnostic to rewards", this is too general as well. Along a similar line of thought, the results, while strong, are not as convincing as they could be, because the paper does not focus on the benefits of reward agnostic representations learned by ATC. In total, I would argue that this is a well written paper with interesting analysis that could be a lot stronger by narrowing the scope of the contained argument. I argue for rejection, because I can see an updated version of this paper to be a great paper.<BRK>This paper proposes an auxiliary task for learning better representations for reinforcement learning. My main concern is that the paper is mostly an empirical evaluation, as the novel algorithm is mostly an extra contrastive loss. Further, it seems their method is almost exclusively meant for pixel based environments (see point 1 below), so the authors should make this point more explicit. Given this empirical emphasis, I feel the authors could have performed a deeper exploration into understanding _why_ their proposed algorithm performs the way it does in the different environments considered. In particular, some very specific design decisions were made in evaluation (see, for instance, points 2, 5, 9, 11, 12, 13, 14, 15, 16 below). The clarity of exposition could also use some improvement, as I detail below. In Section 3, the authors write "This task encourages the learned encoder to extract meaningful elements of the structure of the MDP from observations.". It s not clear what "positives from  all other elements" means. 6.In section 4.1 the authors say "multiple seeds were run", please specify how many. 8.It s not clear what the difference between ATC and UL training is. Are they the same thing? For example, in Figure 3, which ones are ATC? 9.In Figure 3, why does one environment compare with pri and the other with 2x, but not both in both environments? On SpaceInvaders it s possible the screen changes could cause issues, but what do the authors think cause the subpar performance in Breakout? Were these all drawn from the same checkpoint? 13.In the **DMControl** section, the VAE is trying to reproduce a frame $T$ steps in the future? Also for this section, does your VAE try to predict individual frames or stacked frames (as frame stacking is common in Atari experiments)? If that is the case, regular RL would have used fewer frames in comparison, no? 18.In Figures 14, 15, and 16 it s not at all clear what we re supposed to be looking for, nor how they show that ATC/UL is focusing on the score/enemy and the others are not. At the bottom of page 2, the term "POMDP" has not been introduced yet.<BRK>The paper introduces an unsupervised task called Augmented Temporal Contrast which associates pairs of observations separated in time using a contrastive loss. The paper uses the task in several training regimes (in online RL, pretraining and multi task RL). Improves sample efficiency on most of the environments and setups, and improves over prior methods. The attention maps in the paper and appendix are great and although they may be hand picked(?) examples, it highlights the issue with many approaches that can t model a goal rarely seen. Cons:  You write you use multiple seeds but I don t see anywhere details on this? Consider adding it to the tables in appendix. In Figure 3 one agent step is 4x environment frames? I suggest to make it clear in plot or in caption. In Figure 2, environment frames is used. 4.1, to what extent is the "small" replay for DMLab necessary over just using observations in batches/unrolls? Looking at Table 3 I can t see how large the replay is? 10k as in SAC or smaller? Update: Not all the experiments are particular thorough and the novelty less than expected.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. rating score: 5. <BRK>What are the authors  thoughts about this? For example, in Figure 3 and in the case of L 8, the setup K 1 implies K/L   0.125 , but the K 1 point for SkipW is at less than 10% mark on the xaxis. Figure 7 seems to be a bit unclear.<BRK>Note that I do not need deployment for these datasets but would like to see these numbers in the paper for more datasets to make the paper stronger. I appreciate the authors for ablation and on device experiments which are very thorough. But that doesn t stop the method from being useful. While I appreciate the efforts, I would like to see results on at least one or two (at least two preferred but I understand the time limitations, so one good dataset also works for the discussion phase) more real world datasets to check the generalization. This is a key question to be addressed during the discussion.<BRK>The authors proposes a new framework skipW to strictly limit the computation in RNN. This detail needs to be included. 3.ThrRNN did one experiment in MNIST. As far as I know, the paper is novel.<BRK>L needs to be set prior to training, whereas K can be modified at test time. The model is evaluated in two tasks, namely a synthetic adding task and human activity recognition. This potential is shown in the human activity recognition (HAR) results.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 3. <BRK>As an alternative, the paper proposes SOLAR (SPARSE ORTHOGONAL LEARNED AND RANDOM EMBEDDINGS), a model which uses high dimensional, ultra sparse embeddings, on which near neighbour search can be done using simple lookup operations. Hence, SOLAR could be trained on multiple GPUs in an embarrassingly parallel way without requiring any communication between the GPUs. SOLAR also performs equally well on various extreme classification benchmarks. Can the performance of the baselines be improved by training more? Similarly, I could not find information about the compute used during inference for the baselines and SOLAR. The paper is also well presented and is easy to follow. I currently support acceptance, but would form a final opinion based on the authors response.<BRK>The label embeddings are high dimensional, sparse and mostly orthogonal by design. As observed in DSSM paper, the choice of tokenization affects the retrieval accuracy significantly and a fair comparison would necessarily need to use the best tokenization scheme. As the label embeddings are random and each bit in the embedding is independent of all other bits by design, the embeddings can be partitioned into multiple disjoint blocks and each block can be used to independently learn partitioned embeddings for the documents. Empirical performance on the chosen data sets is impressive compared to the baselines. First is about the rationale behind using random binary embeddings for labels. Further, in the proposed approach, semantically related labels are likely to have dissimilar embeddings.<BRK>The work describes an extreme classification strategy which leverages the computational efficiency of multiclass (and multilabel) efficiency on small label sets (circa 10K) composed with the near orthogonality of random sparse embeddings; while exploiting inherent parallelism of repeated independent instantiations of this primitive technique to mitigate statistical issues. The use of fixed near orthogonal label embeddings is elegantly motivated, the computational properties of the technique are favorable (especially, amenability to inverted indexing), and the statistical performance is competitive.<BRK>The paper proposes to use high dimensional sparse representation as opposed to low dimensional dense representation for representing input text. This can be useful in information retrieval applications. All in all, I believe the authors are working on an interesting problem but the experimental results are not convincing and some claims should be changed. 2.2.As mentioned multiple times in the paper, SNRM is the only paper with the same idea of learning high dimensional sparse representation. I also recommend the authors to include some non neural baselines, such as BM25 and RM3 as they sometimes outperform neural network models on information retrieval related tasks.
Reject. rating score: 3. rating score: 5. rating score: 6. rating score: 7. <BRK>Unless I have greatly misunderstood something,I do not see how the paper could be modified for me to recommend it foracceptance. The current paper set this distance arbitrarily by sampling from a Gaussian with covariance $I$. **Summary of work**They propose an epistemic uncertainty estimation method (though itis not really estimating the uncertainty, but just whether a datapointexisted in the dataset or not). The proposed method samples newpoints $x_{epi}$ in the vicinity of the training data points $x_{tr}$from a Gaussian distribution (they set the covariance to $I$). Weaknesses:  It is not clear to me how to pick the covariance for the sampling distributionof $x_{epi}$. Being only confident close to the data trajectory may not be the bestapproach. It seemed the number of data points gathered was around 150, which shouldbe easily handled with GPs, so the task does not seem to suggest that GPswill not scale to it.<BRK>**Pros and the Key Idea**This paper studies uncertainty quantification (UQ) in model based learning for control, which is a timely and important research direction. The advantage of this framework is that it is very simple, and doesn t need sampling or test time dropout at the inference time. Also, adaptive control can handles epistemic uncertainty in an online manner as well. It would be great to discuss the difference between adaptive control. As I mentioned above, the key idea of *EpiOpt* is to train a "distance function" $\eta(\cdot)$ to quantify the distance between the source and target data. A few questions pop up from this paper: * Why do you need to sample $X_{epi}$ around $X_{tr}$? In other words, you could just use $X_{tr}$ to estimate a density function for the source data, and then evaluating this density function to get $\eta(\cdot)$. I didn t see a clear reason to *train* a neural network to estimation this distance. However, the key method *EpiOpt* is only about the epistemic uncertainty, and how to deal with the aleatoric uncertainty only appears in the experimental section in equation (9).<BRK>Epistemic uncertainty is a useful measure to have from a learned model. This paper proposes to output epistemic uncertainty from NN by training with samples outside the training set. The paper samples epi points around the training set. How does this perform when training set is very sparse or very spread out. 2.Changing the yellow line to something darker and also making the thick black line to be a lighter color would improve clarity of the results.<BRK>If so this should be made clear in the paper. Strong points  1. I suggest that references to material in the appendix are made in the paper. 3.GP regression with a squared exponential (SE) automatic relevance determination (ARD) kernel has, for a given training data (X,Y), a maximum epistemic regression uncertainty, given by the signal variance parameter $\sigma_f$ of the kernel: As $d(x^*, x)$ grows the exponential in the kernel decrease towards 1 rapidly, where $x$ is a training data point and $x^*$ is an arbitrary data point. Figure 1 is not updated (it looks the same w.r.t.the GP, but the change from variance to std should be noticeable? It would indicate, as mentioned in the paper, that the epistemic uncertainty estimate is off for the GP model. Figure 3 (appendix) does indeed seem odd (or unclear) for the GP model, as does Figure 2 (appendix).
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 6. <BRK>They show on CIFAR 10 and ImageNet that their method results in lower ECE over varying levels of corruptions. Strengths: * This method could be applied post hoc to a variety of models * Seems fast to compute * Results in better calibration estimatesWeaknesses: * The terminology of the paper is ill defined. In this work, the authors aim to improve the calibration of probabilities obtained from these models. The rely on the benchmark provided in Ovadia et al.as a basis for their analysis.<BRK>This paper studies the problem of providing calibrated predictions for out of distribution data. They evaluate their approach on a standard image datasets including CIFAR 10 and ImageNet, and show that their approach outperforms existing work. This information is not assumed in existing work, and fundamentally alters the problem, making it simple to address and uninteresting. In addition, there recent work in this area that the authors do not cite, for instance:Park et al., Calibrated Prediction with Covariate Shift via Unsupervised Domain Adaptation. In particular, the calibration under distribution shift techniques can still be applied, just using either just a single test image or their set of multiple test images.<BRK>The prior work [2] that this submission builds upon reports results across a fairly wide range of tasks and out of distribution types. It would be more interesting to see if such calibrations with a specific set can be relevant for more realistic OOD cases as well, as in [2]. The sentence "Heuristically we always want clean images in our calibration set while having different shifted means” is not clear, could the authors elaborate? More experiments as described above would improve the paper, by making a more compelling case for such exposure based recalibration techniques. Ovadia et al.Post Rebuttal:Thanks for the response, and the new experiments. I continue to think that this is a nice simple method that works well enough to be interesting.<BRK>Thank you for the additional experiments. Summary:The paper addresses the important problem of over confident predictions on out of distribution (OOD) data. Strong points:  Even when the type of corruption in the validation dataset is different from the corruption in the test set, the proposed method can considerably improve over existing methods. The proposed method, similar to other post hoc calibration methods, can be used in combination with any model. The illustrations, in particular Figure 2, are well done. It would be interesting to see results when the test data is completely OOD data, like using SVHN dataset for testing predictions of a model that was trained on CIFAR 10 (see Ovadia et al, 2019). in "of p_max under each of the calibration models, using the probability density",I think it is easier to understand if "calibration models"  > "calibration sets".
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 7. <BRK>Summary:The paper proposed an extension on Neural radiance field for better generalization across novel scenes by introducing multi view pixel aligned features as additional input to NeRF. To fuse multi view information and implicit reason about occlusion, an attention aggregation module is applied. Strengths:+ The idea of using pixel aligned feature for making NeRF generalize to novel scenes is interesting. But in the related work section, the author states that GRAF is unable to generalize to novel scenarios which seems to be an unfair claim.<BRK>This paper presents a new neural learner for learning generic scene radiance function. Compared with existing methods such as NeRf which are scene specific requiring per scene based training, the proposed new method is potentially able to generalize to novel unseen scenes or objects. The work is well motivated, aiming to relax the restriction of existing per scene based radiance field learning methods  (NeRF, for example). + The idea of using attention model to address multi view consistency issue is interesting,  which appears to be sound and promising. What are the results on the other two datasets, and what are the training testing split on unfamiliar scenes? However it is unclear which model was used for testing which datasets. The authors  also claimed that the attention module is invariant to the input view number and orders;  Yet,  there is not any  evidence (either theoretic analysis,  or experimental) provided in the paper.<BRK>+ The proposed GRF can empirically generalize to novel scenes. Concerns:  The novelty is limited as the concept of this paper is highly similar to NeRF, despite a significant improvement is the general feature for novel view rendering. How does it work on unseen scenes? As described in Eq.(6), the color channels are estimated through MLPs with learned feature \bar{F}_p and the query viewpoint \mathcal{V}_p as input. The authors give an explanation that SRN requires to be retrained on novel scenes to optimize the latent code.<BRK>This paper presents an extension of NeRF. The key idea is to represent a 3Dscene as a collection of K "posed" images. The paper writes "This simple design of GRF follows the principle of classicmulti view geometry (Hartley & Zisserman, 2004), therefore guaranteeing thelearned implicit representations meaningful and multi view consistent." I hope the exposition can beimproved in revisions and am comfortable leaving the experimental shortcomingsto future work. "for very query"  > "for every query" This should be clarified early on and the confusionissue with the image augmentation with viewpoint position remains.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. rating score: 5. <BRK>The paper presents theoretical analysis of MDPs with execution delay together with an algorithm that achieves better performance on the task than the baselines. *Originality*To my best knowledge, the results are new and the need for non stationary policies is a novel highlight.<BRK>This paper investigated the problem of learning agents when there are execution delays. The proposed method is promising and verified by experiments. On the other hand, there are still a number of questions that need to be clarified, such as the theoretical guarantees of the proposed delayed Q, and issues about its model based nature.<BRK>Update  The arguments in the rebuttal clarify some confusions I had and illuminate the contributions of the paper further. The delayed Q algorithm seems sensible. The theoretical analysis of the augmentation approach seems reasonable (but I did not check the math).<BRK>Compared to a naive strategy of state expansion (which appends a history of actions to the observation), the authors show that amending a policy to be nonstationary can:  (a) achieve the optimal policy in analysis for tabular MDPs  (b) performs well in a series of computational experiments. The paper takes on a clear issue in reinforcement learning, and comes to some equally clear recommendations and findings. The proposed solution method is reasonable, and appears to perform well in practice. The series of experiments are well thought through... with a clear progression from theory  > didactic examples  > deep RL.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 6. rating score: 5. <BRK>This paper proposes a scheme for fast distance preserving binary embeddings of R^n into { 1/+1}^m. This is a very natural idea and it is somewhat surprising that this hasn’t been done before. On a technical level the results in this paper are hardly too surprising for the JL community, but it is nice to see this analysis worked out in detail. The key weakness of this paper is that in order to get optimal time/space complexity strong assumptions on the input vectors are required. Despite this shortcoming, I think the paper is still going to be of moderate interest to the theoretical community.\Overall, the paper is clearly written and presents and contributions well within the landscape of the previous work.<BRK>In this paper, the author proposed a fast, distant preserving, binary embedding algorithm. Their method is fast and memory efficient, and accurate, so that the error is compared to that of a continuous valued Johnson Lindenstrauss embedding plus a quantization error. In addition to theoretical results, the authors also test the proposed algorithm on nature images, showing strong performance of their method. The strong points includes+Proposing a new binary embedding algorithm+Providing theoretical results of approximate error+Empirical results supporting the superiority of the proposed methodThe weak points includes Some parts not easy to follow Experiments could be remarkably improvedOverall, the technical part looks strong, but the presentation and experiment could be improved. First, the introduction part is difficult to follow, since too many symbols are used in this paper while some of which are not explained at all. Besides, in Lemma 2.6, it is unclear where to use $s$. Second, the proposed algorithm is tested on one real world dataset based on distance reconstruction error, and compared with FJLT.<BRK>The paper studies binary embeddings that preserve Euclidean distances for the case when the vector mass is spread fairly evenly across the coordinates, which is a very common case in practice. What the paper essentially does is a standard observation that uniform subsampling of coordinates (in spirit of Ailon   Chazelle) of such dense vectors gives a Johnson Lindenstrauss guarantee on the pairwise distances, and then it uses the quantization procedure developed earlier by Huynh and Saab. To me the result sounds like a fairly straighforward ramification of the result of Huynh and Saab, but I can see it potentially being accepted, since the studied problem is extremely important. I m happy to revise the score if I got something wrong and the main resul is _not_ a straightforward variant of Huynh Saab.<BRK>The paper presents a new mapping of Euclidean vectors to bit vectors (quantization), along with a de quantization method. The method is closely related to recent work by Huynh & Saab (2020), but instead of using a random rotation DBx on the input vector x, a sparse, Gaussian random projection is used. After this mapping, a so called Σ∆ quantization method is used, following Huynh & Saab. Notably, there is no comparison with product quantization (IEEE transactions on pattern analysis and machine intelligence, 2011). Another claimed contribution, a speedup from O(n log n) to O(m) time, is not surprising (or new): It is known that for well spread vectors, even random sampling is an optimal dimension reduction method. The writing can be made clearer. Even though I worked on related things, I found several parts of the paper hard to make sene of. For example:  The abstract suggests that the results apply to all vectors that are not sparse, but in fact they apply only to vectors that satisfy a hard to satisfy L infinity norm restriction. Is it important that L1 distance is used in algorithm 2, rather than L2 distance? After all, these distances are similar up to scaling for "spread out" vectors.<BRK>The authors present a distance preserving embedding algorithm to reduce the dimensionality / encoding of a high dimensional Euclidean point set. The main contribution of the paper is Theorem 1.1 where the authors prove a bound on the distortion of the proposed embedding. Theorem 1.1 states that the input points are well spread and moreover Algorithm 1 assumes (implicitly) that the input is normalized. Indeed, but then the running time is not O(m), right? How does your results compare to these papers? Is it possible to decouple your contribution on quantization with JL projections? * Theorem 1.1: Isn t the assumption of well spreadness too strong here? If the vectors are so well spread, I believe that uniformly sampling of coodinates could also work. * Algorithm 1: does algorithm 1 requires scaled data points or is this required only for the analysis? * Theorem 1.1: "with high probability"  > this can be made more explicit using the \delta parameter as in the appendix.
Accept (Oral). rating score: 8. rating score: 8. rating score: 7. rating score: 6. <BRK>* **+** Cross domain few shot classification is very relevant to the few shot learning research community, and the availability of unlabeled data for the test domains is a plausible assumption. * **+** The proposed approach is sound and straightforward. * ** ** The paper should be more rigorous when reporting which approach performs best in a given setting. #### RecommendationI recommend acceptance. Overall the paper is clear, the problem being tackled is relevant to the research community, the proposed idea is sound, and evaluation is rigorous. The main concern I have is with the way in which results are reported in Table 1.<BRK>Problem: The paper introduces the problem of few shot transfer when there is an extreme difference between the base task and the target task. Crop Disease and Satellite Images   The proposed approach works for Crop Disease and Satellite Images which are partly in the domain of base task. + use of unlabeled data from the target domain to learn a better representation. At test time, *this new representation* is used for learning a classifier on the few labeled examples.<BRK>This nicely written paper explores the situation in which one tries to do domain transfer from a large dataset with many labels to a partially labeled dataset with relatively few unlabeled and even less labeled examples, if the domain gap is very large. The paper presents convincing ablation analysis to back the latter claim: training on the target domain is helpful for transferring to that specific domain. While the novelty of the approach is somewhat limited: it just combines an existing pretraining approach with co training on another dataset, its simplicity makes up for it.<BRK>**Summary of the paper**This paper studies transfer learning in an episode learning setting, where at evaluation time a few shot few example task is generated. The technical novelty of the paper is to use the unlabeled dataset by (a) obtaining soft labels from miniImageNet classes, and (b) using SimCLR (Chen et al, 2020) as self supervised loss. **Strengths**This setting is interesting especially due to the large domain differences. In this paper only a single self supervised method (SimCLR) is evaluated, while there are other methods also focussing on self supervision for few shot learning (Gidaris et al., 2019, Su et al., 2020). This is confirmed by the ablation of different startup strategies. **Post Rebuttal**My main concerns for the paper are not withdrawn, yet not shared with the other reviewers.
Reject. rating score: 6. rating score: 6. rating score: 6. <BRK>The paper is very clearly written and very comprehensible. Furthermore, it is very detailed about the experimental setup. Besides this, the experimental evaluation is solid. UPDATE AFTER REBUTTAL:I thank the authors for their responses and appreciate the inclusion of some of the requested changes in the paper. However, the paper still misses the comparison to other adaptive methods which is the paper s greatest weakness.<BRK>I think this paper makes a borderline case: DPS provides a framework that combines the compressed sensing part (sparse data acquisition) and the subsequent learning part in an end to end manner. Update:Thank the authors for their responses, clarification, and additional experiments. I read through authors’ responses and the comments from the other reviewers. The simplicity of the changes proposed (over DPS) is not a limitation, but it could be accompanied by an in depth theoretical analysis, a convincing qualitative discussion or _extensive_ experiments demonstrating the practical relevance of the proposed approach.<BRK>#### Questions/comments that do not effect the review:Why use an LSTM/any network with memory? Thanks to GPUs and deep learning, active sampling is becoming more practical and its interesting to see new work in this direction. ### WeaknessesNovelty: The method is a small extension to the DPS method where the network that selects which rows to samples is conditioned on the existing measurements. Validation: The paper did not compare to any other active sampling strategies. ### CommentsWhile the proposed method was computationally impractical, active sampling was discussed extensively in [A] from a information theoretical perspective. ### UpdateI thank the authors for their comprehensive response. While I have reservations about the paper s lack of comparisons, I think its publication still might be a net positive for the research community.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 6. <BRK>By introducing probability of taking local updates, the framework allows different workers to take different local steps within a given time interval. 3.The analysis of the proposed algorithm is not trivial. ## Cons1.It seems that the results of MTL SGD cannot recover local SGD? However, in local SGD, the additional error terms increases linearly with $\tau$, as shown in (Wang & Joshi, 2018). However, in MTL SGD, the time slots used per round can be exactly $\tau$ by allowing workers to have different number of local steps. The response and the updated version clarify my concerns.<BRK>In the level of workers, the local copies of models can be averaged within a sub network; however, workers cannot communicate directly with those from a different sub networks. In such setting, MLL SGD is proved to enjoy certain convergence property. # ProsHierarchic networks are a common object in practice, but current SGD algorithms fail to cover it due to the hierarchic communication restrictions. I think this paper can serve as a ground work for developing more efficient algorithms for training models in hierarchic networks. The proof mainly follows (Wang & Joshi, 2018) and the technical contribution is limited. It can be great if there are some experiments that involve training models in real hierarchic networks.<BRK>The paper proposed a multi level SGD algorithm, where workers are assigned to different groups, and each group averages local workers  model. My concern about this paper is that it seems to me the algorithm has a similar motivation with grouping SGD:W. Jiang et al., "A Novel Stochastic Gradient Descent Algorithm Based on Grouping over Heterogeneous Cluster Systems for Distributed Deep Learning," 2019 19th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID), Larnaca, Cyprus, 2019, pp. which is a straightforward extension of AllReduce SGD. I suggesting also comparing with it in the experiments.<BRK>The paper extends the idea of hierarchical local SGD by extending the top hierarchy level to decentralized communication. * A convergence analysis is given for the provided system formulation. however, in both theoretical analysis and numerical results, only the iid data case is been investigated. It somehow contrasts motivation. due to the extra gossip averaging steps over hubs, the current observations (i.e.improved convergence speed over local SGD) are reasonable but it is hard to identify the exact benefits of the proposed algorithm. is it possible to include the numerical results of hierarchical local SGD e.g.in Lin et al (i.e.All reduce is performed for hubs) as an extra baseline?
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 7. <BRK>However, I cannot find such an inclination from Fig.2.It is unclear that the presented methods can achieve a better fairness accuracy trade off than the existing fair representation methods. The authors derived the upper and lower bounds on the difference of accuracy between groups to demonstrate that imbalance in the groups  sizes leads to accuracy disparity. Furthermore, they propose learning algorithms enabling us to mitigate the accuracy disparity, which is accomplished by minimizing the upper bound they derived. The fair representation methods may work in the regression setting with small modifications, even if the original ones are designed for the classification setting. (1) and (3) in a game theoretic manner? This paper lacks a comparison with some important existing methods that share the same concept for mitigating unfairness. Also, I have concerns that the experimental results are something wrong. The theoretical contribution the authors claim to make is to clarify the cause of the accuracy disparity. Theorem 3.1 and Theorem 3.2 give insight into the authors  claim; however, they are slightly weak to support the statement that the group imbalance yields accuracy disparity. In this case, the minimization of the overall error can result in matched accuracy. Indeed, there is a counterexample of the claim. The authors design the proposed algorithm to match the distribution over the representation conditioned on the true outcome and sensitive attribute. These values are calculated by my hand from the MSE shown in these papers using the relationship $R^2   1   \mathrm{MSE} / \mathrm{Var}[Y]$.<BRK>This paper has two parts: (i) in the first part the authors provide an upper and a lower bound for error disparity among groups in the regression setup. (ii) In the second part, they propose an adversarial learning method to mitigate the disparate error among groups. Mainly they focus on reducing the error disparity by reducing the distance between the joint distribution of Z (the new learned representation) and Y between the two groups. The paper was finely written, and it was interesting. I will address my concerns regarding this paper as follows:First part: I did not understand the importance of Theorem 3.1 the lower bound is dependent on the classifier (thus is not inherent) and the authors only focused on the first term and did not talk about the second term. I think the authors should explain the other work on understanding the source of error disparity. Second part:My main concern regarding this part is that I feel like the authors did not place their work accurately among the related work for the fair representation part. I think the authors should explain why the regression setup provides new challenges on fair learning in comparison to the classification setup. In the experiment section, for the binary datasets do we get the same result as this paper if we use the fair representation methods in classification? What is inherently different here. Minor suggestion: I got a bit confused about equation 3, you wrote that the signature of f is Z > R but it seems that you are trying to minimize joint distribution. I think instead of the game theoretic interpretation (which is kind of clear) if you can expand on this and why the optimization is easier is better.<BRK>This paper theoretically and empirically studies accuracy disparity in regression problems. It proves an information theoretic lower bound on the joint error and a complementary upper bound on the error gap across groups to depict the feasible region of group wise errors. It further proposes to achieve accuracy parity theoretically and empirically by learning conditional group invariant representations using statistical distances. The discovery between accuracy parity and the distribution gaps across groups is interesting. The paper provides a deeper understanding of the accuracy parity, which is interesting to me and motivates the proposed algorithms. The motivation for the studied problem is not very clear to me. I know the importance of both regression and accuracy parity. But why we want to consider the combination of these two notions? What is the main difference between considering classification with accuracy parity? 2.Page 4, Geometric Interpretation. Mention Theorem 3.3 by a mistake?<BRK>I suggest authors include a simulated/semi synthetic dataset (semi synthetic versions can be created using the datasets that are already being used for experiments) and show clearly that their adversarial training procedure is likely to learn hypothesis within the feasible region that guarantees or is close to minimizing the disparity. 3."Nevertheless, throughout this paper we mainly focus accuracy parity as our fairness notion, due to the fact that three widely used commercial face recognition systems have been shown to exhibit substantial accuracy disparities between different demographic subgroups (Buolamwini & Gebru, 2018). I have concerns about this claim in the paper. The takeaway of highlighted problems with commercial face recognition is not that that the methods will be ready for practical use by ensuring approximate accuracy parity. I suggest the authors remove this motivation as facial recognition as a system is fraught with many other societal challenges that cannot be resolved by ensuring parity of performance. 4.Please be more elaborate in proofs in the appendix rather than having the reader fill in many gaps. "Now it is suffice to bound the term" proof of Thm 3.3
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 5. <BRK>Summary:This study aims to search for topology jointly with the network training. The topology is optimized by a heuristic search in structural modifications including adding/removing neurons/layers. Experiments on SVHN and CIFAR 10 are conducted to show the effectiveness. Pros:The paper is well organized. The proposed method enjoys good interpretability because the decision of modification is made based on the weight matrix properties by SVD and projecting onto its lower rank subspaces. Besides, the authors only claim that the method can be generalized to convolutional network. 2.The current experiments are not sufficient. It is weird to use fully connected network on real image datasets, CIFAR 10 and SVHN. The methods proposed in this paper are mainly heuristic implementations.<BRK>The paper seems to devise a set of modification modules and develops an overall genetic algorithm that utilizes the recommendation module to identify optimized topologies for networks. The paper is a combination of genetic algorithm with recommendation rule which builds on some tools like BIC. While these tools are organized in a reasonable manner, the overall idea seems rather simplistic and the contributions seem rather humble. For example, the surgeon is tested on very small networks. As such, the evaluation seems very primitive. Without the results of networks for large datasets like ImageNet, it is hard to give credit to their evaluation nor gain confidence about the performance of this method in real scenarios. Also, it would be very interesting to see the evolution of the topology. While this may be excessive for small networks like the ones authors rely on, I believe this may lead to new and interesting observations. Furthermore, it would be very inspiring if the paper presents some prospect of the work in improving the mainstream neural architecture search algorithms.<BRK>this paper presents the Surgeon, a ANN/evolutionary algorithm hybrid optimization de  signed for neural architecture search. On SVHN and CIFAR 10, the network generated by the Surgeon is able to outperform the baseline in case of suboptimal topologies, or reach comparable accuracies while pruning the underlying network structure to less resource intensive topologies. My major concern is the novelty, as the SVD technique and net2net techniques have all be proposed before. The experiment is not solid. The accuracy on Cifar is far from the state of the art. There is no large scale datasets. There s no performance comparison with related work, making the paper less convincing.<BRK>This paper presents a neural network training framework based on a genetic algorithm called Surgeon. The idea consists of two modules. The paper is well written and easy to follow. As the authors stated, the paper is the first of the series of work. However, I think the theoretical basis and experiments are a bit weak in their current form. First, the heuristic used by Surgeon, either by weights or by BIC, does not establish direct association to the objective function that the network is optimizing. Overall I like this paper, but for the two reasons listed above I think it may fall short to be accepted.
Reject. rating score: 3. rating score: 3. rating score: 4. rating score: 4. <BRK>The authors focused on the overparameterization of existing NN based unsupervised anomaly detection methods, and the proposed method aims to overcome the overparameterization problem. But because it seems the topic of this paper belongs to "unsupervised anomaly detection" (the labels indicating whether anomaly or not are assumed available in the training data), the point that your method is in an unsupervised learning fashion is pretty obvious. 2) In Introduction section, it is confusing what the main focus of this paper is.<BRK>The proposed approach differs from autoencoder based anomaly detection approach in the following ways(a) Autoencoders are trained using only selected data points with small reconstruction errors. Similar comments for Figure 3(b) and Table 1. However, it is key to understanding utility of such an ensemble based shuffling framework.<BRK>The analysis is reduced to "winning" the AUC score against the baselines on as many data sets as possible. This should be examined in an experimental evaluation. Putting this aside, both figures 3a and 3b are unnecessarily difficult to process. Are the theoretical results reflected in the experimental evaluation? If not, why? This should have been a much stronger focus of the paper, given that anomaly detection is very application driven.<BRK>### StrengthsThe paper follows a standard structure and is logically organized. ### WeaknessesMy main criticism revolves around the extent of the experimental section. the instructions for submission. * The formatting of Alg. * Table 3 is hard to read, this might be better in a figure.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 5. <BRK>**Strong points:**The authors propose to use soft labels to overcome the mislearning features in mixup images. **Weak points:**The contribution of this work is incremental. On Tiny ImageNet, it’s also helpful to show the performance of Mixup and CutMix. Some details are not clearly explained. The method should be evaluated on ImageNet. Why?Why is “target soft labels too far away” an issue for the model? Will the model take more time to converge or it won t converge?<BRK>This paper suggests a new Mixup approach, called LaMix, that does not require input mixing. The authors argue that LaMix achieves superior performance without input mixing. Although empirical results look nice, I am suspicious about the experimental settings for the comparison with other methods. Pros:  The paper includes diverse results on probing experiments. However, there is no explanation about the reason for using it. First, I think beta   1.0 means not using the global soft label, and it is equivalent to standard Mixup. If then, I think providing these results would be helpful to check whether the regularization effect is orthogonal to input mixing.<BRK>This work proposes to change the labels for the mixed examples in mixup. 1.The motivation of adopting soft label for mixup is not clear. Label smoothing is helpful for generic training but why it can benefit mixup? Additional experiments on ImageNet can make the results more convincing.<BRK>Comment1.This paper introduces a combination method between mixup and self distillation, and simply use an additional FC layer to have adaptive soft label, which is intuitive and clear but lack of novelty. Can you give more insightful comment about how adaptive label can help mixup soft label? 2.In Figure 3, I think the provided evidence for the effect of considering adaptive soft label with mixup approach is promising.
Reject. rating score: 4. rating score: 4. rating score: 6. rating score: 8. <BRK>Minor comments:\Background section should be made shorter, especially the part regarding the probabilistic box model training, which is not that relevant to the overall goal of this work. 4.It would be nice to see a visualisation of the learned embeddings. Although the proposed model shows promise by outperforming several baselines on the above mentioned dataset, I believe that the paper is not ready for publication in its current form, mainly due to (i) missing comparison to the highly relevant line of work on hyperbolic embeddings of hierarchical multi relational data; and (ii) lacking additional experiments on a dataset with more than 2 relations.<BRK>I assume if you apply the order embeddings to this multi hierarchical relation setup, then it is the same as the two box model? 12.I am curious about the performance of the proposed model in an imbalanced dataset (as introduced in Li et al.ICLR 2019), where the ratio of positive and negative is 1:10? Given there exists two hierarchical relations, the paper transforms the box of one entity under relation 1 to the box of the entity under relation 2 with a parameterized linear function. The model seems sound, however I have two major concerns. I guess it is the two end points of the box in one dimension. They include many strong baselines including the order embeddings, hyperbolic embeddings and even some KG embeddings. The results on the KG embeddings clearly show that their methods work much better in this (a little specific) hierarchical relation modeling setting. The paper also introduces a new missing edge setting, where they show that joint modeling achieves better generalization than independent parameters. I understand the page limit but these two aspects are essential to a machine learning model.<BRK>This paper builds upon the work of Patel et al.(2020) in modeling two hierarchies jointly within the box embedding framework. It also incorporates the GumbelBox formulation of Dasgupta et al.(2020) to resolve local identifiability issues during training. In contrast, Patel et al.(2020) does not have these constraints in their model. I find the novelty of these constraints to be incremental, especially in view that the joint hierarchy problem and evaluation methodology have already been formulated by Patel et al (2020) in the context of box embeddings. The paper is well organized and clearly written for the most part, but the exposition can be improved in some areas. * Section 4.1, (a <_1 b) ^ (b <_2 c)  > (a <_2 b): Could the authors provide examples of what <_1, <_2, a, b, and c represent? This means that the consequent should (a <_2 c) rather than (a <_2 b), no?<BRK>This paper deals with tree like structure embedding with box embedding on the lattice (poset). This paper is well motivated and well presented. Though there is a limitation on data structure, this paper still presents a novel idea in this area. This method also achieved promising results in experiments. Thus, I would like to recommend to accept this paper.
Accept (Spotlight). rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>The paper considers the problem of coming up with summary statistics for ABC type approximations. It is particularly remarkable as a lot of articles have been written on the topic of the summary statistics for ABC, without much progress in the last few years. Proposition 2 itself is not very surprising, given that the "posterior mean as statistic" is not designed to minimize the same information theoretic loss, and given the lack of fixed dimensional sufficient stats in generic models. I wonder if the choice of  expert  statistics could be commented on further. On the last page, "For future works, we can consider other infomax approaches." Finally, how does the method relate to theoretical works on ABC, such as Frazier et al or Li & Fearnhead?<BRK>page 6: "Note that the sufficient statistic"  > "a sufficient"? It focuses on using this statistic for performing posterior inference in a likelihood free setting. The results suggest the method is able to learn good statistics and compares favorably with other LFI algorithms. # Major comments## Pros:The paper is well written and easy to follow. Moreover, the introduction of SMC ABC and SNL+ is interesting as it improves the performance of the classic algorithms. Although the training procedure in itself is not new I think (see comments below), finding a sufficient statistic is conceptually new and is very relevant. 3) You don t write about minimal sufficient statistics whereas this is exactly what you are looking for as it is the one that will provide the best SNR. 4) Why don t you compare to more standard tasks such as the one used by SNL or SRE? I agree that your work is original and conceptually distinct from SRE. 2) This is just a question!<BRK>The authors pose the problem as finding the summary statistics function (in the form of a neural network) that maximises the mutual information between parameters and summary statistics. The study is technically sound and will be of interest to the likelihood free community and inspire further method development. ### QualityThe paper is technically sound, but some of the claims are not sufficiently backed by empirical evidence: in the main section of the paper, tables 1 3, the authors report a significant improvement in performance compared to previous methods (SRE and SNPE) (with the exception of the Gaussian copula case, where SNPE seems to have a slightly better performance). However, in the appendix, the differences in performance (using the MMD metric) between algorithms are much more nuanced and in fact, it is unclear what algorithm is the best. This is not described at all in the manuscript, and I would urge the authors to also include a few more details on the MCMC procedure in the supplement; what were the hyperparameters used for SRE and SNPE?<BRK>### SummaryThe paper introduces a new method for learning approximately sufficient summary statistics in the context of likelihood free inference. To do so, neural networks are trained with a loss maximising mutual information, using a JSD surrogate estimator previously proposed in the literature. The authors show how their approach can for example be combined with SMC ABC or SNL, leading to marked performance improvements on three problems. ### ScoreI appreciate the core idea of the paper as well as the empirical improvements over SMC ABC and SNL. Even though part of the definite answer about directly extracting the posterior from the MI network is left to future work and no test of their method on a high d example was included, I believe that the paper is a valuable addition to the literature.
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. rating score: 5. <BRK>Summary: The paper addresses the important topic of understanding why self supervised learning methods show very good performance when used as pretraining for fine tuning tasks. It is introduced very late in the paper and it is not clear where the proposed loss helps. Understanding difference between self supervised and supervised pretraining is relevant to advance in this field. The paper is well written and motivates the issue very well.<BRK>Summary:The paper draws an interesting research question: why instance discrimination (ID) pretraining good for transfer learning? The message has been deemed by researchers, but this paper shows empirical evidence with extensive experiments. Also, it seems that there is room to improve the paper presentation further. Also, the findings could be extended to few shot, semi supervised, and fully supervised learning regimes. Other comments:  This reviewer thinks that VOC and COCO datasets could be considered as a subset of the ImageNet dataset at a semantic level.<BRK>**Summary:**This work aims to explore why unsupervised contrastive pretraining works as well (if not better) than the tried and true Supervised ImageNet classification pretraining. Interesting findings:   Augmentation doesn’t make much difference for supervised transfer (from imagenet) and is essential for unsupervised transfer, with the effect monotonically increasing as more augmentations are added. 3.Finally, they look at the impact of this new pretraining on two other tasks   Few shot learning: The Supervised Exemplar model outperforms the unsupervised methods, and the original cross entropy Imagenet supervised model. **Positives:**  They introduce a supervised pretraining method that can transfer better than the unsupervised method and the original supervised imagenet method. Ultimately, I do believe I have a better understanding of the differences between the supervised and unsupervised pretrained models.<BRK>This paper presents a detailed analysis of the task transfer abilities of an self supervised representation, instance discrimination. The paper also proposes a new representation learning framework. However, the "take home" message of the paper is not very clear. There are a lot of empirical findings throughout the paper, but it is left to the reader to decide what to do with the empirical findings.
Accept (Poster). rating score: 8. rating score: 8. rating score: 7. rating score: 5. <BRK>The authors present a binning free calibration measure from a Kolmogorov Smirnov based test. Besides, the cumulative probability distribution is estimated using a spline based fitting from percentiles. The paper is clear and well founded.<BRK>While this should not impact comparative results, it would have been more satisfactory to use baseline architectures that match the state of the art. Additional experiments or discussions on the following would greatly help: 	As the spline method is not always better than Temp scaling, how do they compare from a computational viewpoint? They do it for both calibration and its measure: 	The use of the Kolmogorov Smirnov test to measure calibration between the target and the output distributions. This contradicts results reported in Table 2 (this paper) and Table 3 (Kull et al) and should be explained.<BRK>The paper presents a post hoc calibration method for deep neural net classification. That is only one way of measuring the calibratedness of a network, and furthermore that is the score the proposed method is maximizing. The method proposes next to recalibrate the classification probabilities by fitting a cubic spline to the KS test score. * Although the results in Tables 1 and 2 are groundbreaking in favor of the proposed method, they are calibration scores based on the KS score.<BRK>The paper proposes a post re calibration spline based approach for the re calibration of multiclass predictions. Experiment results on image datasets are provided and evaluated according to the Kolmogorov–Smirnov (KS) statistic. **Strengths**:   The proposed binning free calibration measure is a good idea and widely used in binary classification problems  As expected, experimental results demonstrate that optimizing calibration with a spline based approach results in better calibration**Weaknesses**:*Inconsistent notation*:   The authors should stick to either $x $ or $X$   The definition of the KS statistic is inconsistent with standard formulations Eq (8).
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 5. <BRK>SummaryThis paper introduces potential use of intermediate structured representation of input space called “concepts” which are most likely human interpretable. This intermediate space is then used for few shot learning instead of using only the input space. This leads to better classification performance on the task, and it shows that injecting human interpretable structured representation into task correlates with better performance (as one would hope). The paper uses datasets from different domains and shows improvement over approaches that don’t use the above defined “concepts”. Does that mean there is a non uniform distribution of part annotations per class? It could be that beak is important for most of the classes simply because it was present more often that the other parts. If I understood correctly, you need concepts (or part based annotations) to be consistently present in all input images and all classes, because if they are not and the distribution of concepts is heavily skewed towards a subset of classes, then your approach won’t be effective. Does that mean that your approach is only effective for fine grained classification datasets? Minor concerns (suggestions, typos, etc.) Table 1      Are the methods reimplemented or you cited results from papers?<BRK>**Overview** Inspired by DeepEMD s obersvation that compositional representations generalize better for few shot image classification, this paper (COMET) introduces "Concepts Embeddings" components to the Prototypical Networks. "Concepts Embeddings" are part based representations and are learnt by a set of independent networks $ \\{ f_{\theta_i} \\} $ (can also share weights). It takes Prototypical Networks as baseline and adds a novel "Concept Embedding" component. The authors show the proposed approach can significantly improve ProtoNet baseline on 3 datasets. The authors introduce a novel concept embeddings idea, and show that it can significantly improve ProtoNet baseline on 3 datasets. However using multiple prototypes per class is not a new idea and the authors should highlight the differences with prior works (what s the strength of this particular formulation?).<BRK>For the vision domain, the paper only evaluated this method on the fine grained classification tasks, which is uncommon in few shot classification literature. Following the prototypical networks, the method first computes the concept embeddings of an input, and then takes the summation of the distances between those embeddings and their corresponding concept prototypes in each class to estimate the class probability. The experiments validates the proposed methods on 4 benchmarks in three different domains, including vision, language and biology. For the biology task, the authors also develop a new benchmark on cross organ cell type classification.<BRK>This paper proposes concept learners to effectively combines the outputs of independent concept learners. The model is evaluated on several datasets from different domains. First of all, why the authors define it as generalizable few shot learning, the settings targeted in this paper seem to do no different from traditional few shot learning. The other two main concerns are:The idea of learning to attend different segments of an image or learning to the segment has been proposed in previous literature [a,b,c,d]. Even if they are not specifically targeted on a few shot image classification, the proposed concept learners are still pretty similar to previous works and are not specifically designed for few shot image classification tasks. In experiments, even if the authors choose three datasets for comparisons. I am more interested in results on standard benchmarks, such as miniImageNet, tieredImageNet. A minor point I am curious about is that by simple data augmentation method: crop, is it possible that multiple random cropping can generate different concepts and achieve similar effects by simply cropping multiple times on one image?
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 5. <BRK>It is an interesting idea to apply reinforcement learning to this problem. It defines the state of a target node as a vector from f_s by taking the searched neighbors as input. After a fixed number of steps T, the added neighbors are still within the 2 nd order neighbors. The presented approach shares a lot similarity with that in [1]. However, [1] is not cited, and not used as a baseline to compare. 2) Recent work should be aware. IJCAI 2020Thanks to authors for the clarification. I have updated my score.<BRK>**Summary:**The paper proposes a method for avoiding the oversmoothing happening in standard GNN methods. **Conclusion:**While the proposed method is sound and interesting and the paper has some good evaluations. As the other reviewers pointed out, the proposed method is computationally expensive. The authors pointed out that their contribution consists of proving that the adaptive receptive fields are useful, and I agree that an efficient method is not necessary for this, but the analysis should be made. The smoothness of the weights should be more clearly defined. The computational complexity of the constructor should be discussed. The weight s smoothness should have a clearly presented definition, especially to avoid confusions to the smoothness of the node features, as pointed out by the authors in footnote 3.<BRK>Summary:The authors theoretically and empirically show that soft attention mechanism uses in GCNs suffers from over smoothness in large neighborhoods. The reward is the performance of the trained GCN on constructed ARF. 2.Using RL might accompany multiple challenges involved with learning policy which might restrict the applicability of the approach. 2.Please explain the reason behind the vast difference in the performance of the baseline models on cora, citeseer, and pubmed datasets. Based on the issues pointed out by the other reviewers. I have decided to reduce my score for the paper.<BRK>This strategy can address the smoothness of attention weights issue, and capture long distance dependencies in graphs. Considering the following pros and cons comprehensively, I decide to give a recommendation of weak reject to this work. This observation is insightful and might helpful for future exploration. #####Cons#####(1) The main shortcoming of this work is the high complexity of the method. (2) Also, the dataset used in this paper is relatively small and some of them are shown to have data quality issue [1].
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 4. <BRK>Strengths:  The paper is very well written and polished. The method shows good results and is scalable, making it a valuable addition to the set of existing GNN operators. The authors make an effort to substantiate their statements about expressivity and scalability with proofs. Weaknesses:  The proven expressiveness is not a very strong statement, since most pooling approaches adhere to this property. Using the identity matrix as adjacency (as described in Appendix B, to work around the scalability issue of node clustering methods) seems to make the approach identical to the set transformer in all layers except the first, which dampens the contribution. There is some potential for improvement in clarity of presentation:  In abstract: "may yield representations that do not pass the graph isomorphism test", in introduction, page 2: "accurate representations of given graphs that can pass the WL test".<BRK>This paper proposes a Transformer like model: Graph Multiset Transformer to perform the graph pool/aggregation. The authors also prove the expressive power regarding WL test. However, several points need to be clarified or addressed. "Spectral clustering with graph neural networks for graph pooling." 1.The idea to utilize the Transformer like architecture to model the graph neural network (GNN) is not new. It’s better to make discussions in the related works. 1.About the experimental settings. I would like to see the evaluation about the time efficiency of GMT with other baselines. Overall, this paper is well written and the experiments look solid.<BRK>Update after rebuttal I have read the authors  rebuttal. Experimental results show the effectiveness of the proposed method. + The proposed method is interesting. + The experimental results are promising. Weaknesses:  Even though the method is called Multiset Pooling, its method is not related to Multiset. The pooling operation is defined as reducing n node input to k node output. However, in the proposed GMT, the GMPool is connected with a self attention layer. The use of graph structures is not very convincing. They should be discussed and compared.<BRK>This work proposes a Graph Multiset Transformer (GMT) that uses a multi head attention based approach to capture potential interactions between nodes when pooling nodes to produce a graph representation. Cons: Several parts of the manuscript need more explanations. Additional experimental results (see below for details) are needed. However, it is not clear how the proposed approach make improvement(s) on this. (6) The proposed method does not necessary pass graph isomorphism as the nodes in the manuscript have attributes but the proof does not consider node attributes.
Accept (Spotlight). rating score: 8. rating score: 7. rating score: 7. rating score: 7. <BRK>I have a few questions/remarks:(Q1) This is a general question. The author(s) provide a general upper bound on the width of $\max(d_x+2, d_y+1)$. It looks like this is the case, and it would be better to clarify this point. Thus, I am generally positive about the submission.<BRK>Summary: The authors tighten the analysis of the minimum width for universal approximation with relu networks for Lp functions, where they have an exact characterization in terms of the input and output dimensions. The paper is well written and easy to follow. minor comments. However, step functions are not ideal from a practical perspective due to the a.e.<BRK>I recommend the paper be accepted, since the paper provides exact bounds that close the gap between lower and upper bounds, and that helps us understand these networks better.<BRK>With that disclaimer, here are my opinions of the paper. The paper provides a very interesting set of results. First, it fully solves the problem and gives exact bounds, where prior works have only given bounds that without getting the precise constants. I had a few suggestions on this paper, that I feel could make the results in this paper even better. Currently you seem to require one for every input coordinate.
Reject. rating score: 3. rating score: 3. rating score: 4. rating score: 7. <BRK>**SUMMARY**This work proposes a novel neuro algorithmic policy architecture for solving discrete planning tasks. **STRENGTHS**  The general idea of integrating BlackBox combinatoric shortest path algorithms in a differentiable planning module is interesting and has a lot of potential to be useful. The related work is also missing numerous relevant papers. Which ideas are used from related work and what is new to this work? I still recommend rejection of this work given that the novelty is not yet fully clear. Relevant information on training (e.g.which optimizer was used? This does not hold in most planning tasks. Simply using the PPO baseline is insufficient.<BRK>Summary: The paper studies the problem of image based planning in discrete state/action spaces. The paper proposes a neuro algorithmic policy that can be trained end to end by differentiation and used together with a shortest path solver. Strengths:i) The motivation, organization and the overall writing of the paper are clear. Similarly, experimental comparison to work [3] is missing. [6] Scalable Planning with Deep Neural Network Learned Transition Models, Wu et al.JAIR.[7] Optimal Control Via Neural Networks: A Convex Approach, Chen et al., ICLR 2019.<BRK>[2] Yonetani, Taniai, Barekatain, Nishimura, Kanezaki. Path Planning using Neural A* Search (2020). The method is trained in a supervised manner from expert trajectories. #### Decision:Given the low novelty and not so strong evaluation, I do not recommend this paper for acceptance. [1] Vlastelica, Paulus, Musil, Martius, Rolinek. Differentiation of Blackbox Combinatorial Solvers (2020).<BRK>This paper proposes a policy architecture that embeds graph searchwithin it. Overall, I think this is a very solid work, and I recommendacceptance. The biggest concern I have with the paper is that it is extremelyunclear what exactly is meant by the assumption of a known topology ofthe gcMDP. If so, then itseems unfair to compare to IL and RL baselines that are not givenaccess to this information. I am also curious how much stochasticity in the environment can behandled by your method. Perhaps a path forward here would be to learn an embedding toa discrete latent space, and then apply NAP.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>The paper proposes a tensor formulation as a unified framework for studying different architectures of neural networks. This formulation is mainly used under the special case of linear networks with “orthogonal” tensor representation to provide a unified proof of many existing results on the implicit bias of gradient descent under different architectures. Overall, the tensor formulation is an interesting and new (to my knowledge) abstraction. 3.One of the main differences from the existing work is claimed to be the fact that this analysis does not make assumptions on convergence of loss.<BRK>The paper gives a general framework of what they call linear tensor networks, which is essentially linear neural networks that is expressed in the tensor formulation. They further corroborate their results partly by a few very simple regression and classification task. The framework is written with examples of simple models. 3.The incremental contribution from prior work is valid. While the setting that the paper has considered is still arguably over simplified, I still view it as a nice step toward a better understanding for the implicit bias of the gradient flow for linear neural networks that offer nice insights. If not, some examples will be nice. Also, I believe it will be helpful to say a few words first on the assumptions of matrices, for theorems like Theorem 7.<BRK>Here are some minor comments:1. For linear tensor networks, I think eq.(6) is actually a multi linear form. It is also nice to have a unified analysis of the implicit bias which also removes a few assumptions from prior results. On the other hand, I think it would be better to include a thorough comparison between proof techniques used in this paper and in prior work.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 8. <BRK>It shows that by appending the input to each mid layer s output, one can use a deeper network to get better learning performance. The idea is very similar to the residual connection or the skip connection. ##########################################################################**Strengths**:The paper is well written and provides PyTorch code that is easy to read and understand. It experiments across a wide range of environments and tasks. This paper uses concatenation. Such an idea is not new to the learning community. There is nothing specific in RL that prevents one from using such standard techniques in the networks either. The novelty of the paper is limited. What about using addition instead of concatenation? Each layer will be `y x+f(x)`, which is fairly common in many deep networks. How does it perform? The paper only shows the results with one kind of network width. * It is not clear how many skip connections are added in CURL.<BRK>This paper proposes a deep neural net structure for deep reinforcement learning methods (e.g., SAC) to replace the original fully connected layers, by concatenating the state input into every hidden layers. The authors conduct experiments on OpenAI gym and MuJoCo environments and show that the proposed structure can further improve the performance of SAC or CURL. The biggest issue of this work is that the proposed method, regardless of the activation function, is similar to a special version of resnet. Stacking residual layers can make it possible to have skip connections from every layer to any layer after it. At least, resnet should be compared. It seems this structure is independent from the RL setting.<BRK>In this work, the authors propose a neural network architecture that concatenates the input state with hidden state activations over multiple layers in order to train deeper networks in an RL setting. The idea of residual connections or concatenation to improve stability of networks is not a new one. Although there is nothing technically wrong with this paper and there is an improvement over a vanilla network, I do not feel the work is enough for a publication at ICLR, the work is not novel enough and the authors should focus on bigger steps rather than incremental work. More ablations, particularly vs. resnet architectures, it would be good to see figure 2 with a resnet comparison. Update:I thank the authors for their significant updates to the paper.<BRK>This submission takes inspiration from work on deep learning architectures for visual tasks in order to make targeted model changes to deep reinforcement learning models. The authors show that by including “dense connections” (concatenating the state or state action pair to the input of each hidden layer of the network) they are able to successfully train deeper networks. The paper is also well written and presents a thorough set of experiments, making it a good submission for ICLR. * The authors evaluate their method on a diver set of tasks, and their model outperforms the benchmark for the majority of tested conditionsConcerns and Questions:* The results comparing the ResNet style architecture with the DenseNet style architecture are interesting, particularly because the ResNet architecture does not see the same benefit. * It is unclear why the authors chose to use a 4 layer D2RL. It looks like an experiment was done in 6b varying the number of layers, but perhaps introducing this earlier (ie as a direct comparison to Figure 2) would make this choice more clear.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 7. <BRK>This paper focuses on contagious disease prediction with the consideration of observed data bias and patient exposure. The authors present a Model for Infections under Incomplete Testing (MIINT) and the experimental results show the proposed model outperforms baselines on *some* metrics. The paper is not well written and pretty hard to follow. It is unclear why the authors use w^1(X) in eq (1). Why should t be set as 1? The authors want to consider the exposure relationships between patients. The authors just simply use the count of exposure of observed true infection patients at the last time point. 4.Due to missing technical details (especially how to generate Q), it is hard to re implement the proposed models.<BRK>The authors propose a recurrent neural network based model to impute the infection statuses over time, incorporating information on both the individuals (features of the individual) as well as a contact network that defines which individuals are related to each other. The problem the authors describe is interesting and important. If not, what is an ancestor? This doesn t appear to be defined**General: What assumptions are being made about the missingness mechanism for the true infection states. That seems to be what the authors are getting at with their conditional independence assumption but i) it s not obvious that assumption is sufficient (see above) and ii) the authors haven t done a great job of linking that assumption to their argument for the efficacy (identification) of their model. It implies a sort of ground truth causal efficacy to the work when this is not causal work. Minor:Throughout the paper there are several instances of incorrect grammar or other writing issues that make it hard to determine the meaning of the sentence in question. This is also a bit weird of a way to define it: the probability of being tested is the authors  numerator P(Ot   o) and the conditional probability of being tested (or propensity for being tested) is the authors  denominator p(Ot   o | Xt) so it doesn t make sense that the inverse should be this fraction rather than just 1/(the numerator). It s unclear to me whether this is a confusion on probability algebra or these some other assumption being made here (overlap should not be sufficient).<BRK>This paper formulates the contagious disease into a missing label problem with dependence between each data point. The paper targets an important problem, especially in this pandemic, and the effort is greatly appreciated. However, the writing of this paper is confusing and it makes it hard to catch the main contribution of this paper. There are some concerns: 1. If my understanding is correct, I am not sure why the authors choose the current formulation rather than graph one. 3.All the experiments are self compared and make the result less convincing. For example, it is not clear whether different NN network would result in different performance because only NN is fixed here.<BRK>In this work, the authors propose an approach, MIINT, for identifying infected individuals using a network based approach. A detailed simulation framework is used to compare MIINT to relatively weak baselines. The authors also identify and empirically evaluate the conditions under which the model breaks down. I also appreciate the effort of creating a simulation model to reflect how the spread may develop over time; I believe the simulation framework could also be useful for other researchers in this area. The model may also be relevant for other network spreader domains, such as “influencers in social media” studies. My main concerns about the work are about the experiments. Second, it is not clear to me of the practical significance of the “isolation policy” results. Even using an oracle, Figure 1 seems to suggest that between 30 40% of the population would need to be tested to see a reduction in the infection rate. Is there any explanation for that?
Reject. rating score: 5. rating score: 6. rating score: 7. <BRK>The experimental results are technically sound but would be improved by adding a stronger baseline. It would make more sense to compare to the RNN/RNN model described in the Related Work section, or to another method of data preprocessing that reduces dimensionality along the time axis. **To improve the paper. Then they train a neural controlled differential equation (Neural CDE) on the transformed dataset and show that their model learns more quickly and achieves better test generalization. Make sure the reader is able to quickly grasp how a log signature of, say, order 2, works in practice. ** This paper is technically sound and the method shows clear improvement over “no preprocessing.”**Weak points. One way to improve the experimental results would be to compare to a stronger baseline such as RNN/RNN. Strangely, the title “Neural CDEs for Long Time Series via the Log ODE Method” implies that they are proposing a new model. The only difference is that here the authors are using a continuous time analogue of an RNN. This work seems very similar to Liao et al (2019); compare Figure 1 in the two papers, for example. To the authors: are there ways that we can simplify the theory section so that it remains technically correct while also being readable and accessible? The experiments are technically sound and the results are presented well. However, it is surprising that the authors did not compare to the RNN/RNN model that they mentioned in the second paragraph of related work. ** The Log Signature method has already been shown to be a viable preprocessing technique for time series data. The main contribution of this paper is to show that it also works with CDEs.<BRK>### **Summary and Contributions of Paper**This paper proposes a new method for computing Neural CDEs via the signature transform, which transforms a path integral into log signatures, i.e.a collection of iterated integrals. Then standard ODE tools are applied to each piecewise log signature. ### **Strengths**  The writing quality is rigorous. While the authors claim that there is an ease of implementation via pre existing tools, the larger bottleneck seems to be actually understanding the method itself (which seems to also be a function of how the paper treats this material). While I have no doubt that this work would be great for a very mathematically minded community, I am unsure of its merits for the ICLR conference community. I think the authors should provide more high level overview of the signature transform, and keep the strict math in the appendix. I am not an expert on these types of methods, so my confidence will not be as high, but I believe that this paper contributes via its insight with the signature transform, and thus my rating is marginally above the acceptance threshold.<BRK>Summarizing the paper claims The paper introduces an approach that allows training Neural Controlled Differential Equations (CDEs) for long time series. The authors propose to use log signature as input instead of the original time series. Log signature can be understood as a lossy representation for time series, which has a much smaller length and varies slower over the same time interval. Hence, larger steps may be used in the numerical solver, and that yields to the training speed up. By applying log signature transform, the solution of the CDE may be approximated by the solution of the ODE. The experiments are conducted for four real wold problems (worms classification, predicting a person s heart rate/respiratory rate/oxygen saturation). That would be interesting to see how good is a proposed method comparing to the RNN based one in terms of test accuracy/training time/memory usage. Questions   Which type of ODE solvers has been used for the experiments? Does the solver s order influence the neural CDE performance? Could you provide time and accuracy for these experiments? Haven t you tried to tune training hyperparameters to reduce it? I m curious why in Table 1, for the same number of solver s steps, the training time for $NCDE_2$ is longer than for  $NCDE_3$? (It seems that for the same number of training epochs, the time should be shorter because we do less preprocessing computations for log transformation)
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 6. <BRK>This paper presents a new knowledge distillation (KD) method for distilling BERT. Not to mention *pairwise* and triple wise relations are being modeledMy biggest question in the paper (which the doesn t paper address) is that this is bound to be expensive. Yet there is hardly any mention of training time (or time needed to cache these values from the teacher). This seems even worse when there is not much gain over existing baselines and can be attributed to simply noise/variance. Overall, I don t think this method will be impactful at all and it is probably not worth having over existing approaches. It is far too complex. Experiments on the GLUE benchmark alone is also not convincing. The authors can try other tasks and perhaps SuperGLUE to make their experiments more convincing. How much more expensive is it to align and compute these values during training of the student. Please make this clear.<BRK>This paper presents an interesting knowledge distillation method based on a newly defined contextual knowledge of transformer based models. The proposed contextual knowledge models the pair wise or triple wise relations across BERT based contextual representations, based on which the local structures between Teacher and Student models are encouraged to be well aligned. The main contribution of this work lies in the newly proposed two types of contextual knowledge: Word Relation and Layer Transforming Relation. Compared with existing BERT compression methods, like MobileBERT/DistilBERT/TinyBERT, this CKD has the advantage of being directly applied on top of other pre trained small BERT models without conducting time consuming pre training process. The authors evaluate their approach on GLUE datasets and compare it to other state of the art models. However, I have several concerns:* This proposed KD method is designed for the distillation on downstream tasks, so the whole distillation process should be conducted for each task, while the task agnostic KD method, like MobileBERT, can be directly used with fine tuning, please identify this fact in the introduction part. It would be more interesting, if experiments can be conducted during the pre training stage and further evaluated. * More experiments on challenging tasks like QA should be added. * In the Table 1, the performance of TinyBERT on MNLI mm is 82.6, while in an old version of tinybert paper, (https://arxiv.org/pdf/1909.10351v4.pdf), in the Table 10, the corresponding value is 83.2. And on the official GLUE benchmark the TinyBERT has comparable performance as the proposed CKD(w/DA). This comparison is not that fair, since MobileBERT can also be improved by other self distillation method. * In the section 5.3, “we observe that the BERTMINI trained with the CKD shows the higher averagescore even though BERTMINI has fewer model parameters.” this comparison is unfair since BERTMINI has 6 layers and TinyBERT is a 4 layer model, and less number of model parameters does not always mean fast inference.<BRK>This paper proposes two new distillation objective, word relations and layer transforming relations. Word relations constrain the pairwise/triplet relations of embeddings at each layer to be closer to the teacher network. The layer transforming relations constrain that the pairwise/triplet relations of embeddings between different layers should match the teacher network. pros:1.The methods in this paper consider the pairwise or higher order (i.e.triplet)  relations to constrain  the student embeddings, while previous methods usually consider embeddings separately. 2.Comparison with previous methods, ablation study and other experiments like model sizes, etc demonstrate the effectiveness of the proposed method. In some cases, the student network even outperforms the teacher network (more explanation about this might be needed). Cons (or questions): 1. As a result, I am not sure whether the number of parameters are comparable when comparing to the baselines. Could the authors also show the number of parameters of previous methods and their own methods?<BRK>The paper proposed a contextual knowledge distillation approach by leveraging two types of contextual knowledge: word relations and layer transforming relation. Recent advancement in this area emphasizes the promising effect of this area in language modeling. The paper is well written and well structured. The experiment section shows a complete set of experiments including the baselines, benchmark and ablation study. The results are relatively incremental in comparison with TinyBert. Considering that the improvement has been relatively incremental, it would be helpful to compare the models with respect to FLOPs and speedup. Novelty: It seems that the notion of structural knowledge distillation have been used previously by Wang et al [1]. It would be great if the authors clarify about their contribution.
Accept (Poster). rating score: 6. rating score: 6. rating score: 5. rating score: 4. <BRK># SummaryProposes contrastive learning method for conditional text generation. Additional positives and negatives are created in the sequence representation space by adding perturbations to decoder (output) hidden states to minimize/maximize conditional likelihood p(y|x). Modest improvements over T5 small are observed on Translation, Summarization, and Question generation seq2seq tasks. # Pros1.Diversity of seq2seq tasks, with consistent improvements over baseline T5 MLE (small). 3.Complementary to seq2seq MLE training and can be used to improve it in general, not just text generation. 2.Please add SOTA results in the tables for the various tasks for reference. 4.Since this is generation, more non cherry picked example decodes would be informative to have in the appendix. 5.Even better would be some basic human evaluation of generated outputs to verify whether meaningful quality improvements are made. 6.Scheduled Sampling (Bengio et al) should be discussed and perhaps compared as it is a well known method for addressing exposure bias. Are all the models initialized with T5 MLE or are they trained from scratch on C4 for the same number of steps as T5 MLE?<BRK>This paper presents a method for conditional text generation tasks that aims to over the "exposure bias" problem through contrastive learning where negative examples are generated by adding small perturbations to the input sequence to minimize its conditional likelihood, and positive examples are generated by adding  large perturbations while enforcing it to have a high conditional likelihood. My only concern is that compare to MLE, the improvements either on Table 1 or on Table 2 are relative small. So I think in the experiments, all results should be compared using the trick of "temperature sweep". Hopefully the authors can address my concern in the rebuttal period.<BRK> Paper Summary:This paper proposes to add contrastive learning to the sequence to sequence generation problem. For example, how many hard negatives are actually being recognized as negative by human. Some human evaluation on a larger set of generated examples would help. Overall reviewAlthough the proposed method seems to be effective and new, the concerns outweighs the contributions in my opinion. Please try to address my concerns during the rebuttal period. The method is only applied on a small version of T5, which is limited. How about other pretrained (potentially larger) models? How about non pretrained models such as randomly initialized Transformers/LSTMs? Detailed ReviewThe authors claimed to mitigate the exposure bias problem for sequence generation. It does not mitigate train test mismatch at all. This is inconsistent with the intuition of near negative and distant positive claimed in the paper. In experiments, maybe add a CLAPS w/o negative so that readers would know which is more important.<BRK>  Overall commentsThis paper propose a principled method to generate "hard" positive and negative samples based on conditional likelihood for contrastive learning of seq2seq models, and it shows significant improvements in training conditional text generation tasks compared to naïve approach with random negative samples. Overall, the idea is interesting, and the experiments are well conducted. However, I still have some detailed questions regarding to the method and experiment as follows:  Methods:(1) I am a bit confused with Eq(2). *It would be nice to include for discussion or comparison. It is possible the hidden states may not be corresponded to any sentences. Experiments(1) It seems that all experiments are initialized with T5. Does it mean that the proposed method only works with large scale pre training? It would be more important to show results with training from scratch. (3) For many tasks, the improvements of the proposed method are actually marginal. It may improve the paper by include discussion of statistical significance. (4) There are also methods such as Reinforcement learning which also aims to overcome the problem of teacher forcing. It should be also discussed in experiments.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. rating score: 5. <BRK>They also show that disentanglement does not seem to correlate with better generalization. In the real world, human like performance might require the ability to construct *new decompositions on the fly,* because the appropriate decompositions may change as the task or data shifts. They also showed that disentanglement did not necessarily lead to better generalization, with a wider range of experiments (though therefore perhaps less deep in evaluating different types of generalization). This would be necessary to allow the sort of attentive disentanglement described in the previous point. The paper uses a single dataset (though with two different tasks), and it is very toy. "Transforming task representations to allow deep learning models to perform novel tasks." For example, Hill et al.(2020) showed that e.g.generalization was better in a 3D setting than 2D setting, and that an RL agent showed 100% compositional generalization in a setting where a classifier only showed ~80%, for instance (although this generalization was recombination, not extrapolation). Thus, the poverty of the stimuli may alter the paper s conclusions.<BRK>3.The paper should test the idea more thoroughly, on more datasets and on more disentanglement approaches. For example, it could include other datasets or tasks with different ground truth factors of variation (e.g., 3D chairs [1]). Previous work has not distinguished between these kinds of generalization whentesting how disentangled representations generalize. It could also include more disentanglement approaches like [2]. Seeing 3d chairs: exemplar part based 2d 3dalignment using a large dataset of cad models. In CVPR, 2014. Strengths The central claim of the paper may help clarify the disentanglement literature. For example, initially one might think that beta VAE is inherently disentangled. This paper aims to characterize an existing line of work in detail rather than proposing a new approach/dataset/etc. The experiments that were run don t support a clear conclusion and more experiments should have been run to support a more general conclusion. Significance   This paper might help clarify the disentanglement literature and more broadly help people think about combinatorial generalization. * The main support comes from table 1.<BRK>Summary:This paper studies the performance of models producing disentangled representations in the downstream task of combinatorial generalization. The experiments suggest that models producing disentangled representations do not generalize well enough. The study was conducted only for one dataset; I would suggest to include several other datasets in your study, e.g., MPI 3D, Shapes 3D, Cars 3D datasets. How do you think, why it does not hold as well for combinatorial generalization? Minor comments:  In some places, you write "generalization", in other   "generalisation". UPD: The authors addressed my concerns and added additional experiments. The paper is improved, therefore, I increase the rating. international conference on machine learning.<BRK>The paper shows that the models support only weak combinatorial generalization. The paper concludes that learning disentanglement representation is not sufficient for supporting more difficult forms of generalization. WeaknessesThe paper s study is limited to beta VAE and dSprites dataset. The paper should conduct experimental studies on other datasets, e.g.those in the above reference. However, the paper s study is very limited to specific model and a single dataset. This begs the question how good the GT decoder is. For an experimentation paper, it should be more thorough and go beyond just two shape datasets. I applaud the additional results the authors provided. I still think the paper is borderline (more toward 6 now).
Reject. rating score: 5. rating score: 5. rating score: 6. <BRK>The main result is a communication lower bound, which basically follows by picking the approximation parameter so restrictive that each party has to send the minimum of its f_i, to appropriate precision, in all dimensions. *******************************************Communication complexity is an important bottleneck for optimization. The theoretical developments in this paper are not particularly exciting and the writing is not great.<BRK>While the problem studied in the rest of the paper is cast purely in theoretical terms in the classical Message Passing setting in distributed optimization, and therefore a conference with more theoretical bent seems more appropriate, given the many prior works that have appeared in machine learning conferences I am not too worried on this count. A couple of concerns:1) Given the result for the two machine setting, what would one expect to be the lower bound in the N machine setting?<BRK>This paper studies the problem of optimizing a sum of $\beta$ strongly convex and $\alpha$ strongly smooth functions in the distributed point to point communication setting. It provides deterministic and randomized lower bounds on the communication bit complexity and a quantized based algorithm with asymptotic matching upper bounds (for constant $\beta/\alpha$). If it is mainly of theoretical relevance, then it would be nice to give reasons why this optimization task is an important one.
Reject. rating score: 5. rating score: 6. rating score: 7. <BRK>In this paper, the authors propose the usage of complex numbers in deep neural networks. all can be used in neural networks. The authors also claims benchmark performance in large scale image classification and language modeling. It seems that the authors took the second approach, however, ImageNet is not that challenging and there may be no clear need to switch to complex numbers. The figures seem to show good advantages. I would hope the authors clarify their methodology, and then present that advantages obtained in the experiments. This paper is working on a huge title, which is attractive.<BRK>The paper proposes an interesting kind of networks, AlgebraNets, which is a general paradigm of replacing the commonly used real valued algebra with other associative algebras. The work in the paper is interesting and this paper is generally written well. However, there are a few issues/comments with the work:1.The citation of the references in the main body of this paper is not easy to read. It will be better to replace the format “author(s) (year)” with the format “(author(s), year)” ;2.Some figures and tables do not appear near the discussion, for example, Figure 1 is shown on Page but it is discussed until page 5, which makes it difficult to read;3.In Figure 1, the subfigure in the second row and first column, it seems that the performance of model with H and whitening the best stable performance. The subfigure in the second row and second column, it can be seen that the model with H  is not better than the baseline model;4.There are many inconsistencies in the format of the reference, for example,1)In some places the author s name is abbreviated, while in others it is not. ".However, after getting the response from the author(s), I more doubt the significance of the work in this paper: although many types of models have been proposed in this paper, the improvement over the baseline models is limited.<BRK>## SummaryThe authors propose AlgebraNets   a previously explored approach to replace real valued algebra in deep learning models with other associative algebras that include 2x2 matrices over real and complex numbers. The paper is very well written and follows a nice narrative, and the claims are mostly backed empirically with experimental results. ## Pros * Empirically justified with experiments on state of the art benchmarks in both computer vision and NLP. * Establishes that exploring other algebras is not just an exercise for mathematical curiosity but also practical, and encourages deep learning practitioners to extend the results. * Are certain algebras more amenable to specific hardware architectures?
Reject. rating score: 5. rating score: 5. rating score: 7. rating score: 7. <BRK>This paper mainly improves the idea of "PRUNING FILTERS FOR EFFICIENT CONVNETS" by encouraging the pruning with a {0 1} optimization instead of a greedy manner. Experiments validate the effectiveness of the proposed method. Cons:  The novelty is limited. The formulation of the 0 1 optimization for pruning is simple and intuitive. For me, I am not sure whether the novelty is up to the standard of ICLR venue. Authors claim that current pruning papers can not reach a strict constraint for FLOPs during pruning. Necessary discussions are needed.<BRK>Summary：In this manuscript, a new pruning method is proposed by considering the inherent quadratic constraint between consecutive layers. Strengths: 	The paper is well written and well motivated. Weaknesses: 	I am not an expert in the area of pruning. Moreover, I believe the results should be evaluated from more aspects, e.g., the actual latency on target device, the memory consumption during the inference time and the actual network size. I am willing to change my rating according to the feedback from authors and the comments from other reviewers.<BRK>This paper introduces an optimization method for pruning channels in networks. Then the authors introduce a QCQP optimization method that can constrain the exact amout of resources during the optimization process. Extensive experiments are conducted on different benchmarks with different backbones. ####### Strengths######+ The motivation is clear and the presentation is generally good. + Extensive studies have been conducted in terms as different datasets/backbones. It would be nice to discuss this when the optimization is introduced?<BRK>The authors proposed a pruning method that aims to reduce the parameters and heavy computational cost of large convolutional neural networks (CNNs). Or could this proposed strategy be applied to reduce the parameters of the CNN models to the level of MobileNet? (2) Authors determined that the proposed method is useful to tackle several classification tasks, did this method also perform well on the CNN models aimed at segmentation, detection et al.Performance of this kind of models may decrease more than the classification.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 6. rating score: 6. <BRK>The paper proposes AuxiLearn, a framework that can be used to combine losses from multiple auxiliary tasks (if present) into a single combined, loss function that does not require expensive grid search over possible linear combination.<BRK>how to design useful auxiliary tasks, (2) how to combine auxiliary tasks into a single coherent loss. The paper is globally well organized and clearly written. 2.The paper proposes sound the technique contributions.<BRK>This paper proposes to make use of implicit differerntiation to improve auxiliary learning, including learning to combine the manually designed auxiliary tasks and learning new auxiliary tasks. In general the experiments could have been stronger. Overall the paper is clearly written and technically sound.<BRK>3.Comprehensive experiments are made to verify the effectiveness of the proposal. Cons:1.The idea of learning new auxiliary tasks is wired and less intuitive.<BRK>This paper proposes a way to combine the auxiliary tasks  losses.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK>As I m not an expert in reinforcement learning, I will leave the novelty and significance to other reviewers to decide. So what s point to define interference with different granularity if only one is used in the end? 3.The coefficients of forgetting and interference in the subfigures of Fig.1 should be provided. The authors also introduces three different types of representation loss and OML obviously outperforms the others, but there is no enough analysis about why OML is supreme and then no guidance of how to choose a proper representation loss.<BRK>The authors draw the connection with previous methods and introduce some reasonable measure of interference. Without an extensive empirical analysis, and considering the absence of a rigorous theoretical analysis motivating the proposed interference measure, I think this paper is not ready for publication and I encourage the authors to improve it, especially showing stronger and more significant empirical evidence over representative baselines, perhaps even considering some multi task and/or lifelong learning problems where interference constitutes a bigger issue. I think that this paper studies an interesting problem, but its analysis is a bit superficial and not supported by rigorous theoretical analysis. I still think that this paper has the major problem of presenting results about interference that are not strong enough to be published.<BRK>SummaryThe paper studies interference and forgetting in the context of reinforcement learning (RL). The paper is written clearly and generally easy to follow. The proposed method for learning representations is based on meta learning. An ablation study will improve the quality of the paper. Similarly, the authors do not compare with other methods that address the forgetting in the RL context. The proposed measure of forgetting, being a difference of current returns and previous best returns, has limited novelty too.<BRK>It designs new measure for interference, and shows that the interference measure is correlated with forgetting. Overall, the paper is well written but I have some questions regarding the logic of the paper. If this is not the case, then why forgetting is an interesting measure to compare with?
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. <BRK>It claims that the failure of knowledge distillation in object detection is mainly caused by the imbalance between pixels of foreground and background, and the relation distillation between different pixels. The authors then propose non local distillation to tackle the problem. Extensive experiments are conducted on MS COCO and verify the effectiveness of the proposed method.<BRK>I believe it would be helpful to summarize the key difference/similarity wrt to the previous work, thus to provide a better understanding of the relation in a larger context. After reading the authors  response and other reviews. The authors provide very detailed information to reproduce the method. The observation that a high AP teacher is important for distillation is quite intriguing.<BRK>The structure of the paper can be improved. The related work section for example can be moved earlier in the paper to give the bigger picture and the position of this work with respect to the literature. No explanation is given on why the proposed method leads to this behavior.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. rating score: 5. <BRK>Summary Current methods on adversarial robustness certificates consider data points independently which are highly pessimistic for structured data. This work proposes the first collective robustness certificate that considers the structure of the graph by modeling locality in order to derive stronger guarantees  that the predictions remain stable under perturbations. In summary, I like the novelty of this method and the through experiments that were conducted that illustrate the efficacy of the proposed collective certificate, thus I recommend an accept. The paper tries to address a very common problem of adversarial attacks where data points are structured.<BRK>** Summary:In the context of structured prediction, where multiple predictions are simultaneously made based on a single input, this works argue that existing robustness certificates independently operating on each node prediction end up with overly pessimistic results. I think this is a valid method to assess robustness of classifier satisfying locality like GCN. The motivation, arguments and results are convincing. ** Strength:   This work is well motivated.<BRK>This paper addresses the limitation of the existing adversarial robustness certificates that ignores that a single shared input is present, and thus assumes an adversary can use different perturbed inputs to attack different predictions. The problem and the method are both clearly presented. I think the collective robust certificate has some impacts for robust graph node classifications.<BRK>This paper studies classifiers that collectively output many predictions based on a single input. This paper proposes a collective certificate that computes the number of simultaneously certifiable nodes for which the predictions can be guaranteed to be stable (not change). Pros: This is the first effort that considers collective robustness certificate.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 6. <BRK>I applaud the authors for taking my feedback on board, and certainly many changes have been made to improve the paper, but my main concern of novelty regrettably still remains. The paper offers a very simple improvement over Jung et al (effectively scaling the importance measure), which I believe is quite incremental in this setting. While I believe simple advances can often have broad impact (eg.dropout, batchnorm, etc), in this case it is not clear that the proposed change offers any benefits outside of the very specific area of importance based continual learning.<BRK>This paper proposes a method that tackles the problem of catastrophic forgetting in continual neural networks by assigning importance to neuron activations while tasks are executed in sequence. Similar to previous research (Jung et al., 2020), the proposed method measures neuron importance using average activation values divided by corresponding standard deviation. The method is tested in benchmark datasets for continual learning. This is an important aspect to demonstrate the validity of the results. This compromises the novelty of the proposed approach. I am missing some insights on how the experimental results are connected to the motivation of the proposed method.<BRK>This paper introduces a regularization approach for stable continual learning of sequential tasks. The proposed method computes neuron importance based on the activation values of nodes with their respective standard deviation. Experimental results do not show the proposed method is promising. From the analyses, the paper proposes an approach to defining new neuron importance, as shown in equation (2).<BRK>In doing so, the authors introduce neuron importance as weight factor in minimizing catastrophic forgetting via regularization term. They also investigate continual learning by changing the order of the tasks. Strengths:  introduces the neuron importance in regularization terms for minimizing catastrophic forgetting    the paper is well written   sound quantitative evaluation and performance analysis  using several data setsWeaknesses:  lack novelty and limited methodological contributions   it is unclear the need for introducing neuron importance in regularization techniques for catastrophic forgetting, since the activation value of neurons is also derived from the network weights  the gains are marginal for some data setsQuestions:  Since neural networks (neuron activity) is too sensitive with change in representation (or input) and therefore, does not guarantee stable results on a particular historical task. This work focuses on determining neuron importance by activation value, however does this activation value correlates to task performance? Since neural activity/importance is calculated based on the network weights, introducing neural importance as well as weight regularization is redundant? In ICML 2020.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 5. <BRK>The authors also reinforced their empirical investigation by reporting standard deviation of the results, which allows to better appreciate the performances of SGDP and AdamP. Finally, they also added the experiments with higher weight decay, showing that indeed 1e 4 was the best value. + The paper is well written and contains a good balance of illustrative examples, theoretical analysis and experiments. Good job!+ The empirical evaluation of the proposed algorithms is quite large, and many tasks and architectures are considered.<BRK>Summary: Based on the assumption that the rapidly decrease step size \delta \omega leads to a solve effective convergence of \omega. The experiments look good to me while the derivation of this paper based on many assumptions and conjectures. However, I am not an expert at this area I will not be sad if this paper is reject by other reviews. minor problems:1) Can the author explain more on how to derive eq.(4) ?<BRK>This paper points out that momentum in GD optimizers results in a far more rapid reduction in effective step sizes for scale invariant weights. To solve the problem, two algorithms called SGDP and AdamP are proposed, which project the updates to tangent space of the parameter. Questions:(1) What is the difference between the proposed algorithm and the algorithm in paper "Cho & Lee, Riemannian approach to batch normalization"? It is an important related work which should be cited and compared with. (4) The theory part analyzes the effect of momentum, while the experiments shows the effect of weight decay. Does it mean that "SGD is a better choice than momentum SGD"? Momentum SGD is a standard algorithm to train deep neural networks with BN in practice but not vanilla SGD.<BRK>However, there are relevant points needed to be clarified on the theory and experiments. It is better if the authors remove AdamP in the title. The paper points out a relevant issue in using normalization techniques such as batch normalization together with momentum based optimization algorithms in training deep neural networks. 2.The paper provides experimental results on various tasks and datasets to demonstrate the advantage of the proposed method. 3.The paper is well written with illustrative figures. It is not clear to me that the proposed update in equation (12) yields smaller norms ||w_{t+1}|| than the momentum based update in equation (8).
Reject. rating score: 3. rating score: 5. rating score: 6. rating score: 6. <BRK>Summary: This paper proposes to use the squared Bures distance in discriminator feature space to match the generated and real distributions. The proposed method produces good empirical results with simple generator architectures in synthetic and real datasets. The experiments are not thorough and the quality of writing is subpar. Pros.This method introduced in this paper is theoretically sound. Cons1.The choice of synthetic experiments may be too easy to discern difference between methods. 2.The Bures metric is one among many metrics for comparing covariance matrices. 3.Higher performing methods with similar network capacities have been left out in the performance tables. Very few recent methods evaluate DCGAN on cifar10 or STL as the architecture is too limiting for these complex datasets. “issue – the ‘mode collapse’ – appears”  > remove “the”“complemented by a additional term”  > “an additional term”; I stopped tracking grammar mistakes after this point. I am now more confident with my original assessment.<BRK>This paper proposes a new penalty to deal with mode collapse, and the authors claimed that it could be easily added to any existing GAN variants. I suggest the authors to test their penalty with other GAN variants to stronger support their claim. This paper proposes a penalty to the generator loss to encourage the diversity of fake data to match the diversity of real data. To do this, the last layer (providing the representation for each input) of the discriminator is used to define the diversity in input. Bures distance is then used to define the similarity between the diversity of real data with that of fake data, leading to the novel method called BuresGAN. Four datasets are used for their evaluation and comparison with 7 baselines. Pros:  Bures distance is an interesting metric and promising to deal with mode collapse. There exist various approaches to deal with this problem, such as using more generators, more discriminators, using different losses, or penalty. However, this paper does not provide an extensive overview of the existing literature on mode collapse. Unclear significance: The authors use different network architectures for different methods in their experiments, e.g., MDGAN often uses architectures which are different from other methods. Such a comprison will provide more evidents to see the significance of the proposed penalty. Minor comment:  The results of some baselines, e.g., MDGAN, VEEGAN, UnrolledGAN, are sometimes not very good as reported in Table 3.<BRK>**However, I am happy to adjust the scope if the authors can provide evidence regarding comparisons to the state of the art methods during the rebuttal. More specifically, the paper proposes to add a regularization which matches the Bures distance between the covariance matrices of the features of real and generated data. However, I cannot recommend this work for acceptance at this point, mainly because the paper did not compare with the state of the art (and some widely used) methods for fighting mode collapse, and the improvements on the benchmark datasets are rather weak. However, the baselines are rather weak and old. The experiments in your paper seem to build on a better DCGAN architecture (993.3 modes already). I understand that possibly the hyper parameters and architectures in (Alt )BuresGAN and those papers are different, so we cannot directly conclude that (Alt )BuresGAN is worse than [1,2,3,4]. Besides this point, I also have some other questions/suggestions:* You use the features from the last layer of the discriminator. How the performance would be if you are using other layers? The same problem exists in all other tables in the paper.<BRK>However, this expression has been proposed in prior work (Oh et al., 2020). * The paper discusses connections with Wasserstein GAN and integral probability metrics. In particular, shows that the proposed distance os proportional to the 2 Wasserstein distance. * Detailed information on the architectures and datasets is given in the Appendix, aiding reproducibility. My main concerns are with the evaluation,* Experiments on synthetic data: The following two papers outperform the proposed approach [1,2], especially in terms of number of high quality samples on both the Ring and Grid sets. * Experiments on CIFAR 10 with DCGAN architecture   Both [1,2] again outperform the proposed approach in terms of the the FID metric. * Experiments on CIFAR 10/STL 10 using ResNet architecture: The FID scores should be also reported for fair comparison with the state of the art. It is unclear whether the proposed approach really achieves a new state of the art inception score on STL 10. * The cost of the computing the Bures distance in terms of training time in comparison to simpler losses like Hinge loss [3] or gradient penalty loss of WGAN GP should be clarified. It is unclear whether the additional resources required (if any) justly the limited performance gain of the proposed method.
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 6. rating score: 7. <BRK>I can see what you are trying to do here, but I think it should be laid out step by step. **Overview**The paper discuss on a theoretical level what mutual information related quantities are well suited for representation learning in the context of reinforcement learning. The paper argues that for reinforcement learning, forward information is the only one that is well suited for learning representations by a certain sufficiency definition, and state only transition information and inverse information do not satisfy the sufficiency property; thus only the forward information is *a well suited principle* for representation learning in reinforcement learning. **Strengths**I think this paper is an interesting read, and that it is justified to consider a very simplified view over this setting (mutual information and be well approximated from samples, exploration is a non issue, etc.). It can be used to discuss which MI based objective is adequate for RL in RL. But if we consider the next timestep, then the compressed representation might not be optimal. I am inclined to believe that the claim is true with added assumptions over the action distribution, but the proof presented here is not correct. I think you need additional independence assumptions.<BRK>SummaryThis paper studies 3 mutual information (MI) optimization objectives for learning a latent representation Z in a sequential decision making task of the MDP style:forward information: MI between Z at time t+k, and Z at time t concatenated with the action at time t,state only transition information: MI between Z at time t+k, and Z at time t,inverse information: MI between the action at time t, and Z at time t+k, conditioned on Z at time t.All of the 3 objectives above are theoretically analysed with the result that only forward information can lead to a sufficient representation for reinforcement learning (based on the concept of Q* sufficiency). In these 2 games, a latent representation based on forward information for reinforcement learning will eventually recover an end to end trained agent, while the other latent representations won t.Quality and DetailsThe motivation of the paper is interesting and a systematic approach is taken to answer the question which representation learning objective is best, starting from a theoretical analysis and ending in an empirical study. However, I am a bit concerned about the experimental side of the paper which is a bit scarce. Originality and SignificanceThe research question is interesting and original, but the experiments only deal with 2 simple environments. MinorI have the feeling that the experiments in Figure 5 and 6 are missing baselines? In Figure 5, I can t see a state only ablation, while in Figure 6, I can t see an inverse information ablation? Maybe I skipped it, but what was k in the experiments?<BRK>Summary of the work: This work studies which mutual information representation learning objectives (1. forward information, 2. state only transition information, 3. inverse information) are sufficient for control in terms of representing the optimal policy, in the context of reinforcement learning (RL). Besides, they conduct some empirical studies on a video game (i.e.Catcher) and show that the sufficiency of a representation can have a substantial impact on the performance of an RL agent that uses that representation. To the best of my knowledge, Q^* sufficiency analysis for mutual information objectives is novel. The counterexamples in sufficiency analysis are interesting. The paper is well written. Regarding the experiment results, the authors give some intuitive descriptions to show that state only transition objective and inverse objective may be insufficient, but forward objective works in the catcher game. Regarding the proof of Proposition 4, l am sorry that l do not fully understand the derivation from Q(s,a) to Q^* (s,a). Can the authors provide more details on that?<BRK>**Summary**: The paper discusses three mutual information (MI) objectives for representation learning in RL, referred to as forward, state, and inverse. This paper may not be entirely representative of previous work. The paper shows that of these three common objectives, only the forward objective is sufficient for learning the optimal policy / value function. While the authors claim that this enables a fair comparison between the objectives, it also somewhat limits the scope/impact of the paper, as it is less realistic. While I agree with the direction of this paper, I feel that these aspects would need to be improved for the paper to have substantial impact on the rest of the research community. Overall, the paper is well written. It’s unclear whether analyzing the sufficiency of representations for downstream tasks is impactful for future work. However, there are infinitely many representations that are sufficient. The paper is almost entirely lacking in experiment details.<BRK>This paper studies which commonly used mutual information objectives for learning state representations are sufficient for reinforcement learning. In particular, they provide counterexamples to show that state only and inverse MI objectives are not Q* sufficient, while proving that forward MI is Q* sufficient. They validate their findings empirically with experiments in a simple RL domain. The counterexamples shown are simple and the authors do a good job of explaining the intuition. I think this paper will be of great interest to the ICLR community. Minor: there’s a reference in the last paragraph on page 6 to Figure 5 which I think should be to Figure 4.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. rating score: 5. <BRK>The idea to unify invariances of the loss function by using symmetries and derive corresponding conservation laws (for $\lambda  0$) in the gradient flow is very elegant. Thus, the costs are not lower semi continuous. Overall, I really enjoyed reading this paper. Based on the abstract, this seems to be a relevant related work. After the rebuttal: I d like to thank the authors as well as my fellow reviewers for the interesting discussions and corresponding clarifications. The validity of the continuous dynamics is demonstrated in numerical results only. For our understanding of how symmetries/invariances in the weights of network architectures influence the training, I believe this paper does provide interesting insights such that I recommend its acceptance.<BRK>The current work studies the implications of continuous symmetries of a DNN on its weight dynamics. They do so both in the continuous/vanishing learning rate case and for small learning rates and manage to provide accurate quantitative predictions for the dynamics of these quantities. They do so with a refreshing toolbox, that of symmetries. On the other hand, what makes their quantities tractable, seems to be the very fact that they have no impact on the DNNs final outputs. For example, this is why they are unaffected by the real world dataset. While being a clever trick, it can be viewed as an inherent limitation of the approach: one understands quantities that have no bearing on what the DNN learns. Two technical comments: 1.<BRK>This paper analyzes the learning dynamics of DNNs from the perspective of symmetry of some parameters. Specifically, the paper derives analytical form of parameters under the cases of translation, scale and inversion invariances, and also modified the underlying gradient flows to accommodate stochastic gradients. The results are very interesting, which I like a lot. However, I have some confusions about some results, which make me not able to give the paper a pass at this time. And in Figure 3 5, the authors verify the convolutional layers for these invariances. 2.In 6.1, a new solution for the translation invariance case is derived. It is more clear to me that this is an interesting paper on describing the dynamics of DNN parameters in the training, which seems novel to me. This makes the results not as existing as what I thought.<BRK>This paper studies the dynamics of the parameters while training a neural network via SGD. For each dynamics, the authors show that some invariant properties of the loss function (which are often satisfied in practice) imply some invariant quantities for the dynamics. Overall, the paper is rather clear. It tries to provide a physical meaning behind the dynamics of learning, which is an interesting question. The technical contribution is not clear to me. For Eq (13), only intuition is provided. To improve the paper, I suggest the authors to   clearly locate their work within the existing literature. Turn their paper into an experimental one. This does not mean adding more simulations, just explain them in the main text and how they contribute to the main message of the paper.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 6. <BRK>The authors demonstrate their framework by evaluating the performance of many well known RL agents across a variety of these playground environments. Additionally they conduct similar experiments on Atari and Mujoco tasks and observe similar trends in agent performance when injecting noise, reward delays, and varying action max values. To this end the authors describe some findings that may be applicable in the design of new environments (such as action max needing tuning in continuous action environments), but little is shown about new insights gained toward understanding shortcomings of existing algorithms or routes for building better RL agents. Additionally, why do we believe that the structure of the MDPs generated by MDP Playground will resemble that of the problems that RL practitioners in the community are interested in solving? It s not clear that insights gained from Playground environments will transfer to more complex environments. I have read the author response and stand by my original score of the paper.<BRK>This paper presents "MDP Playground", a family of procedurally generated MDPs that can be used to benchmark certain dimension of difficulty considered by the authors to be challenging to current RL algorithms. The "MDP Playground" is slated to be open sourced so that the community can benchmark against it. Pros:I think that high level difficulties of MDPs are under studied and that the over reliance on benchmarks such as Atari or Mujoco make people look more at per environment/game performance instead of thinking about high level issues with the MDPs (exploration, delays et.c). Although this is done in a hand wavy manner, all attempts at formalising this seem essential to better understand what the actual degrees of difficulty are for particular tasks and whether novel proposed approaches are really tackling the challenge they claim to be tackling. I feel the nomenclature around the dimensions of  hardness  (nit, perhaps  difficulty  would be a better word here) is not very clear. I think this will be hard to achieve for the rebuttal phase, but I encourage continued work in this domain and look forward to seeing the authors  response.<BRK> POST REBUTTAL COMMENTSAs a result of the discussion the paper has improved, so I m increasing my score. However, the core issue, that the proposed benchmarks don t seem to capture the difficulty structure of either real problems or more complex benchmarks, remains. The paper also assess how the identified dimensions of hardness that can be exercised in MDP Playground transfer to more complex benchmarks. is rather obvious and well known. In reality, and even in some existing benchmarks, the learner can reach irrecoverable (absorbing) failure states, such as robots damaging themselves or objects they interact with, unless they are very careful. Given these gaps, I doubt that MDP Playground in its current form adds much value over the existing RL benchmarks, which have some drawbacks but have interpretability as a big asset for debugging. In POMDPs, observations are Markovian   their probability depends only on the current (hidden) state. "State formulation" is also Markovian, as it is fully observable MDPs.<BRK>The paper describes a new benchmark for evaluating reinforcement learning techniques (MDP playground). It can be seen as a toolbox allowing to generate different MDPs with different characteristics. In addition to this toolbox, the authors also evaluate some of the classical algorithms in the domain. Said otherwise, the methodology would gain if these metrics could be connected to real use cases e.g what are the relevant dimensions underlying atari environments, robotics ones, etc... A second drawback is readability: the approach is somehow proposing too many metrics such that being able to understand which model is good and which model is bad is very difficult. I would propose the authors to think about organizing these metrics in a way that they can be easily presented to users (e.g using spider plots ?by using a hierarchy ? ) To conclude, if I really like the approach proposed in this paper, and if I think that it is a nice step toward a better evaluation of RL algorithms, I find that the paper is lacking some important characteristics to make MDP playground really usable: i) a good way to summarize the performance of RL algorithms too many metrics allowing a good understanding of the methods ii) a comparison of more algorithms and iii) a  link between the proposed metrics and classical benchmarks. Considering the modifications made on the paper, I increase my score
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 5. <BRK>The paper talks builds upon the recent work from Ho (2020) about generative models that use noise diffusion. The authors suggest that the proposal in Ho can not only be used in good quality sample generation (as already shown by Ho), but also leads to reasonable improvements in likelihood. Firstly, from an application point of view, what does achieving a high log likelihood mean, if the samples are already good enough or high quality? And more generally, what are we aiming for in terms of a reasonable change? This is in section 3.1 second paragraph; Either I am missing something of the argument here is that we need to tune noise variance and cannot fix it? It is interesting to think that a periodic decay noising schedule is better than a linear one? The whole point here is some small non zero s is ok; why specifically 0.008?! 6.One of the main conclusions in section 3.4 is kind of confusing   based on the summary, if we are not interested in sample quality but only interested in maxing of likelihood, then the proposal of this work is not good, and working with L_vlb suffices?<BRK>However, it has yet to be shown that they can achieve competitive log likelihoods. This paper shows that with several small modifications, diffusion models can achieve competitive log likelihoods in the image domain while maintaining high sample quality. This paper is well written and good organized. However, I have the following concerns. 1.The authors claim that the noise schedule used in Ho et al.(2020) was, experimentally, sub optimal for ImageNet $64 \times 64$, which lacks theoretical guarantees. 2.This manuscript is mainly based on the previous work Ho et al.(2020).The novelty seems to be too limited. 3.I am not convinced that only one dataset (ImageNet $64 \times 64$) is sufficient to demonstrate the performance of the proposed strategy.<BRK>The paper found several methods to improve log likelihood of diffusion models while maintain their sample quality, including cosine instead of linear noise schedule, using a hybrid objective to learn parameters of the covariance function, and using importance sampling to improve the gradient noise. The authors also explore how sample quality and log likelihood scale with the number of diffusion steps and model capacity. Significance of this work: The necessity for having larger log likelihood for the diffusion model is not very well motivated. But I m not very familiar with the denoising diffusion model and could be wrong. The results on how the model scales with computation seems trivial and may have been known already. Finally, it ll be beneficial if the authors can verify findings got in this paper can apply to other dataset or types of data more broadly.<BRK>Specifically, the paper managed to improve the log likelihood performance by identifying the issue of the simplified objective and proposed to learn the variance using a hybrid objective. I think this technique along with others are useful practical techniques to improve the training of diffusion models. Otherwise, it s hard to get a sense of how much improvement has been actually achieved by directly looking at the numbers in this paper and the ones in previous papers. About the noise schedule: is this a generally better noise schedule, or it is only tailored for ImageNet 64x64. I will consider raising my score if the above concerns can be addressed.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 7. <BRK>Finaly, the authors compare their choice of $\zeta$ with the one from [1] and show that on graphs with sparsity $3\sqrt{\log n}$ and sufficient assortativity the Bethe Hessian based method more frequently estimates the right number of communities. Non extensive experiments (K is set only to 3 and 4, and the average degree is set to a single value)  The right choice of the parameter $\zeta$ is non straightforward, and the authors suggest non guaranteed estimations. The problem itself is well motivated and the regime of sublogarithmic average degree is important, and typically real world graphs fall under this category. * I think it would be beneficial to discuss the time complexity of the method. My main complaint is the very limited number of communities generated under the SBM. Estimating the number of communities in networks by spectral methods.<BRK>SummaryThe authors propose a spectral framework using the Bethe Hessian matrix to infer the number of communities in sparse networks. This leaves me wondering if the approach would actually work in a practical setting and in particular, if the procedure for estimating the scalar interval based on procedure 4.2 can be used in practical applications. The impact of the assumption that B in the stochastic block model is full rank and assortative is unclear. QualityThe paper is in general well written and clear. It would strengthen the paper to consider also real networks for instance as in (Le & Levina, 2015) considering networks with “Ground Truth” (see also https://arxiv.org/pdf/1507.00827.pdf).<BRK>Should the threshold not depend on K? 3) It is also unclear from previous literature if the work presented by the authors is incremental or not. Cons1) I would have really liked to see a sketch of the proofs of the lemmas in the main draft. It would provide intuition on why the spectrum of the Bethe Hessian matrix is used to determine the number of components. But I believe that the pros of this paper outweigh its cons and therefore I recommend acceptance.<BRK>Summary:The paper studies the estimation of the number of communities in a graph drawn from a stochastic block model (SBM) using spectral properties of the Bethe Hessian of the observed graph. While this scheme has been proposed in the prior literature, the focus here is on sparse graphs, with average degrees bounded as $o(\log N)$. The procedure to do so is well described, with the performance gains clearly demonstrated for small $K$. I think the paper does have some flaws   most critically the empirical study with respect to $K$, and the issues I mentioned regarding Corollary 3.4 (which is a bit oversold as of now). Information theoretic thresholds for community detection in sparse networks. b) In my opinion, the experiments need to be deepened.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>The paper builds on recent working attempts to debais sentence encoders by considering modified sentences. The core novelty in the work is to train a lightweight modification the encoding of the sentence and its swap to (a) reduce the distance between the two embeddings, using a contrastive learning objective and (b) reduce the mutual information between cueing words and the new embeddings. Positives:+ The method is conceptually simple and the modification of the existing embedding is cheap to compute. + Conceptually simple method+ Overall extremely clean and self contained presentationNegatives:+ (nitpicking) The T SNE experiment is unclear. I am unsure how it was constructed and why the graph on the right is better than the one on the left.<BRK>This paper studied a debiasing method to remove social bias in pretrained NLP models. Moreover, the network can be further trained by minimizing the mutual information between the sentence representation and its sensitive word representation. The experiments show that the proposed method can effectively reduce bias while achieving better downstream task performance of the pretrained model. I think it would be better if the authors experiment their methods with less restrictive constraints on what are replaceable (e.g., allowing “boy” to be replaced by “her”) and see if the performances would be severely hurt. For example, how about using more than one layer of fully connected neural networks for the fair filter?<BRK>The paper proposes a method for debiasing pretrained sentence representations. Evaluation is performed on SST 2, CoLA and QNLI, showing that the method is able to produce more similar representations for sentences containing bias words, while sacrificing a small amount of performance over regular BERT. The idea of measuring bias only through cosine similarity is questionable. It is unclear where the lexicon of bias words used in this work came from, is it available to everyone, how large it is and how much work would be required to construct it for other types or biases or other languages. If so, then some simpler approaches could possible be just as effective, e.g.replacing all the gendered words with one gender equivalents or averaging over different versions of the same sentence containing different genders.<BRK>This model is trained by minimizing the InfoNCE between the representation produced of original sentence and the representation of that same sentence with some tokens replaced with differently biased tokens (e.g."his"  > "hers"). This paper also introduces a regularizer which minimizes the CLUB between the generated representation and a word embedding for a biased token. If it s parameterized as a neural network, and the weights are updated, then I think this is just part of the model and not really a regularizer. That should definitely be included in the paper (not just as a citation), and if there isn t space it should be added to the appendix. The experiments are a little light, and the regularization approach is a little unorthodox, and I would increase my score if there were further experiments (on other fine tuning tasks and measuring other types of bias) and the regularization was better motivated.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 7. <BRK>The paper gives "almost private" algorithms for problem of sign recovery of mean vector and of linear regression. Why is loss of worst case privacy acceptable? This is what happens in the paper, and without a good justification for that, I don t see why such a loss of privacy is okay. Even privacy has not been formally defined. Apart from group privacy, no other important properties of the "definition" have been proved. I wouldn t claim things like that without formal justifications.<BRK>The paper shows that in the sparse mean estimation setting, Med DC is correct with high probability under some assumptions and Med DC satisfies a weaker notion of differential privacy proposed by the paper. As discussed in the concerns, the modification of the differential privacy notion makes the problem very different and this modification is not well justified in the paper. In my understanding, the main difference between this notion and the standard differential privacy is that the standard differential privacy considers the worst case of the input but the modified notion considers the average case when inputs are assumed to be sampled from a distribution.<BRK>This paper considers the problem of private sign recovery for sparse mean estimation and sparse linear regression in a distributed setting. In fact, utilizing robust estimators, as combined with propose test release (PTR) is a very basic technique in the literature of differential privacy. Furthermore, the paper states that this is the first deterministic algorithm with a provable high probability privacy guarantee. 2.It is not appropriate to say this is the first deterministic algorithm with a provable high probability privacy guarantee.<BRK>  Sumary of the problems considered and paper contributionThis paper studies the problem of sign recovery for sparse mean estimation and sparse linear regression. However, it seems like a little bit of a stretch to call this "roughly (0,0) DP”. Also “random differential privacy”, which has appeared in the literature previously, seems like the notion the authors are looking for.
Reject. rating score: 3. rating score: 3. rating score: 4. rating score: 5. <BRK>————————————————Summary————————————————The author presented Linguine as a subgraph GCN training framework to train GCNs on large graphs. It includes bootstrapping and smart pruning algorithm to improve the quality of GCN models. + It is good to include rich baseline methods. GCN prone to overfit the error (error with low degree can easily get low training loss), the proposed methods will aggravate the negative impact brought by the error. From my point of view, the nodes with the highest degree are the most import node which affects message passing among nodes in the graph the most. However, it seems that smart pruning tends to prune these important nodes instead.<BRK>Sub graph selection is an important problem in the domain for training GCN models on large graphs + The paper presents a theorem to show that under a convex setting, the proposed join training during bootstrapping is guaranteed to converge+ The benchmarking results illustrate that the bootstrapping step with a 2 layer SageNet architecture performs similar to the much bigger DeepGCN model on the OGBN Product (Amazon product co purchasing graph) dataset+ On the OBGN protein dataset, the results demonstrate that the smart pruning step can improve model performance by selecting sub graphs without nodes  that are hard to learn or have a high degree of neighboring nodesThe paper also presents visualizations and results on batch size effect and pruning ratios on the model performanceWeaknesses:  + My understanding from the method section is that LINGUINE is a framework that combines the meta model learning and smart pruning to produce high quality subgraphs that can be used to train a GCN effectively. + What is the architecture of the light weight GCN? How do the two compare in terms of cost effectiveness? This is a different dataset from the ones for which the F1 score has been reported. Similarly, the visualization results have been presented for Yelp data, and pruning ration graphs in the appendix for Flickr.<BRK> Summary This paper proposes an approach to compute GNNs on pruned subgraphs. The authors use a "meta model" to learn a good node pruning strategy during training. * The connection to meta learning is weak. Some theoretical analysis is performed. Evaluation on several graph benchmarks show that the proposed LINGUINE framework achieves good accuracy using pruned subgraphs. Recommendation: Reject In summary, I think there are multiple major issues with the analysis and design choices, as detailed in the above "Cons" section. Pros + The paper is well organized. + Scalability of GNNs is an important topic.<BRK>### SummaryThe main idea of this paper is to improve the training of GCNs by smartly selecting subgraphs to train on in order to reduce memory consumption when running on large graph datasets. The model is evaluated on standard large datasets (Flickr, Reddit, Yelp, PPI), as well as the newer OGB product and OGB proteins datasets. In this stage, a lighter weight GCN and a meta model are alternately trained.
Reject. rating score: 3. rating score: 5. rating score: 5. rating score: 6. <BRK>In this paper, the authors develop a new policy gradient method to reduce the variance in the gradient estimations. In the commonly used policy method, the bias is a function of the state. e.g., V(x_t). and why the authors think a better credit assessment is needed and is the way to go. Later, the authors also show that their method performs well in practice. Are the authors talking about using one trajectory for all the estimates? Since this is the main key component of the paper, it would be great if the authors could explain it in depth. I also could not find it clear in the appendix. What motivates the authors to state the issue is the credit assignment? It is not clear where the performance gain comes from. The confidence rating is reduced.<BRK>The message and paper propose a couple of environments where there is exogenous noise added to the reward function and the particular method in the paper specifically looks at this type of noise. While the method proposed may work in these types of environments it s not clear if more interesting environments do have these properties and we should be more concerned with this problem or that the environments used in the paper were specifically constructed to fit the use case of the algorithm. However, the results in the paper are not overly convincing with respect to understanding the importance of this method on more realistic tasks that the community is generally interested in. Some more detailed notes:  The introduction does not state that the particular credit assignment problems being looked into is that of partially observed environments. If it s still not clear from the middle section to let the detail of the contribution is going to be period by this point it sounds like the method is just going to be a modification to a q function.<BRK>This is also learned from data. Claimed contributions:Proposing a set of environments with difficult credit assignment. + The idea of constructing value functions conditioned on future trajectory information is not novel (Hindsight Credit Assignment does this), but the idea of learning the conditioning variable is (HCA uses states or returns). + The paper is clearly written. The authors claim that they do not require a model of the environment but a classifier $h(A_T|X_T, \phi)$ is learned which resembles an inverse model. Is it difficult to learn a model for the proposed tasks? I think the work contains enough novelty, the writing is clear and the experimentation is extensive. But, I am unsure whether to recommend acceptance without a model based baseline trained on data available to the classifier used in this approach.<BRK>#### Summary:The paper explores a new approach to credit assignment that complements existing work. Second, it provides experimental evidence that the novel estimators are beneficial compared to some prior work (in particular (Harutyunyan et al.2019)).#### Comments:Overall, I found the contributions of the paper interesting, but I m somewhat on the fence about this paper due to the following pros and cons. The flow of the paper is currently misleading, given that there is prior work that does propose quite similar ideas, e.g., the content between the title to section 2.4 does not seem to be reflect relevant prior work. In the interest of making the claims more precise, it would be very useful to add important dependencies where needed. Appendix: I think adding some parts from the appendix could improve the clarity of the content. It is also not clear if all the content in the appendix is relevant for the results described in the main text. remove one  the ?
Accept (Spotlight). rating score: 7. rating score: 7. rating score: 7. rating score: 7. <BRK>The paper studies the effect of importance weighting schemes in deep learning models. A deep learning setting is considered where the empirical loss of labeled training data is weighted with importance weights and regularization on the network parameters is also included in the objective, which is optimized with gradient descent. Two main results are presented in the paper. The first result focuses on a linear prediction scheme, in which case the convergence is shown to be faster if the weights of the samples are matched with the inverse of their SVM margin. The other main result considers multiple layer feedforward networks in a covariate shift setting.<BRK>The impact of importance weighting on the generalization ability of the model is also shown empirically. The authors conjecture and show empirically that the results still holds if the importance weights are jointly learned with the modelPros:The paper seems to offer important theoretical results and empirical validations regarding the role of importance weighting on the implicit bias of gradient descent and the generalization ability of linear and non linear models in some settings. The paper is clear and well written.<BRK>### SummaryThis paper studies the inductive bias of gradient descent (GD) on smooth non linear models when optimizing a weighted ERM. They prove a generalization bound for weighted ERM and together with experiments provide insights on the generalization performance of GD in this case. ### Reason for scoreThe paper provides several novel theoretical results in a practical setting. The proof techniques might be useful for analyzing non linear models in other settings. For example, "characterize the impact of importance weighting…" is not clear.<BRK>In this paper, the authors study the impact of importance weighting on the implicit bias of gradient descent for both linear and non linear predictors. Could you give some references ? Keep the same notation along the paper The proofs of the paper are a little bit hard to follow but seem to be correct. I also have an other question. It is, in my opinion, an interesting question.
Reject. rating score: 5. rating score: 6. rating score: 7. <BRK>are established under some necessary but maybe strong conditions. It is proved that under some (perhaps strong) assumptions, CSG identifies the semantic factor on the training domain. I hope the authors to add further content in their next version, regardless of whether the paper gets accepted or rejected. Given this consideration, I also lower my score. in the main text, please be consistent. 5.Also for Assumption 5.2,  It is a common sufficient condition for the fundamental requirement of causal minimality for identifiability. Can the authors give more details about  For noisy or degenerate mechanisms, ambiguityoccurs during inference (Fig.2), and the inferred result notably relies on the prior.<BRK>• Section 3.2   I suggest to add a first sentence to introduce what this section is about. It develops a new variational approach to estimate the generative distributions, and test the approach on two datasets for domain generalization and domain adaptation. Overall, the paper suggests a novel approach and theory to an important problem. Although the authors answered most of my questions, I decided to keep the score as is, because I share similar concerns with R2 about the presentation, and because experiments are still lacking. Its major weaknesses are in its clarity and the experimental part. **Strong points**Novelty: The paper provides a novel approach for estimating the likelihood of p(class|image), by developing a new variational approach for modelling the causal direction (s,v >x). What was the search protocol?<BRK>Would be interested to see performance in ColoredMNIST task for causal identification and OOD generalization as the generative structure is well understood as well as performance capabilities. The authors present variations for learning this model which account for correlation/independence between semantic and diversity latent variables (CSG and CSG ind) and also extend to settings where some data does contain labels (CSG DA). The work presents empirical results demonstrating the effectiveness of this approach in both OOD settings (no test adaptation) and domain adaptation settings (test adaptation). While the experiments present a compelling proof of concept, the tasks Shifted MNIST and ImageCLEF DA are not the most representative challenges in their respective domains.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 7. <BRK>2. a new 3D net architecture for action recognition. For me, it seems too good to be true. After rebuttal:The authors addressed some of my concerns, and the proposed optimization plan is interested. 2.The way authors compared to others are mixed, making that it is a little bit difficult to compare the contribution of the paper.<BRK>  The paper proposed a Dual head Global contextual Pseudo 3D (DG P3D) network and an automated optimization path to train 3D ConvNet for action recognition. Another contribution of the paper is its decreased cost.<BRK>Strengths:++ The paper provides a novel perspective of designing the hyper parameters for 3D ConvNets. ++ The experiments are thorough. Also the experiment results corroborate the effectiveness of optimization planning. Weaknesses:  The construction of transition graph has some strict principles that could be relaxed. However, this seems somewhat counter intuitive to me.<BRK>The proposed optimization planning doesn t seem to work specifically for the 3D convnets and for the action recognition task.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 6. <BRK>### SummaryThe paper evaluates three procedures for selecting models for transfer learning. The choices are task agnostic selection, linear training, and the hybrid approach. They empirically show that the hybrid algorithm works the best on few shot learning on images. ### Feedback* The paper is a straightforward paper and easily understandable. The message is practical, but not very surprising. Unfortunately, it is only on image data; it would have been great if the authors had used an example from NLP too. * The hybrid approach is super simple, which is nice. Although it does not necessarily outperform the linear algorithm in Figure 6. The authors could study the generalization performance of the hybrid algorithm to provide further insights. * An empirical run time analysis is missing. * Overall, the idea is simple and practical, but the methodological contributions of this paper is rather limited. ### Post Response UpdateUnfortunately, the authors  response is not satisfactory on multiple issues.<BRK>[Summary] This paper presents a large scale study on model selection strategies for transfer learning, by performing task agnostic and task aware strategies on a large number of models evaluated on a diverse range of tasks. [Strength] The problem setting is novel and interesting. [Weakness] The major weakness of this paper is that it seems there is no consistent strategy to out perform all other methods in every task. So, I hope the authors could re emphasize what we really learn from this large scale study. Considering this limited effective information from this paper, I think it s not suitable for publishing.<BRK>The paper is well written and easy to follow. Overall, I vote for rejecting the paper as the paper has very limited novelty and experiments are not convincing. Furthermore, many experiments and comparisons are missing which should be included in the paper for a better understanding of the empirical study. Comparison with many simple baselines are missing in the paper. E.g., How does the hybrid strategy comparable to fine tuning with early stopping. Can we select pre trained models by finetuning for only few epochs? Mutual information between the features and discrete labels of the downstream task can be used to rank different models for transfer learning. More experiments and analysis should be performed in the experiments.<BRK>Paper summary: This paper looks at the problem of efficiently choosing pre trained models as initialization for downstream target tasks. I especially like the fact that the authors consider the different axes along which pre trained models differ (model capacity, generalist/experts etc.) + The suggested strategy is simple and easy to implement. + The problem is significant in practice since almost all practical applications of neural networks have this prroblem, and the gains seem large. I wish there was more work on this problem. The use of the JFT dataset hampers reproducibility since the dataset is not public. I d like to see results with JFT excluded.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>The paper is well written. The experiments demonstrate that the proposed method outperforms several baselines. The authors assume that the input features to the TOQ net are hand engineered, and thus, are not learnable. The biggest weakness of the paper is its experimental evaluation. The authors evaluate their approach on 2 small scale artificial video datasets. Thus, it is not clear whether the proposed approach would generalize to real datasets such as Kinetics, Something Something, EPIC Kitchens, etc. Missing relevant work: Wang et al., "Something Else: Compositional Action Recognition with Spatial Temporal Interaction Networks." Rebuttal Requests:  The authors should include thorough experiments on the real world large scale datasets such as Kinetics, Something Something, and Charades. I appreciate the authors  efforts to add new experiments on other datasets. In my initial review, I listed a few datasets that are most commonly used for action recognition comparisons. If the authors could demonstrate close to state of the art performance on those datasets I would be more convinced that the proposed approach is effective. Currently, most of the comparison are done w.r.t baselines that are implemented by the authors which is insufficient in my opinion.<BRK>The model leverages relational reasoning layers which are the Neural Logic Machines (NLM) to capture the spatial information. To further capture the temporal information, this paper proposes temporal reasoning layers.The results show that their method outperforms conventional graph neural networks with high accuracy and generalization with a large margin. The proposed temporal reasoning layer borrows the idea from temporal logic   programs which is interesting and promising. The paper is well organized and easy to read. This paper collects the soccer dataset by themself and sets STGCN as their baseline which is not originally used for this task. Both datasets used in the paper are collected by the simulators. Missing Ablation study of the temporal reasoning layer. However, the experiments fail to convince me of its performance. Mengshi Qi et. The experiments in the rebuttals shows the effectiveness of TOQ Nets in other large, real world dataset.<BRK>This paper overall presents a model that defines the multi person activities using logic expressions and uses neural logic models to generate recognitions and predictions over events. Specifically, the model follows the neural logic machines (Dong et al.) to define the operations in the networks. Using their self generated datasets from simulators, the authors found that their model performs better than other baselines. + The experimental results support authors  claim on the modelPotential cons:1. There are not very unique modeling contributions in this paper. The model used is an application of neural logic machines. 2.Although I like the logical way of defining the events, it seems that the current refinement modules are quite similar to graph neural networks  message passing operations. Is the reason for the proposed model outperforming baselines that there are higher order terms considered compared to baselines?
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 6. <BRK>This paper proposes an easy positive sampling method for deep metric learning which aims to reduce the class collapse problem which is found to harm the performance of existing DML methods. 2.The authors only provide theoretical analysis on the binary case and claims it can be easily extended to the multi label case, which I find not trivial. 3.A concern is the limited batch size, which might cause the easy positive sample of one particular sample at different iterations to be different (and possibly from different subcluster). 5.For the experiments, the performance improvement using the proposed easy positive sampling is not strong. In summary, I think this paper is solid and well motivated, but I find the idea not new and the experiments not satisfying. The latter weighs more in my decision. 2020: 2474 2482.<BRK>This paper proposes/adopts a simple positive sampling scheme in metric learning: only sampling the easiest positive for each anchor. Using the sampled easiest positive, nearly all current metric learning methods got improved performance. Cons:1.I don t think this easiest positive sampling scheme is a contribution, though authors give a theoretical analysis of why this scheme can reduce class collapse. In the paper of Arandjelovic et al.(2016), they don t clean the positive set (only minor negatives could be included) for efficient training. However, I would suggest using three classes to derive theorems, rather than using two classes. For example, in the following paper:Radenović, Filip, Giorgos Tolias, and Ondřej Chum. European conference on computer vision. Combining the conclusions from the above paper and the paper under review, I would say this easiest positive sampling scheme has limited contribution, as it is not broadly applicable.<BRK>Motivated by the theoretical results on class collapse problems, this paper proposes a simple positive sampling mechanism called EPS for metric learning. The method is simple   each sample selects its nearest same class counterpart in a batch as the positive element. The authors provide both theoretical motivations and empirical studies on the proposed method. Strengths: + Theoretical analysis on the existing class collapse problem for triplet and margin loss+ well motivated and simple solutions that are proven to be effective in theoryWeaknesses:  there is a gap between theoretical analysis and empirical studies. In the analysis, the paper shows in the noisy label setting, margin and triplet loss also induce the class collapse problem but in the empirical study, the paper only conducted analysis on dataset with clean labels. However for any fixed deep nets, it does not satisfy the requirement Overall, the EPS method is simple and supported by theoretical analysis. I would find it more convincing if the paper can provide empirical analysis on noisy labeled data.<BRK>The authors find that the popular triplet loss will force all same class instances to a single center in a noisy scenario, which is not optimal to deal with the diverse and distinct sub classes. The analyses in the paper provide insights. Here are some possible issues of this paper. 3.Maybe the authors need to find another real world dataset with multiple meanings in one class and show the advantage of the proposed method. We can find the improvement of performance on the benchmarks, but the numbers are hard to illustrate the effect of the method.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 6. <BRK>In this paper, the authors study the theoretical properties of meta learning. In particular, the train validation split to tackle the linear centroid meta learning problem is investigated using statistical asymptotic theory. First, the authors proved that the train validation method has statistical consistency, while the train train method has a statistical bias to the centroid. Under the noise free setting, however, both methods have statistical consistency. Furthermore, the train train method is superior to the train validation method in the sense of the asymptotic MSE. Based on the asymptotic analysis the optimal ratio of the data splitting for the train validation method was also derived. The theoretical findings are confirmed by some numerical experiments.<BRK>This paper compares two approaches of data splitting in meta learning: train validation split and train train split. This paper shows that the best of the two approaches depends on the specific problem. It is unclear to me how  this problem capturesthe essence of meta learning with non linear models (such as neural networks) in practice  as claimed on page 2. I would say to improve the significance of the paper, the authors could like at the regime where d, n, T both grows to infinity at certain rates e.g.$d \Theta(Tn)$. In the linear regression setting, leave one out cross validation should be a more proper approach.<BRK>In meta learning, a common practice is to do a train/validation split of the data within each that, so that optimization of meta parameters is performed on validation, not training, losses. 2.I would like to see more details as to why interchange of the derivative and both the limit and the integral is justified in the proof of Theorem 7 (uniform convergence). There seems to be a major typo in the statement of Theorem 7: you talk of the train val method but the math has tr tr written. Thus, my review will mostly focus on the results themselves, with the hope that other reviewers can better evaluate how this work compares to other work in the area. Globally, I think the results of this paper are interesting. [pros]1.The central result of the paper is, in my mind, Corollary 8. It s very elegant. 2.I think Propositions 2 3 are the other interesting results, namely by showing that consistency of the methods are intimately linked to model specification. 3.To obtain a workable theoretical analysis, the authors analyzed a simple linear model, (6). However, this model also leads to counterintuitive results, such as Corollary 6. However, it s possible that the gap in learning rates between train val and train train is an artifact of how simple the linear model is. I think Corollary 6 and its ensuing discussion should disappear as well. The rest of Section 2 is fine for me.<BRK>The authors verify the importance of train validation split in meta learning theoretically, which is commonly used in the meta learning paradigms. By analyzing the linear centroid meta learning problem, the authors show that the splitting method converges to the optimal prior as expected, whereas the non splitting method does not in general, without structural assumptions on the data. The paper provides new insights on the usage of the train validation splits in meta learning theoretically. Here are some suggestions:1. More ablations like Table 2 should be investigated. 2.The authors could discuss whether the theory can be applied to other kinds of meta learning methods and how the theoretical results can guide the design of new meta learning methods.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 6. <BRK>The paper proposes to increase the adversarial robustness of a neural net by training the model on both clean and adversarial samples. Therefore, the noise magnitude is estimated separately for each training sample, such that the decision boundary (suppose a classification problem) of the neural net has maximum distance to each training sample. b.Benefits compared over other adversarial training methods are not clear. b.	Sec.3.1: Since this is the toy dataset, a discussion why the decision boundaries look as they do, would be interesting. 1 can be written in 2,3 lines.<BRK>Summary:The paper proposes increasing margin adversarial training (IMA) to improve adversarial robustness of a classifier. Lack of evaluation on datasets such as MNIST, CIFAR10/100 or imagenet IMA’s assumption that clean samples from different classes are equally spaced from the boundary might not be valid for images. IMA assumes that clean samples from different classes are equally spaced from decision boundaries when in an equilibrium state. More discussions and theoretical studies would make IMA more convincing. The statement “a model robust to noises less than the level of 0.2 is good enough for this application“ is not substantiated by any previous work or experiments.<BRK>The authors propose a new training method, named Increasing Margin Adversarial (IMA) training, to improve DNN robustness against adversarial noises. The IMA method increases the margins of training samples by moving the decision boundaries of the DNN model far away from the training samples to improve robustness. Overall, I vote for ok but not goor enough   rejection. However, when it was applied to more complicated real dataset such as Fashion MINST, SVHN, and COVID 19 CT image dataset; there was no significant achievement if compare to the MMA approaches. Thus further investigation is needed to convince benefit of the IMA on real datasets. In addition, the authors tested only one medical image dataset, COVID 19 CT image dataset.<BRK>The idea is based on a common intuition that adversarial attacks are most influential to the points close to the decision boundary. The paper is written clearly. There is no difficulty in understanding the content. But this does not say anything about the choice of $\epsilon_\max$ for IMA. It might be interesting to see how IMA deals with the well known trade off between robust and standard accuracy, which is currently one of the main concerns of adversarial training methods.
Reject. rating score: 3. rating score: 5. rating score: 7. rating score: 8. <BRK>In this work, the authors train a model on a subset of architectures (~60k) in the DARTS search space and use this model to predict the performance of architectures outside of that subset in DARTS. I recommend rejection for this paper, as I do not believe it represents a step forward in the way we benchmark our NAS algorithms. Additionally, this paper only considers CIFAR 10. This is a step back from NAS Bench 201 which despite its small size, did contain multiple datasets. The compute used by the authors (training 60k networks) has gone into differentiating a bunch of networks that are all quite good, within a few percentage points of error. A problem with NAS Bench 101 and 201 is that the search spaces are small (423k, 6k) as the authors point out. NAS Bench 301 encapsulates the DARTS search space which is much bigger (10^18). I would argue that it doesn’t matter how large a space is if it is lacking in variety; Every possible network works well enough. I believe this is more interesting from a research perspective as we would like to apply NAS to situations where networks can break (and avoid this happening). In terms of presentation, this paper is well written, although the figures could be larger. This paper doesn’t present a benchmark. It provides a model that represents computationally efficient means of getting network accuracies from the DARTS search space. This space, although large, has very little variety in terms of network accuracy.<BRK>Summary:The authors propose a new benchmark for evaluating surrogate functions for architecture search. According to the authors, existing tabular architecture search benchmarks are insufficient for this purpose due to using overly small search spaces. The main difference of this benchmark and other existing architecture search benchmarks such as NAS Bench 101 and NAS Bench 201 is that they do not attempt to evaluate all the architectures in the search space and do so for a much larger search space (DARTS). Additionally, the authors compare the proposed benchmark (based on surrogate functions) with a real benchmark and observe that the results are qualitatively similar. The experiments conducted that compare different surrogate functions on this benchmark are solid + The paper suggests that building benchmarks for different search spaces can be accomplished through the surrogate function route where first a dataset of architectures is collected and then it is used to train a surrogate model. The surrogate function is then used as an interface between the search space and the search algorithm. This approach for building benchmarks is general and is likely to be useful in the construction of future benchmarks for architecture search. It is well known in architecture search work that existing search spaces (DARTS being one of them) have limited performance variability and that much of the performance variation ascribed to different architecture search methods can often be ascribed to differences in search spaces. For example, how do we guarantee that NAS Bench 301 is not just another dataset and guarantee that addresses some of the perceived problems with existing architecture search search spaces and benchmarks? It would be warranted to show that this trend is consistent with other metrics such as squared error and Kendall Tau (i.e., showing the ranks of different architectures are also preserved better).<BRK>##########################################################################Summary:This work filled an important gap in the NAS benchmarks. The previous benchmarks only contain small search space due to the expensive cost of evaluation of neural architecture. Thus, to provide meaningful comparison, this work provided a benchmark in a large NAS search space (same as in DARTS), and using  surrogate models to predict validation performance of untrained neural architecture. The empirical results suggested using the surrogate benchmarks resulted in similar optimization trajectory as real evaluations and the author also shows one can derive/validate research ideas quickly with the benchmarks. 2.The empirical results are very solid; the observation on the noise in the training is very insightful. ##########################################################################Cons: I only have some minor comments:1. In Section 3.2, first paragraph, the author mentioned that validation and test error are highly correlated. This is clear if the poor performing architectures are included. 3.Given the mean and noise estimation based on the surrogate models, is the assumption there is gaussian and every experiment will draw one value from this gaussian? If so, could you state it clearly in the paper? If not, please clarify. POST REBUTTAL COMMENTS  Initially I had only minor comments and the authors addressed all of them.<BRK>Tabular benchmarks like 101 and 201, take a search space and train all possible architectures in them. While this is possible to do for relatively small search spaces and datasets, this is impractical to repeat for larger search spaces (e.g.DARTS  search space which has 10^18 architectures). So a predictive model trained on a sparse subset of architectures can actually outperform an exhaustive tabular method. Lots of careful experiments are reported on the DARTS search space to create predictive models which can accurately predict architecture performance (accuracy, latency) and hence can be used as a  simulator  by NAS algorithms for rapid research and fair comparison. This is true in the case of tabular benchmarks like 101 and 201. Can we construct surrogates without knowing anything about any particular optimizers the community may invent in the future? One part of an ablation study answering this question has been presented in Appendix E.2 where a model has been only trained on well performing architectures (above 92% accuracy) and in Figure 21 has been found adequate for predicting the trajectories of BANANAS and Random Search (RS). But can we do the easy baselines first for which the authors already have the data:1. Also in this paper itself if more than 21500 architectures on 101 are used to train (unclear from the paper whether they were randomly sampled and diverged models rejected or some other technique was used to select them, since it says "subsets of D^{train}" but I think they were randomly sampled, right?), then that itself is better than the tabular benchmark. Happy to be convinced if these are fair baselines or not.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 8. <BRK>However, I think there is much room for improvement in the presentation of the paper. **Clarity**I would say that there is much improvement in the clarity of the paper. The authors proved the universal approximation theorem for horocycle and $f^1_{a, p}$. For example, Theorem 1 and Theorem 2 does not apply to the Poisson neuron model. I want to know if there are theoretical justifications for the Poisson model.<BRK>The idea is based on well known concept of horocycle and horospheres which are known to be hyperbolic counterpart of line and plane in Euclidean geometry (see Coxter). Then the authors show the universal approximation which kind of follows similarly from the Euclidean counterpart. This essentially reduces the ``````"novelty" of the paper. 1) The work should be better motivated, for example what is the motivation of using Horocycle layer and Poisson neuron layer? 3) In Theorem 2, eq.9, why the inner product is Euclidean instead of hyperbolic?<BRK>Representation theorems alla Cybenko for layers constructed from these neurons are presented. In addition, these drawbacks probably imply that the paper might not be accessible but to a few niche in the community, and might have a very limited impact. * The paper needs restructuring. In particular, I find sentences like "Suppose this Poisson neuron is non trainable ... " very confusing. * The reported advantage of H over G/S seems to be mostly prominent in low dimensions of the Poincare ball. I would like to see a discussion on why this is the case.<BRK>Theoretically, they proved that the proposed hyperbolic neurons are universal approximators (Theorem 2). The presentation has high clarity with good intuitions through illustrations. Therefore it should be interesting to the large group of audiences in those areas. Are there any explanations and technical arguments of the good empirical performance? Figure 8 x axis and y axis are not clear After rebuttal:Thank you for the revision and the clarifications. It is now clear that this work actually proposed two different neurons: the horocycle neuron defined on H^n and the Position neuron defined on R^n (removing one point). After the revision, they are proved to satisfy the universal approximation property.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 6. <BRK>This paper proposes a variant of AWR with an added Q function. It is motivated by the *state determines action* assumption, which is used to argue that AWR should always return the data collecting policy if optimized to convergence. Experimental evaluation of QWR focuses on online sample efficiency and offline performance. It seems that the issues pointed out during that review process have not been addressed. Most importantly, the main motivating theorem (Theorem 1) for QWR appears to be incorrect: it relies on $\log \pi(a \mid s)$ being nonpositive, but this is not true in general in continuous action spaces. Instead of fixing the theorem, it has been moved to the appendix. The algorithm presented here is largely the same as that in two recent papers: [advantage weighted actor critic](https://arxiv.org/abs/2006.09359) and [critic regularized regression](https://arxiv.org/abs/2006.15134). It is possible this comparison was not performed because the offline setting studied is different than the usual offline setting: the data collecting sampling policy $\pi(a \mid s)$ is required for the optimization in Equation 3, unlike most offline RL works which use only logged trajectories.<BRK>I think the paper is not novel enough to guarantee acceptance. The contribution of the paper of the paper is limited given the existing work on AWR. The Section 2.2 is very confusing. They prove that AWR convergences to a policy that takes the actions appeared in the replay buffer in state determines action assumption holds. I think no algorithm can learn if the state determines action assumption holds. Many existing off policy papers make assumption on sampling policy (strong ignorability (1)). "Estimating individual treatment effect: generalization bounds and algorithms."<BRK>Summary: This paper presents a Q value weighted regression (QWR) on top of the advantage weighted regression (AWR) to improve the sample efficiency for offline RL settings. I think the topic investigated in this work is critical. However, the authors need to pay attention to a few major issues that discourage me to give a decent score. It would be better to see formally how QWR improves on top of AWR, in terms of returns and some constants. 2.The state determines action assumption is problematic. In the paper, the authors say “But in the case of limited data, when only a few trajectories were collected, this assumption may hold, at least for a large subset of the replay buffer, which makes it relevant to the study of sample efficiency.” Such an assumption is quite strong in the paper. Since the authors leverage this assumption to give Theorem 1 for AWR, which reveals shortcomings for AWR. How to justify this assumption?<BRK>Based on the Advantage Weighted Regression algorithm, this algorithm calculates the advantage of the sampling policy \mu by estimated Q value function. Experiment results show that the QWR algorithm has better performance than the AWR algorithm with limited data. This paper is well written and easy to follow. The main contribution is delivered:  New sample efficient algorithm. Second, as the authors mention in related work, several recent works have developed algorithms similar to the QWR algorithm. It is better to show the performance of those algorithms in the experiment. Finally, in the formula (5), it seems strange that there is an expectation for random variable a’ and variable s , but none of them appear in the formula.
Reject. rating score: 3. rating score: 5. rating score: 6. rating score: 7. <BRK>The authors define model targeting attack, in which the attacker aims to poison training data such that the learnt model is a given target model. The authors define distance between the poisoned model and the target model using their loss difference (this reduces to objective driven poisoning attack). However, when measuring success using distance between the poisoned model and the target model, the attack is obviously unsuccessful when the loss function is non convex. So the attack could only be successful for strongly convex loss functions, which have a global optimal solution. I would suggest the authors to explicitly mention that the attack is limited to such setting. However, once limited to such setting, the paper s contribution is also limited. The evaluation does not compare with subpopulation attacks. I suggest evaluating other models and datasets. Objective driven attacks are more relevant. In fact, the evaluation is on subpopulation attack, which is an objective driven attack.<BRK>This paper presents an improvement on an interesting problem: poison a dataset to induce a machine learning process to a misleading model. The basic idea is that the attacker iteratively generates new data points so as to minimize the difference between the poisoned model and the target model. The approach is only evaluated on one dataset (Adult dataset), considers only one model (SVM) and compared against one approach (KKT attack). This is not enough to justify the universal effectiveness of the approach applying to other machine learning algorithms. In particular, modern deep learning algorithms typically suffer catastrophic forgetting issues when applying online learning algorithms, and these models are of more interests in the context of poisoning attacks. So this work does not provide enough evidence to justify that the proposed approach is effective in dealing with them. Maybe I neglect them, but I do not find the description of the adult dataset s stats, i.e., how large is the dataset? The paper studies poisoning more 1500 data points, what percentage is these data to the entire dataset? It looks like, from the description, that these dataset might be 100% of the entire dataset. In addition, it might be important to compare against Jagielski s 2018 paper [1], which is not cited, as well in the non subpopulation setup, in addition to KKT, which should provide a better baseline.<BRK>**Paper summary**The paper proposes an algorithm, that works in an online fashion, for targeted poisoning attacks. If the loss function is convex, then the algorithm is guaranteed to converge to the target as the number of poisoned samples increases. The paper claims that this is the first model targeted attack which has theoretical guarantees. The lower bound provided is interesting in the sense that it can give a lower bound on the number of samples needed to reach the target model from the current model. 3.On SVMs, the experiments show that the attack is almost optimal in the sense that it matches the theoretical lower bound. I think that would be the true lower bound for poisoning. It would be more interesting to see if the attack also works well on deep neural networks. I believe the attack proposed in this paper is strong and theoretically backed. However, I have some concerns regarding the attack s success on deep neural networks.<BRK>Instead of formulating the problem as a bi level optimization as standard, the paper proposes reducing the attack to an online learning problem, where the adversary decides on a poisoning data point in a sequential manner. The data point that leads to the largest difference of loss between the current model and the target model is selected as the next point to inject into the training data. Another strength of the paper is that the proposed online learning based attack can even outperform the traditional KKT attacks. I am wondering if the authors could provide some intuition about why the proposed attack results in superior attack performance as compared to traditional data poisoning attacks? I am also curious if the attack in this paper extends to the deep learning setting, where the victim learners are no longer convex. Empirically it would be nice to show some results on that, although the theoretical results definitely do not easily extend.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 7. <BRK>Experiments: The experimental results are overall nice and promising. The paper is mostly easy to follow, and is well written and well motivated. based on its own state? It seems like there must exist multi agent works that exploit the local structure of the interaction to reduce the dimensionality. how can one reproduce this? Then, the other main issue is with improving the rigor of the presentation and the math. Please clarify and unify the definitions. Then it s not clear what s going on with the dimensions of the local states. On that note, the idea of "nearby agents" should be more rigorously defined. Subsection 2.3 raises some concerns in this regard since the algorithm resorts to "end to end learning of Q_i", in what seems like a total bypass of the idea of the perceived rewards. This raises the question: why isn t it possible to bypass the idea of the perceived rewards and motivate the paper based on (4), which is closer to the practical algorithm?<BRK>The proposed CollaQ could be easily built on QMIX and trained end to end. CollaQ outperforms other baselines on various tasks with the ad hoc team play setting. The paper is very clear and well structured. The Ad hoc MARL is an important problem in real world applications but has not been fully studied. The interactive term with regularization is a practical and promising method to solve this problem and could be followed by other researchers. However, I still have some concerns:First, the theoretical analysis of reward assignment is not close to the implementation of CollaQ. I think it is over claimed and should be removed. Statistical results are more convincing to verify how CollaQ influences the decision. Update after author response I thank the authors for the detailed response. Most of my concerns have been addressed, and I decide to keep my score.<BRK>Good theoretical analysis and compliant experiment performance1. But in experiments (including experiments in appendix), the authors only discussed the claimed optimal setting (“using the observation o_i of agent i covers $s_i^{local}$”). The definitions of them are not very clear. 3.As this work is eventually an MARL work in solving ad hoc team setting games by decomposing reward. Some explicit comparisons (May be in form or experiments or brief analysis) should be added with some MARL methods(SSD: Social Influnce as Intrinsic Motivation for Multi Agent Deep Reinforcement Learning; PBRS: Reward shaping for knowledge based multi objective multi agent reinforcement learning).<BRK>This is an old idea, but the paper essentially applies it to the state of the art deep learning machinery, producing impressive results on the hardest games state of the art algorithms can manage. The paper does not offer a huge amount of novelty, but rather presents a solid deep RL engineering approach to solving the wider problem in a specific setting. Nonetheless, the technical material is well developed, the presentation is overall of a high quality, and the experimental results extensive (and impressive).
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 5. <BRK>Despite the method being about learning representations, it does not compare to regular methods that treat images as images, not a set of (location,value) tuples. But it seems to be suboptimal for all of these goals. Con:  The goal and motivation of this paper is lost to me.<BRK>recommendation and reasoningThis paper is slightly below the acceptance threshold. I have read the author responses but my initial review remains unchanged. The writing in Section 3 needs to be improved. The paper describes a method for using contrastive learning to align representations of observations from the same generator.<BRK>The authors  responses also address most of my concerns. The remaining issues are: 1) the writing needs to be improved to make the paper easy to read; 2) I am still not so convinced with the experiments. What s a good N here? Is this practical?<BRK>The paper proposes to find a good representation of the underlying data generating function (data distribution) via contrastive learning. However, as most of these tasks (especially Sec 4.2 and 4.3) are formulated into few shot problems by authors, can you provide mathematical problem formulation at the beginning?
Reject. rating score: 3. rating score: 5. rating score: 5. rating score: 5. rating score: 6. <BRK>This paper introduces a method for improving robustness of neural networks to domain shifts by adversarially perturbing the feature statistics. This is a very interesting idea, by playing a middle ground between the worst case of PGD and not doing anything. My main problem about the paper is the evaluation and particularly the lack of evaluation of certain models and certain datasets. It is unclear to me why the quite related AdvProp model is not evaluated here. Even if they are difficult to train the pre trained models are available here: https://github.com/rwightman/pytorch image models. Same with the Noisy Student L2 model which doesn t have any sort of adversarially perturbation and performs much better on ImageNet C than the best number reported here. For reference pretrained weights for both model types are available in the above link. With the availability of pretrained models it seems inexcusable to only have 5 arbitrary comparison points, especially when there are models with significantly better accuracy. Next on the distribution shift side, I d also like to see more than just 3 distribution shifts. There have been two recent papers that do a metastudy of many distribution shifts: https://arxiv.org/abs/2007.08558 and https://arxiv.org/abs/2007.00644. A thorough evaluation on other distribution shifts can give a more complete picture of the advantage of the proposed approach to distribution shift rather than just 3 numbers out of context. For these reasons I recommend rejection.<BRK>This paper proposes an algorithm for generalization to unseen domains. The algorithm performs adversarial training on the batch normalization coefficients. Strength:I think the algorithm in this paper is simple and straightforward and the paper is easy to follow. The authors provided experiments on large scale dataset, some ImageNet based datasets. Weakness:My major concern is that the experimental results does not fully validate the effectiveness of the proposed algorithm. For example, in Table 1, the results are very close to Augmix on Imagenet C and Imagenet Instagram. Only on Stylized Imagenet, the proposed method shows benefits. I have a hypothesis that the proposed AdvBN method may bias the model to be more robust to image style change (the images in Figure 2 and 3 kind of show this), but may not improve the robustness to other types of corruptions. Overall, it is not convincing that the proposed method can provide universal robustness gain to all kinds of corruption/domain changes. For example, there are 15 different types of image corruptions in Imagenet C, and I suggest the authors to provide the test results on each type of corruption, so that the readers can better understand what types of corruption this method is more effective on. If possible, I would also like to see some experimental comparison between this method and distributional robust optimization, which I think is a more principled approach to getting general robustness. After author response: I would like to thank the authors for providing the details of each corruption in ImageNet C dataset. I would like to see more theoretical or experimental evidence that can help us get a deeper understanding of this approach. Overall I decided to keep my score.<BRK>This paper proposes adversarial batch normalization (ABN) to perturb feature statistics. It is to makes CNNs more robust to image style or appearance changes. Experiments show it can improve robustness on image classification and segmentation. 3.Experiments show its effectiveness on two tasks: classification and segmentation. I have read the paragraph describing this ablation study (AutoAugment*) but still do not understand it. Does it mean ABN in Table 1 is with AutoAugment during the finetuning? 4.Experiments only use one model, ResNet 50, which is insufficient. It is not clear whether the optimal layer changes if the network architecture changes. 6.The paper emphasizes the finetuning efficiency when compared with other methods trained from scratch. I think the pre trained time also needs to be considered. For a new task, there is usually no well known pre trained models. In summary, the proposed ABN is simple and effective in improving model robustness to style changes. My main concern is that the experiments are insufficient and have some space for improvements. See points 2 6 in the above for details. Post rebuttal updatesThank the authors for the great efforts in addressing the concerns. The new experiments on two new backbones DenseNet 121 and EfficientNet B0 show the method can work well with multiple backbones, which is good. However, my other concerns remain unsolved. 1.Combination with AugMix seems necessary to demonstrate state of the art performance and its orthogonality. 3.The running time comparison should take the model s pre trained time into account in Table 8. 4.Regarding the blocks in the ablation study, I remember a ResNet50 for ImageNet has 4 blocks. Table 3 only lists 3 blocks (2,3,4).<BRK>Moreover, the authors show improved results on a semantic segmentation task. However, some experimental details presented in the paper require a further clarification. The paper proposes an interesting approach that can help to improve robustness towards domain shifts without requiring any extra data. I would be willing to increase the score if the paper improves its experimental part, in particular by properly reporting the results of AdvBN (see **Cons**) and its baselines. The approach doesn’t require any extra data (labeled or unlabeled). In particular, in Table 1, “AdvBN” rather refers to “AdvBN + AutoAugment”. But it’s clear that adding AutoAugment to any other competing method would also improve them. Table 2 shows the results of *AutoAugment alone* (and apparently, there is a benefit of combining AutoAugment with AdvBN), but what one really needs to know is the performance of *AdvBN alone*. Otherwise it’s not even clear whether the proposed AdvBN method is better than standard Lp PGD training. Any Lp robust models from Engstrom et al.(2019) is clearly suboptimal for the tasks considered in this paper since these models have much lower clean accuracy. I couldn’t find a discussion on this, so I assume you used AutoAugment with **all** its data augmentations including those that are present in ImageNet C. If it’s true, then the comparison to AugMix is unfair as in their method they have removed all overlapping corruptions. This is a very important detail that should’ve been clearly discussed in the main part and not just in the appendix: *“The results we report in previous sections with regard to ImageNet, ImageNet C and ImageNet Instagram are obtained by using BN statistics corresponding to original features. We only use auxiliary BNs, which keep the batch statistics of adversarial features, to test performance on Stylized ImageNet in Table 1.”*And then I’m not sure what the results in Table 1 for AdvBN+AutoAugment tell us: that there exist 2 models obtained via AdvBN, and one of them is good on one domain shift, and another is good on another one? But how do you know at test time which of the 2 models to apply? Algorithm 1 has multiple mistakes: (1) an additional loop over batches is missing, (2) not clear how $\delta_\mu$ and $\delta_\sigma$ are initialized, (3) *“Minimize the total loss w.r.t.network parameter”*   argmin there seems to be inappropriate, I think what was rather meant is doing *one* step of gradient descent wrt $\theta$, (4) in the same place: there should be a clear distinction regarding when the loss is taken wrt a single data point, and when wrt a batch of points (this is particularly important since AdvBN introduces the dependency of the perturbation set on the batch). Table 2: the ablation regarding where to put the AdvBN layer is inconclusive since it had to be done with respect to different epsilons.<BRK>The authors go on to apply this normalization in an autoencoder arrangement in order to show the relationship between the effect of the normalizer on the features and the image that they represent. Finally, the method is evaluated by pre training on imagenet, finetuning with the proposed batch normalization, and testing on three versions of imagenet with global transformations applied. Pros: Novel application of normalization ideaClear presentation Useful and relevant to the conferenceVery good quantity and relevance of experimentation Cons: Would like to see more introspection on the results All in all, I think this is a strong paper with only minor issues and would be a great addition to ICLR. The idea presented is simple but also seemingly very strong in principle and I appreciate that it does not require much in the way of algorithmically complex calculations. I am most happy about the presented level of experimentation which the authors have done a good job of a) contrasting to the state of the art, b) exploring their own model, and c) exploring other applications. That to me would be something that could elevate the paper to the top 50% or more of papers. Questions for the authors: Why is AdvBN not improving on AugMix for imagenet c but does for the other datasets does this indicate some drawbacks for the method? Why does performance appear to get better in the ablation study on $\epsilon$ but then get worse after a certain threshold? Post rebuttal updates:I thank the Authors for their response. After reading all the reviews and comments I feel that there are aspects of the proposed approach that are not fully understood, despite the improvements. For example, those related to AugMix, and providing fully symmetric comparisons between Cityscapes and GTA5, as several reviewers have pointed out. For these reasons, I have decided to revise my ratings as I also recognize the importance of these observations.
Reject. rating score: 5. rating score: 6. rating score: 6. <BRK>This paper proposes an efficient meta learning approach using implicit processes. In addition, the paper is well organized and clearly written. 3.Authors demonstrate the effectiveness of this IPML on benchmark datasets and real world tasks. For example, how much is the contribution of representing each task as a continuous latent vector? 3.Apart from the performance in the experiment, authors should also mention the efficiency of the IPML compared with other baseline methods.<BRK>Motivating the setup with a real world use case would strengthen this aspect of the paper. The real world risk detection experimental results in the appendix are nice and interesting. [1] Garnelo, Marta, et al."Conditional neural processes." The proposed meta learning algorithm based on IPs is sound and the extensive experiments demonstrate that it works well in practice.<BRK>This paper proposes Implicit Process Meta Learning (IPML) where each task is represented as a continuous latent vector $\mathbf{z}$, and corresponding data points are described as function values evaluated at an implicit process conditioned on the task latent vector $\mathbf{z}$. If so, did you observe any empirical degrade in the performance? Overall I find this paper is clearly written. My first concern is that whether this is a property only for the proposed IPML.
Reject. rating score: 4. rating score: 7. rating score: 7. <BRK>This paper presented the variational dynamic mixtures as a deep probabilistic model for time series forecasting. The research issue, called the taxi trajectory prediction problem, is addressed. Pros:A new solution to mixture density network as a kind of generative model with latent states and multinomial observations was proposed. This matter made the reading to be easily confused. A clear algorithm or working flow for complicated system was missing. Some descriptions were not clear.<BRK>SummaryThis paper introduces variational dynamic mixtures (VDM), a new variational family, and demonstrates that using VDM to model the approximate posterior in sequential latent variable models can better capture multi modality in data. VDM includes a distribution over recurrent states in the inference model, such that a sampling based marginalization of this distribution reduces the approximate posterior to a mixture model. Positives+ This paper tackles an important and well motivated problem: capturing multi modality in data. + Paper is well written and easy to follow. Concerns   I think this paper would benefit from one additional dataset where the multimodality is inherent in the data. The “sequence forecasting” paragraph can be omitted/combined with “neural recurrent models”. The results on the taxi dataset look good. It would be great if you can also provide analysis on the resulting latent space, similar to what was done in Figure 3.<BRK>To achieve this, the authors start from VRNN and alter the inference model so that it uses stochastic recurrent states and a mixture variational posterior distribution (with 0/1 weights to trigger only the most likely mixture components encouraging multi modality). As a minor contribution they propose a new evaluation metric for measuring the diversity of generations based on Wasserstein distance. This is important in their case when the likelihood evaluation may favour generations from a single mode   a situation their model shall prevent. The paper is very well motivated (with taxi trajectory prediction as a running use case) and well positioned with respect to the state of the art. The paper is well written and structured to help the reader follow the main thoughts. However, there are some points in the mathematical formulation of the model which raise questions and deserve to be explained better   see below. For this reason I recommend not accepting the paper for now but I m am very much willing to improve my score significantly once these will have been clarified.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. <BRK># SummaryThis paper studies the relationship between extractability of features from pre trained representations and how much a fine tuned model uses that feature. Apropos footnote 1. How does that impact the analysis? This paper fills an important gap in the NLP interpretability literature that has recently been a cause of concern in the community. The paper aims to connect these two aspects, and it does so quite convincingly, although I have some reservations below. 2.The experimental setup is well designed. ## Natural language examples9. 4.The paper makes use of recent advances in interpretability work, including information theoretic probing, and draws connections to a broad range of related work.<BRK>The paper aims to bridge the gap between model interpretation using probing and model s use of spurious features. I really like the premise of the paper, which is connecting the research on the linguistic learning of a model with the presence of important and spurious features in the data.<BRK>So my impression of the finding is not changed substantially by the author response. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~This paper addresses a seeming contradiction between findings that indicate encoding of linguistic information in models  internal representations, and findings that show models not to use more sophisticated linguistic information during fine tuning on downstream tasks. So it s not clear to me that the paper is making a sufficiently novel, surprising contribution at present. The authors test on toy, non language data as well as natural language data, and find support for their hypothesis.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 5. <BRK>Summary:The paper proposes to use LSTM to learn partial differential equations. Experiments include Wave equation, Heat equation, Burgers  equation, and Navier Stokes equation. They compare with PINN on Allen Cahn and Burgers equation. The paper is nicely written and easy to understand. But I have several concerns:Novelty:Using the RNN/LSTM type of networks for time series / time dependent PDEs doesn t seem to be novel. In my opinion, it is not a fair comparison.<BRK>This work proposes a sequence to sequence approach for learning the time evolution of PDEs. The method employs a bi directional LSTM to predict solutions of a PDE based formulation for a chosen number of time steps. By itself this is an interesting, and important goal, but the method does not seem to contain any novel components apart from demonstrating that LSTMs can be used to learn data from PDEs. The paper only compares to a simple form of PINNs, but not to a variety of other time forecasting algorithms available in the deep learning field (LSTM are just one of many methods used these days, a more state of the art one being e.g.transformers). In addition, the examples only contain single cases with relatively simple model equations.<BRK>This paper is about using deep neural nets to predict solutions of dynamical systems described by PDEs. The paper is overall well written, and the contribution is clearly stated. Indeed, the idea is as simple as correct; mainly using a DNN with dynamics, e.g., RNNs, to predict solutions of a dynamical systems over time, based on some past (timewise) training data. This is clearly stated in the paper and there is nothing wrong with this idea, except that it seems to me that the authors are exaggerating the value of their findings. I hope this makes sense. These tests will show you that you are not learning the physics of the system, as you are claiming throughout the paper; please remove these exaggerated statements.<BRK>The paper is a good contribution to spatio temporal modeling in complex physics. There is nothing particularly interesting in this approach. $O(10^{ 4})$ and $O(10^{ 5})$ as RMSE are essentially equal good for a super simple example like inviscid Burgers or any of the other examples. The authors should cite these papers and also consider RC as a baseline. 2) I have some concerns with the comparison with PINNs. There is an advantage to using PINNs that is overlooked by the authors.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>This paper studies the learning to optimize (L2O) for minimax optimization. Since L2O has been studied in a few works, extending L2O from continuous minimization to minimax is a straightforward idea and not super novel. But it also is a non trivial effort, as minimax problems are much harder and unstable to solve. The authors then presented two extensions to improve the generalization of Twin L2O. The first one is based on curriculum learning to focus the meta training gradually from easy to hard instances. I appreciate the authors clearly and openly discussed the current work’s limitations by end of the paper.<BRK>Minimax optimization is in general unstable and harder to solve, challenging whether an L2O model can indeed figure out effective learning rules from data. Further, in order to design L2O for minimax problems, one has to decide to what extent the learning models for min and max updates should be coupled, and what reward shall be used (minimizing the negative cumulative objective value is no longer viable)By discussing and comparing a number of design options, the authors find that two decoupled LSTMs sharing one variation based reward is the best empirical design. Is this really correct? The authors presented Safeguarded Twin L2O, a preliminary theory effort saying that under some strong assumptions, it is possible to theoretically establish the general worst case convergence of Twin L2O.<BRK>On the math side, even though the authors tried to motivate their work from the limitation of classical minimax algorithms, I feel its impact may be limited for the optimization field, as it does not reveal many insights on how to design new minimax algorithms or providing better theory guarantees. To extend L2O from minimization to minimax where two groups of variables need be updated, the authors designed and explored a variety of model options. As an empirical paper, it would definitely become stronger if the authors can prove their concept on some real minimax problems such as GAN or robust/private training. The paper is in general well written.<BRK>### SummaryThe paper introduces the _learning to optimize_ (L2O) framework into the solution of minimax problems. On top of this, the authors further investigate two possible improvements. This problem is particularly challenging and the authors manage to obtain some preliminary results. Lack of clear motivation. 3.The definition of the loss function is not convincing. Although I fully understand this paper is just intended to be a proof of concept study that demonstrates the usefulness of L2O in minimax problems, I believe the authors should justify more the framework and their algorithmic choices (as done for the decoupled design). The definition of this loss function is probably one of the most important things in the framework.
Reject. rating score: 3. rating score: 6. rating score: 6. rating score: 7. <BRK>The authors propose to change agents  incentives in order to lead to a better Nash equilibrium outcome. They propose a practical decentralized approach to computing the "optimal" change of incentives. I m confused by many aspects of this paper:1. 3.Why do you optimize the price of anarchy instead of social welfare at worst Nash equilibrium? How does f^A_i depend on t? Or does (4) hold for all \beta_i (e.g.approaching infinity)? 6.What should I conclude from your experiments? The Nash equilibrium utilities at the modified games seem better, but it s not clear how much the games were modified. (B) "set of the equilibria of the game restricted to the line"   I don t understand what this sentence means. (C) Why don t you define utilitarian/egalitarian before Theorem 1? (D) Typos: "one hot" "the the"<BRK>The paper details a method by which it attempts a method to change the utility function for agents so they incorporate the utilities of other agents, resulting in more cooperating agents, so that the price of anarchy is minimized. As more a game theorist than a ML expert, I did struggle a bit with the technical bits in section 2. I could not follow the setting of the "zero sum election", and what is the described game. Moreover, the introduction promises an incentive compatible setting. What is intended to be an IC mechanism?<BRK>##########################################################################Summary:This paper proposes a differentiable, local estimator of multi game inefficiency, as measuredby price of anarchy. ##########################################################################Reasons for score: Overall the paper is well organized and provided clear motivation of the problem. I would be willing to see more discussions on if these restrictions could be relaxed to achieve Nash under the mixed losses. 3.What are the computation times for running D3C, and how does that compare to the baselines? The authors  response are helpful w.r.t the questions I raised in the review.<BRK>This paper proposes a (decentralized) method for online adjustment of agent incentives in multi agent learning scenarios, as a means to obtain higher outcomes for each agent and for the group as a whole. The paper uses the “price of anarchy” (the worst value of an equilibrium divided by the best value in the game) as a proxy for the efficiency of the game outcome, and derive an upper bound on a local price of anarchy that agents can differentiate. The Appendix also seems to be missing from the paper, although there are references to it in the text (e.g.reference to F.4). I appreciated the frankness with which the paper describes their method, e.g.in the following excerpts:“Ideally, one meta algorithm would allow a multi agent system to perform sufficiently well in all these scenarios.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 6. <BRK>The paper claims that an ensemble of multiple classifiers can achieve better certified robustness than a single model with comparable cost in terms of the total # parameters. However, I generally feel both the theoretical and empirical results of the paper are somewhat weak to meet the ICLR bar for the current submission:  It seems to me that Lemma 1 and Theorem 2 are somewhat straightforward (or incremental) implications of the (well known) generalization theory (e.g., [1]), and I could not find a significant reason why they should be presented in the main text. I think Algorithm 1, on the other hand, should be rather justified more rigorously instead of SWEEN itself: "could Algorithm 1 really recover the MC estimate? ", or "is there a theoretical guarantee that this algorithm is more efficient than MC? The empirical results are not fully convincing to show that model ensemble reliably improve robustness: the paper only present two specific configurations of ensemble, namely SWEEN 3 and SWEEN 7, but there should be much more combinations to verify the effectiveness of ensemble. p5, "... will require 10,010,000 local evaluations ...": it would be a bit confusing for some readers to figure out why it requires additional 10,000 inferences in the count. Could SWEEN further improve ResNet 110 if one consider ResNet 110 x 3 ensemble model? Why SWEEN 7 is not considered in Table 2?<BRK>In this work, the authors study the effect of ensembling on randomized smoothing certification. By learning diverse classifiers and then applying randomized smoothing the authors arrive at an ensemble of smoothed classifiers. The authors make an argument for the optimality of the proposed approach and show that it performs favorably compared to randomized smoothing certification of only a single model. Using a linear combination of models based on their empirical risk seems to be something which one could do in general, just that in theorem 2 the authors are using a notion of risk which also includes the certified robustness of the models. Given some time, I will read further into the exact details in the supplementary which I was unable to do sufficiently during the initial review period. The results that the authors show for their models are favorable and show increased robustness wrt the adversarial smoothing criterion. Following a reading of all of the other reviews and re evaluating the paper I remain optimistic and slightly positive about the paper as I think it is an important and interesting research direction; however, I am not fully convinced to increase my score given that some of the empirical comparisons could be greatly strengthened to be more than marginal improvements over MACER trained networks.<BRK>In this paper, the authors introduce SWEEN, an ensembling scheme for smoothed classifiers. During deployment, smoothed models rely on sampling a large number of input perturbations and ensembling exacerbates the computation burden. Overall, the paper is well written and provides solid evidence that ensembling can improve certified robustness. I am generally curious about the limits of the approach. 2) While the adaptive scheme is useful, it distracts the reader from the main message. I d suggest moving it to the appendix and consider experiments on ImageNet. Details:A) Fig.1 is not color blind friendly and difficult to read (small font). The new results on ImageNets are greatly appreciated too. However, in light of other reviews, I would have hoped that the authors try better training procedures on SWEEN 7 (e.g., MACER even if it means using other ResNets instead of VGG), as otherwise it is difficult to judge whether ensembling really helps. It is unclear why MACER was not used on ImageNet (since all models are ResNets).<BRK>This paper demonstrates the advantages of ensembles of smoothed classifiers to achieve better approximately provable robustness. The authors also show that adaptive prediction ensembling can accelerate the computation. Cons:1.There is a significant gap between theoretical analysis and experimental settings. Based on the condition above Equation (25), both the number of individual models and the number of training samples are trivially large. PresentationPros:Generally, this paper is well written. This part is not well justified. Especially, the authors should point out what is new in Algorithm 1 compared with [Inoue 2019]. SummaryFrom my point of view, this is a borderline paper. It systematically study the ensemble of randomized smooth models and demonstrate interesting results.
Reject. rating score: 3. rating score: 4. rating score: 5. <BRK>This paper proposes TraDE, a transformer based density estimator that is capable of learning a density of real valued tabular data. The motivation of using transformers for auto regressive modeling of continuous data stated in Section 3 is persuasive. The experiments include training a regression model with generated samples, two sample testing using a classifier, detecting out of distribution samples, and learning on noise corrupted data. These analyses provide a rich view of how the proposed model behaves and confirms the effectiveness of the proposed approach. Weakness:I reckon the contribution of the paper as a density estimation method is marginal. This change of output parametrization seems trivial and straightforward, compared to architectural improvements presented in [1, 2]. Another difference is the use of maximum mean discrepancy (MMD) as a regularizer (or an auxiliary objective function). The analyses conducted to evaluate generative models (contribution no.2 on page 1) are valid, but the paper is not the first to perform such experiments and therefore it is not adequate to claim those experiments as a core contribution. Examining the predictive performance of a model trained on generated samples is used to evaluated generative adversarial networks [3, 4]. Also, measuring out of distribution detection performance is used widely in generative modeling literature [5, 6]. Also, the maximizer of the objective in (1) is indeed p(x), given infinite data and a correctly specified model. There is no description of what transformers are used as the baseline in Table 3. How exactly do these transformers differ from TraDE? Minor comments:  It would be more appropriate to use Proposition instead of Lemma for Lemma 1 and Lemma 3. I suggest to number them separately. It would be nice to mention that the datasets used in the experiments are tabular data, just in case if a reader is not familiar to the datasets. [1] Katharopoulos, Angelos, et al."Transformers are rnns: Fast autoregressive transformers with linear attention." arXiv preprint arXiv:2006.16236 (2020). [2] Child, Rewon, et al."Generating long sequences with sparse transformers." Computer Vision and Image Understanding 179 (2019): 41 65. "Implicit generation and modeling with energy based models." Advances in Neural Information Processing Systems.<BRK># SummaryThis paper uses the Transformer architecture for density estimation. It performs well on several non trivial synthetic datasets and standard benchmark datasets. The authors also tested the model on other tasks that rely on density estimation. ## Pros1.Addresses the issue of variable ordering on learning auto regressive models and long range dependencies with a new model architecture1. Why do the authors believe that a better model should be more robust to changes in the training dataset? 1.An overarching question is: are all the benefit of the proposed method a result of introducing the MMD regulariser? Excellent written quality and comprehensive review of related literature. The additional evaluation tasks are not new, and the particular instances implemented have issues to be addressed1. No mentioning of computational costs for training compared with benchmark methods. I think I may be missing something here. 1.The equation of $alpha_j \dots$ on page 4 is missing a parenthesis ")"1. If the model is modelling each conditional correctly, why would it not model the joint well? This is mentioned in a strange place right after Lemma 3 which says it can model any joint. A colour bar would help. Can the author simply mention that adding this improves the model on small datasets? 1.Two sample testing is not novel, and the results can be reported at the same place as log likelihoods. 1.OOD: how is the threshold swept?<BRK>**Summary**This work proposes a new auto regressive density estimator built using self attention module from the popular Transformer network. TraDE can be seen as an extension of decoder only Transformer network where an input embeddings are given by a simple RNN based encoder. Like Transformer, TraDE leverages multiple layers of self attention module to implicitly model long range dependencies. This effectively eliminates the need for explicit vertex ordering and hence useful on data with no known canonical ordering. The proposed model is general and can be applied to both continuous as well as discrete data. To further evaluate the qualitative performance of density estimators, it proposes suite of various tasks on which TraDE is shown to work well. **Quality**The paper is very well written and easy to follow. The experimental evaluation followed are standard (for density evaluation) and additional tasks depicts the usefulness of the sampled samples. **Originality**As summarized above, TraDE is a simple extension of decoder of Transformer. Minor modification like RNN based inputs (inspired by Wang et.al) and MMD loss led to dramatic improvement for density estimation tasks. Moreover, the tasks used for qualitative evaluation are also not completely novel. As pointed by the author, some of them (regression, two sample test, OOD) are already employed by the prior work. I do not find any mention of methodology employed for sampling. 2.Please include citations in Table 1. 3.From Table 3, I note that inclusion of MMD loss has very minor effect on performance. What is the training time tradeoff for including MMD loss ? Also compare quantitative results of (TraDE   MMD) model for various tasks.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 7. <BRK>* The proposed method achieves good accuracy compared to the other approaches considered in the experiments. * The results in Figure 2 indicate that typically a large number of rules are learned, yielding models that are unlikely to be interpreted reliably. I think further investigating what tradeoff exists between accuracy and interpretability is necessary before the paper can be accepted. * Some of the responses to other reviews have reinforced some of my concerns. In particular, it seems the distribution of weights attached to rules is not optimal for the proposed (but unevaluated) rule pruning method. Also, the authors conflate statistical significance with practical significance when replying to Reviewer 4.<BRK>Authors propose a new scalable classifier, named Rule based Representation Learner (RRL), that can automatically learn interpretable rules for data representation and classification. However, it is questionable whether the difference in accuracy between decision trees such as C4.5 and RRL in the experimental results is crucial. Alternatively, the choice of experimental data may not be appropriate to demonstrate the superior performance of RRL. I also didn t understand why the fuzzy/soft rule sacrifices the model interpretability.<BRK>Indeed, rule based learners are considered to be more interpretable. However, I have several questions and comments about this achievement as I list below:  Are the results given for test set? What are the computation times? How does this affect the interpretability? This is how it would be presented by other rule learning methods. Am I missing something here? In other words, is it possible that a test sample is not classified with the output set of rules?<BRK>Summary:This paper presents a new rule based  classifier called Rule based Representation Learner (RRL), that automatically learns interpretable non fuzzy rules for data representation. Through experiments on 9 small and 4 large datasets shows that RRL improves over other methods, has low complexity and interpretable. This paper presents a solid contribution in learning rule based classifiers and hence to interpretable machine learning. The paper is clearly written and superiority of the presented models are backed by strong experimental results.
Reject. rating score: 3. rating score: 5. rating score: 5. rating score: 6. <BRK>It is better to conduct further experiments to show the effectiveness of the proposed GCN and LPA integration mechanism over simplified combination. From the evaluation results, this baseline performs much worse than GCN and GAT, which may indicate that the predict combination involves some noise. In the experiments, only model scalability comparison between the new GCN LPA method and GCN, is studied.<BRK>Experiments show that the proposed model is better at splitting embedding of nodes from different classes, and achieve improvement in node classification performance. 3.Besides, there are some other papers also seeking to adapt edges for the training of GCN, which are not mentioned. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.<BRK>Reweighting the edges using only GCN is a very natural ablation model. Moreover, my main concerns are about the experiments. The theoretical analysis of the correlation between LPA and GCN is interesting.<BRK>#########################################################################Questions:In the experimental results. The manuscript is overall well written, and the motivation of the proposed method is well explained by the proposed theorems. This is generally referring to a penalty term on a penalty on the model complexity.
Accept (Spotlight). rating score: 7. rating score: 7. rating score: 7. rating score: 7. <BRK>The major thing holding the paper back for me is that the lack of a good theoretical explanation for the CPT phenomenon. EDIT: I managed to set this up on a small transformer and was able to replicate the paper results. On ImageNet, CPT achieves accuracy on part with regular quantized training but saves bit ops.<BRK>### OverviewIn this paper, the authors proposed Cyclic Precision Training (CPT) for low precision training. The paper provides an in depth analysis of the advantages of varying precision. 2.The proposed CPT is effective for improving training convergence and reducing training cost. 4.The experiments are solid. The results of CPT is impressive.<BRK>Their proposed Cyclic Precision Training (CPT) cyclically varies the precision during the training and the boundary of precision values is determined by a precision range test (PRT). This hypothesis is interesting and it would be better to support the hypothesis with empirical validation and theoretical justification. In addition, I think the proposed precision range test (PRT) method is unfair and requires modification. The paper is mostly well written. Section 4.4 seems redundant and should be put before the experimental section. The connections are also required to be explained in detail.<BRK>3.The original hypothesis was that cyclic precision would have a similar effect to cyclic learning rate, but there s little discussion of this in the paper. The method is simple and easy to implement . 1.This work appears to be an another example of multiprecision training (with a precision switching mechanism): https://arxiv.org/abs/2006.090492.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 5. <BRK>This paper proposes a transformer architecture for image colorization. Pros:+ The paper is well written and easy to read. + Image colorization is an important problem in computer vision. + Both automatic metric (FID) and human evaluation are used to compare the method with existing approaches. The performance of the proposed method significantly outperforms the previous state of the art.<BRK>The paper is well written and the performance of proposed transformer architecture is strong. I think that this work is above the threshold of acceptance. **Strengths**The motivation of the proposed architecture is reasonable. For upsampling, do we really need to make use of an autoregressive model? How about using cosine annealing for a learning rate scheduler?<BRK>  Update  The authors have addressed several concerns that I had regarding the work. The recap/explanation of the Axial Transformer is clear and concise. If not, how were they implemented? In 4.2 it says they “adapt the Axial Transformer model for colorization”. I believe that challenge has a lot to do with some of the difficulties in the paper around the methods and experiment explanation. Or perhaps combining the two sections?<BRK>Thank the authors for addressing reviewers  comments extensively. After rebuttal, I agree with the significance of the proposed method in terms of performance improvement in this particular task. Thus, I increased my rating to 5. In this paper, the authors propose an autoregressive image colorization method based on self attention. Experimental results show that each component of the proposed method is effective and the proposed method outperforms an existing autoregressive method. The clarity also needs to be improved in the method and experiment sections.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 5. <BRK>This paper proposes an alternative way to conduct hard attention. The proposed method is tested on image classification. The visualization in Fig.5 shows only that the region selected in each timestep indeed has the maximum EIG. But how to interpret the explainability from the glimpse sequence is still confusing. I can hardly perceive the sequence using my knowledge.<BRK>**Summary:** This paper follows a less explored strategy for achieving explainability via hard attention. They proposed a recurrent architecture which sequentially observe regions (glimpse) from an image. The authors validated the system on several benchmarks and show comparable performance with baselines. However the output of the system is not so appealing either in performance or explainability. It seems that all benchmark datasets used in the paper are not so difficult (e.g., 10 way classification with 32x32 images) and the performance of the proposed system is still far from satisfactory. Therefore, it s questionable whether the system could be scaled to even more challenging (but more practical) datasets like ImageNet.<BRK>This paper presents a visual hard attention image classification model. Secondly, the baseline hard attention model in the experiments, (Mnih et al.2014), is very old and it is not surprising that the proposed method outperforms it. Due to the above, the recommendation is Reject   but the authors are strongly encouraged to do experiments on more challenging data and compare to a newer baseline. However, the paper suffers from two major flaws.<BRK>##########################################################################Summary:This paper proposed a new hard attention model for the image classification. ##########################################################################Questions during rebuttal period:  For me, it s a bit hard to say the proposed methodology is novel. Authors needs to explain why the proposed model is different from pre existing methodologies regarding attention mechanism.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>This paper introduces a method for performing safe exploration in RL. Can any of the bounds derived in this paper be used to ensure the former type of safety? I believe that this paper introduces an important contribution to the RL community that is concerned with safety.<BRK>This is shown via extensive proofs providing theoretical guarantees on both convergence and the likelihood of failure as well as experimental results in a number of compelling tasks. ## Originality & SignificanceThe work provides original techniques with both theoretical and empirical improvements over previous techniques. An experiment that could add to the paper would be to pretrain a safety estimator and then restart training with a newly initialized policy how does this affect the convergence with different thresholds?<BRK>In this submission the authors are trying to tackle the very important problem of safe RL with safety guarantees. The problem formulation is rather clear, and the paper is overall well written. However, the unconstrained algorithm  Base  in the first tests, Fig.3 top number 1, the Base algorithm does not achieve a similar performance, could it be better tuned in that case ?<BRK>This paper would like to address the problem of ``"safe exploration" with a conservative estimation of the environment. I would like to say, considering the unsatisfactory of the problem description and theoretical analysis, I don’t think this paper is suitable for publication. Standard RL assumes the reward is not revealed to the agent, and I feel the safety constraints cannot be revealed to the agent in prior as well. I feel the derivation part in Section 3 is not so clear, as several notations have been introduced without explanation, though it is not hard to understand.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 5. <BRK>Summary: The paper proposes a modification for the adjoint method, such that to improve the training efficiency of neural ODEs. Comments: At first, I think that the proposed idea is quite interesting. Even if I am not an expert in the field, the theoretical argumentation seems to be rather reasonable. However, I think that the writing of the paper can be improved. Also, I find the coherence of the story a bit lacking. Therefore, I tend to keep my score and vote for rejection, because I feel that the submission has to become more accessible to general audience.<BRK>Summarizing the paper claims The paper addresses the problem of reducing the number of function evaluations (NFEs) during neural ODE training with adaptive solver and the adjoint method. Namely, the authors claim that for the variety of applications,  NFEs at backward pass can be reduced if the automated step selection will be based only on the part of the information of the adjoint state. Strong points The paper is clearly written. Therefore, I wonder if the proposed method shows similar behavior when weight decay is omitted. The authors provided some intuition about the cases when the method is effective, however, I think it is necessary to more thoroughly explain the applicability of the method before sharing it with the community at the conference, that is why I don t change my score. For example, for Continuous Normalising Flows (Table 2), the gain is not so significant as for Neural CDEs (Table 1). What are the crucial properties of the task to benefit a lot from the proposed technique? Are there any limitations? However, the reported mean loss for seminorm is twice bigger, and the reported standard deviation is four times bigger than corresponding values for the default norm.<BRK>The paper has good theoretical insights, however I agree with the other reviewers that more completeness is required to make the proposal stronger. SummaryThe paper recognizes that the error calculations in Neural ODEs involve some terms that don’t need to be accounted for. It then shows that by ignoring those terms, the number of function evaluations reduces while approximately maintaining accuracy. For example, the graph of the Backward NFE vs Epoch is showcased for the 1st task of speech classification, but not the rest of the tasks. The fact that some papers that work on Neural ODEs are not as cited or implemented in software packages as others does not mean they are not “standard” (or any such related term), and vice versa. The “12 lines of code” contribution also only works for the “standard” implementation. This is not a weakness per se, but software related changes are highly subject to the respective software being used.<BRK>**Summary**The paper suggests improving the training run time of neural ODEs when using the adjoint sensitivity method by relaxing the computational steps of $a_{\theta}$ and $a_{t}$. In the CNF setting, Table 2, I do not see a major improvement in NFE. All neural ODEs have been tested with a single type of ODE solver. Could the authors comment on this please? This paper is a 10 out of 10 as a blog post for the users of the adjoint method for training neural ODEs. I believe that the contribution of this paper is limited to a software hack that would benefit the community best if presented as a blog post. **Clarity**An extremely clear submission. **Originality**The paper is an original contribution.
Reject. rating score: 3. rating score: 3. rating score: 4. rating score: 5. <BRK>The authors tackle the problem of class imbalance in supervised learning. They propose an oversampling algorithm that is based on a genetic algorithm. Although the problem is important and the solution might be interesting, I feel that the paper is not scientifically sound and the English level is not satisfactory. Authors did not use the correct citation format, please see instructions by ICLR. Need to define.<BRK>In this work, the authors propose an oversampling technique which creates a population of synthetic samples for an imbalanced dataset using genetic algorithms. Each individual in the GA population corresponds to a synthetic sample, and the fitness function is based on the similarity (in feature space) of the synthetic sample compared to nearby samples of the same and different classes; standard crossover and mutation operators are used. In a limited set of experiments, the proposed approach sometimes outperforms competing approaches. However, I believe the paper has several key limitations that need to be addressed. While the proposed approach likely differs from these in some respects, the complete lack of context with respect to existing GA approaches for addressing class imbalance makes it difficult to judge the methodological novelty. The references are not consistently formatted.<BRK>The paper introduces ROGA   an oversampling method that uses a genetic algorithm to generate synthetic samples for the minority class. I would suggest that these could be removed entirely from the paper as this are basic machine learning terms that are well known and understood. Why not report the first order statistics based on the 20 experiments?<BRK>The authors focus on the class imbalance problem and propose an algorithm named ROGA to generated samples of minority classes to balance the quantitative difference between classes. Smote: Synthetic minority over sampling technique. 2.The experiments are insufficient in Section 4.3. 3.In Section 4.4, “traditional oversampling paradigm that try to …” should be “traditional oversampling paradigm that tries to …”.
Accept (Poster). rating score: 7. rating score: 7. rating score: 5. <BRK>The experiment results seem that the proposed method achieves good performance in large scale ResNets. It will be better to prove the effectiveness of the proposed method in more tasks, not only the classification task. Comments:(1)	The batch size you used for training is 1024. Can the proposed method decrease the batch size to a small value like 2/4/8 after removing batch normalization? And if it is difficult, can you give an explanation on how batch size affects the performance after removing batch normalization with the proposed method. Can you give the visualization of the same indicators of other initialization methods like Fixup initialization and make some comparison? I think it’s within your ability. (3)	It’s better to add some accuracy comparisons with other removing batch normalization works. UPDATE: The author has addressed most of my concerns, but regarding the motivations and the benefits for the community, I still keep my score.<BRK>This is used to train batchnorm free Resnets that compare to their baseline. Does your method allow models to be trained more quickly, i.e.how does accuracy compare after $N$ minutes of training time? Regarding Figure 3: Why compare your method **with** data augmentation and EfficientNets **without**? update I am upgrading to 7: Good paper accept; as my concerns have been addressed by the additional experiments. The proposed method is not yet a drop in replacement for BatchNorm in general, but it can be useful in specific circumstances, i.e.small batch size training.<BRK>This paper proposes the signal propagation plot (spp) which is a tool for analyzing residual networks and analyzes ResNet with/without BN. Based on the investigation, the authors first provide ResNet results without normalization with the proposed scaled weight standardization. 2.It is not clear that the trained model without NF with SWS can be used as a backbone that can be directly applied to downstream tasks (e.g., object detection). This type of paper would be better to be focused on investigating the characteristics of a network. The major problem of this paper is none of the advantages of NF with SWS are highlighted over BN, so it is hard to find any reasons for replacing BN with NF SWS. I mean non of the disadvantages of BN are addressed by the proposed method. 4.How did the authors compute gamma in eq.(3) in a training phase?
Reject. rating score: 5. rating score: 6. rating score: 6. <BRK>It is more convincing to see how much score loss can the proposed framework cause, compared with clean data samples. The authors propose a new framework for constructing adversarial data samples for deep reinforcement learning training. Some of them can generate data samples fairly different from the original data samples. 2.The authors do experiments with the proposed technique to check the generalizability on 10 atari games, and show that the generated data samples can fail the agent training to a different extent.<BRK>Summary:This work focuses on assessing the generalization of deep reinforcement learning by applying semantically meaningful perturbations to the RL agent’s observation system, e.g., brightness, blurring, contrats, shifting etc. The authors demonstrate strong degradation in the performance of RL agents in various Atari games, despite the semantic perturbations having significantly lower perceptual distance as opposed to planned adversarial attacks. Post rebuttal update  The reviewers have addressed my concerns for the most part and I am happy to update my score to recommend acceptance. The authors state that "Score_{min} is the score at the end of the episode achieved by the agent who takes the action that minimizes its Q(s, a) function in every state visited". Can the authors please comment on why they chose the definition of Impact as such and if this could be having potential impacts on the results shown throughout the paper? Direct policy based agents may not be amenable to the Daylight framework.<BRK>################################################Summary:Previous work on crafting attacks for deep reinforcement learning has relied on computing adversarial examples using knowledge of the environment, policy, and optimizer. Using Atari games and DDQN, this paper shows that simple image distortions, such as brightness changes, blurring, and rotations, often has greater impact on the agent s performance and is perceptually more similar to the original images. What happens if we combine several of these attacks? Technically it is not very sophisticated and the proposed attacks have been considered for image classification [1], but I believe that the results have strong practical implications.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 6. <BRK>In this paper, the authors aim at generating the right questions based on the textual answers and corresponding visual regions of interest (ROIs). Correspondingly, a simple method is designed to generate the noisy annotations of the ROIs using the pretrained Masked RCNN and the questions. This core part is mainly divided into two components: 1) aligning the object features with the answer word embeddings using attention mechanism; 2) constructing the object graphs and getting the GCN refined object features; 3) refining the question embeddings by attending the refined object features and image ones. Experimental results show that the proposed method outperforms the existing approaches significantly. Although the visual regions of interest can help to guide the question generation, the one to many mapping issues still exist when generating the visual regions of interest (no additional information is given), which may lead to the same problem in the question generation stage. Nevertheless, the proposed approach is still valuable. Although it helps little in mitigating the one to many issues, it helps to learn a better question generation network, as the additional region information can mitigate the issue during the training stage, which will help to avoid learning a generator that prefers to give general questions. The main novelty is leveraging the object features in generating questions. In addition, the graph construction part does not make sense to me, and no ablation study show that pruning the non hints objects can help to improve the performance. In addition, the confusing equations and poor writing of the paper cannot make this paper an unaccepted one:* In Eq.(2), \beta X^{a} seems like a matrix, which cannot be concatenated with a vector. * In Eq.(3), \beta_{j} should be \beta_{i, j}. * In Eq.(3), what I_{j} means? Why the image feature has a subscript? The product signs are different in (2) (3) and (4)* In Sec.3.4, it lacks detailed explanations of how the model will attend the image and graph to get a better representation of the question embedding between the two LSTMs, which is one of the core components of the proposed methods.<BRK>The model uses cross modal alignment between the object features, position features and answer hints to find the right subset of relevant visual hints to be used to generate the relevant question. The model also ensures that the latent space features capture the answer and position information by predicting them back from it. Informed by the visual hints, the object and image features are passed to a GCN network that is used to get the final hidden state which is passed to an attention and language lstm similar to BUTD model to generate the final question. Overall, the paper is strong and provides a solid foundation for the intuition and framework behind the model supported with detailed ablation analysis and case studies. What I find missing, is the actual test of how good these questions actually are by using them to train on the actual VQA task. Understanding that and how it performs as extra data on VQA 2.0 would give us a better understanding on how good the generation actually is where it matters. Please add citation for VQA 2.0Overall, I would like to recommend the paper for acceptance but it is hard to understand the actual value of this work without downstream application on the task of VQA 2.0. Edit after rebuttal: I have read the author response and I thank the authors for their valuable insights and answers to my questions.<BRK>Summary: The paper proposes a model for the task of Visual Question Generation (VQG) which uses the answer as well as object regions to generate the question. The paper models interactions between various visual entities and the answer tokens using a graph and then use it to generate the question. The proposed approach outperforms existing methods on the VQA and COCO QA task. While some of the proposed techniques borrows from existing works, they showed how to combine it to improve over exisitng methods. More importantly, what are the most critical components of the proposed method? For instance, the authors claim that the "amount of objects which are visual hints are much smaller than ones not" and "when humans ask questions from the image, we will infer whether the object is important clues by looking around in the image". The paper was hard to follow and it seems like there are a lot of moving parts. In general, this makes reproducing the paper and adopting ideas from the paper difficult.<BRK>Overview: This submission focuses on the problem of visual question generation and proposes to use two hints: answer and visual regions. After Rebuttal  The authors did a good job in the rebuttal. Most of my concerns have been addressed so I am happy to raise my score to 6. Strengths:+ The description of the method (model) is easy to follow. + Good ablation studies and human study. Weakness:  The motivation of VQG: I understand VQG is a recently raised task in vision and language research, but I can not get any ideas that why this is an important task, at least from this paper. This paper mentioned these two directions very briefly but failed to explain it well and there are no experiments presented in this paper to show whether their question generation model can benefit these two areas. I don t think a better BLEU score can show its potential since it only means this model can generate similar questions as the training data. I think this is more reasonable since it gives more freedom to the model to generate questions. And I don t think it is an issue that  one answer/image can be potentially mapped to many different questions . Instead, I think this provides a diversity of generated questions and further improves the VQA generalisation ability, from a data augmentation view. There might be two issues: at first, it seems not fair since it requires more annotation. Secondly, how can you get the answer during the inference? According to the visual hints, if I am right, from section 4.1, they are mined from the questions and answers directly, for both training and testing. This suggests you (authors) already got some info from the target questions (even in the testing!). The 3.2.1 is basically an attention mechanism? It decides the sparsity of the graph. In section 4.1, another \epsilon, which decides the quality of the visual hints.
Accept (Oral). rating score: 9. rating score: 7. rating score: 7. rating score: 7. <BRK>This article is concerned with convergence guarantees of online stochastic gradient descent for a rather generic class of three layers neural networks (instead of similar analyses that treated two layers). The main results state that in a proper limit of infinite width + vanishing learning rate, the dynamics of online SGD is proven to be tracked thanks a mean field description in the form of coupled ordinary differential equations. Once this mean field description at disposal, the main result is obtained: in the infinite width + vanishing learning rate + infinite time (  number of training samples), the generalization error tends to it minimal value for a broad class of models and losses (not necessarily convex, which is a novelty of the work) as well as generic data distribution. Overall this paper is very well written, enjoyable to read despite the technicality of the results, and understandable even for non specialists of this line of works (like myself). I did not check the appendices and proofs. Also I would find useful to have some hints about the meaning of the (trained third layer) hypothesis in Theorem 8. The authors may comment on that in the final version. I recommend publication. Even if I m not a specialist, it is obvious that the authors made a big effort of redaction, that the results are very solid, the proof technique seems original and requires less assumptions than previous works (I liked very much the "idea of proof" part).<BRK>The paper extends the recent studies and provides global convergence guarantees for an unregularized feedforward three layer NN. This is the first time global convergence is established for neural networks of more than two layers in the mean field regime. **Questions**:   In the paper, it is said several times that the convergence result “does not rely critically on convexity”. What do you mean by “critically”? You still assume the convexity. Maybe it would be better to add in the beginning, e.g.“the following network at time k”? I would say “W(k) consist of the weights …” instead of “W(k) is the weight with ….”   It perturbs me a bit that the difference between NN notations and MF is its boldness. Definition 2.<BRK>This paper studies the behavior of a 3 layer fully connected network when the width of the network is large. The authors define a mean field regime and prove that the behavior of the network under stochastic gradient descent converges to this mean field regime (for any finite time horizon). This transient regime is complemented by a long term analysis under quite restrictive assumptions (which imply essentially than the mean field regime always converge to the minimizer of the loss function). I did not check all details of the proof but the approach seems mathematically sound. Yet, as always, the devil being in the details and defining the right model and using the right notations is a difficult task. It uses almost the exact same notations and the same structure (overall paper and proofs). Also, if I can admit that the present paper is a resubmission of the arXiv paper, I do not understand why does the current paper focus on 3 layers and not the more general model of the arXiv paper.<BRK>This paper studies some theoretical properties of three layer neural networks (NNs) under the mean field (MF) regime. The authors proposed neuronal embedding in order to study large width neural networks. Then, the quantitative relation between finite width NN and the MF limit was clarified. The global convergence of the continuous limit of the stochastic gradient descent (SGD) was proved without assuming the convexity of the loss function. The problem considered in this paper is important. In order to prove the global convergence, the authors assumed the uniform approximation property rather than the convexity of the loss function. Though I m not very familiar with the MF regime, the high level idea to prove the theorem is well written. I read some proofs in the appendix, and I found that the description is accessible to a wide range of audiences. Some comments are shown below:  Neuronal ensemble is introduced to analyze the dynamics of the MF limit. The constant K appears in the upper bound in Theorem 3. What is the typical K in this case? In Theorem 8 and Corollary 10, the global convergence was proved.
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. rating score: 6. <BRK>This work constructs non asymptotic confidence intervals for off policy evaluation. I found this paper to be very well written and presented, with impressively thorough theoretical results and good empirical validation.<BRK>This paper proposes an approach to construct confidence intervals using finite samples for off policy evaluation. Please address and clarify these points above. It is also noted that the results do not only apply to independent data. The paper is well written.<BRK>The objective of this paper is to provide a method to produce tighter confidence intervals for off policy evaluation. However, it is unclear what problem this paper overcomes in previous methods to make this a substantial extension to the non asymptotic region.<BRK>**General overview**The paper studies an off policy evaluation (OPE) problem for Markov decision processes (MDPs). It suggests an optimization based method that can construct a non asymptotic confidence interval, for a given confidence level, for the value function of a policy starting from a fixed initial distribution. **Weaknesses of the paper**  The paper is obscurely written, for example, several objects are not precisely defined. Also, increasing this probability will make the resulting interval less tight. Mathematically, the Type I and II errors are traded off against each other.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 6. <BRK>I would have expected that for all i there is a j s.t. **Summary of evaluation**While the paper addresses a very important problem and contributes some interesting ideas and results, the submission has too many technical flaws and misses too many important details to merit publication in the current form. The assumption of a non factorising distribution over $\xi$ is not stated as part of the proposition. 3rd paragraph refers to $p_E(x,z)$ which is not defined. **Post rebuttal comments**I thank the authors for the detailed response.<BRK>This paper presents a novel and interesting method to formulate learning of correlated disentangled causal factors as a part of encode decoder framework using a Gan style learning approach.<BRK>This paper presents a latent variable model where the variables in the latent space are causally disentangled, i.e.the disentanglement is ensured according to a structural causal model (SCM). Is the DAG defined by Z not the same as the SCM defined by \xi? The second part is supervised and accounts for the causal disentanglement of the factors that are assumed to underlie the distribution; the authors claim the fewer supervised samples are required to estimate the second part of the loss alone.<BRK>The paper propose to learning causal disentangled representation which conforms human s cognition. The uses of this method is interesting. The paper has some advantages:It only requires part of sample is supervised, the previous methods need fully supervied information. It is not clear what is the meaning of target one and spurious one. The paper require the information about SCM as prior, which is a strong requirement.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 7. <BRK>The paper tackles the problem of recovering a probability distribution, which is supported in a low dimensional manifold. When the dimension of that manifold is full, the problem can be solved by the Normalizing Flow method. Since the dimension is not full, it is suggested to add a gaussian noise to the data points (this is equivalent to a certain convolution of the initial distribution function, ie equation 3). Certain mathematical aspects of the narrative are vague. Eg \tilde{X} is defined as X x Noise, but then in equations 4 and 5, it becomes a subset of the plane. It is only slightly stronger than "all the normal spaces have no intersections at all".<BRK>This paper proposes a method for estimating the probability deinsity distribution on a low dimensional manifold embedded in a high dimensional space using Normalizing Flow (NF). However, it is not realistic to find this at each coordinate x. This also argues that the noise variance $ \ sigma $ at that time should be set according to the inverse of the density. The gist of the manuscript is well written, and the issues it deals with are also important and interesting. I have some questions.<BRK>This work presents a novel theoretical development to tackle the problem of estimating normalising flows (NF) for data with support on complex manifolds. The idea proposed in the paper is interesting, and of potentially utility. I can understand that this aspect is related to the choice of the noise discussed in Section 3.3, but this issue still undermines the conclusion of Theorem 1. I would encourage the authors to clarify this point. In this case, the estimation of the noise parameter in formula (9) may be practically impossible.<BRK>In this paper, the authors address main limitations of Normalizing Flows (NFs) method for estimation of density functions on manifolds. It is shown that when D>>d, Gaussian noise is an excellent approximation of noise restricted in the normal direction and the experiments seem to confirm this. They provide theoretical guarantees on the variance and type of added noise that make the method work and illustrate with synthetic experiments. See below:Major issues: 	How this method can be used for a real problem with real datasets.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 6. <BRK>This paper proposes a new type of separable convolution to improve ConvNet efficiency. Based on a few assumptions (receptive field condition, channel condition, group conv condition), it mathematically calculate the “optimal” configurations for separable convolutions. Experiments are mostly done on CIFAR and ImageNet. For example, Eq (5) “channel condition” requires g1 * g2   C2, which doesn’t make sense to me: there is no intuition, and most existing convs doesn’t satisfy this assumption: (1) regular conv g1 g2 1 ! C2 doesn’t satisfy this; (2) spatial separable conv g1 g2 1 ! 2).Second, the CIFAR results show the new layers are not much better than others. As shown in Figure 3, the largest gain is <1%, and sometimes the o ResNet (~88%) is slightly worse than d ResNet (which indicates the propose layers might be not "optimal"?) The improvements on ImageNet in Table 4 seem to be promising, but as discussed in DARTS+ and other recent works, the search process of DARTS is often unstable and could potentially have high variance. As this paper is study separable convs, it should compare to separable conv based models like MobileNet/FBNet/EfficientNet, rather than the full conv based ResNet. I highly recommend the authors to conduct their experiments on these baselines. Instead of formulating it as a mathematically optimal solution based on unrealistic assumptions, I recommend the authors to conduct more empirical studies on these design choices. For example, the paper only shows the performance results of “optimal” (g1, g2) computed by equation (7), but it would be helpful to show the performance for different (g1, g2) values, and compare them with the “optimal” (g1, g2). 2).I recommend the authors to use the latest MobileNet or EfficientNet (or other separable conv based models) as baselines, and replace their separable convs with the proposed “optimal separable convs”, and compare the performance gains.<BRK>**Summary**This paper proposes a novel type of convolution called optimal separable convolution. Compared with existing separable convolutions like depth separable and spatial separable convolutions, the authors design a scheme to achieve an optimal separation. The volumetric RF condition requires that a properly decomposed separable convolution maintains the same volumetric RF as the original convolution before decomposition. Comprehensive mathematical proof seems reasonable. Ablation experimental results to show the effectiveness of their method. **Weaknesses**  The idea seems a little bit incremental in that it is a straightforward combination of group convolution and depthwise separable convolution. Some crucial ablation study/experimental results are missing. **Clarity**  The paper is well organized and easy to read. I understand the FLOPS results have shown the efficiency of the proposed optimal separable convolution, but it s still necessary to show the actual running times. The volumetric receptive field (RF) condition seems reasonable. However, the authors don t provide any ablation study on the volumetric RF condition. Say, what if removing this condition? This work seems an incremental version of group convolution and depthwise separable convolution. Could the authors give more discussion on this concern? The authors only provide experimental results on the classification task. It s interesting to see if this proposed convolution can be applied to other tasks, like segmentation. **After Rebuttal**I appreciate that the authors partly answered my questions and conducted experiments to show the runtime. After reading through their rebuttal and the other reviews, I will keep my original rating.<BRK>### SummaryThis paper proposes a novel analysis for optimal separable convolution considering the number of parameters and FLOPs. This paper provides a principled way of designing convolution to minimize FLOPs or parameter counts without resorting to black box optimization or algorithms of sorts. The key insight brought by the paper is to keep the volumetric receptive field constant, which seems reasonable for me. ### Strengths  A novel and principled approach to design separable convolution, which is of critical importance. Good empirical results by simply replacing old convolution operators with the proposed one. The analysis is easy to follow and generally agreeable to read. ### Weaknesses  It would be interesting to see results on wall clock time in addition to FLOPs and # Params. With that said, this is not a deal breaker. While it might be the case that the proposed convolution operator falls short compared to the existing ones given the hardware and software implementation we have now, I still think this work would be a great motivation for hardware and software research to look into. Again, this is good to have, but not a deal breaker from my perspective. ### Post rebuttalI appreciate the authors  efforts in conducting experiments to show the latency results. After reading through the rebuttal and the reviews from other reviewers, I would like to down grade my score by 1. Specifically, I agree with R3 and R4 that it would be better if experimental results are done for MobileNets/EfficientNets to empirically demonstrate the effectiveness of the optimal convolution. With that said, I agree with the authors that the DARTS experiments have shown that the optimal convolution can be better than depth wise separable convolutions. As a result, I still recommend acceptance for this paper.<BRK>## SummaryThe paper presents a new convolution structure which tries to achieve a better balance between the efficiency and accuracy. The proposed approach is well motivated and theoretically proved. ## Pros1.The paper is well presented and the motivation of the paper is clear. 2.The proposed convolution structure has theoretical small FLOPs and well justified based on the proof. 3.Reasonable experiments have been reported to valiate the the performance gain over the baselines. ## Cons1.Besides from the FLOPs, is it possible to provide the computational cost for the proposed algorithm, e.g., including the inference speed in the experiments like Table 3. Also, I would suggest to include the comparison with the baseline with depthwise convolution. ## Reasons for the ratingThe exploration of the structure of the convolution is challenging but important to the community. The discussion of the convolution based on balance of the efficiency and effectiveness is meaningful. Although the experiments do not cover all of my concerns, I would rate it as marginally above the acceptance threshold. Also, it would be better to report the baseline with depthwise convolution.
Reject. rating score: 3. rating score: 5. rating score: 5. rating score: 7. <BRK>Reject.SummaryThis paper works on the problem of create new recipes. "Note that although we also trained our models without the recipe restriction, " What does it mean by without recipe restrictions? Does a pair mean two ingredients or mean one ingredient set and one ingredient? This paper s writing can be greatly improved. Most of the techniques are not proposed in this paper. There is no related work section of position this paper in the literature. The authors can do a better literature Survey. A few example below  He, Xiangnan, et al."Neural collaborative filtering." Proceedings of the 1st workshop on deep learning for recommender systems. However, from figure 1, NMF is the best performing one on HR@10 on CulinaryDB. "At the embedding layer,this", add space after comma"In this task we"  > In this task, we"eventhough"  > even though"This result is some good " need to change. Questions"where the only concern is whether a user has interacted with an item and the system ..." Why this is a concern?<BRK>There are two variants of the model, first is to model latent relation between two ingredients, the second is to leverage external knowledge base and results from TransE to learn relational representations. The problem setting is interesting: using ingredient recommender systems to help chefs to create better or more creative ingredient combinations. I like the way the paper models the problem but there are several technical components are too important to miss:1. FREQ, PMI and TFIDF are simple rule based methods and NMF is the only commonly used method in recommender system but matrix factorization is too weak a baseline given that the proposed method is a neural network approach. This is also related to second point. Because the problem can be formulated as given a list of ingredients the recipe has interacted with, what is the next ingredient that this particular recipe is most likely to engage next? Here ingredient is the item and recipe is the user in sequential recommendation, so all the methods that have been developed for sequential recommendation can be used for this problem. 3.Many ablation studies are missing. Going over the entire paper, I am not entirely sure what components are most important to the good performance of the model. Many neural network design choices seem too arbitrary to me, e.g.why we need to add p and q in Figure 1. 4.Regarding qualitative study, I was just wondering if it is possible to ask crowd sourcing chefs to rate some newly created recipes for unseen recipes.<BRK>The paper studies a promising task of interpretable food ingredients recommendation   there has been a growing interest in modeling recipes. The idea of leveraging KG to improve the interpretability/faithfulness of recipe related ML tasks seems like a contribution to the community. In particular, the author proposes a method to learn pair specific relational representations for one to one (i.e.ingredient to ingredient) and many to one (ingredient set to ingredient) food pairing tasks. Pros:The task itself is an interesting application; meanwhile, the task is non trivial as the ingredient pairing is complicated and affected by various factors. It proposes a method based on the memory network. Most of the baseline models are non neural network based methods. The author proposes two new evaluation tasks to show the model s performance   both of them fall into a category of ingredients completion. There are more practical and challenging alternatives that could be used for better evaluation. For example, predict the complementary ingredients given the recipe name or recipe steps. Is there any reason? It may have a limited scope of the audience at ICLR as it s more like an industrial track application paper, though from the application perspective the paper may be impactful. Other top venues in the field of data mining and recommender systems seem like a better fit.<BRK>Overall, I vote for accepting. The authors propose a novel approach to support chefs with creating/experimenting with new recipes to overcome the challenging combinations of taste/texture etc. that can result from addition of new ingredients. The author’s proposed solution is well supported by the robust and detailed evaluation results. ######################################################################Pros:(1) The model architecture for both implicit and explicit use cases in the paper has a strong appeal with solid foundation. (3) The proposed model’s efficacy is well supported by empirical experimentation on two large food datasets and good comparisons with established baseline methods. Cons:(1) The readability of the paper can be improved. The authors have used a lot of prior work from different authors as a basis for many critical components of their architecture. The authors could have given a brief summary of previous work they are using as critical components of their solution.
Reject. rating score: 5. rating score: 5. rating score: 6. <BRK>Summary The paper presents a reinforcement learning approach to learn a routing policy for a family of Vehicle Routing Problems (VRPs). This is important and not clear. Action a_t in {V_c union V_R} is not mathematically correct with the definition of V_C and V_R. First RL based approach to tackle the multiple vehicle setting of VRPs3. How is that taken into account in the state s_t? 8.Sec 3.4 about the training should be more precise. Although the problem addressed is interesting, the paper is not well written and there are too many typos and missing explanations to understand the method.<BRK>This paper considers the problem of capacitated vehicle routing which is a famous combinatorial optimization problem that is known to be NP hard. The strengths of this paper are as follows. Having said that, the paper has a number of weakness in my opinion. I find this paper to be rather incremental since this is not the first paper to study RL algorithms for this problem. The paper gives two reasons, but I do not find it convincing enough. I would like the authors to expand/justify more along these lines. May be the paper brings to light some hard application that could lead to new algorithmic developments?<BRK>The edge values are considered a message that the source node sends to the target node. An end to end RL algorithm, Graph centric RL based Transferable Scheduler (GRLTS) is proposed to solve the capacitated multi vehicle VRP problem. What is the point of having those results in the main body of the paper?
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 6. <BRK>The paper proposes a teacher student framework to ensure fairness by letting the teacher choose examples for the student from either the training data or from a counterfactual distribution. The main contributions are a counterfactual generative model and an algorithm for learning the teacher policy. Strengths: 1. The idea seems interesting and the proposed teacher student framework is novel in the area of fair learning. More details needed on how Agarwal et al., 2018 was run for example. 3.The fact that authors were able to make the complex optimization work is itself a good thing since the objective has a lot of moving parts. 30  Agarwal seems to be more competitive than what is shown here. 6.Generally in experiments, cross validated results are needed. My basic question is regarding the motivation for such a framework. Why is this approach important? Isn t it possible to create Structural causal models that subsume conditional independences like DP and EO? Why is the teacher behavior of choosing real samples in the beginning and synthesized samples later justified? Post rebuttal Thanks to the  authors for  their detailed response to  my  questions. Some of the answers are indeed satisfactory, but some questions remain   such as extensive comparisons to other methods (probably using more datasets), how  the method would  behave (practically)  with  a different fairness measure like DP, and more  carefully situating the method in  the  fairness literature. In general the authors should provide a DAG which encodes their assumptions and justify them.<BRK>This paper combines counterfactual modeling with adversarial training for fair machine learning tasks. For a given fairness metric chosen from a variety of canonical examples, the method ensures fairness by augmenting the data with counterfactual examples during training. The approach has potential, which is best demonstrated on examples where the counterfactual data generation is interesting, like the CelebA data. I believe the main weakness of the paper is a low degree of novelty. For example, the idea of training with counterfactual data is present already in the Kusner et al.(2017) reference. The current paper expands the uses of that technique and combines it with an adversarial training architecture. So the strength of this work depends on the suitability of the counterfactual model and training architecture. The paper could be improved by replacing these with one or more examples that better leverage the strength of adversarial training. The method is "related" to all of them? Does it not fall into any of the categories?<BRK>The proposed approach involves learning a latent probability model that simulates the training data. A more "fair" classifier is trained on the manipulated data mixing with the "counterfactual" samples. I like the simplicity of the proposed method while future work is needed to explicate the theoretical condition under which it is sufficient. More specifically, the counterfactual fairness concerns with the potential outcome of prediction $\hat Y_{a }$ has the sensitive attribute $A$ been $a $. Post Rebuttal  I read other reviewers  comments and the authors  responses. However, I could also see why other reviewers are not particularly excited about it. However, many questions regarding the proposed methods are left unanswered, e.g., under which condition the proposed GAN approach is ensured to obtain unbiased counterfactual samples. With this being said, I think this paper could be most improved by further elaborating how it contributes to the existing causal inference literature, especially in computing counterfactual probabilities. Due to these reasons, I intend to keep my score but won t strongly champion for it.<BRK>The paper proposes to pair a GAN based model for generating counterfactual samples given protected attribute labels and a reinforced data sampler for choosing whether to let a model train on generated data or original data. The key to the method seems to be a reinforced data sampler, which picks, when to use a counterfactual sample versus the original sample. Given that the core of what is making this work is the data sampler, I wish there were more details. The paper says the reward was the fairness measure on the held out set. Do you mean a different held out set or the same final evaluation set? I am confused by Figure 6. So I would expect results very similar to all fake baseline, but in terms of EO, the results are very different. If only a sample number of samples are different than that baseline, how is this possible? Answering some of the above questions would help.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>Summary:The paper considers the Wasserstein Barycenter problems in the continuous setting. In particular, the authors propose an algorithm to compute the Wasserstein 2 barycenter when only samples from the marginals are accessible. Some theoretical analysis of this method is presented. Reasons for score:The proposed algorithm utilizes an interesting regularization of the dual formulation of Wasserstein 2 Barycenter, resulting in a single minimization problem instead of a min max problem. 2.The experiments are overall good and clear. 2.The paper is well written and easy to follow. Please comment on it.<BRK>Congruency is a property on the set of optimal potential functions that ties them together. What are the closest methods for barycenter that do not use potential functions? The paper proves that the optimal solution of this objective is the true potentials and thus no bias is introduced. The proposed approach is demonstrated on the tasks of generative modeling (2 256 dimensions), posterior inference, and color pallete barycenters (3D)**Strengths:**  Nice problem formulation and setup with respect to prior methods.<BRK>This paper proposes a method to scalably compute Wasserstein 2 barycenters given samples from input measures. Inspired by Li et al.(2020) the paper uses a potential based approach and recovers the barycenter by using gradients of the potentials as pushforward maps. In general, I feel this paper is well written and provides a fast solution to a meaningful problem, thereby supporting the claim of novelty. The theoretical developments in the paper are reasonable and the experiments carried out are quite decent, both in simulation and real data settings.<BRK>This work introduces a new Wasserstein 2 barycenter computation method. The authors first derive the dual formulation of the Wasserstein 2 barycenter problem, and then parametrize the convex potentials by ICNNs. Areas to improve:1. Therefore, it would be better if there is an experiment showing that how the conditions are satisfied. I therefore believe this paper should be accepted.
Accept (Poster). rating score: 7. rating score: 7. rating score: 5. <BRK>The provided experiments indicate that the method achieves good performance, although the gain is not consistent. The overall derivation and methodology of this paper are technically sound, and I guess discarding $S^\prime$ was a practical choice for learning time invariant representation. [Quality]The paper is clearly written overall. [Originality]The originality of the paper is not stellar, but sufficient for acceptance. [Significance]The significance of this work is mainly for model architecture.<BRK>This paper proposes a new framework for spatiotemporal disentanglement. In particular, it contributes to the disentanglement of content and dynamics using neural ODEs. I think it is a well written paper with interesting experimental design, especially the “swap” and “multi view” experiments that measure the degree of disentanglement. These experiments mainly show the general ability of the proposed model for video prediction rather than spatiotemporal disentanglement.<BRK>The authors present a generative model for videos where the latent trajectories have two components   a term without a slowness loss that represents "content" and a term with a slowness loss that represents "style". The results are generally good, especially for long roll outs, and they demonstrate something like disentangling by showing that the identities of the digits can be swapped in the moving MNIST data. My main objection with the paper is that it has nothing to do with PDEs or separation of variables.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 5. <BRK>The proposed MSNAS yields the better accuracy than its backbone FPN and other NAS methods on the COCO object detection benchmark dataset. I have a concern in technical novelty. The most contribution of this paper is very similar to FPN NAS in that it re designs FPN by choosing the optimal connections between all the modules. Furthermore, this search method does not retain the main claim of FPN, in which features at every level can be trained to have the same level of semantics. I also have a concern in its presentation. Algorithm 1 is very important for understanding how to implement the proposed method.<BRK>These are critical to make people use the proposed method. The proposed searching method is straightforward and easy to understand, and the performance compared to the baseline is OK. Q5: Thank you for considering. Figure 1 is good. #### Weaknesses  A clear limitation is the authors did not compare to other NAS based FPN as discussed in the related work section. In my opinion, the most important comparison is to BiFPN, which also has similar FLOPS as FPN and performs 2 4 mAP higher according to EfficientDet paper. The second major complaint is the scalability. Ideally, the author should show their method works on this large backbone to really push the state of the art. While the overall idea is straightforward, it also makes the paper lack excitement. In its current status, I suggest a rejection. #### After rebuttalQ1: Thanks for running the additional experiments.<BRK>It is also unclear how this setup would facilitate using multiple feature maps for one head. This paper brings new perspective for NAS on object detection task. Cons:  The proposed method for searching a path throw super net across multiple stride is not new. Could authors clarify? See comments below for more details. Agree with the statement and there has been quite a few work that tries to better align object scales with feature maps (like the papers mentioned above). The method proposed in this paper is based on one shot search and should be simpler than SpineNet. It would be great to have SpineNet in comparison (specially when SpineNet seem to show better performance)? Sec 3.2, Eq 3. Should it be $stride_{j+1} \ne 1$?<BRK>This method improves the performance based on several baselines. Cons:However, there are some concerns about this paper. 1.This paper seems like an improved version of the SpineNet. Searching for ﻿strides instead of permutation and utilizing a one shot method instead of reinforcement learning is not very novel. SpineNet improves the performance from 37 to 42.7 for ResNet50.
Reject. rating score: 3. rating score: 5. rating score: 5. <BRK>This paper presents an approach to deep subspace clustering based on minimizing the correntropy induced metric (CIM), with the goal of establishing when training should be stopped and generalizing to unseen data. The authors argue that these methods are tuned using ground truth labels, but even tuning with the KSS Cost (as in Lipor and Balzano 2020) results in better performance on the given datasets. A key benefit of CIM as a loss is that it does not decrease arbitrarily with training epochs, so it provides a means of estimating when training should cease without needing ground truth labels.<BRK>The authors propose to add a correntropy induced metric (CIM) loss term to improve the robustness of the self supervised convolutional subspace clustering network (${\rm S}^2$ConvSCN) to data corruption. For example:    1.1 As this paper is based on the comparison of [1], the differences in Figure 1 should be clearly stated. This is very important for evaluating the contributions of this paper. The reviewer believes that this paper is modified from [2], but the presentation flow is not carefully addressed. 2.The authors mention "unfair" comparison in the paper. I think a possible fair way to compare the performance of ${\rm S}^2$ConvSCN method and robust ${\rm S}^2$ConvSCN method on an "unseen" dataset. 5.For "fair" comparison, the author modified the method in [1].<BRK>This paper proposes the robust formulation of the self supervised convolutional subspace clustering network. In order to handle the data corruptions, the correntropy induced metric (CIM) of the error is embedded into the loss function. The major issue of this work is the limited novrlty. 3.Experimental comparison is not sufficient. However, there are many deep learning based clustering methods, they should be used for comparison.
Reject. rating score: 1. rating score: 2. rating score: 3. rating score: 4. <BRK>It’s not that authors do not know that Dropout and they cite the corresponding paper. Sorry if this whole review sounds rude, but to be honest I am amazed that authors thought about submitting a paper in this state to ICLR. Dropout is just used as a verb in place of pruning.<BRK>Why combine these two approaches and not any two other approaches? This doesn t seem to be explained in the text. Key aspects of the methodology (as mentioned in the notes below) are missing. Dropout is something different entirely.<BRK>As dissipating gradient dropout needs training for a couple of epochs, it s unfair to compare with random dropout. exist in 2.1.2 line 2 and the last line of 3.1  // the explanation of sparsity is colloquial in 2.1.3 2. As for SparseMatrices in equation 1, does it mean the explanation of K?<BRK>* Equation 3: it supposed to be a sum over the absolute values of the gradients? It is also a promising idea to prune early based on gradient info from the early epochs.
Reject. rating score: 5. rating score: 5. rating score: 7. rating score: 7. <BRK>This problem is tackled by adding a cubic spline smoothing component on top of ODE RNN to produce smooth and continuous hidden state/outputs. They derive a closed form solution for the cubic spline component based on the output of ODE RNN and obtain an error bound for 4th order derivable inputs. The paper focus on the task of learning from irregularly sampled data which is important in many domains. The key concern about the paper is the lack of rigorous experimentation to study the usefulness of the proposed method. The authors mention that this approach would be most useful in continuous time series interpolation for irregularly sampled time series , but there have been no experiments on real world irregularly sampled time series (e.g.PhysioNet). 2.Another concern I have is with the performance of Latent ODE baseline. 2.It would be interesting to see if the improved interpolation leads to improved performance in downstream tasks such as sequence classification.<BRK>The paper proposes to use cubic spline interpolation to smooth out discontinuities outputs and state inferred by ODE RNNs. 2) The premise that the jumps inside the ODE RNN are shortcomings or inconsistencies is erroneous. It is expected that their would be jumps with each new observation as the filtration with respect to which the latent state is conditioned changes then with a discontinuity. It would in fact be unexpected to have a continuous posterior. This is for instance what typically happens to stock market stock series any time an earnings report is issued. A comparison with a simpler smoothing method such as a convolutions, Fourier and wavelet denoising is also warranted.<BRK>The increase in performance is not guaranteed. This approach is also applied to correct a hidden state trajectory. Pros:+ The paper targets the main drawback of ODE RNN that both the output and the hidden state have a "jump" at the observation points. In standard CSSC, my concern is that the predictions at observation times \hat(t_k) are explicitly set to x_k, making the model inapplicable to noisy data in real world applications. + The authors provided the theoretical justification for their method and an interpolation error bound. The idea of correcting the trajectory or hidden state using cubic splines to make it continuous is novel and interesting. I think the paper is a valuable contribution to the domain of irregular time series. The paper does not provide the error bars for the quantitative results. It is quite restrictive. Was the same set of sampled points used for all the models? In the experiments, only a small fraction of the time points is used as input, and the model performance may change drastically based on the exact set of sampled points. They use cubic splines on the input data, which allows Neural ODE to query the data at any time point and produce a smooth hidden state trajectory.<BRK>This paper presented how cubic spline smoothing function was used to compensate the insufficiency in ODE RNN (ordinary differential equation recurrent neural network) for irregularly sampled sequences. The solution was evaluated in different tasks. Cons: This solution required the calculation of inverse matrix in a compensation term which would cause a significant increase in training time when the dimension is enlarged.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 7. <BRK>This paper studies the implicit acceleration of gradient flow for training a two layer linear model. after reading rebuttal  I do not agree with the author s response to my first comment. Moreover, the authors investigate the convergence of gradient flow with an arbitrary initialization and show its connection to Riccati differential equations as well as the explicit regularization. Compared with the one layer linear model, the authors show that gradient flow over an overparameterized two layer linear model may achieve a faster convergence rate, given a nice data spectrum and proper initialization. "Width provably matters in optimization for deep linear neural networks." From the presented theorems, I do not see whether the derived results have a dependency on the dimension of the matrix U or V. The authors may clearly specify why one needs to consider overparameterization linear models.<BRK>This paper considers the gradient flow dynamic for two player linear neural networks. It studies the convergence for gradient flow under both balanced or imbalanced linear networks, and with spectral or non spectral initialization. Compared with previous work, this work is the first to provide an explicit characterization of the gradient flow with respect to their eigenvalues. Experiment results suggest that such an implicit acceleration indeed exists. Is $Y$ in (3) the same as that in (2)? However, for the asymmetric case, the convergence rate is $e^{ t\sqrt{4\sigma_i^2 + \lambda_0^2}}$, which explicitly depends on the initial matrix $X_0$. Can the authors make more comments about that? The ‘acceleration’ compared with original gradient flow over $X$ suggests that by a matrix factorization, the convergence of eigenvalues varies may be accelerated according to the eigenvalues over the data matrix $Y$. Can the authors provide some examples?<BRK>Summary of review:This paper provides a detailed analysis of gradient flow in (over parametrized) two layer linear neural networks. The main results state the precise dynamics of gradient flow for both symmetric and asymmetric matrix factorization, starting from certain spectral initialization. Results(i) This paper focuses on the convergence of the gradient flow of U for minimizing the mean squared loss, starting from spectral initializations. For non spectral initializations, this paper observes that $U^TU   V^TV$ is still preserved during gradient flow, but this quantity now depends on how "balanced" the initializations of U, V are. However, I am not completely sold on this comparison. WritingOverall, this paper is well written and easy to follow. I do not quite understand the claims in Figure 2.<BRK>1.Paper Summary This work analyzes the implicit acceleration of gradient flow for over parameterized 2 layer networks (i.e.1 hidden layer networks) used for matrix factorization. Namely, the matrix Y has fully observed entries whereas in some prior works, Y is not completely observed. The connection between gradient flow and Riccati type differential equations is novel to the best of my knowledge and provides a simpler and clearer means of understanding implicit acceleration in over parameterized models than prior works. I believe the authors are missing some references to related work. (2) https://arxiv.org/abs/2003.06340   This work analyzes spectral initialization under gradient descent in deep linear networks (of arbitrary layer structure). I believe the main strength of the paper was in providing a well presented, rigorous, and novel analysis for understanding acceleration in over parameterized matrix factorization. As this is an important point, I feel that it could be emphasized a bit more.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 6. <BRK>This paper proposes a unified way to augment data in the latent (embedding) space. In particular, the paper combines three existing techniques including adversarial training, triplet loss, and joint training for data augmentation. Since augmentation is done in the latent space, the proposed technique can be applied to any modality including text, images, time series data. Empirical results show that the proposed method works better than the standard transformations on all modalities except image datasets. Paper also provides ablation studies to show the effectiveness of combining adversarial and triplet loss. To show the usefulness of this work, it s crucial to show when the proposed method is competitive to the existing strong DA baselines. Paper is using weak baseline methods for input space augmentations. The claim that MODALS s joint training for augmentation is different from previous approaches is not entirely true as the joint training for augmentation has been explored in past.<BRK>[Summary]This paper proposes a framework to apply automated augmentation in the latent space which is not restricted to any specific modality. The proposed method, called MODALS, follows the procedure of Population Based Augmentation approach and consists of two main parts: latent space transformation to generate harder examples and auxiliary adversarial loss &   triplet loss to improve augmentation quality. MODALS is evaluated on multiple datasets for text, tabular, time series and image modalities and achieves higher performance than competitors except on image data. + This paper is well organized and easy to follow. [Cons]  The proposed framework is interesting but somewhat incremental since most of the modules/losses are off the shelf. 1.It would be better to equip all the competitors with the triplet loss and conduct the evaluation, to show how the data transformations (Eq.(1 4)) contributes to the learning.<BRK>Comprehensive empirical study across multiple domains are presented. The paper demonstrates that the proposed method is particularly effective when the training data is limited. In this sense, the paper is tackling very important problem. The paper demonstrates consistent performance improvement over baselines across multiple problem domains. This could be a limitation of the proposed framework, as data augmentation methods become essential for semi supervised and self supervised learning methods. Figure 3 reminds me of center loss (Wen et al., A Discriminative Feature Learning Approach for Deep Face Recognition) and large margin softmax loss (Liu et al., Large Margin Softmax Loss for Convolutional Neural Networks), which pointed out the problem of softmax loss and presented fixes. It would be instructive to discuss some potential of these losses combined with latent space augmentation proposed in this paper.<BRK>The paper proposes an automatic data augmentation method that is modality agnostic by modifying the data in a latent space (rather than in input space). They design latent space interventions that yield hard examples (which they claim should improve downstream model learning). They apply population based training on the latent transformation policy search (which modifies the latent representation of a classification model directly), on top of training the classification model in question. The larger concern, however, is that it’s not clear what parts of the method yield good results. They also apply a triplet loss. The ablation study presented shows the baseline models with the losses added to them, where a more convincing study would show the final MODALS method, with each modification removed one by one. In particular, augmentation has proven to be a hard research direction in text and tabular data, so the fact that this method outperforms the baselines is exciting. I continue to think it s a strength that the method works in many domains, but this strength is slightly diminished due to the variability of domain performance (e.g.: image domain).
Reject. rating score: 1. rating score: 3. rating score: 4. rating score: 5. <BRK>As it has been not much focused on the relationship with optimizers, it is fresh and interesting. (2) The authors do not consider quantitative approaches such as compositionality [ICLR19] or compositional generalization [ICLR20]. In this paper, the main claim is very broad argument. Theoretically, they try to show that reducing loss values in the optimization process induces utilizing other input variables including useful information based on mutual information. Some steps in theoretical derivation seem to be wrong. I recommend ‘trivial and wrong’ for this paper. Pros:They deal with the relationship among compositionality, compositional generalization and gradient descent. Concerns: 	It is not clear the assumptions on models is covered in the main claim. What if a model is naïve Bayes classifier which assumes conditional independence? If the classifier is trained with gradient descent, the key argument of the paper has counterexamples, which becomes wrong. The proof in Proposition 2 seems not valid. The validity of this result is a factor that also affects subsequent verification.<BRK>This paper studies if gradient descent will affect the compositionality generalization. It attempts to prove the results by information theory and demonstrated several experimental results. Unfortunately, I think the proof has mistakes, and the conclusion doesn t hold. The major claim of the paper is that the gradient descent tries to use all available and redundant information from input. This is not true. As for random initialization case, simple experiments show that the neural network can learn the identity mapping with enough data under the MSE loss (this is obvious as linear regression is a convex problem..). The neural network will have good compositional generalization. The mistake might be in section 4.3. So, I think it s a clear rejection.<BRK>Summary: the paper investigates what neural networks learn when trained with gradient descent, in case parts of the inputs are only partially relevant to the output. The main claim is that GD is what prevents compositionality. In a set of synthetic experiments it is shown that indeed GD learns to use all information in the input, which results in poor generalization ood when only a subset of it was relevant. My main concerns are the following:1) It seems to me that compositionality is not really the main aspect of the paper and it is not being tested. The examples that the authors make earlier in the paper (e.g.shape and colour being entangled in the image), do not reflect the data used later in the experiments. For example, in the case of MNIST there seems to be only 1 factor that is relevant (the digit on the left side), and 1 factor that is spurious (the digit on the right). 2) The paper is purely of “descriptive” nature, i.e.not “prescriptive” at all. Minor:  By the time Figure 1 is mentioned, the caption mentioned “entangled”, but it’s unclear what that means in this context (and as a consequence it’s hard to interpret the figure). P6: “Y2 is chosen from {Y, Y + 1}” : do you mean $Y_i$ for both instead of $Y$?<BRK>This work analyzes the effect of gradient descent training on the compositionality of the learned model. Experiments are conducted on three simple benchmarks to demonstrate that when gradient descent trained model would use redundant information and not generalize compositionally. The theorem doesn t seem to add too much new information since the conclusion that gradient descent leverages redundant information seems quite straightforward given it is taking the partial derivative w.r.t to each input. It would perhaps help to highlight the *new* insights from the proofs. The experiments do show that the trained model is not able to neglect the redundant information, and the redundant information makes the training faster, but it doesn t support the claim that model architecture design couldn t help or achieve compostionality if trained by gradient descent.
Reject. rating score: 4. rating score: 6. rating score: 6. <BRK>The paper shows that acquiring the point maximizing the expected reduction in predictive uncertainty across all points is equivalent to maximizing the expected improvement (EI). However, the reviewer thinks this may be a wrong claim. The reviewer thinks that it may be better for this paper to position the contribution (abstract/introduction…) as the uncertainty based approach for active learning, then claim the minor/secondary contribution as showing the connection to the EI for the active learning setting.<BRK>Concerns:Overall, the novelty of the paper is limited. The method computes the expected reduction in the predictive variance across a representative set of points and selects the next data point to be queried from the same set. Pros:The method is rather simple and (up to some extent) computationally efficient.<BRK>The proposed approach is built upon the existing idea of selecting points that maximally reduce expected mean squared error (MSE) on a large representative sample of points. This idea is used for active learning in regression and classification problems with CNNs. Although the proposed approach is based on an existing idea, the application to CNNs specifically seems novel. There is limited description about how is the  representative sample chosen  in the experimental section. The writing of the paper can be improved.
Reject. rating score: 3. rating score: 5. rating score: 5. rating score: 6. rating score: 7. <BRK>Pros:The paper is well written and clear for the most part. Cons:I have two primary concerns about the paper and the proposed technique. 1.The positioning of the technique is not entirely clear to me. The difference here is that the authors find the model early in the training run, but it seems like the EarlyBERT procedure could be run once and the resulting model architecture could be saved and re trained like NAS models are. Firstly, the rough time estimates in Table 2 are very odd given the primary value of the proposed technique is to reduce training time. This problem shows up quite commonly in the model compression literature [1] and I’d encourage the authors to show full accuracy training time tradeoff curves so that the training time savings for a given accuracy can be more clearly established.<BRK>The authors present a method that builds on the lottery ticket hypothesis (LTH). Are they statistically significant? 2.Second paragraph of related work: McCarley et al.(2019) appears twice with different descriptions, is this intentional? For fine tuning, the authors run their model for 2.2 epochs, while their baseline model runs for 3 epochs, roughly 30% more which accounts for much of the reduction observed in Table 2. As the main contribution of this paper is the increased efficiency of the proposed approach, it must be clear how efficiency is measured. The authors say "the axes in the plots are the number of training steps finished." 7.Implementation details are only given for the vanilla BERT Are they similar to the EarlyBERT model as well? 8."Since we observe that the randomly pruned models do not competitive performance ...": how uncompetitive? I would have liked to see these results (also, please fix grammar in this sentence)9.<BRK>This paper proposes an approach to sparsifying BERT. #### ProsThe paper is well written and the presentation of the contribution is simple and well motivated. Since one of the main contributions of this work is to make progress on improving the training / inference speed of large transformers, the authors could spend more time going over how the “Time Saved” column is computed. Could we have uncertainty estimates for all results by reporting mean/std dev. * While a central argument is that most model distillation techniques still require expensive pre training, it would still be useful to include some of those results in Table 2 since EarlyBERT is comparable to those techniques for the purpose of Table 2. 3.Baselines: Experiments on pre training compare with no baselines. The phrase “structured sparsity’ is used in multiple places, but never defined.<BRK>The main contribution of this work is to use Early Bird Lottery Tickets to reduce pre training and fine tuning time for BERT. The usage of lottery tickets during the pre training phase is the biggest strength of the paper since it can result in significant computational savings. The authors perform interesting ablation studies but they could be augmented by a few more experiments (see below). It would be interesting to see how soon we can switch from the searching stage to the efficient training stage. * In Section 4.3, the authors discuss reducing training time by reducing training steps for EarlyBERT. I think a similar analysis for BERT would be helpful to understand if the training time can be reduced for the baseline model as well. Also, pre training techniques have tended to show improvements in downstreams tasks with longer pre training as the long as the pre training dataset is large enough. Overall, I find the approach interesting and the authors show computational savings in the pre training models.<BRK>More specifically, they adapt EarlyBird lottery tickets to the BERT setting in order to find winning configurations in early stages of training combine it with structured pruning methods to ensure the resulting network is more efficient to train. Experiments show that performance isn’t that much worse when EarlyBERT is used for fine tuning and for pre training. The goal of the paper is to find structured winning tickets for BERT in the early stages of training/fine tuning. Prasanna et al., (2020) are doing structured pruning too, but via an iterative pruning method. The distance metric is still Hadamard like in EarlyBird? Chen et al., (2020) showed that these make for better tickets that are performant on many of the downstream tasks. The work isn t terribly novel, but it s still interesting. It seems to not be that important, but would using separate values for attention and FC make a difference? 3.It would probably be helpful to expand the table/figure captions, make them a bit more detailed.
Accept (Poster). rating score: 9. rating score: 7. rating score: 6. rating score: 6. <BRK>##########################################################################Summary:Authors start from an assumption: “local negative sampling is the bottleneck of dense retrieval’s effectiveness”. To overcome this limitation, authors propose ANCE (Approximate nearest neighbour Negative Contrastive Estimation), a new contrastive representation learning mechanism for dense retrieval. The idea is that the model considers as negatives borderline cases. I believe that this research results are very important also in practice  ##########################################################################Pros:  1. The paper gives a nice theoretical justification for the reasons why they have to use hard negatives3. This aspect, in my opinion, is the weakest of the paper and it would deserve more attention by the authors.<BRK>The experimental results support that the proposed method outperforms several baselines. Similarly, “TREC 2019” is an important benchmark but it has not definitions related to the inputs and outputs. It will be good if the authors can address the above two issues in the rebuttal. The paper proposes a novel negative sampling method. Based on the method, the paper proposes a new dense text retrieval framework ANCE.<BRK>The paper explores how to effectively do negative sampling for dense retrieval. The paper shows that negatives sampled locally in batch are not informative, and proposes ANCE, a learning mechanism that selects hard training negatives globally from the entire corpus, using an asynchronously updated ANN index.<BRK>This paper studies the problem of representation learning for first stage retrieval in text ranking/matching. Specifically, it investigates the role of negative instances and how to select them in the quality of representations. The paper is well written and it does a fair job in motivating the problem and discussing related works. * Through various experiments, the authors show that by exploiting the top retrieved texts from the corpus, using the current learned representations, as  hard  negative instances, the performance of text ranking methods can be significantly improved. However, at many places it is pointed out that small losses prevent the model from learning and slow down the convergence rate. However, the arguments here are not quite rigorous.
Reject. rating score: 5. rating score: 5. rating score: 7. rating score: 8. <BRK>Post discussion update: The authors have clarified their work considerably, and I believe the work is probably correct. However, the paper still suffers from poor presentation and poorly motivated or justified modelling choices. The paper overall presents a good idea, but I believe the authors made poor modelling choices, which led the kludgy math. The paper proposes to combine deep kernel learning (DKL) with PDE/ODE prior knowledge for learning spatiotemporal systems. The following GP prior for g(x) is then placed on incorrect inputs and seems misguided or at least insufficient to model the differential. Second, the eq 6 states that a non linear differential of a GP is some other Gaussian process. p(0|g).This is clearly wrong, and the probabilistic model is then wrong as well. o The paragraph “to incorporate..” is difficult to follow since its technical but does not open up the math yet. It would help to write this in more conceptual wayo It would greatly help the reader understand the method to include the sup fig1 in the main papero It seems that both f and g are assumed to be separate GPs.<BRK>This work proposes a deep Gaussian Process (GP) framework for data modeling informed by dynamical systems. The analogous to the one of gradient matching, where a GP regression problem is penalised by constraints taking the form of differential equations acting on the GP itself. This study merges this framework to the scalable deep GP framework of Wilson et al, and proposes an approximated inference setting in which the problem can be optimised through stochastic variational inference. In this sense, the comparison with respect to the state of the art appears weak. The comparisons proposed in the study are with respect to either shallow models, or deep models not allowing the integration of physics informed contraints. In this sense, the feeling is that proposed methodology largely overlaps with these more recent studies, for example for what concerns the use of deep models and variational inference schemes. In particular, the idea of soft regularisation of deep GP models has been already explored in [2], and the authors may want to compare the method with respect to this approach. This aspect is overlooked in the paper, although it is quite relevant for interpretability purposes.<BRK>The authors  proposal is to incorporate physics knowledge into this process. ### EvaluationThe problem setting is clear. The proposed method is technically sound, and the experiments are enough convincing to show its superiority to baselines. I think this paper is a sound work. ### NoteI think a concern lies in the paper s clear violation of the formatting instruction of ICLR 2021. I delegate judgment on this regard to the chairs and for now, I decided the rating ignoring this matter.<BRK>The paper is clearly written, technically sound and innovative. ### Impact:The paper is bringing an important contribution in the domain of Physics Informed Machine Learning. It would be useful to provide more intuition and high level interpretation on the most technical aspects of Deep Kernel inference and on the usage of ELBO methods. That would go at the benefit of understanding for the non specialists of Kernel based learning who are interested in Physics Informed ML. The benchmark used for comparison are relevant. ### Applicability:It would be interesting to have a more high level interpretation and analysis of the applicability of the method to 3D simulation data.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 5. <BRK>This paper looks to investigate when VAEs fail to learn the maximum marginal likelihood (MML) model and some of the implications this can have for downstream tasks. Moreover, the experiments seem to demonstrate that the conditions cannot formally be demonstrated in practice as only very hand wavy explanations are given rather than concrete demonstrations. One important missing reference that should be added is  https://arxiv.org/abs/2006.10102 which already makes important related arguments about trade offs in M2 style semi supervised VAEs (in particular, they discuss the fact this type of semi supervision imposes a mismatch between the marginal posterior q(z) and the prior p(z), which closely relates to this paper s discussion of mismatch between p(z|x) and q(z|x) but without needing to assume a ground truth posterior). For example, various claims are made about the assumptions in Theorem 1 holding or not for different experiments, but the justifications for these are never really properly explained, let alone formally demonstrated. At the very least, it is certainly not the case that all the conclusions of the work have been fully demonstrated. Moreover, because it is obvious that optimizing the ELBO does produce the MML parameters if the encoder can match the posterior, we see that Theorem 1 provides very little insight other than this already well known fact. Too many of the experimental results are partially relegated to the appendices. For example, the abstract just talks about generic "pathologies" whereas the topic of the paper is about some quite specific issues rather than a general analysis of different VAE pathologies.<BRK>This paper exposes the pathologies of VAEs and characterizes them with concrete conditions. The authors also analyze the corresponding effects for specific downstream tasks and give insightful suggestions to avoid these problems. The trade off between the generative distribution and inference distribution in VAEs has been studied for a long time and has been revealed from several perspectives such as information theory, etc. It is good to see in this paper that the two conditions in theorem 1 summarize well why VAEs work well or poorly. In the paper, IWAE is applied to avoid the issues mentioned in VAEs. In general, this is an interesting paper that well analyzes the training issues in VAEs and provides insightful guidance to the VAE studies. The paper is well written (the figure labels are too small to read) and easy to follow. Update after the discussion stageI appreciate the authors  responses to address my questions in the experiments. However, I agree with the concerns of the other reviewers, especially the redundancy of Theorem 1 raised by Reviewer3.<BRK>Summary:The paper presents two analysis: (1) Characterization of when the training of VAEs using the ELBO leads to suboptimal generative models (biased towards ones with simple posteriors); and (2) How this suboptimality may affect downstream tasks that use the learned models. Pros:  The paper is very clearly written. I think this is quite relevant, and often does not receive as much attention as new training methods for VAEs. Cons:  As mentioned in the paper, the failure modes of VAEs are known. The paper s first contribution is a characterization of when they happen. While the theorems give precise conditions and expressions, it is not clear to me whether they are useful in practice for real scenarios. For instance, the analysis regarding disentangled representations states that given several models with equal likelihood, optimization will choose the one with lower KL divergence (which may be disentangled or not, depending on the scenario). Thus the relevance of this result is not clear to me. All in all, I do not find the first part of the analysis to be very relevant to the community. (I updated the score after the discussion.)<BRK>Furthermore, the paper examines the affect of VAE pathologies on downstream tasks a number of unsupervised and supervised downstream tasks. The paper is clearly written and the authors have attempted cover all cases of the theoretical reasons they have identified for VAE failures with corresponding experiments. The first case is presented as occuring when the true posterior is difficult to estimate and there is no good likelhood function with a simple posterior. This brings me to my main concern: Theorem 1 seems to say that if 1) the model cannot match the true posterior (Gaussian assumption is false) and 2) the model cannot match p(x) without matching the true posterior (decoder is not too powerful) then the model will not match p(x). If this is the case, then I would suggest that the statement does not present any new insight into VAE failures since a good VAE model of p(x) would either match the true simple posterior or it will model p(x) without matching the posterior. In particular, they should state whether or not the case of posterior collapse is handled by their characterization. The authors also present theorem 2 which suggests that using the ELBO to choose output variance results in a biased estimate.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 4. <BRK>This paper tackles the problem of long horizon visual planning, with the aim of of being able to plan actions to reach distant goals. Pros:The problem of long horizon visual planning to goal images is important, and the paper does a good job of motivating it and describing the prior work in the area. Cons:My main criticisms are on (1) the generality of the proposed planning procedure/representation learning, and (2) the thoroughness of the experimental comparisons. The idea of building a separate graph for each of the i one hot encodings, and planning along a single one hot encoding at a time seems very specific to the domains used for evaluation. First, there are many relevant works which the paper cites, but does not compare to. Also as mentioned above, the experimental domains are visually simple, and designed in a way which is very particular to this method.<BRK>This paper presents a method that combines learning discrete representations together with planning using graph search to solve long horizon tasks from vision. Next, these representations are combined together with an abstract planner to generate a sequence of waypoints to the goal. Pros:1.This paper tackles an important problem of representation learning for long horizon planning tasks. It proposes a standard bi level architecture of a planner and low level controller but it nicely brings together different ideas from prior work such as the visual foresight based low level controller and graph search based high level planner. I would suggest a weak reject. 1.The approach is only tested on two visually simple 2D scenes where the task of learning a discrete object centric representation is quite straightforward. It would be useful to have a discussion in the paper in this regard.<BRK>The paper presents learning discrete encoding (as one hot vectors) given a scene image to represent various semantics about the scene with an aim to perform graph search over these representations in performing high level tasks. The research direction is quite relevant and the motivation make a good case for it. Related work is adequately discussed. The two main environments studied are game like and too simplistic. More task planning baselines (particularly non learning based) should be considered. What if a goal image is not available as the target but the task is provided by other means? Learning  low level control  is referenced several time in the paper but the experiments have no mention of it or are designed to explore/study this.<BRK>This representation can then be planned over through a sequence of small alterations to the discrete embedding, which are then executed via MPC. DORB is demonstrated to solve long horizon tasks and learn representations that consider objects and their properties. The paper is clear, though there are several typos on the paper, including the two words of the intro (“In the future”). The work is interesting, particularly the efficient search and graphical representation of a state space to plan high dimensional and long horizon problems. Such approaches are quite promising and the approach is well founded. How often are one hot’s not represented in the data? 1) The primary contribution is the framework for learning this discrete embedding and planning over it, but the need for a discrete embedding is never fully considered.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. rating score: 4. <BRK>It is shown that this model successfullyrecovers the hierarchies underlying the data on two newly proposed hierarchical variants of theSprites and CLEVR datasets. The paper is well written, and despite the considerable complexity of the method, its    presentation is relatively easy to follow. Both quantitative and qualitative evaluations make it very clear that the model has    learned to infer the correct scene graphs as desired. Its ability to infer the appearance of    occluded parts is especially impressive. 3.While there are no direct competitors on this newly defined task, the paper does a decent job of    comparing to the closest available baseline, showing how the additional structure can be    beneficial. One may argue that the datasets have been deliberately constructed to showcase the model. While    that is probably true, I think this is a valid approach given the novel nature of the task and    the lack of supervision. More complicated structures would    likely require supervision. Reporting results for multiple different datasets with randomly    chosen object types would be somewhat more convincing. 4.As is common for unsupervised scene models, the proposed method likely only works on synthetic    images in its current state. Overall, the paper presents an effective new method for the task it sets out to solve. While it isquestionable how it would work on real world data, I believe the paper is of sufficient interest asa proof of concept, and am therefore leaning towards acceptance. It is stated that auxiliary KL terms are added, with the sparseness constraint on $z^{pres}$    being one of them. But is not clear if there are others. This would be important to know in    order to evaluate how strong the model s inductive biases are. 2.The downstream task used for Fig.5 is not clear to me. If only distinct parts are counted,    how is equality of parts defined on the dataset?<BRK>The experiments are done in two image datasets of single color, simple shape 2D/3D objects: Multi dSprites and CLEVR, and the model is able to discover objects without supervision. **Strength**: I find the direction important, and the method well established in a variational inference framework with graphics inspired designs. **Weakness**: Perhaps my biggest concern is that the current datasets are a bit weak. I would expect hierarchal structure like an object being human body, and parts being legs, arms, head and so on. But in this work a "part" is a single color object, and an “object” is a bunch of single color adjacent objects. Based on these two points, I believe the paper will be much stronger with experiments on more complex objects like humans or tables or Another concern is the application of learned scene graph. For example, I d expect scene graphs to be used for image manipulation, as one can change part of the object (shape, color, pose) without changing the rest. Finally, I wonder how variational the learned scene graphs can be, as the objects in the datasets are fairly simple and the learning might be easy. I d be happy to see some analysis but this is not my main concern. As the authors said, the paper is a proof of concept of unsupervised hierarchal scene graph learning, and the rebuttal to some degree reassured me. Of course experiments are still toy from computer vision perspective, but I m now okay with acceptance.<BRK>The model is trained using variational inference. Experiments are performed on two new datasets (2D Shapes and Compositional CLEVR), demonstrating that the model is able to successfully uncover recursive scene/object/part decompositions in an unsupervised setting. **STRENGTHS**  The paper presents a novel generative model that can infer tree structured latent variables  The method is technically impressive, and a clear improvement over the non hierarchical modeling used in prior work  The paper presents two new datasets (2D Shapes and Compositional CLEVR) for studying hierarchical scene decomposition. I hope these can be publicly released! **WEAKNESSES**  The method is quite complex, and though it is technically impressive I wish that it had been compared against very simple baselines  No experiments on real world data  Unclear how the model will scale to wider and deeper trees  Many implementation details are unclear**SCALABILITY**A big selling point of the proposed model is that it can model the hierarchy of scenes into objects and parts, and the tree based formulation of the latent space used to achieve this is technically impressive. All experiments used a relatively small three level hierarchy where all nodes have a fixed out degree of 4; thus in all experiments the full tree has just 21 nodes total. In the experiments, these hyperparameters are perfectly matched to the synthetic datasets: you use a two level tree with out degree of four, and each image has between 1 and 4 objects, each of which is composed of between 1 and 3 parts. However in more complex real world scenarios, you may not have such detailed knowledge of the world’s compositional structure. For this reason, I’m curious as to how the method would behave when the structural hyperparameters are mismatched to the underlying statistics of the dataset. **SIMPLE BASELINES**The model is evaluated on two synthetic datasets   2D Shapes, and Compositional CLEVR. How difficult is the scene graph inference problem on these datasets? However in this case, I’m not convinced that this complex variational method would be any simpler to implement or tune, or even give better results than, a very simple “handcrafted” baseline like the above on these synthetic datasets. There are also no experiments to demonstrate its scalability to real world datasets. This leads to a pointed question: If someone wanted to infer scene graphs from images, why should they prefer your approach over a very simple “handcrafted” approach? Even if some of these details do make sense as part of the main text, they should be specified more explicitly in the supplementary material. This is not clear from Equations 3 or 4, nor the surrounding discussion. However on the whole I’m not sure whether the complexity of the method is actually necessary to solve the problem at hand; I wish that the authors had done a better job demonstrating the benefits of the proposed method over very simple baselines. On the whole I lean slightly toward acceptance, but I hope the authors can address my concerns in their rebuttal. I am pleased to see the additional experimental results provided by the authors; I think that these do improve the paper. I still feel that some well tuned handcrafted approach could likely perform on par with the results of the proposed method, but the comparison with [Wei et al] show that achieving such results is at least not trivial, which does help to better ground the complexity of the task.<BRK>While my overall opinion of the work is slightly more positive post rebuttal, I still maintain that this is a clear reject, primarily for the following reason:  The technical contribution (adding a hierarchical layer to SPAIR and demonstrate the hierarchy can also be learned without supervision) is not significant enough to accept purely based on a "proof of concept" of a "new" direction. The proposed method is evaluated on two datasets created for this paper. But for now, I tend to give a pretty clear reject. Strengths   The motivation of the need for hierarchy is solid, and the solution proposed seems to me to be a reasonable way to impose some sort of hierarchy. Following previous point: would like to see examples of the same learned object being used in multiple scenes. Good performance for the evaluations chosen in the paper. I am not too familiar with the AIR line of work, but I think quite a few ideas can be traced back to prior works. It would be much better if the authors can, in addition to a brief one sentence mention in related works, add clear discussions for the inspirations of the main design choices. The authors claim that “other works can’t work on our dataset”, but I think the burden of the proof is on the authors to show that their method is superior, even under a more specific setting. In other words, if the method is indeed “general” and can learn good decompositions, then I would expect it to perform better even under an slightly unfair setting i.e.comparing against metrics/datasets adopted in other works. Furthermore, the datasets used in this paper appears to be way too simple as compared to real world data. The authors argue that dataset with a single shape is easier, but I disagree: datasets like partnet contains much more complicated part structures, as well as joints between parts, than what is used here, even with only a single shape. “GSGN is a general framework for representing and inferring scene graphs of arbitrary depth”: I don’t think a model being able to work a toy setting with three levels will mean that the same framework can be used for more complex settings of arbitrary depth (as an analogy: MLP works for MNIST but not on ImageNet). 3.“First deep generative model for unsupervised scene graph discovery”: there are a lot of works that infer structures in an unsupervised way, I don’t think it’s fair to give a very narrow definition of “scene graph” and claim “the first”.
Accept (Spotlight). rating score: 7. rating score: 7. rating score: 7. <BRK>The authors provide convergence and generalization properties of the algorithm, and demonstrate its improvement of group and individual fairness metrics in several numerical experiments. The paper is well established and written. It seems that this method can be readily extended to multi class classification problems.<BRK>The authors presented in the submission a thorough study on enforcing the aggregated individual fairness with non differentiable ML models. The paper is highly completed, well structured (though a bit dense given the page limit) and well written   a clear accept. Though stated by the authors that "our method readily extends to other supervised learning setting", the theoretical discussion and the empirical study covered in the paper are both based on solving a binary classification problem. However such understanding seems problematic and inaccurate throughout the rest of the paper.<BRK>Also thanks for the running time numbers. **********The paper presents an interesting idea to train boosted decision trees to satisfy individual fairness constraints (for a pre specified similarity metric "d_x"). Cons:  The main concern is the scalability of the approach.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 5. <BRK>This paper is more like a review of singular learning theory and its implication on deep learning. Then, a series of topics for deep learning, such as flatness and generalization, are studied within the framework singular learning theory, with a combination of theoretical analysis and numerical experiments. The paper is clearly written and well organized. pro:The authors point out that the study of deep learning should be put into the framework of singular learning theory. con:It seems most results in the paper are illustration or clarification of existing results.<BRK>In order to make a claim about deep learning, (say the bayes predictive error is superior to MAP or MLE) there should be either an extensive experimental demonstration of the claim (not two experiments) with proper ablation of when the claim fails (not to mention that the std of the experimental results puts all the claims to question). I would like to qualify by saying that statistics is not my main field of study and I would be happy to receive clarifications if I misunderstood anything. +++++++While I enjoyed the primer to singular learning theory, I found the paper s contribution marginal given the related work.<BRK>The paper discusses the singular learning theory approach of Sumio Watanabe and argues for more exploration of this theory for understanding generalization performance of deep networks. There is a large amount of existing work on singular learning theory. I agree with the paper that the approach is a promising direction to understand generalization in deep learning. 2.The paper would benefit from Appendix A.1 4 to the main text. How does this explain the fact that one sample from the posterior distribution also results in good generalization for deep networks? 2.Singular Learning Theory is not new.<BRK>The paper is a terse account of singularity of deep learning with a probabilistic view. The main byproduct of singularity is the inapplicability of classical methods. This is no news to many people in the field, yet I find the perspective provided in this work fresh and I think it has potential for further developments, although its current applicability is limited and it doesn’t say something that was not already known. Especially section 3 would benefit a lot from such clarity and would help a wider audience to follow the work. The end of section 3 contains the key idea and would benefit from further clarity. What does it imply for existing models (examples)?
Accept (Poster). rating score: 8. rating score: 7. rating score: 5. <BRK>Is that what is happening? [/EDIT]##########################################################################Summary: this paper extends the lottery ticket hypothesis to life long learning. The paper proposes a top down and bottom up approach to network pruning and shows that the bottom up pruning reaches SOTA performance on several datasets while reducing the network size to a few percent of the full model size. The paper also shows higher performance against SOTA for class incremental learning. However, I am interested in the opinion of the authors and other reviewers on these questions. However, for this particular paper, I would tend to think that moving it up in the paper (close to the beginning) could make sense too.<BRK>##########################################################################Summary: The paper provides an interesting extension of the lottery ticket hypothesis in the lifelong learning setup, showing the existence of these tickets for class incremental learning. The paper also explores top down and bottom up tickets. The authors performed experiments on CIFAR10,CIFAR100, and Tiny ImageNet datasets showing the effectiveness of the proposed ticket strategy. sometimes not, will be good to fix it. My major concern is about the clarity of the paper and some additional experiments (see cons below). Overcoming catastrophic forgetting in neural networks. There is a lot of work that has been done in continual learning.<BRK>The research question of this paper is the existence of an extremely sparse network with an initial weight assignment that can be trained online to perform multiple tasks to compete with a dense network, in a lifelong continual learning configuration. The network considered by the authors has a common base for all models and a head for individual tasks. While the topic that combines lifelong learning and network sparsity is definitely interesting, the development of this paper is incremental,  and there lacks some theoretical justification on why introducing a new task will both keep the network sparse and reuse weights of the previous networks. + The proposed schema works, as shown in the experiments. Cons:  There needs work to satisfactorily define the new lifelong winning ticket framework.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 6. <BRK>Summary:The authors proposed a WAE based algorithm for outlier detection, aiming at mapping outliers to a low probability region and inliers to a high probability region. Experiments were performed to show the effectiveness of the proposed algorithm. This is not true. .In experiment, the authors compared with benchmarks using F1 score. Another commonly used metric in outlier detection is AUROC.<BRK>Autoencoders suffer from the issue of assigning unusual or outlying samples to regions in the latent space which contain a large amount of nominal samples: i.e.unusual samples are not separated from more normal looking samples. Overall this paper paper suffers from two main problems:1. This is further exacerbated by the fact that standard blackbox "classic" outlier detection methods are missing (e.g.iForest) and details are missing about other classic methods, e.g.what is the kernel for the OC SVM. Lack of novelty.<BRK>In other words, the proposed solution combines several existing approaches to tackle the problem and address the challenges. ##Strong Points(1) The proposed method is technically correct##Weak Points(1) The level of novelty is not high(2) The proposed method does not work in end to end manner. More precisely, the main focus is on the input in an appropriate format for traditional outlier detection method.<BRK>The main idea of the paper is to ensure that outlier points are mapped to areas distant from the inliers in the embedding space. The experiments demonstrate that the proposed method works very well and better than other methods. Combined with a normal/multivariate prior distribution, this then enables the use of simple distance based outlier detection methods. However, this does not guarantee that the embedding being learned will actually do so.
Reject. rating score: 3. rating score: 6. rating score: 7. rating score: 8. <BRK>##########################################################################Summary:Instead of back propagation, the authors consider a randomized search heuristic to train the parameters of neural networks. The proposed work is based on the hypothesis that the initial set of neural network weights is close to the final solution. The authors identify the problem that existing randomized search methods update all the parameters of the network in each update. Thus the proposed method updates only a single weight per iteration. Experimental results on MNIST and CIFAR10 show that the proposed method delivers competitive results. 3.This paper provides experiments on well known benchmark data sets. The results suggest that randomized search heuristics can work well for training the weights of deep neural networks. ##########################################################################WEAK 1. Highly relevant theoretical work in this field is not referenced or discussed, e.g., Nesterov s "Efficiency of coordinate descent methods on huge scale optimization problems" or work on randomized search in general, like "Evolution strategies   A comprehensive introduction" by Hans Georg Beyer and Hans Paul Schwefel. To make this more clear: it is well known that randomized search heuristics work very well on a wide variety of optimization problems (both, combinatorial and numerical). What is the expected number of objective function evaluations? For me, answering any of these questions would make the paper at hand acceptable. 3.The formal presentation regarding classic an recent results can be improved (see below). This impreciseness in statements appears for recent works as well: On page 3, the authors state that "weight agnostic neural networks (WANN) also searches for architectures, but keeps the set of weights fixed.". WANNs evaluate the expected performance of a model over various parameters which are shared by all connections.<BRK>The paper proposes an RSO (random search optimization) method for training deep neural networks. In particular, it adds a perturbation to weight in a deep neural network and tests if it reduces the loss on a mini batch: the weight is updated if this reduces the loss, and retained otherwise. Merits of the paper: + This paper shows that repeating the RSO process a few times for each weight is sufficient to train a deep neural network. As a result, the number of weight updates is an order of magnitude lesser when compared to backpropagation with SGD. + It can make aggressive weight updates in each step as there is no concept of learning rate. The weight update step for individual layers is also not coupled with the magnitude of the loss. + RSO is evaluated on classification tasks on MNIST and CIFAR 10 datasets where it achieves competitive accuracies. The current figures only show the comparison in terms of training iterations. It would be interesting to see the performance of RSO on ImageNet and/or COCO. I have read the response, and the rating is not changed.<BRK>In this paper, rather than training a DNN using SGD, the proposed idea is to perturb the weights of the network and accept the perturbation if it improves the performance. There are at least two things that I would like to see fixed:  Markov Chain Monte Carlo is mentioned in the abstract and never discussed again. In fact, a comparison in terms of cycles would be useful, but using the same update schedule for SGD that you use for RSO (which of course would make SGD, the competing approach, take a higher computational cost per cycle). Right now you are stating that RSO is an order of magnitude faster... when measured in a unit (cycles) that is much more costly for RSO. b)  Given that a) is done, it would also be useful to show the current version of  accuracy vs. cycle time if you consider the two potential versions of SGD cycles: parallel updates and sequential updates. That might make RSO not seem as good compared with SGD but would be much more useful to judge when RSO is to be preferred to SGD. They are properly backed up by citations, unlike the comment of "getting stuck for sure in a local optimum", which is very much dependent on the optimization problem. If the paper is accepted, I d like it to include the original statement, which can be very informative for some readers.<BRK>This paper discusses a possible method for training a deep neural network without using backpropagation. The motivation for finding suitable replacements or approximations is to reduce the computational complexity of training neural networks. These kinds of methods have advantages on better utilizing memory bandwidth, making cheaper hardware more relevant to the training side of NNs. The main result of the paper is that a small number of sequential weight updates using the authors  proposed algorithm rivals the performance of an established method like backpropagation. Nearly every question I asked myself while reading it was answered in a subsequent section. Suggestions:Section 3: what is the motivation for using a Gaussian distribution to initialize weights? Not that I see anything wrong with that, but is there some reason this might be better or worse than other initializations? Section 3: “We first update the weights of the layer closest…”. This could be an area of additional research as to where to update first. Section 4.1: It could strengthen the paper to include some analysis on the number of MAC operations required and the number of reads/writes to memory for SGD vs RSO.
Reject. rating score: 4. rating score: 5. rating score: 7. rating score: 7. <BRK>**Paper summary**The main goal of this paper is to show that LSTM units in a network trained to solve a T maze task, show similar activity patterns as neurons in rats solving a similar task. For example, I would find it interesting to show that the forward sweeping observations done in rats, which is often interpreted as a signature of planning or prediction of the consequences of future actions, arises simply from a next step prediction loss in an overtrained rat. Minor concerns:   “As such, a network of Gated Recurrent Units [...] or vanilla RNN units was unable to perform well in either the pre training or joint RL task due to these prevalent long term dependencies.”  How many steps are there between cue and choice, and between choice and reward? **Cons**I have concerns about key points of the paper and the interpretation of the results:1. Because the network was trained with a predictive loss on two very specific sequences of observations, it seems plausible that it is robust to the change of input statistics and follows the same sequence, maybe with some instabilities. In the network trained also by RL in particular, the authors interpret this as “The agent appears to be sampling the trajectory concerning the alternate return arm of the maze before ultimately settling on the rewarding return arm”. But this is, in my opinion, an over interpretation, as the agent has no sampling capability in the first place (there is no generative model of observations), nor any particular planning mechanism. Alternatively, the authors may be claiming that this jumping behavior happens only after the RL training and not before, in which case they should emphasize this difference and quantify it explicitly. Although in this case, a simple explanation for this could be that due to the epsilon greedy, only during the RL training the network is exposed to the wrong cue arm combination. However, in my opinion, this is problematic for their analysis. This is more similar to a demonstration than to pre training.<BRK>Summary: The authors trained a recurrent network to perform a sensory prediction task and this gave rise to units that resembled hippocampal place fields. The work is interesting, but the clarity of the paper is not at the level of the findings. Cons:1.While explaining the task the authors claim that the colours are chosen at random, however it is not clear whether these stay fixed across episodes or they changed. However, in the paper there are no details about the number of steps or how the discount affect the results. Does it mean that the loss_{rgb} is also fine tuned while training with the Q learning objective? This point need clarification. 4.How the threshold for place cells are defined? This needs further analysis to support the decision, which otherwise seem very arbitrary. This is an important point as I think the results will be more powerful with the same image fed as input, or even better with no image, just with 0ed input to simulate pondering. Can you please explain.<BRK>In particular, these LSTM networks demonstrate both metric representations of their environment and nonlocal extrafield firing at decision points along the maze (anticipating the future trajectory of the agent). + I appreciate that no velocity input is given to their model, in contrast to prior approaches. + I also liked the qualitative comparisons to hippocampal recordings from rodents trained on the same task (especially Figures 6 and 7). Weaknesses:  The primary conclusion, namely, training an RNN on a maze like environment gives you place cells, is really not all that new, especially considering that the network is still supervised to predict position and landmarks. Does their approach explain more variance in these neurons than prior approaches? Question: The authors mention that Q learning performs poorly on tasks in dynamic environments – however, I do not see any evidence of this in the paper, it would be imperative to show this explicitly for the environments they consider. As it stands, I think the ideas of this paper are interesting and think it unifies prior approaches, but I do not think the conclusions from the modeling add all that much novel insight from prior approaches. Therefore, I recommend a weak accept.<BRK>In this paper, the authors train a recurrent neural network on a navigation task, and observe the emergence of several phenomena reminiscent of the hippocampus: appearance of place cells with a secondary receptive field at task relevant locations; anticipation of possible future paths in the activity of the model, with alternation in time between possible future paths; a high proportion of neurons tuned to task variables rather than animal trajectory. Strong points:  these findings are compelling, they account for some hallmark properties of the activity of hippocampus, and they could lead to a better understanding of the role and function of the hippocampus. I recommend to accept this paper because of its strengths listed above. Why were these required in addition to the primary cue point and choice point? Additional feedback:1) It would be interesting to see a discussion on what we learned about the function of the hippocampus from this model, and/or what predictions this model makes about neural activity in hippocampus. 4) The first sentence of the abstract is difficult to understand.
Reject. rating score: 3. rating score: 5. rating score: 5. rating score: 5. <BRK>The paper proposes a method for unsupervised image translation between unpaired domains of images. Overall I have the following concerns about the paper:The motivation for this architecture is unclear. The introduction motivates this model with fractals and iterated function spaces, but that seems to have nothing to do with the types of applications shown here. No comparison is provided to the state of the art in unpaired image translation: Contrastive Learning for Unpaired Image to Image TranslationTaesung Park, Alexei Efros, Richard Zhang, Jun Yan ZhuECCV 2020Visually, the results are not convincing.<BRK>This approach helps to decrease the number of weights, and, if done properly, does not harm quality. The results in deblurring/denoising look rather interesting. All in all, to my mind there is great room for improvement for your submission to demonstrate the real power of PoL. Could you provide a comparison of the inference speed (FPS or FLOPS or any other measure) between CycleGAN/NiceGAN and PoL?<BRK>I think it would be more appropriate to put the results obtained along the iterations of the recurrent block in the main text instead of in the appendix. Pros: 1) Compared with the vanilla CycleGAN, the proposed PoL has significantly fewer parameters and similar performance. Besides, CycleGAN is not a good competitor for image restoration tasks (debluring, denoising, etc.), so the potential of the proposed PoL is questionable.<BRK>This paper presents an approach for image to image translation by introducing extra layers into the generator, which can be trained in an unsupervised way. The paper is generally easy to follow. I have the following concerns: the novelty is quite marginal since the backbone network and the training process are well developed before and the technique of employing more layers to the generator seems like simply extending the network.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 5. rating score: 7. <BRK>This paper proposed to use both fine and coarse grained tokenizations of text to train large language models like BERT. The method is relatively straightforward. The input is tokenized at different two granularities (words and phrases for English; characters and words for Chinese). Each type of tokenized text is passed through BERT layers with shared parameters to generate contextual representations. Is it not using the BERT WordPiece tokenizer ? Also, if it is not using BERT tokenizer, I am very surprised to see it perform better than Google BERT in all cases. Since Google BERT uses a more fine grained tokenization, I would expect it to perform equally well or better. Please clarify this part. There are two baselines which I think are important for evaluation. First is a BPE tokenizer trained on the data you are using. This might resolve the problems with single granularity for both language considered here. Second is the "whole word masking" approach of BERT. You can use fine grained tokenization but mask out whole words / phrases as the case may be, which might give you the best of both worlds. Without these baselines, I am not convinced that we should prefer Ambert to other approaches.<BRK>This paper introduces AMBERT, a general purpose pre trained model that uses both fine grained and coarse grained tokenizations of the sentence. Given a sentence, AMBERT tokenizes it with both vocabs, then each sequence is passed independently through a shared transformer block. * For CLUE, the results are more impressive. The goal of this paper is to explore whether using multiple levels of granularity for tokenization can lead to better models. They implicitly use a similar masking for AMBERT (since they use the masks derived from the coarse grained for both the coarse grained and fine grained representations) but their BERT baseline uses the subpar wordpiece masking. * The authors have evaluated on a wide range of tasks and on both Chinese and English tasks. This should be easier than other ablations as it is fine tuning specific. * The method seems to perform well overall and better than the two proposed baselines (Combo/Hybrid). * There is little discussion of previous work that uses different levels of tokenization, such as models using both character and word based inputs. However, some comparisons to other systems are more unfair. The authors highlight in the abstract that the model “outperforms the existing best performing models in almost all cases”, making it a key contribution of their paper. A fairer “Our BERT” baseline would be trained for twice longer or use an ensemble of two BERTs. ALBERT has shown in the past that the # of parameters in a model could be significantly reduced at the cost of more compute. (Ebert style)Other concerns:* I was not able to find anywhere whether English version of the model uses a cased or uncased vocabulary. * The authors do include ALBERT, but they do so inconsistently. I believe the paper could be made stronger by trying to assess what are the changes introduced by using multiple levels of tokenization.<BRK>The paper combines fine and coarse grained tokenizations to learn word and phrase level representations. The authors introduce two variations on AMBERT: (1) using two separate encoders for fine and coarse, and (2) combine fine and coarse into a single encoder. This method is more expensive in terms of computations. The improvement is incremental, but it seems very effective in representing Chinese. However, the motivation for adding fine and coarse granularities is not well introduced. Strengths:  The performance is consistently good, although AMBERT has more parameters than baselines: BERT and ALBERT. Weaknesses:  The model is more complex and computationally more expensive (2 4x)  The proposed method seems not very effective for English, only to Chinese  Lack of intuitionQuestions and Suggestions:  Can you train a model with the same parameters as the baselines? I think it would show the significance of the approach. The paper has merits, but the comparison is not fair since they have different parameters with the baselines unless they have smaller parameters like ALBERT.<BRK>[General Review]  In this paper, the authors propose a new pre trained language model called AMBERT, which focuses on both fine grained and coarse grained tokenizations. Could you explain more about this? The final hidden representations of two encoders are concatenated and perform MLM+NSP pre training tasks. The experimental results show the proposed AMBERT could achieve significant improvements over various baseline systems. Overall, the design of the model is straightforward, and the paper is easy to read. However, I also have several concerns about this paper. 1) Using multi granularities in a pre trained language model is not novel, considering the SpanBERT (Joshi et al., 2020) and ZEN (Diao et al., 2019) has already been existing for some time. 2) The source of the improvements is not clear. As the proposed model uses about 2x parameters of BERT base, it is not clear whether the improvements are benefited from these additional parameters or the design of the multi grained encoders. Demonstrate that the combination of word level and phrase level information is helpful in pre trained language models. The technical novelty is limited. There is nothing much exciting to know that the model could achieve better scores, considering its parameter size (about 2x of BERT base) and the use of variants of whole word masking or n gram masking. I ve checked Appendix C.1, but did not find if your model is pre trained from scratch or starting from BERT checkpoint by Devlin et al.(2018)?I think the fine grained encoder is the same with original Chinese BERT. 2.For the Chinese word segmentation, the authors mention that  a word segmentation tool based on an n gram model . If so, how was the word vocabulary (72,635) obtained? The authors indicate that this may be caused by the inaccurate CWS for these tasks. However, when it comes to English counterparts, AMBERT shows similar or better performance than state of the art counterparts.<BRK>The novelty of the work relies on two encoders sharing parameters: one focusing on a fine grained representation of the text (characters or words) and the other one on a coarse grained representation of the text (words or n grams). Both fine grained and coarse grained representations are used during pre training and fine tuning. The architecture, which combines fine grained tokenized representations with coarse grained tokenized representation, achieves very strong performance on benchmark datasets for Chinese and English, outperforming BERT and other Transformer based models most of the time. The authors also provide ablation studies and analyses using variations of AMBERT: AMBERT Combo (fine grained and coarse grained encoders but without parameter sharing) and AMBERT Hybrid (one encoder using the concatenation of fine grained and coarse grained representations). Pros:  	The architecture is novel and take advantage of the multi grained components within the sentences (character, word, n gram) while sharing parameters between the two encoders and thus improving efficiency. The architecture’s performance is validated empirically on many tasks and datasets. AMBERT outperforms most of the time recent Transformer models on both Chinese and English benchmark datasets. Ablation studies in the form of a comparison with similar architectures (Combo or Hybrid), analyses of attention maps and distance between representations are proposed and help understand the performance and differences between modelsCons:  	Even though the performances are almost always significantly better than BERT or other Transformer models, the cost is also a significant increase in the number of parameters to train (108M for Google BERT vs 176M for AMBERT in Table 1 and 110M vs 194M parameters in Table 4)Questions:  	Could the authors provide more information regarding the coarse grained tokenization algorithm used? Minor Comments:  	There is a typo at the end of the second paragraph of section 1: "tokeniztion" instead of "tokenization" 	It would be interesting to see the study generalized to other scales of the input documents (sub words, pairs of characters, pairs of phrases etc.)
Reject. rating score: 1. rating score: 2. rating score: 2. rating score: 3. <BRK>The works proposes a generalization of MMD squared distance. However, the submission seems to be an incomplete one. Definition 2 seems to be the key definition in the work. However, there are multiple issues:       a. It is not clear why it is called a kernel? 3. section 4.3.1 are known results and need to be skipped.<BRK>This paper tries to propose a kernel based discrepancy measure called generalised probability kernel that can unify MMD and KSD which is an interesting topic of discussion. e.g.kernel K_{p,q} depends on density p and q. also a symmetric version on discrete KSD is discussed. Despite the idea is interesting, there are several flaws which can be reviewed. it is not clear from the Bernstein polynomial introduced in appendix.<BRK>Summary.The authors describe a family of kernel functions on discrete probability measures. * This is in particular as the authors do not provide any sort of asymptotic analysis of the presented estimators. I would recommend a major work over before considering a submission again. There are many missing points in theory, experiments (there are none), and presentation.<BRK>They propose to estimate these generalized probability kernel distance using empirical estimators. The authors should better motivate this setting by giving at least on relevant example, either theoretical or practical, where such structure is relevant. * What does a Stein operator in a discrete setting means? Notations between this equation and the next are not consistent ($n$ is paired with $x$ and then with $y$ in the next equation). * p.1:  remain futher study  should be for instance  is left for future work . * p.7:  preliminary results . * p.7: what does  justing  mean? Please give proper referencing. * p.2: Why is there a line break right at the start of 4.1?
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 6. <BRK>Summary: The authors attempt to investigate to what extent languages are hard to conditionally language model. There is no complexity that intrinsic to a language except its statistical properties concerning sequence length and vocabulary (unless word based methods are used). They also observe phenomena such as Double Descent and erraticity. Strengths:  The Experiments are extensive. Weaknesses:   The diagrams are difficult to read  The paper is hard to follow and would benefit from a clearer focus rather than the broad range of topics covered here. For example:      It is difficult to understand what the methods/terms (the information theoretic measure used, double descent) are   little time is spent explaining these. Double descent is discussed in the paper but it is still made not clear why this is relevant in the paper. Several portions of text are repeated   with some editing, space can be made to discuss concepts important to the paper  The authors make recommendations for modeling (Eg.using char level or byte level models for certain models   which have been extensively studied for this): this is not followed up with any concrete results on translation/downstream tasks or pointing out relevant work.<BRK>This paper is trying to answer an important question: How does representation play a role in carrying meanings? In doing so, the authors experimented with 6 languages in  3 + 5 kinds of representations. However,  as much as I agree with some of the final conclusions, the soundness of the experiments appears to be in question. Wubi was originally invented for professional typesetters so that they can type fast. The segmentation may not be correlated with the meaning of the word at all, as claimed by the papers cited by the authors. Pinyin, on the other hand, is highly ambiguous. Each language carries meaning differently and the information density is drastically different.<BRK>The paper investigates whether languages are equally hard to Conditional Language Model (CLM). I appreciate the authors  effort for their systematic controlled experiments. However, I m leaning towards rejecting this paper since I think some of the claims made in the paper are too strong and not really backed up by their experiments. Some comments:* The term "Conditional Language Model" can be misleading, since this paper model a target language conditioned on a source language, so more like in a machine translation setting rather than standard language modeling setting where you can also condition on the previous history. * I also think the summary in Section 1.2 stating that linguistic typological information is not necessary given "statistical properties concerning sequence length and vocabulary" is not necessarily valid since these two properties are the results from linguistic typology information.<BRK>The paper provides an empirical investigation of an important problem: the transferability of language modeling signals across languages in the transformer model. This is an important question because it can teach us both on the relations between languages and the properties of the transformer model (although it is not easy to tease the two effects apart). These conclusions are likely to be useful for the research community as part of its on going investigation of language transfer and the transformer model. Just as a couple of examples: It was very hard for me to follow the abstract, the first paragraph, the (very long) sentence that start with "in order to eliminate" (1.1), item 3 in the list of contributions and this is just a partial list. I ask that if the paper is accepted the authors will try to improve this aspect.
Reject. rating score: 3. rating score: 3. rating score: 6. rating score: 6. rating score: 7. <BRK>Pros:1.A stage wise approach to train GANs for video is defined to reduce the computational costs needed to generate long high resolution videos. 2.The authors provide some quality results of the proposed approach. Cons:1.The contribution of this paper is very limited. The authors just do some incremental improvement based on current GAN models, and the theoretical analysis for the stage wise training approach is not enough. 2.The experiments are not convincing. The authors only compared the baseline methods in the experiments. Besides, the proposed training strategy should be applied in different generation models based on GAN to show the effectiveness in different cases. 3.This paper aims to reduce the computation cost of the model training, but do not achieve significant effect, which takes 23 days for model training.<BRK>Summary:The paper proposes a stage wise training pipeline for training 128x128 resolution videos of up to 100 frames. It starts by generating low resolution and temporally downsampled videos, and upsample the results in a stage wise manner. The paper is well written and easy to follow. Quantitative results show that the proposed method is superior than existing methods under some circumstances. The novelty is very low. Stage wise and progressive training have been proposed for such a long time, they have been used everywhere. Since the paper claims the computation cost is lower, one would expect the model can generate higher resolution and much longer duration videos, but in fact it’s quite the opposite. To prove the effectiveness, I feel the authors need to show something higher than 256x256, say 512 or 1024 resolution. On the other hand, the hardware requirement is still high (128 GPUs) instead of some normal equipment that everyone can have, so I really don’t see any benefit of the model. If the authors can train DVD GAN using only a handful of GPUs, that might also be a contribution, but it’s not the case now. 3.Output quality is reasonable, but still far from realistic. Recent GAN works have shown amazing quality in synthesized results, and the bar has become much higher than a few years ago. In that aspect, I feel there’s still much room for improvement for the result quality.<BRK>The paper proposes a GAN based model which generates videos in multiple stages. This is the key feature of the proposed model allowing the model to generate videos of higher temporal resolution while using significantly less computational resources. **Strengths**+ The paper is clearly written+ The model performs competitively with relevant baselines with respect to quantitative metrics+ The evaluation of the model has been conducted on real world datasets+ Implementation details have been mentioned clearly**Weaknesses**  There have been earlier attempts for multi stage video generation  [1,2]. Also, apart from condition for the generation, how is the proposed model different from the existing multi stage ones? The generated samples for Kinetics dataset are not temporally consistent and misses several details especially for smaller entities in video. To list a few: in Figure 2 row 4, the baby s face looks distorted and different in every frame; in Figure 3 row 2 and in Figure 2 row 1, the face of the person is completely incomprehensible. The generated samples in the paper do not have a lot of perceived motion in them. Overall, the paper presents a scalable way to generate video with higher temporal resolution. *References used in the review:*[1] Learning to generate time lapse videos using multi stage dynamic generative adversarial networks. International Journal of Computer Vision, 2020 **Post Rebuttal Comments** I appreciate the revisions and additional results presented by the authors. The authors have addressed my concerns as well as improved the clarity of the model description in the revised version of the paper. While that results are not perfect, I acknowledge that the problem of video generation is difficult and I believe such multi stage model can motivate future methods in this direction of scalable video generation. Therefore, I would like to improve my score to 6 and would recommend acceptance of this paper.<BRK>The method shows promising results on on generating high duration (up to 100 frames) class conditional videos with convincing Inception scores, indicating quality similar to DVD GAN, while consuming less memory and with better coherence. While the contribution of the paper is mainly to improve the DVD GAN architecture to reduce training time and memory consumption, the reviewer believes that the paper would be a good contribution to the venue. Below there are some questions on the methodology:1. From the architecture it is not evident whether or not it only does this or it is also entangled with assessment of how good the low resolution sample $x_w^l$  was. In other words, if the low resolution sample scores good (e.g.because it s the real world data) but the upsampling does not match,  would the objective of the matching descriptor training still score it as a good upscaling? 2.Although, as mentioned in the introduction, it may not be as big problem as for VAE based models, the problem of blurring might exist for DVD GAN like models. It is written in the caption of Figure 5 that  Despite the two stages of local upsampling, the frame quality does not degrade noticeably through time.’ Although the reviewer appreciates that previous work reported only IS/FID/FVD metrics and that defining proper evaluation metrics for generative models is an open question, it might be a good idea to show some other quantitative metrics such as power spectral density (PSD) plots similar to figure 5 from [1]. This would help get an idea how it compares to the real world video in terms of blurring of the results. 3.Given that the generation of videos is class conditional, is it possible to show the metrics per class?<BRK>**Paper Contributions**The paper proposes SSW GAN, an adversarial generative model of video which proposes a new generator architecture along with splitting the training into multiple stages. Finally, there is a major question this paper does not address: **is the two stage training of SSW GAN necessary, or can it be trained in a single pass** (but with the generator decomposition as described)? It is missing an important ablation showing the results of training the stages jointly. * The generator architectures changes from prior work are quite concrete, but the high level description doesn t clearly reflect that. * Some of the comparisons and model descriptions are lacking details, which make it difficult to understand experiments. I do not think this ablation is required to maintain an accept rating, but including it would push my rating closer to clear accept, and be quite a strong addition to the paper s content. **I believe this paper is an accept (7), however I think there are a number of places where the description of the architecture and experiments needs more detail, and for my final rating I would like to see these addressed in rebuttal. Were that to be added I would strongly consider moving to clear accept (8). **Supporting arguments for your recommendation. * Splitting the training into two phases, where at first you only train stage 1 and then you train stage 2. These are very interesting ideas, since either one substantially reduces the training cost of the video model (something aptly described in the paper), and not something strongly touched on by previous work. For that reason I think the work in this paper is worthy of acceptance. * In section 4, it is unclear if all four bold sections are trained independently or not. * In general, the paper is not super clear where upsampling (both in time and space) occurs. * I am not super clear on the comparisons between SSW GAN and DVD GAN in “comparison with prior work”. When you say "our model trained to generate 128x128/12 videos “ is this the model which had a first stage trained on 32s32/25 and then you trained the second stage on input windows of 6 frames, generating 128x128/12, and took just single samples from that to compare? * Similarly, when you say “However, our model is only trained on 128x128/12 outputs, as it is unrolled and applied convolutionally over the first stage output to generate 48 frames.”, isn’t it the case that the first stage is trained on longer sequences?