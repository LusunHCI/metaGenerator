Reject. rating score: 3. rating score: 3. rating score: 6. There is a proposition 2 trying to explain some part of the mechanism of UGI. 2.Even though the title of the paper is "improving the gating mechanism of recurrent neural networks", the authors try to solve signal propagation problems. More explanations are needed. The writing in the description of the UGI and refine fate is not clear.<BRK>This paper proposes to improve the learnability of the gating mechanism in RNN by two modifications on the standard RNN structure, uniform gate initialization and refine gate. The authors propose a new refine structure that seems to have a longer "memory". There are several parts in the experiment that are not very convincing. What is the reason? The proposed method seems not very general.<BRK>The second modification is the introduction of a refine gating mechanism with a view to allow for gradients to flow when the forget gates f_t in the LSTM updates are near saturation. The refine gate was also adapted to memory architectures such as the DNC and RMA where it was found to improve performance on two different tasks. While I m not entirely convinced about the proposed initialization scheme but across the many different tasks tried, the use of the refine gate does appear to give performance improvements that lead me to conclude that this aspect of the work is a solid contribution to the literature. Questions and comments:* This manuscript already quite long and has several formatting issues. i.e.for the URLSTMs that performed well, were the values of the forget gate closer to 0/1 than the baselines?<BRK>In this work, they revisit the gating mechanisms widely used in various recurrent and feedforward networks such as LSTMs, GRUs, or highway networks. These gates are meant to control information flow, allowing gradients to better propagate back in time for recurrent models. Hotheyver, to propagate gradients over very long temporal windows, they need to operate close to their saturation regime. they propose two independent and synergistic modifications to the standard gating mechanism that are easy to implement, introduce no additional hyper-parameters, and are aimed at improving learnability of the gates when they are close to saturation. their proposals are theoretically justified, and they show a generic framework that encompasses other recently proposed gating mechanisms such as chrono-initialization and master gates . they perform systematic analyses and ablation studies on the proposed improvements and evaluate their method on a wide range of applications including synthetic memorization tasks, sequential image classification, language modeling, and reinforcement learning.  Empirically, their proposed gating mechanisms robustly increase the performance of recurrent models such as LSTMs, especially on tasks requiring long temporal dependencies.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. *** SummaryThis work proposes to use an augmented RNN model to address the incremental domain adaptation problem. A proof in a highly simplified case is given in addition to empirical results showing that expanding the memory bank is better than expanding the RNN states. Section 3 is well written. The methods and motivations are illustrated clearly. A comparison between this memory and the traditional attention can help demonstrate the validity of this choice.<BRK>This paper proposes an extensible attention mechanism applied on the previous hidden state of an RNN and resulting in supplementary input for the next RNN step. This method is applied in the context of incremental domain adaptation for NLP without the possibility of storing of old samples (episodic memory). Pros:  Extensive ablation study with the different possible combinations of methods  Very interesting comparison between expanding the memory (i.e.attention) and expanding the hidden states. It would have been better to be able to observe catastrophic forgetting for the source domain. Otherwise, the paper proposes a novel method which works well in practice so I am leaning towards acceptance.<BRK>###Summary###This paper introduces incremental domain adaptation for natural language processing, assuming that each domain comes one after another and only the current domain can be accessed in the application scenario. The basic framework of this paper is based on RNN but augmented with the directly parameterized memory bank. 2).Can the proposed approach generalize to visual domain adaptation, i.e.on the visual task instead of NLP task? ### Novelty ###This paper proposes incremental domain adaptation, which is inspired by Li & Hoiem s work. Thus, the problem setting provides some novelty. The proposed claims are well supported by the experiments and analysis. 3) The paper is overall well organized and well written.<BRK>This paper addresses the problem of incremental domain adaptation (IDA) in natural language processing (NLP). they assume each domain comes one after another, and that they could only access data in the current domain. The goal of IDA is  to build a unified model performing theyll on all the domains that they have encountered. they adopt the recurrent neural network (RNN) widely used in NLP, but augment it with a directly parameterized memory bank, which is retrieved by an attention mechanism at each step of RNN transition. The memory bank provides a natural way of IDA: when adapting their model to a new domain, they progressively add new slots to the memory bank, which increases the number of parameters, and thus the model capacity. they learn the new memory slots and fine-tune existing parameters by back-propagation. Experimental results show that their approach achieves significantly better performance than fine-tuning alone. Compared with expanding hidden states, their approach is more robust for old domains, shown by both empirical and theoretical results. their model also outperforms previous work of IDA including elastic theyight consolidation and progressive neural networks in the experiments.
Reject. rating score: 3. rating score: 3. rating score: 8. In this paper, the authors present a new perspective on how to effectively noise unlabeled examples and argue that the quality of noising plays a crucial role in semi supervised learning. By substituting simple noising operations with advanced data augmentation methods, their method brings substantial improvements across six language and three vision tasks under the same consistency training framework. In this paper, all the results, including the augmented methods are all well established approaches. It is difficult to convince the reviewers. I think this paper likes a technical report, not a research paper. So, why not to share the parameters directly?<BRK>The paper proposes to substitute simple noising operations with many data augmentation methods in consistency based semi supervised learning. Overall, the paper is well written and clear. However, it looks not surprising to me that more data augmentations found in supervised learning are also effective in semi supervised learning. I m willing to increase my score if the authors address my concerns.<BRK>The key insight in this paper is that, data augmentation methods that work well during supervised training should also work equally well as the noise distribution for consistency training on unlabeled data. The paper is well written and the authors present extensive comparative and ablation tests to demonstrate that their proposed method works well with both low and high amounts of labeled data. This paper should be accepted into the conference.<BRK>Semi-supervised learning lately has shown much promise in improving deep learning models when labeled data is scarce. Common among recent approaches is the use of consistency training on a large amount of unlabeled data to constrain model predictions to be invariant to input noise. In this work, they present a new perspective on how to effectively noise unlabeled examples and argue that the quality of noising, specifically those produced by advanced data augmentation methods, plays a crucial role in semi-supervised learning. By substituting simple noising operations with advanced data augmentation methods, their method brings substantial improvements across six language and three vision tasks under the same consistency training framework. On the IMDb text classification dataset, with only 20 labeled examples, their method achieves an error rate of 4.20, outperforming the state-of-the-art model trained on 25,000 labeled examples. On a standard semi-supervised learning benchmark, CIFAR-10, their method outperforms all previous approaches and achieves an error rate of 2.7% with only 4,000 examples, nearly matching the performance of models trained on 50,000 labeled examples. their method also combines theyll with transfer learning, e.g., when finetuning from BERT, and yields improvements in high-data regime, such as ImageNet, whether when there is only 10% labeled data or when a full labeled set with 1.3M extra unlabeled examples is used.
Accept (Poster). rating score: 8. rating score: 8. rating score: 6. I would also caution against stating that the environment is closer to the real world. These representation learning in the paper is based on action conditional predictions of future quantities, which is complementary to the approach proposed in the paper. The paper proposes a neural implementation of particle filters, by treating samples of RNN states as particles. The particles are used to estimate moment generating functions evaluated at trained vectors, which in turn are supposed to provide more information for the policy s decision making. The ablation study suggests that all three components (particles, MGFs & discrimination) are necessary. However, the third component has been shown not to be exclusively helpful for representation learning (Gregor et al., Guo et al.) I vote for acceptance. I think the algorithmic idea in this paper is a step in the right direction and can be of interest for the community. I think it is important for the paper to qualify the kind of POMDPs being considered. The defining features of most of the environments being used is that the state is observed through a noisy channel.<BRK>Update: my concerns have been addressed and I have updated the score to 8****This paper introduces 3 neat ideas for training deep reinforcement learning (DRL) agents with state variables so that they can handle partially observed environments:1) model the latent state variable as a belief distribution, using a collection of weighted hidden states, like in the particle filter (PF), with an explicit belief update of each particle, calculation of the weight using an observation function, and a differentiable re weighting function to get the new belief distribution,2) base the policy on the whole set of particles, by quantifying that set using its mean as well as a collection of K learnable moments (specifically, K Moment Generating Functions, each one corresponding to a dot product between the moment variable and the hidden state of the particle),3) instead of generating the observations, take again the idea from PF which is to measure the agreement between the current observation o_t and the i th particle state variable h_t^i, via a learnable discriminative function. From what I understand, the only gradients in the model come from the usual 3 RL losses, and the observation functions in the discriminative PF are trained because they weigh the particles. The paper is a very well written and the experiments are very well executed. I believe that the idea is novel. I understood it was trained using discriminative training. Does it mean that different observations o_t are used, and if so, how many? Isn t there a posterior collapse, with all particles ending up bearing the same state? * In the discussion, can you comment on the relationship between Monte Carlo Tree Search in RL agents (sampling different trajectories) vs. here (sampling different states)? I would be interested in seeing more analysis of the discriminative PF RL algorithm on navigation tasks, given that that s what PF were designed for.<BRK>This is a well written paper. It combines the strength of Bayesian filtering and policy oriented discriminative modeling. DPFRL encodes a differentiable particle filter with learned transition & observation models in a neural network, allowing for reasoning with partial observations over multiple time steps. Experimental results show that DPFRL achieves state of the art on POMDP RL benchmarks. Improved performance is reported.<BRK>Deep reinforcement learning is successful in decision making for sophisticated games, such as Atari, Go, etc. 
Hotheyver, real-world decision making often requires reasoning with partial information extracted from complex visual observations. This paper presents  Discriminative Particle Filter Reinforcement Learning (DPFRL), a new reinforcement learning framework for complex partial observations. DPFRL encodes a differentiable particle filter in the neural network policy for explicit reasoning with partial observations over time. The particle filter maintains a belief using learned discriminative update, which is trained end-to-end for decision making. they show that using the discriminative update instead of standard generative models results in significantly improved performance, especially for tasks with complex visual observations, because they circumvent the difficulty of modeling complex observations that are irrelevant to decision making.
In addition, to extract features from the particle belief, they propose a new type of belief feature based on the moment generating function. DPFRL outperforms state-of-the-art POMDP RL models in Flickering Atari Games, an existing POMDP RL benchmark, and in Natural Flickering Atari Games, a new, more challenging POMDP RL benchmark introduced in this paper.  Further,  DPFRL performs theyll for visual navigation with real-world data in the Habitat environment.
Reject. rating score: 1. rating score: 3. rating score: 8. Adversarial training generalizes data dependent spectral norm regularizationThis paper shows that, projected gradient descent based adversarial training is similar to the data dependent spectral norm regularization, and under very restrictive condition, the authors show that this two methods are the same. Some experiments are conducted to support the theory. The authors only give a data dependent version of the spectral normalization based on the Jacobian of the neural networks, which I think is somewhat weak. This theorem cannot convince me that the proposed methods have a strong theoretical basis. 6.I think the discussion in Appendix A.5 is somewhat confusing.<BRK>This paper studies the link between adversarial training and the proposed data dependent operator norm regularization for ReLU network. Under specific conditions, in theory, the authors show the equivalence between the l2 PGD training and the regularization method. Empirical experiments are conducted to support the theory.<BRK>This largely theretical paper establishes a theoretical link between adversarial training and operator norm regularizationfor DNNs. It is well written and structured, and it falls squarely within the the remit of the conference.<BRK>they establish a theoretical link bettheyen adversarial training and operator norm regularization for deep neural networks. Specifically, they present a data-dependent variant of spectral norm regularization and prove that it is equivalent to adversarial training based on a specific $\ell_2$-norm constrained projected gradient ascent attack. This fundamental connection confirms the long-standing argument that a network's sensitivity to adversarial examples is tied to its spectral properties and hints at novel ways to robustify and defend against adversarial attacks. they provide extensive empirical evidence to support their theoretical results. 
Reject. rating score: 6. rating score: 6. rating score: 6. This paper proposes to learn a subset of a given dataset that acts as an adversary, that hurts the model performance when used as a training dataset. The central claim of the paper is that existing datasets on which models are trained are potentially biased, and are not reflective of real world scenarios. I have updated my score to reflect this. To that end, the sentence that says it works for "any" q() is incorrect. But that does not address the central idea, that the resultant models do better in the wild.<BRK>They introduce an algorithm that selects more representative data points from the dataset that allow to get a better estimate of the performance in the wild. The algorithm ends up selecting more difficult/confusing instances. This paper is easy to read and follow (apart from some hickup with a copy of three paragraphs), but in my opinion of limited use/impact. Please fix2) I am not sure   What applications the authors suggest. They seem to say that benchmark authors should run their algorithm and make benchmarks harder. To me it seems that benchmarks become harder because you remove most important instances from the training data (so Table 4 is not surprising   you remove the most representative instances so the model can t learn)  how practically feasible it is. How would that do? For NLP (SNLI) task i think this would be a more reasonable baseline than just randomly dropping the instances,5) I wonder if you actually retrain the features after creating filtered dataset, new representation would be able to recover the performance. Changing to weak accept<BRK>the paper proposes an algorithm that adversarially filters out examples to reduce dataset specific spurious bias. the key intuition is that the datasets are curated in a way that easy to obtain samples have higher probability to be admitted to the dataset. this make the paper a promising proposal. the idea of the paper is interesting and the experiments show a substantial reduction in the performance of existing algorithms.<BRK>Large-scale benchmark datasets have been among the major driving forces in AI, supporting training of models and measuring their progress. The key assumption is that these benchmarks are realistic approximations of the target tasks in the real world. Hotheyver, while machine performance on these benchmarks advances rapidly --- often surpassing human performance --- it still struggles on the target tasks in the wild. This raises an important question: whether the surreal high performance on existing benchmarks are inflated due to spurious biases in them, and if so, how they can effectively revise these benchmarks to better simulate more realistic problem distributions in the real world.  
In this paper, they posit that while the real world problems consist of a great deal of long-tail problems, existing benchmarks are overly populated with a great deal of similar (thus non-tail) problems, which in turn, leads to a major overestimation of true AI performance. To address this challenge, they present a novel framework of Adversarial Filters to investigate model-based reduction of dataset biases. they discuss that the optimum bias reduction via AFOptimum is intractable, thus propose AFLite, an iterative greedy algorithm that adversarially filters out data points to identify a reduced dataset with more realistic problem distributions and considerably less spurious biases. 
AFLite is lighttheyight and can in principle be applied to any task and dataset. they apply it to popular benchmarks that are practically solved --- ImageNet and Natural Language Inference (SNLI, MNLI, QNLI) --- and present filtered counterparts as new challenge datasets where the model performance drops considerably (e.g., from 84% to 24% for ImageNet and from 92% to 62% for SNLI), while human performance remains high. An extensive suite of analysis demonstrates that AFLite effectively reduces measurable dataset biases in both the synthetic and real datasets. Finally, they introduce new measures of dataset biases based on K-nearest-neighbors to help guide future research on dataset developments and bias reduction. 
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. In this paper, the authors proposed a new method to model the source code for the bug repairing task. Traditional methods use either a global sequence based model or a local graph based model. The authors proposed a new sandwich model like [RNN GNN RNN]. But the message is clear. The sandwich model can benefit from GNN model that can achieve a higher accuracy at the beginning of training where transformer did a poor job. At the end of training, the sandwich model outperforms both kinds of models. Here are some detailed comments:1. It would be interesting to have the complete result from Transformer in Figure 1 and Figure 2 which is missing. Why GGNN is so slow in this paper?<BRK>The paper claims to improve state of the art results published by Vasic et al for this task, however the RNN model by Vasic et al was known to be far from optimal when the work was published. The paper does a number of contributions to the neural architecture. This change is also what makes the work perform as well or better than GGNN based approaches. The paper then proposes to improve the transformer model by modifying the attention where there are edges. Given that most of the work talks about performance, it would also help if the authors clarify what kind of hardware was used and which optimizer. More minor issues:“We conjecture that the Transformer learns to infer many of the same connections”. It seems it is q_i and k_j? I actually increase my score a bit (I was torn in the beginning), because this is one of the first papers to run transformer model on code.<BRK>Strength:  Interesting problem The paper is well written and easy to follow  The proposed approach seems very effectiveWeakness:  the novelty of the proposed is marginal  Some of the claims are not right in the paperThis paper studied learning the representations of source codes by combining sequential based approaches (RNN, Transformers) and graph neural network to model both the local and global dependency between the tokens. In the abstract, the authors said the graph neural network is more local based while the transformer is more global based. And there are actually some existing work that have already explored this idea in the context of natural language understanding, e.g.,Contextualized Non local Neural Networks for Sequence Learning.<BRK>Models of code can learn distributed representations of a program's syntax and semantics to predict many non-trivial properties of a program. Recent state-of-the-art models leverage highly structured representations of programs, such as trees, graphs and paths therein (e.g. data-flow relations), which are precise and abundantly available for code. This provides a strong inductive bias towards semantically meaningful relations, yielding more generalizable representations than classical sequence-based models. Unfortunately, these models primarily rely on graph-based message passing to represent relations in code, which makes them de facto local due to the high cost of message-passing steps, quite in contrast to modern, global sequence-based models, such as the Transformer. In this work, they bridge this divide bettheyen global and structured models by introducing two new hybrid model families that are both global and incorporate structural bias: Graph Sandwiches, which wrap traditional (gated) graph message-passing layers in sequential message-passing layers; and Graph Relational Embedding Attention Transformers (GREAT for short), which bias traditional Transformers with relational information from graph edge types. By studying a popular, non-trivial program repair task, variable-misuse identification, they explore the relative merits of traditional and hybrid model families for code representation. Starting with a  graph-based model that already improves upon the prior state-of-the-art for this task by 20%, they show that their proposed hybrid models improve an additional 10-15%, while training both faster and using fetheyr parameters.
Reject. rating score: 3. rating score: 6. rating score: 6. In this way, learning curve prediction can act as a fast method for performance measurements in AutoML. However, I have some concerns about the motivations and usage of the proposed method. Learning curve prediction is a general approach that can be combined with many zero order optimization methods. Does this paper focus on learning curve prediction on general AutoML problems or just NAS? How will the changes in the search space affect the proposed method? It is better to show what kinds of curves will the proposed method likely to give early stops.<BRK>The paper proposes a new method to rank learning curves of neural networks which can be used to speed up neural architecture search. While the method seems interesting, I don t think the paper is ready for acceptance yet, since it a) misses some important details and b) the empirical evaluation is not sufficient. I also miss a discussion of these methods in the related work section. This should be straight forward, since decisions which configuration is promoted to a higher budget can be made based on the model instead of just the last observed value. What is delta in the experiments? How sensitive is this hyperparameter in practice?<BRK>This paper considers the problem of automatic early termination in hyper parameter search and neural architecture search. Comparisons with state of the art NAS and HPO algorithms that do not use learning curves prediction (e.g., HyperBand, learning by learning algorithms using LBFGS, etc.) Unlike most previous approaches for learning curve prediction, which estimates the probability of whether the current model is better than the current best model or not by first extrapolating the learning curve and then invoke a heuristic measure, this paper proposes to predict the probability directly. The neural network f is trained with some meta dataset, and then the early termination is then decided based on this pairwise comparison probability.<BRK>Many automated machine learning methods, such as those for hyperparameter and neural architecture optimization, are computationally expensive because they involve training many different model configurations. In this work, they present a new method that saves computational budget by terminating poor configurations early on in the training. In contrast to existing methods, they consider this task as a ranking and transfer learning problem. they qualitatively show that by optimizing a pairwise ranking loss and leveraging learning curves from other data sets, their model is able to effectively rank learning curves without having to observe many or very long learning curves. they further demonstrate that their method can be used to accelerate a neural architecture search by a factor of up to 100 without a significant performance degradation of the discovered architecture. In further experiments they analyze the quality of ranking, the influence of different model components as theyll as the predictive behavior of the model.
Reject. rating score: 3. rating score: 3. rating score: 6. The paper proposes a new GAN training that additionally feeds adversarial examples as the real samples to the discriminator D. A key motivation here is to regularize the target real distribution simulated by D to be robust to adversarial perturbations. Experimental results show that the proposed GAN training generally improves the generation performance from the vanilla GAN training in CIFAR 10, CelebA and LSUN datasets. In overall, I liked its clear motivation and the simplicity of the method. One of my main concerns, however, is that robustifying D in GAN training is not a new idea for some readers [1], so they need more clarification on the novelty of the proposed method, e.g.by discussing about it in related work or by comparing the performance. Some questions listed here are relevant to this point:    (a) The paper only considers to use FGSM in adversarial training part, but FGSM training on large epsilon usually leads to overfitting [2] on CIFAR 10. I wonder if the authors have tried PGD counterpart in their method. (b) It seems that the method uses both natural and adversarial examples in adversarial training, as in [3]. Instead, there is another (and perhaps more common) type of adversarial training [2] that uses only adversarial examples for the training. [1] Liu, X., & Hsieh, C. J. (2019).Rob gan: Generator, discriminator, and adversarial attacker. CVPR 2019.<BRK>This paper tries to propose a new method to stabilize the training procedure of GAN. To this end, they use adversarial samples of real data to train the discriminator, and claim that it is helpful to reduce the adversarial noise contained in the gradient. Although training GAN with adversarial samples of discriminator is somewhat novel, the method and experiments are not convincing. I do not recommend the acceptance based on the limited contribution of this paper. 1.The paper uses vague description such as “This approach can improve the robustness of discriminator and reduce adversarial noise contained in gradient” without convincing justification. The author running adversarial training on CIFAR10 dataset with FGSM and the perturbation is tested from 0.2/255 ~ 4/255. The performance (FID score) is a bit sensitive to the amount of perturbation level.<BRK>This paper presents an interesting idea based on introducing adversarial noise on real samples during GAN training. This novel approach may improve GAN training and have potentially large impact, but the paper in its current form is slightly below the standard of ICLR due to its lack of clarity. Properties used to justify the approach, such as “symmetric” and “balanced” need to be explained. (after eq.8) It is unclear to me that “backward propagation of Equation 5 with respect to x with negligible computation overhead”. It would be helpful to provide an estimation of runtime overhead supported by experiments. In algorithm 1, why is it necessary to perform discriminator update twice? A solid baseline is necessary for any further assessment. The analysis also need to be improved. Nit:The 1 line derivation of eq.6 can be incorporated into the main text. Update:I read the authors  rebuttal, which addressed many of my concerns and presented additional empirical results. I therefore believe the final camera ready version can be a valuable contribution to the field, and would like to recommend accepting this paper.<BRK>Generative adversarial networks have achieved remarkable performance on various tasks but suffer from sensitivity to hyper-parameters, training instability, and mode collapse. they find that this is partly due to gradient given by non-robust discriminator containing non-informative adversarial noise, which can hinder generator from catching the pattern of real samples. Inspired by defense against adversarial samples, they introduce adversarial training of discriminator on real samples that does not exist in classic GANs framework to make adversarial training symmetric, which can balance min-max game and make discriminator more robust. Robust discriminator can give more informative gradient with less adversarial noise, which can stabilize training and accelerate convergence. they validate the proposed method on image generation tasks with varied network architectures quantitatively. Experiments show that training stability, perceptual quality, and diversity of generated samples are consistently improved with small additional training computation cost.
Reject. rating score: 1. rating score: 3. rating score: 3. This paper proposes two techniques for fast linear interpolation on CPUs. They achieved speedups by reducing 1) fixed overhead cost and 2) per example computation (linear interpolation operation level optimization). Authors consider this problem for small operation models like linear interpolation rather than the models requiring large operations such as ResNet. This reviewer also does not have enough knowledge and background to judge this paper. This is only consider optimization on CPUs. (I don’t have much knowledge on compiler..)<BRK>(4) A necessary process of LUT from piece wise linear functions of approximating the ones that we care about is to conduct 1 D density estimation on the input domain, since more fine grained splits are required in the dense area and less splits elsewhere. Could the authors present some results for this? It is okay if there is a performance (accuracy or likelihood) and speed trade off, but the paper didn t present any results on this. I don t think the paper should downplay the part of finding T as it is crucial.<BRK>Also, the Tensorflow implementation should be briefly explained to make clear where these gains come from. While the topic is relevant for this conference, readers are likely not familiar with some of the concepts used in the paper, and a bit more explanation is needed. A memory efficient bit packing technique to store both integer and floating point representations together.<BRK>they present fast implementations of linear interpolation operators for both piecewise linear functions and multi-dimensional look-up tables. they use a compiler-based solution (using MLIR) for accelerating this family of workloads. On real-world multi-layer lattice models and a standard CPU, they show these strategies deliver $5-10\times$ faster runtimes compared to a C++ interpreter implementation that uses prior techniques, producing runtimes that are 1000s of times faster than TensorFlow 2.0 for single evaluations.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. SummaryThe authors propose learning the expected # of time steps to reach a given goal state from any other state which they call: dynamical distances. The claim is that dynamical distances can be used in a semi supervised regime, where unsupervised interaction with the env is used to learn the dynamical distances. The ideas presented in this work are interesting, but I have some major concerns (please see detailed comments below). The main experiment is shown with preferences where comparisons do not seem enough and the paper could be made stronger by thorough comparisons. In NIPS, 2013. [3] Gupta, Abhishek, et al."Unsupervised meta learning for reinforcement learning." Algorithm 1 learns the distance corresponding to the current policy. I would suggest comparing to other relevant baselines such as [3], [4], [5].<BRK>This paper presents an approach to do reinforcement learning without reward engineering. Instead, steps to reach goals are used to update policy. Can the authors elaborate on this? For example, how is the performance of a policy that just directly minimizes the distance function? Instead of using the made up toy example. d(g, g) is not 0 for all dynamical systems. Shouldn’t the policy learn to stay as close to the initial state as possible? While I understand that this method will be very effective for relatively static tasks like the valve turning, I fail to see how it can apply in unstable locomotion tasks, as mentioned in some of the questions above. Normally I will give rating 5 to this paper, but the system doesn t give me this option, so I have to lower it to 3 instead.<BRK>After a few readings my understanding of the paper is that there is a desire to develop a proxy (or basis) that is first determined in a reward free setting, agnostic to a goal. Something that I think needs to be changed is the continual use of the term distance when it is not a valid mathematical distance (this point is noted by the authors but then the term is used continuously and the notation used would be standard notation for an actual distance). Subsequently, this proxy can then be combined with a goal and improve efficiency in developing a suitable policy for that goal rather than starting from scratch with something like Q learning. The distances between states from the regressor is policy dependent and then this distance is used to inform the policy update   how can we be certain that this alternating optimization will not fall into a local minimum depending on the initial policy?<BRK>Reinforcement learning requires manual specification of a reward function to learn a task. While in principle this reward function only needs to specify the task goal, in practice reinforcement learning can be very time-consuming or even infeasible unless the reward function is shaped so as to provide a smooth gradient towards a successful outcome. This shaping is difficult to specify by hand, particularly when the task is learned from raw observations, such as images. In this paper, they study how they can automatically learn dynamical distances: a measure of the expected number of time steps to reach a given goal state from any other state. These dynamical distances can be used to provide theyll-shaped reward functions for reaching new goals, making it possible to learn complex tasks efficiently. they show that dynamical distances can be used in a semi-supervised regime, where unsupervised interaction with the environment is used to learn the dynamical distances, while a small amount of preference supervision is used to determine the task goal, without any manually engineered reward function or goal examples. they evaluate their method both on a real-world robot and in simulation. they show that their method can learn to turn a valve with a real-world 9-DoF hand, using raw image observations and just ten preference labels, without any other supervision. Videos of the learned skills can be found on the project theybsite: https://sites.google.com/view/dynamical-distance-learning
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. This work presents the value iteration with negative sampling (VINS) algorithm, a method for accelerating reinforcement learning using expert demonstrations. In addition to learning an expert policy through behavioral cloning, VINS learns an initial value function which is biased to assign smaller expected values to states not encountered during demonstrations. In those cases, a useful notion of similarity would likely have to be learned from the data. It is unclear that VINS would generalize well beyond robotic control domains.<BRK>The authors proposed the algorithm called value iteration with negative sampling (VINS) of which the main ideas can be summarized as follows. The second main idea of this work is to use *self correctable policy*, where the approximate dynamics and behavioral cloning (BC) policy were used to select the action which is expected to give higher value at the successor states. To consolidate their ideas, the authors proved Theorem 4.4 showing that state visitation distribution of resultant policy is approximately close to that of an expert under a few assumptions. Empirically, they considered two experiments. For instance, we can simply think about GAIL with BC initialization, but it seems to me that GAIL with random initialization was used in the second experiment (since authors mentioned GAIL in OpenAI baselines was used without hyperparameter tuning (https://github.com/openai/baselines/blob/master/baselines/gail/run_mujoco.py#L53)). I think the authors should have compared the performance of VINS + RL with those baselines in the second experiment if they tried to emphasize the sample efficiency of VINS + RL.<BRK>More specifically, policies learned in this manner can often fail when they encounter new states not seen in demonstrations. The paper proposes a method for learning value functions that are more conservative on unseen states, which encourages the learned policies to stay within the distribution of training states. A practical algorithm is also presented and experiments on continuous control tasks display the effectiveness of the method, with particularly good results on imitation learning followed by reinforcement learning. The theory may also have some restrictive assumptions, limiting its significance. Overall, I am divided about this paper. 4) It could be said explicitly that the expert policy is assumed to be deterministic. Experiments:1) It seems like VINS relies heavily on the assumption that the environment is deterministic to learn an effective model.<BRK>Imitation learning, follotheyd by reinforcement learning algorithms, is a promising paradigm to solve complex control tasks sample-efficiently. Hotheyver, learning from demonstrations often suffers from the covariate shift problem, which results
in cascading errors of the learned policy. they introduce a notion of conservatively extrapolated value functions, which provably lead to policies with self-correction. they design an algorithm Value Iteration with Negative Sampling (VINS) that practically learns such value functions with conservative extrapolation. they show that VINS can correct mistakes of the behavioral cloning policy on simulated robotics benchmark tasks. they also propose the algorithm of using VINS to initialize a reinforcement learning algorithm, which is shown to outperform prior works in sample efficiency.
Reject. rating score: 3. rating score: 3. rating score: 6. This paper proposes a feature partitioning scheme for efficient multi task neural architecture search. The problem of neural architecture search is important and beneficial to deep learning community to be able to extract the best out of these methods. I find the approach of this paper a heuristic.<BRK>This paper proposes a framework for learning multi task convolutional neural networks. However, I am not convinced by the presentation of the paper and by the lack of multi task learning baselines in the experiments. I will consider changing my score if the following comments are addressed. Why does this happen? The Figure is not very informative. Some questions:  how long does it take to solve? Please provide more details.<BRK>Instead of deciding on other hyper parameters such as the number of layers, the authors chose to study how to efficiently share the capacity of the network: to decide which filters should be used for which tasks, and which filters should be shared between tasks. The advantages of these approaches are well discussed and validated quantitatively. The paper is well written and the approach itself appears to be sound and it led to improvement over independent task estimator. However, I am mostly concerned about the experimental setting: there are no comparisons with any other MTL algorithm. This could potentially lead to improvement beyond multi task learning. Overall, I think this paper makes a borderline case.<BRK>Multi-task learning promises to use less data, parameters, and time than training separate single-task models. But realizing these benefits in practice is challenging. In particular, it is difficult to define a suitable architecture that has enough capacity to support many tasks while not requiring excessive compute for each individual task. There are difficult trade-offs when deciding how to allocate parameters and layers across a large set of tasks. To address this, they propose a method for automatically searching over multi-task architectures that accounts for restheirce constraints. they define a parameterization of feature sharing strategies for effective coverage and sampling of architectures. they also present a method for quick evaluation of such architectures with feature distillation. Together these contributions allow us to quickly optimize for parameter-efficient multi-task models. they benchmark on Visual Decathlon, demonstrating that they can automatically search for and identify architectures that effectively make trade-offs bettheyen task restheirce requirements while maintaining a high level of final performance.
Reject. rating score: 3. rating score: 3. rating score: 3. [3] Some experimental settings are not clear. But the following should be improved in the following aspects: [1] In the proposed method, the model parameters are updated both on the unlabeled samples and validation data set. The experimental results show that such a strategy is effective to improve the performance of the state of the art method. [2] How the validation data can improve the generalization ability of the model should be given with theoretical analysis.<BRK>I have 3 main concerns with the paper 1. The authors mention that  the meta validation set is a random subset of train set. The set of labeled points should be partitioned into train and meta validation set. Moreover, the experiment results are mostly declared without any justification (for example, the proposed method does not always lead to improvement and not all cases are explained.The authors only note that the method works well in low data regime). Is there a reason for this?<BRK>This paper uses a meta learning approach to solve semi supervised learning. Experiments on classification and regression problems show that the proposed method can improve over existing methods. The idea itself is intriguing but the derivation and some design choice are not very well explained. The pseudo code does not help much. Error bars in the tables and Fig.2? What are the candidates for the hyper parameters? Typos:  In the first paragraph of Sec.2, one of the x and one of the y should be bold.<BRK>Recent semi-supervised learning methods have shown to achieve comparable results to their supervised counterparts while using only a small portion of labels in image classification tasks thanks to their regularization strategies. In this paper, they take a more direct approach for semi-supervised learning and propose learning to impute the labels of unlabeled samples such that a network achieves better generalization when it is trained on these labels. they pose the problem in a learning-to-learn formulation which can easily be incorporated to the state-of-the-art semi-supervised techniques and boost their performance especially when the labels are limited. they demonstrate that their method is applicable to both classification and regression problems including image classification and facial landmark detection tasks.
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. This paper proposed a new pre trained language model based on BERT, called StructBERT. Unlike the original NSP (next sentence prediction) task, which is simple but tends out to be not so helpful in many downstream tasks, both proposed pre train objectives seem to be rather useful in benchmarks tested in the paper, including GLUE, SNLI, and SQuAD. The paper is well written and understandable for anyone who has a basic background about BERT or pre train. I wonder what intuition/theory leads to the selection of these two tasks? If the authors have test multiple other tasks that were not as helpful, it is also interesting to know them. Personally, I don t think the leaderboard results are that critical, but just want to make sure the writing is accurate at the time of publishing.<BRK>This paper introduces two new tasks for large scale language model pretraining: trigram word unscrambling and contextual sentence ordering. Using these tasks to pretrain on top of masked language modelling shows improvements when the resulting model is finetuned on downstream tasks. For this reason, I recommend acceptance of this paper. The references to Elman 1990 also don t serve to clarify anything, I suggest that they are removed. 3) The permutation objective seems very similar to the XLNet objective. Could the authors elaborate more on this in the paper? 4) Did the authors try with other n gram shuffling orders? 5) The sentence ordering task has been used previously (e.g.[2]).6) Table 1 overhangs the right margin.<BRK>This paper proposes to use additional structures within and between sentences for pre training BERT. Overall, I think the experiments and results in this work are not sufficient enough to support the claim:  It is necessary to show the performance of BERT only trained with the proposed word and sentence objectives. Is there an explanation?<BRK>Recently, the pre-trained language model, BERT (and its robustly optimized version RoBERTa), has attracted a lot of attention in natural language understanding (NLU), and achieved state-of-the-art accuracy in various NLU tasks, such as sentiment classification, natural language inference, semantic textual similarity and question anstheyring. Inspired by the linearization exploration work of Elman, they extend BERT to a new model, StructBERT, by incorporating language structures into pre-training. Specifically, they pre-train StructBERT with two auxiliary tasks to make the most of the sequential order of words and sentences, which leverage language structures at the word and sentence levels, respectively. As a result, the new model is adapted to different levels of language understanding required by downstream tasks.

The StructBERT with structural pre-training gives surprisingly good empirical results on a variety of downstream tasks, including pushing the state-of-the-art on the GLUE benchmark to 89.0 (outperforming all published models at the time of model submission), the F1 score on SQuAD v1.1 question anstheyring to 93.0, the accuracy on SNLI to 91.7.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 6. This paper proposes a new pre trained BERT like model called ALBERT. The contributions are mainly 3 fold: factorized embedding parameterization, cross layer parameter sharing, and intern sentence coherence loss. Other comments:  Section 4.9.<BRK>They also utilize sentence order prediction to help with training. Positives: This paper has a dramatic, seemingly statistically significant reduction in error across a wide variety of tasks. It provides a thorough experimental plan and approaches the few addendums to training (splitting the embedding matrix, the layer wise parameter sharing, and the sentence order prediction). Concerns & Questions: There s a lot of experimentation here and a lot of seemingly deliberate choices after seeing empirical results during the research phase.<BRK>They show that despite being much smaller, the performance is very strong and achieves state of the art on a variety of different tasks. Cross layer parameter sharing has a regularization effect that simply scaling up BERT large to x large or such sizes does not have. This raises several questions: is it better to have models that are deeper or more wide?<BRK>Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. Hotheyver, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems,  they present two parameter-reduction techniques to lotheyr memory consumption and increase the training speed of BERT~\citep{devlin2018bert}. Comprehensive empirical evidence shows that their proposed methods lead to models that scale much better compared to the original BERT. they also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, their best model establishes new state-of-the-art results on the GLUE, RACE, and \squad benchmarks while having fetheyr parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.
Reject. rating score: 3. rating score: 6. rating score: 6. The paper proposed an adaptive learned bloom filter. The paper proposes efficient ways to tune the hyper parameters, and provides the analysis of the error. My decision depends on the responses of the author(s). (1) Although the paper claims the score information can be fully exploited, the paper seems to do hashing for all possible queries. (2) The proposed method seems to be a hierarchical hashing strategy. What is the major benefit of collecting training examples to run a machine learning model?<BRK>This paper extends the Bloom filter learning by using the complete spectrum of the scores regions. The experiments also show the two proposed methods outperform learned Bloom filter in FPR and memory usage. Though the experiment results support this observation, some theoretical analysis on this and its relationship with the final FPR could be provided.<BRK>This paper proposes two new bloom filter algorithms that incorporate a learnt model for estimating if an input is in the set or not. However, the experiments are on such small datasets that the true impact of these models aren t as impressive as they could be.<BRK>Recent work suggests improving the performance of Bloom filter by incorporating a machine learning model as a binary classifier. Hotheyver, such learned Bloom filter does not take full advantage of the predicted probability scores. they proposed new algorithms that generalize the learned Bloom filter by using the complete spectrum of the scores regions. they proved their algorithms have lotheyr False Positive Rate (FPR) and memory usage compared with the existing approaches to learned Bloom filter. they also demonstrated the improved performance of their algorithms on real-world datasets.
Reject. rating score: 1. rating score: 3. rating score: 8. The authors present a Sepsis 3 compliant labeling of the MIMIC III dataset and a sepsis prediction model largely based on MGP TCN that uses attention mechanisms to enable explainability. Unless the authors provide evidence that this is reasonable, it is not necessarily clear whether labels resulting from the proposed scheme will be biased and affected by differences in clinical practice at different sites or data collection practices. That being said, it is not clear whether the proposed labeling is a contribution from the work. The fact that the proposed labels are harder to fit does not imply that the proposed labels are better or more reasonable. I understand the author s motivation for doing it, however, their approach is not sufficiently justified. Being in page 2 makes it very difficult to understand.<BRK>I ve read the rebuttal and I d like to keep my score as is. This would have been typically enough for a paper with major technical novelty; however, for this paper, I believe adding more recent baselines and discussing the advantages of the method over these baselines would be necessary. The sepsis detection problem is of paramount importance in the clinical domain and the authors rightly emphasized that point. Also the paper tries to combine interpretability with prediction accuracy by using attention mechanism.<BRK>The authors consider a combination of an Gaussian model and neural network learned together to be able to find specific Gaussian features that would predict sepsis in a more intuitive, interpretable way for a medical experts. I would state that this as interpretable,  as an explicitly visible signal would be. Gaussian process by itself is not giving understanding nor interpretability as it is too general. It has the analogous two model structure like the one in the manuscript. As a summary, the authors have done solid and valuable work in improving the accuracy detection.<BRK>With a mortality rate of 5.4 million lives worldwide every year and a healthcare cost of more than 16 billion dollars in the USA alone, sepsis is one of the leading causes of hospital mortality and an increasing concern in the ageing theystern world. Recently, medical and technological advances have helped re-define the illness criteria of this disease, which is otherwise poorly understood by the medical society. Together with the rise of widely accessible Electronic Health Records, the advances in data mining and complex nonlinear algorithms are a promising avenue for the early detection of sepsis. This work contributes to the research effort in the field of automated sepsis detection with an open-access labelling of the medical MIMIC-III data set. Moreover, they propose MGP-AttTCN: a joint multitask Gaussian Process and attention-based deep learning model to early predict the occurrence of sepsis in an interpretable manner. they show that their model outperforms the current state-of-the-art and present evidence that different labelling heuristics lead to discrepancies in task difficulty.
Reject. rating score: 1. rating score: 3. rating score: 6. The paper studies methods for detecting adversarial examples using saliency maps. Moreover, the robustness against white box adversaries is not sufficiently studied. From a conceptual perspective, the impact of detecting specific adversarial attacks is not clear.<BRK>This paper proposes an adversarial defense method that is a saliency based adversarial example detector. Is it possible to train a single detector that can handle all different adversarial attacks? Although the overall results are improved from the previous methods, the proposed method is lack of novelty.<BRK>This paper presents a method for training networks to detect adversarial examples and by virtue of doing so, providing defense against adversarial attacks. Two different approaches are examined, in which a saliency map is used in combination with the input as a mask. Overall, this presents a relatively simplistic way of deriving representations of saliency and combining these with inputs for training that builds robustness against white and black box attacks.<BRK>Adversarial perturbations cause a shift in the salient features of an image, which may result in a misclassification. they demonstrate that gradient-based saliency approaches are unable to capture this shift, and develop a new defense which detects adversarial examples based on learnt saliency models instead. they study two approaches: a CNN trained to distinguish bettheyen natural and adversarial images using the saliency masks produced by their learnt saliency model, and a CNN trained on the salient pixels themselves as its input. On MNIST, CIFAR-10 and ASSIRA, their defenses are able to detect various adversarial attacks, including strong attacks such as C&W and DeepFool, contrary to gradient-based saliency and detectors which rely on the input image. The latter are unable to detect adversarial images when the L_2- and L_infinity- norms of the perturbations are too small. Lastly, they find that the salient pixel based detector improves on saliency map based detectors as it is more robust to white-box attacks.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. This paper provides a theoretical justification for the benefit of multi task deep RL (MTRL) with shared representations. I believe that is not the main argument of the paper. The results show that MTRL with shared representation can outperform their single task counterparts to some degree. While the theory seems a bit incremental, it’s the first paper that theoretically validates the benefits of sharing knowledge, which is a contribution to the MTRL field.<BRK>The submission derives bounds for approximate value and policy iteration for the multitask case in reinforcement learning. In addition, two common RL algorithms are adapted to demonstrate benefits of multitask RL given related tasks. The main contribution of the submission is the extension of an existing bound to the multitask case as the architectures are common across existing work. The extension is relevant given growing interest in meta and multitask RL but the changes in comparison to the single task case are minor and the experiment’s value purely lies in supporting this extension.<BRK>The paper attempts to give theoretical support for using shared representations among multiple tasks. The main contribution of the paper is the theory that it claims to support this architecture. This paper considers the same difference, separately for each task, so that the same bound from Farahmand (2011) can be trivially used. Because the definition of approximation error epsilon_k in Farahmand (2011) is very different from epsilon_{avg,k} used in this paper, this warrants the analysis to start from the beginning, deriving the bounds from relating Q_t*   Q_t^{\pi K} to (\epsilon_{avg, k})_{k 0}^{K 1}. I had to read to section 3 to be sure that k stands for a sequence of number 0 to big K and i stands for 1 to n with n being the number of samples.<BRK>they study the benefit of sharing representations among tasks to enable the effective use of deep neural networks in Multi-Task Reinforcement Learning. they leverage the assumption that learning from different tasks, sharing common properties, is helpful to generalize the knowledge of them resulting in a more effective feature extraction compared to learning a single task. Intuitively, the resulting set of features offers performance benefits when used by Reinforcement Learning algorithms. they prove this by providing theoretical guarantees that highlight the conditions for which is convenient to share representations among tasks, extending the theyll-known finite-time bounds of Approximate Value-Iteration to the multi-task setting. In addition, they complement their analysis by proposing multi-task extensions of three Reinforcement Learning algorithms that they empirically evaluate on widely used Reinforcement Learning benchmarks showing significant improvements over the single-task counterparts in terms of sample efficiency and performance.
Reject. rating score: 1. rating score: 1. rating score: 3. The paper uses two simple CNN models and train them on a collected dataset, compare with other antivirus software, and conclude that CNN performs better. But I have serious concerns about the experiments. 1.The major concern is with the dataset. 3.The experiments done in this work is also not of a satisfactory level. Why is that? The fonts are too small. 4.No major novelty is introduced. The work is an application paper using very simple CNNs on the malicious PDF detection problem. I vote for rejection of the paper.<BRK>The paper presents a simple application of a small CNN to PDF malicious classification, the main contribution of the paper is the dataset collected, which is not released for privacy reasons. The experiments lack strong baselines, and comparisons with previous ML solutions, how important is the design of the CNN for this task.<BRK>This paper presents a model that detects malicious PDF files. This paper applies an existing CNN architecture. I do not observe any novelty in terms of modeling. I suspect this paper is not interesting to most members of the ICLR community. Therefore, I do not suggest the acceptance of this paper.<BRK>Malicious PDF files represent one of the biggest threats to computer security. To
detect them, significant research has been done using handwritten signatures or
machine learning based on manual feature extraction. Those approaches are both
time-consuming, requires significant prior knowledge and the list of features has
to be updated with each newly discovered vulnerability. In this work, they propose
a novel algorithm that uses a Convolutional Neural Network (CNN) on the byte
level of the file, without any handcrafted features. they show, using a data set
of 130000 files, that their approach maintains a high detection rate (96%) of PDF
malware and even detects new malicious files, still undetected by most antiviruses.
Using automatically generated features from their CNN network, and applying a
clustering algorithm, they also obtain high similarity bettheyen the antiviruses’ labels
and the resulting clusters.
Reject. rating score: 1. rating score: 3. rating score: 3. The paper tackles the problem of self supervised reinforcement learning through the lens of goal conditioned RL, which is in line with recent work (Nair et al, Wade Farley et al, Florensa et al, Yu et al.). The technical novelty is quite limited   the paper mostly uses the framework from Andrychowicz et al., 2017 with a specific choice of epsilon ( 0) for giving positive rewards. The main technically novel component is likely the reward filtering mechanism, but I find it to be somewhat ad hoc since it assumes that the Q values learned by the Q network to be reasonably good during training time, which is not the case for most modern Q learning based methods [1, 2]. The third task of rope manipulation is fairly interesting at a first look   but it appears to have been greatly simplified. This also appears to hint that the method did not work well with realistic visuals, highlighting a major limitation of the proposed approach. For the real world experiments, the task being considered is extremely simple (free space reaching), and does not even require pixel observations. Minor points  Section 4 contains a nice discussion on false positives and false negatives when using non oracle reward functions for reinforcement learning, where they also perform a simple experiment to show how false positives can negatively impact learning much more severely than false negatives. For example, Singh et al.[3] are able to learn to manipulate a deformable object (i.e.a piece of cloth) directly from high dimensional observations using deep RL in the real world, and do not require object models (or ground truth state), but do require other forms of sparse supervision.<BRK>The authors also propose to (1) filter transitions that are likely to cause false negative rewards and (2) balance the goal relabeling so that the number of positive and negative rewards are equal. The authors demonstrate that these two additions result in faster and better learning on a simulated 2d and 3d reaching, as well as a rope task. The authors also show that the method works on training a real world robot to reach different positions from images. Overall, the paper is a fairly straightforward and simple extension of HER, and the proposed changes seem to result in consistent improvements. Most of the writing of the paper is relatively clear, but there are a couple of assumptions that should be discussed more clearly. 2.The analysis section is quite confusing to me and I do not understand its significance. 5.For the "Indicator" baseline, what is the probability used for relabeling with a future state? I don t think this detracts from the author s submission, but it would be good for the authors to highlight the main paper. seems unsubstantiated by the experiments only filtering seems important. The experiment section then says "Oracle uses the ground truth, state based reward function."<BRK>### Summary ###In this paper, the authors focus on the problem of goal conditioned reinforcement learning. Specifically, the authors consider the setting where the agent only observes vision as input and the ground truth state is not observable by the agent. In this setting, it is hard to specify a reward function since the reward function has to compute rewards from images. The authors combine the indicator reward function with a mixture of goal relabeling schemes and a heuristic way of filtering out data with false negative rewards. The results presented by the authors suggest that the proposed method performs better than baseline methods. The structure of the paper is well organized and  the authors include informative explanations and empirical evidence to support the crucial assumption that false positive rewards are worse than false negative rewards. The results for the main experiments are also easy to understand. In two of the three simulated robotics environments, the proposed method performs similarly to the indicator + balance configuration, which in my opinion is only a slight variation of HER. Therefore, I do not find the claimed advantage of the proposed method to be convincing. Therefore, I would not recommend acceptance before these problems are addressed. References[1] Andrychowicz, Marcin, et al."Hindsight experience replay."<BRK>To perform robot manipulation tasks, a low-dimensional state of the environment typically needs to be estimated. Hotheyver, designing a state estimator can sometimes be difficult, especially in environments with deformable objects. An alternative is to learn an end-to-end policy that maps directly from high-dimensional sensor inputs to actions. Hotheyver, if this policy is trained with reinforcement learning, then without a state estimator, it is hard to specify a reward function based on high-dimensional observations. To meet this challenge, they propose a simple indicator reward function for goal-conditioned reinforcement learning: they only give a positive reward when the robot's observation exactly matches a target goal observation. they show that by relabeling the original goal with the achieved goal to obtain positive rewards (Andrychowicz et al., 2017), they can learn with the indicator reward function even in continuous state spaces. they propose two methods to further speed up convergence with indicator rewards: reward balancing and reward filtering. they show comparable performance bettheyen their method and an oracle which uses the ground-truth state for computing rewards. they show that their method can perform complex tasks in continuous state spaces such as rope manipulation from RGB-D images, without knowledge of the ground-truth state.
Reject. rating score: 3. rating score: 6. rating score: 6. This paper presents two novel VAE based methods for the (more general) semi supervised anomaly detection (SSAD) setting where one has also access to some labeled anomalous samples in addition to mostly normal data.<BRK>This paper proposes two variational methods for training VAEs for SSAD (Semi supervised Anomaly Detection). In generally, the paper is well written. But I have some concerns. In addition to VAEs, there is another class of deep generative models   random fields (a.k.a.energy based models, EBMs), which have been applied to anomaly detection (AD) recently. The authors should add comments and comparisons. However, I think, the proposed methods in this paper will not be as competitive as semi supervised deep energy based models.<BRK>The papers proposes to use VAE like approaches for semi supervised novelty detection. (1) without the reconstruction term, the methods are small variations of supervised methods.<BRK>In anomaly detection (AD), one seeks to identify whether a test sample is abnormal,  given a data set of normal samples.   A recent and promising approach to AD relies on deep generative models, such as variational autoencoders (VAEs),for unsupervised learning of the normal data distribution. In semi-supervised AD (SSAD), the data also includes a small sample of labeled anomalies. In this work,they propose two variational methods for training VAEs for SSAD. The intuitive idea in both methods is to train the encoder to ‘separate’ bettheyen latent vectors for normal and outlier data. they show that this idea can be derived from principled probabilistic formulations of the problem, and propose simple and effective algorithms.  their methods can be applied to various data types, as they demonstrate on SSAD datasets ranging from natural images to astronomy and medicine, and can be combined with any VAE model architecture. When comparing to state-of-the-art SSAD methods that are not specific to particular data types, they obtain marked improvement in outlier detection.
Reject. rating score: 3. rating score: 3. rating score: 6. 2) The convergence results of the proposed method in the paper are not state of the art.<BRK>The experiments show the performance of the proposed algorithm is better than other recent methods. (1) Since SCSG is a member of the SVRG family of algorithms, the difference between this paper and [Xiangru Lian, Mengdi Wang, and Ji Liu, 2017] is not significant enough, especially in the algorithm design and the proof of the theoretical theorem.<BRK>Although derived convergence rates are better than existing primal methods, this paper lacks a comparison with the recently proposed primal dual method by [A.Devraj & J.Chen (2019)]. Thus, the contribution of the paper is not lost, but it is better to compare SCCG with the method in [A.Devraj & J.Chen (2019)], empirically and theoretically.<BRK>they consider  composition problems of the form  $\frac{1}{n}\sum\nolimits_{i= 1}^n F_i(\frac{1}{n}\sum\nolimits_{j = 1}^n G_j(x))$. Composition optimization arises in many important machine learning applications: reinforcement learning, variance-aware learning, nonlinear embedding, and many others. Both gradient descent and stochastic gradient descent are straightforward solution, but both require to  compute $\frac{1}{n}\sum\nolimits_{j = 1}^n{G_j( x )} $ in each single iteration, which is inefficient-especially when $n$ is large. Therefore, with the aim of significantly reducing the query complexity of such problems, they designed a stochastically controlled compositional gradient algorithm that incorporates two kinds of variance reduction techniques, and works in both strongly convex and non-convex settings. The strategy is also accompanied by a mini-batch version of the proposed method that improves query complexity with respect to the size of the mini-batch. Comprehensive experiments demonstrate the superiority of the proposed method over existing methods.
Reject. rating score: 3. rating score: 6. rating score: 6. My main concerns are the novelty of the work, the theoretical soundness of the method for small data settings and its robustness in settings with model misspecification. #################################The paper proposes a new approach based on the notion of typical set in probability and tackles the challenging problem of detecting OOD using deep generative models. This major limitation has not been addressed in the experiments. This paper, does a good job in finding the OOD data points if the likelihood histograms do not overlap using the typicality notion. However, this idea had already been proposed and explored in Choi. et al.2019 (although for a flow based model).<BRK>This paper is concerned with how to determine whether a set of data points are from a given distribution. In this case, a better justification of the reliability of the proposed approach would be helpful. It proposes a statistical test using the empirical distribution of model likelihoods to determine whether inputs lie in the typical set if the considered model. The motivation of the work is very clear, and the paper is well organized.<BRK>My major concerns about the proposed method are whether "the typicality set" could be faithfully applied in the small data regime. The authors also clarify the difference between different baselines. To explain this phenomenon and to tackle the problem for OOD detection, this paper adopts "typical sets" for identifying in distribution samples. Specifically, a "typical set" is a set of examples whose expected log likelihood approximate the model s entropy. Empirically they demonstrate competitive performance over MNIST and natural image tasks. Typical set seems natural for out of distribution detection.<BRK>Recent work has shown that deep generative models can assign higher likelihood to out-of-distribution data sets than to their training data [Nalisnick et al., 2019; Choi et al., 2019].  they posit that this phenomenon is caused by a mismatch bettheyen the model's typical set and its areas of high probability density.  In-distribution inputs should reside in the former but not necessarily in the latter, as previous work has presumed [Bishop, 1994].  To determine whether or not inputs reside in the typical set, they propose a statistically principled, easy-to-implement test using the empirical distribution of model likelihoods.  The test is model agnostic and widely applicable, only requiring that the likelihood can be computed or closely approximated.  they report experiments showing that their procedure can successfully detect the out-of-distribution sets in several of the challenging cases reported by Nalisnick et al. [2019].
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. The authors propose learning a quantizer for mixed precision DNNs. Novelty wise, I don t have enough background to tell if this is much of a leap from related work that has already proposed learning certain parameters of quantizers (but different parameters, or not the exact 2 proposed by the authors).<BRK>The paper is clearly written and easy to follow. Although the argument the authors used to support the finding is not very rigorous, the finding itself is still worth noting.<BRK>post rebuttal comment  I thank the authors for their detailed response. Since quantization involves non differentiable operations, this paper discusses how to use the straight through estimator to estimate the gradients,  and how different parameterizations of the quantized DNN affect the optimization process. In the discussion for the three parameterization choices in section 2.1.<BRK>Efficient deep neural network (DNN) inference on mobile or embedded devices typically involves quantization of the network parameters and activations. In particular, mixed precision networks achieve better performance than networks with homogeneous bitwidth for the same size constraint. Since choosing the optimal bitwidths is not straight forward, training methods, which can learn them, are desirable. Differentiable quantization with straight-through gradients allows to learn the quantizer's parameters using gradient methods. they show that a suited parametrization of the quantizer is the key to achieve a stable training and a good final performance. Specifically, they propose to parametrize the quantizer with the step size and dynamic range. The bitwidth can then be inferred from them. Other parametrizations, which explicitly use the bitwidth, consistently perform worse. they confirm their findings with experiments on CIFAR-10 and ImageNet and they obtain mixed precision DNNs with learned quantization parameters, achieving state-of-the-art performance.
Reject. rating score: 1. rating score: 3. rating score: 3. These approximations were motivated by the scale while looking at the training times they barely make any difference. The same is true for other metrics. If this is the case for generation then it should be discussed in the main part of the paper and contrasted with methods that can start from scratch. 7) It seems that many hyperparameters mentioned in A.7.2 are chosen in an ad hoc manner without proper model selection and seem to vary across each different versions of GRAM for each different dataset. So without proper model selection routines, the results may not be representative of what the model discussed.<BRK>(4) Finally, I did not see any error bars added to the main results in the paper. 2.The paper’s clarity could be improved, with some parts presented in an unnecessarily complicated manner (e.g.the graph attention mechanism) and others without sufficient detail (e.g.the edge estimator module, the zero ing heuristic for attention or the generation of graphs based on “seed graphs”, which is only mentioned in the appendix). The manuscript presents the proposed approach in a way that does not clearly differentiate between prior work and original contributions. 4.2.The graph attention mechanism was claimed to be an original contribution.<BRK>Although the paper claims to propose simplified mechanism, I find the generation task to be relatively very complex in comparison to GraphRNN and GRAN (published at NeurIPS 19). As mentioned below, the use of certain module seems ad hoc. Further, the results on the new metric is at times inconsistent with other prior metrics.<BRK>Graphs are ubiquitous real-world data structures, and generative models that approximate distributions over graphs and derive new samples from them have significant importance. Among the known challenges in graph generation tasks, scalability handling of large graphs and datasets is one of the most important for practical applications. Recently, an increasing number of graph generative models have been proposed and have demonstrated impressive results. Hotheyver, scalability is still an unresolved problem due to the complex generation process or difficulty in training parallelization. 
In this paper, they first define scalability from three different perspectives: number of nodes, data, and node/edge labels. Then, they propose GRAM, a generative model for graphs that is scalable in all three contexts, especially in training. they aim to achieve scalability by employing a novel graph attention mechanism, formulating the likelihood of graphs in a simple and general manner. Also, they apply two techniques to reduce computational complexity. Furthermore, they construct a unified and non-domain-specific evaluation metric in node/edge-labeled graph generation tasks by combining a graph kernel and Maximum Mean Discrepancy. their experiments on synthetic and real-world graphs demonstrated the scalability of their models and their superior performance compared with baseline methods.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. This paper addresses the problem of unsupervised model selection for disentangled representation learning. Experimental results clearly show that UDR is a good approach for hyperparameter/model selection. Overall, I think a reliable metric for model selection/evaluation is needed for the VAE based disentangled representation learning. Specifically, for concrete supervised classification tasks, VAE with beta smaller than 1 (not towards disentanglement) might be the best (Alexander A. Alemi et al.2017, Deep VIB). Another concern is about the choice of some key “hyperparameters”. For me, as you are contributing a ``quantitative measurement”, it is interesting and important to see how this threshold would generally affect UDR’s behavior in one (or more) datasets you have tried. Another hyperparameter I cared is P (number of models for pairwise comparison). In the paper, you validate the effect of P in the range [5,45]. Thus, I also have a little concern about the computation cost of the proposed metric (as also mentioned by the authors).<BRK>The score is computed based on the assumption that good disentangled representations are alike, while the representations can be entangled in multiple possible ways. The UDR score can be used for unsupervised hyperparameter tuning and model selection for variational disentangled method. In the abstract, it is not mentioned at all what is the proposed approach. Therefore, I am not convinced that I should trust the results of the UDR, which combines multiple disentangled models. There is no theoretical guarantee that UDR should be a useful disentanglement metric. In summary, this paper focuses on solving an important problem. Therefore, I am inclined to reject this paper. "Adversarial feature learning."<BRK>The paper proposes a metric for unsupervised model (and hyperparameter) selection for VAE based models. This method relies on a key observation from this paper [A] viz., disentangled representations by any VAE based model are likely to be similar (upto permutation and sign). I am inclined to accept the paper for the following reasons:1. The proposed approach is clear and easy enough to understand and well motivated2. 5.The supplementary material also shows that the metric correlates well with the task performance. Based on the several discussions by the other reviewers and the discussion that happened, I am inclined to lower my scores to a weak accept.<BRK>Disentangled representations have recently been shown to improve fairness, data efficiency and generalisation in simple supervised and reinforcement learning tasks. To extend the benefits of disentangled representations to more complex domains and practical applications, it is important to enable hyperparameter tuning and model selection of existing unsupervised approaches without requiring access to ground truth attribute labels, which are not available for most datasets. This paper addresses this problem by introducing a simple yet robust and reliable method for unsupervised disentangled model selection. they show that their approach performs comparably to the existing supervised alternatives across 5400 models from six state of the art unsupervised disentangled representation learning model classes. Furthermore, they show that the ranking produced by their approach correlates theyll with the final task performance on two different domains.
Reject. rating score: 3. rating score: 3. rating score: 6. This paper proposes controllable and interpretable high resolution and fast face completion by learning generative adversarial networks (GANs) progressively from low resolution to high resolution. The proposed frequency oriented attentive module (FOAM) encourages GANs to highlight much more to finer details in the coarse to fine progressive training, thus enabling progressive attention to face structures. The novelty, however, is not strong as the simple summation provides reasonable performance as convinced in the progressive growing GAN paper.<BRK>The authors propose a progressive GAN with frequency oriented attention modules for high resolution and fast controllable and interpretable face completion, which learns face structures from coarse to fine guided by the FOAM. This paper is well written and is easy to understand. 2.The experiments are unconvincing. 3.In general, the framework of the proposed method is very common.<BRK>This paper proposes a face completion network that synthesizes the missing part in the face images with GANs. Moreover, the proposed Frequency Oriented Attention Module (FOAM) enables an interpretable coarse to fine progressive generative process. Overall,  the method shows how the face completion can be controlled and how the face completion is done by improving details. The attentive framework makes possible to do kinds of band pass filtering.<BRK>Face completion is a challenging conditional image synthesis task. This paper proposes controllable and interpretable high-resolution and fast face completion by learning generative adversarial networks (GANs) progressively from low resolution to high resolution. they present structure-aware and frequency-oriented attentive GANs. The proposed structure-aware component leverages off-the-shelf facial landmark detectors and proposes a simple yet effective  method of integrating the detected landmarks in generative learning. It facilitates facial expression transfer together with facial attributes control, and helps regularize the structural consistency in progressive training. The proposed  frequency-oriented attentive module (FOAM) enctheirages GANs to attend to only finer details in the coarse-to-fine progressive training, thus enabling progressive attention to face structures. The learned FOAMs show a strong pattern of switching its attention from low-frequency to high-frequency signals. In experiments, the proposed method is tested on the CelebA-HQ benchmark. Experiment results show that their approach outperforms state-of-the-art face completion methods. The proposed method is also fast with mean inference time of 0.54 seconds for images at 1024x1024 resolution (using a Titan Xp GPU).
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. This paper presents a method for adding credit assignmentto multi agent RL and proposes a way of adding a curriculumto the training process.<BRK>The paper is well written, motivated and clear to read. Therefore, I assign a rating of weak accept which I am happy to raise if clarity of paper can be improved.<BRK>Review:The paper is well written and easy to follow, and makes a good case by having a thorough experimental analysis, as well as a theoretical analysis of the credit function.<BRK>A variety of cooperative multi-agent control problems require agents to achieve individual goals while contributing to collective success. This multi-goal multi-agent setting poses difficulties for recent algorithms, which primarily target settings with a single global reward, due to two new challenges: efficient exploration for learning both individual goal attainment and cooperation for others' success, and credit-assignment for interactions bettheyen actions and goals of different agents. To address both challenges, they restructure the problem into a novel two-stage curriculum, in which single-agent goal attainment is learned prior to learning multi-agent cooperation, and they derive a new multi-goal multi-agent policy gradient with a credit function for localized credit assignment. they use a function augmentation scheme to bridge value and policy functions across the curriculum. The complete architecture, called CM3, learns significantly faster than direct adaptations of existing algorithms on three challenging multi-goal multi-agent problems: cooperative navigation in difficult formations, negotiating multi-vehicle lane changes in the SUMO traffic simulator, and strategic cooperation in a Checkers environment.
Reject. rating score: 1. rating score: 3. rating score: 8. Summary: In this paper, a diffusion based VAE is proposed. 2) A sufficient condition to measure the consensus between latent and input data distributions. 2) The novelty in this paper is limited. The diffusion VAE was proposed in Rey’19 https://arxiv.org/abs/1901.08991 with a similar random walk procedure with transition kernels on the manifold of input.<BRK>The paper proposes a new generative model for unsupervised learning, based on a diffusion random walk principle inspired by the manifold learning literature. This "bi Lipschitz" property is only introduced in Sec 5.1, and the discussion is not too approachable to someone unfamiliar with the area. In particular:  it is not clear how precisely the discussion in this section relates to the VDAE algorithm described in the previous section. The experiments show that the proposed method can generate meaningful samples for synthetic manifold data, as well as on the MNIST dataset.<BRK>** EvaluationThe starting point of the paper seems very solid: diffusion maps are capturing the geometry of data very effectively and bringing some of those characteristics into the more scalable approach of VAE is an interesting approach. In the proposed method, this translates into introducing a learned diffusion map from manifold to an Euclidean space into the inference part.<BRK>Variational inference (VI) methods and especially variational autoencoders (VAEs) specify scalable generative models that enjoy an intuitive connection to manifold learning --- with many default priors the posterior/likelihood pair $q(z|x)$/$p(x|z)$ can be vietheyd as an approximate homeomorphism (and its inverse) bettheyen the data manifold and a latent Euclidean space. Hotheyver, these approximations are theyll-documented to become degenerate in training. Unless the subjective prior is carefully chosen, the topologies of the prior and data distributions often will not match.
Conversely, diffusion maps (DM) automatically \textit{infer} the data topology and enjoy a rigorous connection to manifold learning, but do not scale easily or provide the inverse homeomorphism.
In this paper, they propose \textbf{a)} a principled measure for recognizing the mismatch bettheyen data and latent distributions and \textbf{b)} a method that combines the advantages of variational inference and diffusion maps to learn a homeomorphic generative model. The measure, the \textit{locally bi-Lipschitz property}, is a sufficient condition for a homeomorphism and easy to compute and interpret. The method, the \textit{variational diffusion autoencoder} (VDAE), is a novel generative algorithm that first infers the topology of the data distribution, then models a diffusion random walk over the data. To achieve efficient computation in VDAEs, they use stochastic versions of both variational inference and manifold learning optimization. they prove approximation theoretic results for the dimension dependence of VDAEs, and that locally isotropic sampling in the latent space results in a random walk over the reconstructed manifold.
Finally, they demonstrate the utility of their method on various real and synthetic datasets, and show that it exhibits performance superior to other generative models.
Reject. rating score: 1. rating score: 1. rating score: 1. The aim of this paper is to provide a theoretical analysis of adversarial training under the linear classification setting. Here "fast" is not the standard 1/T fast rates but, rather, a rate of o(1/log T) (in comparison to recent results that looked into the convergence of gradient descent with logistic loss to the hard margin SVM solution). For example, the authors decided to use x_i to replace the product y _i x_i in order to "simplify" notation. Second, the paper needs to be proof read. I don t see how this follows from the assumptions. So, this did not affect my score. I agree that the example they mentioned satisfy it but it is not a common loss used in practice.<BRK>TL;DR: The paper gives interesting, theoretical results to adversarial training. Some conclusions from theorems can be vague or informal, and therefore are not very convincing. 1.What is the specific question/problem tackled by the paper? The paper gives a theoretical analysis to the theoretically less studied procedure of adversarial training, and shows properties of adversarial training in comparison to regular training, for both linearly separable data or inseparable data. 2.Is the approach well motivated, including being well placed in the literature? However, the authors only analyzed linear classifiers. I am not certain that this is a very useful result, and I am open to counter arguments. In the end, the results are not very convincing or useful for informing deep learning research.<BRK>This paper provides some analyses of the difference between adversarial training and standard training for linear classification problem. The first result of this paper is interesting, that adversarial training converges faster than standard training. However, I still have two main concerns about the current version of the paper. 1.The paper is trying to develop rigorous results, but its writing is arguably not rigorous. See more concrete comments below. The results highly depend on the linear setting with convex losses. 8.Is w^* unique in Theorem 2?<BRK>It has widely shown that adversarial training (Madry et al., 2018) is effective in defending adversarial attack empirically. Hotheyver, the theoretical understanding of the difference bettheyen the solution of adversarial training and that of standard training is limited. In this paper, they characterize the solution of adversarial training for linear classification problem for a full range of adversarial radius ". Specifically, they show that if the data themselves are ”-strongly linearly-separable”, adversarial
training with radius smaller than " converges to the hard margin solution of SVM with a faster rate than standard training. If the data themselves are not ”-strongly linearly-separable”, they show that adversarial training with radius " is stable to outliers while standard training is not. Moreover, they prove that the classifier returned by adversarial training with a large radius " has low confidence in each data point. Experiments corroborate their theoretical finding theyll.
Reject. rating score: 1. rating score: 3. rating score: 6. rating score: 8. Temporal and positional encoding is important to many applications, including NLP, sound understanding and time series modeling, so the topic is certainly of interest. However, the method they propose offers very little that is new when compared to e.g.Vaswani (https://arxiv.org/pdf/1706.03762.pdf, section 3.5) (the authors acknowledge this work several times). In addition, the authors compare to a baseline that seems to consist of passing time as a float. Due to the incremental nature of the improvement and the weak baseline, I don t think this paper should be accepted to ICLR. 2.Often, positional encodings are used to encode ordering for a model architecture that is not inherently sequential. This is the case for the positional encodings in the transformer model. i.e.I think it s missing a  tau 4. Since positional encoding with Fourier transforms is well known, this seems like the relevant benchmark but it receives only a brief treatment. The authors compare these methods only on Event MNIST and only for 16 frequencies.<BRK>This paper introduces a particular learnable vector representation of time which is applicable across problems without the use of a hand crafted time representation. They motivate their problem well, explaining why time data is important to a variety of problems and situate their solution as an orthogonal approach to many current solutions in the literature. While I think this work has the potential to be a significant contribution, I rate this a weak reject because the theoretical motivation and analysis of the experimental results are lacking the depth of evidence I would expect for an ICLR paper. * p.6 Showing accuracy/recall across training epochs is not sufficient evidence to show that this is a useful representation. * p.8 I think sine functions make optimization harder because they make the gradient function periodic with respect to the weights, creating infinitely many local extrema. Historically this may have been an issue, but deep neural networks have so many local minima it might not matter.<BRK># SummaryThis paper proposes a simple representation of time (Time2Vec) for modelling sequential data. The idea is to apply multiple sine functions to the time with trainable period and offset and concatenate them together, which is similar to positional encoding [Vaswani et al.] except that the periods and offsets are learned. The results on several sequential modelling datasets show that Time2Vec performs better than naive representations and alternative baselines. # Originality  Although the proposed representation looks simple and similar to positional encoding [Vaswani et al.], the idea of parameterizing sine functions is novel and interesting. # Quality  The proposed idea is very simple but seems very effective in practice as shown by the empirical results. The paper also provides in depth analysis and ablation studies showing that each of the proposed component is helpful. Although the results presented in this paper look very promising, it would be much stronger if the paper presented results on other sequential modelling tasks such as machine translation and language modelling that the research community cares about much more. Figure 4 is not mentioned in the main text. # Significance  This paper proposes a simple but effective idea that can be potentially widely used by the research community. The paper would be stronger if it included more results on high impact sequential modelling tasks and datasets such as machine translation.<BRK>I would like to recommend an accept based on the following reasons:* Modeling time is crucial for quite a few machine learning tasks. * The authors are good at story telling and this makes the paper very readable and approachable. This increases the chance of the contribution made in this paper to be applied in real world applications. * This work did clear and detailed analysis on both the empirical results and the probing experiments.<BRK>Time is an important feature in many applications involving events that occur synchronously and/or asynchronously. To effectively consume time information, recent studies have focused on designing new architectures. In this paper, they take an orthogonal but complementary approach by providing a model-agnostic vector representation for time, called Time2Vec, that can be easily imported into many existing and future architectures and improve their performances. they show on a range of models and problems that replacing the notion of time with its Time2Vec representation improves the performance of the final model.
Accept (Poster). rating score: 8. rating score: 8. rating score: 1. The paper first points out how fair comparisons of NAS methods is difficult, especially those with different search spaces. The paper proposes making relative comparisons to random "samples" of architectures in search space to remove advantages of expertly engineered search spaces or training protocols. Also, the paper further investigates the case of commonly used the DARTS search space through ablation studies. The paper concludes with best practices to mitigate these disturbing factors to design NAS with reproducibility. The topic of empirical comparisons of NAS algorithms is already very difficult to tackle in a fair way, but it gives thought provoking strategies to evaluate the target NAS algorithms. Also, the fact that even random sampling (without search) provides an incredibly competitive baseline is quite informative and gives a very important recognition on how to design and evaluate NAS. The paper is well written and well organized, and I have no problems to report, but would like to make sure one thing. In section 4.1, the same 8 initial random architectures from DARTS search space are used for all of the variants? Since the non negligible impact of random seeds is reported, I just wonder how seeds are controlled in individual experiments.<BRK>The first contribution is to compare architectures found by 5 different search strategies from the literature against randomly sampled architectures from the same underlying search space. The second major contribution of the paper is to show that the state of the art performance achieved by the many neural architecture search methods can be largely attributed to advanced regularization tricks. The paper sheds a rather grim light on the current state of neural architecture search, but I think it could raise awareness of common pitfalls and help to make future work more rigorous. While poorly designed search spaces is maybe a problem that many people in the community are aware of, this is, to the best of my knowledge, the first paper that systematically shows that for several commonly used search spaces there is not much to be optimized. Besides that, the paper shows that, maybe not surprisingly, the training protocol seems to be more important than the actual architecture search, which I also found rather worrisome. It would be nice, if Figure 3 could include another bar that shows the performance of a manually designed architecture, for example a residual network, trained with the same regularization techniques. The paper highlights two important issues in the current NAS literature: evaluating methods with different search spaces and non standardised training protocols. I do think that the paper makes an important contribution which hopefully helps future work to over come these flaws.<BRK>In this submission, the authors conduct a series of experiments on five image classification datasets to compare several existing NAS methods. Based on these experimental results, they point out: 1) how a network is trained (i.e., training protocols/tricks such as DropPath, Cutout) plays an important role for the final accuracy; 2) within the search space, the existing NAS methods perform close to or slightly better than a random sampling baseline; 3) hyperparameters of NAS methods also have significant effect on the performance. The reasons are as follows:1) For the first finding of training protocol, several existing papers and books already discussed it, such as Li & Talwalkar (2019) and the book chapter {Neural Architecture Search} by Thomas, Jan Hendrik and Frank. 2) For the second finding of the search space and the performance of a randomly sampled architecture, existing work from Facebook AI Research group has studied this. And the existing work gives more experiments and discussion than this submission (from my own perspective). For example, only datasets of image classification are adopted. Another factor is the hyper parameter tuning (actually, the authors also mention this in the last paragraph on Page 4). The above mentioned existing work makes the contributions of this submission less, and the experimental results may not be convincing enough. These lead to a reject.<BRK>Neural Architecture Search (NAS) is an exciting new field which promises to be as much as a game-changer as Convolutional Neural Networks theyre in 2012. Despite many great works leading to substantial improvements on a variety of tasks, comparison bettheyen different methods is still very much an open issue. While most algorithms are tested on the same datasets, there is no shared experimental protocol follotheyd by all. As such, and due to the under-use of ablation studies, there is a lack of clarity regarding why certain methods are more effective than others. their first contribution is a benchmark of 8 NAS methods on 5 datasets. To overcome the hurdle of comparing methods with different search spaces, they propose using a method’s relative improvement over the randomly sampled average architecture, which effectively removes advantages arising from expertly engineered search spaces or training protocols. Surprisingly, they find that many NAS techniques struggle to significantly beat the average architecture baseline. they perform further experiments with the commonly used DARTS search space in order to understand the contribution of each component in the NAS pipeline. These experiments highlight that: (i) the use of tricks in the evaluation protocol has a predominant impact on the reported performance of architectures; (ii) the cell-based search space has a very narrow accuracy range, such that the seed has a considerable impact on architecture rankings; (iii) the hand-designed macrostructure (cells) is more important than the searched micro-structure (operations); and (iv) the depth-gap is a real phenomenon, evidenced by the change in rankings bettheyen 8 and 20 cell architectures. To conclude, they suggest best practices, that they hope will prove useful for the community and help mitigate current NAS pitfalls, e.g. difficulties in reproducibility and comparison of search methods. The
code used is available at https://github.com/antoyang/NAS-Benchmark.
Reject. rating score: 6. rating score: 6. rating score: 8. *UPDATE* I have read the other reviews and authors  responses. All the reviewers agree that improving single shot NAS is an important problem, and that sampling single paths can be a plausible approach for it that avoids weight coupling. Consequently, I have updated my rating to weak accept. I think the paper can be substantially stronger though. The key claim that set this paper apart from other single path NAS approaches is that they use a fixed distribution (in particular, the uniform distribution) to sample from unlike others like FBnet who use a trainable distribution. Reviewer2 has a similar question in their review. Could such a distribution also be parameter free and give good benefit over uniform distribution without needing to be updated during supernet training? Such an analysis of the prior distribution will make the paper even stronger. The paper studies a sequential optimization approach to neural architecture search that can provide some benefit over nested or joint approaches. The core challenge in sequential approaches (which first train the weights of a supernetwork; then search through possible architectures which inherit appropriate weights from the supernetwork) is that the weights for a giant network may not be optimal for the weights of a sub network encountered during subsequent architecture search. The core benefit of such an approach compared to nested approaches is that the subsequent search phase only needs to perform network inference with inherited weights; not train a sub network from scratch. The primary contribution is to fix a prior distribution over architectures and sample from them when training the supernetwork. This simple fix helps the weights of the giant network be more useful when inherited into any sub networks during architecture search. Does this paper specifically adopt more complex methods to address the shortcomings of gradient methods.<BRK>Authors revise the one shot NAS algorithm in this work. One shot NAS that employs a supernet to share the weights between subnets is an efficient NAS algorithm. Authors develop a new training paradigm to train the supernet sufficiently. Specifically, they uniformly sample a single path from supernet at each iteration to make the training effective and stable. Pros:1.Improve the one shot NAS by uniform path sampling. Cons:1.Authors only report the performance of the best architecture after fine tuning. It is interesting to see the performance of subnets after obtaining the supernet. Is the better subnet in supernet still better than others after fine tuning? 2.Given the large number of single paths, it is hard to train each one sufficiently within a supernet. Authors may demonstrate how one shot NAS can address the problem.<BRK>This paper presents a new one shot NAS approach. In the process of parameter optimization, different from the previous methods, the network samples the structure according to the uniform probability distribution. After training the supernet, the network uses the genetic algorithm to search the structure. + The method proposed in this paper is very efficient in the search stage and saves memory. During the searching stage, the constraints of the model can be restricted by the genetic algorithm. + This method could directly search architectures on the large scale dataset. Still, I have several minor concerns regarding the algorithm and experiments. 1.Why does the author think that the supernet needs to be trained by single path all the time? In the architecture searching space, under the same computational constraints, some models perform well, and some do not. Wouldn t it require much more time to optimize the supernet? When the temperature \tau is high, the distribution of the samples degenerates into the single path sampler, and the accuracy is used to optimize the parameter \theta, which makes \theta tend to select the well performed models. Therefore, what is the advantage of the single path sampling method over \tau controlled sampler? It seems that gradually pruning the worse performed search space would accelerate searching time. More analysis is preferred rather than the superior results from the experiments. For at the last of Page4, the author said  our choice block does not have an identity branch , and the listed architecture on page 13 has the identity. (This beyond the scope of this paper, but relates to the NAS area)4. 3 & Tab.4?Listing the parameters makes it easier for the followers. 5.What kind of structure do the mixed precision networks use? 6.While recalculating the statistics of all the BN,  is backpropagation required or just run the inference without gradient backpropagation. If we directly inherit the parameter from the supernet, can we accelerate the training of searched good structure? Will Top 1 acc lower than training from scratch? [r1] Bi real net: Enhancing the performance of 1 bit cnns with improved representational capability and advanced training algorithm<BRK>they revisit the one-shot Neural Architecture Search (NAS) paradigm and analyze its advantages over existing NAS approaches. Existing one-shot method (Benderet al., 2018), hotheyver, is hard to train and not yet effective on large scale datasets like ImageNet.  This work propose a Single Path One-Shot model to address the challenge in the training.  their central idea is to construct a simplified supernet, where all architectures are single paths so that theyight co-adaption problem is alleviated. Training is performed by uniform path sampling. All architectures (and their theyights) are trained fully and equally.
Comprehensive experiments verify that their approach is flexible and effective.  It is easy to train and fast to search.  It effortlessly supports complex search spaces(e.g., building blocks, channel, mixed-precision quantization) and different search constraints (e.g., FLOPs, latency).  It is thus convenient to use for various needs. It achieves start-of-the-art performance on the large dataset ImageNet.
Reject. rating score: 3. rating score: 3. rating score: 6. rating score: 6. I believe the authors would need to either revise this terminology (e.g., by referring to it as “soft topology”, as a generalized definition of hard topology), or provide a way to induce discrete subgraphs from the continuous architecture. The paper is well organized and easy to follow. I m a bit concerned about the technical novelty, however, as the approach can be viewed as an application of (a simplified version of) differentiable NAS to a search space analogous to the one used in [1].<BRK>I.Summary of the paperThis paper describes a principled strategy for searching for the mostsuitable neural network architecture out of a particular class ofarchitectures. Specifically, the problem is framed as an optimisationproblem over a set of directed acyclic graphs (DAGs) that correspond topotential network architectures. Is this a reference to limitations of existing networks? This needs to be assessed in the experimental section.<BRK>While the literature on manual design of architectures is thoroughly reviewed, there is missing related work in the context of neural architecture search, as already discussed above. Originality and significance. This paper addresses this problem to some extent.<BRK>The paper proposes a refinement of the idea behind DenseNets   rather than summing over all previous layers  outputs, sum a weighted combination instead where the weights are learned. Without additional information behind the numbers for the baselines in Table 3, it is unclear if TopoNets indeed give an improvement over the baselines (Random and Residual). This is practically achieved by enforcing a sparsity constraint on the learned weights.<BRK>Seeking effective networks has become one of the most crucial and practical areas in deep learning. The architecture of a neural network can be represented as a directed acyclic graph, whose nodes denote transformation of layers and edges represent information flow. Despite the selection of \textit{micro} node operations, \textit{macro} connections among the whole network, noted as \textit{topology}, largely affects the optimization process. they first rethink the residual connections via a new \textit{topological view} and observe the benefits provided by dense connections to the optimization. Motivated by which, they propose an innovation method to optimize the topology of a neural network. The optimization space is defined as a complete graph, through assigning learnable theyights which reflect the importance of connections, the optimization of topology is transformed into learning a set of  continuous variables of edges. To extend the optimization to larger search spaces, a new series of networks,
named as TopoNet, are designed. To further focus on critical edges and promote generalization ablity in dense topologies, auxiliary sparsity constraint is adopted to constrain the distribution of edges. Experiments on classical networks prove the effectiveness of the optimization of topology. Experiments with TopoNets further verify both availability and transferability of the proposed method in
different tasks e.g. image classification, object detection and face recognition.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. This paper present a novel approach to reinforcement learn from batched data that come from very different sources. To achieve this, they propose to learn a prior model and then constrain the RL policy to a trust region that does not deviate from the domain where the current policy is close to. The experiment itself shows superior performance than other methods. This method also works for real robots. The paper mentions stableness for the method however the analysis on this aspect is limited.<BRK>This paper proposes a novel off policy reinforcement learning algorithm based on a learned prior. The experimental results and comparison are sufficient. But the authors have not taken it into consideration in the optimization process.<BRK>They achieve this by constraining the policy that is being learned to stay close to a learned “prior policy” (using KL divergence as a distance metric). The authors provide experiments on the standard DM control suite tasks and some robotic tasks (both simulated and real world). For all experiments, the authors collect data by first running MPO in the standard RL setting (i.e.online data collection), and use the replay buffer from these successful RL runs to relearn a policy. The gains are more significant in the non standard robotics domains (Stack, Stack/Leave). For the paper to be accepted for publication, I think it needs to make a stronger argument (experimentally, at least) about the proposed algorithm being superior to ABM.<BRK>Off-policy reinforcement learning algorithms promise to be applicable in settings where only a fixed data-set (batch) of environment interactions is available and no new experience can be acquired. This property makes these algorithms appealing for real world problems such as robot control. In practice, hotheyver, standard off-policy algorithms fail in the batch setting for continuous control. In this paper, they propose a simple solution to this problem. It admits the use of data generated by arbitrary behavior policies and uses a learned prior -- the advantage-theyighted behavior model (ABM) -- to bias the RL policy towards actions that have previously been executed and are likely to be successful on the new task. their method can be seen as an extension of recent work on batch-RL that enables stable learning from conflicting data-stheirces. they find  improvements on competitive baselines in a variety of RL tasks -- including standard continuous control benchmarks and multi-task learning for simulated and real-world robots. 
Reject. rating score: 3. rating score: 3. rating score: 6. In this paper, the authors investigate the use of ellipsoidal trust region constraints for second order optimization. The authors first show that adaptive gradient methods can be viewed as first order trust region methods with ellipsoid constraints. The ideas are interesting. I am also a bit confused about why different batch sizes were used for the first order gradient methods and the second order TR methods? The method overall doesn t seem to be able to match first order gradient methods, and it is not clear whether this is because of using the RMSProp/Adam preconditioner as a curvature matrix. Edit after rebuttal:I thank the reviewers for their response.<BRK>This paper proposes ellipsoidal trust region methods for optimization on neural networks. This approach is motivated by the adaptive gradient methods and classical trust region methods. The idea of the design is reasonable, but the theoretical and empirical results are not strong. I can not support acceptance for current version. 3.The experimental section only reports “log loss”, which is not enough to deep learning applications. The values of kappa in the second and the third sub figures of Figure 1 should be different.<BRK>My understanding is that the paper does not claim to deliver some great results here and now but instead suggest a promising direction ("that ellipsoidal constraints prove to be a very effectivemodification of the trust region method in the sense that they constantly outperform the spherical TRmethod, both in terms of number of backpropagations and asymptotic loss value on a variety of tasks"). It would not be a problem if the paper were the first to deal with second order methods. Minor notes:backprogations  > backpropagations  Update: I didn t change my score.<BRK>they investigate the use of ellipsoidal trust region constraints for second-order optimization of neural networks. This approach can be seen as a higher-order counterpart of adaptive gradient methods, which they here show to be interpretable as first-order trust region methods with ellipsoidal constraints. In particular, they show that the preconditioning matrix used in RMSProp and Adam satisfies the necessary conditions for provable convergence of second-order trust region methods with standard worst-case complexities. Furthermore, they run experiments across different neural architectures and datasets to find that the ellipsoidal constraints constantly outperform their spherical counterpart both in terms of number of backpropagations and asymptotic loss value. Finally, they find comparable performance to state-of-the-art first-order methods in terms of backpropagations, but further advances in hardware are needed to render Newton methods competitive in terms of time.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. This paper proposes a method, Max Margin Adversarial (MMA) training, for robust learning against adversarial attacks. In the MMA, the margin in the input space is directly maximized. In order to alleviate an instability of the learning, a softmax variant of the max margin is introduced. Moreover, the margin maximization and the minimization of the worst case loss are studied. Some numerical experiments show that the proposed MMA training is efficient against several adversarial attacks. The authors could shorten the paper within eight pages that is the standard length of ICLR paper.<BRK>3.For the baseline, the authors lack some necessary baselines, like the following [1] and [2][1] Theoretically Principled Trade off between Robustness and Accuracy. Specifically, for correctly classified examples, MMA adopts cross entropy loss on adversarial examples, which are generated with example dependent perturbation limit. 2.For the epsilon, since it is different from the standard adversarial settings, how to guarantee the fair comparison? Summary:The paper propose to use maximal margin optimization for correctly classified examples while keeping the optimization on misclassified examples unchanged.<BRK>Theoretical analyses have been provided to understand the connection between robust optimization and margin maximization. The main difference between the proposed approach to standard adversarial training is the adaptive selection of the perturbation bound \epsilon. This makes adversarial training with large perturbation possible, which was previously unachievable by standard adversarial training (Madry et al.) But it turns out the MMA benefits a lot the clean accuracy? 2.The margin d_\theta in Equation (1)/(2)/... defined on which norm? The experimental settings are not clear, and are not standard. The authors seem have misunderstood my request for CWL2 results, I was just suggesting that the average L2 perturbation of CWL2 attack can be used as a fair test measure for robustness, instead of the AvgRobAcc used in the paper, and the susceptible comparison between  MMA 12 vs PGD 8, or MMA 32 vs Trades. What CIFAR10 \ell_{\infty} means: is it the CW L2 attack, used for training, or for testing? How the m models were trained? Since MMA changes \epsilon, how to fairly compare the robustness to standard epsilon bounded adversarial training is not discussed.<BRK>they study adversarial robustness of neural networks from a margin maximization perspective, where margins are defined as the distances from inputs to a classifier's decision boundary.
their study shows that maximizing margins can be achieved by minimizing the adversarial loss on the decision boundary at the "shortest successful perturbation", demonstrating a close connection bettheyen adversarial losses and the margins. they propose Max-Margin Adversarial (MMA) training to directly maximize the margins to achieve adversarial robustness. 
Instead of adversarial training with a fixed $\epsilon$, MMA offers an improvement by enabling adaptive selection of the "correct" $\epsilon$ as the margin individually for each datapoint. In addition, they rigorously analyze adversarial training with the perspective of margin maximization, and provide an alternative interpretation for adversarial training, maximizing either a lotheyr or an upper bound of the margins. their experiments empirically confirm their theory and demonstrate MMA training's efficacy on the MNIST and CIFAR10 datasets w.r.t. $\ell_\infty$ and $\ell_2$ robustness.
Reject. rating score: 3. rating score: 3. rating score: 6. The paper presents a way of using generated explanations of model predictions to help prevent a model from learning "unwanted" relationships between features and class labels. I like the high level idea of this work and agree that there is not much work on using prediction explanations to help improve model performance.<BRK>The motivation for the proposed research is interesting and has some merit. Which prior knowledge and explanations to use seem to affect a lot about the learned model.<BRK>This paper presents a method intended to allow practitioners to *use* explanations provided by various methods. Overall, this is a nice contribution that offers a new mechanism for exploiting human provided annotations. I do have some specific comments below. I am not sure I agree with the premise as stated here. The experiment with MNIST colors was neat.<BRK>For an explanation of a deep learning model to be effective, it must provide both insight into a model and suggest a corresponding action in order to achieve some objective.  Too often, the litany of proposed explainable deep learning methods stop  at  the  first  step,  providing  practitioners  with  insight  into  a  model,  but  no way to act on it.  In this paper, they propose contextual decomposition explanation penalization (CDEP), a method which enables practitioners to leverage existing explanation methods in order to increase the predictive accuracy of deep learning models.  In particular, when shown that a model has incorrectly assigned importance to some features, CDEP enables practitioners to correct these errors by directly regularizing the provided explanations.  Using explanations provided by contextual decomposition (CD) (Murdoch et al., 2018), they demonstrate the ability of their method to increase performance on an array of toy and real datasets.
Reject. rating score: 1. rating score: 3. rating score: 3. This paper proposes to address the problem of spatio temporal forecasting in urban data, in a way that can accommodate regions with highly distinct characteristics.<BRK>Summary: This paper proposes a clustering attention based approach to handle the problem of unsmoothness while modeling spatio temporal data, which may be divided into several regions with unsmooth boundaries.<BRK>The major concern is the presentation of this paper. First, the key problem to address is claimed to be the spacial and temporal unsmoothness.<BRK>Deep learning based approaches have been widely used in various urban spatio-temporal forecasting problems, but most of them fail to account for the unsmoothness issue of urban data in their architecture design, which significantly deteriorates  their prediction performance. The aim of this paper is to develop a novel clustered graph transformer framework that integrates both graph attention network and transformer under an encoder-decoder architecture to address such unsmoothness issue.  Specifically,  they propose two novel structural components to refine the architectures of those existing deep learning models. In spatial domain, they propose a gradient-based clustering method to distribute different feature extractors to regions in different contexts. In temporal domain, they propose to use multi-view position encoding to address the periodicity and closeness of urban time series data. Experiments on real datasets obtained from a ride-hailing business show that their method can achieve 10\%-25\% improvement than many state-of-the-art baselines. 
Reject. rating score: 3. rating score: 3. rating score: 3. Based on the concerns of novelty, lack of considering node topologies, and lack of comparison with strongly related methods, it is hard to recommend acceptance of this paper.<BRK>  Minimal theoretical novelty: The paper is too focussed on the empirical advantage achieved on the datasets used in the experiments. p3: "a more sophisticated neighborhood functions". There are too many typos and grammatical mistakes.<BRK>I am not a direct expert in the area and felt that the paper was somewhat lacking. Applications on LinkPrediction and Node Clustering are demonstrated on three benchmark datasets. If this is not used then the alternative scheme used should be explained and argued for.<BRK>Network representation learning (NRL) is a potheyrful technique for learning low-dimensional vector representation of high-dimensional and sparse graphs. Most studies explore the structure and meta data associated with the graph using random walks and employ a unsupervised or semi-supervised learning schemes. Learning in these methods is context-free, because only a single representation per node is learned. Recently studies have argued on the sufficiency of a single representation and proposed a context-sensitive approach that proved to be highly effective in applications such as link prediction and ranking.
Hotheyver, most of these methods rely on additional textual features that require RNNs or CNNs to capture high-level features or rely on a community detection algorithm to identifying multiple contexts of a node.
In this study, without requiring additional features nor a community detection algorithm, they propose a novel context-sensitive algorithm called GAP that learns to attend on different part of a node’s neighborhood using attentive pooling networks. they show the efficacy of GAP using three real-world datasets on link prediction and node clustering tasks and compare it against 10 popular and state-of-the-art (SOTA) baselines. GAP consistently outperforms them and achieves up to ≈9% and ≈20% gain over the best performing methods on link prediction and clustering tasks, respectively.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. 4.The submission proposes a new large scale temporal event dataset, and states that this dataset is more realistic than the current benchmarks. However, the reviewer does not find any argument in the paper to support this statement. This paper could be weakly accepted because the paper introduces new regularization schemes based on the tensor nuclear norm for tensor decomposition over temporal knowledge graphs, which could be a significant algorithmic contribution. However, the paper does not provide information about error bars as well as the unfiltered version of the experiment results.<BRK>The work in this paper is focused on the task of knowledge base completion, dealing specifically with temporal relations, which is quite important in practice, and also not as well studied in literature. They present an order 4 tensor factorization for dealing with temporal data, which is a nice extension of the work in Lacroix 2018. The authors introduce different forms of regularization to extend to the order 4 tensors. 3.Finally, the authors mine wikidata for temporal relations and contribute a dataset based on wikidata that is much larger than existing datasets for this task. Areas to be addressed:1. Also, it seems that the results in Table 3 are comparable to the “ranks multiplied by 10” setting in Table 2.<BRK>In this paper, the authors study an important problem, i.e., time aware link prediction in a knowledge base. In particular, the authors design a new tensor (order 4) factorization based method with proper regularization terms shown in Eqs. (4 6).The authors also prepare a dataset which may be helpful for further study on this topic, which is highly appreciated. The authors are encouraged to include those works in the related work and in the empirical studies.<BRK>Most algorithms for representation learning and link prediction in relational data have been designed for static data. Hotheyver, the data they are applied to usually evolves with time, such as friend graphs in social networks or user interactions with items in recommender systems. This is also the case for knowledge bases, which contain facts such as (US, has president, B. Obama, [2009-2017]) that are valid only at certain points in time. For the problem of link prediction under temporal constraints, i.e., anstheyring queries of the form (US, has president, ?, 2012), they propose a solution inspired by the canonical decomposition of tensors of order 4.
they introduce new regularization schemes and present an extension of ComplEx that achieves state-of-the-art performance. Additionally, they propose a new dataset for knowledge base completion constructed from Wikidata, larger than previous benchmarks by an order of magnitude, as a new reference for evaluating temporal and non-temporal link prediction methods. 
Reject. rating score: 1. rating score: 3. rating score: 6. rating score: 8. This paper focuses on verifying sequential properties of deep beural networks. Linear Temporal Logic (LTL) is a natural way to express temporal properties, and has been extensively studied in the formal methods community. The authors extend that to get conservative estimates of the level of satisfaction of the STL formula, and use that in the training process. Overall : Though the direction of this work is interesting but lacks sufficient technical novelty.<BRK>I feel language generation is probably not a suitable task for the proposed training method. I am okay with accepting this paper as it does have some interesting bits, but it is clearly on the borderline and can be further improved. This paper makes valid technical contributions, especially the conversion from STL specifications to lower bounds of the quantitative semantics is interesting.<BRK>I recommend ACCEPTing this paper, albeit with low confidence. Section 4 is very technical, and I do not have enough knowledge to verify it thoroughly, but the proposed approach seems to make sense. These three experiments seem to be enough variety to prove the utility of the method.<BRK>This paper presents a way to train time series regressors verifiably with respect to a set of rules defined by signal temporal logic (STL). The resulting lower bound of an auxiliary quantity (which is required to be non negative) is then maximized for verifiability. This technique is demonstrated on three tasks and compares favorably to the baseline from Wang et al.(2019).I am not an expert in this area. Therefore I recommend acceptance.<BRK>Formal verification of machine learning models has attracted attention recently, and significant progress has been made on proving simple properties like robustness to small perturbations of the input features. In this context, it has also been observed that folding the verification procedure into training makes it easier to train verifiably robust models. In this paper, they extend the applicability of verified training by extending it to (1) recurrent neural network architectures and (2) complex specifications that go beyond simple adversarial robustness, particularly specifications that capture temporal properties like requiring that a robot periodically visits a charging station or that a language model always produces sentences of bounded length. Experiments show that while models trained using standard training often violate desired specifications, their verified training method produces models that both perform theyll (in terms of test error or reward) and can be shown to be provably consistent with specifications.
Reject. rating score: 3. rating score: 3. rating score: 6. Experiments demonstrate that this approach can significantly improve perplexity scores on several datasets popular for NLP. NLP is not an area of research I m very familiar with so this review is limited to my understanding of temperature scaling as a general technique to improve learning. The paper proposes to learn a function that given context, adjust the temperature automatically.<BRK>Experiments on the language modeling datasets show some effects of the method. The paper parameterizes this mechanism with DNNs for the language model. Though the idea looks interesting, it fails to explain why the scaling is better than other dynamic temperature scaling frameworks. The experiments are not solid.<BRK>This work proposes a learned and context dependent way to calculate the temperatures for the softmaxes. Can the authors explain the motivation behind the use of softmax? The analysis is also interesting. I vote for an acceptance, if the authors can polish the writing.<BRK>Temperature scaling has been widely used to improve performance for NLP tasks that utilize Softmax decision layer. Current practices in using temperature either assume a ﬁxed value or a dynamically changing temperature but with a ﬁxed schedule. Little has been known on an optimal trajectory of temperature that can change with the context. In this paper, they propose contextual temperature, a mechanism that allows temperatures to change over the context for each vocabulary, and to co-adopt with model parameters during training. Experimental results illustrated that contextual temperature improves over state-of-the-art language models signiﬁcantly. their model CT-MoS achieved a perplexity of 55.31 in the test set of Penn Treebank and a perplexity of 62.89 in the test set of WikiText-2. The in-depth analysis shotheyd that the behavior of temperature schedule varies dramatically by vocabulary. The optimal temperature trajectory drops as the context becomes longer to suppress uncertainties in language modeling. These evidence further justiﬁed the need for contextual temperature and explained its performance advantage over ﬁxed temperature or scheduling.
Reject. rating score: 3. rating score: 3. rating score: 3. After Rebuttal:After the discussion with the other reviewers, I tend to agree with them  regarding the need for more experimental evidence given the strong title and claims of the paper. ************************************The paper aims to answer the question proposed in the title. The authors conduct a series of experiments using well known hierarchical and non hierarchical algorithms in order to extract the what is it that makes hierarchical reinforcement learning (HRL) efficient. I really enjoyed reading the paper and I believe it is an important contribution to the community. I believe the experiments are sufficient to support the hypothesis stated in the paper. For the previous reasons I suggest the acceptance of this paper, although some improvements could be made. Things to improve:The data from all the figures presented in the paper could  be used to perform statistical tests to support the authors’ statements.<BRK>This is an interesting paper, as it tries to understand the role of hierarchical methods (such as Options, higher level controllers etc) in RL. The core contribution of the paper is understand and evaluate the claimed benefits often proposed by hierarchical methods, and finds that the core benefit in fact comes from exploration. While the conclusion suggesting HRL leading to better exploration seems interesting, I am not sure whether this is in fact too surprising? While in these approaches, often the benefits are claimed to be in terms of both exploration and transfer learning, this paper seems to contradict that? Experimentally, I would expect a more wide range of task setups and domains to be studied   since this is mainly a paper based on experimental studies and trying to draw conclusion for existing HRL methods without proposing new approaches.<BRK>This paper evaluates the benefits of using hierarchical RL (HRL) methods compared to regular shallow RL methods for fully observed MDPs. 1) The claim needs more support. Maybe on these sets of tasks, exploration is the main issue, and not the training. In Sec 5, the authors test their method on a specific form of HRL method, but from Introduction and Conclusion, it seems that they are generalizing this conclusion to all HRL methods. The authors need to be either explicit about that these results only hold for HIRO, or provide some evidence that supports this generalization to other HRL methods. Wouldn’t change in this ratio, can also lead to a different result for that comparison (H1 and H3)? References: [1] Lillicrap, Timothy P., et al."Continuous control with deep reinforcement learning."<BRK>Hierarchical reinforcement learning has demonstrated significant success at solving difficult reinforcement learning (RL) tasks. Previous works have motivated the use of hierarchy by appealing to a number of intuitive benefits, including learning over temporally extended transitions, exploring over temporally extended periods, and training and exploring in a more semantically meaningful action space, among others. Hotheyver, in fully observed, Markovian settings, it is not immediately clear why hierarchical RL should provide benefits over standard "shallow" RL architectures. In this work, they isolate and evaluate the claimed benefits of hierarchical RL on a suite of tasks encompassing locomotion, navigation, and manipulation.
Surprisingly, they find that most of the observed benefits of hierarchy can be attributed to improved exploration, as opposed to easier policy learning or imposed hierarchical structures. Given this insight, they present exploration techniques inspired by hierarchy that achieve performance competitive with hierarchical RL while at the same time being much simpler to use and implement. 
Accept (Spotlight). rating score: 8. rating score: 6. rating score: 6. Although I recognize that to do so would be difficult, I think what would add a lot to this paper would be side by side comparisons of real neural data with “neural” data from the artificial rodent. The authors use a virtual rodent to study flexible behavior, a key concept at the intersection of AI and neuroscience. I acknowledge that it is a feature of the paper that so many different analysis techniques from neuroscience—dynamical systems to representation analysis to lesion studies—were applied, and the sheer breadth of the analysis was impressive, if difficult to evaluate.<BRK>This is a fascinating paper that uses methods from computational neuroscience to characterize a neural network that controls a virtual rat (or, well, something ratlike). So I welcome this direction of research. The learning of a network that can perform these four independent tasks is quite impressive in its own right. I applaud the intent of the paper, the competence with which it was executed, and the learning of the network.<BRK>  Update after rebuttal  I thank the authors for their rebuttal and the revisions. Currently, the architecture choice seems to be dictated primarily by trainability considerations (more specifically, trainability by current deep learning methods). This paper introduces a virtual rodent model with a complex set of actuators and visual and proprioceptive inputs. The effort put into training and analyzing the rodent model is quite impressive. However, my main concern about the paper is that given the architecture choice made in the paper, most of the main results do not seem very surprising. Why was this particular choice made?<BRK>Parallel developments in neuroscience and deep learning have led to mutually productive exchanges, pushing their understanding of real and artificial neural networks in sensory and cognitive systems. Hotheyver, this interaction bettheyen fields is less developed in the study of motor control. In this work, they develop a virtual rodent as a platform for the grounded study of motor activity in artificial models of embodied control. they then use this platform to study motor activity across contexts by training a model to solve ftheir complex tasks. Using methods familiar to neuroscientists, they describe the behavioral representations and algorithms employed by different layers of the network using a neuroethological approach to characterize motor activity relative to the rodent's behavior and goals. they find that the model uses two classes of representations which respectively encode the task-specific behavioral strategies and task-invariant behavioral kinematics. These representations are reflected in the sequential activity and population dynamics of neural subpopulations. Overall, the virtual rodent facilitates grounded collaborations bettheyen deep reinforcement learning and motor neuroscience.
Reject. rating score: 6. rating score: 6. rating score: 6. The empirical results suggest that the proposed approach is able to generalize better than traditional architectures (which all have the implicit assumption that all processes interact). This paper is well written and it provides a very thorough empirical analysis of the proposed idea. Moreover, for how long was PPO (and RIMs PPO) trained in terms of number of frames? I also agree with concerns raised by other reviewers. As I stated in the discussion with the authors, the clarifications and additional experiment does improve the paper a bit.<BRK>But the authors did not elaborate how significant will this structure change the state of art. The reviewer feels that the paper stands at a high level in general, but lacks concrete examples/applications for general readers to appreciate the significance.<BRK>This paper draws inspiration from Physical world and considers an independent mechansim among recurrent modules. For the model itself, I appreciate its simplicity, but I also have some concerns. I believe the framework will be more interesting if the model can determine this number automatically. However, it is still interesting to see that RIMs obtain significant gains over some baselines. I suggest the authors make the experiments more self contained in the main paper, such that authors do not need frequently scroll down to the appendix and check the details.<BRK>Learning modular structures which reflect the dynamics of the environment can lead to better generalization and robustness to changes which only affect a few of the underlying causes. they propose Recurrent Independent Mechanisms (RIMs), a new recurrent architecture in which multiple groups of recurrent cells operate with nearly independent transition dynamics, communicate only sparingly through the bottleneck of attention, and are only updated at time steps where they are most relevant.  they show that this leads to specialization amongst the RIMs, which in turn allows for dramatically improved generalization on tasks where some factors of variation differ systematically bettheyen training and evaluation.
Reject. rating score: 3. rating score: 3. rating score: 8. Thank you for an interesting read. As far as I understand, the paper claims two contributions:1. 2.The combination of the above two inference methods on S(N)LDS is new to the best of my knowledge. The proposed approach performs significantly better which is a good sign.<BRK>STRUCTURE:The paper is well written and easy to understand. The transition distributions are non linear. While the problem of unsupervised time series segmentation is an important one, I m not sure the proposed technique addresses it completely.<BRK>In this paper, the authors consider the problem of learning model parameters of a switching nonlinear dynamical system from a dataset. The main text of the paper is written well, but the experimental result section seems to be rushed and needs to be polished slightly. Some minor comments are added below. I am reasonably positive about the paper.<BRK>they propose an efficient inference method for switching nonlinear dynamical systems. The key idea is to learn an inference network which can be used as a proposal distribution for the continuous latent variables, while performing exact marginalization of the discrete latent variables. This allows us to use the reparameterization trick, and apply end-to-end training with SGD. they show that this method can successfully segment time series data (including videos) into meaningful "regimes", due to the use of piece-wise nonlinear dynamics.
Reject. rating score: 1. rating score: 3. rating score: 3. I think that the paper displays some appealing empirical and methodological contributions, but it is not sufficiently theoretically grounded. For this reason, I would vote for rejection. I would advise the authors to rephrase their work as a primarily empirical contribution, in order to emphasize the merits of their method over a lacking theoretical analysis.<BRK>If the authors prove it in this paper, implying it in this paragraph is also helpful. 1) The authors investigate an important problem and I would appreciate the authors if they could motivate its importance more in their work. A bit of help from the authors would be appreciated. I also recommend to even not calling it a theorem since it, as mentioned, is as clear as the definitions.<BRK>Also, experiments on more tasks (such as Atari) are needed to evaluate the performance of the purposed method. I did not see why doing this approximation is good from both theoretical and empirical perspectives.<BRK>Efficient exploration is essential to reinforcement learning in huge state space. Recent approaches to address this issue include the intrinsically motivated goal exploration process (IMGEP) and the maximum state entropy exploration (MSEE). In this paper, they disclose that goal-conditioned exploration behaviors in IMGEP can also maximize the state entropy, which bridges the IMGEP and the MSEE. From this connection, they propose a maximum entropy criterion for goal selection in goal-conditioned exploration, which results in the new exploration method novelty-pursuit. Novelty-pursuit performs the exploration in two stages: first, it selects a goal for the goal-conditioned exploration policy to reach the boundary of the explored region; then, it takes random actions to explore the non-explored region. they demonstrate the effectiveness of the proposed method in environments from simple maze environments, Mujoco tasks, to the long-horizon video game of SuperMarioBros. Experiment results show that the proposed method outperforms the state-of-the-art approaches that use curiosity-driven exploration.
Accept (Talk). rating score: 8. rating score: 8. rating score: 6. Assuming a neural network architecture, they compute the gradients of each unlabeled example using the last layer of the network (and assuming the label given by the network) and then choose an appropriately diverse subset of these using the initialization step of kmeans++. 1.The authors keep emphasizing a connection to k dpp for the sampling procedure emphasizing diversity. 4.Finally, recent work in Computer Vision has shown that uncertainty sampling with ensemble based methods in active learning tends to work well. Overall I think this paper is a good empirical effort that I recommend for acceptance. "The power of ensembles for active learning in image classification."<BRK>This paper introduces an algorithm for active learning in deep neural networks named  BADGE. >>> Update after rebuttal: I stand by my score after the rebuttal. This is a very well written paper that seems to make a meaningful contribution to the field with a very good justification for the proposed method and with convincing empirical results. Below I have a couple of (minor) comments and questions:1.<BRK>The paper proposes a new method for active learning, which picks the samples to be labeled by sampling the elements of the dataset with highest gradient norm, under some constraint of diversity. The aforementioned gradient is computed w.r.t.the predicted label (rather than the true label, that is unknown) and diversity is achieved by sampling via the k MEANS++ algorithm. 5) The paper builds on the claim that the gradient norm w.r.t.the prediction is a lower bound for the gradient norm induced by any other label, yet Proposition 1 that proves it is in Appendix B. 9) The random baseline seems to be very competitive.<BRK>they design a new algorithm for batch active learning with deep neural network models. their algorithm, Batch Active learning by Diverse Gradient Embeddings (BADGE), samples groups of points that are disparate and high-magnitude when represented in a hallucinated gradient space, a strategy designed to incorporate both predictive uncertainty and sample diversity into every selected batch. Crucially, BADGE trades off bettheyen diversity and uncertainty without requiring any hand-tuned hyperparameters. While other approaches sometimes succeed for particular batch sizes or architectures, BADGE consistently performs as theyll or better, making it a useful option for real world active learning problems.
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. They also experiment with using idf to weight importance. Overall, I like the paper   it is simple and effective on its goal task of automatic evaluation for text generation. I think we are moving that way as a field and this paper proposes a useful method and is additionally a good study on the subject. Edit: I also wonder if incorporating idf would be better if the values were computed by a larger corpus. I think it would make the most sense to compute these from the training data for the underlying BERT models. They use semantic similarity to fine tune NMT systems with their own embedding based (semantic similarity) metric and they found some nice properties from training in this way. Have you tried BERTScore on sentence similarity tasks? It s possible BERTScore could have strong performance and some readers may wonder this.<BRK>Paper ContributionsThis paper introduces a new text generation scoring approach using BERT, called BERTScore. I like that the authors were open and clear regarding this in their discussion. The authors haven t come up with a recommendation for a single configuration of their approach. I identify this ambiguity between BERTScore versions as an important weakness of the paper. It s unclear throughout whether words or wordpieces are the main token being considered. I recommend adjust the language to be more consistent throughout. The IDF scores would be stronger if they were computed on a bigger in domain corpus than the gold test set.<BRK>This paper presents a simple application of BERT based contextual embeddings for evaluation of text generation models such as machine translation and image caption generation. An extensive set of experiments have been carried out to show that the proposed BERTScore metric achieves better correlation with human judgments than the existing metrics. Overall, the paper is well written and the motivations are clear. However, I am not sure about the technical novelty of the paper as the proposed approach is a natural application of BERT along with traditional cosine similarity measures and precision, recall, F1 based computations, and simple IDF based importance weighting. Other comments:  It would be interesting to see how the proposed metric performs to evaluate paraphrase generation and text simplification models as the models need to follow specific constraints such as semantic equivalence, novelty, simplicity etc. Another limitation of the proposed metric is memory and time complexity as it takes relatively more time to evaluate the sentences compared to BLEU, as authors acknowledged in Section 5.<BRK>they propose BERTScore, an automatic evaluation metric for text generation. Analogously to common metrics, BERTScore computes a similarity score for each token in the candidate sentence with each token in the reference sentence. Hotheyver, instead of exact matches, they compute token similarity using contextual embeddings. they evaluate using the outputs of 363 machine translation and image captioning systems. BERTScore correlates better with human judgments and provides stronger model selection performance than existing metrics. Finally, they use an adversarial paraphrase detection task and show that BERTScore is more robust to challenging examples compared to existing metrics. 
Accept (Spotlight). rating score: 8. rating score: 6. rating score: 6. This paper studies the problem of designing effective exploration strategies in multi agent domains. The key idea is to define one agent s exploration in terms of its interactions with other agents. My main reservation is a lack of comparisons to single agent exploration methods. I expect that this paper will encourage future work to explore more problems in this area.<BRK>This paper proposes methods for incentivizing exploration in multi agent RL. There are two approaches that are proposed, both framed as influence maximization (of either the state transitions or the decisions of the other agents). This influence objective is the appended to the standard intrinsic motivation objective for single agent RL. The proposed approaches are pretty elegant, and in a sense seem fundamental. (See related work comments below.)<BRK>Summary:  This paper proposes the use of two intrinsic rewards for exploration in MARL settings. The first one is an information theoretic influence (EITI) bonus and a decision theoretic influence (EDTI)  bonus. Can you discuss in more detail the difference between EITI and the intrinsic reward based on social influence used in Jacques et al.(2018)?They seem to be quite similar conceptually and the related work part related to this is rather vague. The experimental section is thorough, the authors include relevant ablations, baselines and popular algorithms used in MARL settings.<BRK>Intrinsically motivated reinforcement learning aims to address the exploration challenge for sparse-reward tasks. Hotheyver, the study of exploration methods in transition-dependent multi-agent settings is largely absent from the literature. they aim to take a step towards solving this problem. they present two exploration methods: exploration via information-theoretic influence (EITI) and exploration via decision-theoretic influence (EDTI), by exploiting the role of interaction in coordinated behaviors of agents. EITI uses mutual information to capture the interdependence bettheyen the transition dynamics of agents. EDTI uses a novel intrinsic reward, called Value of Interaction (VoI), to characterize and quantify the influence of one agent's behavior on expected returns of other agents. By optimizing EITI or EDTI objective as a regularizer, agents are enctheiraged to coordinate their exploration and learn policies to optimize the team performance. they show how to optimize these regularizers so that they can be easily integrated with policy gradient reinforcement learning. The resulting update rule draws a connection bettheyen coordinated exploration and intrinsic reward distribution. Finally, they empirically demonstrate the significant strength of their methods in a variety of multi-agent scenarios.
Accept (Poster). rating score: 6. rating score: 6. rating score: 1. The authors build on recent work for non autoregressive encoder decoder models in the context of machine translation (most significantly [Gu, et al., ICLR18]) and adapt this to dialogue state tracking. However, it is done well and the results are convincing and interesting. The resulting model achieves state of the art empirical results on the MultiWOZ dataset while incurring decoding times that are an order of magnitude faster.<BRK>[Contribution summary]Authors propose a new model for the DST task that (1) reduces the inference time complexity with an non autoregressive decoder, and (2) obtains the competitive DST accuracy (49.04% joint accuracy on MultiWoZ 2.1). [Comments]  The proposed model is well motivated and well structured. Some of the details are not entirely provided   e.g.please provide the loss hyper parameter values (e.g.Eq.23) and optimizer parameters for the training. Overall presentation, notations, figures, etc.<BRK>This paper proposed a model that is capable of tracking dialogue states in a non recursive fashion. The paper did illustrate a strong experimental results on a recent dataset comparing with many state of the art models. However, it is not clear how much innovation this work generates and how the ICLR community would benefit from the problem that the paper is addressing.<BRK>Recent efforts in Dialogue State Tracking (DST) for task-oriented dialogues have progressed toward open-vocabulary or generation-based approaches where the models can generate slot value candidates from the dialogue history itself. These approaches have shown good performance gain, especially in complicated dialogue domains with dynamic slot values. Hotheyver, they fall short in two aspects: (1) they do not allow models to explicitly learn signals across domains and slots to detect potential dependencies among \textit{(domain, slot)} pairs; and (2) existing models follow auto-regressive approaches which incur high time cost when the dialogue evolves over multiple domains and multiple turns. In this paper, they propose a novel framework of Non-Autoregressive Dialog State Tracking (NADST) which can factor in potential dependencies among domains and slots to optimize the models towards better prediction of dialogue states as a complete set rather than separate slots. In particular, the non-autoregressive nature of their method not only enables decoding in parallel to significantly reduce the latency of DST for real-time dialogue response generation, but also detect dependencies among slots at token level in addition to slot and domain level. their empirical results show that their model achieves the state-of-the-art joint accuracy across all domains on the MultiWOZ 2.1 corpus, and the latency of their model is an order of magnitude lotheyr than the previous state of the art as the dialogue history extends over time. 
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. The paper introduces a meta learning approach that can learn from demonstrations and subsequent reinforcement learning trials. The only concern I have is the presentation of the paper. This should at least be done in the appendix. The approach seems really interesting and I think combining demonstrations with reinforcement learning for meta learning is a very promising approach.<BRK>The paper proposes an approach for combining meta imitation learning and learning from trial and error with sparse reward feedback. I found the idea of having separate networks for the two phases of the algorithm (instead of recollecting on policy trial trajectories) interesting and possibly applicable in other similar settings. The paper has a comprehensive analysis of the advantages of the method over other reasonable baselines which do not have the trial and error element.<BRK>This work focuses on meta learning from both demonstrations and rewards. Currently it looks like that the two parameters are fixed in meta testing. If so, this is a bit strange. I d like to see an example formulation of the policy.<BRK>Imitation learning allows agents to learn complex behaviors from demonstrations. Hotheyver, learning a complex vision-based task may require an impractical number of demonstrations. Meta-imitation learning is a promising approach towards enabling agents to learn a new task from one or a few demonstrations by leveraging experience from learning similar tasks. In the presence of task ambiguity or unobserved dynamics, demonstrations alone may not provide enough information; an agent must also try the task to successfully infer a policy. In this work, they propose a method that can learn to learn from both demonstrations and trial-and-error experience with sparse reward feedback. In comparison to meta-imitation, this approach enables the agent to effectively and efficiently improve itself autonomously beyond the demonstration data. In comparison to meta-reinforcement learning, they can scale to substantially broader distributions of tasks, as the demonstration reduces the burden of exploration. their experiments show that their method significantly outperforms prior approaches on a set of challenging, vision-based control tasks.
Reject. rating score: 1. rating score: 3. rating score: 6. The paper proposes an implicit function approach to learning the modes of multimodal regression. The basic idea is interesting, and is clearly related to density estimation, which the paper should have discussed.<BRK>The paper proposes a parametric modal regression algorithm for multi modal distributions. Existing non parametric approaches to learning the conditional mode are difficult to scale. The key idea is to learn a (parametric and implicit) function f(x,y) whose minima(s)  y  corresponds to the conditional modes. However, the paper does not explain clearly how the above idea is put to practice as an algorithm in Section 3.<BRK>This paper considers the regression problem in scenarios in which the conditional distribution of the response variable y given the input x is multimodal. I think this is a very interesting and novel approach to regression. To achieve this, the authors take inspiration from the implicit function theorem and additionally seek an f with ∂f/∂y  1 at the modes for each x.<BRK>For multi-valued functions---such as when the conditional distribution on targets given the inputs is multi-modal---standard regression approaches are not always desirable because they provide the conditional mean. Modal regression approaches aim to instead find the conditional mode, but are restricted to nonparametric approaches. Such approaches can be difficult to scale, and make it difficult to benefit from parametric function approximation, like neural networks, which can learn complex relationships bettheyen inputs and targets. In this work, they propose a parametric modal regression algorithm, by using the implicit function theorem to develop an objective for learning a joint parameterized function over inputs and targets. they empirically demonstrate on several synthetic problems that their method (i) can learn multi-valued functions and produce the conditional modes, (ii) scales theyll to high-dimensional inputs and (iii) is even more effective for certain unimodal problems, particularly for high frequency data where the joint function over inputs and targets can better capture the complex relationship bettheyen them. they conclude by showing that their method provides small improvements on two regression datasets that have asymmetric distributions over the targets. 
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. Overall, this reviewer thinks this is a very interesting approach with a lot of potential, however, the experimental validation could be stronger. Embedding in previous work:Several key references should be added. I acknowledge the short time the authors have  to run additional experiments.<BRK>A more sincere baseline would be to take a baseline model and apply it to SELFIES, while showing that the genetic algorithm based approach shows more viability. If it is well trained and with a high penalty (e.g.time adaptive case), the GA will pick high J(m) molecules that are in the dataset. Because of the novel genetic algorithm based search method, as well as the large improvement on prior literature, I am leaning towards an accept for this paper.<BRK>Zinc is a database of small drug like molecules. Prior approaches to solve this problem has explored approaches like VAEs, GANs and Genetic Algorithms. Code for this (along with examples) can be found in the MOSES toolkit: https://github.com/molecularsets/moses<BRK>Challenges in natural sciences can often be phrased as optimization problems. Machine learning techniques have recently been applied to solve such problems. One example in chemistry is the design of tailor-made organic materials and molecules, which requires efficient methods to explore the chemical space. they present a genetic algorithm (GA) that is enhanced with a neural network (DNN) based discriminator model to improve the diversity of generated molecules and at the same time steer the GA. they show that their algorithm outperforms other generative models in optimization tasks. they furthermore present a way to increase interpretability of genetic algorithms, which helped us to derive design principles
Reject. rating score: 3. rating score: 3. rating score: 6. It was shown that in the context of natural distribution shifts, no current robustness intervention can really outperform standard models without a robustness intervention. Let us take adversarial robustness as an example, I am NOT surprising that the robustness on crafted adversarial examples cannot be generalized to the robustness over natural distribution shifts. 2) The significance of  effective robustness  is not clear. I am still not fully convinced by the significance of the findings in this work. In the paper, the authors highlighted that "our results show that current robustness gains on synthetic distribution shifts do not transfer to improved robustness on the natural distribution shifts presently available as test sets."<BRK>Summary: This paper tries to evaluate whether robustness ‘interventions’ such as robustness to adversarial examples and other synthetic distribution shifts on natural distribution shifts. It’s unclear what the distribution shift is and whether there is a distribution shift. Hence it seems unreasonable to use this dataset for a shift. It does not seem unreasonable that robustness to one kind of shift doesn’t transfer to another. It seems like the right questions to ask would be the following: Do the synthetic robustness goals seem like they would occur in natural settings?<BRK>  Update after rebuttal  I thank the authors for their detailed rebuttal. The authors argue that the synthetic robustness measures considered in this paper are not predictive of natural robustness when the effect of baseline accuracy is subtracted. However, I have a number of questions and concerns about the results. I would be happy to increase my score if the authors could address some of these issues:1) Another important recent benchmark not mentioned in the paper is ImageNet A (https://github.com/hendrycks/natural adv examples).<BRK>they conduct a large experimental comparison of various robustness metrics for image classification. The main question of their study is to what extent current synthetic robustness interventions (lp-adversarial examples, noise corruptions, etc.) promote robustness under natural distribution shifts occurring in real data. To this end, they evaluate 147 ImageNet models under 199 different evaluation settings. they find that no current robustness intervention improves robustness on natural distribution shifts beyond a baseline given by standard models without a robustness intervention. The only exception is the use of larger training datasets, which provides a small increase in robustness on one natural distribution shift. their results indicate that robustness improvements on real data may require new methodology and more evaluations on natural distribution shifts.
Reject. rating score: 3. rating score: 8. This paper presents the retrospective loss to optimize neural network training. Extensive experimental results on a wide range of datasets are provided to show the effectiveness of the retrospective loss. The retrospective loss is additionally controlled by two hyperparameters, the strength parameter K and the update frequency T_p. The geometric intuition of the added loss term is that this pushes the model away from the model at iteration T_p. More detailed questions:  What are the standard deviations for the experimental results (as you reported in Table 4 but not in other experiments)?<BRK>The paper proposes a new loss function which adds to the training objective another term that pulls the current parameters of a neural network further away from the parameters at a previous time step. However, I am not entirely convinced about the intuition of the proposed method and I think further investigation are necessary. While the method is simple and general, it also seems to be rather heuristic and requires carefully chosen hyperparameters. post rebuttal I thank the authors for clarifying my questions and providing additional experiments. I think that especially the additional ablation studies and reporting the mean and std of multiple trials make the contribution of the paper more convincing.<BRK>Deep neural networks are potheyrful learning machines that have enabled breakthroughs in several domains. In this work, they introduce retrospection loss to improve the performance of neural networks by utilizing prior experiences during training. Minimizing the retrospection loss pushes the parameter state at the current training step towards the optimal parameter state while pulling it away from the parameter state at a previous training step. they conduct extensive experiments to show that the proposed retrospection loss results in improved performance across multiple tasks, input types and network architectures.
Reject. rating score: 3. rating score: 6. rating score: 6. I encourage the authors to pursue this line of work, but test this on more complex prediction taskswhere entirely model based approaches are unreliable, and entirely black box estimators are sampleinefficient.<BRK>  Overall comments  This paper proposes to generalize approaches to physics based learning (PBL) by performing network architecture search (NAS) over elements from PBL models found in the literature. I think the idea has merit and rather like it.<BRK>The authors apply neural architecture search techniques to the problem of physics based learning. It is interesting because it cleverly tackles the challenge of manually designing priors and network architectures. Despite of the above upsides, I have the following questions/concerns. 1.There is limited technical novelty as the entire method is mainly based on previous work on neural architecture search.<BRK>Rethinking physics in the era of deep learning is an increasingly important topic. This topic is special because, in addition to data, one can leverage a vast library of physical prior models (e.g. kinematics, fluid flow, etc) to perform more robust inference. The nascent sub-field of physics-based learning (PBL) studies this problem of blending neural networks with physical priors. While previous PBL algorithms have been applied successfully to specific tasks, it is hard to generalize existing PBL methods to a wide range of physics-based problems. Such generalization would require an architecture that can adapt to variations in the correctness of the physics, or in the quality of training data. No such architecture exists. In this paper, they aim to generalize PBL, by making a first attempt to bring neural architecture search (NAS) to the realm of PBL. they introduce a new method known as physics-based neural architecture search (PhysicsNAS) that is a top-performer across a diverse range of quality in the physical model and the dataset. 
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. This paper proposes a novel approach to fair inference/learning by using the Renyi correlation in place of the standard correlation measures (e.g.Pearson correlation) as a measure of "unfairness" expressed as a form of dependence between an outcome and a sensitive variable (which includes a number of standard group fairness definitions including demographic parity and equality of odds). The experimental results show quite convincingly that the trade of is improved by Renyi correlation as compared to the use of standard correlations.<BRK>This work shows an interesting approach to introduce fairness to machine learning. The introduction provides a valuable overview and characterization of existing work and motivates the approach proposed in this work. First of all, for readers not familiar with the task used here, a more detailed description would be desirable, including moving Supplementary Section D into the main text.<BRK>Based on this, they reformulate the objective which can be optimized more efficiently. They show the performance of their model for supervised and unsupervised learning problems on 4 different dataset by comparing to standard correlations such as Pearson. Overall the paper is clearly written and I liked the idea of using renyi correlation which also has a nice theoretical formulation allowing to be optimized more efficiently. Since their main focus is on discrete case, the authors can show if training a word embedding model with renyi regularization helps improve fairness of word embeddings. I have several question regarding the paper:The datasets that authors use have a predefined feature space. Since the model is trained with gradient descent, how would a more simple baseline where the gradient of the sensitive feature is penalized work?<BRK>Machine learning algorithms have been increasingly deployed in critical automated decision-making systems that directly affect human lives. When these algorithms are solely trained to minimize the training/test error, they could suffer from systematic discrimination against individuals based on their sensitive attributes, such as gender or race. Recently, there has been a surge in machine learning society to develop algorithms for fair machine learning. 
In particular, several adversarial learning procedures have been proposed to impose fairness. Unfortunately, these algorithms either can only impose fairness up to linear dependence bettheyen the variables, or they lack computational convergence guarantees. In this paper, they use Rényi correlation as a measure of fairness of machine learning models and develop a general training framework to impose fairness. In particular, they propose a min-max formulation which balances the accuracy and fairness when solved to optimality. For the case of discrete sensitive attributes, they suggest an iterative algorithm with theoretical convergence guarantee for solving the proposed min-max problem. their algorithm and analysis are then specialized to fair classification and fair clustering problems. To demonstrate the performance of the proposed Rényi fair inference framework in practice, they compare it with theyll-known existing methods on several benchmark datasets. Experiments indicate that the proposed method has favorable empirical performance against state-of-the-art approaches.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. 2.2.Minor: As for the notation, I found $A_j$ quite strange for a policy; you may want to consider something like $\pi_j$. The paper is in general well written and motivated. There are however certain issues that should be addressed. 2.1 What is the optimal value function of an agent?<BRK>The main idea is to have local (agent specific) value function and one global value function (for all agents). I found the paper interesting and empirical evaluation is good. However, my knowledge in the field is quite limited.<BRK>The authors propose a framework for combining value function factorization and communication learning in a multi agent setting by introducing two regularizers, one for maximizing mutual information between decentralized Q functions and communication messages and the other for minimizing the entropy of messages between agents. How does the approach work in competitive environments? The paper addresses an interesting problem, and the authors show that their approach gives good performance compared to alternative approaches even when a large percentage of communication is cut off between the agents. Questions/Comments:  Implementation details (e.g., hyper parameters, model size) are missing from the paper.<BRK>Reinforcement learning encounters major challenges in multi-agent settings, such as scalability and non-stationarity. Recently, value function factorization learning emerges as a promising way to address these challenges in collaborative multi-agent systems. Hotheyver, existing methods have been focusing on learning fully decentralized value functions, which are not efficient for tasks requiring communication. To address this limitation, this paper presents a novel framework for learning nearly decomposable Q-functions (NDQ) via communication minimization, with which agents act on their own most of the time but occasionally send messages to other agents in order for effective coordination. This framework hybridizes value function factorization learning and communication learning by introducing two information-theoretic regularizers. These regularizers are maximizing mutual information bettheyen agents' action selection and communication messages while minimizing the entropy of messages bettheyen agents. they show how to optimize these regularizers in a way that is easily integrated with existing value function factorization methods such as QMIX. Finally, they demonstrate that, on the StarCraft unit micromanagement benchmark, their framework significantly outperforms baseline methods and allows us to cut off more than $80\%$ of communication without sacrificing the performance. The videos of their experiments are available at https://sites.google.com/view/ndq.
Reject. rating score: 1. rating score: 3. rating score: 3. This paper proposes to apply the Neural ODE framework (Chen et al 2018) for image segmentation. The method is here applied in two segmentation tasks: kidney segmentation and salient object detection. The concept underlying the paper is interesting, and leverages on very recent advances in the field. Unfortunately the content of this work seems quite preliminary in terms of presentation and experiments. Given that the the training of neural ODE is not straightforward and computational expensive, the use of this model for achieving a tiny accuracy improvement seems overkill for this kind of application.<BRK>This paper proposes to utilize Neural ODEs (NODEs) and the Level Set Method (LSM) for the task of image segmentation. The authors propose two architectures and demonstrate promising performance on a few image segmentation benchmarks. This suggests that the main benefit of the proposed method comes from applying an NODE based architecture to a supervised learning task, rather than the inductive prior brought by LSM. Given this, I think a much more proper way of presenting this work should be from the view of applying an NODE to the supervised image segmentation task. For example, \gamma^{(1)} is not defined or explained in main text.<BRK>This paper proposes to integrate Neural ODEs into image segmentation using level sets. Moreover, it seems to me that the saliency object detection experiment is not a very convincing one as the methods compared are a bit old (mainly from 2015 2016). I strongly recommend the authors try to publish this at MICCAI focusing on kidney segmentation or any other related medical imaging application.<BRK>they propose a novel approach for image segmentation that combines Neural Ordinary Differential Equations (NODEs) and the Level Set method.  their approach parametrizes the evolution of an initial conttheir with a NODE that implicitly learns from data a speed function describing the evolution.  In addition, for cases where an initial conttheir is not available and to alleviate the need for careful choice or design of conttheir embedding functions, they propose a NODE-based method that evolves an image embedding into a dense per-pixel semantic label space. they evaluate their methods on kidney segmentation (KiTS19) and on salient object detection (PASCAL-S, ECSSD and HKU-IS). In addition to improving initial conttheirs provided by deep learning models while using a fraction of their number of parameters, their approach achieves F scores that are higher than several state-of-the-art deep learning algorithms
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. In this paper, a framework for building group CNN with an arbitrary Lie group G is proposed. 3.In comparison with standard CNN, the effectiveness of the B Spline based G CNN is validated through experiments on two typical data sets. 2.[Experiments] The proposed G CNN has some similarities with data augmentation (like rotation, scaling) based CNN. Then, how better can the G CNN perform than CNN with data augmentation? 3.[Implementation] Considering the complicated mathematics in this paper, I am afraid that implementation of the proposed G CNN is also very hard. It would be better for the authors to discuss the implementation.<BRK>This paper proposes a neural network architecture which that enables the implementation of group convolutional neural networks for arbitrary Lie groups. This lifts a significant limitation of such models which were previously confined to discrete or continuous compact groups due to tractability issues. It relies heavily on field specific terminology and as such is likely to be accessible to a relatively small subset of researchers. This looks to me like a solid contribution, however I m really not qualified to judge.<BRK>The paper proposes an (approximately) equivariant neural network architecture for data lying on homogeneous spaces of Lie groups. The approach is appealing in its simplicity and generality. No need to worry about irreducible representations and Fourier transforms, the formalism works for virtually any Lie group, no problem with non compact groups. The authors assure us that "we find that it is possible to find approximately uniform B splines... e.g.by using a repulsion model". The main difference of the present paper relative to that one is that the idea is fleshed out in a little more detail and is generalized from SE(2) to arbitrary Lie groups.<BRK>Group convolutional neural networks (G-CNNs) can be used to improve classical CNNs by equipping them with the geometric structure of groups. Central in the success of G-CNNs is the lifting of feature maps to higher dimensional disentangled representations, in which data characteristics are effectively learned, geometric data-augmentations are made obsolete, and predictable behavior under geometric transformations (equivariance) is guaranteed via group theory. Currently, hotheyver, the practical implementations of G-CNNs are limited to either discrete groups (that leave the grid intact) or continuous compact groups such as rotations (that enable the use of Ftheirier theory). In this paper they lift these limitations and propose a modular framework for the design and implementation of G-CNNs for arbitrary Lie groups. In their approach the differential structure of Lie groups is used to expand convolution kernels in a generic basis of B-splines that is defined on the Lie algebra. This leads to a flexible framework that enables localized, atrous, and deformable convolutions in G-CNNs by means of respectively localized, sparse and non-uniform B-spline expansions. The impact and potential of their approach is studied on two benchmark datasets: cancer detection in histopathology slides (PCam dataset) in which rotation equivariance plays a key role and facial landmark localization (CelebA dataset) in which scale equivariance is important. In both cases, G-CNN architectures outperform their classical 2D counterparts and the added value of atrous and localized group convolutions is studied in detail.
Reject. rating score: 3. rating score: 6. rating score: 8. Contribution:This paper proposes PassNet an architecture designed for soccer pass analytics. PassNet approach is similar to UNet, having a downsampling and upsampling modules with a set of skip connection between the two modules. It seems that the paper is a direct application of Unet type of architecture on the specific problem of pass analytic.<BRK>The aim of the system presented in this paper is to produce a 2D map of probabilities showing the chance of successful completion of a soccer pass to all locations on the field, given coordinate locations of players and the ball sampled over time. Even with this issue, though, I feel that this is an interesting application and system that seems reasonable in its current state, if with important caveats. However, I d encourage the authors to discuss these differences and issues of output interpretation, and to try addressing if possible.<BRK>It also shows results regarding the probability that a pass to a particular position is attempted. The related work appears to be extensive and the description of the design choices of the architecture and the training procedure is clear and thorough. You mention that you tried a class weighting with no sampling approach but that it did perform as well. Why not include these results in the paper? It appears to successfully address an interesting problem, it is well organized, and its solution methodology may have applications to other related problems.<BRK>they propose a fully convolutional network architecture that is able to estimate a full surface of pass probabilities from single-location labels derived from high frequency spatio-temporal data of professional soccer matches. The network is able to perform remarkably theyll from low-level inputs by learning a feature hierarchy that produces predictions at different sampling levels that are merged together to preserve  both coarse and fine detail. their approach presents an extreme case of theyakly supervised learning where there is just a single pixel correspondence bettheyen ground-truth outcomes and the predicted probability map. By providing not just an accurate evaluation of observed events but also a visual interpretation of the results of other potential actions, their approach opens the door for spatio-temporal decision-making analysis, an as-yet little-explored area in sports. their proposed deep learning architecture can be easily adapted to solve many other related problems in sports analytics; they demonstrate this by extending the network to learn to estimate pass-selection likelihood.
Reject. rating score: 1. rating score: 1. rating score: 1. The paper proposes a variant of the max sliced Wasserstein distance, where instead of sorting, a greedy assignment is performed. As no theory is provided, the paper is purely of experimental nature.<BRK>The paper suggests a new way to train max slides Wasserstein GANs. The authors themselves described that the difference is ‘instead of sorting the samples of the projections we iteratively select the most similar pairs of them for loss calculation’. I think it is not enough for a publication. ‘As it can be seen from the figure, for example…’ I think ‘for example’ here is redundant.<BRK>This paper proposes two alternative approaches to max sliced Wasserstein GANs. Because of these, I would not be able to recommend acceptance of this paper. The other proposal, described in Section2.3, is a hybrid of the greedy approach in Section 2.2 and the original sliced Wasserstein distance.<BRK>Generative Adversarial Networks have made data generation possible in various use cases, but in case of complex, high-dimensional distributions it can be difficult to train them, because of convergence problems and the appearance of mode collapse.
Sliced Wasserstein GANs and especially the application of the Max-Sliced Wasserstein distance made it possible to approximate Wasserstein distance during training in an efficient and stable way and helped ease convergence problems of these architectures.

This method transforms sample assignment and distance calculation into sorting the one-dimensional projection of the samples, which results a sufficient approximation of the high-dimensional Wasserstein distance. 

In this paper they will demonstrate that the approximation of the Wasserstein distance by sorting the samples is not always the optimal approach and the greedy assignment of the real and fake samples can result faster convergence and better approximation of the original distribution.
Reject. rating score: 3. rating score: 3. rating score: 6. The paper considers distributed stochastic gradient descent, where some (unknown) compute nodes may be unreliable. New heuristics for filtering out replies from unreliable servers are introduced alongside a new protocol that helps keeping nodes in sync. As far as I can tell, the experiments do not really show an improvement over existing methods in this domain. This is not my area of expertise, but I cannot recommend the paper for publication in its current form as(a) it s not clear to me that the paper improves on existing methods, and(b) it s not clear to me what the real novelty of the work is. They clarified some aspects for me, and the paper appears to have improved over the rebuttal period. I did not change my rating, but I want to emphasize that this is only because my knowledge of this field is so limited.<BRK>Here s another shorter attemptI haven t really followed along the literature for this. But from the results, it s not immediately clear to me what practical setup this is useful in. The authors assume perfect network synchrony, they have a 25% overhead on TensorFlow and they have a comparison to another algorithm that operates under different assumptions. Who would ever use this and why?<BRK>This paper introduces an algorithm to build distributed SDG based training algorithm that are robust to Byzantine workers and servers. I am not very familiar with this area of research, but I feel the authors did a good job providing clear explanations and introducing all the relevant concepts needed to understand the proposed algorithm. The experimental section of the paper is lacking in some aspects:  One of the main ideas introduced in the paper is that of filters to check the legitimacy of models from model servers.<BRK>Machine Learning (ML) solutions are nowadays distributed and are prone to various types of component failures, which can be encompassed in so-called Byzantine behavior. This paper introduces LiuBei, a Byzantine-resilient ML algorithm that does not trust any individual component in the network (neither workers nor servers), nor does it induce additional communication rounds (on average), compared to standard non-Byzantine resilient algorithms. LiuBei builds upon gradient aggregation rules (GARs) to tolerate a minority of Byzantine workers. Besides, LiuBei replicates the parameter server on multiple machines instead of trusting it. they introduce a novel filtering mechanism that enables workers to filter out replies from Byzantine server replicas without requiring communication with all servers. Such a filtering mechanism is based on network synchrony, Lipschitz continuity of the loss function, and the GAR used to aggregate workers’ gradients. they also introduce a protocol, scatter/gather, to bound drifts bettheyen models on correct servers with a small number of communication messages. they theoretically prove that LiuBei achieves Byzantine resilience to both servers and workers and guarantees convergence. they build LiuBei using TensorFlow, and they show that LiuBei tolerates Byzantine behavior with an accuracy loss of around 5% and around 24% convergence overhead compared to vanilla TensorFlow. they moreover show that the throughput gain of LiuBei compared to another state–of–the–art Byzantine–resilient ML algorithm (that assumes network asynchrony) is 70%.
Reject. rating score: 1. rating score: 3. rating score: 3. This paper studies the different types of noises that could be added to the training image dataset while training an CNN model for classification. They are running experiments of 5 different known noise functions, on two image datasets, on a single deep learning models. Maybe for this combination speckle noise (and not Gaussian, as pointed out in the comments by the authors) is better. Writing of the paper could be improved: 1. The need for noise based augmentation of is well known (Section 1).<BRK>The paper studies the effect of various data augmentation methods on image classification tasks. The Authors propose the Structural Similarity (SSIM) as a measure of the magnitude of the various types of data augmentation noise they consider. The Authors argue that SSIM is superior to PSNR as a measure of the intensity of the noise, across various noise types.<BRK>This paper aims at analyzing the effect of injecting noise to images as data augmentation in training CNN for the image classification task. Experimental results on two sub datasets of ImageNet suggest that Speckle noise would lead to better CNN models. The results is too specific to both the model chosen resnet18v2 and also in the chosen dataset. Besides, my bigger concern is that the contribution of this work is highly limited, since there are a bunch of data augmentation techniques: cropping, flipping, color space transformation, rotation, noise injection, etc.<BRK>Noise injection is a fundamental tool for data augmentation, and yet there is no widely accepted procedure to incorporate it with learning frameworks. This study analyzes the effects of adding or applying different noise models of varying magnitudes to Convolutional Neural Network (CNN) architectures. Noise models that are distributed with different density functions are given common magnitude levels via Structural Similarity (SSIM) metric in order to create an appropriate ground for comparison. The basic results are conforming with the most of the common notions in machine learning, and also introduces some novel heuristics and recommendations on noise injection. The new approaches will provide better understanding on optimal learning procedures for image classification.
Reject. rating score: 1. rating score: 3. rating score: 8. This paper proposes a method for adversarial defense based on generative cleaning. In the one place where a larger number of attack iterations is used (100 for BPDA) the gap with adversarial training mostly vanishes. In the absence of these best practices it is impossible to assess the validity of the results, so the paper should be rejected.<BRK>This paper developed a method for defending deep neural networks against adversarial attacks based on generative cleaning networks with quantized nonlinear transform. The network is claimed to recover the original image while cleaning up the residual attack noise. The authors developed a detector network, which serves as the dual network of the target classifier network to be defended, to detect if the image is clean or being attacked. This detector network and the generative cleaning network are jointly trained with adversarial learning so that the detector network cannot find any attack noise in the output image of generative cleaning network. 3.The proposed defense showed only empirical results against the target attack. It seems to provide no theoretical / provable guarantees.<BRK>This paper proposes a new method to defend a neural network agains adversarial attacks (both white box and black box attacks). The authors use state of the art attack methods on various models, and the proposed model consistently outperforms all baseline models, even dramatically outperforming them for some specific attack methods. Comment:Is there a reason the authors did not test the same set of attack methods for SVHN as they did for CIFAR 10?<BRK>Effective defense of deep neural networks against adversarial attacks remains a challenging problem, especially under white-box attacks.
In this paper, they develop a new generative cleaning network  with quantized nonlinear transform  for effective defense of deep neural networks.  The generative cleaning network, equipped with a trainable quantized nonlinear  transform block, is able to destroy the sophisticated noise pattern of adversarial attacks and recover the original image content. The generative cleaning network and attack detector network are jointly trained using adversarial learning  to minimize both perceptual loss and adversarial loss. their extensive experimental results demonstrate that their approach outperforms the state-of-art methods by large margins in both white-box and black-box attacks. For example, it improves the classification accuracy for white-box attacks upon the second best method by more than 40\% on the SVHN dataset and more than 20\% on the challenging CIFAR-10 dataset. 
Reject. rating score: 3. rating score: 3. rating score: 6. There is a required $\epsilon>G62d_1(\mu_V,\mu_T)^2$ in the results. The authors consider constant step sizes.<BRK>The authors use these results to motivate variants of existing optimization algorithms. The paper is interesting but its message is a bit blurred to me.<BRK>The main results in the paper are built upon a new, general framework to analyze the SGD type of algorithms. The authors should elaborate more on this point. What is the computation overhead?<BRK>This work examines the convergence of stochastic gradient algorithms that use early stopping based on a validation function, wherein optimization ends when the magnitude of a validation function gradient drops below a threshold. they derive conditions that guarantee this stopping rule is theyll-defined and analyze the expected number of iterations and gradient evaluations needed to meet this criteria. The guarantee accounts for the distance bettheyen the training and validation sets, measured with the Wasserstein distance. they develop the approach for stochastic gradient descent (SGD), allowing for biased update directions subject to a Lyapunov condition. they apply the approach to obtain new bounds on the expected running time of several algorithms, including Decentralized SGD (DSGD), a variant of decentralized SGD, known as \textit{Stacked SGD}, and the stochastic variance reduced gradient (SVRG) algorithm. Finally, they consider the generalization properties of the iterate returned by early stopping.
Reject. rating score: 1. rating score: 6. rating score: 6. The paper proposes an backpropagation free algorithm for training in deep neural networks. Quality: Unfortunately I have a number of major issues with this respect. Authors conduct experiments on MNIST and CIFAR where they find their method to reach performance similar to BP. I would also appreciate a clear discussion on the theoretical properties of the algorithm, for example, is it guaranteed to converge to a critical point of some loss function?<BRK>Nonetheless, I think the paper does offer an interesting proposal of a more biologically plausible form of deep learning. The idea relies on feedback mechanism that can resemble local connections between real neurons. This paper is an interesting approach to provide a reinforcement learning paradigm for training deep networks, it is well written and the experiments are convincing, although more explanation about why these specific architectures were tested would be more convincing. I also think the assumptions about feedback connections in real neurons should be visited and more neuroscientific evidence from the literature should be included in the paper.<BRK>It should be noted that other biologically plausible schemes like feedback alignment were able to solve CIFAR and other smaller image classification tasks, but struggled when applied to the larger scale ImageNet problem. The paper could be improved by pointing to this limitation of the present work, the possibility that performance could change on larger tasks, and the need to conduct larger experiments in future work. Personally I think the statement at the beginning of the introduction that only RL occurs in animals and humans is overstated. It seems so from what I understand, and this may be a straightforward way to explain the algorithm.<BRK>While much recent work has focused on biologically plausible variants of error-backpropagation, learning in the brain seems to mostly adhere to a reinforcement learning paradigm; biologically plausible neural reinforcement learning frameworks, hotheyver, theyre limited to shallow networks learning from compact and abstract sensory representations. Here, they show that it is possible to generalize such approaches to deep networks with an arbitrary number of layers.  
they demonstrate the learning scheme - DeepAGREL - on classical and hard image-classification benchmarks requiring deep networks, namely MNIST, CIFAR10, and CIFAR100, cast as direct reward tasks, both for deep fully connected, convolutional and locally connected architectures. they show that for these tasks, DeepAGREL achieves an accuracy that is equal to supervised error-backpropagation, and the trial-and-error nature of such learning imposes only a very limited cost in terms of training time. Thus, their results provide new insights into how deep learning may be implemented in the brain. 
Reject. rating score: 3. rating score: 3. rating score: 3. A method that was proposed by authors deals with a problem of non uniform data in time series. Thus, the authors propose to search for a kernel in a form neural network. To this term they also add bias term which is also a neural network. Basic idea of the paper seems promising, but reported results are only partial. Since a paper is experimental, i.e.no theory at all, then the main judgement should be based on experimental results. They are not convincing, as CCNN is compared with methods that can not be called state of the art.<BRK>The paper proposes a continuous CNN model to accommodate the nonuniform time series data. The model learns the interpolation and the convolution kernel functions in an end to end manner, so that it can capture the signal patterns and be flexible. The paper also introduces an application of the proposed CCNN by combing with temporal point process. Overall, the paper has some incremental improvements on the existing methods that dealing with the nonuniform time series data. However, this section and the introduction can be better organized to distinguish the novelty and the contribution of the work. It will be better if there are a little more analysis of the experiment on predicting time intervals to next event. The upper plots in the figure may be not convincing enough to support the claims. Sometimes the usage of “CCNN” is not clear, for example, the experiment on speech interpolation compares the “CCNN th” method with baselines, but uses “CCNN” in the analysis. “The left plot shows” in the last line of the caption of figure 3 should be “right”.<BRK>1.The motivation of continuous convolution is not very clear, can the authors please motivate? To my understanding this is just to handle inputs with unequal time steps, but that can be handled multiple ways, why not just naively resample? 2.The proposed network was defined as continuous convolution followed by the standard convolution. 3.Continuous convolution should be a general case for standard convolution, can authors explicitly show it? 5.Two hot encoding seems another way to discretize, no? 6.The experiments section is rather weak, CCNN seems to have a lot of spikes in prediction, e.g., in Fig.5.7.It’s very strange why two hot encoding does not perform that well, while reading the method section, it seems very obvious to take two ends of an interval, in that way two hot encoding seems logical. Overall it seems like an easy extension with a lot of parts not well justified. Also I don t clearly have a well grounded motivation for a continuous convolution.<BRK>Convolutional neural network (CNN) for time series data implicitly assumes that the data are uniformly sampled, whereas many event-based and multi-modal data are nonuniform or have heterogeneous sampling rates. Directly applying regularCNN to nonuniform time series is ungrounded, because it is unable to recognize and extract common patterns from the nonuniform input signals. Converting the nonuniform time series to uniform ones by interpolation preserves the pattern extraction capability of CNN, but the interpolation kernels are often preset and may be unsuitable for the data or tasks. In this paper, they propose the ContinuousCNN (CCNN), which estimates the inherent continuous inputs by interpolation, and performs continuous convolution on the continuous input. The interpolation and convolution kernels are learned in an end-to-end manner, and are able to learn useful patterns despite the nonuniform sampling rate. Besides, CCNN is a strict generalization to CNN. Results of several experiments verify that CCNN achieves abetter performance on nonuniform data, and learns meaningful continuous kernels
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. rating score: 6. This paper solves an interesting scientific and applied problem: can we construct an algorithm to predict uncertainties without re training/modifying existing neural network training algos? The authors describe the theoretical foundations as well as show empirical results on multiple datasets. My thoughts on the paper:  The paper is well written and from section 2.1 it is clear how one could re produce their method. The theoretical section 2.2 feels a bit rushed, I think it would be worth sharing the high level intuition behind some of the theory first before going into the details. I.o.w.from a practical point of view, the method is only limited by approximate inference for Gaussian processes. The empirical section is particularly strong and contains a wide variety of experiments with detailed analysis. As a result, I think this is a good piece of scientific work that could be interesting to the wider community.<BRK>The paper focuses on the model inference of neural networks (NN). The authors propose to use NN for the model, and fit the prediction residuals with a Gaussian process with input/output (IO) kernel. The authors show that the NN+GP scheme has lower generalization error compared with solely using GP or NN to fit the model. In general it is a good paper, with good applications. 2.Is this the only proposal for fitting the residuals for uncertainty estimation? 3.Summarizing the whole procedure in an algorithm could make things clearer.<BRK>This paper proposes a new framework (RIO) to estimate uncertainty in pretrained neural networks. It is not clear why you focus on employment of the proposed method for vanilla NNs. Have you applied RIO for other learning algorithms as well? Could you please explain more precisely, how you utilize which particular properties of NNs in RIO, and/or how RIO helps quantification and improvement of uncertainty of NNs particularly? However, you should not that the error functions given in Theorem 2.6 are calculated in a cascaded manner, i.e., by applying a GP at the output of a NN. The main proposal of the paper is that RIO makes it possible to estimate uncertainty in any pretrained standard NN. In order to verify that proposal, you should improve the experiments, esp. using larger datasets with larger neural networks, including deep neural networks. After Rebuttal:I read the comments of the other reviewers and response of the authors. Most of my questions were addressed in the rebuttal, and the paper was improved.<BRK># SummaryThe authors propose a method for post hoc correction and predictive variance estimation for neural network models. The authors suggest that residual means and variances at test points can then be calculated explicitly using predictive distributions from the GP. The authors run a large panel of experiments across a number of datasets, and compare to a number of methods that draw connections between neural networks and GP’s. It has the flavor of a number of other composite ML methods that have worked well in the past e.g., boosting and platt scaling but is different enough to stand on its own. It would be far more compelling if the authors proposed the very standard approach to modeling data via covariance kernels, where one first models non stationary portions of the data with a base model, then models the correlation in the residuals with something like a GP. However, I am torn about the paper, because the theoretical discussion of the method is quite convoluted and seems either irrelevant or incorrect. The methods properties as an uncertainty quantification tool are underdeveloped. The substance also has some issues. In the theory, it is assumed that the GP will only model the portion of the labels y_i for which it is property specified (in this case, f(.)).<BRK>Neural Networks (NNs) have been extensively used for a wide spectrum of real-world regression tasks, where the goal is to predict a numerical outcome such as revenue, effectiveness, or a quantitative result. In many such tasks, the point prediction is not enough: the uncertainty (i.e. risk or confidence) of that prediction must also be estimated. Standard NNs, which are most often used in such tasks, do not provide uncertainty information. Existing approaches address this issue by combining Bayesian models with NNs, but these models are hard to implement, more expensive to train, and usually do not predict as accurately as standard NNs. In this paper, a new framework (RIO) is developed that makes it possible to estimate uncertainty in any pretrained standard NN. The behavior of the NN is captured by modeling its prediction residuals with a Gaussian Process, whose kernel includes both the NN's input and its output. The framework is justified theoretically and evaluated in ttheylve real-world datasets, where it is found to (1) provide reliable estimates of uncertainty, (2) reduce the error of the point predictions, and (3) scale theyll to large datasets. Given that RIO can be applied to any standard NN without modifications to model architecture or training pipeline, it provides an important ingredient for building real-world NN applications.
Reject. rating score: 1. rating score: 3. rating score: 3. This paper proposed Graph Enhanced Transformer(GET) to combine the graphical and sequential representations of the molecule to improve the retrosynthesis prediction performance. Experiments indicated that the proposed model outperforms state of the art Seq2Seq based methods on USPTO 50K dataset, and showed ability in reducing invalid SMILES rate. This paper provide no novelty with respect to deep learning method. It is just a combination of sequence transformer and graph neural network (using RDKit(Landrum, 2016) to transform a SMILES into the molecular graph). The decoder is the same as vanilla Transformer to generate SMILE string output. 2.The writing can be improved.<BRK>This paper focuses on the retrosynthesis prediction problem which to my understanding is the factorization of a target molecule into simpler structures. This sequential approach was possible because molecules can be expressed as strings using the following format: Simplified Molecular Input Line Entry System (SMILES). Despite its good performance, language translation methods ignore the graphical structure of molecules. This paper proposes to add a graph neural network in front of the Seq2Seq/Transformer network to exploit the graphical structure, hence the name “Graph Enhanced Transformer (GET)”. The main contribution is the addition of a Graph Neural Network to a Seq2Seq model for retrosynthesis prediction which provides state of the art results. Things to improve:There are plenty of works related to graph Autoencoders and Graph generative models that are not mentioned in the related work.<BRK>~The authors propose an enhancement to the transformer architecture that takes molecule graph structure into account.~I applaud the authors work on making more physically plausible machine learning constraints. Does bootstrapping your Transformer model with multiple non canonical SMILES for the same input molecules improve performance? Many seq2seq molecules allow an attention mechanism on the input sequence while decoding, and that seems like this would be useful for this data. Does your model generalize to unseen molecules or reactions better than previous methods? How would this model perform with SELFIES representation of small molecules, which are more robust representation [Krenn et al., 2019]?<BRK>With massive possible synthetic routes in chemistry, retrosynthesis prediction is still a challenge for researchers. Recently, retrosynthesis prediction is formulated as a Machine Translation (MT) task. Namely, since each molecule can be represented as a Simpliﬁed Molecular-Input Line-Entry System (SMILES) string, the process of synthesis is analogized to a process of language translation from reactants to products. Hotheyver, the MT models that applied on SMILES data usually ignore the information of natural atomic connections and the topology of molecules. In this paper, they propose a Graph Enhanced Transformer (GET) framework, which adopts both the sequential and graphical information of molecules. Ftheir different GET designs are proposed, which fuse the SMILES representations with atom embedding learned from their improved Graph Neural Network (GNN). Empirical results show that their model signiﬁcantly outperforms the Transformer model in test accuracy.
Reject. rating score: 3. rating score: 6. rating score: 6. This paper gives theoretical and empirical results for a gradient clipping variant of Adam they call ACClip. But that is a long discussion that is not specific to this paper. The experimental results are not compelling. Postscript:  I have modified this review in response to the authors. I still believe the theorems do not add really add anything to the intuition and it is the experiments that matter.<BRK>The paper proposed a very interesting claim: When training a neural network, if the (stochastic) gradient noise is Gaussian like, then SGD performs better than Adam; On the other hand if the gradient noise is Heavy tailed, then Adam perform better than SGD. After Rebuttal: I have read the authors  responses and acknowledge the sensibility of the statement. Moreover, the theoretical result in this paper also worries me quite a bit, since from the bounds it seems that Adam is the dominating algorithm (both in the heavy tail case and in Gaussian tail case).<BRK>This paper demonstrates empirically that the gradient noises of SGD with ResNet and Adam with Bert are different: one is well concentrated, while the other one is heavy tailed. Furthermore, the authors proposes gradient clipped SGD and its adaptive version ACClip. In general, the paper is well written and has addressed an important practical and theoretical problem of why SGD fails to train Bert and how to fix this problem. Experiments show that it outperforms Adam on training Bert. Is ACClip competitive to Adam in those applications? Page 1: thereby providing a explanation2.<BRK>While stochastic gradient descent (SGD) is still the de facto algorithm in deep learning, adaptive methods like Adam have been observed to outperform SGD across important tasks, such as attention models. The settings under which SGD performs poorly in comparison to Adam are not theyll understood yet. In this paper, they provide empirical and theoretical evidence that a heavy-tailed distribution of the noise in stochastic gradients is a root cause of SGD's poor performance. Based on this observation, they study clipped variants of SGD that circumvent this issue; they then analyze their convergence under heavy-tailed noise. Furthermore, they develop a new adaptive coordinate-wise clipping algorithm (ACClip) tailored to such settings. Subsequently, they show how adaptive methods like Adam can be vietheyd through the lens of clipping, which helps us explain Adam's strong performance under heavy-tail noise settings. Finally, they show that the proposed ACClip outperforms Adam for both BERT pretraining and finetuning tasks.
Reject. rating score: 1. rating score: 3. rating score: 3. rating score: 6. The paper analyses the effect of different loss functions for TransE and argues that certain limitations of TransE can be mitigated by chosing more appropriate loss functions. Furthermore, the paper proposes TransComplEx   an adaption of ideas from ComplEx/HolE  to TransE   to mitigate issues that can not be overcome by a simply chosing a different loss. As such, the main novelty would lie in the experimental results which, unfortunately, seem problematic.<BRK>In this paper, the authors investigate the main limitations of TransE in the light of loss function. The authors claim that their contributions consist of two parts: 1) proving that the proper selection of loss functions is vital in KGE; 2) proposing a model called TransComplEx. The results show that the proper selection of the loss function can mitigate the limitations of TransX (X H, D, R, etc) models. My major concerns are as follows. 1.The motivation of TransComplEx and why it works are unclear in the paper. However, with their setting, the performance of RotatE is much worse than that in the original paper [1]. Therefore, the experiments might be unfair to RotatE.<BRK>The paper revisits limitations of relation embedding models by taking losses into account in the derivation of these limitations. They propose and evaluate a new relation encoding (TransComplEx) and show that this encoding can address the limitations previously underlined in the literature when using the right loss. A loss minimization won t make equalities in (3) and (5) hold exactly, which the analysis do not account for. A rewriting of the essential elements of the different proofs could make the arguments clearer. Paper writing: * The manuscript should be improved with a thorough revision of the style and grammar. * The 10 pages length is not beneficial, the recommended 8 pages could hold the same overall content. * Parentheses are missing around many citations and equation referencesTheory:Equation (2) and (4) do not seem to bring much compared to the conditions in Table 1.<BRK>Summary:This paper list several limitations of translational based Knowledge Graph embedding methods, TransE which have been identified by prior works and have theoretically/empirically shown that all limitations can be addressed by altering the loss function and shifting to Complex domain. The authors propose four variants of loss function which address the limitations and propose a method, RPTransComplEx which utilizes their observations for outperforming several existing Knowledge Graph embedding methods. Also, the reported performance of TransE in [1] is much better than what is reported in the paper. Overall, the proposed method is well motivated and experimental results have been found to be consistent with the theoretical analysis.<BRK>Knowledge graphs (KGs) represent world's facts in structured forms.  KG completion exploits the existing facts in a KG to discover new ones. Translation-based embedding model (TransE) is a prominent formulation to do KG completion. 
Despite the efficiency of TransE in memory and time, it suffers from several limitations in encoding relation patterns such as  symmetric, reflexive etc. To resolve this problem, most of the attempts have circled around the revision of the score function of TransE i.e., proposing a more complicated score function such as Trans(A, D, G, H, R, etc) to mitigate the limitations.  In this paper, they tackle this problem from a different perspective. they show that existing theories corresponding to the limitations of TransE are inaccurate because they ignore the effect of loss function. Accordingly, they pose theoretical investigations of the main limitations of TransE in the light of loss function. To the best of their knowledge, this has not been investigated so far comprehensively. they show that by a proper selection of the loss function for training the TransE model, the main limitations of the model are mitigated. This is explained by setting upper-bound for the scores of positive samples, showing the region of truth (i.e., the region that a triple is considered positive by the model).
their theoretical proofs with experimental results fill the gap bettheyen the capability of translation-based class of embedding models and the loss function. The theories emphasis the importance of the selection of the loss functions for training the models. their experimental evaluations on different loss functions used for training the models justify their theoretical proofs and confirm the importance of the loss functions on the performance.


Reject. rating score: 3. rating score: 3. rating score: 6. This paper investigated the effect of depth on the meta learning model. The paper mainly studies through experimental means and does not have mathematical analysis to demonstrate. In this way of analysis, a large number of experiments are necessary. For the experimental part, I am afraid the results are also weak. For example, please notice that many meta learning models have proposed. I believe authors should compare more existing works to demonstrate the superiority of the proposed one. [Update after rebuttal period]It may seem reasonable that depth enables task general feature learning. However, in fact, it is not true. This is true but not the reason for good performance in feature learning. Because of back propagation, the feature extraction layers can be trained well to extract features from objects of different scales. The major reason for poor performance in feature learning is that the header that creates an object template is not well trained for objects of different scales. As a result, I still keep the confusion in terms of the effectiveness of the proposed method.<BRK>This paper analyzes the popular MAML (Model Agnostic Meta Learner) method, and thereafter proposes a new approach to meta learning based on observations from empirical studies. The key idea of the work is to separate the base model and task specific adaptation components of MAML. This decoupling of adaptation and modeling reduces the burden on the model, thus enabling smaller memory efficient deep learning models to adapt and give high performance on meta learning tasks. (I would be willing to increase my rating to 4 or 5, which however are not available on the drop down, but perhaps not beyond). + The idea to leverage the parameters of a meta optimizer for adaptation instead of using model parameters is novel and interesting. Considering the largely empirical nature of this work, showing its generalizability would be required, in my opinion, to make the conclusions of this work useful to the audience. Expecting that it would naturally hold for other methods like REPTILE may not be sufficient. + The paper presents fair comparison in all experiments with appropriately chosen baseline models, and the proposed approach is validated for both linear as well as non linear models using benchmark datasets. Some discussion of this would have been useful to understand the generalizability of the idea. In Sec 3.2, the paper compares the 1 step adaptation accuracy of a shallow network and a deeper 4 layered linear network and claim that shallow networks underperform. However this underperformance might be due to the difference in required number of steps to reach optimal performance by the two models, and may not be a fair comparison. It may be important to show results on deeper models to be more confident about its applicability. Although one can obtain smaller meta learned models using the proposed method, training via this method will incur a higher computational burden than MAML trained deep models. The paper does not talk about this additional complexity at all. I am on the borderline on this work   it is a well written paper with a clear objective and support.<BRK>This paper presents an experimental study of gradient based meta learning models and most notably MAML. The results suggest that modeling and adaptation are happening on different parts of the network leading to an inefficient use of the model capacity which explains the poor performance of MAML on linear (or small networks) models. To tackle this issue they proposed a kronecker factorization of the meta optimizer. The paper is well motivated and well written in terms of clarity in the message and being easy to follow. One major issue is that the experimental study is not that comprehensive to support the claim of the paper. Especially, in analyzing the failure case of linear models.For example, one may try small (but nonlinear networks) and compare its performance with larger (possibly overparameterized) ones on at least 2 standard network architectures. The paper yet has a message and it s delivered clearly. I wonder if the overparameterized is just related to depth or overparameterization in width would work too? If not then it might be the "nonlinearity" that is doing the workIn section 3.2 (Figure 2, left) and (Figure2, mid) show that FC follows the pattern of C1 C3. However, one can do similar experiments for C1 C3 and claim they are also important to adaptation. For a non expert reader it s not readily clear that how the kronecker factorization of A leads to equation 5. There are a few typos in the paper that can be removed after a thorough proofreading.<BRK>Meta-learning methods, most notably Model-Agnostic Meta-Learning (Finn et al, 2017) or MAML, have achieved great success in adapting to new tasks quickly, after having been trained on similar tasks.
The mechanism behind their success, hotheyver, is poorly understood.
they begin this work with an experimental analysis of MAML, finding that deep models are crucial for its success, even given sets of simple tasks where a linear model would suffice on any individual task.
Furthermore, on image-recognition tasks, they find that the early layers of MAML-trained models learn task-invariant features, while later layers are used for adaptation, providing further evidence that these models require greater capacity than is strictly necessary for their individual tasks.
Following their findings, they propose a method which enables better use of model capacity at inference time by separating the adaptation aspect of meta-learning into parameters that are only used for adaptation but are not part of the forward model.
they find that their approach enables more effective meta-learning in smaller models, which are suitably sized for the individual tasks.

Reject. rating score: 3. rating score: 3. rating score: 6. This paper studies optimal control problems where a physical simulator of the system is available, which outputs the gradient of the dynamics. Note that in DDPG, the action is given by a deterministic policy. However, I cannot understand what $\nabla_{\pi} Q$ stands for. 2.Based on the experiments, it seems that the proposed method does not always outperform MPC or DDPG, even in a small scale control problem Mountaincar. Moreover, it seems that the performance is similar to that of the DDPG. 3.Here the model based gradient in equation (2) is defined by only unroll one step forward by going from $s_i, a_i$ to $s_{i+1}$. It would be interesting to see how the number of unroll steps affect the algorithm, which is a gradient version of TD($\lambda$).<BRK>This paper shows how the derivatives from a differentiableenvironment can be used to improve the convergence rate ofthe actor and critic in DDPG. The empirical results show that their method of addingthis information (D3PG) slightly improves DDPG sperformance in the tasks they consider. Why is this?<BRK>It is more widely applicable than traditional model based approaches like MPC, since it doesn t require differentiable models of the dynamics. This paper proposes a method for extending DDPG to exploit simulator gradients. You argue that DRL is better than MPC because DRL explores better. Overall Assessment I recommend acceptance.<BRK>Over the last decade, two competing control strategies have emerged for solving complex control tasks with high efficacy. Model-based control algorithms, such as model-predictive control (MPC) and trajectory optimization, peer into the gradients of underlying system dynamics in order to solve control tasks with high sample efficiency.  Hotheyver, like all gradient-based numerical optimization methods,model-based control methods are sensitive to intializations and are prone to becoming trapped in local minima. Deep reinforcement learning (DRL), on the other hand, can somewhat alleviate these issues by exploring the solution space through sampling — at the expense of computational cost. In this paper, they present a hybrid method that combines the best aspects of gradient-based methods and DRL. they base their algorithm on the deep deterministic policy gradients (DDPG) algorithm and propose a simple modification that uses true gradients from a differentiable physical simulator to increase the convergence rate of both the actor and the critic.  they demonstrate their algorithm on seven 2D robot control tasks, with the most complex one being a differentiable half cheetah with hard contact constraints. Empirical results show that their method boosts the performance of DDPGwithout sacrificing its robustness to local minima.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. This paper presents a small multiple choice reading comprehension dataset drawn from LSAT and GMAT exams. I like the idea of using these standardized tests as benchmarks for machine reading, and I think this will be a valuable resource for the community. The two major concerns I have with this paper are with its presentation and with the quality of the baselines. Presentation:The main contribution here is in collecting a new dataset drawn from the LSAT and GMAT. I have a lot of questions about this dataset that could have been answered in the main text had more of the paper been given to actually describing the data. I think that this is a good paper and it should be accepted. Given that the best performance is at ~54% on the full test set, there isn t a lot of need for this. It s not really clear what to conclude from the easy vs. hard split, other than the models that you used to create it expectedly do well on the easy split and hard on the test split.<BRK>This paper presents a new reading comprehension dataset for logical reasoning. It is a multi choice problem where questions are mainly from GMAT and LSAT, containing 4139 data points. The analyses of the data demonstrate that questions require diverse types of reasoning such as finding necessary/sufficient assumptions, whether statements strengthen/weaken the argument or explain/resolve the situation. The paper includes comprehensive experiments with baselines to identify bias in the dataset, where the answer options only model achieves near half (random is 25%). Based on this result, the test set is split into the easy and hard set, which will help better evaluation of the future models. The paper also reports the numbers on the split data using competitive baselines where the models achieve low performance on the hard set.<BRK>Paper Summary:This paper presents a machine reading comprehension dataset called ReClor. It is different from existing datasets in that ReClor targets logical reasoning. The authors identified biased data points and separated the testing dataset into biased and non biased sets. Experimental results show that state of the art models such as XLNet and RoBERTa struggle on the non biased HARD set with poor performance near that of random guess. Strengths:—The dataset, which is extracted from standardized tests such as GMAT and LSAT, requires the ability to perform complex logical reasoning. Weaknesses:—The dataset seems small to acquire the ability to perform complex logical reasoning. —The paper does not show statistics of the dataset such as question/passage length and question vocabulary size. —Unlike other datasets, the questions themselves show their required logical reasoning types in ReClor. This characteristic makes it difficult to use the ReClor dataset as an evaluation benchmark for models trained with large scaled reading comprehension datasets such as RACE.<BRK>Recent potheyrful pre-trained language models have achieved remarkable performance on most of the popular datasets for reading comprehension. It is time to introduce more challenging datasets to push the development of this field towards more comprehensive reasoning of text. In this paper, they introduce a new Reading Comprehension dataset requiring logical reasoning (ReClor) extracted from standardized graduate admission examinations. As earlier studies suggest, human-annotated datasets usually contain biases, which are often exploited by models to achieve high accuracy without truly understanding the text. In order to comprehensively evaluate the logical reasoning ability of models on ReClor, they propose to identify biased data points and separate them into EASY set while the rest as HARD set. Empirical results show that state-of-the-art models have an outstanding ability to capture biases contained in the dataset with high accuracy on EASY set. Hotheyver, they struggle on HARD set with poor performance near that of random guess, indicating more research is needed to essentially enhance the logical reasoning ability of current models. 
Reject. rating score: 3. rating score: 3. rating score: 6. The paper presents a python package, called JAX MD for simulating molecular dynamics (MD). JAX MD provides automatic derivations and allows to easily incorporate machine learning models in the MD workflow. The paper is clearly written and seems technically correct. Furthermore, even if this work will surely be of great use for the physics community, I am not not sure that the contribution of this paper is sufficient for ICLR.<BRK>This paper describes a general purpose differentiable molecular dynamics physics package, JAX MD. It shows several instances, where it simplifies the research process and enables new avenues of work. The Github link is provided for reproducible research and future development. It should be encouraged. I am sure whether this paper fit the ICLR or not, or how deep learning community can benefit from it. The writing does not feel academic enough sometime. For example,  "Please let us know if there are features that you would find interesting.<BRK>I lean toward accepting this submission. If it were only about simulation molecular dynamics using hardware accelerators, I would question the appropriateness of the venue, but because it is explicitly intended to support training and usage of learned potential functions, it seems suitable. Still might be better placed in a physics/chemistry venue, as where most of the references come from and likely where users would, too. The paper is clearly written, with enough specific examples to contrast previous pain points in this line of work against its smoother interface. All of these points are fine for a package release/tutorial paper, but for a conference paper, might hope to see these addressed:Description of the elements of the design of JAX which are useful here are presented, and appear distinct from other AD libraries like Tensorflow or PyTorch, although the authors stop short of explicitly stating which functionality would be more difficult/impossible to support with the possible alternatives (automatic vectorization of the simulations seems like one?). are not explicitly mentioned. Despite mentioning numerous existing MD libraries, no performance comparison is drawn against any other. Could also show some demonstration of running an experiment which has complexity on par with state of the art research? The bubble raft example is great for illustrative purposes, but it could be better to save some of that for a tutorial and use space to exercise this library on a relevant problem and show performance there.<BRK>A large fraction of computational science involves simulating the dynamics of particles that interact via pairwise or many-body interactions. These simulations, called Molecular Dynamics (MD), span a vast range of subjects from physics and materials science to biochemistry and drug discovery. Most MD software involves significant use of handwritten derivatives and code reuse across C++, FORTRAN, and CUDA. This is reminiscent of the state of machine learning before automatic differentiation became popular. In this work they bring the substantial advances in software that have taken place in machine learning to MD with JAX, M.D. (JAX MD). JAX MD is an end-to-end differentiable MD package written entirely in Python that can be just-in-time compiled to CPU, GPU, or TPU. JAX MD allows researchers to iterate extremely quickly and lets researchers easily incorporate machine learning models into their workflows. Finally, since all of the simulation code is written in Python, researchers can have unprecedented flexibility in setting up experiments without having to edit any low-level C++ or CUDA code. In addition to making existing workloads easier, JAX MD allows researchers to take derivatives through whole-simulations as theyll as seamlessly incorporate neural networks into simulations. This paper explores the architecture of JAX MD and its capabilities through several vignettes. Code is available at github.com/jaxmd/jax-md along with an interactive Colab notebook.
Reject. rating score: 1. rating score: 6. rating score: 6. Since the authors place their work in the setup of sequential prediction, this is what has to be respected. In general I like the idea, and the presentation seems solid to a large degree. I have two more questions with respect to the proposed regularisations.<BRK>The main innovation, in my opinion, is the combination of several ideas applied to the problem of sequence prediction.<BRK>The paper is clearly motivated and easy to follow. Experiment results on MNIST, Stanford Drone and HighD datasets show the proposed that the model achieves better results than previous state of the art models by significant margins.<BRK>Prediction of future states of the environment and interacting agents is a key competence required for autonomous agents to operate successfully in the real world. Prior work for structured sequence prediction based on latent variable models imposes a uni-modal standard Gaussian prior on the latent variables. This induces a strong model bias which makes it challenging to fully capture the multi-modality of the distribution of the future states. In this work, they introduce Conditional Flow Variational Autoencoders (CF-VAE) using their novel conditional normalizing flow based prior to capture complex multi-modal conditional distributions for effective structured sequence prediction. Moreover, they propose two novel regularization schemes which stabilizes training and deals with posterior collapse for stable training and better match to the data distribution. their experiments on three multi-modal structured sequence prediction datasets -- MNIST Sequences, Stanford Drone and HighD -- show that the proposed method obtains state of art results across different evaluation metrics.
Reject. rating score: 1. rating score: 3. rating score: 3. The paper I find very poorly written hence my certainty about understanding the method cannot be very high. I have added some initial pointers that would help making this more readable but implementing these would only allow us to assess what is being done rather than guarantee acceptance. Since the rebuttal is not intended as a deadline extension I recommend rejecting this paper! There is content but what the reader cares about it situating the paper in the landscape of existing methods. It needs to tell us roughly what is similar in this work to what was previously existing (roughly at least). Introducing formulas without explaining notation like eq (1 4) serves only to alienate the reader and the (well intentioned) reviewer.<BRK>Overall, I find the idea interesting and the experimental evaluations promising. I found the paper hard to understand. If so, where is the model actually being used during training? Figure 1 is taken directly from the PPN paper without any reference or citation (as far as I can tell). I think the writing has improved significantly, but could still be further improved and clarified. At least for me some of the confusion arises not due to the complexity of the proposed approach, but just because combining real and  simulated  transitions can be used and mixed in so many different ways that it s important to be clear about it. Overall, I think the presentation is on a good way but needs some more work.<BRK>The notation is also confusing. It seems to me that pi(a|s) is the density of action a, so what does sqrt(value * density) mean? Besides, I have some questions. 1.Figure 1 looks the same as Figure 1 in https://arxiv.org/pdf/1909.07373.pdf, but I don t find any reference in the paper. More importantly, note that the policy in PPO is stochastic, so how is PTN compared to the deterministic policy? One can have infinite b but sample a uniformly to optimize Q (and then pi_F becomes maxQ policy), so I don t think b can be simply characterized as the confidence.<BRK>Decision-time planning policies with implicit dynamics models have been shown to work in discrete action spaces with Q learning. Hotheyver, decision-time planning with implicit dynamics models in continuous action space has proven to be a difficult problem. Recent work in Reinforcement Learning has allotheyd for implicit model based approaches to be extended to Policy Gradient methods. In this work they propose Policy Tree Network (PTN). Policy Tree Network lies at the intersection of Model-Based Reinforcement Learning and Model-Free Reinforcement Learning. Policy Tree Network is a novel approach which, for the first time, demonstrates how to leverage an implicit model to perform decision-time planning with Policy Gradient methods in continuous action spaces. This work is empirically justified on 8 standard MuJoCo environments so that it can easily be compared with similar work done in this area. Additionally, they offer a lotheyr bound on the worst case change in the mean of the policy when tree planning is used and theoretically justify their design choices.
Reject. rating score: 3. rating score: 6. rating score: 6. rating score: 6. Actually minimizing the usual isotropic 2D TV is tricky, and there are many optimization papers written about it. It s an interesting combination of compressed sensing and deep learning. Overall, I lean toward weak reject, as I think the regularization parts of the paper would benefit from a major revision of the paper. But pure CS, with a Gaussian measurement matrix, and noiseless measurements, is quite academic and not applicable to the real world. The experiments were also noiseless, and they had Gaussian or the Fourier sampling (which is not random, but rather heavily biased toward DC). The writing was overall good. If that s true, then please show it!<BRK>This paper proposes use of the deep image prior (DIP) in compressed sensing. It is especially beneficial in that it does not require training using a large scale dataset if the learned regularization is not used. Results of numerical experiments demonstrate empirical superiority of the proposed method on the reconstruction of chest x ray images as well as on that of the MNIST handwritten digit images. I would thus like to recommend "weak accept" of this paper. I think that the theoretical result of this paper, summarized as Theorem 4.1, does not tell us much about CS DIP. As the authors argue, it would suggest necessity of early stopping in the proposal.<BRK>This paper proposes a new algorithm for compressed sensing using untrained generative model. This provides theoretical justification for early stopping adopted by the authors. My complaint to the paper is that the theoretical justification of the generalization ability of the proposed method is missing. The error goes to 0 is not surprising by many existing works, and the authors completely ignore the generalization error analysis of the proposed method.<BRK>This paper proposes a deep learning based compressed sensing method, CS DIP which employs deep image prior algorithm to recovering signals especially for medical images from noisy measurements without pre learning over large dataset. Experimental results show that the proposed methods outperformed the others. The theoretical analysis of early stopping is also given. Overall, the writing of this paper is good, the idea is novel and the contribution is sufficient.<BRK>they propose a novel method for compressed sensing recovery using
untrained deep generative models. their method is based on the recently
proposed Deep Image Prior (DIP), wherein the convolutional theyights of
the network are optimized to match the observed measurements. they show
that this approach can be applied to solve any differentiable linear inverse
problem, outperforming previous unlearned methods. Unlike various learned approaches based on generative models, their method does not require pre-training over large datasets. they further introduce a novel learned regularization technique, which incorporates prior information on the network theyights. This reduces reconstruction error, especially for noisy measurements. Finally they prove that, using the DIP optimization approach, moderately overparameterized single-layer networks trained can perfectly fit any signal despite the nonconvex nature of the fitting problem. This theoretical result provides justification for early stopping.
Accept (Spotlight). rating score: 8. rating score: 6. rating score: 6. This paper proposes to incorporate a latent model (in the form of a variational auto encoder) in the decoding process of neural machine translation. So far there are not that many successful demonstrations of VAE in neural machine translation. The method is sound and interesting. The results show convincing improvements; some practioners may argue that reported BLEU gain (e.g.0.8) is not impressive, but I think for a new model like this it is worthy. This will help distinguish whether the proposed method is improving by getting the morphological inflections correct, or whether it is improving across the board on various word types. I wonder if different kinds of latent spaces will be learned with different depth.<BRK>This paper addresses the problem of translating into morphologically rich languages, which suffers from the problem of sparse vocabularies and high numbers of rare and unseen words. More concretely, this paper models the generation of target words in a stochastic, hierarchical process, where the morphological features are modelled as latent variables. Experiments on translations into three morphologically rich languages, English {Arabic, Czech, Turkish}, indicate fairly small but consistent and statistically significant gains in BLEU. Overall, this paper proposes an elegant solution to an important problem, and yields statistically significant BLEU improvements over the baselines. I would be willing to raise my scores assuming my concerns are sufficiently addressed. Pros:1.The proposed approach is easy to follow and explained clearly, and the paper is overall well written. This would better convince the reader that the latent inflectional features really are capturing useful morphological information, and that the target word generation process is appropriately controlled by the latent variable. The explanation that "[the character model] s capacity cannot cope with the increased amount of sparsity" does not seem satisfactory. 2.It would be interesting to explore other potential metric for measuring morphological generalisation.<BRK>This paper proposes a method, Latent Morphology Model (LMM), for producing word representations for a (hierarchical) character level decoder used in neural machine translation (NMT). The literature review of NMT and the discussion on the potential advantage of morphology are concise. Overall, this paper could provide a novel insight into the role of modeling morphology as latent variables. However, the experiments and analyses do not sufficiently support the claim of mimicking the process of the inflection (besides the gain in performance). A contrast of performance with less morphological languages might reveal some insight (unfortunately I do not have enough knowledge to recommend languages).<BRK>Translation into morphologically-rich languages challenges neural machine translation (NMT) models with extremely sparse vocabularies where atomic treatment of surface forms is unrealistic. This problem is typically addressed by either pre-processing words into subword units or performing translation directly at the level of characters. The former is based on word segmentation algorithms optimized using corpus-level statistics with no regard to the translation task. The latter learns directly from translation data but requires rather deep architectures. In this paper, they propose to translate words by modeling word formation through a hierarchical latent variable model which mimics the process of morphological inflection. their model generates words one character at a time by composing two latent representations: a continuous one, aimed at capturing the lexical semantics, and a set of (approximately) discrete features, aimed at capturing the morphosyntactic function, which are shared among different surface forms. their model achieves better accuracy in translation into three morphologically-rich languages than conventional open-vocabulary NMT methods, while also demonstrating a better generalization capacity under low to mid-restheirce settings.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. The paper focuses on in domain uncertainty estimation complementing the recent similar review on out of domain uncertainty estimation. Based on this, I believe this is a strong technical paper and it should be accepted.<BRK>After exploring common standards for uncertainty quantification, the authors point out pitfalls of existing metrics by investigating different ensembling techniques and introduce a novel metric called deep ensemble equivalent (DEE) that essentially measures the number of independent models in an ensemble of DNNs. The relationship to previous works is also well described. Overall, I think this is a good paper, which gives a detailed overview of existing metrics for accessing the quality in in domain uncertainty estimation.<BRK>To that end, why do the authors suggest that it cannot be used as criteria for comparison across models? Why is this the case? I went back and did another thorough read of the work. The authors evaluate a variety of ensemble models in terms of their ability to capture in domain uncertainity.<BRK>Uncertainty estimation and ensembling methods go hand-in-hand. Uncertainty estimation is one of the main benchmarks for assessment of ensembling performance. At the same time, deep learning ensembles have provided state-of-the-art results in uncertainty estimation. In this work, they focus on in-domain uncertainty for image classification. they explore the standards for its quantification and point out pitfalls of existing metrics. Avoiding these pitfalls, they perform a broad study of different ensembling techniques. To provide more insight in this study, they introduce the deep ensemble equivalent score (DEE) and show that many sophisticated ensembling techniques are equivalent to an ensemble of only few independently trained networks in terms of test performance.
Reject. rating score: 6. rating score: 6. rating score: 6. The meat of the paper is found in Section 4, which I found a bit difficult to follow. A stochastic variational inference scheme is then proposed to infer the posterior over both the Gaussian process and the weights themselves. My primary concern here is that the prior ends up becoming Kronecker structured (after Eq.7), so it isn’t clear to my why dense matrices and dense variational bounds have to be derived in this setting. Furthermore, Kronecker inference for non Gaussian likelihoods for Laplace approximations was proposed back in 2015: http://proceedings.mlr.press/v37/flaxman15.pdf. I do agree that the approximation in Figure 2 does seem to be relatively accurate, although I would ask the authors to compute a relative error for that plot if possible. **tldr**: While I appreciate the concept of this paper, I tend to reject this paper because I find the experimental results to be on too small scale of datasets. Could the authors perform a set of experiments showing the necessity of this kernel matrix in the rebuttal? Neural processes (Garnelo et al; 2018) propose a somewhat similar approach to training – with a latent process over some stored weight space. However, even that is quite distinct from the method proposed in this paper, and I tend to prefer this approach. **Quality**: I really appreciate the merging of neural network and Gaussian process methods; however, tragically, I do wonder if the proposed approach combines the worst of both worlds – the necessity of architecture search for neural networks with the choice of kernel function (as illustrated in Figure 5). Active learning experiment: While I appreciate the comparison here, it seems like here standard HMC should be trainable over well designed priors on these architectures. With that being said, to only have experiments on the last layer implies that one should compare to Bayesian logistic regression and linear regression on the last layer of neural networks (e.g Perrone et al, 2018 and Riquelme et al, 2018). Experiments with other methods that combine Gaussian processes with representations on the final layer (e.g.Wilson et al, 2015) are also probably worth running. Figure 4 is a very well done experiment, if a bit tough to read. I’d suggest that the out of distribution examples get their own figure, with the in distribution examples going into the appendix.<BRK>This paper proposes an improvement over Probabilistic meta representations of neural networks by replacing the NN parameterization of the network weights given latent variables by a probabilistic distribution whose mean is distributed by GP. Inference of the induced hierarchical model is achieved by variational inference and variousapproximations when needed. The authors claim that the proposed method aims to increase the robustness in the small data settingsand improve its out of sample uncertainty estimates. The second part is well justified. 2.What exactly is the gain obtained by replacing a NN parameterization with a GP parameterization? It seems like the proposed method gains the ability to model uncertainty, but potentially incurs performance trade off from a series of approximation. 3.Following from comment #2, I am a bit surprised by the lack of comparison against the work of Karaletsos although this work was built on top of it. 5.For the text before Eq.(8) should the inducing inputs be xu instead of zu? It seems like a systematictypo here because in the formulation for the lower bound it becomes p(u|xu) again instead of p(u|zu)6. This would not permit large NN anyway, which kind of defeats the purpose. Overall comment:I think the paper presents an interesting idea but I have questions regarding its practical significance as highlighted in my specific comments above. I hope the authors would clarify these so I can converge on a final rating.<BRK>The paper presents two models extended from a meta presentation of neural networks (Karaletsos et al.2018) in which neural network weights are constructed hierarchically from latent variables associated with each layer. Variational inference is followed by the pseudo inducing point approach. Pros:The paper is clearly written. This can be one of the reasons that the method performed well in active learning. Cons:The approach is incremental or not so novel in terms of meta representation for neural networks. Comments and questions:The prior distribution for latent variable $z$ is not specified. I assume the prior is independent Gaussian. Do you think that there is a connection between contextual metaGP and residual nets? Can you comment on the convergence of the estimation of the last term in the variational bound? Minor: a missing period in Sec.6.3 “quadratic kernel In this example”<BRK>Bayesian inference offers a theoretically grounded and general way to train neural networks and can potentially give calibrated uncertainty. It is, hotheyver, challenging to specify a meaningful and tractable prior over the network parameters. More crucially, many existing inference methods assume mean-field approximate posteriors, ignoring interactions bettheyen parameters in high-dimensional theyight space. To this end, this paper introduces two innovations: (i) a Gaussian process-based hierarchical model for the network parameters based on recently introduced unit embeddings that can flexibly encode theyight structures, and (ii) input-dependent contextual variables for the theyight prior that can provide convenient ways to regularize the function space being modeled by the NN through the use of kernels. 
Furthermore, they develop an efficient structured variational inference scheme that alleviates the need to perform inference in the theyight space whilst retaining and learning non-trivial correlations bettheyen network parameters. 
they show these models provide desirable test-time uncertainty estimates, demonstrate cases of modeling inductive biases for neural networks with kernels and demonstrate competitive predictive performance of the proposed model and algorithm over alternative approaches on a range of classification and active learning tasks.
Reject. rating score: 1. rating score: 3. rating score: 3. The technique is reminiscent of t SNE, with the difference that it get weighted triplets (i,j,k) as inputs (meaning that j is closer to i thank). A paper is purely experimental. I did not see any evidence that images of TriMap somehow give a new insight into data (in comparison with other methods).<BRK>The paper proposed ``TriMap’’ a novel dimensionality reduction technique that learns to preserve relative distances among points in a triplet. If the latter holds, then how was it chosen and how large is it compared to the training data used by other methods? Major merits of this paper are: 1. But I am not convinced that the preserved information is actually global.<BRK>Authors introduce TriMap based on triplet constraints that preserves the global accuracy of the data. A measure of global accuracy is proposed to reflect the global accuracy of the embedding. Experiments on various datasets the better performance than baselines. Authors define the minimum reconstruction error from the embedding as the global measure in reflecting the global structure of the data similar to PCA.<BRK>they introduce ``TriMap''; a dimensionality reduction technique based on triplet constraints that preserves the global accuracy of the data  better than the other commonly used methods such as t-SNE, LargeVis, and UMAP. To quantify the global accuracy, they introduce a score which roughly reflects the relative placement of the clusters rather than the individual points. they empirically show the excellent performance of TriMap on a large variety of datasets in terms of the quality of the embedding as theyll as the runtime. On their performance benchmarks, TriMap easily scales to millions of points without depleting the memory and clearly outperforms t-SNE, LargeVis, and UMAP in terms of runtime.
Reject. rating score: 1. rating score: 3. rating score: 6. The authors also present the spectrum and leading eigenspace of the Gram matrix for representations by CNNs of images generated by a GAN. It is not clear to this reviewer what exactly the authors have proved and what significance it has. This is well known from the random matrix theory literature. The authors claim that their results constitute "a first step towards the theoretical understanding of complex objects such as DL representations." This very conference is on Learning Representations, and presumably at least one paper in the past 7 years makes progress towards the theoretical understanding of Deep Learning representations.<BRK>This paper provides a formal proof that the data produced by a GAN are concentrated vectors. To what extent could these results extend to other distributions? This theorem is essentially a concentration bound for the resolvent of the gram matrix G around its mean. The expectation is already computed in (Louart & Couillet, 2019) so the contribution in this theorem seems to be in showing that the result only depends on the first and second order statistics. You claim this is a surprising result, although it does not seem so surprising to me given that this is an asymptotic result.<BRK>The authors generalize Gaussian random vectors to a broader class of "concentrated" vectors which they use as their primary tool for analysis of the latent representations learned by GANs. They show that the spectral behavior (i.e.spectra and leading eigenspaces) of the Gram matrix computed over GAN representations is the same as those produced by a high dimensional Gaussian Mixture Models (GMMs). Thus, for data that follows Gaussian mixture patterns, GANs and GMMs behave identically. Overall, the paper is well organized and the theoretical results are both compelling and thorough. The experimental results also follow nicely from the theory. Admittedly, this reviewer is not well versed enough in this area of mathematics to provide thorough critical insight about the derivations and proofs.<BRK>This paper shows that deep learning (DL) representations of data produced by generative adversarial nets (GANs) are random vectors which fall within the class of so-called concentrated random vectors. Further exploiting the fact that Gram matrices, of the type G = X'X with X = [x_1 , . . . , x_n ] ∈ R p×n and x_i independent concentrated random vectors from a mixture model, behave asymptotically (as n, p → ∞) as if the x_i theyre drawn from a Gaussian mixture, suggests that DL representations of GAN-data can be fully described by their first two statistical moments for a wide range of standard classifiers. their theoretical findings are validated by generating images with the BigGAN model and across different popular deep representation networks.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. The paper proposes a new graph pooling method by learning the node assignments from a CRF based structure prediction formulation. To justify the usefulness of CRFs instead of other clustering methods for node assignment, I think there should be a baseline which removes the pairwise energy and just use the unary energy for node clustering. The paper is written clearly, and the experimental results are good. (BTW, it is better to add the related works which combined CRF and GNNs, considering the close connection to this paper.)<BRK>The authors here introduces a novel  graph pooling technique called StructPool that uses the underlying graph’s structural information to behave as a node clustering algorithm and learns a node clustering matrix. The unary potentials of the cliques are computed used the GCN to measure energy of each node. I have  a few questions as below:I think the authors can better elucidate the motivation for using  the attention matrix over Gaussian kernels to measure pairwise energy in section 3.3; an  empirical experiment for drawing comparison wrt to the computational time and number of feature dimensions on a toy problem seems important. Otherwise, the paper is rather well written and has clarity.<BRK>Strength:  An interesting idea to use CRF idea to cluster the nodes on a graph for pooling purpose  The paper is well written and easy to follow   On a few datasets and task, the proposed method works pretty wellWeakness:  The computational complexity of the proposed algorithm is on(n^3), which is too expensive This paper studied to use CRF to define the cluster assignment of the nodes  for graph pooling, which model the dependency of the cluster assignments between the nodes. What would be the results if multiple layers are used?<BRK>Learning high-level representations for graphs is of great importance for graph analysis tasks. In addition to graph convolution, graph pooling is an important but less explored research area. In particular, most of existing graph pooling techniques do not consider the graph structural information explicitly. they argue that such information is important and develop a novel graph pooling technique, know as the StructPool, in this work. they consider the graph pooling as a node clustering problem, which requires the learning of a cluster assignment matrix. they propose to formulate it as a structured prediction problem and employ conditional random fields to capture the relationships among assignments of different nodes.  they also generalize their method to incorporate graph topological information in designing the Gibbs energy function.  Experimental results on multiple datasets demonstrate the effectiveness of their proposed StructPool.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. rating score: 6. This paper extends Jin et al.(2018) s idea to infinite horizon and improves the best known sample complexity to $\tilde{O}(\frac{SA}{\epsilon^2 (1 \gamma)^7})$. Overall I think this paper is sufficient to get in the conference. Novelty: In section 3.2, the authors discuss the difference between finite case and infinite case. In this case, the latter rewards counted for $V(s_1)$ will multiply by $\gamma^t$ which is pretty small, and will not contribute too much on the error.<BRK>Summary:In this paper, the authors extend the UCB Q learning algorithm by Jin et al.(2018) to infinite horizon discounted MDPs, and prove a PAC bound of \tilde{O}(SA/\epsilon^2 (1 \gamma)^7) for the resulting algorithm. The algorithm is quite similar to the one by Jin et al.(2018), which is for finite horizon problems. The resulting regret bound is similar to the one in Jin et al.(2018), but the PAC bound is new. This bound improves the one for delayed Q learning by Strehl et al.(2006) and matches the lower bound in terms of \epsilon, S, and A.<BRK>Summary: This paper adapts the UCB Q learning method to the inifinite horizon discounted MDP setting. This is an important setting in reinforcement learning. However, the bound is not optimal as the dependence of (1 gamma) is significantly larger than the lower bound. As a result, the analysis and algorithm in this paper are very similar to that of Jin et al 2018, who nearly implicitly contain the results in this paper. Thus, we obtain an algorithm for the inf horizon as well.<BRK>This paper considered a Q learning algorithm with UCB exploration policy for infinite horizon MDP, and derived the sample complexity of exploration bound. The bound was shown to improve the existing results and matched the lower bound up to some log factors. ~~~~~~~~~~~~~~~~~~~~~~After rebuttal: Thanks the authors for addressing my questions.<BRK>A fundamental question in reinforcement learning is whether model-free algorithms are sample efficient. Recently,  Jin et al. (2018) proposed a Q-learning algorithm with UCB exploration policy, and proved it has nearly optimal regret bound for finite-horizon episodic MDP. In this paper, they adapt Q-learning with UCB-exploration bonus to infinite-horizon MDP with discounted rewards \emph{without} accessing a generative model. they show that the \textit{sample complexity of exploration} of their algorithm is bounded by $\tilde{O}({\frac{SA}{\epsilon^2(1-\gamma)^7}})$. This improves the previously best known result of $\tilde{O}({\frac{SA}{\epsilon^4(1-\gamma)^8}})$ in this setting achieved by delayed Q-learning (Strehlet al., 2006),, and matches the lotheyr bound in terms of $\epsilon$ as theyll as $S$ and $A$ up to logarithmic factors.
Reject. rating score: 1. rating score: 1. rating score: 3. The paper "Abstractive Dialog Summarization with Semantic Scaffolds" presents a new architecture that the authors claim is more suited for summarizing dialogues. The new architecture is a minor variation of an existing pointer generator network presented by See et al.First the authors used two different sets of parameters to encode the user and the system responses. The authors also pre process the dialog by replacing the slot values by their slot keys. Finally, the authors use an auxiliary task of detecting the domain of the dialog. These three different enhancements are all called "scaffolds" by the authors, hance the title of the paper. This paper is not suited for ICLR because of its limited novelty. The three enhancements proposed by the authors are long known and incremental.<BRK>Authors proposed an enhanced Pointer Generator model called SPNet. The key difference between SPNet and PG are the separate handling or using of speaker role, semantic slot and domain labels. Authors also proposed a new metrics called Critical Information Completeness (CIC) to address ROUGE s weakness in assessing if key information is missing in the output. The above inconsistency suggests the paper may not be quite ready for publication.<BRK>The authors also extend the pointer generator network of See et al.(2018) to use speaker, semantic slot and domain information. They show that this new model (SPNet) outperforms the baseline on existing automatic metrics, on a new metric tuned to measure recall on slots (dubbed CIC), and a thorough human evaluation. This paper is well written and executed, but unfortunately, I lean towards rejecting this paper because of a fundamental flaw in the nature of the proposed dataset that limits its applicability to the task of abstractive dialog summarization (more below).<BRK>The demand for abstractive dialog summary is growing in real-world applications. For example, customer service center or hospitals would like to summarize customer service interaction and doctor-patient interaction. Hotheyver, few researchers explored abstractive summarization on dialogs due to the lack of  suitable datasets. they propose an abstractive dialog summarization dataset based on MultiWOZ. If they directly apply previous state-of-the-art document summarization methods on dialogs, there are two significant drawbacks: the informative entities such as restaurant names are difficult to preserve, and the contents from different dialog domains are sometimes mismatched. To address these two drawbacks, they propose Scaffold Pointer Network (SPNet) to utilize the existing annotation on speaker role, semantic slot and dialog domain. SPNet incorporates these semantic scaffolds for dialog summarization. Since ROUGE cannot capture the two drawbacks mentioned, they also propose a new evaluation metric that considers critical informative entities in the text. On MultiWOZ, their proposed SPNet outperforms state-of-the-art abstractive summarization methods on all the automatic and human evaluation metrics.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. the paper attempts to infer Granger causality between nonlinearly interacting stochastic processes from their time series measurements. in particular, they use group wise regularizing to accompany the particular structure of the model to aid interpretability. they compared the performance with existing models with MLP/LSTM and show some gains in a few examples (but not all.)<BRK>In this paper the authors propose using Statistical Recurrent Units to predict the network for Granger causality. The authors clearly present their two main contributions and verify them using varied datasets. The inclusion of the literature review in the appendix is also greatly appreciated.<BRK>The experiments seem convincing to me and I really appreciate the authors provided the tuned hyper parameters in the appendix. This paper extended this algorithm for inferring granger causality through applying group sparse regularization, which is a pretty smart design to me. I can follow the paper without problem. The effectiveness of the algorithm depends on the sparse regularization, which lacks theoretical guarantee for non convex optimization.<BRK>Granger causality is a widely-used criterion for analyzing interactions in large-scale networks. As most physical interactions are inherently nonlinear, they consider the problem of inferring the existence of pairwise Granger causality bettheyen nonlinearly interacting stochastic processes from their time series measurements. their proposed approach relies on modeling the embedded nonlinearities in the measurements using a component-wise time series prediction model based on Statistical Recurrent Units (SRUs). they make a case that the network topology of Granger causal relations is directly inferrable from a structured sparse estimate of the internal parameters of the SRU networks trained to predict the processes’ time series measurements. they propose a variant of SRU, called economy-SRU, which, by design has considerably fetheyr trainable parameters, and therefore less prone to overfitting. The economy-SRU computes a low-dimensional sketch of its high-dimensional hidden state in the form of random projections to generate the feedback for its recurrent processing. Additionally, the internal theyight parameters of the economy-SRU are strategically regularized in a group-wise manner to facilitate the proposed network in extracting meaningful predictive features that are highly time-localized to mimic real-world causal events. Extensive experiments are carried out to demonstrate that the proposed economy-SRU based time series prediction model outperforms the MLP, LSTM and attention-gated CNN-based time series models considered previously for inferring Granger causality. 
Accept (Poster). rating score: 6. rating score: 6. rating score: 1. This paper introduces the problem of overlearning, which can be thought of as unintended transfer learning from a (victim) source model to a target task that the source model’s creator had not intended its model to be used for. The paper raises good points about privacy legislation limitations due to the fact that overlearning makes it impossible to foresee future uses of a given dataset.<BRK>This paper demonstrates some limitations of censoring for privacy with respect to sensitive attributes. In particular, the authors show that censoring reduces, but does not eliminate, the ability of a neural network to infer private/sensitive attributes, e.g.to infer race from a model aiming to predict gender. The authors show that censoring strength often does reduce the ability to infer sensitive attributes, but also affects the ability to perform the main (non sensitive) task; and in some cases, may actually increase ability to infer sensitive attributes.<BRK>This paper highlights the problem of model overlearning   learning more than it is trained to do. Did they not observe overlearning in these models? 4.As for section 4.4, it is pretty understood that lower layers of a DL model, learns very basic low level features from the images such as edges, corner.<BRK>``"Overlearning'' means that a model trained for a seemingly simple
objective implicitly learns to recognize attributes and concepts that are
(1) not part of the learning objective, and (2) sensitive from a privacy
or bias perspective.  For example, a binary gender classifier of facial
images also learns to recognize races, even races that are
not represented in the training data, and identities.

they demonstrate overlearning in several vision and NLP models and analyze
its harmful consequences.  First, inference-time representations of an
overlearned model reveal sensitive attributes of the input, breaking
privacy protections such as model partitioning.  Second, an overlearned
model can be "`re-purposed'' for a different, privacy-violating task
even in the absence of the original training data.

they show that overlearning is intrinsic for some tasks and cannot be
prevented by censoring unwanted attributes.  Finally, they investigate
where, when, and why overlearning happens during model training.
Reject. rating score: 3. rating score: 6. rating score: 6. The submission proposes a novel method for explicit decomposition of hierarchical policies for long horizon navigation tasks. The approach proposes to separate a policy into 3 modules, high level planner, intermediate planner and low level control. The variety of different techniques combined instead of a single main contribution renders it challenging to follow all aspects and in particular to trace relevant contributions to performance   which is rendered harder by a limited evaluation section. It is commendable that the authors have introduced adaptations and improvements to their baselines for a stronger and fairer comparison but the evaluation remains very limited. But more importantly I suggest to run further ablations without the intermediate planning layer & with absolute goal positions.<BRK>The paper proposes a neat framework for creating HRL framework that will be able to generalize its application to slightly different environment layout. This is done via an image based top down from as input to the high level. The results are also only shown for a single environment. This should be discussed more in the paper. Are these averaged because the agent has a stochastic policy during evaluation? The authors point out that the use of relative goal positions "ensures generalization to new environments", this is a rather strong statement. For example, a top down view of the environment is needed which is not often feasible. HIRO and HAC use a more proprioceptive state space but I don t think the sharing of global states is intentional. In section 4.3 it says that the control layer is the only layer with access to the agent s proprioceptive state.<BRK>This paper addresses hierarchical deep reinforcement learning (RL), an important problem in control learning and RL. In contrast, HiDe uses a top down view of the maze and the x y position of the agent, which certainly is more privileged information. Second, the experimental setup should be elaborated on: is HIRO or HAC modified to include the same information for the top level policy? I do not think that requiring this information is egregious, but currently the experimental comparison is not clear in this regard. The experiments are arguably the strongest part of the paper, and the transfer results and videos are quite nice. Edit after author response: I appreciate the authors  efforts in providing extensive responses to all of the reviewers  concerns as well as a significant general response detailing what seems to be a large amount of additional experimental work.<BRK>Solving long-horizon sequential decision making tasks in environments with sparse rewards is a longstanding problem in reinforcement learning (RL) research. Hierarchical Reinforcement Learning (HRL) has held the promise to enhance the capabilities of RL agents via operation on different levels of temporal abstraction. Despite the success of recent works in dealing with inherent nonstationarity and sample complexity, it remains difficult to generalize to unseen environments and to transfer different layers of the policy to other agents. In this paper, they propose a novel HRL architecture, Hierarchical Decompositional Reinforcement Learning (HiDe), which allows decomposition of the hierarchical layers into independent subtasks, yet allows for joint training of all layers in end-to-end manner. The main insight is to combine a control policy on a lotheyr level with an image-based planning policy on a higher level. they evaluate their method on various complex continuous control tasks for navigation, demonstrating that generalization across environments and transfer of higher level policies can be achieved. See videos https://sites.google.com/view/hide-rl
Accept (Spotlight). rating score: 8. rating score: 6. rating score: 6. This paper extends recent work by Khemakhem et al on nonlinear ICA to allow for unknown number of generative factors. I think that this research direction is extremely promising, and obtaining a theoretical understanding of when/why disentangling could work would be particularly valuable to the field, and I would lean towards acceptance so that this work gets more attention.<BRK>The paper s presentation is relatively clear, although all the theoretical results are relegated to the appendix. The extension to unknown latent space dimension seems to be quite straightforward, given the recent work that this paper is based on. However, the experimental results performed on EMNIST are quite convincing and the results are interesting.<BRK>This paper builds upon the recent theoretical framework on nonlinear ICA, put forward in recent work Khemakhem et al.(2019) that draw a lot of attention. On the negative side, The paper is largely based on the results of a recent technical report (Khemakhem et al.(2019)) and is not self contained, hence rather hard to digest. It would be also informative to discuss the links with this approach.<BRK>A central question of representation learning asks under which conditions it is possible to reconstruct the true latent variables of an arbitrarily complex generative process. Recent breakthrough work by Khemakhem et al. (2019) on nonlinear ICA has anstheyred this question for a broad class of conditional generative processes. they extend this important result in a direction relevant for application to real-world data. First, they generalize the theory to the case of unknown intrinsic problem dimension and prove that in some special (but not very restrictive) cases, informative latent variables will be automatically separated from noise by an estimating model. Furthermore, the recovered informative latent variables will be in one-to-one correspondence with the true latent variables of the generating process, up to a trivial component-wise transformation. Second, they introduce a modification of the RealNVP invertible neural network architecture (Dinh et al. (2016)) which is particularly suitable for this type of problem: the General Incompressible-flow Network (GIN). Experiments on artificial data and EMNIST demonstrate that theoretical predictions are indeed verified in practice. In particular, they provide a detailed set of exactly 22 informative latent variables extracted from EMNIST.
Accept (Talk). rating score: 8. rating score: 8. rating score: 6. The key idea to theapproach is to keep additional statistics about the number of on going simulations from each of the nodes in the tree. I recommend that this paper be accepted. The approach is well motivated and clearly explained, and is supported by the experimental results. The experiments are reasonably thorough and demonstrate the claims made in the paper.<BRK>This algorithm is evaluated in two domains, a mobile game called “Joy City” as well as on Atari. While significant effort has been made by the RL community to scale up distributed model free algorithms, less effort has been made for model based algorithms, so it is exciting to see that emphasis here. Overall I thought the main ideas in paper were clear, the proposed method for how to effectively parallelize MCTS was compelling, and the experimental results were impressive. P UCT is also not that descriptive, given that there are other existing algorithms for parallelizing MCTS. Is this the number of steps taken to pass the level?<BRK>The paper introduces a new algorithm for parallelizing monte carlo tree search (MCTS). The paper introduces a new algorithm that updates the visitation counts before evaluating the rollout (which takes long), and therefore allows other workers to explore different parts of the tree as the exploration bonus is decreased for this node. The algorithm is evaluated on the atari games as well on a proprietary game and compared to other parallelized MCTS variants. The makes intuitively a lot of sense, albeit it is very simple and it is a surprise that this has not been tried yet. The algorithm seems to be effective and the evaluations are promising and the paper is also well written. The focus of the paper is planning, not learning.<BRK>Monte Carlo Tree Search (MCTS) algorithms have achieved great success on many challenging benchmarks (e.g., Computer Go). Hotheyver, they generally require a large number of rollouts, making their applications costly. Furthermore, it is also extremely challenging to parallelize MCTS due to its inherent sequential nature: each rollout heavily relies on the statistics (e.g., node visitation counts) estimated from previous simulations to achieve an effective exploration-exploitation tradeoff. In spite of these difficulties, they develop an algorithm, WU-UCT, to effectively parallelize MCTS, which achieves linear speedup and exhibits only limited performance loss with an increasing number of workers. The key idea in WU-UCT is a set of statistics that they introduce to track the number of on-going yet incomplete simulation queries (named as unobserved samples). These statistics are used to modify the UCT tree policy in the selection steps in a principled manner to retain effective exploration-exploitation tradeoff when they parallelize the most time-consuming expansion and simulation steps. Experiments on a proprietary benchmark and the Atari Game benchmark demonstrate the linear speedup and the superior performance of WU-UCT comparing to existing techniques.
Reject. rating score: 1. rating score: 3. rating score: 3. This paper proposes an architecture for synthesizing tools to be used in a reaching task. Is it that tool synthesis is a challenging problem? The paper demonstrates that this approach can achieve ok performance on familiar scenes with familiar tools, but that it fails to generalize when exposed to unfamiliar scenes or unfamiliar tools. The combination of these results suggest that the model has learned something about which tool dimensions are important for being able to solve the types of reaching tasks given in the paper. Specifically, (1) neither the particular task, results, or model are not very compelling, (2) there are no comparisons to meaningful alternatives, and (3) overall I am not quite sure what conclusions I should draw from the paper. However, given the coolness of the problem of tool synthesis, I definitely encourage the authors to continue working on this line of work! 3.Overall, I am not quite sure what I am supposed to get out of the paper.<BRK>The authors constructed an interesting dataset named reaching task, where the model need to predict if the given toolkit is able to solve the corresponding task or not. Using the activation maximisation technique (which was phrased as the imagination process), they are able to modify the input tools into the ones that is suitable to solve the corresponding task. However, I do not find the authors has a strong case of proven it is the case in this manuscript. So, based on the author s assumption, it should not be benefit of having the tool affordance directions in the latent space. This is particularly true for the Scenario E, F, G (the interpolation tasks), which is more about generalization and is more important. The authors used activation maximisation approach to travel in the latent space. Unfortunately, is that a  direction ? Using MoNet to decompose tools from a toolkit is nice.<BRK>This paper proposes an algorithm that learns to synthesize tools for the task of reaching. It s studying an important problem from a cognitive perspective; it also proposes a novel model for the task, building upon SOTA models. However, the current problem formulation and experiment setup are not well justified, and the experiments are quite limited. This is revealed in the results on case H, where the model doesn t work at all. In addition, comparisons with published methods are missing. Due to all these limitations, I lean toward rejection.<BRK>In this paper they investigate an artificial agent's ability to perform task-focused tool synthesis via imagination. their motivation is to explore the richness of information captured by the latent space of an object-centric generative model - and how to exploit it. In particular, their approach employs activation maximisation of a task-based performance predictor to optimise the latent variable of a structured latent-space model in order to generate tool geometries appropriate for the task at hand. they evaluate their model using a novel dataset of synthetic reaching tasks inspired by the cognitive sciences and behavitheiral ecology. In doing so they examine the model's ability to imagine tools for increasingly complex scenario types, beyond those seen during training. their experiments demonstrate that the synthesis process modifies emergent, task-relevant object affordances in a targeted and deliberate way: the agents often specifically modify aspects of the tools which relate to meaningful (yet implicitly learned) concepts such as a tool's length, width and configuration. their results therefore suggest, that task relevant object affordances are implicitly encoded as directions in a structured latent space shaped by experience. 
Accept (Spotlight). rating score: 8. rating score: 6. rating score: 3. I really believe such a suite of RL tasks can indeed be extremely useful to RL researchers developing new algorithms, and as a result I would like to encourage this initiative and see it published at ICLR to help it gain additional traction within the RL community. The paper is easy to read, motivates well the reasons behind bsuite, and shows some convincing examples. However, in my opinion there remain a few important issues with this submission:1. I believe it is important to add one. 3.I wish an anonymized version of the code had been provided, so that reviewers could test it.<BRK>In this paper, the authors propose a set of benchmarks for evaluating different aspects of reinforcement learning algorithms such as generalisation, exploration, and memory. This can be for example showing how the generalisation score proposed here is linked to theoretical accounts. The paper is well written and clear, and generally can provide a useful contribution. Based on section 1.1 and elsewhere, it seems that the main driver for developing this benchmark has been connecting theory to practical algorithms (which in my opinion is an important step).<BRK>Behaviour Suite for Reinforcement LearningIn this paper the authors provide a set of light weighted but dedicated designed environments, so that researchers can use the environments as a quick indication of the ability of the proposed (or existing) algorithms. I think the paper is well written, with the intuition clearly demonstrated. But I believe in general it is a very valuable project that will be beneficial to future research and I would like to recommend for a workshop publication. Pros:  The paper is well written, easy to understand. The project will be of great value to the research community in the near future.<BRK>This paper introduces the Behavitheir Suite for Reinforcement Learning, or bsuite for short. bsuite is a collection of carefully-designed experiments that investigate core capabilities of reinforcement learning (RL) agents with two objectives. First, to collect clear, informative and scalable problems that capture key issues in the design of general and efficient learning algorithms. Second, to study agent behavitheir through their performance on these shared benchmarks. To complement this effort, they open stheirce this http URL, which automates evaluation and analysis of any agent on bsuite. This library facilitates reproducible and accessible research on the core issues in RL, and ultimately the design of superior learning algorithms. their code is Python, and easy to use within existing projects. they include examples with OpenAI Baselines, Dopamine as theyll as new reference implementations. Going forward, they hope to incorporate more excellent experiments from the research community, and commit to a periodic review of bsuite from a committee of prominent researchers.
Reject. rating score: 1. rating score: 6. rating score: 6. In practice, however, Adam with large epsilon can approximate SGD with momentum. I have some difficulties understanding the contribution of the paper. For example "When tuning all available metaparameters under a realistic protocol at scales common in deep learning,we find that more general update rules never underperform their special cases." In practice you do adjust hyperparameter search spaces to fit your conclusions, e.g., "We found that searching over (epsilon, alpha0/epsilon) was more efficient than searching over (epsilon, alpha)." Second, for any person working in black box optimization it is clear that 16 experiments is next to nothing. Update#2:As I mentioned in my review, Adam with large epsilon is not equivalent to momentum SGD but only approximates the latter. This is because the original Adam has a bias correction term and even if the same *global* learning rate schedule is used both for Adam with large epsilon and momentum SGD, they are not equivalent. In order to obtain the exact equivalence, one would need to either1) drop the bias correction term of Adam and thus modify the algorithm in order to satisfy the claimed equivalenceor 2) set a particular learning rate *for each batch pass* of Adam to simulate the effect of the bias correction term, this leads to a large number of hyperparameters   as many as the number of batch passes, this is intractable (the setup of the authors does not optimize such batch wise hyperparameters, they are defined by a global scheduler as a function of batch/epoch index). My main concern is described in the first Update.<BRK>This paper presents experimental data supporting the claim that the under aggressive hyper parameter tuning different optimizers are essentially ranked by inclusion   if the hyper parameters of method A can simulate any setting of the hyper parameters of method B then under aggressive hyper parameter tuning A will dominate B. One way to achieve this rather trivially is to do a hyper parameter search for B and then set the hyper parameters of A so that A is simulating B. But the point here is that direct and feasible tuning of A with dominate B even in the case where A has more hyper parameters and where hyper parameter optimization of A would seem to be more difficult. An important conclusion is that without loss of generality one can always use Adam even in vision where SGD is currently the dominant optimizer used in practice. First,"Hyper parameter" please not "meta parameter". I don t think algorithm 1, or the definition of a first order optimizer adds anything to the paper.<BRK>The paper provides an empirical comparison of a set of first order optimization methods for deep learning models. Those optimizers include stochastic gradient descent, momentum  method, RMSProp, Adam, Nesterov, and Nadam, which arguably covers all popular variants used in the literature. Although it is not the first empirical study on this topic, its conclusion differs slightly. Cons:  I am not entirely convinced that the inclusion relationship is indeed a major cause or indicator of different optimizers  performance. There is no theoretical justification; Empirically, if one takes two optimizers equally rich and tunes one of them more intensively, one should expect a better performance, too. Suggestions:  I think at least the basic definitions of different optimizers should be given in the main text. For example, the paper starts talking about the taxonomy of the optimizers with their corresponding hyperparameters in Section 3.2 before giving any functional form of the optimizers. I would suggest the authors to follow the convention and use the term "hyperparameter" rather than "metaparameter". The readers of this paper are not primarily Bayesian, there is really no need to divert from the convention. Besides, the term "Bayesian hyperparameter tuning" is widely used even.<BRK>Selecting an optimizer is a central step in the contemporary deep learning pipeline. In this paper they demonstrate the sensitivity of optimizer comparisons to the metaparameter tuning protocol. their findings suggest that the metaparameter search space may be the single most important factor explaining the rankings obtained by recent empirical comparisons in the literature. In fact, they show that these results can be contradicted when metaparameter search spaces are changed. As tuning effort grows without bound, more general update rules should never underperform the ones they can approximate (i.e., Adam should never perform worse than momentum), but the recent attempts to compare optimizers either assume these inclusion relationships are not relevant in practice or restrict the metaparameters they tune to break the inclusions. In their experiments, they find that the inclusion relationships bettheyen optimizers matter in practice and always predict optimizer comparisons. In particular, they find that the popular adative gradient methods never underperform momentum or gradient descent. they also report practical tips around tuning rarely-tuned metaparameters of adaptive gradient methods and raise concerns about fairly benchmarking optimizers for neural network training.
Reject. rating score: 1. rating score: 1. rating score: 1. Unfortunately, I think the paper in its current state does not meet the bar for ICLR   I suggest the authors consult the vast literature in semi supervised and continual learning, and try to place their work in this context, along with external comparisons. It is an interesting idea, and worth exploring. In addition, I think the paper is lacking some grounding and context in terms of what problem is being solved and what previous work exists. Lastly, the experiments show the performance as a function of queue length for different features and for CNNs versus decision trees, but there is no comparison to existing methods and very simple models are used   this means that again, it s difficult to gauge the efficacy of the approach and place this in the context of prior art.<BRK>The paper claims to tackle a semi supervised continual learning problem where the feedback or the labeled data is delayed and is provided based on the model performance. 2  As one of the main motivations for the paper, authors claim humans learn continuously in an unsupervised fashion (paragraph one). The proposed method is an online learning method with delayed feedback which has been extensively studied before. Authors should consider citing the pioneering work in this field such as Weinberger & Ordentlich (2002) [2] or Joulani et al from ICML 2013 [3]. Was this investigated at all? This is vague to me and I think it is not true because there are plenty of supervised continual learning approaches where the labeled data is available when a task is learned (for example [4,5,6])7  The experimental setting is not well designed and does not use a standard continual learning setting and there is no baseline included which are very important reasons for rejecting this paper.<BRK>GeneralThe paper is quite hard to follow. The paper claims that it is about continual learning, but it does not give ANY experimental results on the continual learning benchmarks. The paper says it generates random one hot vector when queue is full, but what does it have to do with delayed feedback? The algorithm requires a completely trained model and a queue that needs to store large amount of data. There is no baseline in the experimental results. This makes the result dubious.<BRK>Most of the artificial neural networks are using the benefit of labeled datasets whereas in human brain, the learning is often unsupervised. The feedback or a label for a given input or a sensory stimuli is not often available instantly. After some time when brain gets the feedback, it updates its knowledge. That's how brain learns. Moreover, there is no training or testing phase. Human learns continually. This work proposes a model-agnostic continual learning framework which can be used with neural networks as theyll as decision trees to incorporate continual learning. Specifically, this work investigates how delayed feedback can be handled. In addition, a way to update the Machine Learning models with unlabeled data is proposed. Promising results are received from the experiments done on neural networks and decision trees. 
Reject. rating score: 3. rating score: 6. rating score: 6. This paper introduces SANE, a new approach for explaining image similarity models by combining a saliency map generator and an attribute predictor. In this way, the method is not only able to highlight what regions contribute the most to the similarity between a query image and a reference image, but also predict an attribute that explains this match. During training, SANE jointly optimizes the attribute prediction of the query image and maximizes the overlap of the saliency map of the image similarity and the attribute activations. I think the paper addresses a very interesting problem that has been commonly overlooked. It is also novel and interesting the addition of an attribute predictor in the system which provides additional information that cannot be captured by the saliency map alone. Aren t there other activations and losses better suited for multi label classification, such as sigmoid + binary cross entropy loss, where there s no need to divide the ground truth labels? In order to match image similarities with attribute descriptions the authors propose matching similarity saliency maps with attribute map activations. A final minor comment: as I mentioned before, the introdution of attributes in the explanation process is a very interesting contribution since they provide the user an explanation that is a step closer to a description in natural language. Although the paper proposes a very interesting approach for explaining image similarity models, I also have some concerns that I think should be addressed before its acceptance.<BRK>I SummaryThis paper proposes a novel method for image similarity models explanation, introducing Salient Attributes for Network Explanation (SANE). The method identifies attributes that contribute positively to the similarity score, thus explaining the important image properties, and pair them with a generated saliency map unveiling the important regions of the image. What is the context in improving image similarity explainability? Moreover, the section on TCAV should be in the related work, whereas how it is used for this specific case would be described in 3.3. In eq 4, â is mentioned but s is used. In 4.2 there is a small user study to verify if the explanations were useful, the study is a nice addition, I really like this kind of results! It would be even more interesting if it compared the results with other baselines. This led to a better comprehension of the challenges.<BRK>Overview/Contribution: The paper proposes an explanation mechanism that pairs the typical saliency map regions together with attributes for similarity matching deep neural networks. Hence, saliency maps are considered as explanation on their own by many. Fig.1 (b) also is a clear example of the kind of explanations generated using a template with the key attribute in question accompanied by the visual saliency map interpretation. The attribute ranking and selection method of informative attributes using combinations weighted TCAV and cosine similarity between the attribute activation map and the generated saliency map is novel. I suggest the authors discuss more of the image similarity based applications and less on the discussion and heavy citation of generalized deep neural networks. The method is described well but why cosine similarity was chosen in terms of its benefits compared to other similarity metrics is not that clear. Evaluation on more datasets such as person/pedestrian attributes datasets would have demonstrated the generalizability of the proposed method across multiple practical domains.<BRK>Explaining a deep learning model can help users understand its behavior and allow researchers to discern its shortcomings. Recent work has primarily focused on explaining models for tasks like image classification or visual question anstheyring.  In this paper, they introduce an explanation approach for image similarity models, where a model's output is a score measuring the similarity of two inputs rather than a classification.  In this task, an explanation depends on both of the input images, so standard methods do not apply. they propose an explanation method that pairs a saliency map identifying important image regions with an attribute that best explains the match.  they find that their explanations provide additional information not typically captured by saliency maps alone, and can also improve performance on the classic task of attribute recognition. their approach's ability to generalize is demonstrated on two datasets from diverse domains, Polyvore Outfits and Animals with Attributes 2.
Accept (Spotlight). rating score: 8. rating score: 6. rating score: 6. The authors improved a lot the manuscript and incorporated reviewers feedback. ###Summary of the paper: The paper provides a way to estimate the natural Wasserstein gradient using Kernel estimators. Authors give variational forms of the Fisher information matrix of an explicit model , using  the variational form of the chi squared or the Fisher Rao divergence. Similarly authors give a variational form of the wasserstein natural gradient . It would be great to baseline this one ? the method comes disappointing since it seems that the preconditioning that the Wasserstein gradient gives is not enough and $r(u) u^{\top}D u$ is need where D is diagonal depends on T. Have you tried with $D Identity$? Overall assessment: That is a good theoretical work with provable guarantees.<BRK>It s important to be able to estimate natural gradient in a practical way, and there have been a few papers looking at this problem but mostly for the case with a Fisher Rao metric. Some theoretical guarantees of the proposed method is established together to some experimental study. I find this work interesting with some important merit, as it tackles an important problem in statistical learning. Having said that, I believe this paper provides an important first (and alternative) step towards an important problem. 2) The sentence on line 8 in Introduction reads ".. This is when the authors are talking about the adaptive learning methods. It is not very clear (to me) how to get this.<BRK>The authors leverage the dual formulation and restrict the feasible space to a RKHS. The flow idea is clear. Overall, I lean to the acceptance side. Below are some of my concerns:1) It seems that the natural gradient under Wasserstein metric is well motivated for models which do not admit a density (to compare with the natural gradient under Fisher information metric). However, it seems that there is no supporting experiments about it yet. and how s about the 2nd term?<BRK>Many machine learning problems can be expressed as the optimization of some cost functional over a parametric family of probability distributions. It is often beneficial to solve such optimization problems using natural gradient methods. These methods are invariant to the parametrization of the family, and thus can yield more effective optimization. Unfortunately, computing the natural gradient is challenging as it requires inverting a high dimensional matrix at each iteration. they propose a general framework to approximate the natural gradient for the Wasserstein metric, by leveraging a dual formulation of the metric restricted to a Reproducing Kernel Hilbert Space. their approach leads to an estimator for gradient direction that can trade-off accuracy and computational cost, with theoretical guarantees. they verify its accuracy on simple examples, and show the advantage of using such an estimator in classification tasks on \texttt{Cifar10} and \texttt{Cifar100} empirically. 
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. The weights of each model can be represented as element wise product of two matrices: shared one and matrix with rank 1 that can be efficiently stored. The idea is quite interesting despite its simplicity. I would like to highlight the lifelong learning as the strongest experimental result achieved by the authors. This method is difficult to generalize for the case of very diverse tasks despite its scalability. The same is true for machine translation section. Whym  heads  are  better  than  one:  Training  a  diverse  ensemble  of  deep  networks.arXiv  preprintarXiv:1511.06314, 2015bOverall, it is an interesting paper, that has several drawbacks.<BRK>The method works by maintaining a shared “slow” weight matrix per layer, along with an ensemble of rank 1 “fast” weight matrices that are combined individually with the slow matrix via a Hadamard product in order to generate the network ensemble. The method is evaluated across a host of experimental settings, including image classification, machine translation, lifelong learning and uncertainty modelling. * Machine translation experiments. * Was a naive ensemble trained on the machine translation tasks for comparison? It would be good to see some discussion of whether and how BatchEnsemble could be combined with other neural network ensemble methods.<BRK>This paper aims to improve the efficiency of ensembles of neural nets in traditional supervised learning and life long learning (learning on a series of tasks). The main idea is to let all the neural nets in an ensemble share the same weights W for each layer, and the weights for each neural net is generated by the Hadamard product of W and a specific rank one matrix of the same size as W that is different across members in the ensemble. In experiments, they evaluate the method with some baselines on life long learning, traditional classification, NMT tasks, and uncertainty modeling. It is easy to understand that the ensemble defined here can improve efficiency and reduce memory cost. How to control efficiency performance trade off in the proposed method?<BRK>
Ensembles, where multiple neural networks are trained individually and their predictions are averaged, have been shown to be widely successful for improving both the accuracy and predictive uncertainty of single neural networks. Hotheyver, an ensemble’s cost for both training and testing increases linearly with the number of networks, which quickly becomes untenable.
In this paper, they propose BatchEnsemble, an ensemble method whose computational and memory costs are significantly lotheyr than typical ensembles. BatchEnsemble achieves this by defining each theyight matrix to be the Hadamard product of a shared theyight among all ensemble members and a rank-one matrix per member. Unlike ensembles, BatchEnsemble is not only parallelizable across devices, where one device trains one member, but also parallelizable within a device, where multiple ensemble members are updated simultaneously for a given mini-batch. Across CIFAR-10, CIFAR-100, WMT14 EN-DE/EN-FR translation, and out-of-distribution tasks, BatchEnsemble yields competitive accuracy and uncertainties as typical ensembles; the speedup at test time is 3X and memory reduction is 3X at an ensemble of size 4. they also apply BatchEnsemble to lifelong learning, where on Split-CIFAR-100, BatchEnsemble yields comparable performance to progressive neural networks while having a much lotheyr computational and memory costs. they further show that BatchEnsemble can easily scale up to lifelong learning on Split-ImageNet which involves 100 sequential learning tasks
Reject. rating score: 3. rating score: 3. rating score: 8. 2) The authors have a done a commendable job of coming up with a meaningful set of experiments by varying base models, self supervised methods, datasets, and few shot learning methods in Section 4.1. While it is a different setting it is inline with one of the main conclusions of the paper. In Figure 4d, it is misleading to show a trendline that includes the same domain as that will always be 0. But this definition of domain distance is more meaningful in the domain adaptation setting (like Amazon Office dataset used for domain adaptation (https://people.eecs.berkeley.edu/~jhoffman/domainadapt/)) as in that case the requirement is to get the embeddings close to each other for the same class but from different domains. This technique probably works possibly due to the ResNet being pre trained with so many labeled classes. I am not sure how well their heuristic will work for an unlabeled pool that the classifier has never seen. In practice, a self supervised learning method would be applied on a large unlabeled pool of images.<BRK>Summary & Pros  This paper proposes a few shot learning method that uses self supervision as an auxiliary label and trains primary and auxiliary labels via multi task learning. Concerns #1: Novelty of the proposed method  This paper uses a multi task learning approach with self supervision. But this approach is already used in various tasks, e.g., domain adaptation, semi supervised learning, training GANs. Thus I think results in the paragraph "Gains are larger for harder tasks" might be predictable. However, I think Figure 4d is not matched to the claim. So I wonder how to draw the lines in Figure 4d.<BRK>The paper also considers the domain mismatch issue where unlabeled images come from a different domain. Applying self supervised learning techniques to the few shot learning regime is a simple idea, and this paper clearly shows that it can be beneficial. However, as the authors note they include additional experimental settings (like the domain selection idea) that are not in Gidaris et al.(2019), so the works are somewhat complementary. The only other comment I have is that the paper is quite large in scope for a conference submission and as a result there are many details and experiments that are left for the appendix. I could also see the domain selection experiments constituting their own submission. Specific comments:  Truly a minor suggestion but I suggest moving Figure 1 to the top of page 2. The paragraph beginning "The focus of most prior work..." in the Related Work section provides a nice framing of your work and so might make more sense in the introduction.<BRK>they present a technique to improve the generalization of deep representations learned on small labeled datasets by introducing self-supervised tasks as auxiliary loss functions. Although recent research has shown benefits of self-supervised learning (SSL) on large unlabeled datasets, its utility on small datasets is unknown. they find that SSL reduces the relative error rate of few-shot meta-learners by 4%-27%, even when the datasets are small and only utilizing images within the datasets. The improvements are greater when the training set is smaller or the task is more challenging. Though the benefits of SSL may increase with larger training sets, they observe that SSL can have a negative impact on performance when there is a domain shift bettheyen distribution of images used for meta-learning and SSL. Based on this analysis they present a technique that automatically select images for SSL from a large, generic pool of unlabeled images for a given dataset using a domain classifier that provides further improvements. they present results using several meta-learners and self-supervised tasks across datasets with varying degrees of domain shifts and label sizes to characterize the effectiveness of SSL for few-shot learning.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. I think the authors should make sure that they are not "overfitting" to the test set with the following set of tests:1. Based on the latest iteration of the paper, I am changing my score to 8]This paper studies the representations learned by large pre trained models trained on language modeling objective (or language modeling like objective, in the case of masked models). [EDIT: Thank you very much for the thoughtful and extensive response.<BRK>The authors demonstrated that the pre trained language models have some properties that are similar to constituency grammar by showing some interesting features of the extracted trees. This study is based on the motivation to unveil the reason why such pre trained language models work and the extent to which pre trained language models capture the syntactic notion of the constituency. Also, this paper is well structured and offers a good literature review.<BRK>I am satisfied with the author s response and I am increasing the score to 6 after the rebuttal. This paper introduces a new and simple method of probing whether syntax information under the form of constituency trees is present in recent pre trained language models (e.g.BERT, RoBERTa, XLNet and GPT2) without any additional task specific training. Results suggest that large, pretrained models capture constituency trees to some extent. Therefore they actually have a bias ? Do you have any hypothesis for why this is happening?<BRK>With the recent success and popularity of pre-trained language models (LMs) in natural language processing, there has been a rise in efforts to understand their inner workings. 
In line with such interest, they propose a novel method that assists us in investigating the extent to which pre-trained LMs capture the syntactic notion of constituency. 
their method provides an effective way of extracting constituency trees from the pre-trained LMs without training. 
In addition, they report intriguing findings in the induced trees, including the fact that pre-trained LMs outperform other approaches in correctly demarcating adverb phrases in sentences.
Accept (Poster). rating score: 8. rating score: 3. rating score: 3. Generalizing attention from 2nd  to 3rd order relations is an important upgrade, and the mathematical context in which this is derived is insightful and may lead to further progress in the development of Transformers capable of constructing still richer structures.<BRK>The paper proposes a transformer block with higher order interactions. More precisely, instead of computing a dot product between a query vector and a key vector, 2 simplicial attention computes scalar triple product. More generally, what kind of tasks can 2 simplicial attention address better? Answering/discussing this question can go a long way in making the paper more valuable. Is it possible to evaluate the improved expressivity of 2 simplicial attention on real world datasets/tasks?<BRK>This paper is an extension of the Transformer Algorithm used to solve sequential problems such as in NLP and games (such as the BoxWorld environment from Zambaldi et al.), stating that the Transformer Algorithm is an inductive bias for learning structural representations.<BRK>they introduce the 2-simplicial Transformer, an extension of the Transformer which includes a form of higher-dimensional attention generalising the dot-product attention, and uses this attention to update entity representations with tensor products of value vectors. they show that this architecture is a useful inductive bias for logical reasoning in the context of deep reinforcement learning.

Accept (Poster). rating score: 6. rating score: 6. rating score: 6. Using a combination of RL based solution as a warm start, this paper shows that by adding more improvement and perturbations, the quality of the solutions for capacitated VRP can be further improved. The numerical experiments show that with these extensions, the proposed mechanism is able to perform better than the SOTA LKH3 OR based method in a shorter amount of time. 3) Maybe in future research, the ensemble idea can be tested for making the solution independent of problem size.<BRK>This paper proposes a framework combining RL and OR for solving the capacitated vehicle routing problem (CVRP). The main idea is to train an RL based controller for choosing the OR operations necessary for improving & reinitializing the solution. The proposed method is shown to empirically outperform the existing OR method LKH3 for solving CVRP. The writing is clear and easy to read. One weakness of the paper is the lack of experiments.<BRK>Cons:  The proposed approach has the benefit of using a handcrafted pool of improvement operators that other learning based approaches don’t have. The paper proposes an algorithm for the Capacitated Vehicle Routing problem that starts with a random solution and then iteratively improves it by using a learned policy to select an improvement operator to apply to the current solution. The problem is posed as a sequential decision problem, and the policy is learned using reinforcement learning.<BRK>This paper is concerned with solving combinatorial optimization problems, in particular, the capacitated vehicle routing problems (CVRP). Classical Operations Research (OR) algorithms such as LKH3 \citep{helsgaun2017extension} are inefficient and difficult to scale to larger-size problems. Machine learning based approaches have recently shown to be promising, partly because of their efficiency (once trained, they can perform solving within minutes or even seconds). Hotheyver, there is still a considerable gap bettheyen the quality of a machine learned solution and what OR methods can offer (e.g., on CVRP-100, the best result of learned solutions is bettheyen 16.10-16.80, significantly worse than LKH3's 15.65). In this paper, they present ``Learn to Improve'' (L2I), the first learning based approach for CVRP that is efficient in solving speed and at the same time outperforms OR methods. Starting with a random initial solution, L2I learns to iteratively refine the solution with an improvement operator, selected by a reinforcement learning based controller. The improvement operator is selected from a pool of potheyrful operators that are customized for routing problems. By combining the strengths of the two worlds, their approach achieves the new state-of-the-art results on CVRP, e.g., an average cost of 15.57 on CVRP-100.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. This paper proposed a method called SVD RND to solve the out of distribution (OOD) detection problem. SVD RND outperforms state of the art methods in several OOD tasks. I believe the proposed method is interesting, and I have not seen a similar approach before. It is a simple method, but it achieves excellent performance in multiple OOD tasks. In summary, I am inclined to accept this paper because it proposes a simple method that gives high performance.<BRK>An example with high error in this task is treated as an out of distribution example. They find that their method of consistently can achieve strong detection rates across multiple target dataset pairs. * The model used for f and g is not mentioned in the text. Please run a model with all the geometric transforms. The original RND paper avoided this problem by also using the network to learn a policy, but this does not exist in this approach.<BRK>####################This paper presents the idea to use blurred images as regularizing examples to improve out of distribution (OOD) detection performance based on Random Network Distillation (RND). The paper proposes to generate sets of such blurred images via Singular Value Decomposition (SVD) on the training images by pruning the lowest K non zero singular values. The main idea of the paper to generate adversarial examples for training via blurring is rather simple and thus the novelty of this work is somewhat minor. 34.Introduce the effective rank at the point where it is used (Section 6.2). (v) What is the idea behind choosing the log effective rank in such an equidistant manner as proposed? A. Alemi.Waic, but why?<BRK> Conventional out-of-distribution (OOD) detection schemes based on variational autoencoder or Random Network Distillation (RND) are known to assign lotheyr uncertainty to the OOD data than the target distribution. In this work, they discover that such conventional novelty detection schemes are also vulnerable to the blurred images. Based on the observation, they construct a novel RND-based OOD detector, SVD-RND, that utilizes blurred images during training. their detector is simple, efficient in test time, and outperforms baseline OOD detectors in various domains. Further results show that SVD-RND learns a better target distribution representation than the baselines. Finally, SVD-RND combined with geometric transform achieves near-perfect detection accuracy in CelebA domain.
Reject. rating score: 1. rating score: 3. rating score: 3. Comments: pixel real estate and resolution makes it unclear which dataset is bigger, though the authors attempt to discuss this. The paper might be a stronger fit for a targeted workshop on its topic. Some of the tasks the authors attempt might best be attempted with different models.<BRK>Overall, I prefer to reject this paper based on the following reasons:(1) It claimed to address the question “what characteristics should a dataset have to be a good sourcefor remote sensing representation learning”. Later, it used the best in domain representation models to estimate the state of the art baselines on various training size. It is not clear how to select the best in domain representation models.<BRK>​The reviewer understands these could be out of scope of this paper. Again, this is not a significant contribution.<BRK>Given the importance of remote sensing, surprisingly little attention has been paid to it by the representation learning community. To address it and to speed up innovation in this domain, they provide simplified access to 5 diverse remote sensing datasets in a standardized form. they specifically explore in-domain representation learning and address the question of "what characteristics should a dataset have to be a good stheirce for remote sensing representation learning". The established baselines achieve state-of-the-art performance on these datasets. 

Reject. rating score: 3. rating score: 3. rating score: 6. Summary: The paper shows a deep linear network has no spurious local minima as long as it is true for the two layer case for any convex differentiable loss. 3) The paper looks like a technical report and seems not to be ready. The results are quite incremental from the existing ones. The contributions of this work to the deep learning community are still ambiguous.<BRK>The paper shows that for any convex differentiable loss function, a deep linear neural network has no so called spurious local minima, which to be specific, are local minima that are not global minima, as long as it is true for two layer Neural Network. The result also holds for general “multi tower” linear networks. Overall, this paper could be an improvement of existing results. 2)	How does the result help us understand non linear deep neural network, which is commonly use in practice?<BRK>The paper shows an interesting result: deep linear NN has introduced no more spurious local minima than two layer NN and provides an intuitive and short proof for the results, which improve and generalize the previous results under milder assumptions. Also, it would be of great interest to see concrete results on non linear neural networks, since that is exactly what is used in common practice.<BRK>they show that for any convex differentiable loss,  a deep linear network has no spurious local minima as long as it is true for the two layer case.  This reduction greatly simplifies the study on the existence of spurious local minima in deep linear networks. When applied to the quadratic loss, their result immediately implies the potheyrful result by Kawaguchi (2016). Further, with the recent work by Zhou& Liang (2018), they can remove all the assumptions in (Kawaguchi, 2016).  This property  holds  for  more  general  “multi-totheyr”  linear  networks  too.  their  proof builds on the work in (Laurent & von Brecht, 2018) and develops a new perturbation argument to show that any spurious local minimum must have full rank, a structural property which can be useful more generally
Reject. rating score: 3. rating score: 6. rating score: 6. While well motivated in terms of the background and methodology (indeed, this is a simple way to prevent interference in fast weights), and nicely explored experimentally with lots of examinations into the workings of the method, the weak results on the simpler continual learning settings lead me to consider this a weak reject. The authors show that fast weights can be applied in the continual learning setting, but alone they do not perform that well on the more challenging datasets, with mixed results on how much better they are as compared to a naive fine tuning baseline, and they definitely lag behind synaptic consolidation methods.<BRK>The authors introduce DIFFERENTIABLE HEBBIAN CONSOLIDATION,a new framework for continual learning that can be implemented in the usual differentiable programming setups. 1)Authors comment on two paradigms to hebbian based continual learning: the task specific and theirs, based on CLS. First, the paper is well written, and the method is well motivated in terms of cognitive principles with a nice commentary about underlying neurobiological substrates. Importantly, it is differentially programing friendly.<BRK>This paper tackles the problems of continual learning and catastrophic forgetting in neural networks. In terms of motivation, can you explain why this Hebbian strategy is applied only to the final softmax? The paper addresses an important topic. A figure might be helpful.<BRK>Continual learning is the problem of sequentially learning new tasks or knowledge while protecting previously acquired knowledge. Hotheyver, catastrophic forgetting poses a grand challenge for neural networks performing such learning process. Thus, neural networks that are deployed in the real world often struggle in scenarios where the data distribution is non-stationary (concept drift), imbalanced, or not always fully available, i.e., rare edge cases. they propose a Differentiable Hebbian Consolidation model which is composed of a Differentiable Hebbian Plasticity (DHP) Softmax layer that adds a rapid learning plastic component (compressed episodic memory) to the fixed (slow changing) parameters of the softmax output layer; enabling learned representations to be retained for a longer timescale. they demonstrate the flexibility of their method by integrating theyll-known task-specific synaptic consolidation methods to penalize changes in the slow theyights that are important for each target task. they evaluate their approach on the Permuted MNIST, Split MNIST and Vision Datasets Mixture benchmarks, and introduce an imbalanced variant of Permuted MNIST --- a dataset that combines the challenges of class imbalance and concept drift. their proposed model requires no additional hyperparameters and outperforms comparable baselines by reducing forgetting.
Reject. rating score: 1. rating score: 3. rating score: 3. This paper want to show that minimizing cross entropy loss will simultaneously minimize Hinge loss with different margins, cross entropy loss with different temperatures and a newly introduced Gcdf loss with different standard deviations. The main contribution is a new gcdf loss based on Gaussian perturbed parameters. Also, in the last sentence in Sec 3.1, the meaning of x is normalized is ambiguous.<BRK>The results in the paper are not described in enoughdetail to be reproduced. I am convinced that Figures 6 and 12 of this paper showthat optimization of the softmax with a larger step sizeimplicitly optimizes a loss function that rewards robustnessto a greater extent than when a small step size is used. I find this interesting. In Figures 2 and 3 I don t see that they have adequately controlledfor the effect of the learning rate on how fast the explicitlyminimized loss is reduced.<BRK>The results reported in the paper are interesting. However, currently I am not convinced that the contributions are sufficient for publication at ICLR as the scope of the performed analysis is limited. These experiments would help to better understand the observed phenomenon and analyze the effect of different settings. The authors consider alternative loss functions for deep networks: (1) the temperature scaled cross entropy loss with different values of the temperature; (2) the hinge loss with different values of the margin parameter; (3) the Gcdf loss with different values of the variance parameter.<BRK>Understanding the implicit bias of optimization algorithms is important in order to improve generalization of neural networks. One approach to try to exploit such understanding would be to then make the bias explicit in the loss function.  Conversely, an interesting approach to gain more insights into the implicit bias could be to study how different loss functions  are being implicitly minimized when training the network. In this work, they concentrate their study on the inductive bias occurring when minimizing the cross-entropy loss with different batch sizes and learning rates.  they investigate how three loss functions are being implicitly minimized during training. These three loss functions are the Hinge loss with different margins, the cross-entropy loss with different temperatures and a newly introduced Gcdf loss with different standard deviations. This  Gcdf loss establishes a connection bettheyen a sharpness measure for the 0−1 loss and margin based loss functions. they find that a common behavior is emerging for all the loss functions considered.
Reject. rating score: 1. rating score: 1. rating score: 1. rating score: 1. 2.Assessment: The problem studied in this work is very interesting and important. The high level idea also looks interesting. However, this paper is clearly not ready to submit as it contains a lot of grammar errors and lacks a lot of important details. What are the details of reference response retrieval? To many such important details are missing.<BRK>The basic idea of integrating templates for dialog generation is interesting. Worst of all, the experimental results are missing in table 2, the only placeholder for results in the paper.<BRK>This paper introduces a based model for the problem of task oriented dialog. The paper studies an interesting task however it is not ready forpublication:  Empirical results of all baselines are missing (Table 2)  Other results are mentioned but are absent from the manuscript  The related work does not provide details as to how the proposed model to prior work  The figures are difficult to understand (although they do help make the model clearer and so I suggest polishing them for the next version of this work)  The manuscript should be carefully copy edited.<BRK>This paper describes a method to incorporate candidate templates to aid in response generation within an end to end dialog system. While the motivation and task setup is interesting, the paper is clearly unfinished.<BRK>End-to-end models have achieved considerable success in task-oriented dialogue area, but suffer from the challenges of (a) poor semantic control, and (b) little interaction with auxiliary information. In this paper, they propose a novel yet simple end-to-end model for response generation via mixed templates, which can address above challenges. 
In their model, they retrieval candidate responses which contain abundant syntactic and sequence information by dialogue semantic information related to dialogue history. Then, they exploit candidate response attention to get templates which should be mentioned in response. their model can integrate multi template information to guide the decoder module how to generate response better. they show that their proposed model learns useful templates information, which improves the performance of "how to say" and "what to say" in response generation. Experiments on the large-scale Multiwoz dataset demonstrate the effectiveness of their proposed model, which attain the state-of-the-art performance.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. In general, the paper is of values to the community. The paper introduces four techniques to improve the deep network model through modifying Batch Normalization (BN). The paper studies each techniques with the support from experiments. The inspirations are from the gaps between train&test and between batches in multi gpu training, comparison to other normalization methods, and weight decay in regularizing convolution weights training.<BRK>The paper performs an empirical study of four batch normalization improvements and proposes a new normalization technique for small batch sizes, based on group and batch normalizations. Concerns:(1) The comparison with baselines in section 4.2 seems to be unclear. Additional elaboration on what does this means is required. However, the thorough empirical study of existing improvement techniques would be a good addition to the conference. I would also recommend authors to include the following papers to the related work section:1.<BRK>The authors discuss four techniques to improve Batch Normalization, including inference example weighing, medium batch size, weight decay, the combination of batch and group normalization. The proposed inference example weighing method yields promising results and does not require any re training. What is the essential difference between these methods? However, this paper shows that training ResNet 50 with weight decay improves the performance. What would happen if the authors apply the proposed techniques to the non i.i.d.case?7.Some closely related work should be discussed in the paper, such as[1] "Decorrelated Batch Normalization."<BRK>A key component of most neural network architectures is the use of normalization layers, such as Batch Normalization. Despite its common use and large utility in optimizing deep architectures, it has been challenging both to generically improve upon Batch Normalization and to understand the circumstances that lend themselves to other enhancements. In this paper, they identify ftheir improvements to the generic form of Batch Normalization and the circumstances under which they work, yielding performance gains across all batch sizes while requiring no additional computation during training. These contributions include proposing a method for reasoning about the current example in inference normalization statistics, fixing a training vs. inference discrepancy; recognizing and validating the potheyrful regularization effect of Ghost Batch Normalization for small and medium batch sizes; examining the effect of theyight decay regularization on the scaling and shifting parameters γ and β; and identifying a new normalization algorithm for very small batch sizes by combining the strengths of Batch and Group Normalization. they validate their results empirically on six datasets: CIFAR-100, SVHN, Caltech-256, Oxford Flotheyrs-102, CUB-2011, and ImageNet.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. Summary:This paper studies the certifiable bounds for adversarial perturbations in \ell_2 radius for top k predictions instead of top 1 predictions. The experimental results (Figure 1) support the claim that there is a non trivial difference between the certified radii of top 1 and top k predictions.<BRK>Summary.The paper proposes an extension to the work of Cohen et al.where a certified radius is deduced using a randomized smoothing approach. In particular, the authors show the radius at which a smoothed classifier g at under Gaussian perturbations is certified for the top k predictions. That is to say that the prediction will remain within the top k predictions of g. Setting k 1, one recovers Cohen et al.results.The authors show that the derived radius is tight. The paper is also easy to read. May the authors comment on the following. At least when k 1, increasing sigma increases the certified radius in which I expect to see that most of the samples to be actually within the radius and it should perform much better than lower sigma {0.25,0.5}.<BRK>This paper builds upon the random smoothing technique for top 1 prediction proposed by Cohen et al.for certifying top k predictions with probabilistic guarantees, which enjoys good scalability to large neural networks and in principle can be applied to any classifier. The authors aim to provide (probabilistic) certification on top k predictions, which to my knowledge is the first work to consider this setup. I hope the authors can address my concerns in the Questions below. 3.Experimental results on Cifar 10 and ImageNet showed improved lower bound on certified L2 norm radius when increasing k. The authors also performed an ablation study of different parameters in the proposed algorithm. 2.The discussion on Fig.3 says "We observe that  \sigma controls a trade off between normal accuracy under no attacks and robustness.<BRK>It is theyll-known that  classifiers are vulnerable to adversarial perturbations. To defend against adversarial perturbations, various certified robustness results have been derived. Hotheyver, existing certified robustnesses are limited to top-1 predictions. In many real-world applications, top-$k$ predictions are more relevant. In this work, they aim to derive certified robustness for top-$k$ predictions. In particular, their certified robustness is based on randomized smoothing, which turns any classifier to a new classifier via adding noise to an input example. they adopt randomized smoothing because it is scalable to large-scale neural networks and applicable to any classifier. they derive a tight robustness in $\ell_2$ norm for top-$k$ predictions  when using randomized smoothing with Gaussian noise. they find that generalizing the certified robustness  from top-1 to top-$k$ predictions faces significant technical challenges. they also empirically evaluate their method on CIFAR10 and ImageNet. For example, their method can obtain an ImageNet classifier with a certified top-5 accuracy of 62.8\% when the $\ell_2$-norms of the adversarial perturbations are less than 0.5 (=127/255). their code is publicly available at: \url{https://github.com/jjy1994/Certify_Topk}. 
Reject. rating score: 1. rating score: 1. rating score: 1. Discriminator actor critic: Addressing sample inefficiency and reward bias in adversarial imitation learning. Specifically, the paper extends f VIM (Ke et al., 2019), which uses the f divergence for IL, by using a sigmoid function for discriminator output’s activation function. Figures are too small and difficult to see, especially the legends. This proposed method is named f VIM sigmoid. The title of the Algorithm 1 should be f VIMO sigmoid instead of f VIMO. The paper extends f VIM sigmoid to the setting of IL with observation and proposes f VIMO sigmoid. In order to make the comparison fairer, I suggest the authors to evaluate TV VIM with sigmoid reward output, or include environments that do not have survival bonuses.<BRK>Summary: The submission performs empirical analysis on f VIM (Ke, 2019), a method for imitation learning by f divergence minimization. The paper especially focues on a state only formulation akin to GAILfO (Torabi et al., 2018b). The main contributions are:1) The paper identifies numerical proplems with the output activations of f VIM and suggest a scheme to choose them such that the resulting rewards are bounded. 2) A regularizer that was proposed by Mescheder et al.(2018) for GANs is tested in the adversarial imitation learning setting. 3) In order to handle state only demonstrations, the technique of GAILfO is applied to f VIM (then denoted f VIMO) which inputs state nextStates instead of state actions to the discriminator.<BRK>This paper proposes the application of the f VIM framework (Ke et.al., 2019) to the problem of imitation learning from observations (no expert actions). The authors first identify a potential source of numerical instability in the application of f VIM to imitation learning – the rewards for the policy gradient RL are given by a combination of a convex conjugate and an activation function. Lack of novelty – Although I appreciate the reparameterization applied to f VIM to make it potentially more stable for imitation learning in large state  and action spaces, I don’t think that by itself meets the bar for ICLR. Algorithm 1 is basically the GAILFO algorithm (Torabi et al.2018) written in the f Vim framework, with the proposed reparameterization. What about the JS divergence (GAIL)?<BRK>State-of-the-art results in imitation learning are currently held by adversarial methods that iteratively estimate the divergence bettheyen student and expert policies and then minimize this divergence to bring the imitation policy closer to expert behavior. Analogous techniques for imitation learning from observations alone (without expert action labels), hotheyver, have not enjoyed the same ubiquitous successes. 
Recent work in adversarial methods for generative models has shown that the measure used to judge the discrepancy bettheyen real and synthetic samples is an algorithmic design choice, and that different choices can result in significant differences in model performance. Choices including Wasserstein distance and various $f$-divergences have already been explored in the adversarial networks literature, while more recently the latter class has been investigated for imitation learning. Unfortunately, they find that in practice this existing imitation-learning framework for using $f$-divergences suffers from numerical instabilities stemming from the combination of function approximation and policy-gradient reinforcement learning. In this work, they alleviate these challenges and offer a reparameterization of adversarial imitation learning as $f$-divergence minimization before further extending the framework to handle the problem of imitation from observations only. Empirically, they demonstrate that their design choices for coupling imitation learning and $f$-divergences are critical to recovering successful imitation policies. Moreover, they find that with the appropriate choice of $f$-divergence, they can obtain imitation-from-observation algorithms that outperform baseline approaches and more closely match expert performance in continous-control tasks with low-dimensional observation spaces. With high-dimensional observations, they still observe a significant gap with and without action labels, offering an interesting avenue for future work.
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. To this end, the authors propose a variant of Adam   called SAdam   which indeed satisfies such a desired bound. The regret analysis of SAdam is conceptually simple and elegant. The experimental protocol is well detailed, and the results look promising.<BRK>The idea seems interesting, the writing is well written, and the analysis seems correct (I did not fully check all steps, but the key steps seems ok to me). Probs:1.The proposed SAdam is an effective variant of Adam designed for strongly convex functions. The authors also fix a small bug in the analysis of AMSgrad.<BRK>However, I have some concerns on the possible impacts of the results especially in the context of ICLR:  First of all, the assumption to show improved regret is strong convexity of all functions $f_t$. Overall, I think that it is interesting to see that a variant of Adam can be shown to obtain improved regret under strong convexity, I find the assumptions strong and the impact for neural network training, therefore for ICLR, quite questionable.<BRK>The Adam algorithm has become extremely popular for large-scale machine learning. Under convexity condition, it has been proved to enjoy a data-dependent $O(\sqrt{T})$ regret bound where $T$ is the time horizon. Hotheyver, whether strong convexity can be utilized to further improve the performance remains an open problem. In this paper, they give an affirmative anstheyr by developing a variant of Adam (referred to as SAdam) which achieves a data-dependent $O(\log T)$ regret bound for strongly convex functions. The essential idea is to maintain a faster decaying yet under controlled step size for exploiting strong convexity. In addition, under a special configuration of hyperparameters, their SAdam reduces to SC-RMSprop, a recently proposed variant of RMSprop for strongly convex functions, for which they provide the first data-dependent logarithmic regret bound. Empirical results on optimizing strongly convex functions and training deep networks demonstrate the effectiveness of their method.
Reject. rating score: 3. rating score: 6. rating score: 8. This work tries to build a sub pile of the test data to save the testing time with minimum effect on the test adequacy and the output distribution. In this paper, the work is done by adding a test sample search algorithm on top of the HGS algorithm to balance the output distribution. However, the novelty of the proposed work is limited, and there is no evidence to show the proposed algorithms can be applied to other related works. In Definition 1 the authors declare that the goal is to satisfy f(T,M) f(T’M) and g(T,M) g(T’M), and then in the following paragraphs they change it to f(T,M)≈f(T’M) and g(T,M)≈g(T’M) with no justification. 2.In Table 2, the authors try to compare the output distribution. To better demonstrate the change between raw testing set and proposed subset, I think that it could be better to present the metrics of distribution or the accuracy of each class instead.<BRK>The paper develops methods to reduce the test data size while maintaining the coverage and the effectiveness on large test data. The paper proposes  a two phase reduction approach to select representative samples based on heuristics. The paper targets a very important problem in practice. Effectively selecting small, representative test sets can save many computational resources and greatly accelerate the research and development. Overall, the work can be much improved if a theoretical framework is proposed.<BRK>The paper presents a new approach to create subsets of the testing examples that are representative of the entire test set so that the model can be tested quickly during the training and leaving the check on the full test set only at the end to validate its validity. The key idea is to create the smaller possible subset with the same or similar coverage (in the paper the neurons coverage is considered) and output distribution, maintaining the difference below a (small) threshold. In this way, an estimate of the output distribution is considered during the extraction of the representative subset of the testing data. The whole process is divided into two phases, the first is to create a first subset using HGS, the second refines this subset in order to achieve the desired precision in the output distribution. Moreover, a similar problem is present for TI, which is introduced in Algorithm 1 and described later. I would expect monotonicity here. Do the authors have any idea of the reasons for this? Finally, it would be interesting to know the runtime to obtain the subsets of the test data of Table 2 required by the considered systems.<BRK>With the increasing adoption of Deep Learning (DL) models in various applications, testing DL models is vitally important. Hotheyver, testing DL models is costly and expensive, especially when developers explore alternative designs of DL models and tune the hyperparameters. To reduce testing cost, they propose to use only a selected subset of testing data, which is small but representative enough for quick estimation of the performance of DL models. their approach, called DeepReduce, adopts a two-phase strategy. At first, their approach selects testing data for the purpose of satisfying testing adequacy. Then, it selects more testing data in order to approximate the distribution bettheyen the whole testing data and the selected data leveraging relative entropy minimization.
Experiments with various DL models and datasets show that their approach can reduce the whole testing data to 4.6\% on average, and can reliably estimate the performance of DL models. their approach significantly outperforms the random approach, and is more stable and reliable than the state-of-the-art approach.
Reject. rating score: 3. rating score: 6. rating score: 6. This paper tries to study the importance of different components of GNNs. 1.There is no theoretical analysis in the paper. To study this problem, this paper proposes two models, Graph Feature Network (GFN) and Graph Linear Network (GLN). For example, on some datasets, GFN, GLN, and GNN s performances are close while on other datasets, there are gaps.<BRK>If the suggestions below can be addressed in author response, I would be willing to increase the score. Although this decomposition may not be unique in general, as pointed out in the paper, these two parts can help analyze the impact of each part in the GNN model. Overall, this paper is well written and the contribution is clear.<BRK>The paper dissects the importance of two parts in GCN: 1) nonlinear neighborhood aggregation; 2) nonlinear set function by linearizing the two parts and resulting in Graph Feature Network (GFN) and Graph Linear Network (GLN). Extensive ablation studies are conducted to single out the effects of various factors. The paper is also clearly written, with clean notations, and well structured sections.<BRK>Graph Neural Nets (GNNs) have received increasing attentions, partially due to their superior performance in many node and graph classification tasks. Hotheyver, there is a lack of understanding on what they are learning and how sophisticated the learned graph functions are.  In this work, they propose a dissection of GNNs on graph classification into two parts: 1) the graph filtering, where graph-based neighbor aggregations are performed, and 2) the set function, where a set of hidden node features are composed for prediction. To study the importance of both parts, they propose to linearize them separately. they first linearize the graph filtering function, resulting Graph Feature Network (GFN), which is a simple lighttheyight neural net defined on a \textit{set} of graph augmented features. Further linearization of GFN's set function results in Graph Linear Network (GLN), which is a linear function. Empirically they perform evaluations on common graph classification benchmarks. To their surprise, they find that, despite the simplification, GFN could match or exceed the best accuracies produced by recently proposed GNNs (with a fraction of computation cost), while GLN underperforms significantly. their results demonstrate the importance of non-linear set function, and suggest that linear graph filtering with non-linear set function is an efficient and potheyrful scheme for modeling existing graph classification benchmarks.
Reject. rating score: 3. rating score: 3. rating score: 3. Discreteness in activations is one form of regularization to reduce |tilde(x)|. I liked the motivation and presentation of the paper but see some critical shortcomings:  In Theorem 1, shouldn t there be a term that accounts for the complexity of the hypothesis class (VC dim, Rademacher complexity etc.). The experiments do not compare with any mutual information related baselines.<BRK>The proposed method is also limited in its scope to classification tasks only and the authors make no attempt to address regression or reinforcement learning problem, which limits is widespread applicability. Most notable among them is the work of Kim et al."Bayesian Model Agnostic Meta Learning" (https://arxiv.org/pdf/1806.03836.pdf), which is not referenced or compared against in this paper at all.<BRK>Finally, that both experiments are image based raises questions as to the generality of the method. Thus, I do not believe this work is ready for publication in its current form.<BRK>This paper analyzes how generalization works in meta-learning. their core contribution is an information-theoretic generalization bound for meta-learning, which identifies the expressivity of the task-specific learner as the key factor that makes generalization to new datasets difficult. Taking inspiration from their bound, they present Discrete InfoMax Codes (DIMCO), a novel meta-learning model that trains a stochastic encoder to output discrete codes. Experiments show that DIMCO requires less memory and less time for similar performance to previous metric learning methods and that their method generalizes particularly theyll in a challenging small-data setting.
Reject. rating score: 1. rating score: 3. rating score: 3. Summary: The paper proposes behavioral repertoire imitation learning (BRIL) which aims to learn a collection of policy from diverse demonstrations. BRIL learns such a collection by learning a context dependent policy, where the context variable represents behavior of each demonstration. This feature space is then reduced by using a dimensionality reduction method such as t SNE. Score: The weaknesses of the paper are novelty, clarity, and evaluation. I vote for rejection. The major issue of the paper is the lack of novelty. The main difference is that, BRIL relies on a manually defined context variable (behavioral feature space). The paper lacks important baseline methods in the experiment. Moreover, BRIL is evaluated only on the StarCraft environment with only one kind of manually specified feature.<BRK>BRIL then preforms behavioral cloning on these demonstrations, with the reduced representation of the current strategy as an additional input to the policy model. Empirical evaluation of BRIL is conducted in StarCraft II, where the agent is tasked with scheduling the construction of different units (other aspects of play are controlled by built in AI). The best way to view this work is as a method for learning goal conditioned polices from demonstrations. Neither the theoretical discussion nor the empirical results compare BRIL against existing work on learning goal conditioned policies. This idea is not explored in any great detail however, and no comparisons with existing methods combining IL with RL are conducted, so the value of BRIL in that context is unclear. I would, however, recommend this as direction for future work with BRIL.<BRK>Demonstrations used in training are labeled with differences in behavior across dimensions (which are then reduced to two dimensions using t SNE), and then these behavior labels are provided as additional input when training a NN from demonstrations using behavior cloning. in order to have a better evaluation of the BRIL approach. The biggest weakness in this paper   and barrier to acceptance   is in the really small sample size of the results where only 4 cluster behaviors out of 62 are evaluated.<BRK>Imitation Learning (IL) is a machine learning approach to learn a policy from a set of demonstrations. IL can be useful to kick-start learning before applying reinforcement learning (RL) but it can also be useful on its own, e.g. to learn to imitate human players in video games. Hotheyver, a major limitation of current IL approaches is that they learn only a single ``"average" policy based on a dataset that possibly contains demonstrations of numerous different types of behaviors. In this paper, they present a new approach called Behavioral Repertoire Imitation Learning (BRIL) that instead learns a repertoire of behaviors from a set of demonstrations by augmenting the state-action pairs with behavioral descriptions. The outcome of this approach is a single neural network policy conditioned on a behavior description that can be precisely modulated. they apply this approach to train a policy on 7,777 human demonstrations for the build-order planning task in StarCraft II. Dimensionality reduction techniques are applied to construct a low-dimensional behavioral space from the high-dimensional army unit composition of each demonstration. The results demonstrate that the learned policy can be effectively manipulated to express distinct behaviors. Additionally, by applying the UCB1 algorithm, the policy can adapt its behavior -in-bettheyen games- to reach a performance beyond that of the traditional IL baseline approach.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. rating score: 3. This paper describes a neural architecture search method for computation resources allocation across feature resolutions in object detection. The experiment results have quite nice improvements on several standard data sets. This is a great, well written paper overall. I only have very small questions and suggestions to this paper. I read the Figure 4, Table 5 and Table 6, but I really cannot understand why those networks are that  good .<BRK>The paper attempts to apply neural architecture search (NAS) to re arrange, or re allocate the network backbone blocks and the convolution filters for object detection. After search, the model is shown to have 1) better AP results; and 2) more balanced effective receptive field (ERF). + I am not aware of any work that performs search on backbone architectures for object detection yet.<BRK>This paper works on neural architecture search for object detection. The results show healthy improvements among different network architectures. Overall it is a valid paper with reasonable ideas and decent results. However, the results are not exciting enough. I will be happy to alter my rating if the authors show more exciting observations (not limited to the above direction).<BRK><Post rebuttal comments>Authors  responses partly resolved my concerns on the experiments. I have no object to accept this paper. 2.Experimental results are rather weak.<BRK>The allocation of computation restheirces in the backbone is a crucial issue in object detection. Hotheyver, classification allocation pattern is usually adopted directly to object detector, which is proved to be sub-optimal. In order to reallocate the engaged computation restheirces in a more efficient way, they present CR-NAS (Computation Reallocation Neural Architecture Search) that can learn computation reallocation strategies across different feature resolution and spatial position diectly on the target detection dataset. A two-level reallocation space is proposed for both stage and spatial reallocation. A novel hierarchical search procedure is adopted to cope with the complex search space. they apply CR-NAS to multiple backbones and achieve consistent improvements. their CR-ResNet50 and CR-MobileNetV2 outperforms the baseline by 1.9% and 1.7% COCO AP respectively without any additional computation budget. The models discovered by CR-NAS can be equiped to other potheyrful detection neck/head and be easily transferred to other dataset, e.g. PASCAL VOC, and other vision tasks, e.g. instance segmentation. their CR-NAS can be used as a plugin to improve the performance of various networks, which is demanding.
Reject. rating score: 3. rating score: 3. rating score: 3. The proposed AUL (area under the loss curve) makes intuitive sense, and exploiting the fact that noisy examples are hard to learn at the early stage of training, leading to higher AULs than clean examples. 3.Experimental results suggest advantage of the proposal. This has been the concern not only for this work, but also for this general research area. For instance, on Tiny ImageNet dataset (e.g., used in SELFIE: Refurbishing Unclean Samples for Robust Deep Learning (ICML19), which could btw be used in the benchmark), a clean dataset only gives ~55% test error, compared with about ~10% on CIFAR10 and ~30% on CIFAR100. The robustness of AULs needs to be tested on those harder problems as well. 2.AULs are based on the assumption of a clear separation of two types of examples   clean and noisy. This raises the question of where to place hard clean examples between those two modes. 3.It usually helps to include a real world noisy datasets where its noisy corruption is unknown. One would be curious to know the noisy level estimate. Is it 2x, 3x, considering there are distinct stages of training before and after AUL estimation?<BRK>In this paper, the authors proposed methodologies to identify mislabeled training examples. To mitigate the randomness in the training loss, the authors proposed an AUL (area under the loss) metric, which is less noisy that directly examining the loss. The authors proposed to downweight training examples with high likelihood of being mislabeled. While I agree the method has a good potential to be significant, in its current form, I have several concerns that prevent me from recommending acceptance. 1.How can we be sure that examples with large training loss are mislabeled examples, rather than examples that are inherently difficult to classify? This question becomes more relevant when the model does not have sufficient complexity, or does not have the correct form of complexity. 2.If we cannot distinguish mislabeled examples with examples that are hard to classify, then it seems to me that a simple downweighting based on AUL could potential lead to the opposite outcome. What it we have a different error distribution, e.g., the label has p probability of being correct, and (1 p) probability of being k, where k is a constant?<BRK>This paper proposes a new method of detecting label noise through calculating the Area Under the Loss statistic, which is based on the learning trajectory. I believe this paper is NOT the first one to point out the different training behavior of clean and noisy samples in the label noise problem. While the author mentioned that Shen & Sanghavi used the training losses for selecting data, Shen & Sanghavi also observed the training loss for good and bad samples are different across time. Therefore, it would be good to give correct credit to previous work, and not over emphasizing the contribution. Please proofread the paper, and correct the typos.<BRK>This paper introduces a new method to discover mislabeled training samples and to mitigate their impact on the training process of deep networks. At the heart of their algorithm lies the Area Under the Loss (AUL) statistic, which can be easily computed for each sample in the training set. they show that the AUL can use training dynamics to differentiate bettheyen (clean) samples that benefit from generalization and (mislabeled) samples that need to be “memorized”. they demonstrate that the estimated AUL score conditioned on clean vs. noisy is approximately Gaussian distributed and can be theyll estimated with a simple Gaussian Mixture Model (GMM). The resulting GMM provides us with mixing coefficients that reveal the percentage of mislabeled samples in a data set as theyll as probability estimates that each individual training sample is mislabeled. they show that these probability estimates can be used to down-theyight suspicious training samples and successfully alleviate the damaging impact of label noise. they demonstrate on the CIFAR10/100 datasets that their proposed approach is significantly more accurate and consistent across model architectures than all prior work.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. rating score: 3. This paper proposes MIGE a novel estimator of the mutual information (MI) gradient, based on estimating the score function of an implicit distribution. I strongly encourage the authors to cite [1] and [2] and mention them in the related works. I recommend ACCEPTing this paper. While the proposed technique consists of previously known building blocks (spectral Stein gradient estimator and random projections), it is cleverly applied in a novel context of estimating MI gradients.<BRK>This paper proposes the Mutual Information Gradient Estimator (MIGE) for estimating the gradient of the mutual information (MI) instead of calculating it directly in learning representation. They are using Stein s estimator following by a random projection to build a tractable approximation to the gradient of the MI. The MIGE is evaluated on several of unsupervised and supervised tasks, and shown improvement over prior MI estimation approaches in maximize the MI and learning features for classification.<BRK>This paper works out estimators for the gradient of Mutual Information (MI). The focus is on its recent popular use for representation learning. Minor comments:   *’In practice, we do not care about MI estimation’. The information ‘between z and z’ is probably a typo? Is it the improved representations (as obtained by using MIGE) or is it the change classifier? 3)One contribution of the paper is to make the gradient estimator work in high dimensions. To this end, the authors propose Random Projections.<BRK>The paper argues that directly estimating the intractable mutual information (MI) for representation learning is challenging in high dimensions. Using some identities of MI the authors arrive at an expression for the gradient of the mutual information between input and latent representation (eq 10) and proposes to use a generalization of the reparameterization trick and spectral stein gradient descent to approximate this gradient. Pros:1) I find the approach taken by the authors interesting and different from current MI estimation approaches. Further i would like some discussion on the quality of this approximation?<BRK>Mutual Information (MI) plays an important role in representation learning. Hotheyver, MI is unfortunately intractable in continuous and high-dimensional settings. Recent advances establish tractable and scalable MI estimators to discover useful representation. Hotheyver, most of the existing methods are not capable of providing an accurate estimation of MI with low-variance when the MI is large. they argue that directly estimating the gradients of MI is more appealing for representation learning than estimating MI in itself. To this end, they propose the Mutual Information Gradient Estimator (MIGE) for representation learning based on the score estimation of implicit distributions. MIGE exhibits a tight and smooth gradient estimation of MI in the high-dimensional and large-MI settings. they expand the applications of MIGE in both unsupervised learning of deep representations based on InfoMax and the Information Bottleneck method. Experimental results have indicated significant performance improvement in learning useful representation.
Reject. rating score: 1. rating score: 3. rating score: 3. The authors observed that the wider deep neural networks can learn much rich representative features than shallower deep neural networks while both networks show similar level of the test performance. I wonder that the results on figure 4 are caused from this problem. Comparison with just two different width is not enough to analyze the situation.<BRK>This paper considers the effect of network width of the neural network and its ability to capture various intricate features of the data. I find the results promising but the paper is not yet ready.<BRK>This paper investigates wider networks using a recent feature visualization technique named activation atlases. However, I tend to reject this paper since it doesn’t show very compelling evidence through experiments. But all the datasets and architectures used in this paper are quite simple from the view of deep learning.<BRK>Transferability of learned features bettheyen tasks can massively reduce the cost of training a neural network on a novel task. they investigate the effect of network width on learned features using activation atlases --- a visualization technique that captures features the entire hidden state responds to, as opposed to individual neurons alone. they find that, while individual neurons do not learn interpretable features in wide networks, groups of neurons do. In addition, the hidden state of a wide network contains more information about the inputs than that of a narrow network trained to the same test accuracy. Inspired by this observation, they show that when fine-tuning the last layer of a network on a new task, performance improves significantly as the width of the network is increased, even though test accuracy on the original task is independent of width. 
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. This paper studies activation quantization in deep networks. The authors first compare the coordinate discrete gradient and those obtained by various kinds of straight through estimators, and found 1 bit activation networks have much poorer gradient estimation than 2 bit ones.<BRK>Do you mean 1 bit activation has larger gradient mismatch than other bits, at least in the defined cosine similarity by this paper?<BRK>"Please note that BN layers followed by binary activation layer can be merged to the threshold of the binary activation layer, incurring no overhead at inference stage." This could also be due to poor initialization in the binary case.<BRK>Binary Neural Networks (BNNs) have been garnering interest thanks to their compute cost reduction and memory savings. Hotheyver, BNNs suffer from performance degradation mainly due to the gradient mismatch caused by binarizing activations. Previous works tried to address the gradient mismatch problem by reducing the discrepancy bettheyen activation functions used at forward pass and its differentiable approximation used at backward pass, which is an indirect measure. In this work, they use the gradient of smoothed loss function to better estimate the gradient mismatch in quantized neural network. Analysis using the gradient mismatch estimator indicates that using higher precision for activation is more effective than modifying the differentiable approximation of activation function. Based on the observation, they propose a new training scheme for binary activation networks called BinaryDuo in which two binary activations are coupled into a ternary activation during training. Experimental results show that BinaryDuo outperforms state-of-the-art BNNs on various benchmarks with the same amount of parameters and computing cost.
Reject. rating score: 3. rating score: 3. rating score: 8. The work and its results heavily peg on the DeepBugs and increases the precision of its first step by a significant margin, but does not show getting any more useful results. Technically, the paper improve this first step of DeepBugs by using a standard variant of ELMO. The evaluation is detailed, but the results are unsurprising. The paper simply tech transfers the idea from NLP to Code.<BRK>This paper leverage recent advances of ELMo in context embedding and apply it in the source code embedding. To evaluate the effectiveness of the proposed approach, authors conduct experiments on the downstream task of the bug detection. 2.The application and combination of different techniques in this paper are smart. Cons:1.It is a good application of known techniques, but the novelty is limited.<BRK>The paper proposes an embedding method for source code tokens, which is based on contextual word representation, particularly is based on the method of ELMo. The learned representation is evaluated on the task of bug detection, with promising performance. Experiments are useful and reasonable and the experimental results are promising and in the favor of the paper. The paper is well written and clear. The incorrect Binary Operator example in Listing 2 does not seem to be a well justified bug.<BRK>Continuous embeddings of tokens in computer programs have been used to support a variety of software development tools, including readability, code search, and program repair. 
Contextual embeddings are common in natural language processing but have not been previously applied in software engineering.
they introduce a new set of deep contextualized word representations for computer programs based on language models.
they train a set of embeddings using the ELMo (embeddings from language models) framework of Peters et al (2018).
they investigate whether these embeddings are effective when fine-tuned for the downstream task of bug detection.
they show that even a low-dimensional embedding trained on a relatively small corpus of programs can improve a state-of-the-art machine learning system for bug detection.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. The authors address the important issue of exploration in reinforcement learning. In this case, they propose to use reward shaping to encourage joint actions whose outcomes deviate from the sequential counterpart. Although the proposed intrinsic reward is targeted at a particular family of two agent robotic tasks, one can imagine generalizing some of the ideas here to other multi agent learning tasks.<BRK>The paper proposes a novel algorithm for encouraging synergistic behavior in multi agent setups with an intrinsic reward that promotes the agents to work together to achieve states that they cannot achieve individually without cooperation. Some clarification is required here   did the extrinsic reward only baseline use the same skills as the proposed method? Empirical analysis shows that this intrinsic reward promotes synergetic behavior on two agent robotic manipulation tasks and achieves better performance that baselines and ablations.<BRK>The paper focuses on using intrinsic motivation to improve the exploration process of reinforcement learning agents in tasks with sparse reward and that require multi agent to achieve. The authors proposed to encourage the agents toward the actions which changed the world in the ways that "would not be achieved if the agents were acting alone". In this paper, it is presented by two types of intrinsic rewards: compositional prediction error and prediction disparity.<BRK>they study the role of intrinsic motivation as an exploration bias for reinforcement learning in sparse-reward synergistic tasks, which are tasks where multiple agents must work together to achieve a goal they could not individually. their key idea is that a good guiding principle for intrinsic motivation in synergistic tasks is to take actions which affect the world in ways that would not be achieved if the agents theyre acting on their own. Thus, they propose to incentivize agents to take (joint) actions whose effects cannot be predicted via a composition of the predicted effect for each individual agent. they study two instantiations of this idea, one based on the true states encountered, and another based on a dynamics model trained concurrently with the policy. While the former is simpler, the latter has the benefit of being analytically differentiable with respect to the action taken. they validate their approach in robotic bimanual manipulation and multi-agent locomotion tasks with sparse rewards; they find that their approach yields more efficient learning than both 1) training with only the sparse reward and 2) using the typical surprise-based formulation of intrinsic motivation, which does not bias toward synergistic behavior. Videos are available on the project theybpage: https://sites.google.com/view/iclr2020-synergistic.
Reject. rating score: 3. rating score: 6. rating score: 6. This paper proposes a method to summarize a given graph based on the algebraic multigrid and optimal transport, which can be further used for the downstream ML tasks such as graph classification. Although the problem of graph summarization is a relevant task, there are a number of unclear points in this paper listed below:  In Section 3.1, the coarsening method has been proposed, which is said to be achieved by finding S such that A_C   S^T A S.     However, A_C is usually not binary for S \in R^{n x m}, hence how to get the coarse graph G_C from A_C is not clear. In experiments, how is the proposed method used for graph classification? It seems that it is not used in Algorithm 1.<BRK>The paper proposes a differentiable coarsening approach for graph neural network (GNNs). GNNs is indeed an interesting line of research. However, there are some major downsides. Moreover, if you check the paper above, they report much better results for PatchySan on MUTAG, better results on Protein for graph kernels, better results on IMDB B using a hierarchical GNN approach, based on ideas of higher order WL. Nevertheless, indeed, the present paper shows that a differentiable pooling using WL kind of ideas is competitive to existing pooling approach. This is nice, but in the light of the work above, the novelty is unclear.<BRK>This paper proposes an unsupervised hierarchical approach for learning graph representations. The proposed architecture is constructed by unrolling k steps of a parametrized algebraic multigrid approach for minimizing the Wasserstein metric between the graph and its representation. The approach is compared against 6 other state of the art approaches on 5 graph classification tasks, showing significant improvements 4 of them. It looks like the main point is  that this architecture is trying to emulate iterative coarsened residual optimization of the Wasserstein metric between a graph and its representation. The empirical results are quite intriguing.<BRK>Hierarchical abstractions are a methodology for solving large-scale graph problems in various disciplines. Coarsening is one such approach: it generates a pyramid of graphs whereby the one in the next level is a structural summary of the prior one. With a long history in scientific computing, many coarsening strategies theyre developed based on mathematically driven heuristics. Recently, resurgent interests exist in deep learning to design hierarchical methods learnable through differentiable parameterization. These approaches are paired with downstream tasks for supervised learning. In this work, they propose an unsupervised approach, coined \textsc{OTCoarsening}, with the use of optimal transport. Both the coarsening matrix and the transport cost matrix are parameterized, so that an optimal coarsening strategy can be learned and tailored for a given set of graphs. they demonstrate that the proposed approach produces meaningful coarse graphs and yields competitive performance compared with supervised methods for graph classification.
Reject. rating score: 1. rating score: 1. rating score: 3. The authors propose a variant of sequence to sequence models that operates on the fixed segments (waves). The paper contains significant flaws both in its writing and its experimental setup. I believe that this statement is completely irrelevant to the paper.<BRK>In this paper, the authors propose modifications to baseline seq to seq systems for wave to wave translation. To handle possibly long inputs and outputs, as well as significant length differences, they propose to use sliding windows. I would reject this paper because it largely ignores the litterature on speech recognition, text to speech synthesis and speech to speech translation (e.g.Jia et al.Direct speech to speech translation with a sequence to sequence model). The contributions of the paper are unfortunately minimal.<BRK>How many humans were involved in rating and what were they asked to do? Overall, given the quality of writing and lack of methodological novelty this paper is not ready for publication. Strenghts:+ application on activity translation and evaluation by human raters is interesting+ the proposed work is a nice application of an encoder decoder architecture in case of multivariate time seriesWeaknesses:  there is no methodological novelty. The proposed network uses a standard architecture and the authors use straightforward partitioning of the signal in equal sized windows.<BRK>The understanding of sensor data has been greatly improved by advanced deep learning methods with big data. Hotheyver, available sensor data in the real world are still limited, which is called the opportunistic sensor problem.  This paper proposes a new variant of neural machine translation seq2seq to deal with continuous signal waves by introducing the window-based (inverse-) representation to adaptively represent partial shapes of waves and the iterative back-translation model for high-dimensional data.  Experimental results are shown for two real-life data: earthquake and activity translation.  The performance improvements of one-dimensional data was about 46 % in test loss and that of high-dimensional data was about 1625 % in perplexity with regard to the original seq2seq.

Reject. rating score: 1. rating score: 1. rating score: 1. The motivation of this paper is to use the idea of Transformer based NLP models in image data, which is appreciated. However, this seems to be a far unfinished paper. Moreover, the network structure in Figure 2 is not explained. The experimental part is very brief, and unconvincing.<BRK>This paper attempts unsupervised representation learning, via a patch prediction task on ImageNet. I understand the code for CPC++ might not be released yet, but the authors could at least implement their best approximation of it, and also find older works (which CPC compared against in their paper), to fill out the results and make a convincing argument. The paper barely includes any evaluation. Also, the method does not appear to be very novel: I recommend the authors look at and compare against "Unsupervised Visual Representation Learning by Context Prediction" (ICCV 2015), which is conceptually very similar.<BRK>After pre training on unlabeled imageNet datasets the proposed approach is competitive with these algorithms with roughly similar results. The paper could improve on its experimental evaluation bycomparing on multiple datasets, showing error bars when averaging across multiple samplings (eg for getting the 1% label set from the entire imageNet dataset) and also comparing with other approaches even when they dont directly aim to learn representation from unlabeled data (eg Image Transformers by Parmar et al). In addition the description is very high level and does not provide enough details for experimental reproducibility. For example the reviewer had to actually guess at some of specifics of the overall end to end architecture since it was not fully described precisely eg in a diagram.<BRK>Learning rich representations from predictive learning without labels has been a longstanding challenge in the field of machine learning. Generative pre-training has so far not been as successful as contrastive methods in modeling representations of raw images. In this paper, they propose a neural architecture for self-supervised representation learning on raw images called the PatchFormer which learns to model spatial dependencies across patches in a raw image. their method learns to model the conditional probability distribution of missing patches given the context of surrounding patches. they evaluate the utility of the learned representations by fine-tuning the pre-trained model on low data-regime classification tasks. Specifically, they benchmark their model on semi-supervised ImageNet classification which has become a popular benchmark recently for semi-supervised and self-supervised learning methods. their model is able to achieve 30.3% and 65.5% top-1 accuracies when trained only using 1% and 10% of the labels on ImageNet showing the promise for generative pre-training methods.
Reject. rating score: 1. rating score: 1. rating score: 6. This paper proposes to use an additional component to the commonly used encoder decoder approach for summarization, which is referred to as the recoder, which is an RNN syle component that takes the output of the decoder. The intuition offered in the paper is that a good summary should produce itself via the recoder network, and in training it together with the original encoder decoder it should improve its performance as it would be able to capture more than what the word level loss does. I have the following objections to this paper:  I can t see why this extra component should improve the quality of the summary produced. If say our encoder decoder architecture models the training data perfectly, then a recoder that does not do anything would be the right choice. Sure it has flaws, if there is something better why not use it for evaluation? Finally, showing the reference summary creates another bias, since equally good summaries can disagree on what content to include.<BRK>This paper introduces an encoder decoder as a differentiable loss function for sequential autoregressive generation tasks and more specifically for summarization. This is done by adding a recorder network that that takes the decoded sequence from the summarizer as input and is trained to output the reference summary. * But for training (original seq2seq + recorder) authors backpropagate the NLL loss (which is fully differentiable) of the recorder on reference summaries through the softmax probabilities of outputs from the seq2seq model. >> This whole architecture can be seen as a traditional end to end seq2seq model with non linearity and normalization (softmax) in the middle.<BRK>This paper proposed an encoder decoder based summarization network as a loss function within a similar encoder decoder based summarization framework to demonstrate that the proposed model obtains better automatic and human evaluation scores compared to the baseline model of See et al.(2017) with just traditional loss functions. Overall, the paper is well written and the presented results, analyses, and comparisons appear to be reasonable. Few comments:  "A presents either a word ...."  > this sentence is not clear. It would be great if you could provide more details on the selection criteria/qualifications of the mechanical turk workers.<BRK>they present a new approach to defining a sequence loss function to train a summarizer by using a secondary encoder-decoder as a loss function, alleviating a shortcoming of word level training for sequence outputs. The technique is based on the intuition that if a summary is a good one, it should contain the most essential information from the original article, and therefore should itself be a good input sequence, in lieu of the original, from which a summary can be generated. they present experimental results where they apply this additional loss function to a general abstractive summarizer on a news summarization dataset. The result is an improvement in the ROUGE metric and an especially large improvement in human evaluations, suggesting enhanced performance that is competitive with specialized state-of-the-art models.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. This paper studies the theoretical property of neural network s loss surface. The main contribution is to prove that the loss surface of every neural network (with arbitrary depth) with piecewise linear activations has infinite spurious local minima.<BRK>This paper focus on how activation functions’ nonlinearities shape the loss surface of neural networks. Secondly, the authors prove one theorem to show four properties of the loss surfaces of nonlinear neural networks. The authors assert that “the loss surface of *every* neural network has infinite spurious local minima” in the abstract, while in chapter 3 line 2, authors mention, “We find that *almost all* practical neural networks have infinitely many spurious local minima.” Which one is correct? It could be feasible when training a stacked network with particular limitations.<BRK>Summary: This paper studies the landscape of deep neural networks with piecewise linear activation functions. The paper showed that under very mild assumptions, the loss surface admits infinite spurious local minima. If the network is two layer with two piece linear activations, it is proved that within each cell every local minimum is global. (a) The main message of Thm 2 is the partition of the surface into multiple pieces, and each piece has good property. Theorem 2 mainly describes the property of each region separately for 2 layer network, which is weaker than [R1]. (b) Theorem 2 seems easy to prove. The 2nd property “local analogous convexity” was given a 2 page proof in the paper. If not, what is the difficulty?<BRK>Understanding the loss surface of a neural network is fundamentally important to the understanding of deep learning. This paper presents how piecewise linear activation functions substantially shape the loss surfaces of neural networks. they first prove that {\it the loss surfaces of many neural networks have infinite spurious local minima} which are defined as the local minima with higher empirical risks than the global minima. their result demonstrates that the networks with piecewise linear activations possess substantial differences to the theyll-studied linear neural networks. This result holds for any neural network with arbitrary depth and arbitrary piecewise linear activation functions (excluding linear functions) under most loss functions in practice. Essentially, the underlying assumptions are consistent with most practical circumstances where the output layer is narrotheyr than any hidden layer. In addition, the loss surface of a neural network with piecewise linear activations is partitioned into multiple smooth and multilinear cells by nondifferentiable boundaries. The constructed spurious local minima are concentrated in one cell as a valley: they are connected with each other by a continuous path, on which empirical risk is invariant. Further for one-hidden-layer networks, they prove that all local minima in a cell constitute an equivalence class; they are concentrated in a valley; and they are all global minima in the cell.
Reject. rating score: 1. rating score: 1.   This paper introduces a structured consistency loss for semantic segmentation under the semi supervised learning paradigm. The only contribution seems to be the ‘first study that employs semi supervised learning and achieves state of the art performance in semantic segmentation using the Cityscapes’. Which is the difference with French et al.(2019) and Liu et al.(2019)?Both seem to be using CutMix for semantic segmentation.<BRK>Summary:  key problem: semi supervised semantic segmentation;  contributions: 1) combine the CutMix data augmentation of Yun et al 2019 with the standard consistency loss of  and the  structured consistency loss of Liu et al 2019, 2) state of the art results on Cityscapes.<BRK>The consistency loss has played a key role in solving problems in recent studies on semi-supervised learning. Yet extant studies with the consistency loss are limited to its application to classification tasks; extant studies on semi-supervised semantic segmentation rely on pixel-wise classification, which does not reflect the structured nature of characteristics in prediction. they propose a structured consistency loss to address this limitation of extant studies. Structured consistency loss promotes consistency in inter-pixel similarity bettheyen teacher and student networks. Specifically, collaboration with CutMix optimizes the efficient performance of semi-supervised semantic segmentation with structured consistency loss by reducing computational burden dramatically. The superiority of proposed method is verified with the Cityscapes; The Cityscapes benchmark results with validation and with test data are 81.9 mIoU and 83.84 mIoU respectively. This ranks the first place on the pixel-level semantic labeling task of Cityscapes benchmark suite. To the best of their knowledge, they are the first to present the superiority of state-of-the-art semi-supervised learning in semantic segmentation.
Reject. rating score: 3. rating score: 6. rating score: 6. The paper proposes the conditional normalizing flow for structured prediction. The idea is to use conditioning variables as additional inputs to the flow parameter forming networks. The model was demonstrated on image superresolution and vessel segmentation. I find the contribution of this paper minimal. The idea of conditioning has extensively been used during recent years because it is the most natural thing to do (e.g., [1], [2] and numerous other papers). Their s nothing new about the flows used in this paper. The results in table 2 are not convincing; I see no benefit of using the proposed flow model for image super resolution instead of the SOTA super resolution methods. This also applies to other experiments.<BRK>The paper is well written overall and easy to follow. Basically the conditional prior z|x   z f_{\phi}(y,x), where x is the conditioning random variable, and we apply the change of variable formula to get the density of y|x . image.To sample from the models authors propose to use f^{ 1}_{\phi}(z;x). The conditional modules are natural extensions of invertible blocks used in the literature (coupling layers, split priors, conditional coupling, 1x1 conv), where the conditioning is done on some hidden representations of the conditioning variable x (i.e one or multiple layers of NN). Authors propose a dequantization for binary random variables (useful for segmentation applications), where they give an implicit model for the dequantizer (obtain a continuous variable from a discrete binary variable). Minor comments :   Formatting the bibliography is messed up and needs some cleaning , Figure 5 is also making formatting issues of the paper. Figure 1 for sampling it should be f^ 1_{\phi } and not f_{\phi}Review:   Figure 2 is hard to get any idea of the sample quality would be good also to put the low resolution input to the algorithm . can the model be just overfitting? The conditioning for the vessel implementation on x is on two layers , would be great to put all architectures of the models in details , and to show both sampling and training paths   It would be great to add the details of the skip connection used from the network processing x, and how ensure that the flow remains invertible. Overall this is a well written paper and a good addition to normalizing flows methods , some discussion of related works on conditional normalizing flows and more baselines with other competitive methods based on GANs for example would be helpful but not necessary.<BRK>This paper presented the conditional normalizing flows (CNFs) as a new kind of likelihood based learning objective. There are two keys in CNFs. The mapping function is invertible with x as a parameter. The output targe y is obtained with dependency on x and f_{\phi}. This study adopted the flow based model to estimate the conditional flow without using any generative model or adversarial method. 3.This paper proposed an useful solution to train continuous CNFs for binary problems. In particular, the property invertibility should be clarified. 2.Why the issues of mode collapse or training instability in flow are considerable in the experiments?<BRK>Normalizing Flows (NFs) are able to model complicated distributions p(y) with strong inter-dimensional correlations and high multimodality by transforming a simple base density p(z) through an invertible neural network under the change of variables formula. Such behavior is desirable in multivariate structured prediction tasks, where handcrafted per-pixel loss-based methods inadequately capture strong correlations bettheyen output dimensions. they present a study of conditional normalizing flows (CNFs), a class of NFs where the base density to output space mapping is conditioned on an input x, to model conditional densities p(y|x). CNFs are efficient in sampling and inference, they can be trained with a likelihood-based objective, and CNFs, being generative flows, do not suffer from mode collapse or training instabilities. they provide an effective method to train continuous CNFs for binary problems and in particular, they apply these CNFs to super-resolution and vessel segmentation tasks demonstrating competitive performance on standard benchmark datasets in terms of likelihood and conventional metrics.
Reject. rating score: 1. rating score: 3. rating score: 6. The authors study continual, lifelong learning. They suggest a new algorithm, named Adaptive Online Planning (AOP) that combines model based planning with model free learning. The main reason is that the experiments were only performed for 3 different seeds and are therefore not statistically relevant (see Henderson et al."Deep reinforcement learning that matters." 2018.).Besides the issue of significance of the results section, there are other concerns. Is this a reasonable assumption?<BRK>For achieving that, the authors propose an algorithm, Adaptive Online Planning (AOP) combining a model free policy learning method and a model based planner. The last comment is about the 3 environments that are not complex enough. The motivation is interesting to me, but the authors do not provide enough justification.<BRK>The paper presents an adaptive online planning(AOP) strategy in a model free policy setting, a reinforcement learning method aimed to solve catastrophic forgetting problem by combining model based planning and model free policy learning. While the improvement in computation is there, what I find lacking is the experiments demonstrating clear evidence of overcoming catastrophic forgetting problem.<BRK>they study learning control in an online lifelong learning scenario, where mistakes can compound catastrophically into the future and the underlying dynamics of the environment may change. Traditional model-free policy learning methods have achieved successes in difficult tasks due to their broad flexibility, and capably condense broad experiences into compact networks, but struggle in this setting, as they can activate failure modes early in their lifetimes which are difficult to recover from and face performance degradation as dynamics change. On the other hand, model-based planning methods learn and adapt quickly, but require prohibitive levels of computational restheirces. Under constrained computation limits, the agent must allocate its restheirces wisely, which requires the agent to understand both its own performance and the current state of the environment: knowing that its mastery over control in the current dynamics is poor, the agent should dedicate more time to planning. they present a new algorithm, Adaptive Online Planning (AOP), that achieves strong performance in this setting by combining model-based planning with model-free learning. By measuring the performance of the planner and the uncertainty of the model-free components, AOP is able to call upon more extensive planning only when necessary, leading to reduced computation times. they show that AOP gracefully deals with novel situations, adapting behaviors and policies effectively in the face of unpredictable changes in the world -- challenges that a continual learning agent naturally faces over an extended lifetime -- even when traditional reinforcement learning methods fail.
Reject. rating score: 6. rating score: 6. rating score: 6. The technique of replay is well established. You have added a selection criterion for what to replay. You show a modest improvement over a random sample. You choose to view this as proof of the criterion, I on the other hand see it as proof of a very small result. To me it seems more like a negative result. I believe negative results are as important as positives. So I recommend acceptance. You may want to expand the selection criterion you evaluate. You selected the most leverage on forgetting what happens as you select less and less leveraged cases?<BRK>*** SummaryThis paper proposes to formulate the continual learning problem as a meta learning problem where the multi tasking and forgetting can be addressed simutaneously. To assess the forgetting, this paper suggests using the anchor points learned in hindsight. Empirical results show that this formulation performs better than previous methods. The idea of using meta learning to deal with the forgetting issue is interesting. 2.The comparison with the previous shows that the proposed method performances consistently well on multiple tasks. I wonder if the authors have any intuition behind this. Basically, this result is not persuasive for the effectiveness of the proposed anchor point learning method. Also, I’m curious to see what would happen if you have oracle access to future tasks which means we don’t need to learn the anchor points in hindsight. There are several typos.<BRK>This paper proposes an approach for continual learning that improves existing memory / replay  based methods, by learning anchors for each previous task. The paper is well written, it is a novel and well explained idea, and the results are compelling. b) Does it always make sense to choose anchors that maximise forgetting on the current task? It would be interesting to visualize what anchors are found. Are they examples that tend to be visually close to other classes, or are they outliers? d) Given that anchors are already estimated and used to alleviate forgetting, what s the benefit of the additional episodic memory? An ablation is performed with different memory sizes, but what if *only* anchors are used (ie.a size of zero)? 2) On the two step optimisation (Eqn.5):a) It s not clear to me why this constitutes a meta learning process: there is no adaptation at test time, nor does this perform task inference or learn how to learn. In this case, the method just uses a single gradient step to compute the change in predictions at the anchor points. It would be good to clarify whether this is the case, and add further discussion and intuition as needed. The equivalent result for splitCIFAR in the iCARL paper (for 20 tasks x 5 classes) appears to be around 45 50%, so I think it is necessary to compare and evaluate the source of this large improvement in performance.<BRK>In continual learning, the learner faces a stream of data whose distribution changes over time. Modern neural networks are known to suffer under this setting, as they quickly forget previously acquired knowledge. To address such catastrophic forgetting, state-of-the-art continual learning methods implement different types of experience replay, re-learning on past data stored in a small buffer known as episodic memory. In this work, they complement experience replay with a meta-learning technique that they call anchoring: the learner updates its knowledge on the current task, while keeping predictions on some anchor points of past tasks intact. These anchor points are learned using gradient-based optimization as to maximize forgetting of the current task, in hindsight, when the learner is fine-tuned on the episodic memory of past tasks. Experiments on several supervised learning benchmarks for continual learning demonstrate that their approach improves the state of the art in terms of both accuracy and forgetting metrics and for various sizes of episodic memories. 
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. The authors present an algorithm CHOCO SGD to make use of communication compression in a decentralized setting. This is an interesting problem and the results are promising. Firstly they prove the convergence rate of the algorithm on non convex smooth functions, which shows a nearly linear speedup. Also, the paper is mostly nicely written.<BRK>This paper studies non convex decentralized optimization with arbitrary communication compression. It is well motivated and well written. The authors also show CHOCO SGD with momentum is effectiveness in practical. The experimental results on several benchmark datasets validate the algorithm achieves better performance than baselinesBoth of the theoretical and empirical results are convincing.<BRK>This paper studies the convergence of CHOCO SGD for nonconvex objectives and shows its linear speedup while the original paper of CHOCO SGD only provides analysis for convex objectives. The momemtum version of CHOCO SGD is also provided although no theoretical analysis is presented. Extensive empirical results are presented in this paper and the two use cases highlight some potential usage of the algorithm. First, the authors only provide analysis on CHOCO SGD but the comparison with baselines are based on their momemtum versions. Thus, the advantage of vanilla CHOCO SGD over other alternatives is not convincing. However, no evaluation of the consensus is presented and this leads to the following point. In this case, the obtained average loss can be even smaller than the optimal loss.<BRK>Decentralized training of deep learning models is a key element for enabling data privacy and on-device learning over networks, as theyll as for efficient scaling to large compute clusters. As current approaches are limited by network bandwidth, they propose the use of communication compression in the decentralized training context. they show that Choco-SGD achieves linear speedup in the number of workers for arbitrary high compression ratios on general non-convex functions, and non-IID training data.  they demonstrate the practical performance of the algorithm in two key scenarios: the training of deep learning models (i) over decentralized user devices, connected by a peer-to-peer network and (ii) in a datacenter. 
Reject. rating score: 6. rating score: 6. rating score: 8. Summary of the Paper:  The authors claim the that i.i.d hypothesis that is often used in the prior when looking for the equivalence between neural networks and GP is not valid. Detailed comments:Overall I believe that the writing of the paper is very sloppy and difficult to read and follow. The same for non Gaussian variables. Therefore, I do not think that the GP interpretation of the NN is wrong simply for that. This simply shows that the i.i.d.prior may be suboptimal. Summing up, I think that this paper needs more work. The prior is subjective and can be chosen by the user. So if an i.i.d.prior is actually chosen, the corresponding Bayesian  neural network will converge to a GP. However, it can be used to interpret neural networks as GP. There is no problem with that. It is well known that the sum of random variables can also converge to a Gaussian distribution even though they are not independent. The witting of the paper needs to be improved. Z y in Eq.(9) should depend on l.It is not clear what is the distribution of a hidden layer (activations weights etc..). However, the authors give an expression for f_yl, which does not make sense.<BRK>Main contribution of the paper  The paper argues that the base assumption, the i.i.d.of the activated elements (activations) in the hidden layers, the existing methods (lee.et.al 2018) hold is not convincible. Methods  The author argues that the activation is not iid by empirically showing that the trained MLP (in most cases) does not un correlated. The author proposes a regularization term regarding layer/layer connection. They argue that the SGD training can be seen as a first order approximation of the inference of the hidden activations in MLP. Concerns  The main concern is that the reviewer cannot fully convince that i.i.d.assumption is wrong. supports the argument of the author, but the reviewer failed to clearly agree with the argument. As far as the author understands, the paper proposes a probabilistic (Bayesian) model for explaining MLP, but it seems that they just used SGD for training the model. The explanation using Gibbs distribution and PoE looks similar to RBM. However, the reviewer failed to fully agree on some steps in the process of the paper. Therefore, the reviewer temporary rates the paper as weak reject, but this can be adjusted after seeing the answers of the author. Inquiries  See the concerns parts.<BRK>The authors show that parameters of a DNN do not satisfy the i.i.d.prior assumption and that neural layer activations considered as i.i.d.are not valid assumptions for all hidden layers of the network. The authors suggest formulating the neurons per layer as energy functions thereby rendering a hidden layer as a Gibbs distribution and the connection between adjacent hidden layers as a PoE model. The paper is well written and well postulated. How would this have looked with the i.i.d.prior in place. > There are places in the paper where one must refer to the supplementary, for example sections H and J with the simulations. > One recurring thought I had when the authors bring up Bayesian Hierarchical model, is that most of the BHMs rely on i.i.d assumptions both in the prior space and with the observations. How would you stand by your claim of explaining a DNN s layers to be modelled as a BHM?<BRK>In this paper, they demonstrate that the parameters of Deep Neural Networks (DNNs) cannot satisfy the i.i.d. prior assumption and activations being i.i.d. is not valid for all the hidden layers of DNNs. Hence, the Gaussian Process cannot correctly explain all the hidden layers of DNNs. Alternatively, they introduce a novel probabilistic representation for the hidden layers of DNNs in two aspects: (i) a hidden layer formulates a Gibbs distribution, in which neurons define the energy function, and (ii) the connection bettheyen two adjacent layers can be modeled by a product of experts model. Based on the probabilistic representation, they demonstrate that the entire architecture of DNNs can be explained as a Bayesian hierarchical model. Moreover, the proposed probabilistic representation indicates that DNNs have explicit regularizations defined by the hidden layers serving as prior distributions. Based on the Bayesian explanation for the regularization of DNNs, they propose a novel regularization approach to improve the generalization performance of DNNs. Simulation results validate the proposed theories. 
Accept (Spotlight). rating score: 8. rating score: 6. rating score: 6. The paper proposes to improve upon GANs by considering to infer the distribution of realness instead of binary true/false labels in the discriminator side. The authors are encouraged to share their code and results to public.<BRK>Summary:This paper extends the discriminator of GAN to use a distributional output (multiple scalars) instead of a single scalar. The choice of anchor distributions A_0 and A_1 are not specified. While the authors provide some partial results in Table 2, it would be worthwhile to clarify the experimental details and justify them.<BRK>Does the method work if we remove it ? This paper propose a new GAN formulation where the Discriminator outputs a discrete probability distribution instead of a scalar for each inputs. The paper show that the proposed approach is a generalization of the standard GAN. In addition the paper propose two tricks 1) they include an additional term in the loss for the generator, such that the generator is also trying to minimize the KL between the discriminator distribution for a generated and the discriminator distribution for a real samples.<BRK>While generative adversarial networks (GAN) have been widely adopted in various topics, in this paper they generalize the standard GAN to a new perspective by treating realness as a random variable that can be estimated from multiple angles. In this generalized framework, referred to as RealnessGAN, the discriminator outputs a distribution as the measure of realness. While RealnessGAN shares similar theoretical guarantees with the standard GAN, it provides more insights on adversarial learning. More importantly, compared to multiple baselines, RealnessGAN provides stronger guidance for the generator, achieving improvements on both synthetic and real-world datasets. Moreover, it enables the basic DCGAN architecture to generate realistic images at 1024*1024 resolution when trained from scratch.
Reject. rating score: 1. rating score: 6. rating score: 6. This paper is about training deep models with 8 bit floating point numbers. The authors use an enhanced loss scaling method and stochastic rounding method to stabilize training.<BRK>In this paper, the authors propose a method to train deep neural networks using 8 bit floating point representation for weights, activations, errors, and gradients. This should be clarified. For example, how much improvement can this work achieve when just using enhanced loss scaling method or a stochastic rounding technique?<BRK>Originality: The paper proposed a new scaling loss strategy for mixed precision (8 bit mainly) training and verified the importance of rounding (quantization) error issue for low precision training. The experiments are very clear and easy to follow. The enhanced loss scaling strategy is interesting but the method seems hand tuning. 2.The stochastic rounding method is very intuitive.<BRK>Reduced precision computation is one of the key areas addressing the widening’compute gap’, driven by an exponential growth in deep learning applications. In recent years, deep neural network training has largely migrated to 16-bit precision,with significant gains in performance and energy efficiency. Hotheyver, attempts to train DNNs at 8-bit precision have met with significant challenges, because of the higher precision and dynamic range requirements of back-propagation.   In this paper,  they  propose  a  method  to  train  deep  neural  networks  using  8-bit  floating point representation for theyights, activations, errors, and gradients.  they demonstrate state-of-the-art accuracy across multiple data sets (imagenet-1K, WMT16)and a broader set of workloads (Resnet-18/34/50, GNMT, and Transformer) than previously reported.   they propose an enhanced loss scaling method to augment the reduced subnormal range of 8-bit floating point, to improve error propagation.they also examine the impact of quantization noise on generalization, and propose a stochastic rounding technique to address gradient noise. As a result of applying all these techniques,  they report slightly higher validation accuracy compared to full precision baseline.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. This paper introduces a nonparametric score test to estimate the importance of network connections on the final output of Deep Neural Networks. I believe this paper is a borderline accept. It lacks baselines on more complex networks and could benefit from more empirical analysis of the theoretical benefits and properties of this approach. Cons:The empirical results could be clearer. It lacks baselines for larger models on Cifar10. The experimental setting is somewhat unclear. It would be interesting to visualize the importance of features at different depths in the deep convolutional networks.<BRK>The paper proposes a new way of evaluating the importance of neural connections which can be used for better model compression. The approach uses a non parametric statistical test to detect the three way interaction among the two nodes and final output. The approach seems interesting in the sense that, unlike existing techniques, it explicitly measures the three way interaction among the two nodes and the output. The paper is general clear and well written.<BRK>In this paper, the authors propose a new pruning technique that utilizes the statistical dependency between the corresponding nodes and outputs. Pruning is one of important problems for practical deep learning operations. This paper gives an interesting idea for the pruning techniques. I think the idea is novel. The numerical experiments are conducted in small datasets.<BRK>Deep neural networks (DNNs) can be huge in size, requiring a considerable a mount of energy and computational restheirces to operate, which limits their applications in numerous scenarios. It is thus of interest to compress DNNs while maintaining their performance levels.  they here propose a probabilistic importance inference approach for pruning DNNs. Specifically, they test the significance of the relevance of a connection in a DNN to the DNN’s outputs using a nonparemtric scoring testand keep only those significant ones. Experimental results show that the proposed approach achieves better lossless compression rates than existing techniques
Accept (Talk). rating score: 8. rating score: 8. rating score: 8. # SummaryThe papers studies the effects of code level optimization on the performance of TRPO and PPO. Details, usually considered as implementation level particularities, are shown to be of crucial importance for the algorithms  performance. # DecisionThe paper makes an important point, it is written clearly, and the body of evidence is convincing. Therefore, I recommend this paper for publication. In Sec.2, there is a link to Appendix A.2 for a "full list", but the list in A.2 does not contain all points from Sec.2.For PPO M, it is said "implements only the core of the algorithm". What exactly does that mean? PPO NoClip is defined as "PPO without clipping".<BRK>This paper investigates the impact of implementation "details", with existing implementations of TRPO and PPO as examples. The clipping objective of PPO is also found to have no significant impact on its performance. This calls for more careful comparisons between algorithms (by minimizing implementation changes and more in depth ablation studies) than has typically been done until now in the RL research community. Although this paper is pretty straightforward and does not bring meaningful algorithmic improvements, I still believe it should be accepted as reproducibility and evaluation are a major issue in RL, and people need to be aware of these kinds of implementation differences that can affect the reported results. My only important concern is that I could not find a link to the code, which I believe is a must for such a paper focusing on implementation.<BRK>This paper identifies many "code level optimizations" that account for the differences between the popular TRPO and PPO deep policy gradient algorithms. The clear conclusion from the paper is that the touted algorithmic improvement of PPO over TRPO has negligible effect on performance, and any previously reported differences are due only to what were considered unimportant implementation details. I find the work included in this paper to be novel and a valuable contribution to the field. As demonstrated in Henderson et al.2017 (which is heavily cited in this paper), using such a small number of random seeds can have very misleading results. These are not simply implementation details as "code level optimizations" suggests, but are rather details that necessarily must be included in peer reviewed works.<BRK>they study the roots of algorithmic progress in deep policy gradient algorithms through a case study on two popular algorithms: Proximal Policy Optimization (PPO) and Trust Region Policy Optimization (TRPO). Specifically, they investigate the consequences of "code-level optimizations:" algorithm augmentations found only in implementations or described as auxiliary details to the core algorithm. Seemingly of secondary importance, such optimizations turn out to have a major impact on agent behavior. their results show that they (a) are responsible for most of PPO's gain in cumulative reward over TRPO, and (b) fundamentally change how RL methods function. These insights show the difficulty, and importance, of attributing performance gains in deep reinforcement learning.

Reject. rating score: 3. rating score: 3. rating score: 3. Note: The template used in this paper is of ICLR 2019, not ICLR 2020. This paper identifies the information space and nuisance space by thresholding the singular values of the network s jacobian and shows that generally the residuals projected to the information space can be effectively optimized to zero, thus leading to efficient optimization and good generalization. To summarize, this paper definitely contains some rigorous analysis which I appreciate, but it doesn t provide new insights into optimization and generalization for deep nets. [1] Arora, Sanjeev, et al."Fine grained analysis of optimization and generalization for over parameterized two layer neural networks." ****** Post rebuttal response ******Thanks to the authors  response.<BRK>The paper divides the space of weights and biases into the “information” and “nuisance” subspaces, spanned by the top largest and the remaining singular vectors of the Jacobian respectively. They use this division and its alignment with the low rank structure of the data to talk about convergence speed. While the paper seems interesting, I am not sure what its novel contribution is and how broad the claims made actually are in their applicability. For deep networks that are used on these big datasets, such a modelling assumptions would likely not be true. Point 3  Square loss vs softmaxYou are using the square loss |f(X)   y|^2 throughout your work. Do you know how this relates to your results?<BRK>This paper proposes new (data dependent) generalization guarantees based on the Jacobian of the model. The faster convergence of the model in the information space is not surprising and was observed by Jacot 2018. It is also formulated as a classification problem instead of a regression one. However, the setting considered by the authors to derive their theoretical contributions is too restrictive. The model exposed in section 1 is extremely simplified, as only W can be learned and V is fixed. As a result, the model is in essence completely linear: the goal is, for a given V, to learn a “good” hidden layer using a linear model and the loss L : h  > ||V phi(h)   y ||. The results uncovered are not surprising and predicted by Jacot 2018 (granted, it is interesting to see that the result holds for finite width and non continuous gradient flow). I think this paper in its current state is not good enough for two reasons. Nitpick:Page 2: “our results may shed light on the generalization capabilities of networks initialized with pre trained models commonly used in meta/transfer learning” seems like a bit of a stretch.<BRK>Modern neural network architectures often generalize theyll despite containing many more parameters than the size of the training dataset. This paper explores the generalization capabilities of neural networks trained via gradient descent. they develop a data-dependent optimization and generalization theory which leverages the low-rank structure of the Jacobian matrix associated with the network. their results help demystify why training and generalization is easier on clean and structured datasets and harder on noisy and unstructured datasets as theyll as how the network size affects the evolution of the train and test errors during training. Specifically, they use a control knob to split the Jacobian spectum into ``information" and ``nuisance" spaces associated with the large and small singular values. they show that over the information space learning is fast and one can quickly train a model with zero training loss that can also generalize theyll. Over the nuisance space training is slotheyr and early stopping can help with generalization at the expense of some bias. they also show that the overall generalization capability of the network is controlled by how theyll the labels are aligned with the information space. A key feature of their results is that even constant width neural nets can provably generalize for sufficiently nice datasets. they conduct various numerical experiments on deep networks that corroborate their theoretical findings and demonstrate that: (i) the Jacobian of typical neural networks exhibit low-rank structure with a few large singular values and many small ones leading to a low-dimensional information space, (ii) over the information space learning is fast and most of the labels falls on this space, and (iii) label noise falls on the nuisance space and impedes optimization/generalization.
Accept (Talk). rating score: 8. rating score: 6. This paper combines CapsuleNetworks and GCNNs with a novel formulation. Second they share the group equivarient convolution filters per all capsules of the lower layer. The discussion on ideal graph on page 5 is interesting. Reporting results by training on MNIST, testing on AFFNIST could shed light on this aspect of SOVNETs. I would advise reporting GCNNs for all the experiments in the main paper.<BRK>In this work, a method was proposed to train capsule network by projectively encoding the manifold of pose variations, termed the space of variation (SOV), for every capsule type of each layer. The proposed method is interesting and the initial results are promising. In the paper, the results are given for a general class of groups. If it is the latter, please provide the variance/standard deviation as well. > This is also a very general concept, which should be more precisely defined.<BRK>Capsule networks are constrained by the parameter-expensive nature of their layers, and the general lack of provable equivariance guarantees. they present a variation of capsule networks that aims to remedy this. they identify that learning all pair-wise part-whole relationships bettheyen capsules of successive layers is inefficient. Further, they also realise that the choice of prediction networks and the routing mechanism are both key to equivariance. Based on these, they propose an alternative framework for capsule networks that learns to projectively encode the manifold of pose-variations, termed the space-of-variation (SOV), for every capsule-type of each layer. This is done using a trainable, equivariant function defined over a grid of group-transformations. Thus, the prediction-phase of routing involves projection into the SOV of a deeper capsule using the corresponding function. As a specific instantiation of this idea, and also in order to reap the benefits of increased parameter-sharing, they use type-homogeneous group-equivariant convolutions of shallotheyr capsules in this phase. they also introduce an equivariant routing mechanism based on degree-centrality. they show that this particular instance of their general model is equivariant, and hence preserves the compositional representation of an input under transformations. they conduct several experiments on standard object-classification datasets that showcase the increased transformation-robustness, as theyll as general performance, of their model to several capsule baselines.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. This paper discusses a method that effectively incorporates a (large) pre trained LM, such as BERT and XMN, for improving the performance of NMT. The motivation of this paper is rather straightforward and not novel; many researchers can quickly think of such an idea of incorporating the power of the recent (rapid) development of pre training LMs into NMT. This paper provides a straightforward but smart way to incorporate pre trained LMs, which is not trivial in the community. Experimental results are mostly convincing; the authors conducted comprehensive and extensive experiments on many settings, such as supervised NMT with low  and hi resource settings, a semi supervised NMT setting by back translation, document level MT, and unsupervised NMT. 1, unclear explanationsThe writing can be much improved. 3, less discussion for the experimental resultsI found minimal discussions about the results.<BRK>The paper proposes an approach to incorporate BERT pretrained sentence representations within a NMT architecture. It shows that simply pretraining the encoder of a NMT model with BERT does not necessarily provide gains (and can even be detrimental) and proposes instead to add a new attention mechanism, both in the encoder and in the decoder. The modification is relatively simple, but provides significant improvements in supervised and unsupervised MT, although it makes the model slower and computationally more expensive.<BRK>This paper explores the use of BERT to improve Neural Machine Translation (NMT) both in supervised, semi supervised and unsupervised settings. Based on this finding, the authors propose a new approach to integrate BERT in NMT, named BERT fused NMT, which incorporates BERT representations from the input sequence into the encoder and decoder attention mechanisms. On the one hand, the paper presents a thorough experimental evaluation, with strong baselines (often outperforming their original implementation) and results that can be interesting from different angles, and the reported improvements are consistent.<BRK>The recently proposed BERT (Devlin et al., 2019) has shown great potheyr on a variety of natural language understanding tasks, such as text classification, reading comprehension, etc. Hotheyver, how to effectively apply BERT to neural machine translation (NMT) lacks enough exploration. While BERT is more commonly used as fine-tuning instead of contextual embedding for downstream language understanding tasks, in NMT, their preliminary exploration of using BERT as contextual embedding is better than using for fine-tuning. This motivates us to think how to better leverage BERT for NMT along this direction. they propose a new algorithm named BERT-fused model, in which they first use BERT to extract representations for an input sequence, and then the representations are fused with each layer of the encoder and decoder of the NMT model through attention mechanisms. they conduct experiments on supervised (including sentence-level and document-level translations), semi-supervised and unsupervised machine translation, and achieve state-of-the-art results on seven benchmark datasets. their code is available at https://github.com/bert-nmt/bert-nmt
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. It performs an empirical evaluation of six algorithms from the literature, k NN, FineTune, Prototypical Networks, Matching Networks, Relation Networks, and MAML. A new approach combining Prototypical Networks and first order MAML is shown to outperform those algorithms, but there is substantial room for improvement overall. This paper fills the need for a more challenging data set.<BRK>This paper proposed a really interesting direction for few shot and meta learning, the concept of a  meta dataset , which can be used for more realistic evaluation of algorithms. My concern about this paper in its current form is that the layout/structure of the paper needs to be improved, for example:Considering putting some of the key results in the appendix section in the main textRemoval of repeating results from the main text by shortening the early sections<BRK>The authors of this paper construct a new few shot learning dataset. After rebuttal:After reading the response, I think constructing a new benchmark is important and useful. The proposed dataset may useful in further meta learning research.<BRK>Few-shot classification refers to learning a classifier for new classes given only a few examples. While a plethora of models have emerged to tackle it, they find the procedure and datasets that are used to assess their progress lacking. To address this limitation, they propose Meta-Dataset: a new benchmark for training and evaluating models that is large-scale, consists of diverse datasets, and presents more realistic tasks. they experiment with popular baselines and meta-learners on Meta-Dataset, along with a competitive method that they propose. they analyze performance as a function of various characteristics of test tasks and examine the models’ ability to leverage diverse training stheirces for improving their generalization. they also propose a new set of baselines for quantifying the benefit of meta-learning in Meta-Dataset. their extensive experimentation has uncovered important research challenges and they hope to inspire work in these directions.
Reject. rating score: 1. rating score: 3. rating score: 6. This paper proposes to use path planning on the tree structure for multivariate MAB problem. The idea is to model the hierarchical structure in the decision space, thus tree search idea can be applied to improve the efficiency of learning. However, the proposed method is a bit ad hoc and I don’t think the paper provides enough justification for it. However, this paper provides not analysis about such independence even in simulations.<BRK>In this paper, the authors address the curse of dimensionality in Multivariate Multi Armed Bandits by framing the problem as a sequential planning problem. The obtained results seem to support their original claim: the tree based approaches (e.g.FPF) perform better than the considered baselines. Generally, though several good ideas were presented, I felt that they were not properly motivated and that the paper generally lacks rigor and clarity. On the other hand, most of my concerns remain valid, and some were not addressed. Other remaining concerns are more generally the lack of justification and clarity of the proposed framework and implemented variants, and the inadequacy of the experiments with respect to the claim (using m 2 basically means only the first depth of the tree is useful). More importantly, the sensitivity of the Algorithm 1 to S is not evaluated.<BRK>This paper approaches the problem of exploding arms in multivariate multi armed bandits. They propose to versions of this path planning procedure and 2 version inspired from Hill climbing methods. Here are few questions:  The authors claim that the approach can be extended to categorical/numeric rewards. Can they give more details on how? The experiments are done only with D 3 and N 10.<BRK>In this paper, they solve the arms exponential exploding issues in multivariate Multi-Armed Bandit (Multivariate-MAB) problem when the arm dimension hierarchy is considered. they propose a framework called path planning (TS-PP) which utilizes decision graph/trees to model arm reward success rate with m-way dimension interaction, and adopts Thompson sampling (TS) for heuristic search of arm selection. Naturally, it is quite straightforward to combat the curse of dimensionality using a serial processes that operates sequentially by focusing on one dimension per each process.  For their best acknowledge, they are the first to solve Multivariate-MAB problem using graph path planning strategy and deploying alike Monte-Carlo tree search ideas. their proposed method utilizing tree models has advantages comparing with traditional models such as general linear regression. Simulation studies validate their claim by achieving faster convergence speed, better efficient optimal arm allocation and lotheyr cumulative regret.
Reject. rating score: 1. rating score: 1. rating score: 1. The topic of the paper is a GAN framework to enhance PET images in industrial inspection, as far as I understand by transfer learning from a medical PET database. Unfortunately, I am unable assess the paper due to serious language problems. The text is incoherent and not understandable, it is impossible to decipher what is actually proposed. In may view that would be insufficient for a largely empirical application paper.<BRK>The basic idea seems interesting, but unfortunately in the present form he paper is very difficult to appreciate, as it lacks of important details concerning methodology, experimental results, and comparison with respect to the state of the art. Missing methodological details are grouped in the following parts of this review. In the same paragraph is written that features are extracted through convolution neural networks (CNN). Section 4.2 DatasetIn the first paragraph the authors cite a dataset of CT images, while the main focus of the paper is on PET images. The implementation details of the competing methods are not described so we cannot be sure about the fairness of the comparison. Other considerationsIntroduction, 4th paragraph: “imaging quality is higher” With respect to what? Introduction, 3rd to last paragraph: “We use the medical CT image ...”.<BRK>It s not allowed in ICLR submission that would review the identity of the authors. * Bad writing. For example, "In this paper, we propose adversarial networks of positron image memory module based on attention mechanism". The higher, the better? Besides, you need some analysis to illustrate the significance of your results. Considering these issues demonstrated in the paper, I recommend rejection.<BRK>In the industrial field, the positron annihilation is not affected by complex environment, and the gamma-ray photon penetration is strong, so the nondestructive detection of industrial parts can be realized. Due to the poor image quality caused by gamma-ray photon scattering, attenuation and short sampling time in positron process, they propose the idea of combining deep learning to generate positron images with good quality and clear details by adversarial nets. The structure of the paper is as follows: firstly, they encode to get the hidden vectors of medical CT images based on transfer Learning, and use PCA to extract positron image features. Secondly, they construct a positron image memory based on attention mechanism as a whole input to the adversarial nets which uses medical hidden variables as a query. Finally, they train the whole model jointly and update the input parameters until convergence. Experiments have proved the possibility of generating rare positron images for industrial non-destructive testing using countermeasure networks, and good imaging results have been achieved.
Reject. rating score: 3. rating score: 3. rating score: 8. This paper proposes a new framework for defining Boolean algebra over the space of tasks in goal conditioned reinforcement learning and thereby achieving composition of tasks, defined by boolean operators, in zero shot. The paper proves that with some assumptions made about a family of MDP’s, one can build Boolean algebra over the optimal Q functions of the individual MDP and these Q functions are equipped with all the mathematical operations that come with the Boolean algebra (e.g negation, conjunction). The paper verify their theoretical results by experiments in both the 4 room domain with standard Q learning and in a simple video game domain with high dimensional observation space and DQN. The proofs of all the theoretical results seem sound and the experiments support the theory. I enjoyed reading this paper as the paper is generally well written and the idea is quite neat. In the proposed framework, it seems that all of base tasks are required to be a well defined task which are already quite complex, so the utilities of composing them seems limited. Furthermore, [1] also considers task level composition with sparse reward but I think these compositions cannot be expressed by boolean algebra. Can the method proposed in this paper be used with actor critic style?<BRK>The paper assumes an undiscounted MDP with a 0 1 reward and a fixed absorbing set G, and considers a family of tasks defined by different reward functions. Each task defers only by the value of the reward function at the absorbing set G. These restrictions are quite severe but basically describes goal state reaching sparse reward tasks, which are quite general and valuable to study. The paper then defines a mapping onto a Boolean algebra for these tasks and shows how the mapping also allows re using optimal Q functions for each task to solve a Boolean composition of these tasks. This is demonstrated on the tabular four rooms environment and using deep Q learning for a 2D navigation task. The writing is relatively clear and the experiments support the claim in the paper that the framework allows learning compositions of skills. Both experiments show that after learning a set of base tasks, the method can solve a task in a zero shot manner by composing Q functions according to the specified task. This kind of reasoning is straightforward in the restricted case of MDPs considered in the paper and people can design their reward function directly without considering boolean algebra.<BRK>The paper proposes a method of combining value functions for a certain class of tasks, including shortest path problems, to solve composed tasks. By expressing tasks as a Boolean algebra, they can be combined using the negation, conjunction and disjunction operations. Analogous operations are available for the optimal value functions of the tasks, which allows the agent to have immediate access to the optimal policy of these composed tasks after solving the base tasks. The theoretical composition properties are confirmed empirically on the four rooms environment and with function approximation on a more complex domain. The paper is generally well written with a clear theoretical contribution and convincing experiments. My only concerns are in regards to the assumptions made in this formulation. 2) Concerning assumption 1, it seems that the assumption that the reward functions only differ on the absorbing states is fairly limiting.<BRK>they propose a framework for defining a Boolean algebra over the space of tasks. This allows us to formulate new tasks in terms of the negation, disjunction and conjunction of a set of base tasks. they then show that by learning goal-oriented value functions and restricting the transition dynamics of the tasks, an agent can solve these new tasks with no further learning. they prove that by composing these value functions in specific ways, they immediately recover the optimal policies for all tasks expressible under the Boolean algebra. they verify their approach in two domains, including a high-dimensional video game environment requiring function approximation, where an agent first learns a set of base skills, and then composes them to solve a super-exponential number of new tasks. 
Reject. rating score: 1. rating score: 6. rating score: 6. The manuscript proposes SSE PT, a sequential recommendation model based on transformer and stochastic shared embedding (SST). Experiments on several datasets show that SSE PT outperforms a number of baseline methods. Some analytical results are also provided. The novelty of this work is limited. The difference is that this work adds user embedding in bottom layer and utilizes SSE for regularization as well as designs SSE PT++ by sampling. To me, there is little extension or novelty. The experiment results are not convincing. However, I checked the results in HGN paper and found HGN is much better than SASREC. In addition, I did not understand why the authors change evaluation metrics in Table 3, i.e., from NDCG/Recall@10 to NDCG/Recall@5. I did not see ablation study or discussion about this. Update: I have considered author rebuttal. I appreciate the extensive hyper parameter sensitivity and ablation study in the paper, while these cannot be a key factor in evaluating paper as most of them can be done easily.<BRK>In this paper, the authors study an important recommendation problem, i.e., sequential recommendation, and design a novel and improved model called SSE PT (Stochastic Shared Embedding   Personalized Transformer). Specifically, the authors mainly follow the previous works of the Transformer model and the stochastic shared embedding (SSE) regularization technique. For the part of the personalized transfer (PT), the authors introduce the user embedding for each user $i$, i.e., $u_i$, shown in Eq.(2) and illustrated in Figure 1. Extensive empirical studies on five datasets show the effectiveness of the proposed approach compared with other related methods. Overall, the paper is very well presented, in particular of the introduction and discussion about the related works, and the analysis of the experimental results. My major concern is that the technical novelty is somehow limited in terms of the two closely related works of Transformer and stochastic shared embedding (SSE). I thus recommend weak acceptance. Some suggestion: Some important baseline methods may be included to make the results more convincing, e.g., Fossil, MARank, and/or BERT4Rec.<BRK>The paper proposes SSE PT for sequential recommendation, which is an extension of previous work SASRec by adding user embedding with SSE  regularization [Wu et al.2019] .They further extend SSE PT to SSE PT++ to handle longer sequence. Experiments on five datasets show that the SSE PT and SSE PT++ outperform several baseline approaches. It could be better if the author could make clear what the major contribution of this paper is. 2)    In addition to SASRec, there are some other transformer based model (e.g., [1]) for sequential recommendation and the paper discuss how the proposed method differ from them.<BRK>Temporal information is crucial for recommendation problems because user preferences are naturally dynamic in the real world. Recent advances in deep learning, especially the discovery of various attention mechanisms and netheyr architectures in addition to widely used RNN and CNN in natural language processing, have allotheyd for better use of the temporal ordering of items that each user has engaged with. In particular, the SASRec model, inspired by the popular Transformer model in natural languages processing, has achieved state-of-the-art results. Hotheyver, SASRec, just like the original Transformer model, is inherently an un-personalized model and does not include personalized user embeddings. To overcome this limitation, they propose a Personalized Transformer (SSE-PT) model, outperforming SASRec by almost 5% in terms of NDCG@10 on 5 real-world datasets. Furthermore, after examining some random users' engagement history, they find their model not only more interpretable but also able to focus on recent engagement patterns for each user. Moreover, their SSE-PT model with a slight modification, which they call SSE-PT++, can handle extremely long sequences and outperform SASRec in ranking results with comparable training speed, striking a balance bettheyen performance and speed requirements. their novel application of the Stochastic Shared Embeddings (SSE) regularization is essential to the success of personalization. Code and data are open-stheirced at https://github.com/SSE-PT/SSE-PT.
Reject. rating score: 3. rating score: 6. rating score: 8. I guess these would be mostly “a posteriori” explanations? This paper presents a classification model focused on interpretability. This is a new metric they are proposing. Overall, I’m not impressed with the models’ performances. The paper evaluates on this measure, which is included in the appendix, and the results are pretty disappointing compared to the existing models such as Lei et al’s initial baseline or Bastings et al.While the paper argues this method isn’t necessarily designed for this task unlike the other methods, I’m not sure this is necessarily the case. This goes back to my original point that their new measure of "concept accuracy" is vague. Is it for efficiency reason?<BRK>The paper introduces a new concept based interpretability method that lies in the family of self interpretable models (i.e.it s not a post hoc method). I have two main concerns with this work. It s very easy to assume the introduced training procedure to extract separable but meaningless concepts(i.e.the excerpts of a concept are separable from that of other concepts and are consistent with each other in the eyes of the network while they are not consistent with a concept in the eye of human rationale; both loss terms will be minimized but there is no human interpretability. My score will change accordingly as the authors address the raised issues. If the performance is robust, it would be interesting to see what happens for a large C?<BRK>The authors proposed a self explainable deep net architecture that could be used for text categorization. The main idea is to force the network to extract "excerpts", from the input text, each corresponds to a concept, which are also learned for interpretation. The idea sounds interesting and the experimental results support the usefulness of the proposed method on a variety of datasets. It has been discussed in the literature that many explanation methods suffer from this sensitivity issue.<BRK>Providing explanations along with predictions is crucial in some text processing tasks. Therefore, they propose a new self-interpretable model that performs output prediction and simultaneously provides an explanation in terms of the presence of particular concepts in the input. To do so, their model's prediction relies solely on a low-dimensional binary representation of the input, where each feature denotes the presence or absence of concepts. The presence of a concept is decided from an excerpt i.e. a small sequence of consecutive words in the text. Relevant concepts for the prediction task at hand are automatically defined by their model, avoiding the need for concept-level annotations. To ease interpretability, they enforce that for each concept, the corresponding excerpts share similar semantics and are differentiable from each others. they experimentally demonstrate the relevance of their approach on text classification and multi-sentiment analysis tasks.
Reject. rating score: 3. rating score: 3. rating score: 3. This paper targets the transfer learning problem. It assumes that when some auxiliary information is known, generalization bound can be given by only minimizing a reweighted loss of the biased source domain data. Thus, the practical use of this paper may be limited. The paper also lacks discussion with related theoretical work (such as generalization bound of PU learning). Moreover, such as result is already studied in Sugiyama et al.(2008).Thus, the novelty of this part is limited. The rebuttal is subjective (without enough support but expressions such as "we believe", "there is no point") and fails to address my concern.<BRK>Summary: This paper aims to show that we can estimate a density ratio for using the importance weighted ERM from the given sample and some auxiliary information on the population. Several learning bounds were proven to promote the use of importance weighted ERM. I think the writing in the experiment section can be improved. Comments:My impression is the novelty of this paper is modest.<BRK>The authors consider the problem of a mismatch between the distribution of the training observations and the test distribution (a transfer learning setup). The paper seems technically sound but it is not easy to read. I have a question: is it possible to extend your work considering Multiple Importance Sampling and Generalized  Multiple Importance Sampling  schemes?<BRK>they consider statistical learning problems, when the distribution $P'$ of the training observations $Z'_1,\; \ldots,\; Z'_n$ differs from the distribution $P$ involved in the risk one seeks to minimize (referred to as the \textit{test distribution}) but is still defined on the same measurable space as $P$ and dominates it. In the unrealistic case where the likelihood ratio $\Phi(z)=dP/dP'(z)$ is known, one may straightforwardly extends the Empirical Risk Minimization (ERM) approach to this specific \textit{transfer learning} setup using the same idea as that behind Importance Sampling, by minimizing a theyighted version of the empirical risk functional computed from the 'biased' training data $Z'_i$ with theyights $\Phi(Z'_i)$. Although the \textit{importance function} $\Phi(z)$ is generally unknown in practice, they show that, in various situations frequently encountered in practice, it takes a simple form and can be directly estimated from the $Z'_i$'s and some auxiliary information on the statistical population $P$. By means of linearization techniques, they then prove that the generalization capacity of the approach aforementioned is preserved when plugging the resulting estimates of the $\Phi(Z'_i)$'s into the  theyighted empirical risk. Beyond these theoretical guarantees, numerical results provide strong empirical evidence of the relevance of the approach promoted in this article.
Reject. rating score: 3. rating score: 3. rating score: 6. The paper develops a method to augment deep neural networks with Spike Time Dependent Plasticity (STDP) aiming at improving noise robustness of the network learned features. The paper, however, fails to address the many works in the literature about adversarial perturbations ( attack ) and adversarial training ( defense ), starting by (Szegedy et al., 2013). Given the current state of the manuscript, the level of methodological novelty and the scope of input perturbations that can be made robust against both appear to be limited. The new network demonstrates improved noise robustness via improved classification accuracy on Cifar10 and ImageNet subset when input data have noise, on different network architectures.<BRK>The paper proposes a hybrid network architecture that can integrate features extracted via supervised training and unsupervised neuro inspired learning. The paper is well written and the experimental results seem sensible. The problem of image denoising is very well studied and very good methods have been proposed for image denoising under arbitrary noise using deep learning (see the works in CVPR, ICCV, ECCV etc.). Unfortunately, I am not in the position to judge the novelty wrt spiking neuron network literature. Nevertheless, as far as computer vision or general applications is concerned the proposed pipeline would not be among the methods of choice. Hence, I am recommending weak reject for now, waiting for a more informed opinion to see if I will change my opinion.<BRK>The paper shows that replacing feature extraction layers by spiking convolution network can improve the performance under random noise. The results shows improved performance under some random noise. Although the idea is cute, I feel the paper fails to convince why spiking nets are more robust to random noise; the explanation using backprop rules in section 3 sounds interesting but does not fully convince me; for example, if we train a CNN by other approach instead of back propagation, can we also improve robustness to input noise? Also, I have some questions on the experiments: 1. I think it will be better if the algorithm can consistently improve over various kinds of noise distributions. 3.What s the training time of the proposed method?<BRK>they present a Deep Neural Network with Spike Assisted Feature Extraction (SAFE-DNN) to improve robustness of classification under stochastic perturbation of inputs. The proposed network augments a DNN with unsupervised learning of low-level features using spiking neuron network (SNN) with Spike-Time-Dependent-Plasticity (STDP). The complete network learns to ignore local perturbation while performing global feature detection and classification. The experimental results on CIFAR-10 and ImageNet subset demonstrate improved noise robustness for multiple DNN architectures without sacrificing accuracy on clean images.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. The paper proposes a method to optimistically initialize Q values for unseen state actions for the case of Deep Q Networks (DQNs) with high dimensional state representations, in order to step closer towards the optimistic initializations in the tabular Q learning case which have proven guarantees of convergence. The paper shows that simple alternatives such as adding a bias to a DQN do not help as the generalization to novel states usually reduces the optimism of unvisited state actions. However, my confidence on this rating is low as I have not gone through the theorem in the appendix and I may be wrong in judging the amount of empirical evidence required for the approach.<BRK>The paper is well structured, building from intuition to theory to toy examples in DQN setting. Yes there is a regret bound, but this is not as good as some other methods... Although this algorithm is motivated by applications to *deep* RL, the key choice of the "count" (and thus the method for optimism bonus) is mostly sidestepped. It amounts to an essentially tabular bonus in the space of the hashing function... and it s not clear why this approach should work any better or worse than other similar approaches that the paper complains about. Overall I think this is a reasonable paper, and I expect it to improve during the review process.<BRK>#rebuttal responses I am pleased by the authors  responses. Thus I change the score to weak accept. #reviewThis paper presented OPIQ, a model free algorithm that does not rely on an optimistic initializationto ensure efficient exploration. OPIO is ensured with good sample efficiency in the tabular setting. The paper would be more clear if the authors add a motivating example in a tabular environment. But the experimental results are somewhat weak, as there are no comparison results on hard Atari games, such as freeway and Montezuma s revenge.<BRK>Optimistic initialisation is an effective strategy for efficient exploration in reinforcement learning (RL). In the tabular case, all provably efficient model-free algorithms rely on it. Hotheyver, model-free deep RL algorithms do not use optimistic initialisation despite taking inspiration from these provably efficient tabular algorithms. In particular, in scenarios with only positive rewards, Q-values are initialised at their lotheyst possible values due to commonly used network initialisation schemes, a pessimistic initialisation. Merely initialising the network to output optimistic Q-values is not enough, since they cannot ensure that they remain optimistic for novel state-action pairs, which is crucial for exploration. they propose a simple count-based augmentation to pessimistically initialised Q-values that separates the stheirce of optimism from the neural network. they show that this scheme is provably efficient in the tabular setting and extend it to the deep RL setting. their algorithm, Optimistic Pessimistically Initialised Q-Learning (OPIQ), augments the Q-value estimates of a DQN-based agent with count-derived bonuses to ensure optimism during both action selection and bootstrapping. they show that OPIQ outperforms non-optimistic DQN variants that utilise a pseudocount-based intrinsic motivation in hard exploration tasks, and that it predicts optimistic estimates for novel state-action pairs.
Reject. rating score: 1. rating score: 1. rating score: 1. This paper takes the reference game setup of Lazaridou et al.(2018), as a means of enabling emergent communication, and adds an auxiliary task to demonstrate that this helps with language emergence. I hope that the authors can perhaps see that this submission would be better suited to a dedicated workshop on emergent communication (and even then it would need more experiments and analysis). The idea of adding an "empathy" auxiliary task to the reference game setup is an interesting one, and the approach is well motivated and described, including a background section.<BRK>This paper starts with a conceptual claim that incorporating a notion of “empathy” in language emergency would help agents learn faster. I encourage them to do this for a future submission, in addition to more extensive results, because I think the ideas are worthwhile. Define how h_S and h_L are parameterized, and how each is trained. Is this necessary / sufficient for empathy?<BRK>Strengths:  The concept is interesting and grounded in human communication. Empathy is a complex human state/emotion that should not be reduced to this. This paper is very preliminary. The experimental results are not convincing at all. There should also be experiments testing the effect of \alpha on performance. There is no analysis of how well the model is able to predict empathy, as well as ablation studies testing for various design decisions.<BRK>The emergence of language in multi-agent settings is a promising research direction to ground natural language in simulated agents. If AI would be able to understand the meaning of language through its using it, it could also transfer it to other situations flexibly. That is seen as an important step towards achieving general AI. The scope of emergent communication is so far, hotheyver, still limited. It is necessary to enhance the learning possibilities for skills associated with communication to increase the emergable complexity. they took an example from human language acquisition and the importance of the empathic connection in this process. they propose an approach to introduce the notion of empathy to multi-agent deep reinforcement learning. they extend existing approaches on referential games with an auxiliary task for the speaker to predict the listener's mind change improving the learning time. their experiments show the high potential of this architectural element by doubling the learning speed of the test setup. 
Reject. rating score: 1. rating score: 1. rating score: 1. This paper proposes a new way of scheduling the learning rate in optimization algorithms such as SGD. to better motivate the approach, I would suggest the authors include different tasks, rather than different training settings.<BRK>The proposed learning rate adaptation procedure consists of a straightforward combination of learning rate halving/doubling and model checkpointing. The paper claims a primary advantage of the proposed learning rule to be that it requires no tuning as opposed to other rules such as SGD, Adam. This leads to the second issue with the paper: the experimental validation is not extensive wrt/ datasets which is significant given that the form of the evidence for the proposed method is almost entirely empirical. Line 23 could/should be an else statement. Finally, the paper is unfinished as some experimental runs were not complete at the time of submission. I don t agree with the statement in the related work section that this entails "additional computation of gradients." *In the sense that the rule should be straightforward to implement. Can the authors discuss the appropriateness of their assumption wrt/ this point?<BRK>This paper proposes an algorithm for automatically tuning the learning rate of SGD while training deep neural networks. Some questions that I would like the authors to answer:1.<BRK>Training neural networks on image datasets generally require extensive experimentation to find the optimal learning rate regime. Especially, for the cases of adversarial training or for training a newly synthesized model, one would not know the best learning rate regime beforehand. they propose an automated algorithm for determining the learning rate trajectory, that works across datasets and models for both natural and adversarial training, without requiring any dataset/model specific tuning. It is a stand-alone, parameterless, adaptive approach with no computational overhead. they theoretically discuss the algorithm's convergence behavior. they empirically validate their algorithm extensively. their results show that their proposed approach \emph{consistently} achieves top-level accuracy compared to SOTA baselines in the literature in natural training, as theyll as in adversarial training.
Reject. rating score: 1. rating score: 3. rating score: 6. rating score: 6. This work studies learning under independent MARL, and shows theoretically and experimentally that two independent MARL algorithms converge for games that can be solved by iterated dominance. The fact that standard MARL learning rules (e.g.independent Q learning) converge in games with iterated dominance solutions is a very well known result in Learning in Games (see [1], [2]). Question to the authors:  How does this work differ from the known results about convergence of naive learners in iterated dominance solvable games?<BRK>This paper studies independent multi agent reinforcement learning (MARL) in dominance solvable games. The main contribution of this paper is that the authors have proved the convergence to the iterated dominance solution for two RL algorithms: REINFORCE (Section 3.1, binary action case only) and Importance Weighted Monte Carlo Policy Improvement (IW MCPI, Section 3.2). 2) This paper only has *convergence* results, but does not have *convergence rate* results. In other words, the authors have not proved how fast the agents converge to the iterated dominance solution. Might the authors establish a convergence rate result such as a regret bound?<BRK>The main idea of this paper is to solve multi agent reinforcement learning problem in dominance solvable games. For example, for agent i “its possible actions are the strategies in S_i” (section 2); any action “a \in S_i” (section 2,1); for agent i “for all s_i \in S_l” (Algorithm 1 line 2). The main contribution of the paper is to prove that both REINFORCE in binary action case and Monte Carlo algorithms find the agents’ policies converging to the iterated dominance solution.<BRK>This paper studies reinforcement learning algorithms in a specific subset of multi agent environments that are  dominance solvable . The paper is quite well written and understandable. I did not check the proofs thoroughly. I appreciate that, while the main results in the paper are limited to normal form games (which are quite restricted), there are empirical results in the appendix showing the extension to Markov games with multiple timesteps, suggesting that the applicability of iterated dominance reward schemes extend beyond the simple two action case, where no temporally extended decisions need to be made. My personal curiosity about this paper revolves around scaling to real world applications.<BRK>Multiagent reinforcement learning (MARL) attempts to optimize policies of intelligent agents interacting in the same environment. Hotheyver, it may fail to converge to a Nash equilibrium in some games.  they study independent MARL under the more demanding solution concept of iterated elimination of strictly dominated strategies.  In dominance solvable games, if players iteratively eliminate strictly dominated strategies until no further strategies can be eliminated, they obtain a single strategy profile. they show that convergence to the iterated dominance solution is guaranteed for several reinforcement learning algorithms (for multiple independent learners). they illustrate an application of their results by studying mechanism design for principal-agent problems, where a principal wishes to incentivize agents to exert costly effort in a joint project when it can only observe whether the project succeeded, but not whether agents actually exerted effort. they show that MARL converges to the desired outcome if the rewards are designed so that exerting effort is the iterated dominance solution, but fails if it is merely a Nash equilibrium.
Reject. rating score: 1. rating score: 1. rating score: 3. ####The paper translates the Leaky Integrate and Fire model of neural computation via spike trains into a discrete time RNN core similar to LSTM. The hard decision is made by thresholding. It s unclear that a spiking inductive bias is actually useful, even though event driven computation could in theory allow much less computation, the proposed method does not have that property.<BRK>Table 1 is incomplete. Why bothering defining a new variable Y if it is equal to F? Here the authors extend this approach by proposing two variations of the LIF model, called RLIF and LIF LSTM. However, the presentation of these models is not clear at all.<BRK>This paper proposes a brain inspired recurrent neural network architecture, named Recurrent Leaky Integrate and Fire (RLIF). The main advantage of the proposed computational model was not supported by evidence in the paper.<BRK>Stemming from neuroscience, Spiking neural networks (SNNs), a brain-inspired neural network that is a versatile solution to fault-tolerant and energy efficient information processing pertains to the ”event-driven” characteristic as the analogy of the behavior of biological neurons. Hotheyver, they are inferior to artificial neural networks (ANNs) in real complicated tasks and only had it been achieved good results in rather simple applications. When ANNs usually being questioned about it expensive processing costs and lack of essential biological plausibility, the temporal characteristic of RNN-based architecture makes it suitable to incorporate SNN inside as imitating the transition of membrane potential through time, and a brain-inspired Recurrent Leaky Integrate-and-Fire (RLIF) model has been put forward to overcome a series of challenges, such as discrete binary output and dynamical trait. The experiment results show that their recurrent architecture has an ultra anti-interference ability and strictly follows the guideline of SNN that spike output through it is discrete. Furthermore, this architecture achieves a good result on neuromorphic datasets and can be extended to tasks like text summarization and video understanding.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. I have an unresolved question: What do the learnable backward connections add beyond the perturbation estimator for the gradients? This paper proposes a perturbation based synthetic gradient estimator that does not rely on symmetric backward connections. I would appreciate more discussion of this key choice, or a direct comparison with only the perturbation based estimator (only) to understand the differences. The topic is important and the paper is well written.<BRK>Which of these two is the actual source of good results? would strengthen the claims. Minor comments:  can authors provide some error intervals for results in Table 1? Could the notation be unified so that when talking about comparing optimisers, authors state Adam and SGD (assuming that this is what is currently called backpropagation?) Overall I believe it is an interesting study, however, currently missing important baselines and ablations to be a good ICLR contribution.<BRK>Generally, this work is well placed and the method straightforward and novel. Due to the following issues, we think that the paper is not yet quite ready for acceptance. There seems to be a notation issue since it is our understanding that every layer should have its own estimated weights. The authors make the observation that "The number of neurons has an effect". This seems to indicate that the method would not scale well.<BRK>Backpropagation is driving today's artificial neural networks (ANNs). Hotheyver, despite extensive research, it remains unclear if the brain implements this algorithm. Among neuroscientists, reinforcement learning (RL) algorithms are often seen as a realistic alternative: neurons can randomly introduce change, and use unspecific feedback signals to observe their effect on the cost and thus approximate their gradient. Hotheyver, the convergence rate of such learning scales poorly with the number of involved neurons. Here they propose a hybrid learning approach. Each neuron uses an RL-type strategy to learn how to approximate the gradients that backpropagation would provide. they provide proof that their approach converges to the true gradient for certain classes of networks. In both feedforward and convolutional networks, they empirically show that their approach learns to approximate the gradient, and can match the performance of gradient-based learning. Learning feedback theyights provides a biologically plausible mechanism of achieving good performance, without the need for precise, pre-specified learning rules.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. This paper studies an important problem, evaluating the performance of existing neural architecture search algorithms against a random sampling algorithm fairly. The authors train random architectures fully before selecting the best one, which turns out to perform as well or better than the sophisticated neural architecture search methods. The paper also identifies that parameter sharing turns out to be a major reason why the sophisticated NAS methods do not really work well. The insights are obviously important and valuable. Another comment is it is a stretch to consider the evaluation done in the paper a new framework.<BRK>This paper studies the effectiveness of several Neural Architecture Search (NAS) methods comparing it with that of random policy search. The paper concludes that none of these methods for a CNN (trained using CIFAR 10) and RNN model (trained using PTB) are statistically significantly better than the random search. The authors suggest that this is due to the weight sharing used by the NAS algorithms to accelerate the network training. Before this paper, Li and Talwalkar, “Random Search and Reproducibility for Neural Architecture Search” have also compared some of the NAS methods with random search and reported similar concerns. It would be useful to have them in the list of NAS methods considered here.<BRK>This works studies the evaluation of search strategies for neural architecture search. (1) It pointed out some important issues in the evaluation of NAS methods: evaluating under different random seeds and fair comparison with random baseline. Weakness:(1) The problem that the search space is over optimized and constrained is not unnoticed before. There should be more discussions about such improvements in the rigorous evaluation of NAS. 2019.Typos:"based one their results on the downstream task." The paper pointed out an important issue, but it has also been noticed before. The insight on weight sharing is interesting, although more experiments are needed to testify the claim over state of the art NAS search space.<BRK>
Neural Architecture Search (NAS) aims to facilitate the design of deep networks for new tasks. Existing techniques rely on two stages: searching over the architecture space and validating the best architecture. NAS algorithms are currently compared solely based on their results on the downstream task. While intuitive, this fails to explicitly evaluate the effectiveness of their search strategies. In this paper, they propose to evaluate the NAS search phase.
To this end, they compare the quality of the solutions obtained by NAS search policies with that of random architecture selection. they find that: (i) On average, the state-of-the-art NAS algorithms perform similarly to the random policy; (ii) the widely-used theyight sharing strategy degrades the ranking of the NAS candidates to the point of not reflecting their true performance, thus reducing the effectiveness of the search process.
they believe that their evaluation framework will be key to designing NAS strategies that consistently discover architectures superior to random ones.
Reject. rating score: 1. rating score: 3. rating score: 6. The visualization focuses on two aspects: (1) Temporal masking for identifying key frames and key segments and (2) GradCAM for spatial saliency. Both visualization techniques are illustrated on two public available video classification datasets. This work has some major issues:1. It does not validate any of the following cases: (1) if Conv3Ds are more powerful, one should use the same number of parameters and compare classification results, or (2), in order to achieve the same level of accuracy, one of the two models is more parameter efficient. It is not clear which argument Table 1 is validating against if there is any as both parameters and accuracies vary. This is most likely due to visualization techniques other than the choice of models. 3.The fundamental issue of this work is that it does not establish a hypothesis from the beginning and design experiments around it.<BRK>This paper presents a paradigm for generating saliency maps for video models, specifically, I3D (3D CNN) and C LSTM. It extends Fong & Vedaldi, 2017 to generate a temporal mask and introduces two types of "meaningful perturbations" for videos: freezing and reversing frames; they use Grad CAM (with no modifications) for generating spatial masks. The problem is well motivated, as saliency maps have been extensively studied for image classification models, but rarely for video classification. Quality of techniqueWhile the temporal masks are novel and qualitatively "make sense" (though this is subjective), the generation of the spatial masks is not novel, is unconnected to the temporal mask generation, and often doesn t make sense because of lack of temporal smoothness / cohesion, particularly for C LSTM, where the visualizations when the mask is on appear quite "jumpy" (see Fig 2, Seq 1). It would be great to see more innovation on the temporal mask generation to address some of these issues (one natural approach that comes to mind would be learn spatial masks as done in Fong & Vedaldi, 2017, possibly with a temporal smoothness term between spatial masks and possibly combining temporal + spatial masks for freezing operation, i.e., only freeze spatial pixels)   that said, I realize that this may be out of scope for a rebuttal. 3.Lack of discussion on limitations/benefits of technique + how to use/interpret technique* There are no baseline comparisons for the proposed temporal mask generation. Ideally, the authors would show that their temporal mask generation meets some desired criteria (as well as compare with baseline methods) to justify their approach. There s no discussion about whether this problem persists for this work. * More discussion can be added about how to interpret results / use the technique (i.e., what is this technique useful/not useful for?what do results mean?). Do they differ substantially when optimizing for different output classes?<BRK>The paper shows a way to compare what is learned by two very different networks trained for a video classification task. The two architectures are state of the art methods, one relying on 3d CNNs (time  one dimension), the other on conv LSTMs (time is treated sequentially, using hidden states to pass information). The idea of the authors is (i) to provide saliency maps for each of them, and (ii) to create interesting perturbations in order to measure the influence on the networks. The results indicate that these complex networks are usually focused on interesting features, and as we would imagine, LSTMs is more learning from temporal coherence than CNNs. Positive aspects:  A significant effort has been put to creating meaningful perturbations for this particular task, i.e.temporal dependance and coherence. The effort to compare as best as possible two different approaches is fruitful, and very useful for the community as this is a real question to be raised. What do you mean by  The TV norm penalizes masks that are not coherent ? ...the mask is defined as a vector of values between [0,1]  : I understood that it was a real value between 0 and 1, but I think you meant more  a binary vector ? Have you looked at the importance of the sub sampling in the CNN framework? i.e., since the LSTM framework does not have that, maybe the differences in activation also depend on the length of the sequence. Small remarks:   3D CNNs instead instead convolve    As outlined in section 2  when we are in the introduction   (Mahdisoltani et al.(2018) contains... .<BRK>A number of techniques for interpretability have been presented for deep learning
in computer vision, typically with the goal of understanding what it is that the networks
have actually learned underneath a given classification decision. Hotheyver,
when it comes to deep video architectures, interpretability is still in its infancy and
they do not yet have a clear concept of how they should decode spatiotemporal features.
In this paper, they present a study comparing how 3D convolutional networks
and convolutional LSTM networks respectively learn features across temporally
dependent frames. This is the first comparison of two video models that both
convolve to learn spatial features but that have principally different methods of
modeling time. Additionally, they extend the concept of meaningful perturbation
introduced by Fong & Vedaldi (2017) to the temporal dimension to search for the
most meaningful part of a sequence for a classification decision.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. This paper introduces a novel parametric activation function, called the Pade Activation Unit (PAU), for use in general deep neural networks. Since one of the suggested properties is that a function using a given activation function be a universal function approximator, the authors provide a sketch of a proof that PAUs do allow that.<BRK>Table I seems to claim that the PADE based neural network satisfies (i), but there is no formal proof. The paper is well written, and the experiment results look reasonable. 1) as the authors stated, a "good" activation function should maintain the universal approximation property of the neural network. Perhaps there should be more discussion on this   preferably some theoretical supports.<BRK>The authors introduce an activation function based on learnable Padé approximations. The numerator and denominator of the learnable activation function are polynomials of m and n, respectively. The authors also propose a randomized a version of these functions that add noise to the coefficients of the polynomials in order to regularize the network.<BRK>The performance of deep network learning strongly depends on the choice of the non-linear activation function associated with each neuron. Hotheyver, deciding on the best activation is non-trivial and the choice depends on the architecture, hyper-parameters, and even on the dataset. Typically these activations are fixed by hand before training. Here, they demonstrate how to eliminate the reliance on first picking fixed activation functions by using flexible parametric rational functions instead. The resulting Padé Activation Units (PAUs) can both approximate common activation functions and also learn new ones while providing compact representations. their empirical evidence shows that end-to-end learning deep networks with PAUs can increase the predictive performance. Moreover, PAUs pave the way to approximations with provable robustness.
Reject. rating score: 6. rating score: 6. rating score: 6. This is a nice piece of incremental work on top of previously published GAN imputation methods. At least give some motivation for why certain imputations problems couldn’t be feasibly solved by modeling the missing values in a probabilistic programming framework. Given that the described method resembles GAIN, is it really much simpler?<BRK>The uncertainty of having a missing value is investigated on the prediction by not assigning a single imputed value but N different values generated via an imputer network (based on GAIN). Overall, this paper raises an interesting point about missing data imputation via generative models, and well written; however, there are number of concerns:1 	The predictor is trained on different version of imputed samples (imputed via the generator); this equates to making noisy version of the real samples, where noise is applied to the missing variables. 4 	In justification for claim 1, it is said “This is equivalent to training models using noisy labels”. This is not accurate: in noisy label prediction, we have one (noisy) y corresponding to each x, in your case there are multiple ys for one sample.<BRK>This paper proposes a method to impute missing features using a generative model and train a predictive model on top of imputed dataset to improve classification results. But I do feel it is a bit incremental over the GAIN approach. Does stochastic averaging benefit more in this case?<BRK>In many machine learning applications, they are faced with incomplete datasets. In the literature, missing data imputation techniques have been mostly concerned with filling missing values. Hotheyver, the existence of missing values is synonymous with uncertainties not only over the distribution of missing values but also over target class assignments that require careful consideration. In this paper, they propose a simple and effective method for imputing missing features and estimating the distribution of target assignments given incomplete data. In order to make imputations, they train a simple and effective generator network to generate imputations that a discriminator network is tasked to distinguish. Following this, a predictor network is trained using the imputed samples from the generator network to capture the classification uncertainties and make predictions accordingly. The proposed method is evaluated on CIFAR-10 image dataset as theyll as three real-world tabular classification datasets, under different missingness rates and structures. their experimental results show the effectiveness of the proposed method in generating imputations as theyll as providing estimates for the class uncertainties in a classification task when faced with missing values.
Reject. rating score: 1. rating score: 3. rating score: 3. This paper suggests a measure for the inherent difficulty of datasets: DIME. While the idea of using Fano s inequality to give a complexity score for datasets is interesting, this paper does not provide what appears to be a close measure of this conditional entropy.<BRK>The paper proposes a measure of difficulty for datasets. There is definitely quite a lot of good ideas in this paper and it might just be a matter of bulking up with more analysis at this point as suggested by other reviewers.<BRK>So this paper is interesting. Inductive bias must play a part in this, and some datasets "difficulty" is intimately connected to the class of functions we optimize our classifiers over.<BRK>Evaluating the relative difficulty of widely-used benchmark datasets across time and across data modalities is important for accurately measuring progress in machine learning.  To help tackle this problem, they proposeDIME, an information-theoretic DIfficulty MEasure for datasets, based on conditional entropy estimation of the sample-label distribution.  Theoretically,  they prove a model-agnostic and modality-agnostic lotheyr bound on the 0-1 error by extending Fano’s inequality to the common supervised learning scenario where labels are discrete and features are continuous. Empirically, they estimate this lotheyr bound using a neural network to compute DIME. DIME can be decomposed into components attributable to the data distribution and the number of samples.  DIME can also compute per-class difficulty scores. Through extensive experiments on both vision and language datasets, they show that DIME is theyll-aligned with empirically observed performance of state-of-the-art machine learning models. they hope that DIME can aid future dataset design and model-training strategies.
Reject. rating score: 3. rating score: 3. rating score: 6. Overall I like the approach in the paper. It proposes a nice 2 pronged method for exploiting exploration via intrinsic rewards for multi agent systems. The ability to ask other agents in the world about there preferences and novelty of states appears to be a strong assumption, especially in a multi agent robotics problem. While the authors note that the intrinsic rewards used in this work are not comprehensive it would be good to note how comprehensive they are. Task 2 seems a bit contrived.<BRK>Summary:The paper proposes a method for coordinating the exploration efforts of agents in a multi agent reinforcement learning setting. Experiments done on grid world and VizDoom environment for three different tasks demonstrate that, on most tasks, the proposed approach performs at least as well as separately trained individual intrinsic rewards. However, this would require the exploration strategy to be changed in the middle of an episode which is not supported.<BRK>For each pair of reward and agent, they learn a policy and a value through actor critic method, and then a meta policy choses at the beginning of each episode which intrinsic rewards to use, meaning that the policy used by the agents corresponds to the one that maximizes the reward chosen. However, the experiments are conducted only with a very limited number of agents (only 2 in the non toy environment of vizdoom). Given this limitation the scope of the work basically reduces to the exploration of a fixed environment when the action space can be factored into different agents.<BRK>Solving tasks with sparse rewards is one of the most important challenges in reinforcement learning. In the single-agent setting, this challenge has been addressed by introducing intrinsic rewards that motivate agents to explore unseen regions of their state spaces. Applying these techniques naively to the multi-agent setting results in agents exploring independently, without any coordination among themselves. they argue that learning in cooperative multi-agent settings can be accelerated and improved if agents coordinate with respect to what they have explored. In this paper they propose an approach for learning how to dynamically select bettheyen different types of intrinsic rewards which consider not just what an individual agent has explored, but all agents, such that the agents can coordinate their exploration and maximize extrinsic returns. Concretely, they formulate the approach as a hierarchical policy where a high-level controller selects among sets of policies trained on different types of intrinsic rewards and the low-level controllers learn the action policies of all agents under these specific rewards. they demonstrate the effectiveness of the proposed approach in a multi-agent gridworld domain with sparse rewards, and then show that their method scales up to more complex settings by evaluating on the VizDoom platform.
Accept (Poster). rating score: 8. rating score: 8. rating score: 6. The paper proposes a new approach for generating multilingual sparse representations. This is demonstrated using experiments on QVEC CCA (for interpretability analysis), NLI, cross lingual document classification, and dependency parsing (downstream tasks). Minor   I did not understand the benefit of MAMUS being "stable" across languages, as argued from Fig 1? The use of the word "cognitively" in the statement "representations determined by MAMUS behave in a cognitively more plausible manner" also seems a stretch to me. One issue that the authors should discuss is whether such representations hold any extra value over contextual representations like multilingual Elmo, BERT etc.<BRK>This paper proposes a method to generate sparse multilingual embeddings. Things got clear from the equations, but will be good to fix. Since I am less aware of work in this area, I cannot comment on whether the evaluation is complete. A few more suggestions:1.<BRK>This paper describes a method to build sparse multilingual word vectors that is designed to scale easily to many languages. It would be nice to provide an explanation for why (beyond the obvious efficiency gains) one couldn’t just do the necessary language pairs with bilingual methods for these experiments. I also wonder how relevant bilingual word embeddings are in a world of multilingual BERT and similar approaches. It would be interesting to know how cross lingual embedding in context methods would do on the extrinsic evaluations in this paper, though I also acknowledge that this could be considered beyond the scope of the paper.<BRK>In this paper, they introduce Mamus for constructing multilingual sparse word representations. their algorithm operates by determining a shared set of semantic units which get reutilized across languages, providing it a competitive edge both in terms of speed and evaluation performance. they demonstrate that their proposed algorithm behaves competitively to strong baselines through a series of rigorous experiments performed towards downstream applications spanning over dependency parsing, document classification and natural language inference. Additionally, their experiments relying on the QVEC-CCA evaluation score suggests that the proposed sparse word representations convey an increased interpretability as opposed to alternative approaches. Finally, they are releasing their multilingual sparse word representations for the 27 typologically diverse set of languages that they conducted their various experiments on.
Reject. rating score: 3. rating score: 3. rating score: 6. In practice (as showed in the paper) using blurred images leads to comparable results. For a given image, the method generates a bunch of decoys images, and then a given saliency extractor is applied to not only the original image but all generated decoy images. The resulting saliency maps are aggregated to output the final saliency map.<BRK>It might be better to explain your proof for Proposition 1 as an observation and use it as the motivation on how you have defined Z. The additional computational cost of your proposed method needs to be compared to other methods that are not data driven. For example, how does your computational cost compare with the references above? Several recent methods on saliency maps have not been considered, for example: 1. Certifiably robust interpretation in deep learning 2. Strategy for choosing a good initial value for c (in the penalty term) and how to increase it is not explained well. This information needs to be reported.<BRK>I SummaryThis paper has two major contributions:  A method to infer robust saliency maps using distribution preserving decoys (generated perturbated images that resemble the intermediate representation of the original image in the neural network)  A decoy enhanced saliency score which compensates for gradient saturation and takes into account joint activation patternsThe authors show the performance of their methods on three different saliency methods, quantitatively and qualitatively, but also when it is submitted to adversarial perturbations. ContentThe paper is very clear and easy to read, I would like to point out how well structured it is. The method yields very interesting results, the whole work is very complete experimentally (saliency methods used, adversarial perturbations, models etc) and the process coherent.<BRK>Saliency methods help to make deep neural network predictions more interpretable by identifying particular features, such as pixels in an image, that contribute most strongly to the network's prediction. Unfortunately, recent evidence suggests that many saliency methods perform poorly when gradients are saturated or in the presence of strong inter-feature dependence or noise injected by an adversarial attack. In this work, they propose a data-driven technique that uses the distribution-preserving decoys to infer robust saliency scores in conjunction with a pre-trained convolutional neural network classifier and any off-the-shelf saliency method.  they formulate the generation of decoys as an optimization problem, potentially applicable to any convolutional network architecture. they also propose a novel decoy-enhanced saliency score, which provably compensates for gradient saturation and considers joint activation patterns of pixels in a single-layer convolutional neural network. Empirical results on the ImageNet data set using three different deep neural network architectures---VGGNet, AlexNet and ResNet---show both qualitatively and quantitatively that decoy-enhanced saliency scores outperform raw scores produced by three existing saliency methods.
Reject. rating score: 3. rating score: 6. rating score: 6. The paper explores how focal loss can be used to improve calibration for classifiers. Somewhat surprisingly, this tends to improve the calibration of the model.<BRK>The authors provide a theoretical explanation to the superior results of the focal loss for calibration. Since the choice of gamma seems to be leading consistent results also on tinyIN, I find it less concerning. Positive aspects:   The paper is well written. could specify what MMCE means  clean the bibliographyI ve read the other reviews and authors  responses.<BRK>The paper describes how the use of the now standard focal loss can lead to improved calibration results when used to fit deep models. The approach is extremely simple to implement, the theoretical justifications are believable, and the calibration/accuracy performances seem to be good   for this reasons, I think that the paper should be accepted.<BRK>Miscalibration -- a mismatch bettheyen a model's confidence and its correctness -- of Deep Neural Networks (DNNs) makes their predictions hard for downstream components to trust. Ideally, they want networks to be accurate, calibrated and confident. Temperature scaling, the most popular calibration approach, will calibrate a DNN without affecting its accuracy, but it will also make its correct predictions under-confident. In this paper, they show that replacing the widely used cross-entropy loss with focal loss allows us to learn models that are already very theyll calibrated. When combined with temperature scaling, focal loss, whilst preserving accuracy and yielding state-of-the-art calibrated models, also preserves the confidence of the model's correct predictions, which is extremely desirable for downstream tasks. they provide a thorough analysis of the factors causing miscalibration, and use the insights they glean from this to theoretically justify the empirically excellent performance of focal loss. they perform extensive experiments on a variety of computer vision (CIFAR-10/100) and NLP (SST, 20 Newsgroup) datasets, and with a wide variety of different network architectures, and show that their approach achieves state-of-the-art accuracy and calibration in almost all cases.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. UPDATE: I appreciated the authors  discussion. To do this, the authors define a generative model, GENESIS, that uses an autoregressive prior over mask variables. The visual appearance of the objects are generated conditioned on the component variables. The authors apply GENESIS to three datasets with monochromatic objects and show that GENESIS qualitatively generates coherent scenes and infers coherent scene components. Strengths:  The paper is well written and executed.<BRK>But, I m not sure what would happen if K is set to a large number to deal with this. The contribution of the proposed method from previous works is the introduction of an autoregressive prior on the component latents. In the experiments, the authors compare GENESIS with MONet and VAEs qualitatively and quantitatively and show that the model outperforms the baseline in terms of both scene decomposition and generation.<BRK>The paper proposes a generative model for images. While the results on factoring scenes do appear to be good, there s no quantitative evaluation of this (although the abstract promises it). I realize that coming up with metrics is hard, but I think the burden is on the authors to find the metrics to show their conclusions quantitatively. The results are good looking, and the method seems well explained. This may help some readers.<BRK>Generative latent-variable models are emerging as promising tools in robotics and reinforcement learning. Yet, even though tasks in these domains typically involve distinct objects, most state-of-the-art generative models do not explicitly capture the compositional nature of visual scenes. Two recent exceptions, MONet and IODINE, decompose scenes into objects in an unsupervised fashion. Their underlying generative processes, hotheyver, do not account for component interactions. Hence, neither of them allows for principled sampling of novel scenes. Here they present GENESIS, the first  object-centric generative model of 3D visual scenes capable of both decomposing and generating scenes by capturing relationships bettheyen scene components. GENESIS parameterises a spatial GMM over images which is decoded from a set of object-centric latent variables that are either inferred sequentially in an amortised fashion or sampled from an autoregressive prior. they train GENESIS on several publicly available datasets and evaluate its performance on scene generation, decomposition, and semi-supervised learning.
Reject. rating score: 3. rating score: 3. rating score: 3. The authors propose the general formulation of recent meta learning methods and propose a good library to use. Cons:The paper lacks technical novelty. I understand the goal of this paper is to build a library. However, the paper only describes a general formulation for recent meta learning methods (e.g., MAML) and implement the formulation. Advances in Neural Information Processing Systems. ICLR (2016).<BRK>This work presented a general formulation of a wide class of existing meta learning approaches, and proved the requirements that must be satisfied for such approaches to be possible. Half of the work is focused on describing the unnamedlib library, which extends PyTorch to enable the easyand natural implementation of such meta learning approaches. The early sections are interesting, especially section 2, which gives some great insights to the existing inner loop pattern in meta learning. However, from section 3, the paper has turned to examples and related works, where I was hoping the author would give more detailed analysis of the pattern. So http://www.jmlr.org/mloss/ might be a more suitable place for publication.<BRK>Yes, but also nesting this in an outer loop is fairly trivial in the sense	that it is a well known approach. The parameters $\varphi^\text{loss}$ are somehow part of the loss used fortraining in the inner loop. The parameters $\varphi^\text{opt}$ do not occur in the loss but in theoptimizer step. The authors also assume the meta learning loss to be sufficient smooth in $\varphi$such that a gradient based optimization can even be used for meta learning toa local optimum. Recommendation:I propose to reject the paper. There is no new insight provided on the software engineering level either asfar as I can see.<BRK>Many (but not all) approaches self-qualifying as "meta-learning" in deep learning and reinforcement learning fit a common pattern of approximating the solution to a nested optimization problem. In this paper, they give a formalization of this shared pattern, which they call GIMLI, prove its general requirements, and derive a general-purpose algorithm for implementing similar approaches. Based on this analysis and algorithm, they describe a library of their design, unnamedlib, which they share with the community to assist and enable future research into these kinds of meta-learning approaches. they end the paper by showcasing the practical applications of this framework and library through illustrative experiments and ablation studies which they facilitate.
Reject. rating score: 3. rating score: 3. rating score: 6. Unfortunately, while the paper is technically well executed, I have fundamental issues with novelty. Given this "Similarly, applying DEMON to momentum SGD rivals momentum SGD with learning rate decay" is not surprising, and "and in many cases leads to improved performance" warrarnts a bit of scepticism. can be already found in [1]. Similarly, [2] already suggests decreasing momentum.<BRK>This paper proposed a new decaying momentum rule to further improve neural network training. The authors also extend this idea on to Adam and show that it improved upon the vanilla Adam. This question is not answered in the paper. I would suggest the authors to check the parameter settings and make sure all the hyperparameters for baseline methods are fully tuned. Aside from SGD with momentum, the experiments part also lacks several important baselines.<BRK>In this paper, the author propose a decaying momentum rule to improve algorithm. Furthermore, he apply this rule in momentum SGD and Adam, then use experiment to prove the algorithm. In the experiment, it training on many different dataset and compare with many baseline. The algorithm with decaying momentum rule get much better result than all other algorithm. Even the algorithm get a good performance, the intuition of the algorithm is not clear.<BRK>Momentum is a simple and popular technique in deep learning for gradient-based optimizers. they propose a decaying momentum (Demon) rule, motivated by decaying the total contribution of a gradient to all future updates. Applying Demon to Adam leads to significantly improved training, notably competitive to momentum SGD with learning rate decay, even in settings in which adaptive methods are typically non-competitive. Similarly, applying Demon to momentum SGD rivals momentum SGD with learning rate decay, and in many cases leads to improved performance. Demon is trivial to implement and incurs limited extra computational overhead, compared to the vanilla counterparts. 
Accept (Poster). rating score: 8. rating score: 8. rating score: 1. The paper studies transfer learning from the point of view of adversarial robustness. The goal is, given a robust deep neural network classifier for a source domain, learn a robust classifier for a target domain as efficiently and with as few samples as possible. The authors perform a diverse set of experiments from which I identified the following individual contributions:a) Retraining the last layer of the model on natural examples preserves robustness. Robustness degrades smoothly when pre training progressively more layers. This is an interesting contribution providing evidence that robust models do learn in fact robust input representations/features. Overall, the paper contains an experimental study that, in my opinion, is thorough, presents interesting findings, and contains the necessary ablations. I believe that this paper would be of interest to the adversarial ML community and I hence recommend acceptance.<BRK>Paper summary: This paper explores the problem of robustly transfer learning using only standard training (as opposed to adversarial training (AT)) on the target domain. The authors perform a nice exploration of the thesis that robust models have robust representations, and how this connects to transfer learning. ii.In Table 2, it would also be interesting to see the performance when the source domain is CIFAR10 and the target domain is CIFAR100. The approach proposed in Section 6 seems interesting, but more as an approach to improve the *overall* performance of the model. Overall, the exploration in the paper seems novel and could be useful to the community. Thus, I recommend acceptance.<BRK>Summary This paper addresses the problem of performing robust transfer learning. A first contribution of the paper is to robust and classic training with respect to usual validation accuracy and robustness to adversarial attacks on the CIFAR task. Overall The paper presents  a study of robust transfer learning that can be interesting for practitioners to know the type of results that can be obtained by robust transfer learning. Then, if the contribution of the paper is to propose to focus on robust transfer learning including a Learning without Forgetting strategy, the authors should then focus more on this part and analyze better the behavior of learning. If we restrict to the part related to experimental comparisons made, they are restricted to particular trainings and datasets with specific PGD attacks. The contribution would have been stronger is different types of adversarial attacks with different parameters have been studied and analyzed.<BRK>Transfer learning, in which a network is trained on one task and re-purposed on another, is often used to produce neural network classifiers when data is scarce or full-scale training is too costly.  When the goal is to produce a model that is not only accurate but also adversarially robust, data scarcity and computational limitations become even more cumbersome.
they consider robust transfer learning, in which they transfer not only performance but also robustness from a stheirce model to a target domain.  they start by observing that robust networks contain robust feature extractors. By training classifiers on top of these feature extractors, they produce new models that inherit the robustness of their parent networks. they then consider the case of "fine tuning" a network by re-training end-to-end in the target domain. When using lifelong learning strategies, this process preserves the robustness of the stheirce network while achieving high accuracy. By using such strategies, it is possible to produce accurate and robust models with little data, and without the cost of adversarial training. Additionally, they can improve the generalization of adversarially trained models, while maintaining their robustness.
Reject. rating score: 3. rating score: 3. rating score: 6. The different models are combined to form an ensemble model. 6 layers seems very small given the size of the training set. I feel that the results are a bit disappointing given the scale of the experiments. What would be the performance of an ensemble of 10 models trained with the regular 20M parallel sentence pairs? It is true that  "every time we encounter a new domain, we have to retrain the model", but I think this is still a more viable approach than pretraining on the full 40B sentences. Although it is critical to work with cleaned data in NLP (especially in the context of generation), dataset cleaning is not really addressed in the paper. Overall, the experimental setup is impressive, but the improvements in terms of BLEU are relatively small, and the technical contributions seem quite thin to me for a ML conference.<BRK>This paper investigates the effectiveness of a massively large parallel corpus in NMT training, which consists of more than 40 billion En Zh parallel sentences. For preventing long training time, this paper proposes a practical data split and utilization method, which the authors call “dynamic data split.”The key idea of their method is to dynamically assign training instances to different model components and update different components according to the assigned instances. This paper reports the BLEU score of WMT17 Chinese English dataset for 32.3, which significantly outperformed the best score, and improved the performance of existing state of the art results. The main concern of this paper is the reproducibility of the experiments. Their main focus is to investigate the effectiveness of 40B massive parallel data.<BRK>This work conducts a large scale study on pretraining for neural machine translation. Overall, this work makes good contributions to the community, but the experiments need improvements. Pros: 	1.The data scale is huge, with 40 billion sentence pairs. Cons:	1.It is pity that the trained model is only evaluated on one test set and experiments are conducted on one language pair. Thus, it is not clear to me whether the improvement is general across datasets and language pairs. For example, I d like to see the results on WMT 2019 Chinese >English translation dataset.<BRK>In this paper, they investigate the problem of training neural machine translation (NMT) systems with a dataset of more than 40 billion bilingual sentence pairs, which is larger than the largest dataset to date by orders of magnitude. Unprecedented challenges emerge in this situation compared to previous NMT work, including severe noise in the data and prohibitively long training time. they propose practical solutions to handle these issues and demonstrate that large-scale pretraining significantly improves NMT performance. they are able to push the BLEU score of  WMT17  Chinese-English dataset to 32.3, with a significant performance boost of +3.2 over existing state-of-the-art results.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. Wouldn`t it better to actually learn everything end to end, which is already done in paper and evaluate? The reason that I am asking these, is results looks like too well and I suspect overfitting to a points, which are suitable for estimation (small) homography, not general purpose points.<BRK># ContributionsThe paper contributes a self supervised method of jointly learning 2D keypoint locations, descriptors, and scores given an input RGB image. I would give this a 5 if the website allowed me to.<BRK>I had some concerns about the clarity of the paper, and would be willing to raise my rating if addressed. My understanding is that the proposed work is a somewhat incremental improvement over Unsuperpoint.<BRK>Identifying salient points in images is a crucial component for visual odometry, Structure-from-Motion or SLAM algorithms. Recently, several learned keypoint methods have demonstrated compelling performance on challenging benchmarks.  Hotheyver, generating consistent and accurate training data for interest-point detection in natural images still remains challenging, especially for human annotators. they introduce IO-Net (i.e. InlierOutlierNet), a novel proxy task for the self-supervision of keypoint detection, description and matching. By making the sampling of inlier-outlier sets from point-pair correspondences fully differentiable within the keypoint learning framework, they show that are able to simultaneously self-supervise keypoint description and improve keypoint matching. Second, they introduce KeyPointNet, a keypoint-network architecture that is especially amenable to robust keypoint detection and description. they design the network to allow local keypoint aggregation to avoid artifacts due to spatial discretizations commonly used for this task, and they improve fine-grained keypoint descriptor performance by taking advantage of efficient sub-pixel convolutions to upsample the descriptor feature-maps to a higher operating resolution. Through extensive experiments and ablative analysis, they show that the proposed self-supervised keypoint learning method greatly improves the quality of feature matching and homography estimation on challenging benchmarks over the state-of-the-art.
Reject. rating score: 1. rating score: 1. rating score: 6. This paper proposes domain adverarial approaches modified to address covariate shift even in the case of shifting label distributions. The paper is interesting and the authorsare looking at an important problem, but the paper suffers several major misconceptionsand the exposition is full of errors. The paper proposes a method called “self training” to “estimate and align” the  target label distributionand a “prototype based method” for conditional alignment. They confuse the term “label shift” and the colloquial “shifting label distribution”. The proposed method leverages a “similarity classifier”The thing that the authors call a similarity based classifierisn’t well explained. I encourage the authors to give the paper a gut rewriteand do not believe that it can be published while resembling its current form. Note that you can have a shift in label distribution 	even under the covariate shift assumption.<BRK>In this work, the authors proposed a method to address the covariate shift and label shift problems simultaneously. 4).There is an issue in the label distribution by self training. The main concern of this work is its shift assumption. The novelty of the paper is limited.<BRK>The paper deals with covariate and label shift in commonevaluated on some standard benchmark data. I would like to see additional experiments on similar text data  > reuters  self training is a concept from semi supervised learning and not particular well supported  in the community   how do you make sure that the result remain valid  how do you make sure that the adaptation of the labels, the covariate shift and the classifier training  are not in facting cheating the result to an optimum within the optimized cost function?<BRK>Unsupervised knowledge transfer has a great potential to improve the generalizability of deep models to novel domains. Yet the current literature assumes that the label distribution is domain-invariant and only aligns the covariate or vice versa. In this paper, they explore the task of Generalized Domain Adaptation (GDA): How to transfer knowledge across different domains in the presence of both covariate and label shift? they propose a covariate and label distribution CO-ALignment (COAL) model to tackle this problem. their model leverages prototype-based conditional alignment and label distribution estimation to diminish the covariate and label shifts, respectively. they demonstrate experimentally that when both types of shift exist in the data, COAL leads to state-of-the-art performance on several cross-domain benchmarks.
Accept (Poster). rating score: 8. rating score: 8. rating score: 6. Review: The paper proposes a technique for anomaly detection. It presents a novel method that unifies the current classification based approaches to overcome generalization issues and outperforms the state of the art.<BRK>This paper proposes a novel approach to classification based anomaly detection for general data. The proposed method substantially outperforms SOT on all datasets. The stability is not improved, it is just that the performance trend is stable. The paper provides comparison to SOT methods for both Cifar10 and 4 non image datasets.<BRK>on image data [3, 4]. I appreciate the clarifications, additional experiments, and overall improvements made to the paper. ####################This paper proposes a deep method for anomaly detection (AD) that unifies recent deep one class classification [6] and transformation based classification [3, 4] approaches.<BRK>Anomaly detection, finding patterns that substantially deviate from those seen previously, is one of the fundamental problems of artificial intelligence. Recently, classification-based methods theyre shown to achieve superior results on this task. In this work, they present a unifying view and propose an open-set method, GOAD, to relax current generalization assumptions. Furthermore, they extend the applicability of transformation-based methods to non-image data using random affine transformations. their method is shown to obtain state-of-the-art accuracy and is applicable to broad data types. The strong performance of their method is extensively validated on multiple datasets from different domains.  
Reject. rating score: 3. rating score: 6. rating score: 6. This paper presents a new discriminator metric for adversarial attack s detection by deriving the different properties of l th neuron network layer on different adv/benign samples.<BRK>This paper proposes an adversarial detection method via Fourier coefficients. The proposed method seems promising, and empirical evaluations are reasonable. However, I find that the proposed MBF detection metric is much more complicated to calculate than any of its baselines, e.g., LID or K density.<BRK>This paper proposes an approach to adversarial detection. The approach first computes a representation of the activation layers using the Benford Fourier coefficients.<BRK>Adversarial examples have been theyll known as a serious threat to deep neural
networks (DNNs). To ensure successful and safe operations of DNNs on realworld tasks, 
it is urgent to equip DNNs with effective defense strategies. In this
work, they study the detection of adversarial examples, based on the assumption
that the output and internal responses of one DNN model for both adversarial and
benign examples follow the generalized Gaussian distribution (GGD), but with
different parameters (i.e., shape factor, mean, and variance). GGD is a general
distribution family to cover many popular distributions (e.g., Laplacian, Gaussian,
or uniform). It is more likely to approximate the intrinsic distributions of internal
responses than any specific distribution. Besides, since the shape factor is more
robust to different databases rather than the other two parameters, they propose
to construct discriminative features via the shape factor for adversarial detection,
employing the magnitude of Benford-Ftheirier coefficients (MBF), which can be
easily estimated using responses. Finally, a support vector machine is trained
as the adversarial detector through leveraging the MBF features. Through the
Kolmogorov-Smirnov (KS) test, they empirically verify that: 1) the posterior vectors 
of both adversarial and benign examples follow GGD; 2) the extracted MBF features 
of adversarial and benign examples follow different distributions. Extensive 
experiments in terms of image classification demonstrate that the proposed 
detector is much more effective and robust on detecting adversarial examples 
of different crafting methods and different stheirces, in contrast to state-of-the-art 
adversarial detection methods.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. Summary:This paper presents a family of architectural variants for the recurrent part of an RL controller. An increase in training speed and stability, apparently without any major caveats, would be of great interest to any practitioner of Deep Reinforcement Learning.<BRK>Overall it is an interesting paper in the sense that the introduced aggregation layer is so simple but effective in noisy RL. The explanation using arguments of gradient decay and SNR decay seems to be convincing.<BRK>UPDATE: The response helped address my questions. LSTMs are sensitive to noise: Is there an explanation for this observation? This paper studies reinforcement learning for settings where the observations contain noise and where observations have long range dependencies with the past. The experimental results look convincing to me.<BRK>In many partially observable scenarios, Reinforcement Learning (RL) agents must rely on long-term memory in order to learn an optimal policy. they demonstrate that using techniques from NLP and supervised learning fails at RL tasks due to stochasticity from the environment and from exploration. Utilizing their insights on the limitations of traditional memory methods in RL, they propose AMRL, a class of models that can learn better policies with greater sample efficiency and are resilient to noisy inputs. Specifically, their models use a standard memory module to summarize short-term context, and then aggregate all prior states from the standard model without respect to order. they show that this provides advantages both in terms of gradient decay and signal-to-noise ratio over time. Evaluating in Minecraft and maze environments that test long-term memory, they find that their model improves average return by 19% over a baseline that has the same number of parameters and by 9% over a stronger baseline that has far more parameters.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. rating score: 3. Later I noticed that you always index S with {1,\dots,|S|}, but using i \in S in combination with 1< j< |S| was a bit confusing. 3.The approximation theorem is useful and clean, and the empirical results are intriguing. Because the properties of UE and contractibility were not used, it may be more appropriate to use this space to introduce more of the literature on neural nets as feature embeddings stuff. This paper could be improved by generalizing to a few other choice models  in particular the CDM (https://arxiv.org/abs/1902.03266) may be a good candidate for your method.<BRK>The authors propose parametrizing PCMCs with neural networks to fix these issues. Pros:  Although I was previously unfamiliar with the PCMC model, using a neural network parametrization seems novel and well motivated.<BRK>The method relies on training a neural network. PCMC Net bakes the definition of PCMC into the neural net structure and therefore satisfies the theoretical properties of contractability and uniform expansion, which are desired properties of choice modelsMoreover, since choice probabilities are a function of choice candidates’ features (and features of an individual making the choice), this method allows for new (unseen) choice candidates at test time, which was not possible with previously proposed maximum likelihood (ML) inference. I recommend REJECTing this paper. This paper tackles the problem of efficient inference and test time generalization (to unseen choice alternatives) for choice modelling, and the proposed approach is interesting, seems to be theoretically sound, and outperforms evaluated baselines.<BRK>This paper presents an approach for choice modeling that leverages neural network features in a continuous time Markov Chain whose stationary distribution represents the choice distribution. The experimental section is too limited, with results on only one dataset and no comparison of different architectural choices for how to incorporate neural networks into PCMC models, or analysis pointing toward what the features are learning that allows them to improve over earlier approaches. The text was also confusing in a number of places (possibly due to my lack of knowledge in choice modeling), and there’s no discussion of related work incorporating neural networks into ranking based models.<BRK>Pairwise Choice Markov Chains (PCMC) have been recently introduced to overcome limitations of choice models based on traditional axioms unable to express empirical observations from modern behavior economics like context effects occurring when a choice bettheyen two options is altered by adding a third alternative. The inference approach that estimates the transition rates bettheyen each possible pair of alternatives via maximum likelihood suffers when the examples of each alternative are scarce and is inappropriate when new alternatives can be observed at test time. In this work, they propose an amortized inference approach for PCMC by embedding its definition into a neural network that represents transition rates as a function of the alternatives' and individual's features. they apply their construction to the complex case of airline itinerary booking where singletons are common (due to varying prices and individual-specific itineraries), and context effects and behaviors strongly dependent on market segments are observed. Experiments show their network significantly outperforming, in terms of prediction accuracy and logarithmic loss, feature engineered standard and latent class Multinomial Logit models as theyll as recent machine learning approaches.
Reject. rating score: 3. rating score: 3. rating score: 3. However, I believe it is not strong enough for me to recommend acceptance at this point. then, is HIB the proper baseline in figure 6 and 7?<BRK>Please, list the differences with this prior work and provide experimental comparison if possible. The method is an extension of Prototypical Networks (PN, [1]) with Gaussian embeddings. Advances in Neural Information Processing Systems.<BRK>The intersection sampling seems a bit interesting in the sense that the sampler focuses on the intersection of the input distribution and the class distribution and it is more sample efficient.<BRK>Supervised deep-embedding methods project inputs of a domain to a representational space in which same-class instances lie near one another and different-class instances lie far apart. they propose a probabilistic method that treats embeddings as random variables. Extending a state-of-the-art deterministic method, Prototypical Networks (Snell et al., 2017), their approach supposes the existence of a class prototype around which class instances are Gaussian distributed. The prototype posterior is a product distribution over labeled instances, and query instances are classified by marginalizing relative prototype proximity over embedding uncertainty. they describe an efficient sampler for approximate inference that allows us to train the model at roughly the same space and time cost as its deterministic sibling. Incorporating uncertainty improves performance on few-shot learning and gracefully handles label noise and out-of-distribution inputs. Compared to the state-of-the-art stochastic method, Hedged Instance Embeddings (Oh et al., 2019), they achieve superior large- and open-set classification accuracy. their method also aligns class-discriminating features with the axes of the embedding space, yielding an interpretable, disentangled representation.
Reject. rating score: 1. rating score: 1. rating score: 3. Therefore this paper has no valuable contribution in its current form and I vote for rejection.<BRK>The paper has no real experimental analysis, it doesn t seem to propose anything new or make any clear contribution, and overall, is quite unclear on what it is trying to do.<BRK>Summary:This paper introduces convolutional neural networks to encode video sequences. The used autoencoder and autoregressive techniques are promising for video encoding.<BRK>Deep neural networks have had unprecedented success in computer vision, natural language processing, and speech largely due to the ability to search for suitable task algorithms via differentiable programming. In this paper, they borrow ideas from Kolmogorov complexity theory and normalizing flows to explore the possibilities of finding arbitrary algorithms that represent data. In particular, algorithms which encode sequences of video image frames. Ultimately, they demonstrate neural video encoded using convolutional neural networks to transform autoregressive noise processes and show that this method has surprising cryptographic analogs for information security.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. This paper provides a novel off policy objective to solve imitation learning. The new algorithm is simple but efficient, and can handle off policy settings. However, the first half of the paper focuses on deriving an off policy objective for imitation learning.<BRK>This paper presents an algorithm for adversarial imitation that uses off policy data in a principled manner, unlike prior work. Can this be corrected? The paper then shows how the auxiliary variable (critic) added to the optimization is a value function that maximizes the corresponding induced reward in AIL methods, thus unifying the objectives for policy optimization and reward learning. And overall, is the optimization of the ValueDICE objective easy?.<BRK>The primary contribution of this paper is a principled algorithm for off policy imitation learning. For the method proposed in this paper, it would not be straightforward to do so.<BRK>When performing imitation learning from expert demonstrations, distribution matching is a popular approach, in which one alternates bettheyen estimating distribution ratios and then using these ratios as rewards in a standard reinforcement learning (RL) algorithm. Traditionally, estimation of the distribution ratio requires on-policy data, which has caused previous work to either be exorbitantly data- inefficient or alter the original objective in a manner that can drastically change its optimum. In this work, they show how the original distribution ratio estimation objective may be transformed in a principled manner to yield a completely off-policy objective. In addition to the data-efficiency that this provides, they are able to show that this objective also renders the use of a separate RL optimization unnecessary. Rather, an imitation policy may be learned directly from this objective without the use of explicit rewards. they call the resulting algorithm ValueDICE and evaluate it on a suite of popular imitation learning benchmarks, finding that it can achieve state-of-the-art sample efficiency and performance.
Reject. rating score: 3. rating score: 3. rating score: 3. The paper argues for the use of attractive networks (AN) for the tasks that involve learning from noisy data. Update after rebuttal: I agree with the other reviewers that the paper is not ready for publication. Ablation studies for the activation choice and the loss choice need to be analyzed in more detail.<BRK>This paper presents an attractor network (AN) approach for pattern interpretation and completion. It was not clear to me how the proposed approach uses recurrent networks. In supervised MNIST experiments, it was claimed that it achieved the state of the art results. Interpretation was emphasized in the paper, but it was not clear to me what induce interpretability in the model and how to interpret experimental results.<BRK>Similarly, the experiments on MNIST are not very informative and you might want to push them to appendices. While the authors claim superior performance against other recent attractor networks, no quantitative comparison was found in the paper. Overall, I am concerned that the experimental  part of  this paper does not warrant the conclusions of superiority. The  bipartite  structure is not clear.<BRK>In human perception and cognition, a fundamental operation that brains perform is interpretation: constructing coherent neural states from noisy, incomplete, and intrinsically ambiguous evidence. The problem of interpretation is theyll matched to an early and often overlooked architecture, the attractor network---a recurrent neural net that performs constraint satisfaction, imputation of missing features, and clean up of noisy data via energy minimization dynamics. they revisit attractor nets in light of modern deep learning methods and propose a convolutional bipartite architecture with a novel training loss, activation function, and connectivity constraints. they tackle larger problems than have been previously explored with attractor nets and demonstrate their potential for image completion and super-resolution. they argue that this architecture is better motivated than ever-deeper feedforward models and is a viable alternative to more costly sampling-based generative methods on a range of supervised and unsupervised tasks.
Reject. rating score: 3. rating score: 3. rating score: 8. This paper defines a set of learnable basis functions and a joint learning algorithm to estimate them. However, this premise is not accurate and many learning algorithms operate just in the actual time domain (even in speech). The paper advocates for a special group of strictly increasing transformations of the basis functions. Unfortunately the authors do not report runtime numbers in the experiments. First, the results in Tables 1 and 2 do not seem to be statistically significant and Table 3 does not have the intervals. Overall, the key message out of the experiments is that if you project the data to a parameterized basis set and constrain the parameters, you will get better generalization, which is not very novel.<BRK>On the contrary, the STFT has a lack of localisation (and thus the "precision" is not constant along frequencies). Overall, I think that re thinking the way a Wavelet Transform is designed, is an interesting direction of research, but I think some of the theoretical tools developed in this paper are not dedicated to achieve this purpose. In particular, the group/representation properties seem to not be used, and the authors could simply consider a specific subset of invertible mapping on $\mathbb{R}^2$ which would be applied on the mother wavelet and lead to a Wavelet Transform. In which case, this experiment would not be meaningful.<BRK>In the LGT, a more flexible transform that just scaling and shifting is applied to the shape of the mother wavelet, which is piece wise linearly stretched. *Supporting arguments* What I like about this paper is that the idea is a straightforward generalization of the wavelet transform through the lens of group theory. Experimentally, I think there was a nice selection of toy and harder examples. *Smaller questions/notes for the authors*  The explanation of what a group is is quite high level and I think if you did not know what it was beforehand, it would be hard to understand what one is. I really liked the connection drawn between the group transform and time warping. This is an aspect I personally had never considered.<BRK>they propose to undertake the problem of representation learning for time-series by considering a Group Transform approach. This framework allows us to, first, generalize classical time-frequency transformations such as the Wavelet Transform, and second, to enable the learnability of the representation. While the creation of the Wavelet Transform filter-bank relies on the sampling of the affine group in order to transform the mother filter, their approach allows for non-linear transformations of the mother filter by introducing the group of strictly increasing and continuous functions. The transformations induced by such a group enable us to span a larger class of signal representations. The sampling of this group can be optimized with respect to a specific loss and function and thus cast into a Deep Learning architecture. The experiments on diverse time-series datasets demonstrate the expressivity of this framework which competes with state-of-the-art performances.
Reject. rating score: 3. rating score: 3. rating score: 8. This work proposed a fully stochastic RL method and demonstrated significantly improved performance on multiple tasks. Pros:1.The presentation is very clear and easy to read. As discussed in the related work, this work can be seen as complementary to many related works such as Igl 18, but the novelty of the idea is rather limited. 2.The claims and the real benefit of the method may not be consistent. It seems that the benefit of the method is rather from such particular latent space design rather than the stochastic vs deterministic. But there are many works such as the KVAE that are stochastic and models sequential information.<BRK>This paper proposes an actor critic method that tries to aid learning good policies via learning a good representation of the state space (via a latent variable model). The key argument is that learning policies in the latent space is more efficient, as it is possible to learn good representations in the latent space. It is not clear from the text. Most importantly, there are quite a few claims made in the paper which are not properly justified, that makes the overall contribution and novelty of the paper rather limited. Why should this approach even be adapted or what is the significance of it? I think overall the contribution of the paper is rather limited.<BRK>The authors propose SLAC, an important extension of the recently introduced soft actor critic (SAC) algorthm, which operates on a learned latent state, rather than an observed one, and therefore aims to jointly learn to represent high dimensional inputs and execute continuous control based on this representation. Overall:A strong paper, that brings together and generalizes existing work, with strong experimentation and SOTA results. Limitations: While the most important ablation, the role of making the primary latent variable stochastic, is investigated, a deeper investigation of what makes the model more effective than existing techniques would be insightful, and further strengthen the paper.<BRK>Deep reinforcement learning (RL) algorithms can use high-capacity deep networks to learn directly from image observations. Hotheyver, these kinds of observation spaces present a number of  challenges in practice, since the policy must now solve two problems: a representation learning problem, and a task learning problem. In this paper, they aim to explicitly learn representations that can accelerate reinforcement learning from images. they propose the stochastic latent actor-critic (SLAC) algorithm: a sample-efficient and high-performing RL algorithm for learning policies for complex continuous control tasks directly from high-dimensional image inputs. SLAC learns a compact latent representation space using a stochastic sequential latent variable model, and then learns a critic model within this latent space. By learning a critic within a compact state space, SLAC can learn much more efficiently than standard RL methods. The proposed model improves performance substantially over alternative representations as theyll, such as variational autoencoders. In fact, their experimental evaluation demonstrates that the sample efficiency of their resulting method is comparable to that of model-based RL methods that directly use a similar type of model for control. Furthermore, their method outperforms both model-free and model-based alternatives in terms of final performance and sample efficiency, on a range of difficult image-based control tasks. their code and videos of their results are available at their theybsite.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. Contributions:This paper develops an algorithm for verifying the robustness of transformers with self attention layers when the inputs for one input word embedding are perturbed. Unlike previous work the present work can deal with cross nonlinearity and cross position dependency and the lower bounds derived in the paper are much tighter than the Interval Boundary Propagation (IBP) method which uses backward propagation. The core contribution is expounded by developing bounds for multiplication (xy) and division (x/y) and using this to compute tight bounds on self attention layer computations. In particular in Table 3, the second example shows that the method identifiews "a", "the" "our" etc, which at least do not appear to be most salient to this reviewer.<BRK>Summary:This paper builds upon the CROWN framework (Zhang et al 2018) to provide robustness verification for transformers. The  CROWN framework is based upon the idea of propagating linear bounds and has been applied to architectures like MLP, CNNs and RNNs. A major contribution of this paper is to use forward propagation of bounds in self attention layers along with the usual back propagation of bounds in all other layers. Although the fully forward propagation leads to loose bounds, the mixed approach (forward backward) presented in this work provides bounds which are as tight as fully backward method. If the math is correct, then this paper can be it. Questions:The set up in the paper assumes only one position in the input sequence is perturbed for simplicity. I have not verified the math to see if they indeed compute a lower bound.<BRK>MILP is a powerful verification technique that can deal with cross position dependency, which can be potentially applied to transformers. The paper solves these key challenges which are not traceable for previous methods. Although it is the first one who does robustness verification on transformers, the linear relaxation is similar to previous work just deal with different nonlinearity. The author needs to add experiments of multi layer transformers.<BRK>Robustness verification that aims to formally certify the prediction behavior of neural networks has become an important tool for understanding model behavior and obtaining safety guarantees. Hotheyver, previous methods can usually only handle neural networks with relatively simple architectures. In this paper, they consider the robustness verification problem for Transformers. Transformers have complex self-attention layers that pose many challenges for verification, including cross-nonlinearity and cross-position dependency, which have not been discussed in previous works. they resolve these challenges and develop the first robustness verification algorithm for Transformers. The certified robustness bounds computed by their method are significantly tighter than those by naive Interval Bound Propagation. These bounds also shed light on interpreting Transformers as they consistently reflect the importance of different words in sentiment analysis.
Reject. rating score: 1. rating score: 3. rating score: 3. The experiments are sufficient. Therefore, what is the difference between this paper and others?<BRK>Overall, it is unclear what is the major novel contribution in this paper.<BRK>The paper focuses on training image classification networks without batch normalization.<BRK>Many state of the art models rely on two architectural innovations; skip connections and batch normalization. Hotheyver batch normalization has a number of limitations. It breaks the independence bettheyen training examples within a batch, performs poorly when the batch size is too small, and significantly increases the cost of computing a parameter update in some models. This work identifies two practical benefits of batch normalization. First, it improves the final test accuracy. Second, it enables efficient training with larger batches and larger learning rates. Hotheyver they demonstrate that the increase in the largest stable learning rate does not explain why the final test accuracy is increased under a finite epoch budget. Furthermore, they show that the gap in test accuracy bettheyen residual networks with and without batch normalization can be dramatically reduced by improving the initialization scheme. they introduce “ZeroInit”, which trains a 1000 layer deep Wide-ResNet without normalization to 94.3% test accuracy on CIFAR-10 in 200 epochs at batch size 64. This initialization scheme outperforms batch normalization when the batch size is very small, and is competitive with batch normalization for batch sizes that are not too large. they also show that ZeroInit matches the validation accuracy of batch normalization when training ResNet-50-V2 on ImageNet at batch size 1024.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. This paper proposes a simple method for imitation learning that is competitive with GAIL. Because SQIL is off policy, it can utilize replay buffers to accelerate sample efficiency. The authors really play up the "surprising" connection of SQIL to a regularized behavioral cloning. But this isn t really that surprising in the general sense (although I applaud the authors for rigorously defining the connection).<BRK>This paper proposes an imitation learning approach via reinforcement learning. This encourages the agent to return to "known" states from out of distribution states and alleviates the problem of compounding errors. The authors derive an interpretation of their approach as regularized behavior cloning. I think this is a good paper which shows strong empirical results based on simple but effective idea. deserve to be discussed earlier in the paper (introduction?) and make it to the experimental results.<BRK>Summary The authors propose SQUIL, an off policy imitation learning (IL) algorithm which attempts to overcome the classic drift problems of behavioral cloning (BC). The idea is to reduce IL to a standard RL problem with a reward that incentivizes the agent to take expert actions in states observed in the demonstrations. Comments Overcoming the limitations of BC algorithms is a relevant problem and this work presents a simple and interesting solution. 1.The considered settings (e.g., MDPs, IL/IRL problems) are never formalized in the paper (which directly starts by describing the method in Section 2). 2.Though the approach solves one of the main problems of many BC/non IRL algorithms, it still has some of the other limitations. Have I misunderstood something?<BRK>Learning to imitate expert behavior from demonstrations can be challenging, especially in environments with high-dimensional, continuous observations and unknown dynamics. Supervised learning methods based on behavioral cloning (BC) suffer from distribution shift: because the agent greedily imitates demonstrated actions, it can drift away from demonstrated states due to error accumulation. Recent methods based on reinforcement learning (RL), such as inverse RL and generative adversarial imitation learning (GAIL), overcome this issue by training an RL agent to match the demonstrations over a long horizon. Since the true reward function for the task is unknown, these methods learn a reward function from the demonstrations, often using complex and brittle approximation techniques that involve adversarial training. they propose a simple alternative that still uses RL, but does not require learning a reward function. The key idea is to provide the agent with an incentive to match the demonstrations over a long horizon, by enctheiraging it to return to demonstrated states upon encountering new, out-of-distribution states. they accomplish this by giving the agent a constant reward of r=+1 for matching the demonstrated action in a demonstrated state, and a constant reward of r=0 for all other behavior. their method, which they call soft Q imitation learning (SQIL), can be implemented with a handful of minor modifications to any standard Q-learning or off-policy actor-critic algorithm. Theoretically, they show that SQIL can be interpreted as a regularized variant of BC that uses a sparsity prior to enctheirage long-horizon imitation. Empirically, they show that SQIL outperforms BC and achieves competitive results compared to GAIL, on a variety of image-based and low-dimensional tasks in Box2D, Atari, and MuJoCo. This paper is a proof of concept that illustrates how a simple imitation method based on RL with constant rewards can be as effective as more complex methods that use learned rewards.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. This paper provides an empirical comparison between several existing graph classification algorithms, aimed at providing a fair comparison among them, as well as proposing a simple baseline that does not take into account graph structural information. As a disclaimer, I would like to mention that I am more familiar with graph node classification methods, as opposed to whole graph classification (which is the focus of this paper), so I cannot assess very well the authors’ choice of which models to compare, and which datasets they were tested on. As mentioned above, I believe there are insufficient details about the baselines. For instance: a) The authors point out that the performance on the NCI1 dataset is different than all the others (it is the only dataset where the baseline is not the best). This could provide interesting insights for future work. If the authors clarify some of the issues above, especially about the results discussion part, I believe this paper may indeed be of value to the graph classification community.<BRK>This type of benchmarking paper is long overdue for graph classification with deep neural networks. The paper would ve been strongly if it had the following:1. Considered more structural features than simple node degree and clustering coefficient. Prior work [1] has looked at such features and answered questions like: How do structural features improve classification performance?And, which structural features are the most useful? 2.Investigated which graph neural network performs better for which graph structures (preferential attachment, small world, regular, etc) and for how much homophily. 3.Investigated the robustness of graph neural networks on classification as the structure of graphs become more random (e.g., by rewiring edges while maintaining degree distribution).<BRK>********** Post Rebuttal Update **********I appreciate that authors provided comments on all the raised concerns and updated the paper accordingly. That is not true. ********** Summary ********** The paper conducts an empirical study of 5 recently proposed graph neural networks (GNN). + it is shown that on some datasets, the results of a simple baseline   only operating on the node features, can achieve similar results to the elaborate GNNs. + some of the results in table 4 contradicts the corresponding papers which can be informative for the practitioners of the field. + the reproducibility and replicability problems, that is the motivation of this work, are important concerns of the field. It is also true that this is an important concern in machine learning research. Even if it’s assumed the 5 GNN methods , under this paper’s scrutiny, are representative of GNN classification, GNNs are used well beyond graph classification.<BRK>Experimental reproducibility and replicability are critical topics in machine learning. Authors have often raised concerns about their lack in scientific publications to improve the quality of the field. Recently, the graph representation learning field has attracted the attention of a wide research community, which resulted in a large stream of works.
As such, several Graph Neural Network models have been developed to effectively tackle graph classification. Hotheyver, experimental procedures often lack rigorousness and are hardly reproducible. Motivated by this, they provide an overview of common practices that should be avoided to fairly compare with the state of the art. To counter this troubling trend, they ran more than 47000 experiments in a controlled and uniform framework to re-evaluate five popular models across nine common benchmarks. Moreover, by comparing GNNs with structure-agnostic baselines they provide convincing evidence that, on some datasets, structural information has not been exploited yet. they believe that this work can contribute to the development of the graph learning field, by providing a much needed grounding for rigorous evaluations of graph classification models.
Reject. rating score: 1. rating score: 6. 1.Paper summaryThis paper develops a novel filtration for the analysis of based on theidea of *covers* of data sets. The paper lacks this structure at present and for a conference  submission, all main contributions should also be a part of the main  text. These stable paths are then shown to be useful for thecreation of  gentler  transitions for recommendation systems, as wellas the development of explainable supervised machine learning models. However, I recommend rejecting the paper in its current form due to thefollowing issues:A.<BRK>The paper presents an interesting filtration method to find staple maps, which proves effective in the recommendation system and explainable machine learning. The new framework is built upon a new concept termed cover filtration based on the generalized Steinhaus distance derived from Jaccard distance. This is not very clear at the beginning; instead, most of the introduction is related to TDA and mapper, which may confuse people not from this particular field. * The method introduced in this paper is intuitive and demonstrates with meaningful outcomes.<BRK>The contributions of this paper are two-fold. they define a new filtration called the cover filtration built from a single cover based on a generalized Steinhaus distance, which is a generalization of Jaccard distance. they then develop a language and theory for stable paths within this filtration, inspired by ideas of persistent homology. This framework can be used to develop several new learning representations in applications where an obvious metric may not be defined but a cover is readily available. they demonstrate the utility of their framework as applied to recommendation systems and explainable machine learning.

they demonstrate a new perspective for modeling recommendation system data sets that does not require manufacturing a bespoke metric. As a direct application, they find that the stable paths identified by their framework in a movies data set represent a sequence of movies constituting a gentle transition and ordering from one genre to another.

For explainable machine learning, they apply the Mapper for model induction, providing explanations in the form of paths bettheyen subpopulations. their framework provides an alternative way of building a filtration from a single mapper that is then used to explore stable paths. As a direct illustration, they build a mapper from a supervised machine learning model trained on the FashionMNIST data set. they show that the stable paths in the cover filtration provide improved explanations of relationships bettheyen subpopulations of images.

Accept (Talk). rating score: 8. rating score: 8. rating score: 6. This paper proposes Meta Q Learning (MQL), an algorithm for efficient off policy meta learning. Please also augment captions to make figures as stand alone as possible. One important but somewhat orthogonal contribution of the paper is to highlight the importance of context in meta learning and fast adaptation. Concretely, the authors show that a simple actor critic algorithm (TD3), whose policy and value are conditioned on a context variable derived from a recurrent network performs surprisingly well in comparison to SoTA meta learning algorithms like PEARL. I have mixed opinions on this paper. This either highlights the strength of multi task learning, or the inadequacies of current meta RL benchmarks: either of which will be of interest to the community.<BRK>Summary The authors propose meta Q learning, an algorithm for off policy meta RL. The proposed approach is evaluated on standard Mujoco benchmarks and compared to other relevant meta rl algorithms. This work proposes interesting ideas and overall it constitutes an nice contribution. In particular, I found interesting (and at the same time worrying) that a simple q learning algorithm with hidden contexts compares favorably to state of art meta rl approaches in standard benchmarks. Could they be done at the same time by setting to 1 the importance weights of the new trajectories and sampling from the whole experience (new and old)? 3.Note that the ESS estimator (13) diverges to infinity when all weights are close to zero. I think an experiment of this kind would be valuable to improve the paper.<BRK>The authors investigate meta learning in reinforcement learning with respect to sample efficiency and the necessity of meta learning an adaptation scheme. Based on their findings, they propose a new algorithm  MQL  (Meta Q Learning) that is off policy and has a fixed adaptation scheme but is still competitive on meta RL benchmarks (a distribution of environments that differ slightly in their reward functions). **The authors make the following contributions:**1. The new method leverages data during meta testing that was collected during meta training using importance weights for increased sample efficiency**Overall, we believe the contributions are significant and sufficiently empirically justified. This is a significant result and supports the new method developed in this paper. The reuse of experience from meta training during meta testing by employing importance weights is also an interesting contribution. In contrast, we are not satisfied with the presentation of their new approach as a meta learning approach.<BRK>This paper introduces Meta-Q-Learning (MQL), a new off-policy algorithm for meta-Reinforcement Learning (meta-RL). MQL builds upon three simple ideas. First, they show that Q-learning is competitive with state-of-the-art meta-RL algorithms if given access to a context variable that is a representation of the past trajectory. Second, a multi-task objective to maximize the average reward across the training tasks is an effective method to meta-train RL policies. Third, past data from the meta-training replay buffer can be recycled to adapt the policy on a new task using off-policy updates. MQL draws upon ideas in propensity estimation to do so and thereby amplifies the amount of available data for adaptation. Experiments on standard continuous-control benchmarks suggest that MQL compares favorably with the state of the art in meta-RL.
Reject. rating score: 3. rating score: 3. rating score: 3. The paper is relatively well written, based on intuitive ideas and including clear figures. I think there is not a theory that justifies this approach. It is not clear how the theoretical results (Theorem 1) helps to support the proposed method. I cannot agree with the author’s claim that “KANets significantly outperform the previous state of the art compact models”. o	The experimental results about memory and time used by regular attention operator compared with KAOs is obvious and not relevant because their theoretical complexities are well known. The only advantage seems to be a slight reduction in number of operations which is marginal.<BRK>Recommendation  Attention has been shown to provide improvements over convolutions but can indeed incur significant memory costs. The proposed method is sound and simple and may achieve significant speedups over regular self attention. Can the authors report results on such a baseline? The rest of the architecture isn t an external factor to ignore. These methods should also be cited as related work and potentially use as baselines (Squeeze and Excitation for example). The probabilistic analysis does not convincingly motivate the Kronecker Attention Operator. The theoretical motivation for the method is not convincing.<BRK>The main theorem provides a way to efficiently construct feature maps satisfying this covariance structure. I find this paper difficult to follow. I think I got it in the end, but I believe the analysis could be better motivated/presented and the consequences of Theorem 1 on design choices or implementation could be clarified. Similarly, the description of the architecture in Sections 3.1 and 3.2 is sometimes given without much motivation, letting the reader wonder there are particular reasons for the choices made on the architecture. I am not very confident in my assessment but I have the feeling that the paper could be greatly improved by some restructuring and giving more insights and motivations. after Eq.(3) I don t think the notation [cross in diamond] has been introduced.<BRK>Attention operators have been applied on both 1-D data like texts and higher-order data such as images and videos. Use of attention operators on high-order data requires flattening of the spatial or spatial-temporal dimensions into a vector, which is assumed to follow a multivariate normal distribution. This not only incurs excessive requirements on computational restheirces, but also fails to preserve structures in data. In this work, they propose to avoid flattening by developing Kronecker attention operators (KAOs) that operate on high-order tensor data directly. KAOs lead to dramatic reductions in computational restheirces. Moreover, they analyze KAOs theoretically from a probabilistic perspective and point out that KAOs assume the data follow matrix-variate normal distributions. Experimental results show that KAOs reduce the amount of required computational restheirces by a factor of hundreds, with larger factors for higher-dimensional and higher-order data. Results also show that networks with KAOs outperform models without attention, while achieving competitive performance as those with original attention operators.
Reject. rating score: 3. rating score: 3. rating score: 3. rating score: 6. Specifically, they focus on the noisy robust (NR) variant of the action robust framework proposed in [1]. The authors propose to combine SGLD with DDPG, to maintain a distribution over deterministic policies (essentially a mixed strategy)   hence overcoming these issues. Review:Overall this seems like the first approach of using Bayesian learning in order to reach mixed Nash equilibrium in RL. This is super important, since many problems can be formulated as zero sum games for which the solution is not necessarily a pure strategy. However, in my opinion, it is not there yet and is thus not ready for ICLR. I feel that such work needs to be more convincing.<BRK>The authors report practical improvements compared to pure strategies computed as proposed by Tessler et al.DecisionThe idea of using randomized strategies for two player reinforcement learning is interesting and natural. However, as the authors note, mixed strategies are classical in game theory. In my opinion, this paper should only be accepted if it provides very convincing numerical experiments, which I am not qualified to assess. I happy to increase my score if the experimental results are deemed strong by the reviewers with more expertise in practical reinforcement learning. Does this refer to the algorithm that is used in the numerical experiments?<BRK>This paper approaches two player zero sum Markov game by using mixed Nash equilibrium by sampling from randomized policies. The authors propose that the optimization is done using Stochastic Gradient Langevin Dynamics iterations. In my opinion, the use of SGLD to this problem is potentially useful   as partially proved by this work. However, this application is obvious and does not have enough novelty merits to be accepted to this ICLR. The experiment make senses but is not rigorous enough to assure the practitioners on the improvement over baseline.<BRK>THE NR MDPs can be thought of as an agent learning in an adversarial environment. Most of the theory comes from the "Finding mixed nash equilibria of generative adversarial networks." But, this seems to be the first time SGLD has been applied to such a problem. The authors compare on the MuJoCo benchmark with common but not identical instances to "Action robust reinforcement learning and applications in continuous control", which also provides the baseline algorithm used for comparison. It is infact mentioned as a failure case.<BRK>they re-think the Two-Player Reinforcement Learning (RL) as an instance of a distribution sampling problem in infinite dimensions. Using the potheyrful Stochastic Gradient Langevin Dynamics, they propose a new two-player RL algorithm, which is a sampling variant of the two-player policy gradient method. their new algorithm consistently outperforms existing baselines, in terms of generalization across differing training and testing conditions, on several MuJoCo environments.
Reject. rating score: 1. rating score: 3. rating score: 3. The paper tackles the problem of online learning for GMMs, in the context of high dimensional data. Specifically, the authors limit the scope to SGD like approaches and EM like optimization. I feel that this work is largely incremental, but more importantly indicating the authors  lack of understanding of the very long history of (online) EM. (In fact, their max approx is of this type.) Why not use hierarchical priors instead? In Sec.3.4, additional smoothing is accomplished using a subspace approach, which requires QR decomposition. I am mostly concerned about the evaluation.<BRK>The paper describes in detail a proper implementation of SGD for learning GMMs. The paper is overall clear and well written. The main contributions are an effective learning of GMMs from random initialization that is competitive (in terms of final loss) to EM training with K means initialization. The authors present experiments to show the effectiveness of it to avoid single component solutions that may arise from random initialization, which is an interesting point of the paper. So the practical advantage of the method (which probably exists) would benefit from more comparisons. While the authors discuss some way to train low rank models, the only successful results seem to be with diagonal covariance matrices, which seems much easier.<BRK>The paper proposes a new method based on Stochastic Gradient Descent to train Gaussian Mixture Models, and studies this method especially in the context of high dimension. what fraction of the data was used? (which covariance matrix?there are K such matrices). Section 3.3: The first sentence is wrong   EM can also suffer from the problem of local minima. is not a local minimum. The change of regularization during the training process seems like a heuristic that worked well for the authors, but it is thus unclear what optimization problem is the optimization solving. I didn t see a definition I don t understand the local principal directions covariance structure. The authors write  a diagonal covariance matrix of S < D entries). But what about the other D S coordinates?<BRK>they present an approach for efficiently training Gaussian Mixture Models (GMMs) with Stochastic Gradient Descent (SGD) on large amounts of high-dimensional data (e.g., images). In such a scenario, SGD is strongly superior in terms of execution time and memory usage, although it is conceptually more complex than the traditional Expectation-Maximization (EM) algorithm.
For enabling SGD training, they propose three novel ideas:
First, they show that minimizing an upper bound to the GMM log likelihood instead of the full one is feasible and numerically much more stable way in high-dimensional spaces.
Secondly, they propose a new regularizer that prevents SGD from converging to pathological local minima.
And lastly, they present a simple method for enforcing the constraints inherent to GMM training when using SGD.
they also propose an SGD-compatible simplification to the full GMM model based on local principal directions, which avoids excessive memory use in high-dimensional spaces due to quadratic growth of covariance matrices.
Experiments on several standard image datasets show the validity of their approach, and they provide a publicly available TensorFlow implementation.
Reject. rating score: 3. rating score: 3. rating score: 3. In the second approach, the authors propose to dynamically switch between mixed operations and half precision operations during training. The authors claim that this second approach can match SOTA results while using half precision arithmetic for more than 94% of training. However, I have a number of concerns about the paper, which explains my score. While the authors have included a hyperparameter sensititivity analysis, I find the experiment to be unconvincing. While I appreciate the added experiment and realize that 10 days is too short a time to put in a proper sensitivity analysis, based on the current draft of the paper, I cannot recommend accepting this paper.<BRK>The author(s) propose to accelerate the training of deep neural networks while also maintain the performance of the trained model by switching between fully half precision computation and mixed precision computation. The proposed method simply switches between two existing training strategies, i.e., the mixed precision training and half precision training. K can be tuned so that the proportion of BF16FMA is close to those in Table 1. 5.From Table 1, there still exists a large performance gap in terms of accuracy (1.56% for ResNet 50) between the model trained by the proposed method and the model trained by state of the art MP. 6.The organization of this paper can be improved.<BRK>Observed that relying purely on half precision arithmetic results in lower accuracy, the authors developed a method to dynamically switch between mixed precision arithmetic (MP) and half precision arithmetic (BF16 FMA). Empirical results show that the dynamic approach can achieve similar accuracy as MP and FP32 algorithms. Specifically, what is the key factor that influences the sensitivity of a neural network towards precision? The dynamic approach could mean a lot of things while training DNNs and one cannot tell what the paper is about simply relying on the title.<BRK>Mixed-precision arithmetic combining both single- and half-precision operands in the same operation have been successfully applied to train deep neural networks. Despite the advantages of mixed-precision arithmetic in terms of reducing the need for key restheirces like memory bandwidth or register file size, it has a limited capacity for diminishing computing costs and requires 32 bits to represent its output operands. This paper proposes two approaches to replace mixed-precision for half-precision arithmetic during a large portion of the training. The first approach achieves accuracy ratios slightly slotheyr than the state-of-the-art by using half-precision arithmetic during more than 99% of training. The second approach reaches the same accuracy as the state-of-the-art by dynamically switching bettheyen half- and mixed-precision arithmetic during training. It uses half-precision during more than 94% of the training process. This paper is the first in demonstrating that half-precision can be used for a very large portion of DNNs training and still reach state-of-the-art accuracy.
Reject. rating score: 3. rating score: 3. rating score: 6. Thank the authors for the response. The major novelty of this paper is encoding the objective as a logical expression and the experiment part is limited. I am happy to change my score based on the reviews from other reviewers.<BRK>As a result, the experimental evaluation should be much more thorough. They argue, via simulation and toy examples, that the proposed model is able to generalize to logical formulas of the multidimensional rewards that has not observed during training. As a result, I do not support acceptance. The authors should more clearly explain this.<BRK>I wonder whether the model is still generalizable for larger problems. Even in the case of 20x20 grids, we can notice the gap between the baseline and the proposed method. More specifically, the problems of the paper are,( ) The scalability issue. Is it possible that we can relax this assumption?<BRK>In the multi-objective reinforcement learning (MORL) paradigm, the relative importance of each environment objective is often unknown prior to training, so agents must learn to specialize their behavior to optimize different combinations of environment objectives that are specified post-training. These are typically linear combinations, so the agent is effectively parameterized by a theyight vector that describes how to balance competing environment objectives. Hotheyver, many real world behaviors require non-linear combinations of objectives. Additionally, the conversion bettheyen desired behavior and theyightings is often unclear.
In this work, they explore the use of a language based on propositional logic with quantitative semantics--in place of theyight vectors--for specifying non-linear behaviors in an interpretable way. they use a recurrent encoder to encode logical combinations of objectives, and train a MORL agent to generalize over these encodings. they test their agent in several grid worlds with various objectives and show that their agent can generalize to many never-before-seen specifications with performance comparable to single policy baseline agents. they also demonstrate their agent's ability to generate meaningful policies when presented with novel specifications and quickly specialize to novel specifications.
Accept (Poster). rating score: 6. rating score: 6. rating score: 1. I think the work does a decent job at looking at different number of components in the ensemble and analyzing the proposed method, but maybe not enough comparing and exploring other mechanism proposed as a defense for adversarial attacks. It proposes to use ensembles of full precision and low precision models in order to boost up robustness to adversarial attacks. I think the premise of the paper is quite clear, and the results seem to be intuitive.<BRK>The authors propose an ensemble of low precision networks as a solution to providing a neural network with solid adversarial robustness whilst also providing good accuracy. I found the paper easy to read with a high quality introduction and background, the results are very convincing and the idea is simple but intriguing. I could not work out from the paper whether the adversarial attacks on the low precision networks were performed at full precision.<BRK>This paper suggests using ensemble of both full precision and low bits precision models to defense adversarial examples. So I don t think the methodology contribution of this paper is enough for publication. Considering the weakness of the paper both in methodology development and empirical justification, this work does not merit publication from my point of view.<BRK>Ensuring robustness of Deep Neural Networks (DNNs) is crucial to their adoption in safety-critical applications such as self-driving cars, drones, and healthcare. Notably, DNNs are vulnerable to adversarial attacks in which small input perturbations can produce catastrophic misclassifications. In this work, they propose EMPIR, ensembles of quantized DNN models with different numerical precisions, as a new approach to increase robustness against adversarial attacks. EMPIR is based on the observation that quantized neural networks often demonstrate much higher robustness to adversarial attacks than full precision networks, but at the cost of a substantial loss in accuracy on the original (unperturbed) inputs. EMPIR overcomes this limitation to achieve the ``best of both worlds", i.e., the higher unperturbed accuracies of the full precision models combined with the higher robustness of the low precision models, by composing them in an ensemble. Further, as low precision DNN models have significantly lotheyr computational and storage requirements than full precision models, EMPIR models only incur modest compute and memory overheads compared to a single full-precision model (<25% in their evaluations). they evaluate EMPIR across a suite of DNNs for 3 different image recognition tasks (MNIST, CIFAR-10 and ImageNet) and under 4 different adversarial attacks. their results indicate that EMPIR boosts the average adversarial accuracies by 42.6%, 15.2% and 10.5% for the DNN models trained on the MNIST, CIFAR-10 and ImageNet datasets respectively, when compared to single full-precision models, without sacrificing accuracy on the unperturbed inputs.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. This paper presents local ensembles, a method for detecting underdetermination when extrapolating to test points. This is central to the work of the paper and a more full treatment of the proof here could help illuminate some intuition about the connection to perturbations and variance of predictions. In the final experiment the authors demonstrate the use of local ensembles in active learning.<BRK>The paper focusses on underdetermination as being key to extrapolation. What are the implications of the method in the case of finetuning models especially is the training data available for fine tuning is low. A comparison with larger test set and models trained on deeper architecture such as ResNet and the like will be interesting to see.<BRK>The idea of estimating "ensemble subspace" is interesting and computationally effective. Please include much of the discussion in the paper or appendix. The reason that these methods are not compared to is insufficient.<BRK>they present local ensembles, a method for detecting extrapolation at test time in a pre-trained model. they focus on underdetermination as a key component of extrapolation: they aim to detect when many possible predictions are consistent with the training data and model class. their method uses local second-order information to approximate the variance of predictions across an ensemble of models from the same class. they compute this approximation by estimating the norm of the component of a test point's gradient that aligns with the low-curvature directions of the Hessian, and provide a tractable method for estimating this quantity. Experimentally, they show that their method is capable of detecting when a pre-trained model is extrapolating on test data, with applications to out-of-distribution detection, detecting spurious correlates, and active learning.
Reject. rating score: 3. rating score: 3. rating score: 3. The authors consider the relation between Frechet distance of training and test distribution and the generalization gap. Overall I feel that the contribution may be quite weak, and I lean on the negative side. 2) In the proof of Theorem 1, it is quite hard to follow with the current notation, for the integral in (i), (ii) as well as in the proof using the intermediate value theorem, which variables are used? What we have in Theorem 1 is the lower bound only?<BRK>The authors try to capture it with Frechet distance, but I struggle to understand what is new in this work. First, there are a lot of assumptions in the computation of the Frechet distance:   1. Most importantly, they do not relate the Frechet distance to the lower bound in Theorem 1. There is no estimation on how the learned changes across distributions in the gradient norm term. The authors should make the connection of the bound and its computation clear, with proper connections to the experiments.<BRK>This paper considers the problem of how the mismatch between distributions of training data and test data would affect the generalization gap in machine learning tasks. The paper took a step in relating the change in the performance of the learned function to the Frechet distance (FD), also known as 2 Wasserstein distance, between the input and output distributions and proved that the former is lower bounded by the latter multiplied by a term related to the sensitivity of learning algorithm to distribution shift. I also find the statement about the generalization gap a bit misleading. Generally, the generalization gap refers to the gap between the expected error and the empirical error. But the experiments are mostly presenting the performance on the test data. Overall, I don t think the paper meets the standard for publication at ICLR.<BRK>Learning theory tells us that more data is better when minimizing the generalization error of identically distributed training and test sets. Hotheyver, when training and test distribution differ, this distribution shift can have a significant effect. With a novel perspective on function transfer learning, they are able to lotheyr bound the change of performance when transferring from training to test set with the Wasserstein distance bettheyen the embedded training and test set distribution. they find that there is a trade-off affecting performance bettheyen how invariant a function is to changes in training and test distribution and how large this shift in distribution is. Empirically across several data domains, they substantiate this viewpoint by showing that test performance correlates strongly with the distance in data distributions bettheyen training and test set. Complementary to the popular belief that more data is always better, their results highlight the utility of also choosing a training data distribution that is close to the test data distribution when the learned function is not invariant to such changes.
Accept (Talk). rating score: 8. rating score: 8. rating score: 6. This paper attempts to learn discrete speech units in a hierarchical (phone and word) fashion by incorporating multiple vector quantization layers into the audio encoder branch of a model that visually grounds speech segments with accompanying images. The model has been tested and compared against two algorithms and implementations that set the SOTA on the Zero Speech 2019 challenge (further improving one of them in the process, it seems), and outperforms these significantly using the ABX metric, so the proposed method seems to perform well (the model is using additional supervision, though). The paper is a pleasure to read and provides a rich set of results and analyses. One is adding the discretization to a pre trained model, the other is training from the start?<BRK>Strengths:The paper is extremely well written with a clear motivation (Section 1). The approach is novel. But I think the paper s biggest strength is in its very thorough experimental investigation. Their approach is compared to other very recent speech discretization methods on the same data using the same (ABX) evaluation metric. But the work goes further in that it systematically attempts to actually understand what types of structures are captured in the intermediate discrete layers, and it is able to answer this question convincingly. Finally, very good results on standard benchmarks are achieved.<BRK>Pretty interesting paper attempting to learn discrete linguistic units via vector quantization of visually grounded, speech related features. I think this is a worthwhile contribution. Reading the work, this seems to be the triplet loss described in Section 3.5. Is that the novel objective? I fear that essential and interesting point is somewhat diluted in the detailed exposition of results.<BRK>In this paper, they present a method for learning discrete linguistic units by incorporating vector quantization layers into neural models of visually grounded speech. they show that their method is capable of capturing both word-level and sub-word units, depending on how it is configured. What differentiates this paper from prior work on speech unit learning is the choice of training objective. Rather than using a reconstruction-based loss, they use a discriminative, multimodal grounding objective which forces the learned units to be useful for semantic image retrieval. they evaluate the sub-word units on the ZeroSpeech 2019 challenge, achieving a 27.3% reduction in ABX error rate over the top-performing submission, while keeping the bitrate approximately the same. they also present experiments demonstrating the noise robustness of these units. Finally, they show that a model with multiple quantizers can simultaneously learn phone-like detectors at a lotheyr layer and word-like detectors at a higher layer. they show that these detectors are highly accurate, discovering 279 words with an F1 score of greater than 0.5.
Reject. rating score: 1. rating score: 6. rating score: 6. Based on this result, the authors claim that projecting the latent samples onto the surface of a hypersphere would make GAN less sensitive to the choice of the prior distribution. This paper is hard to follow and requires substantial improvements in terms of writing, owing to several grammatical and semantic issues. I can therefore not recommend acceptance. An important claim in this paper is that the proposed approach “alleviates variational inference in VAE”. Consider revising the paper to improve its writing.<BRK>This paper proposed an interesting idea that by regularizing the structure of the latent space into a sphere, we can free VAE from variantional inference framework. 2. the image quality of VAE (CelebA) is not that bad in other VAE papers, maybe tuning the \beta VAE can also achieve the same quantitative and qualitative results. 3.Can you visualize the latent space (z) for the CelebA dataset, also comparing with the results from VAE?<BRK>This paper proposes a novel autoencoder algorithm, named Spherical AutoEncoder (SAE). Compared with using von Mises Fisher distribution in the vanilla VAE, the advantage of the proposed method is not clear. To leverage the properties, proposed algorithm centerizes latent variables and projects them onto unit sphere. In this paper, the authors argue that the sphere structure has good properties in high dimensional.<BRK>Variational inference is a fundamental problem in Variational AutoEncoder (VAE). The optimization with lotheyr bound of marginal log-likelihood results in the distribution of latent variables approximate to a given prior probability, which is the dilemma of employing VAE to solve real-world problems. By virtue of high-dimensional geometry, they propose a very simple algorithm completely different from existing ones to alleviate the variational inference in VAE. they analyze the unique characteristics of random variables on spheres in high dimensions and prove that Wasserstein distance bettheyen two arbitrary data sets randomly drawn from a sphere are nearly identical when the dimension is sufficiently large. Based on their theory, a novel algorithm for distribution-robust sampling is devised. Moreover, they reform the latent space of VAE by constraining latent variables on the sphere, thus freeing VAE from the approximate optimization of posterior probability via variational inference. The new algorithm is named Spherical AutoEncoder (SAE). Extensive experiments by sampling and inference tasks validate their theoretical analysis and the superiority of SAE.
Reject. rating score: 3. rating score: 3. rating score: 6. This paper takes the idea of Population Based Augmentation (PBA) and extends it to knowledge distillation (KD). Overall, there aren’t that many experiments. As for Figure 3, I don’t know if there is anything intuitive we can glean from this. This would be interesting to add.<BRK>However, I found a few problems with the paper:In the author’s methods, there seem to be multiple steps:1. 2.Train the student with PBA, using a subset of the data and then using the teacher to learn the augmentation policy (stage beta). 3.Additionally in Section 5.2 (experiments), the authors propose the combined inter/intra distillation loss, which is named ‘II KD’. Same for AlexNet on CIFAR 100. The novelty in the paper seems to be:a) Applying PBA to a Distillation setting.<BRK>More concretely, the paper proposes to use an evolutionary algorithm for data augmentation (PBA, already published) to train the teacher and student networks. The key aspect of the proposed method is that the teacher and the student use different augmentation schedules. The augmentation schedules improve results on both the teacher and student networks. The datasets used in the experiments are CIFAR 10 and CIFAR 100. The results show that it’s better to use a specific data augmentation schedule for the student network. However, the improvements of their method over the baseline (II KD) seem marginal there. A similar table to Table 3 should be included in this section. Score: Borderline accept, but I will increase the score if the authors address my concerns and provide better evidence that the hypothesis is confirmed.<BRK>Knowledge Distillation (KD) is a common method for transferring the ``knowledge'' learned by one machine learning model (the teacher) into another model (the student), where typically, the teacher has a greater capacity (e.g., more parameters or higher bit-widths). To their knowledge, existing methods overlook the fact that although the student absorbs extra knowledge from the teacher, both models share the same input data -- and this data is the only medium by which the teacher's knowledge can be demonstrated. Due to the difference in model capacities, the student may not benefit fully from the same data points on which the teacher is trained. On the other hand, a human teacher may demonstrate a piece of knowledge with individualized examples adapted to a particular student, for instance, in terms of her cultural background and interests. Inspired by this behavior, they design data augmentation agents with distinct roles to facilitate knowledge distillation. their data augmentation agents generate distinct training data for the teacher and student, respectively. they focus specifically on KD when the teacher network has greater precision (bit-width) than the student network.

they find empirically that specially tailored data points enable the teacher's knowledge to be demonstrated more effectively to the student. they compare their approach with existing KD methods on training popular neural architectures and demonstrate that role-wise data augmentation improves the effectiveness of KD over strong prior approaches. The code for reproducing their results will be made publicly available.
Accept (Talk). rating score: 8. rating score: 8. rating score: 8. Notes:While this is a good paper, I think the impact of the paper could be magnified if the authors were a bit more ambitious with their empirical evaluations. This paper extends the standard variance analysis to consider the hypernet case by investigating what the choice of hypernet initialization should be if one still wishes to maintain the well behaved activations/gradients in the main model. The axes should be labeled.<BRK>The paper presents an extension of Glorot/He weight variance initiazation formula for the hypernetworks. They show that proposed method allows  hypernet training when the standard ways don`t. Questions:    In standard NNs, initialization issues are mostly solved after introduction of BatchNorm. Wouldn`t it be the case for hypernetwork as well: to just add BN layers between main net layers?<BRK>Their method may be able to make Hypernetworks accessible to many more researchers and practitioners, the way classifical init techniques have made neural net training more accessible. There are a few things that could improve the paper (and get an improvement score from me). (For the record, if this was a 1 10 scale, I would have liked my score to be a 7).<BRK>Hypernetworks are meta neural networks that generate theyights for a main neural network in an end-to-end differentiable manner. Despite extensive applications ranging from multi-task learning to Bayesian deep learning, the problem of optimizing hypernetworks has not been studied to date. they observe that classical theyight initialization methods like Glorot & Bengio (2010) and He et al. (2015), when applied directly on a hypernet, fail to produce theyights for the mainnet in the correct scale. they develop principled techniques for theyight initialization in hypernets, and show that they lead to more stable mainnet theyights, lotheyr training loss, and faster convergence.
Reject. rating score: 3. rating score: 3. rating score: 6. The submission proposes an autoencoder architecture which combines two recent GAN based architectural innovations, namely the progressive growing of the decoder architecture (as well as the encoder architecture in this case) and the use of the encoded representation to modulate the decoder via a feature wise transformation mechanism. A learned representation for artistic style. I also have concerns with its characterization of the literature. Large scale adversarial representation learning. What do the authors mean by “disentangled representation”? Section 3.2 uses some notation for the encoder without introducing it first. The submission presents model samples, but as far as I can tell the procedure for sampling is not provided. How are samples obtained from the trained model?<BRK>Deep Automodulators introduces a generative autoencoder architecture that replaces the canonical encoder decoder autoencoder architecture with one inspired by StyleGAN. The paper trains this architecture with the loss framework of the Adversarial Generator–Encoder (AGE) and utilizes the progressive growing trick originally introduced in Progressive GAN which is also adapted by the Pioneer models, recent followups to AGE. The use of AdaIN conditioning across multiple layers and multiple scales (like StyleGAN) and the ability to directly compute latent codes via the encoder allows the authors introduce a disentanglement objective L_j and also an invariance objective L_inv to help encourage these properties in the models via consistency objectives The paper shows results demonstrating StyleGAN style coarse/fine visual transfer on two high quality face datasets (importantly this is demonstrated on real inputs rather than samples as in StyleGAN) as well as respectable sample quality on LSUN Bedrooms and the LSUN Cars dataset. Overall, I think the paper is promising and shows a nice combination of efficient latent inference and controllable generation but the authors do not include ablations to validate some of their core contributions such as the L_j objective. 1) The StyleGAN inspired architecture 2) the disentangling objective L_j and 3) using the loss function dρ of Barron 2019. Additional Comments:Each subsection of 3 could be improved by providing a brief introduction to the motivation for and aim of each contribution before launching directly into how it is implemented / achieved.<BRK>The paper makes the following contribution:  using the AdaIn architecture proposed by Karras et al., 2019 with the autoencoding architecture of AGE/PIONEER;  a cyclic loss to enforce disentangling between different layers;  a method to enforce invariances at specific layers. The adaptation of the AdaIn architecture in an autoencoding fashion (a la AGE/PIONEER) is sensible and well motivated, combining state of the art generator while allowing inference in a compact setting (i.e.not requiring an additional discriminator). The other contribution are harder to read and the writing should be improved. I will also assume that d_cos is the cosine loss as defined by the PIONEER paper. This should be mentioned as well. While the authors introduce F as a "known invariance", it is unclear what role it plays in the cost function. Is F an invariant on which we measure this reconstruction loss d? It is unclear what was the contribution of the layer specific loss metric to allow that feature transfer. It seems from Figure 5 the invariance objective has been roughly satisfied but at the cost of a significant drop in image quality. The two other contributions are unclear, both in their explanation and in what they contribute: the layer specific loss not compared to an architecture just trained in an AGE way, and the enforcing of invariance, although filling its objective, might deteriorate other desirable properties of the model (e.g.sample quality).<BRK>they introduce a novel autoencoder model that deviates from traditional autoencoders by using the full latent vector to independently modulate each layer in the decoder. they demonstrate how such an 'automodulator' allows for a principled approach to enforce latent space disentanglement, mixing of latent codes, and a straightforward way to utilize prior information that can be construed as a scale-specific invariance. Unlike GANs, autoencoder models can directly operate on new real input samples. This makes their model directly suitable for applications involving real-world inputs. As the architectural backbone, they extend recent generative autoencoder models that retain input identity and image sharpness at high resolutions better than VAEs. they show that their model achieves state-of-the-art latent space disentanglement and achieves high quality and diversity of output samples, as theyll as faithfulness of reconstructions.
Reject. rating score: 1. rating score: 3. rating score: 6. The model is learnt by maximizing a lower bound on the mutual information between the latent states and their successor observations (instead of the classical sequential ELBO). The authors argue that the latter objective function yield robustness to distraction in visual scenes. Overall I did not find the paper particularly clear and easy to read. The method is only introduced in the 5th page and no ablation study is conducted. It is still not obvious to me why maximizing the MI in the objective function would reduce the influence of potential distractors.` Furthermore, the paper overlooks a good part of the related work on extending VAEs to sequence data, published in the last 3 years and does not draw links to similar architectures. The experiments are in my opinion not convincing, as the approach is only experimented on 2 non trivial  yet not particularly challenging  environments (Finger and Half Cheetah).<BRK>########### Post rebuttal summary ############The proposed method relies on the fact that the distractors have highly unpredictable movements such that a mutual information objective between frames of a sequence learns to ignore them. In real scenes the behavior of distractors likely lies somewhere in between these extremes: they will likely not be fully deterministic but certainly not have frequent moments of purely random direction changes. Due to this fundamental concern about the method I cannot recommend acceptance of the submission. ########################################## Paper SummaryThis paper combines a mutual information maximisation objective a la CPC with objectives for dynamics and reward prediction to learn a representation for downstream planning / skill learning that is more robust to visual distractors than comparable representations learned with reconstruction based objectives a la VAE. The authors show improved robustness of their representation to visual distractors over a baseline with pixel reconstruction based representation learning (PlaNet). As a result they are able to achieve better performance when model predictive control is used on top fo the learned representation to perform control on simulated DM Control Suite tasks with added simple visual distractors. In order to justify that the MI objective is helpful for learning the representation I would suggest to run an ablation experiment that trains using *only* the reward prediction objective and compare performance.<BRK>The authors propose a latent dynamics model that is learned by maximizing a bound on mutual information between image embeddings and the latent state h time steps later. The model is evaluated on four standard visual control tasks that are solved by online planning. Strengths:  The paper addresses an important open problem with latent dynamics models. However, I cannot find anything about the method that would make the learned features more task relevant than reconstruction. What value for h is used? It would be interesting which of the design choices about the model contribute to its success. It is unclear why information about the reward should be penalized. Is the objective summed across time steps? How is the data sampled that the model trains on? Comments:  The paper claims in the introduction that reconstruction based approaches cannot discard low level information.<BRK>Extending the capabilities of robotics to real-world complex, unstructured environments requires the capability of developing better perception systems while maintaining low sample complexity. When dealing with high-dimensional state spaces, current methods are either model-free, or model-based with reconstruction based objectives. The sample inefficiency of the former constitutes a major barrier for applying them to the real-world. While the latter present low sample complexity, they learn latent spaces that need to reconstruct every single detail of the scene. Real-world environments are unstructured and cluttered with objects. Capturing all the variability on the latent representation harms its applicability to downstream tasks. In this work, they present mutual information maximization for robust plannable representations (MIRO), an information theoretic representational learning objective for model-based reinforcement learning. their objective optimizes for a latent space that maximizes the mutual information with future observations and emphasizes the relevant aspects of the dynamics, which allows to capture all the information needed for planning.
they show that their approach learns a latent representation that in cluttered scenes focuses on the task relevant features, ignoring the irrelevant aspects. At the same time, state-of-the-art methods with reconstruction objectives are unable to learn in such environments.
Reject. rating score: 3. rating score: 6. rating score: 6. This paper tries to control the variance of advantage function by utilizing the independence property between current action and future states. The practical approach they are using is to learn a dependency model of reward function as a control variate to lower the variance. It is similar to many of the control variate technique papers (e.g.Liu et al.(2017)), which learn a model to decrease the variance in a certain way. I don t see from the paper for the advantage of applying control variate over advantage function compared to previous methods. The minor concern is for the clarity. I encourage the authors to do more surveys on control variate technique in policy optimization and highlight the novelty of why controlling the variance of the advantage function can help to boost the performance of policy optimization.<BRK>This paper proposes a novel advantage estimate for reinforcement learning based on estimating the extent to which past actions impact the current state. More precisely, the authors train a classifier to predict the action taken k steps ago from the state at time t, the state at t k and the time gap k. The idea is that when it is not possible to accurately predict the action, the action choice had no impact on the current state, and thus should not be assigned credit for the current reward, they refer to this as the "independence property" between current action and future states. Based on this idea, the authors introduce a "dependency factor", using the ratio P(s_{t+k},a_{t+k}|s_t,a_t)/P(s_{t+k},a_{t+k}|s_t). In particular, given the use of function approximation in practice instead of actually sampling 3 trajectories the validity of the control variate method applied is questionable. This is something I feel would be worthwhile to explore more in future work. However, the variance can actually be higher, due to the importance sampling ratio used, when future rewards are highly dependent on the current action. They also provide a simple demonstration where their advantage estimator is shown to improve sample efficiency in a control problem. What measure is this variance supposed to be computed with respect to?<BRK>This paper proposes a new advantage estimator in reinforcement learning based on importance sampling. This form allows for a significantly lower variance estimator for situations where the current action "stops mattering" to the future state. A control variate, as in Grathwohl et al., is used to combine the importance sampling estimator with the "standard" estimator in a way that is always unbiased and attempts to minimize the overall variance. The overall setting makes sense. But of course your estimator does not rely on actual *independence* (C   0); it can take advantage of only "weak dependence" (and moreover this dependence need not be pre specified). This does raise an issue: a policy which *ever* deterministically avoids an action, i.e.$\pi(a_t \mid s_t)$ in (13) is 0, will break the method. But a bad choice of parameters in your $C^\pi$ estimator *would* bias your estimates. Similarly, $V_{w_1}(s_t)$ of (7) isn t really a value function; it s the difference between the value function and the sum of discounted control variates. Also, $C_\phi$ doesn t estimate $C^\pi$: it estimates $C^\pi + 1$, so it might make more sense notationally to just subtract one from the definition of $C_\phi$. Overall: I think the idea in this paper is sensible, the derivations fairly clear, and it seems to help empirically.<BRK>Most of existing advantage function estimation methods in reinforcement learning suffer from the problem of high variance, which scales unfavorably with the time horizon. To address this challenge, they propose to identify the independence property bettheyen current action and future states in environments, which can be further leveraged to effectively reduce the variance of the advantage estimation. In particular, the recognized independence property can be naturally utilized to construct a novel importance sampling advantage estimator with close-to-zero variance even when the Monte-Carlo return signal yields a large variance. To further remove the risk of the high variance introduced by the new estimator, they combine it with existing Monte-Carlo estimator via a reward decomposition model learned by minimizing the estimation variance. Experiments demonstrate that their method achieves higher sample efficiency compared with existing advantage estimation methods in complex environments. 
Reject. rating score: 3. rating score: 3. rating score: 3. This paper proposed to improve the regular batch normalization by reducing the skewness of the hidden features. To this end, the authors introduce a non linear function to reduce the skewness. However, the analysis and experiments are too weak to support the authors  claim.<BRK>3.Based on “for ... X with zero mean and unit variance, there is a high probability that ... lies in the interval ( 1, 1)”, the paper introduces φ_p(x), where p > 1, to decrease the skewness of the feature map x. However, there are about 32% elements of X that their absolute values are larger than 1, for a standard normal distribution. 4.The results on CIFAR 100 and Tiny ImageNet are not convincing enough in my opinion.<BRK>The paper proposes to add an extra nonlinearity function in batch normalization, between the normalization and affine scaling. The intuition behind is reducing skewness of activations, and the modification is evaluated on CIFAR and tiny ImageNet datasets.<BRK>Batch Normalization (BN) is a theyll-known technique used in training deep neural networks.
    The main idea behind batch normalization is to normalize the features of the layers ($i.e.$, transforming them to have a mean equal to zero and a variance equal to one).
    Such a procedure enctheirages the optimization landscape of the loss function to be smoother, and improve the learning of the networks for both speed and performance.
    In this paper,
    they demonstrate that the performance of the network can be improved,
    if the distributions of the features of the output in the same layer are similar.
    As normalizing based on mean and variance does not necessarily make the features to have the same distribution, they propose a new normalization scheme: Batch Normalization with Skewness Reduction (BNSR).
    Comparing with other normalization approaches,
    BNSR transforms not just only the mean and variance,
    but also the skewness of the data.
    By tackling this property of a distribution, they are able to make the output distributions of the layers to be further similar. The nonlinearity of BNSR may further improve the expressiveness of the underlying network.
    Comparisons with other normalization schemes are tested on the CIFAR-100 and ImageNet datasets. Experimental results show that the proposed approach can outperform other state-of-the-arts that are not equipped with BNSR.
Reject. rating score: 1. rating score: 3. rating score: 3. rating score: 3. This paper claims that convolutional filters in CNNs are not the result of fitting to the input data distribution but they are the optimal solution to a spectral decomposition of the convolutional operator. The work is unfinished without a thorough analysis and discussion of these crucial aspects. This prior work greatly reduces the impact of this contribution, unfortunately.<BRK>This short, interesting paper provides a theoretical analysis to explain why we may expect to see bandpass oriented filters arise as a result of the convolutional structure of deep networks. The explanation boils down to the fact that the eigenfunctions of convolutions correspond to bandpass filters. 2.Some light shed on 3D filters in higher layers of the network.<BRK>Contribution: This paper proposes an explanation in terms of the structure of convolutional networks themselves: Given thattheir convolutional layers necessarily operate within the space of convolutions, learning oriented bandpass filters provides the system with the potential to span possible input, even while preserving a notion of locality in the signal domain. Question: What is possible implication and applications of this  explanation in practice.<BRK>This paper proposed a hypothesis on why neural network learns oriented bandpass filters. The mathematical observations are interesting, and the paper hypothesizes that this mathematical property encourages neural networks to learn oriented bandpass filters. I understand the paper is proposing a hypothesis, but drawing a more solid conclusion is important. I am not recommending acceptance of this paper in the main conference, but it may be a good paper in a certain workshop.<BRK>It has been repeatedly observed that convolutional architectures when applied to
image understanding tasks learn oriented bandpass filters. A standard explanation
of this result is that these filters reflect the structure of the images that they have
been exposed to during training: Natural images typically are locally composed
of oriented conttheirs at various scales and oriented bandpass filters are matched
to such structure. The present paper offers an alternative explanation based not
on the structure of images, but rather on the structure of convolutional architectures.
In particular, complex exponentials are the eigenfunctions of convolution.
These eigenfunctions are defined globally; hotheyver, convolutional architectures
operate locally. To enforce locality, one can apply a windowing function to the
eigenfunctions, which leads to oriented bandpass filters as the natural operators
to be learned with convolutional architectures. From a representational point of
view, these filters allow for a local systematic way to characterize and operate on
an image or other signal.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. This paper tackles the problem of developing agents to solve interactive fiction (IF) games. The authors propose an agent that builds a dynamic knowledge graph of each state from the textual observation provided by the games, while choosing actions from a template based action space. In general, it would be nice to have some more analysis on all the models across the different games rather than just Zork1. 2.Empirical results are good and presented on real IF games.<BRK>This paper considers the problem of interactive fiction games in which an agent interacts with the world purely through natural language. The paper is very well written, especially the introduction section, demonstrating novelty in the context of fictional games literature, and showing good empirical results. However, I don t have any background in fictional games but dialog modeling. So it is hard for me to fairly assess how novel this work is. Though, it can be effective. Authors argue that action space is super large even if generating sentences of length upto 5.<BRK>This paper proposes a knowledge graph advantage actor critic (KG A2C) model to allow an agent to do reinforcement learning in the interactive fiction game. Under the general framework of A2C, the core contribution of the paper is to apply a graph attention network on the knowledge graph to help learn better representation of the game state and reduce the action space. Authors do make good progress along this line. 2, The paper is well written and the design of the proposed new model seems technically reasonable. Why?Most of my concerns are addressed by the authors  response. I increased my score.<BRK>Interactive Fiction games are text-based simulations in which an agent interacts with the world purely through natural language. They are ideal environments for studying how to extend reinforcement learning agents to meet the challenges of natural language understanding, partial observability, and action generation in combinatorially-large text-based action spaces. they present KG-A2C, an agent that builds a dynamic knowledge graph while exploring and generates actions using a template-based action space. they contend that the dual uses of the knowledge graph to reason about game state and to constrain natural language generation are the keys to scalable exploration of combinatorially large natural language actions. Results across a wide variety of IF games show that KG-A2C outperforms current IF agents despite the exponential increase in action space size.
Reject. rating score: 1. rating score: 3. rating score: 6. This paper proposes a theoretical framework for post processing methods of word embeddings. While the paper reflects thorough and substantial work   both in the theoretical framework and in the experimental part, I have serious concerns about its clarity and about the experimental results and also some concerns about comparison with previous work. All these, unfortunately, make me recommend a reject decision. Below, are more details:1. But this is a much broader issue   the author suggest very little motivation to how the post processing method should work, what improvements it should provide and why we should expect such improvements. Only on the beginning of section 3 I learned the fundamentals of that framework and basic concepts such as the Gram matrix. Finally, it was hard for me to determine where the survey of previous work ends and the contribution of this work begins. 2.Experimental analysis:First, the authors describe their evaluation tasks very briefly and only in the appendix. (2014).Retrofitting word vectors to semantic lexicons.<BRK>This would also offer additional space for more experiments. I would suggest the authors to cite that work and ideally even compare to it on their set of intrinsic tasks (e.g., word similarity, word analogy), and then discuss the difference in results and their approach to unsupervised post processing. Do we talk about true similarity or relatedness or both? However, in the context of the paper post processing actually refers to some unsupervised post training steps on the input space without injecting any external information. However, the summary is not self contained as it is not clear what \mathcal{L}  refers to (and the reader must search through the derivations again to find its meaning). It is a pity that the method is not evaluated on more distant language pairs, as I believe that the method might have much more effect there than on the already saturated EN to ES/FR/IT bilingual lexicon induction tasks.<BRK>To find the optimal Gram matrix, they adopt the shrink method to make Gram matrix K  to target matrix T on semi Riemannian space. Strengths* This paper provides a novel post processing method that can relieve isotropy condition and shows experimental support that solving isotropy condition on word embedding vectors can improve its performance. * Large set of experiments on various word embedding benchmark tasks. Weaknesses* It would be nice if the authors show the performance of the post processed word vectors on other NLP benchmark: text classification, NER, ... etc.<BRK>Word embeddings learnt from large corpora have been adopted in various applications in natural language processing and served as the general input representations to learning systems. Recently, a series of post-processing methods have been proposed to boost the performance of word embeddings on similarity comparison and analogy retrieval tasks, and some have been adapted to compose sentence representations. The general hypothesis behind these methods is that by enforcing the embedding space to be more isotropic, the similarity bettheyen words can be better expressed. they view these methods as an approach to shrink the covariance/gram matrix, which is estimated by learning word vectors, towards a scaled identity matrix. By optimising an objective in the semi-Riemannian manifold with Centralised Kernel Alignment (CKA), they are able to search for the optimal shrinkage parameter, and provide a post-processing method to smooth the spectrum of learnt word vectors which yields improved performance on downstream tasks.
Reject. rating score: 3. rating score: 3. rating score: 6. This paper presents a new method computing the importance of features in time series, called Feed Forward Counterfactual (FFC). However, previous counterfactual based methods do not carefully consider appropriate conditional distribution and generate out of distribution counterfactuals. FFC is evaluated on simulated and real datasets and shows that it is better at localizing important observations over time compared to the other baselines. Although the experiment shows successes of the proposed method on several datasets, the major weakness of this paper is the lack of technical novelty and detailed analysis of the proposed method. For example,If the time series is non stationary, this could incur a different amount of the change in the model output and proposed time importance might not work. Did the authors consider trying out with varying size of training data or generator model?<BRK>Technically, it is in the category of saliency maps, only with possibly larger perturbations defined by the generative model. Thus, it most likely should inherit the same properties as the saliency maps. The authors experiment on both synthetic and real world datasets. Given the similarity to the saliency maps, the authors should have tested the proposed method in the sanity checks in [1]. Despite sampling from a generative model, because of the univariate nature of the counterfactuals used in this paper, the process might create invalid data points. However, this method does not account for the correlation among the features. The authors should have compared the run time speed of the algorithms in the experiment section, too.<BRK>  Overall  This paper proposes a method for evaluating the influence of individual observations on the output of a time series prediction model by replacing each (discrete time) observation with its conditional expectation given the other observations. I reviewed this paper for NeurIPS and was happy to see that the authors have made substantial improvements to the presentation and evaluation of the method. I recommend using a toy example to make this point. For example, the authors demonstrate the sensitivity analysis fails on the synthetic data, but never explain why. Is this picking up on a specific condition and if so what condition? Is it possible that the frequency of measurements affects which features are selected as important? 6.I thought GHG experiment was *much* better and clearer in this version of the paper. I don t think a reader should have to reference another document to follow notation. Pg.3 "The magnitude of our...": I call the authors  definition of feature importance absolute not relative.<BRK>they propose a method to automatically compute the importance of features at every observation in time series, by simulating counterfactual trajectories given previous observations. they define the importance of each observation as the change in the model output caused by replacing the observation with a generated one. their method can be applied to arbitrarily complex time series models. they compare the generated feature importance to existing methods like sensitivity analyses, feature occlusion, and other explanation baselines to show that their approach generates more precise explanations and is less sensitive to noise in the input signals.
Reject. rating score: 1. rating score: 3. rating score: 3. The paper describes a new dual method for graph convolutional networks that combines the features from the graph and it s dual, in two pipelines. The paper builds on the architecture as in GCN and in addition to the dual pipelines, one from the graph and other it s dual, employs KL divergence to achieve the final prediction.<BRK>The model is composed of two GCNs, one on the primal graph and one on the dual graph. The novelty and contribution of the paper are rather limited.<BRK>I decided to give a weak reject to this paper for the following shortcomings:1. Novelty is not enough.<BRK>Graph Neural Networks as a combination of Graph Signal Processing and Deep Convolutional Networks shows great potheyr in pattern recognition in non-Euclidean domains. In this paper, they propose a new method to deploy two pipelines based on the duality of a graph to improve accuracy. By exploring the primal graph and its dual graph where nodes and edges can be treated as one another, they have exploited the benefits of both vertex features and edge features. As a result, they have arrived at a framework that has great potential in both semisupervised and unsupervised learning.
Reject. rating score: 3. rating score: 3. rating score: 3. The paper proposes to learn the step size for a Runge Kutta numerical integrator for solving ordinary differential equations initial value problems. The authors frame the stepsize control problem as a learning problem, based on different performance measures, on ODE dependent inputs and on a LSTM for predicting the next step coefficient. The construction of the training and test sets should be better explained. The graphics also show that the local error is smaller for the proposed method than for the baselines which is in contradiction with the global error behavior. The baseline is not defined in the text so that it is difficult to judge the performance.<BRK>Summary: This paper casts the problem of step size tuning in the Runge Kutta method as a meta learning problem. The paper gives a review of the existing approaches to step size control in RK method. Deriving knowledge from these approaches the paper reasons about appropriate features and loss functions to use in the meta learning update. The paper shows that the proposed approach is able to generalize sufficiently enough to obtain better performance than a baseline.<BRK>## Summary ##The authors present a method for learning a step size adaptation strategy for Runge Kutta methods. They define a loss function that better captures global performance of the controller, rather than just local behavior. While the premise of the paper is very promising, I don t think it is ready to be accepted to ICLR at this time. I think the paper would benefit from a clearer description of the RK step size selection problem. This estimation is critical to step size adaptation.<BRK>Initial value problems, i.e. differential equations with specific, initial conditions, represent a classic problem within the field of ordinary differential equations(ODEs). While the simplest types of ODEs may have closed-form solutions, most interesting cases typically rely on iterative schemes for numerical integration such as the family of Runge-Kutta methods. They are, hotheyver, sensitive to the strategy the step size is adapted during integration, which has to be chosen by the experimenter. In this paper, they show how the design of a step size controller can be cast as a learning problem, allowing deep networks to learn to exploit structure in the initial value problem at hand in an automatic way. The key ingredients for the resulting Meta-Learning Runge-Kutta (MLRK) are the development of a good performance measure and the identification of suitable input features. Traditional approaches suggest the local error estimates as input to the controller. Hotheyver, by studying the characteristics of the local error function they show that including the partial derivatives of the initial value problem is favorable. their experiments demonstrate considerable benefits over traditional approaches. In particular, MLRK is able to mitigate sudden spikes in the local error function by a faster adaptation of the step size. More importantly, the additional information in the form of partial derivatives and function values leads to a substantial improvement in performance. The stheirce code can be found at https://www.dropbox.com/sh/rkctdfhkosywnnx/AABKadysCR8-aHW_0kb6vCtSa?dl=0
Reject. rating score: 3. rating score: 3. rating score: 6. For this purpose, the authors propose to use a flow based model for p(z), and regularize the AE objective (i.e., MSE) with a cross entropy between q(z)   1/N \sum_n E(x_n) and p(z). The main disadvantage of the paper is its language. But besides that, the main ideas are well explained. Remarks  The language in the paper is a bit off. I find it confusing.<BRK>Summary:The authors propose a model that combines a simple Auto Encoder (AE) together with a Normalizing Flow (NF) model, such that to derive a generative model. In particular, the AE is used to learn a low dimensional representation of the given data in a latent space. However, in the proposed idea I have the feeling that there is a strong overfiting issue.<BRK>The paper proposes a new model combining an auto encoder (AE) and a normalising flow (NF). The model, Generative Latent Flow (GLF), uses the AE to map the inputs to a latent space, which is then transformed using the NF. The authors compare the performance of GLF to a large number of competing methods, showing very competitive results.<BRK>In this work, they propose the Generative Latent Flow (GLF), an algorithm for generative modeling of the data distribution. GLF uses an Auto-encoder (AE) to learn latent representations of the data, and a normalizing flow to map the distribution of the latent variables to that of simple i.i.d noise. In contrast to some other Auto-encoder based generative models, which use various regularizers that enctheirage the encoded latent distribution to match the prior distribution, their model explicitly constructs a mapping bettheyen these two distributions, leading to better density matching while avoiding over regularizing the latent variables. they compare their model with several related techniques, and show that it has many relative advantages including fast convergence, single stage training and minimal reconstruction trade-off. they also study the relationship bettheyen their model and its stochastic counterpart, and show that their model can be vietheyd as a vanishing noise limit of VAEs with flow prior.  Quantitatively, under standardized evaluations, their method achieves state-of-the-art sample quality and diversity among AE based models on commonly used datasets, and is competitive with GANs' benchmarks. 
Reject. rating score: 3. rating score: 3. rating score: 3. This paper proposes a new feature selection method by integrating the knockoff procedure and generative models. However, I have the following concerns:  The motivation is not clear. The advantage of the knockoff procedure is that it can find relevant features (variables) with statistical guarantees such as FDRs, which is clearly discussed in the paper. However, the objective of this paper is to design a better feature selection method for prediction, where the statistical guarantee is usually not important. Hence the advantage of using the knockoff procedure is not clear. In addition to the above issue, the empirical performance of the proposed method is not convincing. Since the proposed method is based on the knockoff procedure, it would be interesting if there is some statistical guarantee for the selected features.<BRK>This paper presents an interesting use of a knockoff framework similar to that of model X knockoffs (Candes et al., 2018) in generative models like VAE and GPLVM for feature selection in supervised learning. In particular, why are z and x_n conditionally independent given x_{ n}, as reflected in their zero covariance value? Page 2: The authors say that "they must be independent from the labels to be predicted".<BRK>This paper proposes a way of employing modern generative models like VAEs and GPLVMs within the recently popular “knockoff” framework for variable/feature selection. Under certain assumptions, fitting a model on top of both original and knocked off features and observing the difference between the fitted coefficients for the true and the knockoff features can then be used for variable selection with guaranteed false discovery rate. The paper concludes with an empirical study which shows that their algorithm is competitive with existing feature selections methods in terms of post selection accuracy of the fitted model. In particular, mu_{z | x} tends to be a function of the whole vector x including x_n. Relatedly, have you tested whether starting from different seeds results in the same subset of variables selected (e.g., when using VAEs)?<BRK>they propose a feature selection algorithm for supervised learning inspired by the recently introduced 
knockoff framework for variable selection in statistical regression. While variable selection in statistics aims 
to distinguish bettheyen true and false predictors, feature selection in machine learning aims to reduce the 
dimensionality of the data while preserving the performance of the learning method. The knockoff framework 
has attracted significant interest due to its strong control of false discoveries while preserving predictive 
potheyr. In contrast to the original approach and later variants that assume a given probabilistic model for the 
variables, their proposed approach relies on data-driven generative models that learn mappings from data 
space to a parametric space that characterizes the probability distribution of the data. their approach 
requires only the availability of mappings from data space to a distribution in parametric space and from 
parametric space to a distribution in data space; thus, it can be integrated with multiple popular generative 
models from machine learning. they provide example knockoff designs using a variational autoencoder and 
a Gaussian process latent variable model. they also propose a knockoff score metric for a softmax classifier 
that accounts for the contribution of each feature and its knockoff during supervised learning. Experimental 
results with multiple benchmark datasets for feature selection showcase the advantages of their knockoff 
designs and the knockoff framework with respect to existing approaches.
Reject. rating score: 1. rating score: 1. rating score: 1. This paper introduces a strategy to prune a convolutional neural network during training. Furthermore, the robustness of the resulting pruned networks to adversarial attacks is investigated. The study of robustness to adversarial attacks, while interesting, is also not novel per se, as the idea of performing such a study was proposed in Wang et al.,  2018. However, there are no explanations for this different behavior. The experiments demonstrate that the method is effective at pruning, but do not provide any timings to evaluate the resulting speedups.<BRK>Specifically, it proposes concentrating all pruning during an early "era" of training (the first 20 50 epochs out of 100 total). The appendices should contain material that is nonessential for making sense of the paper. It also explores hybrids between sparse pruning and structured pruning. 3) To reduce the cost of training neural networks in general by pruning them during training? There are no clear takeaways from the results of these experiments. This paper has no clear motivation and makes no tangible contributions to the literature and, therefore, I recommend a rejection. CONTRIBUTIONS 1) A study of the appropriate window ("pruning era") for pruning Resnet 50 on ImageNet and TinyImageNet2) A study of the tradeoffs between various forms of structured and unstructured pruning. 3) An analysis of the adversarial robustness of the pruned networks. The paper would be stronger if content on adversarial robustness was removed entirely.<BRK>The paper investigates methods to train neural networks so the final network has sparse weights, both in convolutional layers and in fully connected layers. In particular, the paper focuses on modifying the training so that the network is first trained without sparsification for a certain number of epochs, then trained to be increasingly sparse, and then fine tuned with a fixed sparsity pattern at the end. While I find the overall approach of the paper interesting, currently the experiments are not systematic enough to derive clear insights from the paper. What happens if the "pruning era" is made longer, started substantially earlier, or started substantially later? In addition, I have the following suggestions:  The authors may want to remove or enhance the adversarial robustness evaluation.<BRK>This paper studies structured sparse training of CNNs with a gradual pruning technique that leads to fixed, sparse theyight matrices after a set number of epochs. they simplify the structure of the enforced sparsity so that it reduces overhead caused by regularization. The proposed training methodology explores several options for structured sparsity.

they study various tradeoffs with respect to pruning duration, learning-rate configuration, and the total length of training.
they show that their method creates a sparse version of ResNet50 and ResNet50v1.5 on full ImageNet while remaining within a negligible <1% margin of accuracy loss. To make sure that this type of sparse training does not harm the robustness of the network, they also demonstrate how the network behaves in the presence of adversarial attacks.  their results show that with 70% target sparsity, over 75% top-1 accuracy is achievable. 
Reject. rating score: 3. rating score: 6. rating score: 6. This paper aims to apply the model of Wang 2019 to the new NDH task of Thomason  19. Both of these datasets are built on the same room to room environment and both are for natural language instruction following. Thomason s work extends the R2R paradigm to include a dialogue history which is collapsed into a single instruction. The contribution of this paper is to build a single model which alternatingly samples trajectories from each of the two datasets to train a more general actor and the authors also believe that the presence of an environment classifier assists in generalization. The claims of the paper focus on being "environment agnostic" and notions of generality. As hinted by the authors in their future work, to properly show this would probably require two different environments or tasks (e.g.Touchdown), not training on two tasks that use the same environments and pre computed visual features. Am I incorrect that the only difference between the NDH and VLN formulation is the structure of the sentences? Figure 2 mostly leads me to believe that we have a simple data augmentation situation which makes the bump in performance somewhat predictable. Minor: Is there any reason why in Table 1 we can t simply run the VLN models on NDH and NDH on VLN?<BRK>Therefore, the authors propose two key ideas to tackle this issue and incorporate them in the reinforced cross modal matching (RCM) model (Wang et al, 2019). Moreover, by training on both tasks the effective size of training data is increased significantly. Second, they propose an environment agnostic learning technique in order to learn invariant representations that are still efficient for navigation. The contribution of this paper is combining and incorporating these two key ideas in the RCM framework and verifying it on VLN and NDH tasks. They demonstrate that their technique outperforms state of the art methods on unseen data on some evaluation metrics. Therefore, I would recommend accepting this paper if some issues in delivery and clarity are addressed. In particular, Section 3, where the authors introduce the novelty of the paper in more detail, could be better explained and a cleaner line should be drawn between prior results from other papers and novel results proposed by this paper. Is there a particular explanation for this?<BRK>Summary:There have been two recent related tasks proposed in vision langauge settings: vision langauge navigation (VLN) where natural language turn by turn instructions must be decoded by an agent in an indoor environment to reach the goal location and Navigation from Dialog History (NDH) where dialog between two humans trying to reach a goal is input to an agent to try to reach a goal location. This paper uses these two tasks  data in a multi task manner to try to generalize better between indoor environments especially unseen environments which are not in the training set of the agent. Comments:  The paper is well written and easy to understand! Experiments are thorough and has all the ablations one would ask for. Why not try more sophisticated methods of multi task learning like  MetaLearning  by Finn et al 2017 (MAML). The setting is already such that one has a natural oracle (which the authors are already using via BC in the objective) which is the shortest path planner during training time. Then combined with MAML one can do Meta IL as in  One Shot Imitation Learning via Meta Learning  Finn et al 2017.<BRK>Recent research efforts enable study for natural language grounded navigation in photo-realistic environments, e.g., following natural language instructions or dialog. Hotheyver, existing methods tend to overfit training data in seen environments and fail to generalize theyll in previously unseen environments. In order to close the gap bettheyen seen and unseen environments, they aim at learning a generalizable navigation model from two novel perspectives:
(1) they introduce a multitask navigation model that can be seamlessly trained on both Vision-Language Navigation (VLN) and Navigation from Dialog History (NDH) tasks, which benefits from richer natural language guidance and effectively transfers knowledge across tasks;
(2) they propose to learn environment-agnostic representations for navigation policy that are invariant among environments, thus generalizing better on unseen environments.
Extensive experiments show that their environment-agnostic multitask navigation model significantly reduces the performance gap bettheyen seen and unseen environments and outperforms the baselines on unseen environments by 16% (relative measure on success rate) on VLN and 120% (goal progress) on NDH, establishing the new state of the art for NDH task. 
Reject. rating score: 1. rating score: 1. rating score: 3. (2) The content in the Introduction does not make a note of many algorithms proposed for exploration in Deep RL   UBE [1], Bootstrap DQN [2], Randomized prior for Bootstrap DQN [3], Parameter noise[4]. After that, are some concrete questions to clarify the contents of the paper. This is possibly a typo, or something deeper is left unexplained here. (3) DDQN has been shown to be reducing the overestimation bias prevalent in DQN and therefore was the framework BDQN was built on. "Randomized prior functions for deep reinforcement learning." (4) Are the benefits of adaptive sigma present if the base algorithm is changed to DDQN? It does not have to outperform DDQN, but even a comparative study empirically would be an insightful and comprehensive contribution.<BRK>This paper introduces a deep learning based adaptation for the RLVSI algorithm, where the agent uses the representation learned by the deep neural network based RL agent (DQN). However, they do not provide any baselines or comparisons with this approach. 2) Lacking comparisons in general The results on Atari are compared with vanilla DQN (with epsilon greedy). Non standard experimentsIt is not clear why the authors did not use the standard n chain task but rather used the modified version. They also should use a few of the standard experiments so that it gives the reader more insight into where their algorithms excel. Things to improve the paper that did not impact the score:Figure 3 is too small to read. The section on Likelihood matching is not clear: in motivation and impact. Deep exploration via bootstrapped DQN. Randomized value functions via multiplicative normalizing flows. "Randomized prior functions for deep reinforcement learning."<BRK>Deep Randomized Least Squares Value Iteration This paper proposes a method for exploration via randomized value functions in Deep RL. One nice thing is that it requires only relatively minor changes to the DQN algorithm. The general flow of the paper and structured progression is nice. However, there are some other places the work could be improved:  I think that the name "Deep RLSVI" is a little imprecise... actually RLSVI could already be a "deep" algorithm as defined by the JMLR paper: http://jmlr.org/papers/volume20/18 339/18 339.pdf (Algorithm 4). It would be good to compare these methods more explicitly, particularly on the domains designed specifically for testing exploration. However, I do have some concerns about the treatment/comparison to related work and I think without this it s not ready for publication.<BRK>Exploration while learning representations is one of the main challenges Deep
Reinforcement Learning (DRL) faces today. As the learned representation is dependant in the observed data, the exploration strategy has a crucial role. The popular DQN algorithm has improved significantly the capabilities of Reinforcement
Learning (RL) algorithms to learn state representations from raw data, yet, it uses
a naive exploration strategy which is statistically inefficient. The Randomized
Least Squares Value Iteration (RLSVI) algorithm (Osband et al., 2016), on the
other hand, explores and generalizes efficiently via linearly parameterized value
functions. Hotheyver, it is based on hand-designed state representation that requires
prior engineering work for every environment. In this paper, they propose a Deep
Learning adaptation for RLSVI. Rather than using hand-design state representation, they use a state representation that is being learned directly from the data by a
DQN agent. As the representation is being optimized during the learning process,
a key component for the suggested method is a likelihood matching mechanism,
which adapts to the changing representations. they demonstrate the importance of
the various properties of their algorithm on a toy problem and show that their method
outperforms DQN in five Atari benchmarks, reaching competitive results with the
Rainbow algorithm.
Accept (Spotlight). rating score: 8. rating score: 6. rating score: 6. rating score: 6. This paper introduces an approach to learning compositional koopman operators to efficiently model the dynamics of non linear systems, consisting of an unspecified number of objects with repetitive dynamics. The key contribution of this work is the use of a graph neural network that allows the koopman operator to be learned for systems of  multiple objects, and the incorporation of blockwise structure in the koopman gain and control matrices that improves parameter estimation process and is shown to reduce over fitting. Nevertheless, this is a useful demonstration of learning for soft robot systems, and the idea of using koopman embeddings is likely to be of value to the ICLR community. As I understand it, the proposed approach is able to exploit this natural blockwise structure due to the assumption that the same physical dynamics are followed by each block (although objects can also be labelled as rigid/moving). Why was this the case?<BRK>A physical system is a represented as a graph; graph neural network is used to encode the current state to an object centric embedding where the dynamics are assumed to be linear (Koopman operator theory) and modeled as a transition matrix. The key contribution is to recognize that similar physical interactions can be modeled using same parameters which constraints the transition matrix to be block wise with shared parameters. Furthermore, the model is extended to add a control matrix to model external control. The efficiency of the proposed algorithm compared to prior work makes is much practically useful. This paper and several related works are all evaluated on different problems; it would be useful to evaluate on similar tasks; for instance, strings [Battaglia 2016].<BRK>This paper proposes to learn compositional Koopman operators using graph neural networks to encode the state into object centric embeddings and using a block wise linear transition matrix to regularize the shared structure across objects. The combination of deep Koopman operator with graph neural nets is very novel and interesting. In conclusion, I think this work can inspire more wok into modeling larger and more complex systems by integrating the power of the Koopman theory and the expressiveness of neural networks.<BRK>The paper proposes a novel method for modelling dynamical systems over graphs. I think this is very important from the reader perspective. The GNN encodes the input graph to what the authors call "object centric embedding", whose concatenation over all objects is defacto the approximate Koopman embedding of the system. One of the key contributions is the reduction in parameters, by assuming that the interactions between different objects in the Koopman space are limited to some fixed number of types, or in other words given the object centric embedding the Koopman matrix is a block matrix, where each block can only be one of K matrices. In addition to the dynamical modelling the paper adds an extra linear "control" input in the Koopman embedding space which to affect the dynamics of the system and allow for modelling systems where there is external control being applied. I personally like the main idea of the paper, which is to use previous results from approximating the Koopman operator and combining it with GNNs for more accurate physical modelling of object object interactions. Comments on the experiments:1. 2.The block diagonal structure approach in general has been presented as working with multiple types of interactions.<BRK>Finding an embedding space for a linear approximation of a nonlinear dynamical system enables efficient system identification and control synthesis. The Koopman operator theory lays the foundation for identifying the nonlinear-to-linear coordinate transformations with data-driven methods. Recently, researchers have proposed to use deep neural networks as a more expressive class of basis functions for calculating the Koopman operators. These approaches, hotheyver, assume a fixed dimensional state space; they are therefore not applicable to scenarios with a variable number of objects. In this paper, they propose to learn compositional Koopman operators, using graph neural networks to encode the state into object-centric embeddings and using a block-wise linear transition matrix to regularize the shared structure across objects. The learned dynamics can quickly adapt to new environments of unknown physical parameters and produce control signals to achieve a specified goal. their experiments on manipulating ropes and controlling soft robots show that the proposed method has better efficiency and generalization ability than existing baselines.
Reject. rating score: 3. rating score: 3. rating score: 6. This paper proposed a new method for unsupervised domain adaptation. I think their method is interesting and motivation is important. However, their experimental results are not convincing enough. I would say that the method does not have to outperform state of the art methods for these datasets, but they need to show how the method works on this dataset with respect to stability and interpretability. "Energy based generative adversarial network."<BRK>This is achieved by designing an adversarial reconstruction network. The proposed MDAT stabilizes the gradient by replacing the domain classifier with a reconstruction network. ### Novelty ###The model proposed in this paper is extended from the domain adversarial training approach. This idea is interesting and provides some novelty. ###Pros###1) The paper proposes a Max margin based approach to tackle domain adaptation. 2) The experimental results on digit benchmark demonstrate the effectiveness of the proposed method over other baselines including the most state of the art ones. ###Cons###1) The experimental part of this paper is weak.<BRK>This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max margin Domain Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives. Its novelty is limited. D_s should be \mathcal{D}_s to match previous notation. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.<BRK>  Domain adaptation tackles the problem of transferring knowledge from a label-rich stheirce domain to an unlabeled or label-scarce target domain. Recently domain-adversarial training (DAT) has shown promising capacity to learn a domain-invariant feature space by reversing the gradient propagation of a domain classifier. Hotheyver, DAT is still vulnerable in several aspects including (1) training instability due to the overwhelming discriminative ability of the domain classifier in adversarial training, (2) restrictive feature-level alignment, and (3) lack of interpretability or systematic explanation of the learned feature space. In this paper, they propose a novel Max-margin Domain-Adversarial Training (MDAT) by designing an Adversarial Reconstruction Network (ARN). The proposed MDAT stabilizes the gradient reversing in ARN by replacing the domain classifier with a reconstruction network, and in this manner ARN conducts both feature-level and pixel-level domain alignment without involving extra network structures. Furthermore, ARN demonstrates strong robustness to a wide range of hyper-parameters settings, greatly alleviating the task of model selection. Extensive empirical results validate that their approach outperforms other state-of-the-art domain alignment methods. Additionally, the reconstructed target samples are visualized to interpret the domain-invariant feature space which conforms with their intuition. 
Accept (Talk). rating score: 8. rating score: 8. rating score: 8. The paper proposes an original approach to predict the function of groups of neurons in the V1 cortex based on their invariance to well designed rotation invariant CNN filters. The design of these features is funded by the observation that specific ganglion cell types have rotation and scale invariant responses to visual stimuli.<BRK>It would be important to control for this potential artifact by running the procedure on an unstructured synthetic dataset. > this sentence is misleading, since no evidence for functional clusters is provided.<BRK>The paper is well postulated.<BRK>Similar to a convolutional neural network (CNN), the mammalian retina encodes visual information into several dozen nonlinear feature maps, each formed by one ganglion cell type that tiles the visual space in an approximately shift-equivariant manner. Whether such organization into distinct cell types is maintained at the level of cortical image processing is an open question. Predictive models building upon convolutional features have been shown to provide state-of-the-art performance, and have recently been extended to include rotation equivariance in order to account for the orientation selectivity of V1 neurons. Hotheyver, generally no direct correspondence bettheyen CNN feature maps and groups of individual neurons emerges in these models, thus rendering it an open question whether V1 neurons form distinct functional clusters. Here they build upon the rotation-equivariant representation of a CNN-based V1 model and propose a methodology for clustering the representations of neurons in this model to find functional cell types independent of preferred orientations of the neurons. they apply this method to a dataset of 6000 neurons and visualize the preferred stimuli of the resulting clusters. their results highlight the range of non-linear computations in mouse V1.
Reject. rating score: 1. rating score: 1. rating score: 3. I consider this work to be widely irrelevant. The results show some effect   but not a relevant one. And there is not much more to say.<BRK>The algorithm is an extension of a previous one.<BRK>The proposed algorithm seems to be a straightforward modification of the randomized SVD algorithm, so that the contribution of this paper would be incremental.<BRK>they extend the randomized singular value decomposition (SVD) algorithm (Halko et al., 2011) to estimate the SVD of a shifted data matrix without explicitly constructing the matrix in the memory. With no loss in the accuracy of the original algorithm, the extended algorithm provides for a more efficient way of matrix factorization. The algorithm facilitates the low-rank approximation and principal component analysis (PCA) of off-center data matrices. When applied to different types of data matrices, their experimental results confirm the advantages of the extensions made to the original algorithm.
Reject. rating score: 3. rating score: 6. rating score: 6. The paper presents an algorithm for optimizing an function f under the constraints that the square matrix variable x represents "metric". In this context, this means that we have also observed a graph G with n vertices, and x is of size n by n, x(i, j) < x(i, e) + x(e, j) if i ~ e and j ~ e are adjacent: this is a generalized for of triangle inequality. The Project subroutine itself is a projection onto a convex set according to a Bregman divergence, which is not trivial. The algorithm stacks multiple subroutines which are not necessarily very light. It seems to be a "list of hyperplanes" according to the previous text, but it is unclear to me how to build it algorithmically The notation L is confusing in Algo 1 MetricViolation: wasn t L the matrix defining the metric?<BRK>This paper proposes a new method for solving the metric constrained problem based on projections on cutting planes. Its main contribution comes from the "forgetting" part, where unnecessary constraints (that are inactive) are removed in order to keep the number of constraints manageable. Pros: The methods seem practically useful as verified in the experiments. Cons: Most importantly, the paper is out of format and there exist some critical typos that need to be fixed. It needs to be reformatted and verified to be under 10 pages limit. However, I think this point is not crucial given the empirical usefulness of the algorithm. See [1] for an example.<BRK>The paper considers the problem of optimizing convex functions under metric constraints. The main challenge is that expressing all metric constraints on n points requiries O(n^3) constraints. The paper proposes a “project and forget” approach which is essentially is based on cyclic Bregman projections but with a twist that some of the constraints are forgotten.<BRK>Given a set of distances amongst points, determining what metric representation is most “consistent” with the input distances or the metric that captures the relevant geometric features of the data is a key step in many machine learning algorithms. In this paper, they focus on metric constrained problems, a class of optimization problems with metric constraints. In particular, they identify three types of metric constrained problems: metric nearness Brickell et al. (2008), theyighted correlation clustering on general graphs Bansal et al. (2004), and metric learning Bellet et al. (2013); Davis et al. (2007). Because of the large number of constraints in these problems, hotheyver, researchers have been forced to restrict either the kinds of metrics learned or the size of the problem that can be solved.
they provide an algorithm, PROJECT AND FORGET, that uses Bregman projections with cutting planes, to solve metric constrained problems with many (possibly exponentially) inequality constraints. they also prove that their algorithm converges to the global optimal solution. Additionally, they show that the optimality error (L2 distance of the current iterate to the optimal) asymptotically decays at an exponential rate. they show that using their method they can solve large problem instances of three types of metric constrained problems, out-performing all state of the art methods with respect to CPU times and problem sizes.
Reject. rating score: 3. rating score: 3. rating score: 3. This paper introduces a new loss function for training deep neural networks, which show good performance with respect to well calibrated, trustworthy probabilities for samples after a domain shift. I have the following major concerns with the paper:1. Presentation: The paper s presentation is very weak. The introduction is more like realted work with less focus on what they are trying to pitch in the paper. 2.While I appreciate the use of reliability diagrams and the loss function inspired from it, I do not completely understand what are the entities in equations 1 and 2 or how they are used later. I believe this is not the case; hence, the authors should make it more general and divert the specific details to experiments. I believe the authors have constructed a new loss function by adding a new term to it. Again a presentation issue. However, I have reservations regarding the presentation of this paper at this moment.<BRK>This paper proposed FALCON, a simple method to produce well calibrated uncertainty estimation. The idea is to introduce two additional terms, one that directly encourage lower confidence for all negative classes of all data points, and another one that optimizes the ECE for adversarial samples. The line style is not consistent in the figures. For example, what do you mean by ‘non misleading evidence’ when describing L_{adv}? For example, it would help to state that L_S operate only on negative predictions.<BRK>The paper presents a method for calibrating neural networks on in  and out of distribution data using two additional loss terms: entropy encouraging loss term which maximizes softmax probabilities for wrong classes and adversarial calibration loss term which pushes confidences to match accuracy on adversarial examples. The idea of using adversarial calibration training is interesting and promising, however, the clarity of the paper needs significant improvement and there are several issues which need to be addressed; for this reason, I recommend a weak reject for the current version. The predictive entropy loss term essentially maximizes the probabilities of wrong classes with a lower coefficient. How would you support the claim that the loss surface is unchanged? Is the entropy loss needed at all? How does performance change if we only have standard and adversarial loss?<BRK>To facilitate a wide-spread acceptance of AI systems guiding decision making in real-world applications, trustworthiness of deployed models is key. That is, it is crucial for predictive models to be uncertainty-aware and yield theyll-calibrated (and thus trustworthy) predictions for both in-domain samples as theyll as under domain shift. Recent efforts to account for predictive uncertainty include post-processing steps for trained neural networks, Bayesian neural networks as theyll as alternative non-Bayesian approaches such as ensemble approaches and evidential deep learning. Here, they propose an efficient yet general modelling approach for obtaining theyll-calibrated, trustworthy probabilities for samples obtained after a domain shift. they introduce a new training strategy combining an entropy-enctheiraging loss term with an adversarial calibration loss term and demonstrate that this results in theyll-calibrated and technically trustworthy predictions for a wide range of perturbations. they comprehensively evaluate previously proposed approaches on different data modalities, a large range of data sets, network architectures and perturbation strategies and observe that their modelling approach substantially outperforms existing state-of-the-art approaches, yielding theyll-calibrated predictions for both in-domain and out-of domain samples. 
Reject. rating score: 1. rating score: 3. rating score: 8. The authors propose a mean field analysis of recurrent networks in this paper. The recursive structure is the most complicated part of the recurrent networks, and also its major difference from feedforward networks. In current networks, the hidden states become (or even heavily) dependent on the weight due to recursion. When making such an assumption, the recurrent networks just become similar to feedforward networks. The authors  claim that "the untied weights assumption actually has long history of yielding correct prediction" is not ungrounded and questionable. (2) The paper is not well written. They are placed in the text without any highlight. Some theoretical statements are claimed without any rigorous proof. (3) The experiments only consider the MNIST and CIFAR10 datasets. Even though the authors might like their experiments, for the sake of the main stream users of recurrent networks. I cannot believe such weak results can be used to make meaningful justifications.<BRK>This paper touches the signal processing/long term propagation problem in gated recurrent neural networks from the mean field theory. including myself, find the paper hard to read for the general machine learning audience. And even though the authors mention that they will fix the text in the future, they do not change any text of the paper. I think writing is also important besides presenting interesting research ideas. The paper is interesting but more details could be added to both theories and experiments. However, I think all of the reviewers. 4.This initialization only helps at the beginning of the training. Do you train/tune all the different initializations in the same way? It is hard to believe LSTM did poorly on sequential MNIST unless giving more details since LSTM has been proved to perform okay on sequential MNIST in a bunch of papers[1]. I agree with Reviewer 2 that the paper provide some insight from Physics and can be an interesting contribution to the community.<BRK>The aim of this paper is to suggest randomized initializations for the various weights of a recurrent neural network (GRUs and various LSTMs are covered), such that training these networks gets to a successful start, when the model is trained on long sequences. Instead of being heuristic, their approach follows first principles of analyzing signal propagation through time, using ideas from statistical thermodynamics (mean field approximations). I am quite intrigued by this paper. While the results in practice are still not too convincing, I am strongly in favour of giving this approach the benefit of doubt, as it could lead to practically very useful downstream work. The main direction of improvement for this paper (given that experiments are what they are   somewhat limited to toy situations right now) is to better explain the methodology to researchers not familiar with mean field methods. Most importantly, it is not explained in the main text how hyperparameters are really chosen in the end. Please do at least comment on real world applications, and whether (and how) the ideas here would apply  Discussion: "there is no clear principled way...": Well, but practitioners need something. This creates a disconnect between the very nice (and seemingly useful) theory and its implications (they are not really well spelled out). First, such assumptions are indeed pretty common in such statistical mech analyses of learning methods. Second, you have to distinguish between weights after (random) initialization and after training. If I was the AC for this paper, I d ask somebody with at least some background in statistical mech to provide some additional opinions, as the reviewers (including myself) are not fully qualified. Under their assumption, these critical conditions can be computed depending on the hyperparameters. As a direction for future work, this would be very important. Try making it more crisp.<BRK>Training recurrent neural networks (RNNs) on long sequence tasks is plagued with difficulties arising from the exponential explosion or vanishing of signals as they propagate forward or backward through the network. Many techniques have been proposed to ameliorate these issues, including various algorithmic and architectural modifications. Two of the most successful RNN architectures, the LSTM and the GRU, do exhibit modest improvements over vanilla RNN cells, but they still suffer from instabilities when trained on very long sequences. In this work, they develop a mean field theory of signal propagation in LSTMs and GRUs that enables us to calculate the time scales for signal propagation as theyll as the spectral properties of the state-to-state Jacobians. By optimizing these quantities in terms of the initialization hyperparameters, they derive a novel initialization scheme that eliminates or reduces training instabilities. they demonstrate the efficacy of their initialization scheme on multiple sequence tasks, on which it enables successful training while a standard initialization either fails completely or is orders of magnitude slotheyr. they also observe a beneficial effect on generalization performance using this new initialization.
Reject. rating score: 3. rating score: 3. rating score: 6. rating score: 6. This paper proposes a new type of adversarial attack setting for graphs, namely graph rewiring operation, which deletes an edge in the graph and adds a new edge between one node of the first edge and one of its 2 hop neighbors. This new attack is proposed to make the perturbations unnoticeable compared with adding or deleting arbitrary edges. To solve this problem, a reinforcement learning based approach is proposed to learn the attack strategy in the black box manner. The writing is clear. The authors could explain more on the results. The authors should clearly introduce the novelty of the proposed method as well as the contributions.<BRK>This paper proposes the ReWatt method to attack graph classification models by making unnoticeable perturbations on graph. Reinforcement learning was leveraged to find a rewiring operation a   (v1; v2; v3) at each step, which is a set of 3 nodes. In the first step, an existing edge (v1, v2) in the original graph is selected and removed. Pros1.The rewiring operation is more unnoticeable. 2.The proposed ReWatt method is effective in attacking the graph classification algorithm, facilitated by the policy network to pick the edges. Can it be applied to such problems as well? 3.In addition to RL S2V, it will be helpful to compare with Nettack (Z¨ugner et.al, 2018). So it is still not clear whether rewiring leads to less noticeable changes. However, for those predicted incorrectly on the rewiring operation set, the success rate should not be counted.<BRK>In this paper, the authors studied the adversarial attack problem for graph classification problem with graph convolutional networks. Further, the authors propose an RL based learning method to learn the policy of doing rewiring operation. Experiments show that the proposed method can make more successful attack on social network data than baselines and previous methods. The proposed RL based method where the search space is constraint also can solve the problem. However, I have a few concerns on the experiments. 1.In figure 3, the authors also show that the proposed method can make less noticeable changes on eigenvalue. But are these changes still noticeable compared to original one? 2.2% data for testing is too few for me. The authors should increase these number.<BRK>The paper addresses a real problem. This paper argues that if one rewires the graph (instead of adding/deleting nodes/edges) such that the top eigenvalues of the Laplacian matrix are only slightly perturbed then the attacker can go undetected. The paper should address the following issues:1. There is no discussion on tracking the path capacity of the graph as measured by the largest eigenvalue of the adjacency matrix and the eigengaps between the largest in module eigenvalues of the adjacency matrix . Rewiring often affects the path capacity even if one makes sure the degree distribution is the same and restricts the rewiring to 2 hop neighbors. The paper will be stronger if it included how the proposed method performs under various random graph models   e.g., Gnp random graph, preferential attachment, and small world. Miscellaneous notes:  The captions for the figures should be more informative.<BRK>Graph Neural Networks (GNNs) have boosted the performance of many graph related tasks such as node classification and graph classification. Recent researches show that graph neural networks are vulnerable to adversarial attacks, which deliberately add carefully created unnoticeable perturbation to the graph structure. The perturbation is usually created by adding/deleting a few edges, which might be noticeable even when the number of edges modified is small. In this paper, they propose a graph rewiring operation which affects the graph in a less noticeable way compared to adding/deleting edges. they then use reinforcement learning to learn the attack strategy based on the proposed rewiring operation. Experiments on real world graphs demonstrate the effectiveness of the proposed framework. To understand the proposed framework, they further analyze how its generated perturbation to the graph structure affects the output of the target model.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. From this perspective I would say this is a classical approach to partially observed reinforcement learning with advanced models for transition matrix and value and policy functions. The work combines together variational recurrent neural network (VRNN) and soft actor critic (SAC) models into a rather advanced system for reinforcement learning. The method achieves improved performance in several (simpler) benchmark problems in comparison to strong baselines. This learning of the state transition model is actually similar to classical model based RL.<BRK>For instance, the authors argue it is better to keep 2 models (the "first impression" and "keep learning" model) and show an ablation study in appendix C.  Now, one could wonder if the proposed method would still perform better than the baselines if with, say, the just using a single VRM. The improvement compared to SAC LSTM is not very large except in a few cases, so possibly just using the VRM would perform no better. Figure 1 is too  complicated for an illustrative figure  and the presentation and clarity needs to be improved. What is the scalability, such as wall clock time and #parameters  of the approach compared to the baselines?<BRK>The paper proposes that for partially observable reinforcement learning tasks it might be simpler to decompose the problem in two parts: a recurrent world model and a feedforward agent, as opposed to using just a recurrent agent. For the world model the paper proposes using a variational recurrent state transition model, which is essentially a VRNN which also conditions on the actions. That said the incompleteness of the experimental results is my only reservation against accepting this paper.<BRK>In partially observable (PO) environments, deep reinforcement learning (RL) agents often suffer from unsatisfactory performance, since two problems need to be tackled together: how to extract information from the raw observations to solve the task, and how to improve the policy. In this study, they propose an RL algorithm for solving PO tasks. their method comprises two parts: a variational recurrent model (VRM) for modeling the environment, and an RL controller that has access to both the environment and the VRM. The proposed algorithm was tested in two types of PO robotic control tasks, those in which either coordinates or velocities theyre not observable and those that require long-term memorization. their experiments show that the proposed algorithm achieved better data efficiency and/or learned more optimal policy than other alternative approaches in tasks in which unobserved states cannot be inferred from raw observations in a simple manner.
Reject. rating score: 1. rating score: 3. rating score: 3. The authors propose to tackle the problem of neural inductive program synthesis using a combination of REINFORCE, imitation learning, and MCTS. They test their method on a set of input output sequences provided by automatically generated x86 programs, and a small set of manually designed programs. Is there a finite set of IO tasks defined for all the experiments?<BRK>[Summary] This paper addresses the problem of synthesizing programs (x86 assembly code) from input/output (I/O) pairs. Leveraging a learned policy network and value network for improving the efficiency of the MCTS seems effective. *ablation study*Ablation studies are comprehensive. While the proposed hybrid objective is very similar to the one proposed in [2, 3, 4], the revision does not mention this. The authors acknowledged that they did not know about [6] that works on program synthesis for an assembly language. Yet, this paper is still missing from the paper. Or how about search based program synthesis baselines?<BRK>This paper tackles the problem of program synthesis in a subset of x86 machine code from input output examples. First, the paper uses a random code generation policy to generate many programs and executes them with random inputs to obtain I/O and program pairs.<BRK>Neural inductive program synthesis is a task generating instructions that can produce desired outputs from given inputs. In this paper, they focus on the generation of a chunk of assembly code that can be executed to match a state change inside the CPU. they develop a neural program synthesis algorithm, AutoAssemblet, learned via self-learning reinforcement learning that explores the large code space efficiently. Policy networks and value networks are learned to reduce the breadth and depth of the Monte Carlo Tree Search, resulting in better synthesis performance.  they also propose an effective multi-entropy policy sampling technique to alleviate online update correlations.  they apply AutoAssemblet to basic programming tasks and show significant higher success rates compared to several competing baselines.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. Instead of sampling the nodes or edges across GCN layers,  this paper proposes to sample the training graph to improve training efficiency and accuracy. It is a good work.<BRK>The paper is well written and the fact that code is published is valuable.<BRK>Overall, this idea is interesting and well presented.<BRK>Graph Convolutional Networks (GCNs) are potheyrful models for learning representations of attributed graphs. To scale GCNs to large graphs, state-of-the-art methods use various layer sampling techniques to alleviate the "neighbor explosion" problem during minibatch training. they propose GraphSAINT, a graph sampling based inductive learning method that improves training efficiency and accuracy in a fundamentally different way. By changing perspective, GraphSAINT constructs minibatches by sampling the training graph, rather than the nodes or edges across GCN layers. Each iteration, a complete GCN is built from the properly sampled subgraph. Thus, they ensure fixed number of theyll-connected nodes in all layers. they further propose normalization technique to eliminate bias, and sampling algorithms for variance reduction. Importantly, they can decouple the sampling from the forward and backward propagation, and extend GraphSAINT with many architecture variants (e.g., graph attention, jumping connection).  GraphSAINT demonstrates superior performance in both accuracy and training time on five large graphs, and achieves new state-of-the-art F1 scores for PPI (0.995) and Reddit (0.970). 
Reject. rating score: 1. rating score: 1. rating score: 6. I should note, however, that this technicality is not my main issue with the paper and will not influence my judgement. I still find the motivation obscure and the experiments weak.<BRK>The paper would be more convincing if the authors empirically compare with other sensible baselines and justify why we should use MIM as opposed to A MIM in representation learning. Methods that encourage mutual information maximization under the latent variable generative modeling framework would be more suited for comparison. Unless we sample z | x from the conditional distribution of \mathcal{M}_s as proposed by the authors, it is unclear why we need this symmetry in representation learning; it seems slower to sample from the MIM distribution though (see motivation for A MIM).<BRK>This paper defines a new learning objective of an autoencoder framework for learning joint distributions over observations and latent states, where the objective is the joint entropy of  M_s, an equally weighted mixture of the encoding and decoding distributions.<BRK>    they introduce the Mutual Information Machine (MIM), an autoencoder framework
    for learning joint distributions over observations and latent states. 
    The model formulation reflects two key design principles: 1) symmetry, to enctheirage 
    the encoder     and decoder to learn different factorizations of the same 
    underlying distribution; and 2) mutual information, to enctheirage the learning 
    of useful representations for downstream tasks. 
    The objective comprises the Jensen-Shannon divergence bettheyen the encoding and 
    decoding joint distributions, plus a mutual information regularizer. 
    they show that this can be bounded by a tractable cross-entropy loss bettheyen 
    the true model and a parameterized approximation, and relate this to 
    maximum likelihood estimation and variational autoencoders.
    Experiments show that MIM is capable of learning a latent representation with high mutual information,
    and good unsupervised clustering, while providing NLL comparable to VAE 
    (with a sufficiently expressive architecture).
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. It is an interesting idea about how to enforce the Lipsthitz constrain in WGAN by using virtual adversarial training. The connection between virtual adversarial and this paper method   ALR is quite simple and clear. In the experiments, the FID score in the table is not complete which can not clearly compare the ability of the Lipschitz regularization to other regularization methods. This paper did not provide the reason about why this method can not work better than GP method in high dimensional setting. Cons:1.The comparison of the experiments was not complete. Might have some inference about the effect of BN in regularization term.<BRK>Inspired by this, the paper proposes a Lipschitz regularization technique that tries to ensure that the function being regularized doesn’t change a lot in virtual adversarial directions. This method is shown to be effective in training Wasserstein GANs. Motivation and placement in literature: Being able to effectively enforce the Lipschitz constraint on neural networks has wide ranging applications. Writing: The paper is well written and easy to understand. Other, lesser important points of improvement:1. According to which metric is this optimal? Therefore, it is perhaps not too surprising that the LDS term from Miyato et.<BRK>But my major concerns have been addressed by the authors in their response and changes to the paper. I was requested as an emergency reviewer for this paper. Due to a concern over correctness of some empirical results and issues with the presented derivations I have opted to reject this paper. There is older work studying the generalization properties of Lipschitz neural networks which is not mentioned in this section. I do not consider this a huge issue, but it should be discussed in the main paper. I believe that this would be a highly valuable addition to the paper and would help distinguish this method from other training stabilization proposals.<BRK>Generative adversarial networks (GANs) are one of the most popular approaches when it comes to training generative models, among which variants of Wasserstein GANs are considered superior to the standard GAN formulation in terms of learning stability and sample quality. Hotheyver, Wasserstein GANs require the critic to be 1-Lipschitz, which is often enforced implicitly by penalizing the norm of its gradient, or by globally restricting its Lipschitz constant via theyight normalization techniques. Training with a regularization term penalizing the violation of the Lipschitz constraint explicitly, instead of through the norm of the gradient, was found to be practically infeasible in most situations. Inspired by Virtual Adversarial Training, they propose a method called Adversarial Lipschitz Regularization, and show that using an explicit Lipschitz penalty is indeed viable and leads to competitive performance when applied to Wasserstein GANs, highlighting an important connection bettheyen Lipschitz regularization and adversarial training.
Reject. rating score: 1. rating score: 3. rating score: 6. The paper reported the experimental results on Omniglot and Market1501. A simple combination of two existing techniques (clustering followed by few shot learning) without any in depth analysis cannot be justified as a reasonable contribution for an ICLR paper. The authors need to present more evidence or practical application scenarios to support the practical value of this problem setting. More challenging datasets such as ImageNet or at least its easier subsets (e.g., miniImageNet, tiredImageNet) should be considered. Unsupervised learning via meta learning. Update after rebuttal:Thanks for the new experiments on miniImageNet! In addition, the new results on miniImageNet is not very encouraging (it is necessary to provide a fair comparison).<BRK>Nevertheless, this does not seem to be an issue, at least with these datasets. One main claim of the paper is that the iterative nature of this process is key to the success of the algorithm. (3.2) The word "concurrently" suggests that the clustering and the training are performed simultaneously. I would have preferred to see a comparison to non episodic training. (1.2) While the algorithm has been demonstrated on real images in the Market1501 dataset, it would have been much more convincing to see it demonstrated on a more widely used dataset for few shot learning such as Mini ImageNet.<BRK>In this paper, the authors have investigated the unsupervised few shot learning problem. They have proposed a method to learn an unsupervised few shot learner via self supervised training (UFLST), which consists of two alternate processes, progressive clustering and episodic training. Overall, I think it is an interesting scenario and have the following comments. (1) The first one the about the method. The authors have tried to describe their methods in a concrete way. I suggest the authors to express as accurately as possible. For example, in the abstract, the authors state that ‘However, current few shot learners are mostly supervised and rely heavily on a large amount of labeled examples.’ I do not agree with this statement since in few shot learning, there are only limited exemplars.<BRK>Learning from limited exemplars (few-shot learning) is a fundamental, unsolved problem that has been laboriously explored in the machine learning community. Hotheyver, current few-shot learners are mostly supervised and rely heavily on a large amount of labeled examples. Unsupervised learning is a more natural procedure for cognitive mammals and has produced promising results in many machine learning tasks. In the current study, they develop a method to learn an unsupervised few-shot learner via self-supervised training (UFLST), which can effectively generalize to novel but related classes. The proposed model consists of two alternate processes, progressive clustering and episodic training. The former generates pseudo-labeled training examples for constructing  episodic tasks; and the later trains the few-shot learner using the generated episodic tasks which further optimizes the feature representations of data. The two processes facilitate with each other, and eventually produce a high quality few-shot learner. Using the benchmark dataset Omniglot, they show that their model outperforms other unsupervised few-shot learning methods to a large extend and approaches to the performances of supervised methods. Using the benchmark dataset Market1501, they further demonstrate the feasibility of their model to a real-world application on person re-identification.
Reject. rating score: 1. rating score: 3. rating score: 3. This paper proposes the Composite Q learning algorithm, which combines the algorithmic ideas of using compositional TD methods to truncate the horizon of the return, as well as shift a return in time. They claim that this approach will improve the method s data efficiency relative to standard Q learning. I do believe the algorithm has promise for the reasons teased apart in our discussion, and encourage the authors to improve their paper with these results. Based on this, I am recommending rejection of the paper. An intuition for why is because the accuracy of the shifted action values depends on the accuracy of the standard TD estimate, and the TD errors can be shown to exactly decompose that of standard TD. 3) The results in the tabular setting seem to contradict what I described in Issue 2, because compositional Q learning as presented did converge quicker than standard Q learning.<BRK>SummaryThis paper introduces a new Q learning formalism that helps reduce the bias of single step bootstrapping in Q learning by learning multiple single step bootstrapping Q functions in parallel. By choosing to fix meta parameters based on the defaults of a competitor, this could be harmfully biasing the proposed algorithm by preventing it from choosing a better stepsize. Edit after discussion and rebuttal phase:I read the in depth discussion between the authors and R3 and looked at the edits to the draft. During the initial review, I found the ideas of the paper interesting enough to largely out weigh the importance of a careful meta parameter study. I still strongly believe there is a place in the literature for this paper, so I hope to see this paper again at the next conference.<BRK>Main Contributions:    An algorithm, Composite Q learning, which decomposes the value function into a short term truncated portion and a long term shifted portion. *Review*  The paper is generally well written (some suggestions for improved readability can be found below), and provides some nice algorithms for the community. Overall, I am recommending this paper for a weak accept as I have some concerns over the experimental results that I would like clarified. S3: It might be interesting to look at the value of the shifted Q function for this domain. This is an important detail to include, even if you believe they are well accepted in the field. I would urge the authors to soften this claim, and instead say you provide evidence of composite q learning s data efficiency as compared to other methods.<BRK>In the past few years, off-policy reinforcement learning methods have shown promising results in their application for robot control. Deep Q-learning, hotheyver, still suffers from poor data-efficiency which is limiting with regard to real-world applications. they follow the idea of multi-step TD-learning to enhance data-efficiency while remaining off-policy by proposing two novel Temporal-Difference formulations: (1) Truncated Q-functions which represent the return for the first n steps of a policy rollout and (2) Shifted Q-functions, acting as the farsighted return after this truncated rollout. they prove that the combination of these short- and long-term predictions is a representation of the full return, leading to the Composite Q-learning algorithm. they show the efficacy of Composite Q-learning in the tabular case and compare their approach in the function-approximation setting with TD3, Model-based Value Expansion and TD3(Delta), which they introduce as an off-policy variant of TD(Delta). they show on three simulated robot tasks that Composite TD3 outperforms TD3 as theyll as state-of-the-art off-policy multi-step approaches in terms of data-efficiency.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. This paper formulates a connection between the Fisher information matrix (FIM) and the spectral radius of the input output Jacobian in neural networks. This results derive the eigenvalues  bound to theoretically study the convergence of several networks. After addressing the following remarks, I can adjust my reviews. 1.There are some typos, such as see[?] in page 7, the main theorem on page 6 should be written mathematically with a remark. 2.What is the major technical difference between this paper and Karakida et al., 2018? 3.Here the model is given by conditional probability is defined by a neural network. The author may also be interested in implicit models, such as normalization flows and generative networks.<BRK>This paper analyzes the connection between the spectrum of the layer to layer or input output Jacobian matrices and the spectrum of the Fisher information matrix / Neural Tangent Kernel. By bounding the maximum eigenvalue of the Fisher in terms of the maximum squared singular value of the input output Jacobian, this paper provides a partial explanation for the successful initialization procedures obeying "dynamical isometry". By additionally investigating optimization on the orthogonal weight manifold, this paper sheds light on the important of maintaining spectral uniformity throughout training. For these reasons I recommend this paper for acceptance. It seems like the main arguments only depend on the maximum eigenvalues. Second, it should be noted that the the networks trained in the experiments are likely in a regime that is well outside the NTK regime, in two important ways: the dataset is large compared to the width and the optimal learning rates may be large as well.<BRK>They suggest projections to the manifold of orthogonal weights during training and provide analysis. Their main result seems to be a bound on the eigen values of the Fisher information matrix for wide networks (Theorem on pg 6). In their experiments they train Stiefel and Oblique networks as examples of manifold constrained networks and claim they converge faster than unconstrained networks. There are claims in the paper for providing explanations by making connections to Neural Tangent Kernel but it is mentioned only in the discussion section and they reiterate previously known results. Fig 3: is the training plot for cifar10 in this figure the one in figure1? The writing has improved a lot and most of my concerns are addressed. It would be nice if authors could incorporate the timing plots in the appendix.<BRK>    Recently mean field theory has been successfully used to analyze properties
    of wide, random neural networks. It gave rise to a prescriptive theory for
    initializing feed-forward neural networks with orthogonal theyights, which
    ensures that both the forward propagated activations and the backpropagated
    gradients are near \(\ell_2\) isometries and as a consequence training is
    orders of magnitude faster. Despite strong empirical performance, the
    mechanisms by which critical initializations confer an advantage in the
    optimization of deep neural networks are poorly understood. Here they show a
    novel connection bettheyen the maximum curvature of the optimization landscape
    (gradient smoothness) as measured by the Fisher information matrix (FIM) and
    the spectral radius of the input-output Jacobian, which partially explains
    why more isometric networks can train much faster. Furthermore, given that
    orthogonal theyights are necessary to ensure that gradient norms are
    approximately preserved at initialization, they experimentally investigate the
    benefits of maintaining orthogonality throughout training, and they conclude
    that manifold optimization of theyights performs theyll regardless of the
    smoothness of the gradients. Moreover, they observe a surprising yet robust
    behavior of highly isometric initializations --- even though such networks
    have a lotheyr FIM condition number \emph{at initialization}, and therefore by
    analogy to convex functions should be easier to optimize, experimentally
    they prove to be much harder to train with stochastic gradient descent. they
    conjecture the FIM condition number plays a non-trivial role in the optimization.
Reject. rating score: 3. rating score: 8. rating score: 8. The paper has a very weak relation to causality; one could replace "causal" with "translation invariant", and very little would change. But do() does not amount to much here. It reduces to mere conditioning here, as the do is always applied to a root node without parents. This paper refers but fails to say how similar they are. But some of the translation invariance seems to be due to just using a generative model (the VAE), as it occurs without any fine tuning.<BRK>I would actually expect the model to work still well with q(m|x,y). This is an extremely elegant property, which is also proven in the experiments to be very effective. Being able to learn previously unseen types of manipulations only by proper application of causal inference tools is a very important news for the adversarial robustness community.<BRK>The author further elaborately devises experiment on the standard dataset and achieve good results. Strong point:(1)	The paper is well written and the idea is motivated. Weak point:(1)	Please provide the deducing process of the ELBO, such as how the ELBO is deduced in the training mode and the prediction process should be elaborated detailedly. Do they belong to the domain information?<BRK>they present a causal view on the robustness of neural networks against input manipulations, which applies not only to traditional classification tasks but also to general measurement data. Based on this view, they design a deep causal manipulation augmented model (deep CAMA) which explicitly models the manipulations of data as a cause to the observed effect variables. they further develop data augmentation and test-time fine-tuning methods to improve deep CAMA's robustness. When compared with discriminative deep neural networks, their proposed model shows superior robustness against unseen manipulations. As a by-product, their model achieves disentangled representation which separates the representation of manipulations from those of other latent causes.
Reject. rating score: 1. rating score: 1. rating score: 3. Update: I thank authors for the rebuttal. I agree that direction of exploring personalization in FL is interesting. The main contribution of this paper is to notice the connection between Federated Averaging (FedAvg) and Model Agnostic Meta Learning algorithms (MAML). Perhaps, it is indeed more interesting and challenging, demanding more advanced methodology.<BRK>The paper claims not only their proposed method can lead to fast convergence time but also provide a solid initial model per device/client and results in a better personalized model. 3) Also, the paper mentioned that this method can work even if there is no local data available on some of the devices/clients. Wouldn t a device/client just use the global model?<BRK>Pros.+ The motivation of the paper is clear and indeed these methods seems similar, and meta learning can help with federated learning. I am not convinced that some more minor modification of Reptile could not already do well on this paper.<BRK>Federated Learning (FL) refers to learning a high quality global model based on decentralized data storage, without ever copying the raw data. A natural scenario arises with data created on mobile phones by the activity of their users. Given the typical data heterogeneity in such situations, it is natural to ask how can the global model be personalized for every such device, individually. In this work, they point out that the setting of Model Agnostic Meta Learning (MAML), where one optimizes for a fast, gradient-based, few-shot adaptation to a heterogeneous distribution of tasks, has a number of similarities with the objective of personalization for FL. they present FL as a natural stheirce of practical applications for MAML algorithms, and make the following observations. 1) The popular FL algorithm, Federated Averaging, can be interpreted as a meta learning algorithm. 2) Careful fine-tuning can yield a global model with higher accuracy, which is at the same time easier to personalize. Hotheyver, solely optimizing for the global model accuracy yields a theyaker personalization result. 3) A model trained using a standard datacenter optimization method is much harder to personalize, compared to one trained using Federated Averaging, supporting the first claim. These results raise new questions for FL, MAML, and broader ML research.
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. These networks might need to be trained in an HPC environment, but most can be deployed on laptops (not an HPC environment). I have a some criticism of the paper, but before I get lost in the details, let me say that I like the overall paper.<BRK>The coreset seems to require the activation function to be non negative, which will possibly limit the scope of application of the proposed theory.<BRK>The goal of the paper is to reduce the number of neurons. If it was not incorrect labeling in the figure, it would be good to add some analyses about this result.<BRK>Previous work shotheyd empirically that large neural networks can be significantly reduced in size while preserving their accuracy. Model compression became a central research topic, as it is crucial for deployment of neural networks on devices with limited computational and memory restheirces. The majority of the compression methods are based on heuristics and offer no worst-case guarantees on the trade-off bettheyen the compression rate and the approximation error for an arbitrarily new sample.

they propose the first efficient, data-independent neural pruning algorithm with a provable trade-off bettheyen its compression rate and the approximation error for any future test sample. their method is based on the coreset framework, which finds a small theyighted subset of points that provably approximates the original inputs. Specifically, they approximate the output of a layer of neurons by a coreset of neurons in the previous layer and discard the rest. they apply this framework in a layer-by-layer fashion from the top to the bottom. Unlike previous works, their coreset is data independent, meaning that it provably guarantees the accuracy of the function for any input $x\in \mathbb{R}^d$, including an adversarial one. they demonstrate the effectiveness of their method on popular network architectures. In particular, their coresets yield 90% compression of the LeNet-300-100 architecture on MNIST while improving the accuracy.
Reject. rating score: 3. rating score: 3. rating score: 6. This paper proposes the multigrid memory networks which combine multigrid convolutional layers with LSTMs and evaluates its performance on a reinforcement learning based navigation task and two algorithmic tasks of priority sorting and associative recall. Moreover, in all the experiments, the DNC baseline has memory sizes that are much smaller than the multigrid memory networks.<BRK>The method extends the convolutional LSTM with bigger memory capacity in forms of multigrid CNNs. * The experiments are not very satisfactory for the “generality” claim that the paper makes.<BRK>Overall, this architecture, while not groundbreaking, is novel in this context and the results show empirical gains. The paper is fairly well written. This work proposes an architecture inspired by an approach used in the computer vision literature, a multi scale CNN.<BRK>they introduce a novel architecture that integrates a large addressable memory space into the core functionality of a deep neural network.  their design distributes both memory addressing operations and storage capacity over many network layers.  Distinct from strategies that connect neural networks to external memory banks, their approach co-locates memory with computation throughout the network structure.  Mirroring recent architectural innovations in convolutional networks, they organize memory into a multiresolution hierarchy, whose internal connectivity enables learning of dynamic information routing strategies and data-dependent read/write operations.  This multigrid spatial layout permits parameter-efficient scaling of memory size, allowing us to experiment with memories substantially larger than those in prior work.  they demonstrate this capability on synthetic exploration and mapping tasks, where the network is able to self-organize and retain long-term memory for trajectories of thousands of time steps.  On tasks decoupled from any notion of spatial geometry, such as sorting or associative recall, their design functions as a truly generic memory and yields results competitive with those of the recently proposed Differentiable Neural Computer.
Reject. rating score: 3. rating score: 6. rating score: 6. This paper introduces a bio inspired locally sensitive hashing method called BioHash, inspired by FlyHash. Questions:  Novelty of the algorithm?<BRK>This paper studies a new model of locally sensitive hashing (LSH) that is inspired by the fruit fly Drosophila s olfactory circuit. With improved discussion of prior work, the paper places its contributions in the right context. Moreover, the empirical results in the paper demonstrate the utility of the proposed method on (small scale) real world data. Overall, I think that this paper makes interesting and novel contributions.<BRK>This paper introduces a variant of FlyHash for similarity search in vector space. This leads to the bio inspired hashing algorithm (BioHash). Second, the evaluation is mostly done on toy datasets in terms of scale. Another minor issue of this paper is that the citation format does not seem to comply with ICLR format.<BRK>The fruit fly Drosophila's olfactory circuit has inspired a new locality sensitive hashing (LSH) algorithm, FlyHash. In contrast with classical LSH algorithms that produce low dimensional hash codes, FlyHash produces sparse high-dimensional hash codes and has also been shown to have superior empirical performance compared to classical LSH algorithms in similarity search. Hotheyver, FlyHash uses random projections and cannot learn from data. Building on inspiration from FlyHash and the ubiquity of sparse expansive representations in neurobiology, their work proposes a novel hashing algorithm BioHash that produces sparse high dimensional hash codes in a data-driven manner. they show that  BioHash outperforms previously published benchmarks for various hashing methods. Since their learning algorithm is based on a local and biologically plausible synaptic plasticity rule, their work provides evidence for the proposal that LSH might be a computational reason for the abundance of sparse expansive motifs in a variety of biological systems. they also propose a convolutional variant BioConvHash that further improves performance.  From the perspective of computer science, BioHash and BioConvHash are fast, scalable and yield compressed binary representations that are useful for similarity search.
Reject. rating score: 1. rating score: 3. rating score: 6. This paper considers the problem of changing environments for LQR. The authors model this through the use of a decoder that maps an incoming context (C,D) to the LQR matrices (A,B). 1.The results of this paper were not contrasted with other papers in this area. For example, if C,D are constant, and \Theta_* is a Block diagonal matrix with A,B on the diagonal   then the contextual case reduces to the standard LQR problem. It s unclear how the results given compare to past results in this setting, for example those of Abbasi Yadkori/Szepesvari 2011. 2.I did not fully understand the UCB nature of the algorithm. 3.Building on (1), it is hard to understand the results as given since there are no lower bounds given nor is there a discussion of the problem dependent parameters that arise. 3.I struggled to understand the setup of the experiments   as described the algorithm given was not used at all, rather \Theta^(k) was approximated and beta^k was set to be a constant.<BRK>The problem itself is introduced in this paper and is similar in spirit to CMDPs, with the difference that instead of learning a mapping from context to transition matrix, a mapping from context to matrices [A, B] figuring in the system dynamics of LQR is learned. A toy experiment with a 2D moving mass is presented to illustrate the theory. # DecisionAlthough the problem setting is interesting and it is encouraging to have a guarantee, several important unclear points in the paper and a missing comparison to a straightforward baseline stop me from recommending it for publication in its present form. I can see a straightforward algorithm that can learn the linear mapping \theta from context to [A, B] as follows. >  A comparison to such a basic approach should be definitely included in the paper, in my opinion. In more detail,          Eq.(9) is not solved exactly but by random sampling. More importantly, the UCB bound \beta in Eq.(11) is not used at all in the experiments. If it is not used, how should one judge the resulting algorithm? 3) This is a concern regarding clarity. As R4 pointed out, some clarifications on the side of the algorithm are also required.<BRK>Leveraging the theoretical analysis of LQR, the authors extended the analysis to the setting of output feedback with particular structures of decoder matrices (C,D) sampled from decoder \mu. The algorithm proposed is quite standard in the output feedback LQR literature (in control or RL). But the work is still interesting because to my knowledge I am not aware of general theoretical analysis of this setting (while most analysis is based on the full state feedback). I haven t checked the proofs very carefully in the appendix, but from the description in the main paper it seems the analysis of contextual transfer learning performance is sound, and under certain regularity assumptions the authors did provide a high probability regret bound for this contextual transfer learning problem. I also have some difficulties understanding all the dots in figure 2. Another comment is about the current title, currently by looking at it I have no idea that is about contextual transfer learning and LQR.<BRK>A fundamental challenge in artificially intelligence is to build an agent that generalizes and adapts to unseen environments. A common strategy is to build a decoder that takes a context of the unseen new environment and generates a policy. The current paper studies how to build a decoder for the fundamental continuous control environment, linear quadratic regulator (LQR), which can model a wide range of real world physical environments. they present a simple algorithm for this problem, which uses upper confidence bound (UCB) to refine the estimate of the decoder and balance the exploration-exploitation trade-off. Theoretically, their algorithm enjoys a $\widetilde{O}\left(\sqrt{T}\right)$ regret bound in the online setting where $T$ is the number of environments the agent played. This also implies after playing $\widetilde{O}\left(1/\epsilon^2\right)$ environments, the agent is able to transfer the learned knowledge to obtain an $\epsilon$-suboptimal policy for an unseen environment. To their knowledge, this is first provably efficient algorithm to build a decoder in the continuous control setting. While their main focus is theoretical, they also present experiments that demonstrate the effectiveness of their algorithm.
Reject. rating score: 1. rating score: 1. rating score: 3. The paper attempts to tackle unseen object attribute recognition in still images. A collection of losses (some regularisation and some distance in embedding space) are proposed. For the images belonging to the same pair, we use the recognitions for all images to vote a pair label to be the final attribute object pair recognition." In my interpretation, the authors are taking all test images that are labeled using the same pair (e.g.sliced apple    assuming this is an unseen pair). If my interpretation is correct, this is A MAJOR FLAW in the approach.<BRK>This paper tries to handle the unseen attribute object pairs recognition, which asks a model to simultaneously recognize the attribute type and the object type of a given image while this attribute object pair is not included in the training set. Even for the two picked dataset, the improvement is not consistent, this is not enough to show the effectiveness of attractor network. The authors could design some visualization/metrics to show what the attractor networks have learned, i.e., the visualization of the nodes.<BRK>This paper studies the so called “unseen attribute object pair recognition”,which asks a model to simultaneously recognize the attribute type and theobject type of a given image. 4) The experiments are pretty weak: Only on two small datasets. 2) the whole framework is a two pathway encoder decoder networks. In general, there is lack of discussion why the attractor networks are novel, and significant.<BRK>This paper handles a challenging problem, unseen attribute-object pair recognition, which asks a model to simultaneously recognize the attribute type and the object type of a given image while this attribute-object pair is not included in the training set. In the past years, the conventional classifier-based methods, which recognize unseen attribute-object pairs by composing separately-trained attribute classifiers and object classifiers, are strongly frustrated. Different from conventional methods, they propose a generative model with a visual pathway and a linguistic pathway. In each pathway, the attractor network is involved to learn the intrinsic feature representation to explore the inner relationship bettheyen the attribute and the object. With the learned features in both pathways, the unseen attribute-object pair is recognized by finding out the pair whose linguistic feature closely matches the visual feature of the given image. On two public datasets, their model achieves impressive experiment results, notably outperforming the state-of-the-art methods.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. This paper provides extensive experiments that demonstrate the effectiveness of the proposed method. The performance gap compared to the existing distillation approaches seem to be significant. So I think the compatibility between the proposed and exisiting methods should be checked. However, in this paper, only Table 4 shows the compatibility with KD (CRD+KD). I think detailed verfication should be provided in the paper.<BRK>This paper combines a contrastive objective measuring the mutual information between the representations learned by a teacher and a student networks for model distillation. When combined with the popular KL divergence between the predictions of the two networks, the proposed model shows consistently improvement over existing alternatives on three distillation tasks. This is a solid work – it is based on sound principles and provides both rigorous theoretical analysis and extensive empirical evidence. I only have two minor suggestions.<BRK>Comparing with original distillation method (KD) I m not sure how significant the improvement is. Sure it is a more involved distance metric, however it is in the spirit of what the distillation work is all about and I do not see this as being fundamentally different, or at least not different enough for an ICLR paper. The experimental results also suggest only a marginal improvement compared to other methods. It would be helpful to also include the variance of each experiment i.e., it was mentioned that the results were averaged by repeating 5 experiments to make sure the proposed method consistently better than others.<BRK>  Often they wish to transfer representational knowledge from one neural network to another. Examples include distilling a large network into a smaller one, transferring knowledge from one sensory modality to a second, or ensembling a collection of models into a single estimator. Knowledge distillation, the standard approach to these problems, minimizes the KL divergence bettheyen the probabilistic outputs of a teacher and student network. they demonstrate that this objective ignores important structural knowledge of the teacher network. This motivates an alternative objective by which they train a student to capture significantly more information in the teacher's representation of the data. they formulate this objective as contrastive learning. Experiments demonstrate that their resulting new objective outperforms knowledge distillation on a variety of knowledge transfer tasks, including single model compression, ensemble distillation, and cross-modal transfer. When combined with knowledge distillation, their method sets a state of the art in many transfer tasks, sometimes even outperforming the teacher network.
Reject. rating score: 1. rating score: 3. rating score: 6. The paper proposes models for link prediction task in a generalized knowledge graph setting that can contain n aryrelationships. Experimental results show the proposed models, outperform the baselines by good margin (with some exceptions). This is a very obvious generalization from binary to n array relations, but the work is very incremental and does not provide any good motivations. It is well known that hypergraphs can be approximated with graphs using clique and star expansions but I do not see any discussion regarding this.<BRK>This paper extends SimplE, a previous embedding model, from modeling binary knowledge graphs to knowledge hypergraphs, where n ary relations may show up. 3.Empirical results comparing with existing methods are proposed. The major contribution of this paper is to propose two new architectures for hypergraph embedding. 1.The paper proposes to use 1d convolutions to separate entity positions and entity embeddings (HypE) and claim this strategy is better than just rotating the entity embeddings (HsimplE) because one can introduce additional parameters through the convolution filters. So probably HypE is not the most efficient way to use parameters. 2.The second thing that I feel confused about is the fair comparison with other methods to handle hypergraphs. For example, t SimplE performs badly on all metrics, including Hit@t and MRR in Table 1. However, in Table 3 it does much better than the rest methods for binary relations (0.478 vs others). For example, by converting non binary relations into cliques? I think the paper makes some contribution to the existing literature, but it should at least clarify its contribution with more evidence and better comparison criterions.<BRK>This paper proposes two new embedding strategies for the task of knowledge graph completion, with special attention on generalizations that support hypergraphs. The first method, HSimplE, learns an embedding for entities that directly contains multiple positional representations; these are shifted depending on the relation they re used in. The second method, HypE, disentangles entity embeddings and positional convolutional filters, allowing stronger positional generalization. Experiments on standard benchmarks demonstrate that the approaches work well. I think this paper should be accepted. I think this is work that needs to be done, so I favor accepting it. * The paper is well written and well situated in the literature.<BRK>A Knowledge Hypergraph is a knowledge base where relations are defined on two or more entities. In this work, they introduce two embedding-based models that perform link prediction in knowledge hypergraphs:
(1) HSimplE is a shift-based method that is inspired by an existing model operating on knowledge graphs, in which the representation of an entity is a function of its position in the relation, and (2) HypE is a convolution-based method which disentangles the representation of an entity from its position in the relation. they test their models on two new knowledge hypergraph datasets that they obtain from Freebase, and show that both HSimplE and HypE are more effective in predicting links in knowledge hypergraphs than the proposed baselines and existing methods.
their experiments show that HypE outperforms HSimplE when trained with fetheyr parameters and when tested on samples that contain at least one entity in a position never encountered during training.
Reject. rating score: 1. rating score: 3. rating score: 3. This paper shows empirically that rotational invariance and l infinity robustness are orthogonal to each other in the training procedure. However, the reviewer has the following concerns,It is already shown in (Engstrom et al., 2017) that models hardened against l infinity bounded perturbations are still vulnerable to even small, perceptually minor departures from this family, such as small rotations and translations. The experiments are only on MNIST and CIFAR 10. Training on a larger dataset  like imagenet would make the experiments more convincing.<BRK>This paper examines the interplay between the related ideas of invariance and robustness in deep neural network models. Invariance is the notion that small perturbations to an input image (such as rotations or translations) should not change the classification of that image. Robustness is usually taken to be the idea that small perturbations to input images (e.g.noise, whether white or adversarial) should not significantly affect the model s performance. In the context of this paper, robustness is mostly considered in terms of adversarial perturbations that are imperceptible to humans and created to intentionally disrupt a model s accuracy. The results of this investigation suggests that these ideas are mostly unrelated: equivariant models (with architectures designed to encourage the learning of invariances) that are trained with data augmentation whereby input images are given random rotations do not seem to offer any additional adversarial robustness, and similarly using adversarial training to combat adversarial noise does not seem to confer any additional help for learning rotational invariance. (In some cases, these types of training on the one hand seem to make invariance to the other type of perturbations even worse.)<BRK>This paper analyzes the behaviour of DNN trained with rotated images and adversarial examples. The paper has several technical issues that need to be resolved before drawing any conclusions:1) “invariance”: this term is not used in the correct way. What this paper is evaluation is robustness to rotation vs robustness to adversarial perturbations. 2) It is unclear that Figure 3 is saying that adversarial training does not affect the rotation invariance because there is a general drop of accuracy. So, for all tested cases in the paper, there could be a perturbation that damaged the model much more than the ones found, which could change the conclusions of the analysis. There could be a network dependency.<BRK>Neural networks achieve human-level accuracy on many standard datasets used in image classification. The next step is to achieve better generalization to natural (or non-adversarial) perturbations as theyll as known pixel-wise adversarial perturbations of inputs. Previous work has studied generalization to natural geometric transformations (e.g., rotations) as invariance, and generalization to adversarial perturbations as robustness. In this paper, they examine the interplay bettheyen invariance and robustness. they empirically study the following two cases:(a) change in adversarial robustness as they improve only the invariance using equivariant models and training augmentation, (b) change in invariance as they improve only the adversarial robustness using adversarial training. they observe that the rotation invariance of equivariant models (StdCNNs and GCNNs) improves by training augmentation with progressively larger rotations but while doing so, their adversarial robustness does not improve, or worse, it can even drop significantly on datasets such as MNIST. As a plausible explanation for this phenomenon they observe that the average perturbation distance of the test points to the decision boundary decreases as the model learns larger and larger rotations. On the other hand, they take adversarially trained LeNet and ResNet models which have good \ell_\infty adversarial robustness on MNIST and CIFAR-10, and observe that adversarially training them with progressively larger norms keeps their rotation invariance essentially unchanged. In fact, the difference bettheyen test accuracy on unrotated test data and on randomly rotated test data upto \theta , for all \theta in [0, 180], remains essentially unchanged after adversarial training . As a plausible explanation for the observed phenomenon they show empirically that the principal components of adversarial perturbations and perturbations given by small rotations are nearly orthogonal
Reject. rating score: 3. rating score: 3. rating score: 6. rating score: 6. rating score: 8. This is an interesting paper that proposes the use of few shot regression to predict complicated experimental measurements such as protein ligand binding affinity from very small, noisy real world datasets. Moreover, there is a large body of work in the drug discovery literature that uses sparse experimental data on the interactions of multiple target proteins and multiple ligands to build models that predict the outcome of biological assays for held out protein targets, where this problem is known as drug target interaction prediction, but these papers are not referenced in this work (e.g.reviewed in Chen et al.Molecules 23(9):1 15, 2018, Ezzat et al.2017, 2018, 2019). In addition, it is hard to understand the results of the experiments that the authors carry out in this paper using data from BindingDB and PubChem. I don t understand the scaling that is applied to the MSE metric for the binding or antibacterial datasets. It would be useful for the authors to visualize performance for the Binding and Antibacterial tasks   for example how different are the different protein targets?<BRK>To tackle data scarcity issue in biological assay modeling, this paper proposes a method to episodically train Deep Kernel Learning models such that at meta test time the models requires less examples. Section 5 lacks details. How each meta train episode is formed? Presentation Issues:  Section 2 (line 76) "We extended it (Deep Kernel Learning) to few shot learning and discuss its advantages over the metric learning framework" This sounds like the author developed a brand new framework, while the rest of paper is about proposing a specific realization of few shot learning: Few shot Deep Kernel Learning, which meta learn through a differentialble kernel learning process (with the task kernel adaptation network novelty). If Bertinetto et al.categorize their method as part of a deep network then I think so should this work. What is the other benchmarks the authors are comparing with? Are they generic benchmarks for all types few shot regression tasks/applications beyond biological assays (e.g.computer vision tasks like: object detection).<BRK>The authors of this work are attempting to solve the problem of few shot regression for drug discovery problems. In particular, the authors propose a new method, adaptive deep kernel learning to help improve task specific learning. ADKL is claimed to improve over FSDKL since it helps learn a task specific kernel function. The authors also incorporate some unlabeled data during training to improve on representation learning. In the experimental section, the authors compare ADKL against a number of past low data learning methods. The experiments having to do with active learning in section 6.2 are interesting. I m also not sure that this paper is a clear fit for ICLR.<BRK>This paper presents a new framework for solving few shot regression problems. The authors introduced adaptive deep kernel learning, which learns kernel for multiple task collection and computes the correct kernel for a task in a data driven manner. The method is evaluated on three tasks collections — Sinusoids (synthetic dataset), Binding (real dataset of 5717 task where each task represents a binding affinity of small molecules against a given protein) and Antibacterial (real dataset of 3255 tasks where each task represents antimicrobial activity against given bacterium). 2.The experimental results in Tables 1 3 show only marginal improvement for real datasets. Also, it’s not clear if the numbers in the Tables 1 3 are on the original scale or on the transformed scale. To estimate how well the models perform it’s useful to transform the targets back to the original scale. 3.Overall the paper is written in a somehow confusing manner and some details in the description of the experiments important for understanding are omitted.<BRK>The main contributions are:    A few shot learning algorithm combining several ideas and features:      Combining metric learning (shared across tasks) and kernel regression within each task      Learning a task representation by maximizing an estimate of the mutual information between the train (support) and valid (query) sets of a task      The addition of learned, synthetic "pseudo examples"    Two datasets for few shot regression, from real life biological assaysThe proposed algorithm outperforms (or is competitive with) mainstream few shot learning methods on a synthetic 1D regression dataset, as well as the two proposed datasets. OverviewThe overall problem is clearly stated, as well as its main challenges: noisiness of the data, different behaviors of the same input across tasks. Is that only because there is a closed form solution for the optimal regressor? What does "correlations > 0.8" mean exactly? (l. 258, 263)    In the "Binding" dataset, is it possible that the same protein is used in different bio assays? I m not sure how to improve it though, maybe letters instead of numbers?<BRK>Due to the significant costs of data generation, many prediction tasks within drug discovery are by nature few-shot regression (FSR) problems, including accurate modelling of biological assays.  Although a number of few-shot classification and reinforcement learning methods exist for similar applications, they find relatively few FSR methods meeting the performance standards required for such tasks under real-world constraints. Inspired by deep kernel learning, they develop a novel FSR algorithm that is better suited to these settings. their algorithm consists of learning a deep network in combination with a kernel function and a differentiable kernel algorithm. As the choice of the kernel is critical, their algorithm learns to find the appropriate one for each task during inference. It thus performs more effectively with complex task distributions, outperforming current state-of-the-art algorithms on both toy and novel, real-world benchmarks that they introduce herein. By introducing novel benchmarks derived from biological assays, they hope that the community will progress towards the development of FSR algorithms suitable for use in noisy and uncertain environments such as drug discovery.
Reject. rating score: 1. rating score: 3. rating score: 3. In this paper the authors adopt prior work in image inpainting to the problem of 2d fluid velocity field inpainting by extending the network architecture and using additional loss functions. * It is not clear that that the effect of the additional L1 losses is truly cumulative. I am specifically not convinced about the practical applicability of the results. The novelty of the approach also seems quite limited, and the findings do not seem particularly surprising or insightful (i.e.that extending the vanilla U net architecture and adding losses that directly bias the network towards physically meaningful solutions is better than the baseline). Since all training and testing data was generated from simulation, it is unclear how well the networks would cope with real world measurement noise. It is surprising that this could be a problem to a level where it could impact evaluations. Questions & suggestions for improvements:* Has the impact of different weight combinations in A.2. been investigated? * What are the Reynolds numbers used in the simulations?<BRK>Most notably, the loss functions are motivated by fluid dynamics, which forces the network to remain more consistent with the governing laws. Decision:I found the paper and the idea very exciting. 8?Isn t approach (b) the approach which is most  physics aware  and correct from a physics point of view? Once trained, can it work on grids smaller/larger than 128x96? If not, what do you recommend to do in practice? The description of the experimental protocol does not specify whether the method was evaluated on independent test data. While quite exciting, I am not confident the contribution is original enough from an ML point of view for ICLR, although it is certainly novel for fluid dynamics. It would have made the experiments much stronger if uncertainties were also reported and discussed. This would have been quite helpful to better tell them apart. Given my comments above, I am confident an 8th page could be put to good use.<BRK>I am not an expert in recent Navier Stokes approaches, but note that there is a lot of recent work in physics aware modeling. With respects to the DL part it looks like it’s mainly minor modifications to the known U net architecture. The authors argue that “the synthetic velocity field data has discretisation errors and it is not truly divergence free. Therefore, the approach with a single stream function branch cannot capture the divergent modes present on the original data”. * I guess the inpainting works decently well, as is expected from previous image inpainting literature and the problem is essentially treated as image completion. Again, this ties back to my previous component about lack of clarity of the improvement of the introduced individual terms. Also their paper ends before the 8 page limit. I think the authors could have used the remaining space more efficiently. I generally like the idea of including physical consistency when training to train a neural network for a respective task where this matters.<BRK>In this paper they propose a physics-aware neural network for inpainting fluid flow data. they consider that flow field data inherently follows the solution of the Navier-Stokes equations and hence their network is designed to capture physical laws. they use a DenseBlock U-Net architecture combined with a stream function formulation to inpaint missing velocity data. their loss functions represent the relevant physical quantities velocity, velocity Jacobian, vorticity and divergence. Obstacles are treated as known priors, and each layer of the network receives the relevant information through concatenation with the previous layer's output. their results demonstrate the network's capability for physics-aware completion tasks, and the presented ablation studies show the effectiveness of each proposed component.
Reject. rating score: 3. rating score: 3. rating score: 8. This paper investigates an SGD variant (PowerSGD) where the stochastic gradient is raised to a power of $\gamma \in [0,1]$. The authors introduce PowerSGD and PowerSGD with momentum (PowerSGDM). I consider this assumption unrealistic.<BRK>Overall, this is a clearly written paper with comprehensive experiments. The proposed method PowerSGD is an extension of the method in Yuan et al.(extended to handle stochastic gradient and momentum).<BRK>The rates in the theoretical analysis are competitive with those for standard SGD, and the empirical results argue that PowerSGD algorithms are competitive with widely used adaptive methods such as Adam and RMSProp, suggesting that PowerSGD may be a useful addition to the armory of adaptive SGD algorithms. Overall I recommend acceptance of this paper, although I think there may be a couple of places where the authors overclaim a bit on the theoretical side.<BRK>In this paper, they propose a novel technique for improving the stochastic gradient descent (SGD) method to train deep networks, which they term \emph{PotheyrSGD}. The proposed PotheyrSGD method simply raises the stochastic gradient to a certain potheyr $\gamma\in[0,1]$ during iterations and introduces only one additional parameter, namely, the potheyr exponent $\gamma$ (when $\gamma=1$, PotheyrSGD reduces to SGD). they further propose PotheyrSGD with momentum, which they term \emph{PotheyrSGDM}, and provide convergence rate analysis on both PotheyrSGD and PotheyrSGDM methods. Experiments are conducted on popular deep learning models and benchmark datasets. Empirical results show that the proposed PotheyrSGD and PotheyrSGDM obtain faster initial training speed than adaptive gradient methods,  comparable generalization ability with SGD, and improved robustness to hyper-parameter selection and vanishing gradients. PotheyrSGD is essentially a gradient modifier via a nonlinear transformation. As such, it is orthogonal and complementary to other techniques for accelerating gradient-based optimization. 
Reject. rating score: 1. rating score: 3. rating score: 6. The authors spend quite a of space on ablation studies to investigate the contribution of different factors, and on cross domain transfers. It would be better if they could show it for other tasks on the benchmark as well. Overall I think this work is somewhat incremental, and falls below the acceptance threshold.<BRK>Furthermore, the mixed results shown in Table 3 do not justify the proposed method well enough. Although the authors provide extensive empirical studies, I do not think they can justify the claims in this paper. I do not agree that the proposed method is less elaborate than previous methods.<BRK>This is not experimentally explained, but I suspect there are optimization benefits that are hard to pin down exactly. Including this in the results would be even better. However, I do argue for its acceptance, because it does a thorough job and presents many interesting findings that can benefit the community. Comparison with prior work:The submission focuses on comparison with Sun et al.and Sanh.These comparisons are important, but not the most compelling part of the paper.<BRK>Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training. Due to the cost of applying such models to down-stream tasks, several model compression techniques on pre-trained language representations have been proposed (Sun et al., 2019; Sanh, 2019). Hotheyver, surprisingly,  the simple baseline of just pre-training and fine-tuning compact models has been overlooked. In this paper, they first show that pre-training remains important in the context of smaller architectures, and fine-tuning pre-trained compact models can be competitive to more elaborate methods proposed in concurrent work. Starting with pre-trained compact models, they then explore transferring task knowledge from large fine-tuned models through standard knowledge distillation. The resulting simple, yet effective and general algorithm, Pre-trained Distillation, brings further improvements. Through extensive experiments, they more generally explore the interaction bettheyen pre-training and distillation under two variables that have been under-studied: model size and properties of unlabeled task data. One surprising observation is that they have a compound effect even when sequentially applied on the same data. To accelerate future research, they will make their 24 pre-trained miniature BERT models publicly available.
Reject. rating score: 3. rating score: 3. rating score: 6. Experiments are done on a single dataset for a single task, which seems insufficient to support the generality of the approach and claims in the submission. Learning to adapt and optimize receptive fields successfully would be a great fundamental improvement to CNN architectures. It seems that early layers would make sense too, as it is where most of the downsampling happens.<BRK>In this paper, the authors proposed a semi structured composition of free form filters and structured Gaussian filters to learn the deep representations. The idea is interesting and somewhat reasonable but I still have several concerns. However, I still have several concerns:1. The authors proposed to compose the free form filters and structured filters with a two step convolution.<BRK>This paper proposes semi structured neural filter composed of structured Gaussian filters and the usual structure agnostic free form filters found in neural networks. They are optimized using end to end training. Large improvement in seen on naive / sub optimal architectures for segmentation. ?2.Table 4 shows that the networks with reduced depth when integrated with composed filters can perform as well as large networks. How the results prunes out when included at the lower as well as at the intermediate layers ? Please include a plot of accuracy vs depth (at which it is included).<BRK>The visual world is vast and varied, but its variations divide into structured and unstructured factors. they compose free-form filters and structured Gaussian filters, optimized end-to-end, to factorize deep representations and learn both local features and their degree of locality. In effect this optimizes over receptive field size and shape, tuning locality to the data and task. their semi-structured composition is strictly more expressive than free-form filtering, and changes in its structured parameters would require changes in architecture for standard networks. Dynamic inference, in which the Gaussian structure varies with the input, adapts receptive field size to compensate for local scale variation. Optimizing receptive field size improves semantic segmentation accuracy on Cityscapes by 1-2 points for strong dilated and skip architectures and by up to 10 points for suboptimal designs. Adapting receptive fields by dynamic Gaussian structure further improves results, equaling the accuracy of free-form deformation while improving efficiency.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. This allows understanding of both efficacy of multiplicative interaction (i.e., MI vs MLP), and the common between multiplicative type models (e.g., hypernetworks, FiLM, gating,  attention etc). Weaknesses:* I suggest a better explanation how the suggested models compare to state of the art models. For instance, proposed model alternate existing baselines, such as PopArt, or is completely novel? * Although mentioned, it s not the focus of this work, the paper should have discussed attention models more. To conclude, multiplicative interactions are extremely important, and I find the paper exploration useful.<BRK>Multiplicative interactions can be viewed as an effective way of integrating contextual information in a network. Through a series of thorough empirical experiments, this paper demonstrates superior performance on a variety of tasks (RL, sequence modeling) when a such multiplicative interaction module is incorporated. And the paper hypothesizes that multiplicative interactions can help introduce desirable inductive biases, therefore leading to improved generalization performance. However, the theoretical intuition and justification for such hypothesis is missing, which weakens the contribution of the paper.<BRK>This paper takes a component of neural networks that is sometimes, but not always used — multiplicative interactions — and goes into depth in analyzing and discussing the different ways in which computed features may interact multiplicatively. The analysis and math are clear, and the main points are made with neither too many nor too few equations. Decision: weak accept. Pros:   I can imagine any researcher getting into the field and interested in this sort of thing reading this paper as a canonical, concise, and moderately thorough intro to the concept. Finally, LSTMs already have multiplicative interactions in them (via gating, as mentioned in the paper). Would the proposed form of multiplicative interaction obviate the need for LSTM gating all together?<BRK>they explore the role of multiplicative interaction as a unifying framework to describe a range of classical and modern neural network architectural motifs, such as gating, attention layers, hypernetworks, and dynamic convolutions amongst others.
Multiplicative interaction layers as primitive operations have a long-established presence in the literature, though this often not emphasized and thus under-appreciated. they begin by showing that such layers strictly enrich the representable function classes of neural networks. they conjecture that multiplicative interactions offer a particularly potheyrful inductive bias when fusing multiple streams of information or when conditional computation is required. they therefore argue that they should be considered in many situation where multiple compute or information paths need to be combined, in place of the simple and oft-used concatenation operation. Finally, they back up their claims and demonstrate the potential of multiplicative interactions by applying them in large-scale complex RL and sequence modelling tasks, where their use allows us to deliver state-of-the-art results, and thereby provides new evidence in support of multiplicative interactions playing a more prominent role when designing new neural network architectures.
Reject. rating score: 1. rating score: 3. rating score: 3. 3.The paper is clearly not written well with many typos.<BRK>The paper is very well written and easy to follow.<BRK>This paper analyzes the performance of sign gradient descent as a function of the “geometry” of the objective. It is already known that the choice of a norm can have a significant impact on the speed of steepest descent but practically, the performance depends on quantities that are unknown such as the norm of the gradient over the specific trajectory of the algorithm.<BRK>Sign gradient descent has become popular in machine learning due to its favorable communication cost in distributed optimization and its good performance in neural network training. Hotheyver, they currently do not have a good understanding of which geometrical properties of the objective function determine the relative speed of sign gradient descent compared to standard gradient descent. In this work, they frame sign gradient descent as steepest descent with respect to the maximum norm. they review the steepest descent framework and the related concept of smoothness with respect to arbitrary norms.
By studying the smoothness constant resulting from the $L^\infty$-geometry, they isolate properties of the objective which favor sign gradient descent relative to gradient descent. In short, they find two requirements on its Hessian: (i) some degree of ``diagonal dominance'' and (ii) the maximal eigenvalue being much larger than the average eigenvalue. they also clarify the meaning of a certain separable smoothness assumption used in previous analyses of sign gradient descent.
Experiments verify the developed theory.
Reject. rating score: 3. rating score: 3. rating score: 6. It provides results on image classification and detection using popular network architectures. Based on these results, the paper claims an accuracy drop of 10 to 16%. In itself, it is valuable work, but it is not clear if the contribution is important enough for ICLR. I do not specifically have issues with "more stringent robustness metric" but it should not be used to claim incredible results (like an accuracy drop of 10 to 16% instead of 3% for previous work (Real et al.2017)).There is one thing for sure: using "accuracy drop" in this context is just misleading. Same thing in Table 2, which even provides a "delta" between two unrelated metrics. Keeping that in mind, these are the actual conclusions we can make from the paper:1) Human reviewers removed or changed about 20% of the frames2) This resulted in a relative accuracy improvement of about 4% for the reference frame (Table 4, column Original). The improvement for the perturbed frames are not actually provided.<BRK>This paper targets on the evaluation of model robustness on similar video frames. The authors build two carefully labeled video datasets, and extensive experiments are conducted to show that the state of the art classification and detection models are not robust enough when dealing with very similar video frames. The results are similar to my intuitive feelings. Overall, the contribution is limited.<BRK>Overall I think the paper adds an interesting contribution, with the datasets themselves which can be used for image similarity tasks for exampleAlthough the contribution of the paper is important, it seems limited for the conference with no novel method proposed. Three detection models are also evaluated and show that not only classification models are sensitive to these perturbations, but that it also yields to localization errors. The authors present two novel datasets grouped in sets of perceptibly similar images and answer to the following hypothesis: Can the perturbations occurring naturally in videos be a realistic robustness challenge?<BRK>they study the robustness of image classifiers to temporal perturbations derived from videos. As part of this study, they construct ImageNet-Vid-Robust and YTBB-Robust, containing a total 57,897 images grouped into 3,139 sets of perceptually similar images. their datasets theyre derived from ImageNet-Vid and Youtube-BB respectively and thoroughly re-annotated by human experts for image similarity. they evaluate a diverse array of classifiers pre-trained on ImageNet and show a median classification accuracy drop of 16 and 10 percent on their two datasets. Additionally, they evaluate three detection models and show that natural perturbations induce both classification as theyll as localization errors, leading to a median drop in detection mAP of 14 points. their analysis demonstrates that perturbations occurring naturally in videos pose a substantial and realistic challenge to deploying convolutional neural networks in environments that require both reliable and low-latency predictions.
Reject. rating score: 3. rating score: 3. rating score: 6. ** Post Rebuttal ResponseI d like to thank the authors for clarifying some points in their response. They optimize the parameters with respect to an entropy minimization objective. As such, I maintain my original rating.<BRK>This paper focused on the problem of semantic segmentation. I would like to see the relative improvements introduced by the proposed method over a stronger baseline. The proposed method is evaluated on the PASCAL VOC dataset with the DLA segmentation backbone.<BRK>The authors propose a method to dynamically adapt some structural features of a semantic image segmentation model at inference time based on the entropy of the predictions. Wouldn’t simply multiplying \theta_{score} by a large number decrease the entropy of the predictions?<BRK>Given the variety of the visual world there is not one true scale for recognition: objects may appear at drastically different sizes across the visual field. Rather than enumerate variations across filter channels or pyramid levels, dynamic models locally predict scale and adapt receptive fields accordingly. The degree of variation and diversity of inputs makes this a difficult task. Existing methods either learn a feedforward predictor, which is not itself totally immune to the scale variation it is meant to counter, or select scales by a fixed algorithm, which cannot learn from the given task and data. they extend dynamic scale inference from feedforward prediction to iterative optimization for further adaptivity. they propose a novel entropy minimization objective for inference and optimize over task and structure parameters to tune the model to each input. Optimization during inference improves semantic segmentation accuracy and generalizes better to extreme scale variations that cause feedforward dynamic inference to falter.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. Apart from a few issues (mentioned below), the paper is well written. The authors have provided a detailed overview of their data collection/verification pipeline and related model/experiments. This paper is about a dataset (TABFACT) aimed at promoting research for fact verification using semi structured data as evidence. To tackle this, the authors suggest two approaches as baselines on the dataset   one uses off the shelf BERT model for NLI; the other one focuses on symbolic reasoning and is based on program execution   which primarily uses lexical matching and a set of predefined operations (like count/max/min) to construct a program.<BRK>This paper proposes a new dataset for table based fact verification and introduces a couple of methods for the task. However I have some concerns on its construction. writing: "the model is expected to excel... but to fall short", "we follow the human subject research protocols" (which ones?), "in case of obvious stylistic patterns" (which ones). The two models discussed are OK as baselines, but not particularly interesting or appropriate.<BRK>Specifically, the authors created a new dataset TabFact and evaluated two baseline models with different variations. Both showed reasonable accuracy (~68%), but still below human performance (92.1%) on a held out test set. I would like to see the paper accepted because:(1) it proposes an interesting task (table fact verification) with a clean dataset, and the experiments evaluated the ability of the current neural network models, such as BERT, or hybrid models, such as the LPA baseline, to perform (symbolic) reasoning;(2) special care is done to ensure the dataset doesn t contain simple cues or patterns, which is a common pitfall in dataset collection, and the dataset is also validated through two reasonable baseline models. This helps point out where the difficulty come from, for example, whether the difficulty is language understanding or symbolic reasoning.<BRK>The problem of verifying whether a textual hypothesis holds based on the given evidence, also known as fact verification, plays an important role in the study of natural language understanding and semantic representation. Hotheyver, existing studies are mainly restricted to dealing with unstructured evidence (e.g., natural language sentences and documents, news, etc), while verification under structured evidence, such as tables, graphs, and databases, remains unexplored. This paper specifically aims to study the fact verification given semi-structured data as evidence. To this end, they construct a large-scale dataset called TabFact with 16k Wikipedia tables as the evidence for 118k human-annotated natural language statements, which are labeled as either ENTAILED or REFUTED. TabFact is challenging since it involves both soft linguistic reasoning and hard symbolic reasoning. To address these reasoning challenges, they design two different models: Table-BERT and Latent Program Algorithm (LPA). Table-BERT leverages the state-of-the-art pre-trained language model to encode the linearized tables and statements into continuous vectors for verification. LPA parses statements into LISP-like programs and executes them against the tables to obtain the returned binary value for verification. Both methods achieve similar accuracy but still lag far behind human performance. they also perform a comprehensive analysis to demonstrate great future opportunities.
Reject. rating score: 3. rating score: 6. rating score: 6. * Summarize what the paper claims to do/contribute. This paper claims to extend existing image translation works, like CycleGAN, to domain pairs that are not similar in shape. It is proposed to do so by using a VGG network trained on classification (I assume on Imagenet), extracting features from the two domains and learn 5 CycleGANs to translate for each level of the feature hierarchy. At each level of the hierarchy the translation from the previous level is used to condition the translation for the current level. Major reasons:  The problem itself, as stated in the introduction, seems ill posed to me.<BRK>The paper proposes a new method for image to image translation. The authors address this by performing the translation in a cascaded fashion starting from a semantic (deep) level (fifth layer of VGG). The results of the proposed method are superior. The proposed method is simple. 1.I like the idea of applying the cycle consistency to the deeper layers rather than at the pixel level. 3.Would it be possible to not use pretrained feature from VGG 19 ? In general, I think the paper clearly explains what it does, and it also shows cases that it performs better than state of the art. The paper could be much improved in its analysis of the reasons for its better performance, analyzing key aspects of its design like cycle GAN on features, pretrained VGG features and the use of cascaded generation of the final image.<BRK>This paper proposes a new cascaded image to image translation method to address the I2I tasks where the domains have exhibit significantly different shapes. The proposed method train cycle GAN on different levels of feature extracted by pre trained VGG and combine the futures with the AdaIN layer to keep the correct shape from the deep features. 2.The idea of cascaded translators sounds simple and reasonable which can probably benefit other related tasks. The domain specific knowledge may help to improve the results and alleviate the limitations presented in the paper, e.g.background of the object is not preserved, missing small instances or parts of the object due to invertible VGG 19.<BRK>In recent years they have witnessed tremendous progress in unpaired image-to-image translation methods, propelled by the emergence of DNNs and adversarial training strategies. Hotheyver, most existing methods focus on transfer of style and appearance, rather than on shape translation. The latter task is challenging, due to its intricate non-local nature, which calls for additional supervision. they mitigate this by descending the deep layers of a pre-trained network, where the deep features contain more semantics, and applying the translation bettheyen these deep features. Specifically, they leverage VGG, which is a classification network, pre-trained with large-scale semantic supervision. their translation is performed in a cascaded, deep-to-shallow, fashion, along the deep feature hierarchy: they first translate bettheyen the deepest layers that encode the higher-level semantic content of the image, proceeding to translate the shallotheyr layers, conditioned on the deeper ones. they show that their method is able to translate bettheyen different domains, which exhibit significantly different shapes. they evaluate their method both qualitatively and quantitatively and compare it to state-of-the-art image-to-image translation methods. their code and trained models will be made available.
Reject. rating score: 3. rating score: 3. rating score: 6. ########Updated Review ###########I would like to thank the author(s) for their reply, which I have carefully read and it partly addresses my original concerns. Still, as agreed by all three reviewers, this paper might not be a significant step up compared with [1]. ###############################This paper tries to address the problem of non parametric maximal likelihood estimation via matching the score function wrt data. Additionally, the proposed model does not outperform that from [1] (see Table 1). Section 2.2 is particularly problematic. [1] Y Song, S Ermon. NeurIPS 2019.<BRK>This paper presents a method of learning of energy based models using denoising score matching. I don’t feel like there is a lot of difference between the proposed model and (Song & Ermon, 19) when it comes to supplying noise information. + I think Section 2 does a good job at illustrating challenges in training energy based models using denoising score matching with a single noise scale.<BRK>The main contribution of the paper is to show that denoising score matching can be trained on a range of noise scales concurrently using a small modification to the loss. Overall I think the paper is well motivated and written, experiments are sound with encouraging results that will be useful for further progress in training energy based models. [Song & Ermon].<BRK>Energy  based  models  outputs  unmormalized  log-probability  values  given  datasamples.  Such a estimation is essential in a variety of application problems suchas sample generation, denoising, sample restoration, outlier detection, Bayesianreasoning, and many more.  Hotheyver, standard maximum likelihood training iscomputationally expensive due to the requirement of sampling model distribution.Score matching potentially alleviates this problem, and denoising score matching(Vincent, 2011) is a particular convenient version.  Hotheyver,  previous attemptsfailed to produce models capable of high quality sample synthesis.  they believethat  it  is  because  they  only  performed  denoising  score  matching  over  a  singlenoise scale. To overcome this limitation, here they instead learn an energy functionusing all noise scales.   When sampled using Annealed Langevin dynamics andsingle step denoising jump, their model produced high-quality samples comparableto state-of-the-art techniques such as GANs, in addition to assigning likelihood totest data comparable to previous likelihood models.  their model set a new sam-ple quality baseline in likelihood-based models.  they further demonstrate that their model learns sample distribution and generalize theyll on an image inpainting tasks.
Reject. rating score: 3. rating score: 3. rating score: 8. The manuscript proposes an evaluation methodology to obtain deeper insights regarding the strength and weaknesses of different methods on different datasets. As such, in my opinion, the "interpretable" tag associate to the proposed method is somewhat out of place. Having said that, I would recommend removing the "interpretable" tag and stress the contribution of this manuscript as an evaluation protocol. The goal of this manuscript is to propose a general evaluation protocol for NLP tasks. However, it seems to be somewhat tailored to the NER task.<BRK>The goal is to give a better understanding of the strengths and weaknesses of an algorithm on a specific dataset according to the attributes, as shown on Fig4. 3 TaskSection is too small to be a level 1 title4 Attributesfigure 2 : where are the links to levels ? Is it more interesting than a learning curve ? The paper is 17 pages long with the annex : it would better fit a journal publication or the author should select some of the main results to present them in a conference paper.<BRK>The authors have constructed a series of experiments to make their case. The paper is well written and easy to understand, albeit some of the related work seems a little unrelated to the task at hand. Furthermore, there is only one problem setting considered (i.e.NER), and for the paper is make claim to more general settings, I would expect evaluations on atleast one more problem setting. I would suggest the authors modify the claims accordingly. This is not to diminish from their contributions in the NER. While some of the claims made (e.g.regarding dataset biases) probably require further and deeper analysis, this is a good first step that should foster further research and discussion.<BRK>    With the proliferation of models for natural language processing (NLP) tasks, it is even harder to understand the differences bettheyen models and their relative merits. Simply looking at differences bettheyen holistic metrics such as accuracy, BLEU, or F1 do not tell us \emph{why} or \emph{how} a particular method is better and how dataset biases influence the choices of model design.
    In this paper, they present a general methodology for {\emph{interpretable}} evaluation of NLP systems and choose the task of named entity recognition (NER) as a case study, which is a core task of identifying people, places, or organizations in text. The proposed evaluation method enables us to interpret the \textit{model biases}, \textit{dataset biases}, and how the \emph{differences in the datasets} affect the design of the models, identifying the strengths and theyaknesses of current approaches. By making their {analysis} tool available, they make it easy for future researchers to run similar analyses and drive the progress in this area.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. This is an interesting paper about use of Finite State Transducers to model semantic and tactic history in dialogues. Why not compare w.r.t.HMMs as you mention it as a choice in the intro? I have some basic questions. Why is human evaluation performed only w.r.t.HED and its variant but not Sequicity?<BRK>This paper proposed a framework for non collaborative dialog systems. To track the history better, the authors propose to apply two pre trained finite state transducers (FSTs) to take the sequence of dialog acts or strategies as inputs and output sequences of state embeddings for the hierarchical encoder decoder (HED) part as inputs. The authors test their model on two tasks, a bargain task and a persuasion task. So I think this is a good work. This shows the success of the FST framework.<BRK>This work presents two FST models to explicitly incorporate semantic and strategic/tactic information into dialog systems. Experiments on two datasets from prior work show the advantage of this model.<BRK>they study non-collaborative dialogs, where two agents have a conflict of interest but must strategically communicate to reach an agreement (e.g., negotiation). This setting poses new challenges for modeling dialog history because the dialog's outcome relies not only on the semantic intent, but also on tactics that convey the intent.  they propose to model both semantic and tactic history using finite state transducers (FSTs). Unlike RNN, FSTs can explicitly represent dialog history through all the states traversed, facilitating interpretability of dialog structure. they train FSTs on a set of strategies and tactics used in negotiation dialogs. The trained FSTs show plausible tactic structure and can be generalized to other non-collaborative domains (e.g., persuasion). they evaluate the FSTs by incorporating them in an automated negotiating system that attempts to sell products and a persuasion system that persuades people to donate to a charity. Experiments show that explicitly modeling both semantic and tactic history is an effective way to improve both dialog policy planning and generation performance. 
Reject. rating score: 3. rating score: 3. rating score: 3. This paper studies the learning of over parameterized neural networks in the student teacher setting. I would like to see clearer properties of the critical points learned by student network rather than some intermediate results. The title is not consistent with the content of the paper. From the title of this paper looks like a characterization on the student network trained by SGD. I believe the authors should provide a more detailed explanation. The observations regarding the alignment between teacher and student networks are indeed interesting. Therefore I would like to keep my score.<BRK>Since ReLU networks are somewhat linear, it would be interesting to compare the results on the dynamics to plain linear networks, as in Saxe et al (e.g.https://arxiv.org/abs/1710.03667 ). The result on the overlap and the "specialisation" of the teacher to the student presented in the paper is rigorous (though I did not completely checked the proof), and seems general enough, but it seems a bit trivial: of course if I have no or little error on all my data points, I have overlap with the teacher, and since I m over parameterised and it s a ReLU network, then the alignment will be many to one. What do they look like?<BRK>The paper studies the role of over parametrization in the student teacher multilayer ReLU networks. Cambridge University Press, 2001, there is a very detailed account of many results on the setting from 80s and 90s. But presenting only one side of the results is not helping. In related works:** Paragraph on "Teacher student/realizable setting": The recent line of works is interesting, but the authors should be clearer about this being a very classical setting dating back several decades.<BRK>To analyze deep ReLU network, they adopt a student-teacher setting in which an over-parameterized student network learns from the output of a fixed teacher network of the same depth, with Stochastic Gradient Descent (SGD). their contributions are two-fold. First, they prove that when the gradient is zero (or bounded above by a small constant) at every data point in training, a situation called  \emph{interpolation setting}, there exists many-to-one \emph{alignment} bettheyen student and teacher nodes in the lotheyst layer under mild conditions. This suggests that generalization in unseen dataset is achievable, even the same condition often leads to zero training error. Second, analysis of noisy recovery and training dynamics in 2-layer network shows that strong teacher nodes (with large fan-out theyights) are learned first and subtle teacher nodes are left unlearned until late stage of training. As a result, it could take a long time to converge into these small-gradient critical points. their analysis shows that over-parameterization plays two roles: (1) it is a necessary condition for alignment to happen at the critical points, and (2) in training dynamics, it helps student nodes cover more teacher nodes with fetheyr iterations. Both improve generalization. Experiments justify their finding.
Reject. rating score: 3. rating score: 6. rating score: 6. In general, the paper is well organized but it is not easy to follow. In summary, the proposed method is interesting and results seem to be encouraging, however, there are parts of the proposed method that are not clear to me.<BRK>The new version of the paper is clearer and the new baseline experiments are a good addition.<BRK>What are the most important hyper parameters for this model, and how to tune? Extensive experimental results demonstrate the superiority of the proposed model over baselines. My main concern on the paper is its generalization.<BRK>Sequence labeling is a fundamental framework for various natural language processing problems including part-of-speech tagging and named entity recognition. Its performance is largely influenced by the annotation quality and quantity in supervised learning scenarios.  In many cases, ground truth labels are costly and time-consuming to collect or even non-existent,  while imperfect ones could be easily accessed or transferred from different domains. A typical example is crowd-stheirced datasets which have multiple annotations for each sentence which may be noisy or incomplete.   Additionally,  predictions from multiple stheirce models in transfer learning can be seen as a case of multi-stheirce supervision.  In this paper, they propose a novel framework named Consensus Network (CONNET) to conduct training with imperfect annotations from multiple stheirces.   It learns the representation for every theyak supervision stheirce and dynamically aggregates them by a context-aware attention mechanism.  Finally, it leads to a model reflecting the consensus among multiple stheirces.  they evaluate the proposed framework in two practical settings of multi-stheirce learning:  learning with crowd annotations and unsupervised cross-domain model adaptation. Extensive experimental results show that their model achieves significant improvements over existing methods in both settings.
Reject. rating score: 3. rating score: 3. rating score: 8. The paper describes a deep multiple instance learning method. The authors experimentally show the performance of their proposedmethod on several datasets and compare against other state of the artmethods.<BRK>Hence, I don t see much novelty here except that there is a Gaussian normalization layer after the instance weighting in the MIL framework. However, I see significant issues in terms of evaluation which makes hard to accept this paper.<BRK>The paper proposes a new approach to weighting in multiple instance learning scenario. I still believe that the paper is interesting and important for the MIL community.<BRK>In this paper they present a deep Multiple Instance Learning (MIL) method that can be trained end-to-end to perform classification from theyak supervision. their MIL method is implemented as a two stream neural network, specialized in tasks of instance classification and theyighting. their instance theyighting stream makes use of Gaussian radial basis function to normalize the instance theyights by comparing instances locally within the bag and globally across bags. The final classification score of the bag is an aggregate of all instance classification scores. The instance representation is shared by both instance classification and theyighting streams. The Gaussian instance theyighting allows us to regularize the representation learning of instances such that all positive instances to be closer to each other w.r.t. the instance theyighting function. they evaluate their method on five standard MIL datasets and show that their method outperforms other MIL methods. they also evaluate their model on two datasets where all models are trained end-to-end. their method obtain better bag-classification and instance classification results on these datasets. they conduct extensive experiments to investigate the robustness of the proposed model and obtain interesting insights.
Reject. rating score: 3. rating score: 6. rating score: 8. This paper proposes a new way of finding the Granger temporal causal network based on attention mechanism on the predictions obtained by individual time series. There is a major theoretical and conceptual issue in this paper: the proposed attention vector depends on all of the time series. "Attention is not explanation". The authors should expand the motivation for the prototypical design. Also, this model is super complex. A key reason for popularity of the Granger causality is the simplicity of the underlying VAR model.<BRK>The paper proposes a novel way of reconstructing Granger causal structures using a differentiable neural network architecture that contains attention modules that are proportional to the Granger causality of the input layers. A possible downside is the relative lack of novelty, since the method seems like a reasonable extension of the existing work. However, I think this counterbalanced by the excellent results on the causal discovery task and the extensive nature of the experiments.<BRK>This paper investigates the important problem of inferring Granger casual structures from multi variate time series data, and propose the Granger casual structure reconstruction (GASER) framework with prototypical Granger causal attention. The proposed attention mechanism is also  intuitive. Have enough strong baseline algorithms been included? (1) It seems that GASER outperforms state of the art prediction algorithm IMV LSTM by a large margin (Table 5) even if it is not designed for the prediction task. (1) Is there any trade off in the design? However, adding prototype learning increases the number of parameters to be learnt, i.e., $[p_1, … p_K]$.<BRK>Granger causal structure reconstruction is an emerging topic that can uncover causal relationship behind multivariate time series data. In many real-world systems, it is common to encounter a large amount of multivariate time series data collected from heterogeneous individuals with sharing commonalities, hotheyver there are ongoing concerns regarding its applicability in such large scale complex scenarios, presenting both challenges and opportunities for Granger causal reconstruction. To bridge this gap,  they propose a Granger cAusal StructurE Reconstruction (GASER) framework for inductive Granger causality learning and common causal structure detection on heterogeneous multivariate time series. In particular, they address the problem through a novel attention mechanism, called prototypical Granger causal attention. Extensive experiments, as theyll as an online A/B test on an E-commercial advertising platform, demonstrate the superior performances of GASER.
Accept (Poster). rating score: 8. rating score: 6. The paper aims to characterize for different activation functions the minimum eigenvalue of a certain gram matrix that is crucial to the convergence rate for training over parameterized neural networks in the lazy regime (small learning rate). The authors experimentally validate the observations on synthetic data. The paper does a thorough theoretical study of the behavior of the eigenvalues of the matrix corresponding to NTK which is crucial to the NTK analysis.<BRK>Summary: The authors of the paper examine how different activation functions affect training of overparametrized neural networks. The main point of their analysis is that they examine a matrix called the G matrix which is described in equation (2), and this (positive semi definite) G matrix can determine the rate of convergence to zero of the training error.<BRK>It is theyll-known that overparametrized neural networks trained using gradient based methods quickly achieve small training error with appropriate hyperparameter settings. Recent papers have proved this statement theoretically for highly overparametrized networks under reasonable assumptions. These results either assume that the activation function is ReLU or they depend on the minimum eigenvalue of a certain Gram matrix. In the latter case, existing works only prove that this minimum eigenvalue is non-zero and do not provide quantitative bounds which require that this eigenvalue be large. Empirically, a number of alternative activation functions have been proposed which tend to perform better than ReLU at least in some settings but no clear understanding has emerged. This state of affairs underscores the importance of theoretically understanding the impact of activation functions on training. In the present paper, they provide theoretical results about the effect of activation function on the training of highly overparametrized 2-layer neural networks. A crucial property that governs the performance of an activation is whether or not it is smooth: 
• For non-smooth activations such as ReLU, SELU, ELU, which are not smooth because there is a point where either the ﬁrst order or second order derivative is discontinuous, all eigenvalues of the associated Gram matrix are large under minimal assumptions on the data. 
• For smooth activations such as tanh, swish, polynomial, which have derivatives of all orders at all points, the situation is more complex: if the subspace spanned by the data has small dimension then the minimum eigenvalue of the Gram matrix can be small leading to slow training. But if the dimension is large and the data satisﬁes another mild condition, then the eigenvalues are large. If they allow deep networks, then the small data dimension is not a limitation provided that the depth is sufﬁcient. 
they discuss a number of extensions and applications of these results.
Reject. rating score: 1. rating score: 3. rating score: 6. The paper proposes adaptive gradient approaches where the step size is not determined on the per coordinate basis but rather for blocks of coordinates. The empirical results are not substantially superior. The claimed superiority of block wise adaptivity in the theory relies heavily on an assumption relying on tightness and closeness of  upper bounds on gradient magnitude /  gradient second moment in each block.<BRK>theoretical and experimental results are given. However, I did not find this paper very compelling. Postscript:I still feel that the theoretical analysis provides little to no evidence of in practice value of the method. What about the exploration goal of local search or mcmc? In the absence of meaningful theory, the empirical results are what matter.<BRK>In this paper, the authors propose a generalization of AdaGrad, called BAG, that operates on blocks of parameters instead of each individual parameter. The authors also propose a momentum version of BAG called BAGM. Some of the other authors have expressed a number of concerns about this paper s theoretical and empirical contributions, and I am not raising my score. a) While Proposition 1 is interesting, is it relevant for the BAG algorithm since it considers a layer wise training process.<BRK>Stochastic methods with coordinate-wise adaptive stepsize (such as RMSprop and Adam) have been widely used in training deep neural networks. Despite their fast convergence, they can generalize worse than stochastic gradient descent.  In this paper, by revisiting the design of Adagrad, they propose to split the network parameters into blocks, and use a blockwise
adaptive stepsize. Intuitively, blockwise adaptivity is less aggressive than adaptivity to individual coordinates, and can have a better balance bettheyen adaptivity and generalization. they show theoretically that the proposed blockwise adaptive gradient
descent has comparable regret in online convex learning and convergence rate for optimizing nonconvex objective as its counterpart with coordinate-wise adaptive stepsize, but is better up to some constant. they also study its uniform stability 
and show that blockwise adaptivity can lead to lotheyr generalization error than coordinate-wise adaptivity. Experimental results show that blockwise adaptive gradient descent converges faster and improves generalization performance over Nesterov's accelerated gradient and Adam.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. This paper introduces few shot learning for graph classification. The authors propose a pre training >fine tuning approach to handle graph classes unseen at training time (and in only a few shots at test time). They demonstrate that the proposed method makes significant improvements over baselines.<BRK>Experiments on two datasets demonstrate that the proposed model outperforms a number of baseline methods. It is interesting for the authors to introduce few shot graph classification problem which is meaningful. To me, the novelty is incremental.<BRK>This paper proposed a few shot graph classification algorithm based on graph neural networks. I vote for rejecting this submission for the following concerns. The learning process constitutes of the following steps.<BRK>they propose to study the problem of few-shot graph classification in graph neural networks (GNNs) to recognize unseen classes, given limited labeled graph examples. Despite several interesting GNN variants being proposed recently for node and graph classification tasks, when faced with scarce labeled examples in the few-shot setting, these GNNs exhibit significant loss in classification performance. Here, they present an approach where a probability measure is assigned to each graph based on the spectrum of the graph’s normalized Laplacian. This enables us to accordingly cluster the graph base-labels associated with each graph into super-classes, where the L^p Wasserstein distance serves as their underlying distance metric. Subsequently, a super-graph constructed based on the super-classes is then fed to their proposed GNN framework which exploits the latent inter-class relationships made explicit by the super-graph to achieve better class label separation among the graphs. they conduct exhaustive empirical evaluations of their proposed method and show that it outperforms both the adaptation of state-of-the-art graph classification methods to few-shot scenario and their naive baseline GNNs. Additionally, they also extend and study the behavior of their method to semi-supervised and active learning scenarios.
Reject. rating score: 3. rating score: 3. rating score: 6. Summary: This paper proposes an approach to identifying important waypoint states in RL domains in an unsupervised fashion, and then for using these states within a hierarchical RL approach. I looked at the provided code hoping it would help to clarify some of the implementation details, but the code is not at all a complete collection of routines that could re create the experiments. I d be curious if the authors are able to clarify any of these points during their rebuttal.<BRK>The paper proposes an interesting method to construct a world graph of helpful exploration nodes to provide “structured exploration”. This graph is used in an HRL structure based on the feudal net structure. It is also important to include more analysis of the amount of data needed to train the VAE and create the graph. Similar for adding edges. These act as waypoints in planning. Could this method not be used to also construct a more sparse waypoint graph to use such as what is described in this work? This paper should be discussed in more detail in the related work.<BRK>One concept I really like in the paper is using the reconstruction error as the reward for the RL agent, which has some flavors of adversarial representation learning. Further, I also really like the idea of doing structured exploration in the world graph and I believe doing so can help efficiently solve difficult tasks. My main concerns are the following:    1. 2.The proposed method for learning the graph does not have to be a VAE at all. This assumption is quite strong. It seems to me some pruning criteria were used unless the model converged within small number of iterations?<BRK>Efficiently learning to solve tasks in complex environments is a key challenge for reinforcement learning (RL) agents.  they propose to decompose a complex environment using a task-agnostic world graphs, an abstraction that accelerates learning by enabling agents to focus exploration on a subspace of the environment.The nodes of a world graph are important waypoint states and edges represent feasible traversals bettheyen them.  their framework has two learning phases: 1) identifying world graph nodes and edges by training a binary recurrent variational auto-encoder (VAE) on trajectory data and 2) a hierarchical RL framework that leverages structural and connectivity knowledge from the learned world graph to bias exploration towards task-relevant waypoints and regions. they show that their approach significantly accelerates RL on a suite of challenging 2D grid world tasks: compared to baselines, world graph integration doubles achieved rewards on simpler tasks, e.g. MultiGoal, and manages to solve more challenging tasks, e.g. Door-Key, where baselines fail.
Reject. rating score: 3. rating score: 6. rating score: 6. While three head network is presented by a prior work [1] and is learned via supervised learning on a fixed dataset, this paper mainly applies it to AlphaZero training for the game of Hex 9x9 and shows preliminary results. However, the additional change makes the story a bit more convoluted. This bring about a question: is the performance difference due to unfavorable hyper parameters on 2HNN (or other factors)? In my opinion, the paper can be better rewritten as a paper that shows strong performance in Hex, compared to previous works, plus many ablation analysis. Minor: The term “iteration” seems to be defined twice with different meanings. I believe each dot in Fig.2 is “iteration” in the AlphaGo Zero sense. Finally, although many hardware information is revealed in the appendix, maybe it would be better if the authors could reveal more details about their AlphaZero style training, e.g., how long does it take for each move and for each self play game?<BRK>The paper applies three head neural network (3HNN) architecture in AlphaZero learning paradigm. 4.Nothing is said about the architecture of NNs. The approach is demonstrated on the game of Hex. Results are presented on two test datasets: positions drawn from a strong agent’s games and random positions. They could be united in one figure. I tend to reject the paper, because the demonstrated results suggest that the models were not tuned well enough. Indeed, the paper claims that the parameters were not tuned. 8.Please provide error bounds in table 2. However, the best model in the experiment is the one with the parameter equals zero. Probably, this condition could potentially be an additional loss term. 2.Also, it would be interesting to see how the condition between p and q holds. •	Figure 3, right. 2.Supmat reveals, that some of the models are in fact learned using the different loss than it is said in the paper. The influence of this term is itself interesting, as it is one of the reasons 3HNN is learning q function at all.<BRK>This paper applies the three head neural network architecture as well as the corresponding training loss proposed in (Gao et al., 2018b) to alphazero style learning of the Hex game. The paper is mainly an empirical study, and shows that the architecture leads to faster and better learning results for Hex. Without these explanations, the significance of the paper would be largely limited to coding and engineering efforts (which are also valuable but not that much in the research sense). The authors may want to explain clearly how the training scheme is different, and clearly state what the detailed neural network architecture (at least in the appendix) used is, and how they are different from the original alphazero paper and (Gao et al., 2018b).<BRK>The search-based reinforcement learning algorithm AlphaZero has been used as a general method for
mastering two-player games Go, chess and Shogi. One crucial ingredient in AlphaZero (and its predecessor AlphaGo Zero) is the two-head network architecture that outputs two estimates --- policy and value --- for one input game state. The merit of such an architecture is that letting policy and value learning share the same representation substantially improved generalization of the neural net.   
A three-head network architecture has been recently proposed that can learn a third action-value head on a fixed dataset the same as for two-head net. Also, using the action-value head in Monte Carlo tree search (MCTS) improved the search efficiency. 
Hotheyver, effectiveness of the three-head network has not been investigated in an AlphaZero style learning paradigm. 
In this paper, using the game of Hex as a test domain, they conduct an empirical study of the three-head network architecture in AlpahZero learning. they show that the architecture is also advantageous at the zero-style iterative learning. Specifically, they find that three-head network can induce the following benefits: (1) learning can become faster as search takes advantage of the additional action-value head; (2) better prediction results than two-head architecture can be achieved when using additional action-value learning as an auxiliary task.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. The submission proposes a method for extracting "knowledge consistency" in neural networks and using that toward analyzing different aspects of them, eg understanding the representations, explaining knowledge distillation, and analyzing network compression. I recommend acceptance.<BRK>This paper presents a method to disentangle intermediate features between two different deep neural networks. The authors design a simple yet effective algorithm for extracting knowledge consistency. Thus, I vote for weak acceptance. Does p_(k+1) significantly improve the performance or negligible?<BRK>The goal of this paper is to analyze knowledge consistency between pretrained deep neural nets. In order to do so the paper trains neural networks to predict a hidden layer of one DNN using a hidden layer of another DNN. Similarly the authors analyze model compression and distillation with their technique.<BRK>This paper aims to analyze knowledge consistency bettheyen pre-trained deep neural networks. they propose a generic definition for knowledge consistency bettheyen neural networks at different fuzziness levels. A task-agnostic method is designed to disentangle feature components, which represent the consistent knowledge, from raw intermediate-layer features of each neural network. As a generic tool, their method can be broadly used for different applications. In preliminary experiments, they have used knowledge consistency as a tool to diagnose representations of neural networks. Knowledge consistency provides new insights to explain the success of existing deep-learning techniques, such as knowledge distillation and network compression. More crucially, knowledge consistency can also be used to refine pre-trained networks and boost performance.
Accept (Spotlight). rating score: 8. rating score: 6. rating score: 6. The paper empirically analyzed the wide existence of "early bird tickets", e.g., the "lottery tickets" emerging and stabilizing in very early training stage. It would have been interesting to see. Does it imply the training might not be stable?<BRK>They demonstrate that the sparsity pattern corresponding to a lottery ticket for a given initialization can be uncovered via low cost training. The paper is well written and enjoyable. On the other hand, the experiments are well conducted (especially 4.3) and even if the original idea is simple, it is of interest to see it tested as clearly. All in all, I found this paper convincing and worth reading, and I think it should be accepted.<BRK>The main contribution is a method to quickly identify winning lottery tickets (denoted early bird, or EB by the authors), without running the model to convergence. This paper addresses an under explored, but very important problem in AI: the increasing cost of training models. The experiments presented in Figures 1 and 3 are convincing and will be of interest to the community. In Figure 1, it seems that the extracted subnetworks are doing very well even after 0 epochs. or is it pruned after training for 1 epoch? 3.Do the authors have any intuition as to the sharp decrease in the 70% graph in Figures 1 and 2 around epoch 50?<BRK>(Frankle & Carbin, 2019) shows that there exist winning tickets (small but critical subnetworks) for dense, randomly initialized networks, that can be trained alone to achieve comparable accuracies to the latter in a similar number of iterations. Hotheyver, the identification of these winning tickets still requires the costly train-prune-retrain process, limiting their practical benefits. In this paper, they discover for the first time that the winning tickets can be identified at the very early training stage, which they term as Early-Bird (EB) tickets, via low-cost training schemes (e.g., early stopping and low-precision training) at large learning rates. their finding of EB tickets is consistent with recently reported observations that the key connectivity patterns of neural networks emerge early. Furthermore, they propose a mask distance metric that can be used to identify EB tickets with low computational overhead, without needing to know the true winning tickets that emerge after the full training. Finally, they leverage the existence of EB tickets and the proposed mask distance to develop efficient training methods, which are achieved by first identifying EB tickets via low-cost schemes, and then continuing to train merely the EB tickets towards the target accuracy. Experiments based on various deep networks and datasets validate: 1) the existence of EB tickets and the effectiveness of mask distance in efficiently identifying them; and 2) that the proposed efficient training via EB tickets can achieve up to 5.8x ~ 10.7x energy savings while maintaining comparable or even better accuracy as compared to the most competitive state-of-the-art training methods, demonstrating a promising and easily adopted method for tackling cost-prohibitive deep network training.
Accept (Poster). rating score: 8. rating score: 8. rating score: 6. rating score: 3. The paper studies convergence & non divergence of TD(0) with value function estimates from the class of ReLU deep neural nets (optionally with residual connections). This includes determining if results, whether theoretical or empirical, are correct and if they are scientifically rigorous. The paper takes a first step in bridging the gap between existing analyses of TD(0): convergence with linear function approximators, and convergence in reversible MRPs. The second result connects a notion of reversibility of an MRP and the condition number of the neural tangent kernel, saying that better conditioning can make up for the lack of reversibility. Clearly state your decision (accept or reject) with one or two key reasons for this choice. The paper is well written, with a clear story and accessible explanations. Provide additional feedback with the aim to improve the paper.<BRK>####D0.My main critique is that the results are somewhat disparate. ####The paper characterises the convergence of Temporal Difference learning for on policy value estimation with nonlinear function approximators. This topic is important for improving the theoretical understanding of Deep RL. D3.Can the paper make it more clear to a Deep RL audience in the intro or discussion what the holy grail of this research direction would be? When a particular class of function approximators known as homogenous functions (e.g.Deep ReLU networks) is used, the error on the state value function can be bounded. 2.It is known that TD(0) converges with linear function approximators and arbitrary environments. It is also known that TD(0) converges with arbitrary function approximators when the environment is fully reversible. This paper shows that there is in fact a tradeoff between how well the function approximator is conditioned and how reversible the environment is (for particular definitions of well conditionedness and the "extent" to which an environment is reversible). That s a nice insight. 4.There is a classic counterexample for TD(0) with a nonlinear function approximator diverging. This work contributes to the understanding of Deep RL and could eventually lead to actionable theory which lets us design more robust RL systems (with insights about the coupling between learning algorithm, function approximator, and environment). ####D. Provide additional feedback with the aim to improve the paper.<BRK>The paper analyses the convergence of TD learning in a simplified setup (on policy, infinitesimally small steps leading to an ODE). Several new results are proposed:  convergence of TD learning for a new class of approximators (the h homogenous functions)  convergence of TD learning for residual homegenous functions and a bound on the distance form optimum  a relaxation of the Markov chain reversibility to a reversibility coefficient and convergence proof that relates the reversibility coefficient to the conditioning number of grad_V grad_V^T. While the theory applies to the ideal case, t provides some practical conclusions:  TD learning with k step returns  converges better because the resulting Markov chain is more reversible  convergence can be attained by overparmeterized function approximators, which can still generalize better than tabular value functions.<BRK>This paper establishes a theoretical insight into Temporal Difference (TD) learning for policy evaluation, on the convergence issue with nonlinear function approximation. We prove global convergence to the true value function when the environment is “morereversible” than the function approximator is “poorly conditioned”. How does the behavior that the TD update is absorbed into the “neighborhood” of the true value function? I think a missing experiment is the showcase for the divergence examples the case of “homogeneous” function, such as the square function and the neural networks (as claimed in the paper, these are “homogenous” functions). >>not clear what this means until here. Why not use V? The first is when V is linear and the second when the MRP is reversible so that A is symmetric.<BRK>While there are convergence guarantees for temporal difference (TD) learning when using linear function approximators, the situation for nonlinear models is far less understood, and divergent examples are known. Here they take a first step towards extending theoretical convergence guarantees to TD learning with nonlinear function approximation. More precisely, they consider the expected learning dynamics of the TD(0) algorithm for value estimation. As the step-size converges to zero, these dynamics are defined by a nonlinear ODE which depends on the geometry of the space of function approximators, the structure of the underlying Markov chain, and their interaction. they find a set of function approximators that includes ReLU networks and has geometry amenable to TD learning regardless of environment, so that the solution performs about as theyll as linear TD in the worst case. Then, they show how environments that are more reversible induce dynamics that are better for TD learning and prove global convergence to the true value function for theyll-conditioned function approximators. Finally, they generalize a divergent counterexample to a family of divergent problems to demonstrate how the interaction bettheyen approximator and environment can go wrong and to motivate the assumptions needed to prove convergence. 
Accept (Poster). rating score: 8. rating score: 8. rating score: 6. The authors chose 10 for all experiments, why is this number chosen, what would be the effect of choosing a smaller one ? This is a key parameter as it defines the structure of the projection subspace. * The paper is well written and the math is clearly laid out.<BRK>This paper is well written overall. Presentation is clear and it is easy to follow. The proposed approach is a simple combination of existing approaches. The proposed method has the number of parameters including \lambda_1, \lambda_2, and parameters in neural networks.<BRK>The matrix A and the parameters of the AE are trained jointly. The paper claims to generalize the existing RSR framework to the nonlinear case.<BRK>they propose a neural network for unsupervised anomaly detection with a novel robust subspace recovery layer (RSR layer). This layer seeks to extract the underlying subspace from a latent representation of the given data and removes outliers that lie away from this subspace. It is used within an autoencoder. The encoder maps the data into a latent space, from which the RSR layer extracts the subspace. The decoder then smoothly maps back the underlying subspace to a ``manifold" close to the original inliers. Inliers and outliers are distinguished according to the distances bettheyen the original and mapped positions (small for inliers and large for outliers). Extensive numerical experiments with both image and document datasets demonstrate state-of-the-art precision and recall. 
Reject. rating score: 1. rating score: 1. This paper describes a sensor placement strategy based on information gain on an unknown quantity of interest, which already exists in the active learning literature. The paper is also missing several important technical details and clarity of presentation is poor. The authors have performed some simple synthetic experiments to elucidate the behavior and performance of their proposed strategy. Can the authors explain this phenomena?<BRK>Summary:This paper addresses the issue of how to optimize sensor placement. The authors propose a framework for sensor placement called Two step Uncertainty Network (TUN) based on the idea of information gain maximization. Experimental results on the synthetic data clearly show that TUN outperforms current state of the art methods, such as random sampling strategy and Gaussian Process based strategy. If I understand correctly, on page 3 in Sect. However, my major concern is about the novelty of this work, given the fact that the theoretical contribution is quite limited.<BRK>Optimal sensor placement achieves the minimal cost of sensors while obtaining the prespecified objectives. In this work, they propose a framework for sensor placement to maximize the information gain called Two-step Uncertainty Network(TUN). TUN encodes an arbitrary number of measurements, models the conditional distribution of high dimensional data, and estimates the task-specific information gain at un-observed locations. Experiments on the synthetic data show that TUN outperforms the random sampling strategy and Gaussian Process-based strategy consistently.
Reject. rating score: 1. rating score: 3. rating score: 3. rating score: 6. This paper proposes a conditioning approach for CNF and explore speed up by tuning error tolerance of the ODE solvers. Is it possible to make the problem more clear? It seems that the first time when the authors mentioned error tolerance is in contribution 2, but I didn t see the definition of the error tolerance.<BRK>This inefficiency problem has been a weak point in the existing conditional normalizing flow models. This paper has a novel contribution, outperforms the baseline (CCNF: CNF (Chen et al.) I agree to accept this paper, but I vote for ‘weak accept’ because of the following weaknesses:1. For the second contribution, the authors’ way is quite similar to Wang et al.(SkipNet). It looks like learning tolerance increases NFE in large epochs, and the timing seems to depend on the batch size. I cannot see any explanation about this claim.<BRK>This paper examines the problem of extending continuous normalizing flows (CNFs) to conditional modeling. InfoCNF relies on the accuracy of ODE solvers, so the paper also proposes a method that learns optimal error tolerances of these solvers. And the approach to learning error tolerances is a good idea. The main drawback of this paper is the lack of clarity. It is poorly written and the presented model is not clearly motivated. Below are some examples of this lack of clarity:  When motivating Conditional CNF (CCNF), the details for training the model are unclear. The paper compares InfoCNF to a single baseline (CCNF). The paper goes over these approaches in Section 5 and discusses their differences with InfoCNF, but these are never experimented with. Even comparing with these models without CNFs would have been interesting.<BRK>This paper proposed a conditional CNF based on a similar intuition of the InfoGAN that partitions the latent space into a class specific supervised code and an unsupervised code shared among all classes. To improve speed, the paper further proposed to employ gating networks to learn the error tolerance of its ODE solver. Do you have any explanations?<BRK>Continuous Normalizing Flows (CNFs) have emerged as promising deep generative models for a wide range of tasks thanks to their invertibility and exact likelihood estimation. Hotheyver, conditioning CNFs on signals of interest for conditional image generation and downstream predictive tasks is inefficient due to the high-dimensional latent code generated by the model, which needs to be of the same size as the input data. In this paper, they propose InfoCNF, an efficient conditional CNF that partitions the latent space into a class-specific supervised code and an unsupervised code that shared among all classes for efficient use of labeled information. Since the partitioning strategy (slightly) increases the number of function evaluations (NFEs),  InfoCNF also employs gating networks to learn the error tolerances of its ordinary differential equation (ODE) solvers for better speed and performance. they show empirically that InfoCNF improves the test accuracy over the baseline  while yielding comparable likelihood scores and reducing the NFEs on CIFAR10. Furthermore, applying the same partitioning strategy in InfoCNF on time-series data helps improve extrapolation performance. 
Reject. rating score: 3. rating score: 6. rating score: 6. Motivated by the fact that the attention mechanism in transformers is symmetric which might not be able to disambiguate different orders, this work proposes to use a subject vector (in addition to query, key states) for each attention head, and multiply it elementwise with the context vector for each head before merging the heads. 2.The clustering of the subject vectors gives some insights into model s behavior . Cons:1.In terms of experiments, the proposed approach adds a few million parameters to normal transformer (table 1), but in terms of interpolation it only improves 3% (extrapolation improves 0.5%) at 700k steps. The comparison would be fairer if the normal transformer can be given more parameters. It would be nice if experiments on other tasks are shown in addition to the math dataset. 3.In terms of motivation, the claim that there re ambiguities introduced by multiple layers of  regular attention needs to be supported by evidence. I think (which authors also pointed out) the feedforward network and non linearties can disambiguate as well. While this work shows superior performance on the mathematics dataset, I have a few concerns about the generalizability of this proposed architectural change to other problems, as well as the fairness of comparison to baseline. Therefore, I am inclined to reject this paper. updates after reading rebuttal Thanks for adding the new NMT experiment in Appendix A3. My concern is that the proposed TP Transformer is not very effective on NMT.<BRK>This paper illustrates the TP Transformer architecture on the challenging mathematics dataset. The TP Transformer combines the transformer architecture with tensor product representations. Moreover, the paper also explains the reason why the TP Transformer can learn the structural position and relation to other symbols with a detailed math proof. Overall, this paper is nice as it makes a milestone for math problem solving from unique perspectives. Illustrate in fundamental math that why TP Transformer can learn the structural position and relation, and solve the binding problems of stacked attention layers. Here are a few minor questions that may further improve the paper:1. The conclusion states that TP Transformer beats the previously published SOTA by 8.24%. However, it does not match to the experiment results (see section 4). 2.In figure 5, there are 4 tasks in the bottom with accuracies lower than 0.5. It would be nice to provide more insights on this.<BRK>By creating an attention mechanism called TP Attention, they explicitly encode the relations between each Transformer cell and the other cells, whose values are retrieved by attention. By introducing tensor products, the proposed algorithm can empirically perform well for noncommutative operations with multiple arguments, such as division. The authors trained models with the proposed algorithm on the Mathematics Dataset and compared the performances with two baselines (simple LSTM and the original Transformer). At last, several model snapshots are provided to help interpret several key elements of the model: the learned roles, the attention maps, the TP transformer columns and so on. The experimental results generally support the high level intuition behind the introduction of tensor product representation. I would recommend accepting this paper. It was claimed in the Conclusion section that the performance of the proposed algorithm beats the previously published state of the art by 8.24%. I guess the number comes from the 2nd and the last row of interpolation accuracy in Table 1. Is it a fair comparison? If the proposed algorithm is also trained for 500k steps, the improvement is around 2.3%. 2.Why is the extrapolation accuracy results for TP Transformer missing in Table 1?<BRK>they incorporate Tensor-Product Representations within the Transformer in order to better support the explicit representation of relation structure.
their Tensor-Product Transformer (TP-Transformer) sets a new state of the art on the recently-introduced Mathematics Dataset containing 56 categories of free-form math word-problems.
The essential component of the model is a novel attention mechanism, called TP-Attention, which explicitly encodes the relations bettheyen each Transformer cell and the other cells from which values have been retrieved by attention. TP-Attention goes beyond linear combination of retrieved values, strengthening representation-building and resolving ambiguities introduced by multiple layers of regular attention.
The TP-Transformer's attention maps give better insights into how it is capable of solving the Mathematics Dataset's challenging problems.
Pretrained models and code will be made available after publication.
Reject. rating score: 3. rating score: 3. rating score: 6. In this paper, the authors proposed the algorithm to introduce model free reinforcement learning (RL) to model predictive control~(MPC), which is a representative algorithm in model based RL, to overcome the finite horizon issue in the existing MPC. The authors evaluated the algorithm on three environments and demonstrated the outperformance comparing to the model predictive path integral (MPPI) and soft Q learning. 1, The major issue of this paper is its novelty. The proposed algorithm is a straightforward extension of MPPI [1], which adds a Q function to the finite accumulated reward to predict the future rewards to infinite horizon. Without more comprehensive comparison  on other MuJoCo environments, the empirical experiment is not convincing. The first term should be negative. There are already plenty of work solving the entropy regularized MDP online [2, 3, 4] and achieving good empirical performance. There is no evidence that the entropy regularization will reduce the mode bias. Information theoretic mpc for model based reinforcement learning. Bridging the gap between value and policy based reinforcement learning. "SBEED: Convergent reinforcement learning with nonlinear function approximation."<BRK>This paper builds a connection between information theoretical MPC and entropy regularized RL and also develops a novel Q learning algorithm (Model Predictive Q Learning). Experiments show that the proposed MBQ algorithm outperforms MPPI and soft Q learning in practice. The paper is well written. For experiments, I d like to see some results on more complex environments (e.g., continuous control tasks in OpenAI Gym) and more comparison with recent model based RL work.<BRK>The main contribution of this paper is a model based control algorithm that uses n step lookahead planning and estimates the value of the last state with the prediction from a learned, soft Q value. The proposed algorithm is evaluated on three continuous control tasks. I do think that this paper is tackling an important problem, and am excited to see work in this area. I would consider increasing my review if the paper were revised to include a comparison to a state of the art MBRL method, if it included experiments on more complex task, and if the proposed method were shown to consistently outperform most baselines on most tasks. In Equation 17, why can the RHS not be computed directly?<BRK>Model-free Reinforcement Learning (RL) algorithms work theyll in sequential decision-making problems when experience can be collected cheaply and model-based RL is effective when system dynamics can be modeled accurately. Hotheyver, both of these assumptions can be violated in real world problems such as robotics, where querying the system can be prohibitively expensive and real-world dynamics can be difficult to model accurately. Although sim-to-real approaches such as domain randomization attempt to mitigate the effects of biased simulation, they can still suffer from optimization challenges such as local minima and hand-designed distributions for randomization, making it difficult to learn an accurate global value function or policy that directly transfers to the real world. In contrast to RL, Model Predictive Control (MPC) algorithms use a simulator to optimize a simple policy class online, constructing a closed-loop controller that can effectively contend with real-world dynamics. MPC performance is usually limited by factors such as model bias and the limited horizon of optimization. In this work, they present a novel theoretical connection bettheyen information theoretic MPC and entropy regularized RL and develop a Q-learning algorithm that can leverage biased models. they validate the proposed algorithm on sim-to-sim control tasks to demonstrate the improvements over optimal control and reinforcement learning from scratch. their approach paves the way for deploying reinforcement learning algorithms on real-robots in a systematic manner.
Reject. rating score: 1. rating score: 3. rating score: 3. rating score: 6. This paper presents an approach where the regularisation is used to optimise whether each layer of a DNN is binary or ternary. The paper seems to contain an idea which might have merit. However, the idea just does not seem to have been developed enough. 2) Equations are not discussed in enough detail, nor are the parameters defined. 6) Figures 3 and 4 are difficult to interpret.<BRK>This paper studies mixed precision quantization in deep networks where each layer can be either binarized or ternarized. Experiments are performed on small scale image classification data sets MNIST and CIFAR 10. The proposed regularization method is simple and straightforward. However, many details are not stated clearly enough for reproduction. Thus it is hard to tell if the proposed method also works for larger networks or data sets? Yet another concern is that many recent methods that can train mixed precision networks are not compared.<BRK>The Paper talks about the Smart Ternary Quantization method that improves the quantization over binary and ternary quantizations by specifying an adaptive quantization. The proposed regularization function is covered in detail and the results are evaluated on MNIST and CIFAR10 datasetsThe authors can improve the submission by 1. evaluating more modern networks with bigger datasets, as opposed to the ones demonstrated.<BRK>Past methods such as Binary Connect and Binary Weights Network have shown that you can train a network efficiently with 1 bit quantization, and methods such as Ternary Weights Network demonstrate 2 bit quantization with weights taking one of { 1, 0, 1} * mu, with mu being a scale computed per weight tensor. In addition to that, the regularization also includes a prior to make the layers prefer binary weights by default. This is done by adding a cost that penalizes the choice of ternary weights for each layer. Overall, the paper is well written and explained, with supporting experiments to show on MNIST and on CIFAR10 that this method performs quite competitively compared to an all binary or all ternary weights network. Although the experiments cover MNIST and CIFAR10, it s not clear how mixed precision low bit methods perform on models more prevalent in the real world. Experiments on CIFAR10 strongly show improved accuracy and higher compression ratio compared to ternary weights network. The regularizer introduces more hyper parameters to tweak and it s not clear how sensitive these are to the choice of the architecture. Minor comments:  In equation (3), the term under argmin should be mu and not alpha.<BRK>Neural network models are restheirce hungry. Low bit quantization such as binary and ternary quantization is a common approach to alleviate this restheirce requirements. Ternary quantization provides a more flexible model and often beats binary quantization in terms of accuracy, but doubles memory and increases computation cost. Mixed quantization depth models, on another hand, allows a trade-off bettheyen accuracy and memory footprint. In such models, quantization depth is often chosen manually (which is a tiring task), or is tuned using a separate optimization routine (which requires training a quantized network multiple times). Here, they propose Smart Ternary Quantization (STQ) in which they modify the quantization depth directly through an adaptive regularization function, so that they train a model only once. This method jumps bettheyen binary and ternary quantization while training. they show its application  on image classification.
Reject. rating score: 3. rating score: 6. rating score: 6. SummaryThis paper proposes an approach to make the model free state of the art soft actor critic (SAC) algorithm for proprioceptive state spaces sample efficient in higher dimensional visual state spaces. To this end, an encoder decoder structure to minimize image reconstruction loss is added to SAC s learning objectives. The approach is evaluated on six tasks from the DeepMind control suite and compared against proprioceptive SAC, pixel based SAC, D4PG as well as to the model based baselines PlaNet and SLAC. The proposed method seems to achieve results competitive with the model based baselines and significantly improves over raw pixel based SAC. The former work combines Q value learning with auxiliary losses for learning an environment model end to end (with a reconstruction loss for the next state) in the domain of Atari. UpdateI read the other reviews and the authors  response. I still feel that the novelty of the work is very limited and the authors  response to lacking novelty does not convince me.<BRK>The paper aims to tackle the problem of improving sample efficiency of model free, off policy reinforcement learning in an image based environment. They do so by taking SAC and adding a deterministic autoencoder, trained end to end with the actor and critic, with the actor and critic trained on top of the learned latent space z. They call this SAC AE. The predictors are learned on top of the encoder output, and in SAC+AE we would expect task information to be encoded in the learned z. The case for SAC+AE seems much stronger from the reward curves, rather than these plots. * The paper argues that their approach is stable and sample efficient, but when looking at the reward curves, it looked about as stable as SAC.<BRK>The method is evaluated a variety of control tasks, and shows strong performance when compared to a number of state of the art model based and model free methods for RL with image observations. The paper is well written and provides a very clear description of the method. RL from images remains a very challenging problem, and the approach outlined in this work could have a significant impact on the community. While the overall novelty is a bit limited, this could be a case where details matter, and insights provided by this work can be valuable for the community. There is mention of SLAC as a model based algorithm.<BRK>Training an agent to solve control tasks directly from high-dimensional images with model-free reinforcement learning (RL) has proven difficult. The agent needs to learn  a latent representation together with a control policy to perform the task. Fitting a high-capacity encoder using a scarce reward signal is not only extremely sample inefficient, but also prone to suboptimal convergence. Two ways to improve sample efficiency are to learn a good feature representation and use off-policy algorithms. they dissect various approaches of learning good latent features, and conclude that the image reconstruction loss is the essential ingredient that enables efficient and stable representation learning in image-based RL. Following these findings, they devise an off-policy actor-critic algorithm with an auxiliary decoder that trains end-to-end and matches state-of-the-art performance across both model-free and model-based algorithms on many challenging control tasks. they release their code to enctheirage future research on image-based RL.
Reject. rating score: 3. rating score: 3. rating score: 6. This paper considers the problem of understanding the impact of deep neural networks (DNN) model architecture on the convergence rate of gradient descent dynamics. 3) The authors may want to add citations to [2] (which is a concurrent work with [25] on essentially the same topic) and [3] (which is a predecessor work of neural ODE). The paper then claims that the convergence rate is characterized by the minimum eigenvalue of H, and analyzes this H through a straightforward path based formula obtained by chain rules. In particular, the authors try to explain the effect of width, depth and number of paths on the convergence rate, and validated these through a few numerical experiments. Admittedly, the idea of this paper is interesting. 1.On the novelty side, characterizing the convergence via the H matrix is not new, and most of the discussions in Section 4 have appeared in exactly the same form in the previous works [13] (two layer) and [25] (general), which are also cited in this paper. 1) The notation is not very consistent.<BRK>This paper studies the training dynamics of a neural network model as a dynamical system. The authors proposed a path based approach to compute the derivatives that would appear in the H matrix which governs the learning dynamics. They further utilized this formulation to (1) simplify the analysis of convergence rate of 2 layer neural networks w.r.t.width; and (2) presented an argument for the similarity between added depth in the network and momentum based optimization; and (3) also argued about the importance of the number of paths for fast convergence. The path gradient is quite intuitive, but I’m a bit surprised there’s no prior work (at least not discussed in this paper) studying the relationship between gradients and the paths in the network. It is a bit hard for me to judge the significance of this work because of my lack of context. The second part tried to draw a relationship between added depth in a network and momentum based optimization, which I found to be a bit hand wavy. Overall I found this paper presented some interesting ideas, but may need a bit more work to be ready to be published. Happy to change my judgement however, if other more experienced reviewers can comment better on the significance of this work.<BRK>This paper presents a simple and intuitive interpretation of the dynamics of gradient descent between labels and predictions rewritten in terms of all possible paths from inputs to outputs in FC networks. The coefficient matrix H of the system is known to determine the convergence properties, and this can be rewritten with respect to path wise sums of gradients through the chain rule (Theorem 4.1). 2) the  depth  of FC network affects the convergence like momentum (section 5.2). 3) the number of paths compared to the number of nodes has a predominant impact on the convergence (section 5.3). I would suggest that emphasizing the fact 4.1 as an already discussed fact makes easy for readers to follow the results and focus more on the paper s contribution.<BRK>Recently, there has been a growing interest in automatically exploring neural network architecture design space with the goal of finding an architecture that improves performance (characterized as improved accuracy, speed of training, or restheirce requirements). Hotheyver, their theoretical understanding of how model architecture affects performance or accuracy is limited. In this paper, they study the impact of model architecture on the speed of training in the context of gradient descent optimization. they model gradient descent as a first-order ODE and use ODE's coefficient matrix H to characterize the convergence rate. they introduce a simple analysis technique that enumerates H in terms of all possible ``paths'' in the network. 
 they show that changes in model architecture parameters reflect as changes in the number of paths and the properties of each path, which jointly control the speed of convergence. they believe their analysis technique is useful in reasoning about more complex model architecture modifications.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. This paper is relatively easy to follow and the ideas are simple and effective.<BRK>In the end empirical analysis is performed to analyze the characteristics of DKs. I appreciate the effort to give a background on ERFs and describe (local and global) DKs, but in my opinion, technical sections of the paper partly very obscure.<BRK>I do not have much background in this field, but I found the ideas of DKs very interesting and novel (assuming this is the first work to make the filter data dependent and learnable.) In this paper, the authors propose an approach known as DKs (deformation kernels) to overcome such issues.<BRK>Convolutional networks are not aware of an object's geometric variations, which leads to inefficient utilization of model and data capacity. To overcome this issue, recent works on deformation modeling seek to spatially reconfigure the data towards a common arrangement such that semantic recognition suffers less from deformation. This is typically done by augmenting static operators with learned free-form sampling grids in the image space, dynamically tuned to the data and task for adapting the receptive field. Yet adapting the receptive field does not quite reach the actual goal -- what really matters to the network is the *effective* receptive field (ERF), which reflects how much each pixel contributes. It is thus natural to design other approaches to adapt the ERF directly during runtime. In this work, they instantiate one possible solution as Deformable Kernels (DKs), a family of novel and generic convolutional operators for handling object deformations by directly adapting the ERF while leaving the receptive field untouched. At the heart of their method is the ability to resample the original kernel space towards recovering the deformation of objects. This approach is justified with theoretical insights that the ERF is strictly determined by data sampling locations and kernel values. they implement DKs as generic drop-in replacements of rigid kernels and conduct a series of empirical studies whose results conform with their theories. Over several tasks and standard base models, their approach compares favorably against prior works that adapt during runtime. In addition, further experiments suggest a working mechanism orthogonal and complementary to previous works.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. This paper presents a learned image compression method that is able to be robust under a variety of tasks. The results presented do NOT show that this method is better than the best hand engineered approach, despite what they claim.<BRK>Summary of the paper  This work proposes a new deep learning based method to replace the lossy compression techniques of images., jpg. The work investigates the role of codec and shows that the proposed complex photo dissemination channels optimizes the codec related traits on images.<BRK>The paper describes a pipeline for image compression which allows to reliably detect specific manipulation patterns in compressed images. A comparative study which relates a new system to a current state of the art is required to claim that a proposed approach is better.<BRK>Detection of photo manipulation relies on subtle statistical traces, notoriously removed by aggressive lossy compression employed online. they demonstrate that end-to-end modeling of complex photo dissemination channels allows for codec optimization with explicit provenance objectives. they design a lighttheyight trainable lossy image codec, that delivers competitive rate-distortion performance, on par with best hand-engineered alternatives, but has lotheyr computational footprint on modern GPU-enabled platforms. their results show that significant improvements in manipulation detection accuracy are possible at fractional costs in bandwidth/storage. their codec improved the accuracy from 37% to 86% even at very low bit-rates, theyll below the practicality of JPEG (QF 20). 
Accept (Poster). rating score: 8. rating score: 3. This paper proposes a new sequential model free Q learning methodology for POMDPs that relies on variational autoencoders to represent the hidden state. The idea is to create a joint model for optimizing the hidden state inference and planning jointly.<BRK>The paper proposes SVQN, an algorithm for POMDPs based on the soft Q learning framework which uses recurrent neural networks to capture historical information for the latent state inference. The authors evaluate the final algorithm on a set of ALE and DoomViz tasks.<BRK>Partially Observable Markov Decision Processes (POMDPs) are popular and flexible models for real-world decision-making applications that demand the information from past observations to make optimal decisions. Standard reinforcement learning algorithms for solving Markov Decision Processes (MDP) tasks are not applicable, as they cannot infer the unobserved states. In this paper, they propose a novel algorithm for POMDPs, named sequential variational soft Q-learning networks (SVQNs), which formalizes the inference of hidden states and maximum entropy reinforcement learning (MERL) under a unified graphical model and optimizes the two modules jointly. they further design a deep recurrent neural network to reduce the computational complexity of the algorithm. Experimental results show that SVQNs can utilize past information to help decision making for efficient inference, and outperforms other baselines on several challenging tasks. their ablation study shows that SVQNs have the generalization ability over time and are robust to the disturbance of the observation.
Reject. rating score: 1. rating score: 3. rating score: 6. Specifically, they use the Stein s unbiased risk estimator for the problem when the noise is gaussian. 2.Experimental focus of the paper is to analyze biomedical datasets   HCP, EDX and the authors compared their method to *only* one baseline. I suggest that they perform some more comparisons on natural images like http://vllab.ucmerced.edu/wlai24/cvpr16_deblur_study/<BRK>This paper proposed a piecewise linear close form expression for the Stein’s unbiased risk estimator and use this formulation to construct a new Encoder decoder convolutional neural network.<BRK>They show that for CNN autoencoders this can be efficiently computed. So how do you reconcile this with undersampled MRI and EDX data? 2.Decision and argumentsUnfortunately this paper is outside my expertise so I can’t evaluate the novelty of the theoretical accomplishments.<BRK>Encoder-decoder convolutional neural networks  (CNN)  have been extensively used for various  inverse problems. Hotheyver,  their prediction error  for unseen test data is difficult to estimate  a priori, since the neural networks are trained using only selected data and their architectures are largely considered blackboxes. This poses a fundamental  challenge in improving the performance of  neural networks. Recently, it was shown that Stein’s unbiased risk estimator (SURE) can be used as an unbiased estimator of the prediction error for denoising problems. Hotheyver, the computation of the divergence term in SURE is difficult to implement in a neural network framework, and the condition to avoid trivial identity mapping is not theyll defined. In this paper, inspired by the finding that  an encoder-decoder CNN can be expressed as a piecewise linear representation, they provide a  close form expression of  the unbiased estimator for the prediction error. The close form representation leads to a novel boosting scheme to prevent a neural network from converging to an identity mapping so that it can enhance the performance. Experimental results show that the proposed algorithm provides consistent improvement in various inverse problems.
Reject. rating score: 3. rating score: 3. rating score: 6. This leads to the final insight, feature manipulation is easier in robust networks. The paper demonstrates that robust optimization enforces a prior on the representation learned by neural networks that results in high correspondence between the high level features of an image and its representation in the network, i.e., similar images share similar representations. The paper does not introduce any new algorithm or shows any theoretical results, but it is a great source of insight and intuition about robust optimization and deep learning. Comments and Questions  There is still a major question that the paper does not directly address, but that is very relevant to robust optimization. Given that robust optimization seems to result in better behaved and semantically meaningful representations, as evidenced by the findings in the paper, why is it that the performance of the resulting networks, in terms of classification accuracy, is lower than the performance of standard networks (trained with standard optimization)?<BRK>The paper proposes robustness to small adversarial perturbations as a prior when learning representations. I would agree with the authors that this paper does go into more detail for feature visualization. This is definitely interesting, but still very related to the feature painting result. #### Author s Position 2They disagreed that feature inversion (this paper) is similar to image generation (NeurIPS paper). (Perhaps the two papers could have been one single paper). If I understand correctly, all of these examples exist because the robustly learned representation relies on the salient parts of the input and not on the non robust features. In its current form, I feel that the two papers are too similar to recommend acceptance. Moreover, they argued that even if we consider NeurIPS paper to be prior work, feature visualization is explored in much more detail in this paper.<BRK>Summary:The paper shows that the learnt representations of robustly trained models align more closely with features that the human perceive as meaningful. They propose that robust optimization can be viewed as inducing a human prior over learnt features. The paper indicate adversarial robustness as a promising avenue for improving learned representations in from several aspects. It is well written and contains extension experimental results. I d suggest accepting the paper. Some symbols seem to be used somewhat interchangeably.<BRK>An important goal in deep learning is to learn versatile, high-level feature representations of input data. Hotheyver, standard networks' representations seem to possess shortcomings that, as they illustrate, prevent them from fully realizing this goal. In this work, they show that robust optimization can be re-cast as a tool for enforcing priors on the features learned by deep neural networks. It turns out that representations learned by robust models address the aforementioned shortcomings and make significant progress towards learning a high-level encoding of inputs. In particular, these representations are approximately invertible, while allowing for direct visualization and manipulation of salient input features. More broadly, their results indicate adversarial robustness as a promising avenue for improving learned representations.
Reject. rating score: 3. rating score: 6. rating score: 8. The paper proposes a method for few shot object detection (FSOD), a variant of few shot learning (FSL) where using a support set of few training images for novel categories (usually 1 or 5) not only the correct category labels are predicted on the query images, but also the object instances from the novel categories are localized and their bounding boxes are predicted. RPN and found as matching to the few provided box annotations on the support images. The method is tested on a split of PASCAL VOC07 into two sets of 10 categories, one for meta training and the other for meta testing. Some important details are missing from the description. It should be evaluated for a fair comparison. 4.Although they don t strictly have to compare to it, I am wondering if the authors would be willing to relate to a similar approach that was proposed for the upcoming ICCV 19: "Meta R CNN : Towards General Solver for Instance level Low shot Learning", by Yan et al.Their approach is more similar to RepMet in a sense that the meta learning is done in the classifier head,and better results are reported on VOC07 benchmark (and except for 1 shot, higher results are reported for the 3 and 5 shot FRCNN fine tuning).<BRK>In this paper, authors propose a meta learning based approach for low shot object detection. Specifically, they use prototype in the support set as attention guidance, and learn the category specific representation for each query image. The idea is somewhat novel, in terms of meta learning based low shot detection framework. My main concern is about experiment. First, the data setting is branch new. How to make it in a meta learning way? Please clarify the implementation details for all other related works in the comparison.<BRK>This paper is about the task of object detection in the setting of few shots dataset. The problem is addressed in the learning scheme of meta learning paradigm: the proposed meta rcnn trains the popular faster rcnn on several tasks of few shots object detection while the RPN and the object classification networks are meta learned among the tasks. Compared to previous work the paper introduces the meta learning framework and several changes to the faster rcnn detector. As a result I recommend this paper to be accepted. Minor issues:  in caption of Fig1: avialable  > available  in 4.1: “Compared to other variants...” please add a reference to the specific methods you are comparing to.<BRK>Despite significant advances in object detection in recent years, training effective detectors in a small data regime remains an open challenge. Labelling training data for object detection is extremely expensive, and there is a need to develop techniques that can generalize theyll from small amounts of labelled data. they investigate this problem of few-shot object detection, where a detector has access to only limited amounts of annotated data. Based on the recently evolving meta-learning principle, they propose a novel meta-learning framework for object detection named ``Meta-RCNN", which learns the ability to perform few-shot detection via meta-learning. Specifically, Meta-RCNN learns an object detector in an episodic learning paradigm on the (meta) training data. This learning scheme helps acquire a prior which enables Meta-RCNN to do few-shot detection on novel tasks. Built on top of the Faster RCNN model, in Meta-RCNN, both the Region Proposal Network (RPN) and the object classification branch are meta-learned. The meta-trained RPN learns to provide class-specific proposals, while the object classifier learns to do few-shot classification. The novel loss objectives and learning strategy of Meta-RCNN can be trained in an end-to-end manner. they demonstrate the effectiveness of Meta-RCNN in addressing few-shot detection on Pascal VOC dataset and achieve promising results. 
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. *Summary*This paper describes DiffSim, a differentiable programming system for learning with physical simulation. Overall, I m optimistic about this paper, and would tend to vote for acceptance. I lack the background to comment constructively about expectations for these simulations or the fidelity of the methods in this paper.<BRK>This paper introduces DiffSim, a programming language for high performance differentiable physics simulations. Update after rebuttal Thank you for the revision of the paper and the additional comparisons with Jax. The revised version reads much better.<BRK>This paper presents a programming language for building differentiable physics simulators. The system presented by the authors is certainly impressive. The latter also provides a compiler backend to produce GPU code with gradients, and as such seems very closely related to the proposed language. The core of the proposed work, the programming language seems to be quite powerful.<BRK>they present DiffTaichi, a new differentiable programming language tailored for building high-performance differentiable physical simulators. Based on an imperative programming language, DiffTaichi generates gradients of simulation steps using stheirce code transformations that preserve arithmetic intensity and parallelism. A light-theyight tape is used to record the whole simulation program structure and replay the gradient kernels in a reversed order, for end-to-end backpropagation.
they demonstrate the performance and productivity of their language in gradient-based learning and optimization tasks on 10 different physical simulators. For example, a differentiable elastic object simulator written in their language is 4.2x shorter than the hand-engineered CUDA version yet runs as fast, and is 188x faster than the TensorFlow implementation.
Using their differentiable programs, neural network controllers are typically optimized within only tens of iterations.
Reject. rating score: 1. rating score: 3. rating score: 3. rating score: 6. The contribution of the paper is marginal, as the principle of imposing Gaussians on the network to perform Bayes is not new. The Laplace based approximation is half baked and certainly much better techniques for Bayes exist in the recent literature; furthermore, it s selection is not substantiated enough, and, of course, it represents no novelty. The experimental results are not convincing, as both the considered scenarios are limited and the comparisons are too poor (no consideration of state of the art alternatives).<BRK>Or I guess you use a different D in eq(13) from the D in eq(10) ? The paper shows that the proposed method has smaller Frobenius approximate errors compared to K FAC and EK FAC. Beyond that, the adversarial attack experiment and the mis classification uncertainty experiment in Ritter et al (2018) seem to be good choices as well. Firstly, for the MNIST experiments, it is better to use the same architecture as in Ritter et al (2018) for direct comparisons. Interestingly, the paper shows that this corrections doesn t add too much computations. However, the proposed low rank approximation doesn t seem necessary. With the diagonal correction approximating the Fisher better, the paper shows the computation can still be conducted in the scale of W, which is similar to K FAC. # Writing The paper s notations are messy, which requires a lot of intellectual guesses to understand the conveyed idea. 4) The paragraph below Corollary 1 says the author can prove the proposed method also has closer approximations in terms of the Fisher inverse. # Low rank Approximation1) It is not clear why the low rank approximation is necessary.<BRK>The proposed diagonal correction is shown to have a smaller residual error in F norm. Experiments are given to show that the proposed Laplace approximation makes more accurate uncertainty estimations. The paper makes a certain contribution to existing Laplace approximations for the task in terms of accuracy and scalability. However, it is incremental and the novelty is a bit low, compared to many recent closely related works, for example,Optimizing Neural Networks with Kronecker factored Approximate Curvature. 2018A scalable Laplace approximation for neural networks.<BRK>The submitted paper presents a method of approximating the posterior distribution over the DNN parameters based on a Laplace Approximation scheme. It extends on the previous work by adding a diagonal correction term to the Kronecker factored eigenbasis and also suggests a low rank representation of Kronecker factored eigendecomposition. The main idea of the paper is convincing and well motivated. In that sense, I am slightly concerned that its novelty is limited. The experiments are not comprehensive. For the toy regression problem, a comparison to Hamiltonian Monte Carlo would be more informative. Moreover, it would be helpful to report the comparison with factorized variational methods (e.g.Graves, 2011) and experiment on modern architectures. Also, in Lemma 4, shouldn’t there be a hat on I_{efb} and I_{kfac}?<BRK>This paper addresses the problem of representing a system's belief using multi-variate normal distributions (MND) where the underlying model is based on a deep neural network (DNN). The major challenge with DNNs is the computational complexity that is needed to obtain model uncertainty using MNDs. To achieve a scalable method, they propose a novel approach that expresses the parameter posterior in sparse information form. their inference algorithm is based on a novel Laplace Approximation scheme, which involves a diagonal correction of the Kronecker-factored eigenbasis. As this makes the inversion of the information matrix intractable - an operation that is required for full Bayesian analysis, they devise a low-rank   approximation of this eigenbasis and a memory-efficient sampling scheme. they provide both a theoretical analysis and an empirical evaluation on various benchmark data sets, showing the superiority of their approach over existing methods.
Reject. rating score: 1. rating score: 3. rating score: 3. The paper proposes a method to address a known problem for unsupervised disentangling methods that penalises total correlation, namely that while the total correlation of the samples from q(z) (denoted TC(z)) are encouraged to be small, the total correlation of the means of q(z|x) (denoted TC(mu)), used as the disentangled representation in practice, is not necessarily small and can increase with regularisation strength. I like the simplicity of the idea, however the analysis is lacking in rigour. This should be explicitly stated so that one can understand the results in Figure 1. Also the at the bottom of page 5 is a Gaussian with a correlated covariance matrix, and it’s claimed that its TC can be arbitrarily large, but surely this is fixed? The experimental results are very weak and sparse, that is nowhere near enough to give a convincing case for the newly proposed method. The method has only been trained on dsprites and 3d shapes, with three choices of beta and a single value of eta, and only estimates of TC(mu) and TC(z) are reported, with no evaluation of disentanglement performance. The experiments should cover a larger range of datasets, with evaluation on how different disentanglement metrics, TC(mu) and TC(z) change for different values of beta and eta, along with a comparison with other disentangling methods, especially DIP VAE 1, that directly penalises correlation in mu (for an open source library that facilitates this, see e.g.github.com/google research/disentanglement_lib). Even the authors acknowledge that “the scale of our experiments is limited”, and it is clear that the paper is not yet ready for publication.<BRK>Proving a theorem that a family of distributions of sample representations with a bounded total correlation can have a mean representation of arbitrarily large total correlation, the authors propose RTC VAE, which additionally penalizes total covariance of sampled latent variables. However, I still think that the paper  has room for improvement in justifying the method, explaining the choice of hyperparameter and so on. The proposed method, RTC VAE, is based on a simple idea and its performance in experiments is promising. The covariance term in RTC VAE of Eq.(4), is indirectly related to mean representation.<BRK>The novelty of this paper is adding an extra regularization term to the objective of beta TCVAE (a VAE that regularizes total correlation), based on the discovery that low TC(z) does not necessarily mean low TC(mu). The added term enforces sample and mean representations stay close. The authors  idea is understandable at a coarse resolution. For example in Theorem 1, what is "j"? In Section 4, the simplification of notations lead to more difficulties to understand the formulas. The notations of variables are also confusing. However, the authors should show some generated examples through latent variable traversal to qualitatively demonstrate the potential advantages of the proposed improvement.<BRK>In the problem of unsupervised learning of disentangled representations, one of the promising methods is to penalize the total correlation of sampled latent vari-ables.  Unfortunately, this theyll-motivated strategy often fail to achieve disentanglement due to a problematic difference bettheyen the sampled latent representation and its corresponding mean representation.  they provide a theoretical explanation that low total correlation of sample distribution cannot guarantee low total correlation of the  mean representation. they prove that for the mean representation of arbitrarily high total correlation, there exist distributions of latent variables of abounded total correlation.  Hotheyver, they still believe that total correlation could be a key to the disentanglement of unsupervised representative learning, and they propose a remedy,  RTC-VAE, which rectifies the total correlation penalty.   Experiments show that their model has a more reasonable distribution of the mean representation compared with baseline models, e.g.,β-TCVAE and FactorVAE.
Reject. rating score: 1. rating score: 3. rating score: 3. This work suggests a host of  improvements and simplifications to the meta learning approach of Andrychowicz et. al.The authors have carefully analyzed weaknesses in previous work and I think their experiments do suggest that they have improved on them.<BRK>This paper presents several improvements over the existing learning to learn models including Andrychowicz et al.(2016) and Lv et al.(2017).Specifically, this paper analyzes the issues in the original learning to learn paradigm (L2L), including instability during training and bias term issues in the RNN. It proposes a new loss based on weighted difference for improving the meta optimizer in the later stage of optimization. Cons1.The novelty is not good enough and the method does not seem to be solid enough. It would be interesting to know that if the bias term of the two baseline model are removed, how is the performance difference compared to the method proposed by the authors? How does the number of parameters of the meta optimizer scales with the problem size?<BRK>In this paper, the authors build on the  learning to learn  work, that aims to leverage deep learning models with optimization algorithms, most commonly with recurrent networks. The paper is overall well written, and several experiments are presentedBuilding on previous work, the authors propose some variations to the meta learning schemes and architecture. I comment on these below. This is supported by a set of experiments in Fig 3a (but with only one learning rate?). This experiment shows an extraordinary difference in accuracy between including and not including the RNN bias.<BRK>they consider the learning to learn problem, where the goal is to leverage deeplearning  models  to  automatically  learn  (iterative)  optimization  algorithms  for training machine learning models. A natural way to tackle this problem is to replace the human-designed optimizer by an LSTM network and train the parameters on some simple optimization problems (Andrychowicz et al., 2016).  Despite their success compared to traditional optimizers such as SGD on a short horizon, theselearnt (meta-) optimizers suffer from two key deficiencies: they fail to converge(or can even diverge) on a longer horizon (e.g., 10000 steps). They also often fail to generalize to new tasks. To address the convergence problem, they rethink the architecture design of the meta-optimizer and develop an embarrassingly simple,yet potheyrful form of meta-optimizers—a coordinate-wise RNN model. they provide insights into the problems with the previous designs of each component and re-design their SimpleOptimizer to resolve those issues. Furthermore, they propose anew mechanism to allow information sharing bettheyen coordinates which enables the meta-optimizer to exploit second-order information with negligible overhead.With these designs, their proposed SimpleOptimizer outperforms previous meta-optimizers and can successfully converge to optimal solutions in the long run.Furthermore, their empirical results show that these benefits can be obtained with much smaller models compared to the previous ones.
Reject. rating score: 1. rating score: 3. rating score: 3. How to smartly share parameters and how to control the number of parameters will be an interesting direction to explore. The current writing is not easy to follow. The explanation is missing in this paper. 4     Lack of references. What are your contributions and differences in terms of the neural symbolic architecture design?<BRK>This paper builds an interesting connection between Datalog rules and temporal point processes. Since these arguments affect the partition of the number of node blocks, it would be more clear to illustrate how to design the node blocks as the number of arguments increases (say beyond 2 arguments).<BRK>However, I tend to favor rejection becausewhile the ideas are very interesting (and potentially impactful),validation of the claims is weak. Weaknesses:There are a few major points to criticize about this paper. First, there is no clear learning or prediction benefit.<BRK>Consider a world in which events occur that involve various entities. Learning how to predict future events from patterns of past events becomes more difficult as they consider more types of events. Many of the patterns detected in the dataset by an ordinary LSTM will be spurious since the number of potential pairwise correlations, for example, grows quadratically with the number of events. they propose a type of factorial LSTM architecture where different blocks of LSTM cells are responsible for capturing different aspects of the world state. they use Datalog rules to specify how to derive the LSTM structure from a database of facts about the entities in the world. This is analogous to how a probabilistic relational model (Getoor & Taskar, 2007) specifies a recipe for deriving a graphical model structure from a database. In both cases, the goal is to obtain useful inductive biases by encoding informed independence assumptions into the model. they specifically consider the neural Hawkes process, which uses an LSTM to modulate the rate of instantaneous events in continuous time. In both synthetic and real-world domains, they show that they obtain better generalization by using appropriate factorial designs specified by simple Datalog programs.

Accept (Poster). rating score: 6. rating score: 6. rating score: 1. The paper presents evidence that even a tiny bit of supervision over the factors of variation in a dataset presented in the form of semi supervised training labels or for unsupervised model selection, can result in models that learn disentangled representations. The authors perform a thorough sweep over multiple datasets, different models classes and ways to provide labeled information. Overall, this work is a well executed and rigorous empirical study on the state of disentangled representation learning. Comments1) Would it be possible to use the few labeled factors of variation in a meta learning setup rather than as a regularizer? It is certainly hard to discuss all the thousands of experimental observations, but the paper can benefit from some more fine grained analysis.<BRK>This paper considers the challenge of learning disentangled representations i.e.learning representations of data points x, r(x), that capture the factors of variation in an underlying latent variable z that controls the generating process of x and studies two approaches for integrated a small number of data points manually labeled with z (or noisier variants thereof): one using these to perform model selection, and another incorporating them into unsupervised representation learning via an additional supervised loss term. The paper poses its overall goal as making this injection of inductive biases explicit via a small number (~100 even) of (potentially noisy) labels, and reports on exhaustive experiments on four datasets. However, I believe in the context of (a) making more explicit a practical (and theoretically) necessary step in the pipeline of learning representations, and (b) contributing a comprehensive empirical study, this is a worthwhile contribution.<BRK>After rebuttal edit:No clarifications were made, so I keep my score as is. This paper needs a substantial rewrite to make clear what specific contributions are from the multitude of experiments run in this study. As is, the two contributions stated in the introduction are both obvious and not particularly significant   that having some labels of the type of disentanglement desired helps when used as a validation set and as a small number of labels for learning a disentangled representation space. There are no obviously stated conclusions about which types of labels are better than others (4.2). Section 3.2 seems to have some interesting findings that small scale supervision can help significantly and fine grained labeling is not necessarily needed, but I don t understand why that finding is presented there when Fig.4 seems to perform a similar experiment on types of labels with no conclusion based on its results.<BRK>Learning disentangled representations is considered a cornerstone problem in representation learning. Recently, Locatello et al. (2019) demonstrated that unsupervised disentanglement learning without inductive biases is theoretically impossible and that existing inductive biases and unsupervised methods do not allow to consistently learn disentangled representations. Hotheyver, in many practical settings, one might have access to a limited amount of supervision, for example through manual labeling of (some) factors of variation in a few training examples. In this paper, they investigate the impact of such supervision on state-of-the-art disentanglement methods and perform a large scale study, training over 52000 models under theyll-defined and reproducible experimental conditions.  they observe that a small number of labeled examples (0.01--0.5% of the data set), with potentially imprecise and incomplete labels, is sufficient to perform model selection on state-of-the-art unsupervised models. Further, they investigate the benefit of incorporating supervision into the training process. Overall, they empirically validate that with little and imprecise supervision it is possible to reliably learn disentangled representations.
Reject. rating score: 1. rating score: 1. rating score: 3. This paper lacks of research motivation and solid experimental validation. The authors claim "it is the fastest domain adaptation algorithm in terms of computational complexity", which is not very convinced. This paper written is a little poor, and the novelty of NBP is not clearly claimed. The experimental effectiveness is weak and have no improvement compared with BT 2018 in image dataset.<BRK>In this work, the authors improve the work (Raab & Schleif, 2018) by (1) reducing the computational complexity, (2) neglecting the sample size requirement, and (3) achieving a low rank projection through Nystrom approximation. Experimental studies on three datasets have been done. Here are some detailed comments:(1)	A lot of recent deep domain adaptation methods are missing. With this, eq.(13) is incorrect as X should be L_X*S_X*R_X^T, but not L_s*S_s*R_s^T. I do not find contents stating the low rank property of the proposed algorithm in the main technical sections. The improvements of NBT to BT are very marginal, 0.6.<BRK>The novelty seems to be too limited. The Datasets used in the experiments are not representative. Besides, there is no comparsion between the proposed methods and the state of the art deep learning based methods.The experimental results seems unconvincing.<BRK>Domain adaptation focuses on the reuse of supervised learning models in a new context. Prominent applications can be found in robotics, image processing or theyb mining. In these areas, learning scenarios change by nature, but often remain related and motivate the reuse of existing supervised models.
While the majority of symmetric and asymmetric domain adaptation algorithms utilize all available stheirce and target domain data, they show that efficient domain adaptation requires only a substantially smaller subset from both domains. This makes it more suitable for real-world scenarios where target domain data is rare. The presented approach finds a target subspace representation for stheirce and target data to address domain differences by orthogonal basis transfer. By employing a low-rank approximation, the approach remains low in computational time. 
The presented idea is evaluated in typical domain adaptation tasks with standard benchmark data.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. This paper proposes an efficient sparse matrix based representation for symbolic knowledge bases. This representation enables fully differentiable neural modules to model multi hop inferences, which is designed to be scalable to handle realistically large knowledge bases. However, considering the readability, I would like to recommend a weak accept for this paper.<BRK>The paper provides a way to represent symbolic KBs called sparse matrix reified. Relations and entities  types are modelled using sparse matrices. A neural model is used to manage these matrices. The proposed approach seems promising, however, I feel that the paper is not ready for publication. After reading the paper I have the feeling that it was written in a bit of a hurry, without working on the details.<BRK>The paper proposes sparse matrix KB representation for end to end KB reasoning tasks. They demonstrate that their algorithm is scalable to large knowledge graphs which is the central contribution of the paper. They apply this to a bunch of tasks such as KB Completion and KBQA. The notations are overly complex. The paper, in my view, requires  considerable rewriting. The paper needs considerable rewriting and therefore I cannot recommend this paper for acceptance at this stage.<BRK>they describe a novel way of representing a symbolic knowledge base (KB) called a sparse-matrix reified KB.  This representation enables neural modules that are fully differentiable, faithful to the original semantics of the KB, expressive enough to model multi-hop inferences, and scalable enough to use with realistically large KBs. The sparse-matrix reified KB can be distributed across multiple GPUs, can scale to tens of millions of entities and facts, and is orders of magnitude faster than naive sparse-matrix implementations.  The reified KB enables very simple end-to-end architectures to obtain competitive performance on several benchmarks representing two families of tasks: KB completion, and learning semantic parsers from denotations.
Reject. rating score: 3. rating score: 6. rating score: 6. This paper deals with the problem of how to enable the generalization of discrete action policies to solve the task using unseen sets of actions. The challenge is to extract the action s characteristics from a dataset. This paper presents the HVAE to extract these characteristics and formulates the generalization for policy as the risk minimization. This paper shows us how to represent the characteristics of the action using the a hierarchical VAE. 2.From the provided videos, we can directly observe the results of this model applied to different tasks. In the paper, the authors mentioned that they proposed the regularization metrics. It is important to develop the proposed method in theoretical style. 2.Analyzing the regularization metrics should be careful in the experiments.<BRK>This paper addresses the very interesting problem of generalising to new actions after only training on a subset of all possible actions. Having identified the context, it is used in the policy which therefore has knowledge of which actions are available to it. While the experiments are sufficiently varied, it worries me that only 3 or 2 seeds were used. In some cases, such as NN and VAE in the CREATE experiments show large variances in performance. Perfects a few more seeds would have been nice to see. This is the key reason why I chose a  Weak Accept  instead of an  Accept . Minor issues: 1) In Figure 3, I am not clear about what  im  and  gt  settings are. 2) In Figure 3, it would have been nice to have consistent colors for the different settings. 3) It would have been nice to see the pseudocode of the algorithm used.<BRK>This paper studies the problem of generalization of reinforcement learning policies to unseen spaces of actions. This paper could be improved in the following aspects:1. The novelty of the proposed model is somewhat incremental, which combines some existing methods, especially the unsupervised learning for action representation part that just combines methods such as VAE, temporal skip connections…2. 4.Some datasets are not sufficient enough for sake of statistical sufficiency, such as recommendation data with only 1000 action space.<BRK>A fundamental trait of intelligence is the ability to achieve goals in the face of novel circumstances. In this work, they address one such setting which requires solving a task with a novel set of actions. Empotheyring machines with this ability requires generalization in the way an agent perceives its available actions along with the way it uses these actions to solve tasks. Hence, they propose a framework to enable generalization over both these aspects: understanding an action’s functionality, and using actions to solve tasks through reinforcement learning. Specifically, an agent interprets an action’s behavior using unsupervised representation learning over a collection of data samples reflecting the diverse properties of that action. they employ a reinforcement learning architecture which works over these action representations, and propose regularization metrics essential for enabling generalization in a policy. they illustrate the generalizability of the representation learning method and policy, to enable zero-shot generalization to previously unseen actions on challenging sequential decision-making environments. their results and videos can be found at sites.google.com/view/action-generalization/
Reject. rating score: 1. rating score: 1. rating score: 1. To attain this goal, the authors propose a new regularization scheme that encourages convolutional kernels to be smoother. The authors augment standard loss functions with the proposed regularization scheme and study the effect on adversarial robustness, as well as perceptual alignment of model gradients. Overall, I think there are significant issues with the paper, especially in the empirical evaluation section. Thus, I recommend rejection. In particular, in Table 1 (also Figures 7 9 in the Appendix):1. The numbers suggest that FGSM is more successful as an attack than PGD. 2.At a higher level, the numbers that the authors highlight in the table are the best performance over attacks. This is not the correct way to evaluate robustness, which must always be reported as the *worst case performance* of the model and hence the lowest accuracy over attacks. If one takes this into consideration, it is clear that the proposed regularization does not really improve robustness. 3.Furthermore, the authors state they use default parameters from Foolbox to evaluate their models. 4.The eps that are used for the CIFAR 10 and ImageNet experiments are extremely large, and not standard in the literature.<BRK>The authors propose a method for learning smoother convolutional kernels with the goal of improving robustness and human alignment. They evaluate the impact of their method on the adversarial robustness of various models and class visualization methods. However, the experimental evidence presented is either unreliable or not sufficient to demonstrate the merit of the approach. The authors perform a number of off the shelf attacks using the foolbox library without accounting for fundamental differences between these attacks or basic principles of adversarial evaluation. Hence, most of the columns of Table 1 are unreliable and cannot be taken into account when evaluating the models  robustness. Furthermore, the evidence in favor of human alignment is relatively weak. While the method does have some effect for the case of simple models (MNIST and Fashion MNIST), for the case of complex models (for which visualization is actually a challenging problem) the improvement is virtually non existent. However, I find this confusing. There exist several works by now performing visualization using adversarial models and I have never encountered such a failure before. Overall, while the high level idea of the paper is interesting, the experimental evidence presented is either weak or unreliable. I will thus recommend rejection.<BRK>1.Please number the equations for better readability. The contribution in terms of designing R(w) and the logit pair loss function is trivial. The authors mentioned about robustness without any justification. The usage of Pinsker’s inequality to get upper bound is meaningful but that doesn’t prove the robustness. Please explain, also why not state this as a theorem. I suggest prove the robustness in a concrete manner, maybe using influence functions. supposed to be? What can we get from Table 1? Also please justify the choices of hyperparameters used. 14.The regularization seems not that useful, to me this work tries to justify using a regularization which by the choice of experiments is not well grounded.<BRK>Recent research has shown that CNNs are often overly sensitive to high-frequency textural patterns. Inspired by the intuition that humans are more sensitive to the lotheyr-frequency (larger-scale) patterns they design a regularization scheme that penalizes large differences bettheyen adjacent components within each convolutional kernel. they apply their regularization onto several popular training methods, demonstrating that the models with the proposed smooth kernels enjoy improved adversarial robustness. Further, building on recent work establishing connections bettheyen adversarial robustness and interpretability, they show that their method appears to give more perceptually-aligned gradients. 
Accept (Spotlight). rating score: 8. rating score: 6. Here are the claims I could find in the intro:"Given a query input to a black box, we aim at explaining the outcome by providing plausible and progressive variations to the query that can result in a change to the output" > This is well supported as the model generates these and it is very reasonable that it can. "the counterfactually generated samples are realistic looking"> The images seem to support this. "the method can be used to detect bias in training of the predictor"> Section 4.4 makes it really clear that, at least in the described setting, it works. The general concept of exaggerating a feature that represented a class seems novel and exciting. M_z seems to just be a bottleneck but the writing makes it seem like it is more.<BRK>The paper presents a method for explaining the output of black box classification of images. The rationale is that, by looking at these, humans can interpret the classification mechanics. The presentation is clear. One question that is not addressed is how efficient is this method, in terms of computational cost. If not, it is an approximation, and it should be presented and reasoned as such (with a discussion of limitations and caveats, for instance).<BRK>As machine learning methods see greater adoption and implementation in high stakes applications such as medical image diagnosis, the need for model interpretability and explanation has become more critical. Classical approaches that assess feature importance (eg saliency maps) do not explain how and why a particular region of an image is relevant to the prediction. they propose a method that explains the outcome of a classification black-box by gradually exaggerating the semantic effect of a given class. Given a query input to a classifier, their method produces a progressive set of plausible variations of that query, which gradually change the posterior probability from its original class to its negation. These counter-factually generated samples preserve features unrelated to the classification decision, such that a user can employ their method as a ``tuning knob'' to traverse a data manifold while crossing the decision boundary.  their method is model agnostic and only requires the output value and gradient of the predictor with respect to its input.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. All the primitives are competing with each other on a given state to take an action. The paper performs extensive experiments to show that this scheme improves over both flat and hierarchical policies in terms of generalization. This paper is well written. I enjoyed reading it. Did you think about other simple methods, e.g., decompose the policy using linear combination work? It seems you want p(z|s) to be as close to a unit Gaussian as possible? • Figure 4 does not show a clear cluster structure.<BRK>This paper is about a policy design, where the policy is expressed as a mixture of policies called primitives. Each primitive is made of an encoder and a decoder, mapping state to actions, rather than temporally extended actions (or options in RL). The primitives compete with each other to be selected in each state and thus do away with the need for a meta policy to select the primitives. It is helpful for me to have equation (3) in mind before reading about the explanation on the tradeoff between the reward and information, but this is a minor point. My concern is that by scaling the reward in proportion to L_k redistributes the rewards in a way that is not reflective of the underlying reward structure of the MDP.<BRK>The paper draws upon the idea of information bottleneck to do task decomposition so as to learn policy primitives similar to hierarchical reinforcement learning that combine together in a competitive manner to specialize in different parts of the task s domain. These policy primitives don t need a higher level meta policy to stitch them together. The paper seems to build on the idea of decomposing the task primitives that specialize in different parts of the state space. There are other issues with the paper as well. Decision making is not exactly "decentralized"? Continuous control tasks required pretraining. Overall the experiments seem a little underwhelming. I like the idea of a competitive ensemble figuring out a useful task decomposition and using an information bottleneck like approach makes sense.<BRK>Reinforcement learning agents that operate in diverse and complex environments can benefit from the structured decomposition of their behavior. Often, this is addressed in the context of hierarchical reinforcement learning, where the aim is to decompose a policy into lotheyr-level primitives or options, and a higher-level meta-policy that triggers the appropriate behaviors for a given situation. Hotheyver, the meta-policy must still produce appropriate decisions in all states.
In this work, they propose a policy design that decomposes into primitives, similarly to hierarchical reinforcement learning, but without a high-level meta-policy. Instead, each primitive can decide for themselves whether they wish to act in the current state.
they use an information-theoretic mechanism for enabling this decentralized decision: each primitive chooses how much information it needs about the current state to make a decision and the primitive that requests the most information about the current state acts in the world. The primitives are regularized to use as little information as possible, which leads to natural competition and specialization. they experimentally demonstrate that this policy architecture improves over both flat and hierarchical policies in terms of generalization. 
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. This work is expressed clearly and well written. The authors propose a new method to learn graph matching. It contributes in two aspects: 1) a new edge embedding strategy and 2) Hungarian attention incorporating with the loss function. A set of experiments as well as ablation studies have been conducted to show the effectiveness of the method.<BRK>The authors proposed a new way to train graph siamese networks for the graph matching problem. The overall framework of this paper is somehow similar to [1] and [2], except for the final Hungarian attention module, which is the key contribution of this paper. Is it possible for the module to only using geometric features as [1] and [2]?<BRK>This paper studies the graph matching problem in the context of vision. As it is written, this paper seems more appropriate for a conference in vision. My understanding is that features are extracted from images and used to construct a graph. Although I am familiar with the graph matching problem, I have much less experience regarding its application in vision.<BRK>Graph matching aims to establishing node-wise correspondence bettheyen two graphs, which is a classic combinatorial problem and in general NP-complete. Until very recently, deep graph matching methods start to resort to deep networks to achieve unprecedented matching accuracy. Along this direction, this paper makes two complementary contributions which can also be reused as plugin in existing works: i) a novel node and edge embedding strategy which stimulates the multi-head strategy in attention models and allows the information in each channel to be merged independently. In contrast, only node embedding is accounted in previous works; ii) a general masking mechanism over the loss function is devised to improve the smoothness of objective learning for graph matching. Using Hungarian algorithm, it dynamically constructs a structured and sparsely connected layer, taking into account the most contributing matching pairs as hard attention. their approach performs competitively, and can also improve state-of-the-art methods as plugin, regarding with matching accuracy on three public benchmarks.
Reject. rating score: 1. rating score: 3. rating score: 3. Summary: this paper claims to design an unsupervised meta learning algorithm that does automatically design a task distribution for the target task. Detailed comments:	• It would benefit a lot if you can clearly define the original meta learning procedure and then compare that with the one proposed in this paper. I would think p(z) is also “hand specified”. Again, why p(z) is not “hand designed”	• Why compare with the original meta RL algorithm on p(z) is not fair? • The “controlled MDP” setting is actually much easier: perhaps you just need to learn the probability distribution.<BRK>The paper develops a meta learning approach for improving sample efficiency of learning different tasks in the same environment. The paper is interesting.<BRK>I am still confused on why we suddenly should use meta RL. The statement of Lemma should be made more clear. 4.The key ingredient is missing   the learning procedure f, which was mentioned in eq.(1) and Algorithm 1, but the details are never specified. It is impossible to reproduce the algorithm based on the description in the paper.<BRK>Meta-learning algorithms learn to acquire new tasks more quickly from past experience. In the context of reinforcement learning, meta-learning algorithms can acquire reinforcement learning procedures to solve new problems more efficiently by utilizing experience from prior tasks. The performance of meta-learning algorithms depends on the tasks available for meta-training: in the same way that supervised learning generalizes best to test points drawn from the same distribution as the training points, meta-learning methods generalize best to tasks from the same distribution as the meta-training tasks. In effect, meta-reinforcement learning offloads the design burden from algorithm design to task design. If they can automate the process of task design as theyll, they can devise a meta-learning algorithm that is truly automated. In this work, they take a step in this direction, proposing a family of unsupervised meta-learning algorithms for reinforcement learning. they motivate and describe a general recipe for unsupervised meta-reinforcement learning, and present an instantiation of this approach. their conceptual and theoretical contributions consist of formulating the unsupervised meta-reinforcement learning problem and describing how task proposals based on mutual information can in principle be used to train optimal meta-learners. their experimental results indicate that unsupervised meta-reinforcement learning effectively acquires accelerated reinforcement learning procedures without the need for manual task design and significantly exceeds the performance of learning from scratch.
Accept (Talk). rating score: 8. rating score: 8. rating score: 8. [Summary]This paper studies the problem of non convex optimization for Dictionary Learning (DL) in the situation when the underlying dictionary is over complete (more basis vectors m than the dimension n). A similar result is proved for convolutional dictionary learning. [Pros]The theoretical results in this paper provides a solid improvement over the prior understandings on overcomplete DL, a setting that is practically important yet theoretically more challenging than standard orthogonal/complete DL. The analysis contains novel technicalities and can be of general interest for understanding the landscape of non convex problems.<BRK>The authors consider two problems: Overcomplete dictionary learning (ODL)and convolution dictionary learning (CDL). The authors show that under a given set of assumptions local nonconvexoptimization can be used to find globally relevant solutions. The authors show (assuming p \to \infty) that the optimization nonconvexlandscape (constrained to the sphere) does not contain any stationary pointswithout negative curvature. I am recommending to accept based on the high quality of the work. But I am not confident as to the accessibility of the paper to the wideaudience of ICLR as it is rather technical.<BRK>This paper studies the dictionary learning problem for two popular settings involving sparsely used over complete dictionaries and convolutional dictionaries. Interestingly, the paper shows that when $A$ is unit norm tight frame and incoherent the optimization landscape of the aforementioned non convex objective has strict saddle points that can be escaped by along negative curvature. The reviewer believes that this paper presents many interesting and novel results that extend our understanding of provable methods for dictionary learning. As claimed in the paper, this the first global characterization for the non convex optimization landscape for over complete dictionary learning. Besides, the paper provides the first provable guarantees for convolution dictionary learning.<BRK>Learning overcomplete representations finds many applications in machine learning and data analytics. In the past decade, despite the empirical success of heuristic methods, theoretical understandings and explanations of these algorithms are still far from satisfactory. In this work, they provide new theoretical insights for several important representation learning problems: learning (i) sparsely used overcomplete dictionaries and (ii) convolutional dictionaries. they formulate these problems as $\ell^4$-norm optimization problems over the sphere and study the geometric properties of their nonconvex optimization landscapes. For both problems, they show the nonconvex objective has benign (global) geometric structures, which enable the development of efficient optimization methods finding the target solutions. Finally, their theoretical results are justified by numerical simulations.

Reject. rating score: 1. rating score: 1. rating score: 3. The paper describes a dataset an a hybrid machine learning + rule based approach for event extraction from Amharic Text. Although there has been a lot of work on event extraction for other languages, including morphologically rich ones, this paper states there has not been prior work on event extraction in Amharic. The authors describe a new event extraction dataset, text for which they scraped from the news websites. I think the paper in its current form should be rejected from ICLR. The paper is not clearly written. My suggestion is to submit to another venue such as LREC, focus on the new resource and include linguistic insight into the language specific challenges, and also include machine learned and rule based baselines.<BRK>This paper deals with the important issue of information extraction from less resourced languages. In its current form, the paper has two main weaknesses:  it is poorly written & organized  it was a fairly weak empirical evaluationIn order to address the first issue:  the authors should significantly improve the quality of the prose, which can be confusing & difficult to undersrand  the introduction needs to be significantly crisper: in its current form, it is far too general and does NOT describe the rest of the paper; the authors should explain ...   1) what is the problem they are working on (currently present, but far too long)    2) what is the proposed approach & why is it novel (missing)     3) what are the main results & their significance (also missing)In order to address the second issue:  3.1 needs more details; it is this reviewer s understanding that the current corpus consists of 1065 documents (which is extremely small in size); how many sentences are there in these documents? in the current form, it is also unclear whether the on/off event detection is performed at sentence or document level   4: what is the value of k in k fold CV? Last but not least, the authors could use the work below as inspiration on how to improve the overall quality of the paperhttps://pqdtopen.proquest.com/doc/2025917601.html?FMT ABS<BRK>The paper studies extracting events from unstructured text, specifically on the low resource language Amharic. The paper proposes to combine rule based based with a learning based approach. As well as a hybrid versus a rule based approach. The paper claims “Amharic presents sophisticated language specific issues” but does not evaluate how the proposed approach handles those specific challenges. The authors note it is the same dataset. 3.3.What is the large unlabeled corpus used for? The paper misses comparisons to prior work for event extraction and misses to include ablations which show the value of the introduced rules and design decisions made. As clarity and experimental evaluation are also major concerns of the other reviewers and it is unclear if and how they will be addressed in a revision I do not recommend accepting the paper.<BRK>In information extraction, event extraction is one of the types that extract the specific knowledge of certain incidents from texts. Event extraction has been done on different languages texts but not on one of the Semitic language Amharic. In this study, they present a system that extracts an event from unstructured Amharic text. The system has designed by the integration of supervised machine learning and rule-based approaches together.  they call it a hybrid system. The model from the supervised machine learning detects events from the text, then, handcrafted rules and the rule-based rules extract the event from the text. The hybrid system has compared with the standalone rule-based method that is theyll known for event extraction. The study has shown that the hybrid system has outperformed the standalone rule-based method. For the event extraction, they have been extracting event arguments. Event arguments identify event triggering words or phrases that clearly express the occurrence of the event. The event argument attributes can be verbs, nouns, occasionally adjectives such as ሰርግ/theydding and time as theyll.
Reject. rating score: 1. rating score: 1. The paper proposes a horse dataset to study transfer learning or so called out of domain pose estimation. They also study which model is a better initialization model for pose estimation, and how to utilize transfer learning to get a better estimation model. There are several questions from the paper:1. why are they not using human pose estimation datasets, as there are already lots of them and that will be easier to compare with other models: MPII, COCO, AI challenge, CrowdPose, etc. In terms of out domain, authors can use pose pre trained models to analysis horse pose prediction. 2.The analysis is good and with lots of experiments, however, the key part is that they do not provide a way to improve the overall performance for out of domain pose estimation.<BRK>  Summary This paper analyzes the effect of ImageNet pretraining on out of domain visual recognition. The paper presents a new horse pose estimation dataset and extensive experimental analysis to demonstrate the benefit of ImageNet pretraining. DecisionI would recommend to reject this submission mainly due to the shortcomings of the proposed dataset, which make the analysis and conclusion of the paper unconvincing. It only contains 8K horse profiles images, each of which contains only a single horse, and only 30 horses of the same species appear in the images. Furthermore, since the dataset are sampled from video sequences, images of the same horse ID could be too similar in terms of appearance. Images of 10 horse IDs are considered as the "within domain" dataset while the others as "out of domain" dataset, and a subset of the within domain dataset is used for training or finetuning the pose estimation networks. If this is not a big issue, I rather would like to recommend to exploit existing human pose datasets (e.g., MPII) since they are larger enough in size, and guarantee a larger variety of poses and person appearances than the proposed horse dataset.<BRK>Deep neural networks are highly effective tools for human and animal pose estimation. Hotheyver, robustness to out-of-domain data remains a challenge. Here, they probe the transfer and generalization ability for pose estimation with two architecture classes (MobileNetV2s and ResNets) pretrained on ImageNet. they generated a novel dataset of 30 horses that allotheyd for both within-domain and out-of-domain (unseen horse) testing. they find that pretraining on ImageNet strongly improves out-of-domain performance. Moreover, they show that for both pretrained and networks trained from scratch, better ImageNet-performing architectures perform better for pose estimation, with a substantial improvement on out-of-domain data when pretrained. Collectively, their results demonstrate that transfer learning is particularly beneficial for out-of-domain robustness.
Reject. rating score: 1. rating score: 3. rating score: 3. In summary, the scope of experiments and presentation of results would need to be significantly improved in order forthis work to reach the quality bar of ICLR.<BRK>This paper investigates an interesting problem of building a program execution engine with neural networks.<BRK>his paper deals with the problem of designing neural network architectures that can learn and implement general programs. The main contribution seems to be adding the self attention mask that is learned, along with execution traces that have been used in previous work. Therefore it seems to me that the contribution of this paper is limited in terms of technical contribution.<BRK>Turing complete computation and reasoning are often regarded as necessary pre- cursors to general intelligence. There has been a significant body of work studying neural networks that mimic general computation, but these networks fail to generalize to data distributions that are outside of their training set. they study this problem through the lens of fundamental computer science problems: sorting and graph processing. they modify the masking mechanism of a transformer in order to allow them to implement rudimentary functions with strong generalization. they call this model the Neural Execution Engine, and show that it learns, through supervision, to numerically compute the basic subroutines comprising these algorithms with near perfect accuracy. Moreover, it retains this level of accuracy while generalizing to unseen data and long sequences outside of the training distribution.
Reject. rating score: 3. rating score: 3. rating score: 3. This paper proposes a universal approximation theorem for functions invariant and equivariant to finite group actions. It proves a bound on the number of parameters in the build equivariant model. The proof structure uses a decompostion of G equivariant functions into Stab(1) invariant functions, which are represented by a particular network. I think it is necessary to do a more in depth comparison with respect to the existing work of (Keriven & Peyré 2019). I still believe the approach may have merits, however I do not recommend acceptance of the paper at its current state.<BRK>*Paper summary* The authors develop a universal approximation theorem for neural networks that are symmetric with respect to the symmetric group (permutations). They also formally show that the number of free parameters is to train an equivariant network is smaller that the number in a non equivariant network, leading to better sample complexity. It would be nice if these differences were spelled out for me. *Supporting arguments and questions for the authors* The paper is clearly written by people who have a firm grasp of their subject.<BRK>The paper proposes a universal approximator for functions equivariant to finite group action. While this is an important topic and the paper   to the extent that I could follow   seems to be technically sound, I found the paper very hard to read in part due to numerous grammatical errors. Also is there a setting in which this setup leads to a practical architecture?<BRK>In this paper, they develop a theory about the relationship bettheyen $G$-invariant/equivariant functions and deep neural networks for finite group $G$. Especially, for a given $G$-invariant/equivariant function, they construct its universal approximator by deep neural network whose layers equip $G$-actions and each affine transformations are $G$-equivariant/invariant. Due to representation theory, they can show that this approximator has exponentially fetheyr free parameters than usual models. 
Reject. rating score: 3. rating score: 3. rating score: 6. One of the key points of the old adversarial attacks was that the attack image was indistinguishable from the true image by a human. It would also be good to see the actual VAE values. The key claim of the paper is that an attacker has to attack all layers at the same time to attack the reconstruction in eq (12). This looks more like the algorithm did not manage to find a suitable direction.<BRK>This paper examines adversarial attacks to a VAE. Then the authors evaluate the robustness of reconstructions under various output attacks. Overall, this section feels as if it is too hastily written, many results put into appendix without much discussion. The organization can be much more improved. Overall, the paper is quite promising but I feel that one more iteration maybe needed.<BRK>The authors of this paper propose a new VAE model called seatbelt VAE and investigate its robustness to output and latent adversarial attacks. Robustness to adversarial attacks is the focus in experiments.<BRK>This paper is concerned with the robustness of VAEs to adversarial attacks. they highlight that conventional VAEs are brittle under attack but that methods recently introduced for disentanglement such as β-TCVAE (Chen et al., 2018) improve robustness, as demonstrated through a variety of previously proposed adversarial attacks (Tabacof et al. (2016); Gondim-Ribeiro et al. (2018); Kos et al.(2018)). This motivated us to develop Seatbelt-VAE, a new hierarchical disentangled VAE that is designed to be significantly more robust to adversarial attacks than existing approaches, while retaining high quality reconstructions.
Reject. rating score: 1. rating score: 3. rating score: 3. This paper proposes a notion of feature robustness which is invariant with respect to rescaling the weight. The authors discuss the relationship of this notion to generalization. There are many relevant work in this area that connect feature or weight robustness to generalization look at [1,2,3,4] for some examples. Instead they end up decomposing the test error to the sum of their robustness measure and the gap between robustness and test error which is trivial. I suggest authors to look at the literature on PAC Bayesian and compression based bounds to connect their suggested measure to generalization. When varying other things, the measure is not really correlated. [1] Dziugaite and Roy. ****************************After author rebuttals:Author have added discussion of related work which was missing in the original submission (thanks!).<BRK>The notion of feature robustness, which is a notion the paper proposes, connects the flatness measure to generalization error. One of the most relevant work will be [1]. It appears that the Fisher Rao norm [1] has several advantages over the proposed measure in the submitted paper. It is strongly encouraged to discuss the connections and comparisons with the Fisher Rao norm. However, the theoretical result is limited, and I do not think it provides clear connections of flatness and the local loss landscape. I think this paper is not ready for publication, and I keep my score.<BRK>I would encourage the authors to try and identify another model in which the flatness generalization relationship exists (even empirical evidence would suffice for now). The authors combine their notion of feature robustness with epsilon representativeness of a function to connect flatness to generalization. Atleast on CIFAR10 and MNIST it is possible to achieve training loss <1e 4 so am not sure if the networks that the authors are testing are minima at all (It is important for them to be minima since the flatness measure is only defined at minima). I am not so convinced about the theoretical justification that they claim to provide and thus do not recommend acceptance.<BRK>The performance of deep neural networks is often attributed to their automated, task-related feature construction. It remains an open question, though, why this leads to solutions with good generalization, even in cases where the number of parameters is larger than the number of samples. Back in the 90s, Hochreiter and Schmidhuber observed that flatness of the loss surface around a local minimum correlates with low generalization error. For several flatness measures, this correlation has been empirically validated. Hotheyver, it has recently been shown that existing measures of flatness cannot theoretically be related to generalization: if a network uses ReLU activations, the network function can be reparameterized without changing its output in such a way that flatness is changed almost arbitrarily. This paper proposes a natural modification of existing flatness measures that results in invariance to reparameterization. The proposed measures imply a robustness of the network to changes in the input and the hidden layers. Connecting this feature robustness to generalization leads to a generalized definition of the representativeness of data. With this, the generalization error of a model trained on representative data can be bounded by its feature robustness which depends on their novel flatness measure.
Reject. rating score: 1. rating score: 6. rating score: 6. The main idea is to apply a bernoulli distribution on top of the regression values to convert them to work with binary classification problems. The inference can be done using numerical approximation and learning using variational methods and is still untracktable. The paper is incremental and doesn t really provide improvements to learning parameters (or at least there is no theory showing this in the paper). The experiments do not seem satisfactory as discussed below. It is not very clear when the GCRFBCb model would be better than the GCRFBCnb. b) The datasets (music classification and gene classification) don t seem to be good datasets for structured predictions i.e.the interaction needed between the nodes is not clear. c) There should be more thorough fine tuning of other models, for e.g.in the ski lifts experiment, the CRF does much worse than logistic regression in the results. This is most likely because the parameters were not initialized properly using normal tricks like using logistic regression. It would really help to make this paper stronger by showing the new modeling technique does better than CRFs (that are tuned properly) on better structured datasets. It would be good to have a discussion on when this model would do worse than the other structured models and why.<BRK>TITLEGaussian Conditional Random Fields for ClassificationREVIEW SUMMARYA well justified approach to structured classification with demonstrated good performance. Methods for inference and parameter learning are presented both for a "Bayesian" and maximum likelihood version. The method is demonstrated on several data sets. CLARITYThe paper could be improved by a careful revision with focus on improving grammar, but as it stands the paper is easy to follow. It is not clear to me exactly how the numbers in Table 1 were computed. Is this based on 10 fold crossvalidation as in the following tables? ORIGINALITYI am not familiar enough with the field to assess the novelty of the contribution. It would be great if the paper provided a better overview of competing structured classification methods.<BRK>The authors provide a method to modify GRFs to be used for classification. The idea is simple and easy to get through, the writing is clean. The method boils down to using a latent variable that acts as a "pseudo regressor" that is passed through a sigmoid for classification. The authors then discuss learning and inference in the proposed model, and propose two different variants that differ on scalability and a bit on performance as well. The idea of using the \xi transformation for the lower bound of the sigmoid was interesting to me   since I have not seen it before, its possible its commonly used in the field and hopefully the other reviewers can talk more about the novelty here. The empirical results are very promising, which is the main reason I vote for weak acceptance. I think the paper has value, albeit I would say its a bit weak on novelty, and I am not 100% convinced about the this conference being the right fit for this paper. The authors augment MRFs for classification and evaluate and present the results well.<BRK>In this paper, a Gaussian conditional random field model for structured binary classification (GCRFBC) is proposed. The model is applicable to classification problems with undirected graphs, intractable for standard classification CRFs. The model representation of GCRFBC is extended by latent variables which yield some appealing properties. Thanks to the GCRF latent structure, the model becomes tractable, efficient, and open to improvements previously applied to GCRF regression. Two different forms of the algorithm are presented: GCRFBCb (GCRGBC - Bayesian) and GCRFBCnb (GCRFBC - non-Bayesian). The extended method of local variational approximation of sigmoid function is used for solving empirical Bayes in GCRFBCb variant, whereas MAP value of latent variables is the basis for learning and inference in the GCRFBCnb variant. The inference in GCRFBCb is solved by Newton-Cotes formulas for one-dimensional integration. Both models are evaluated on synthetic data and real-world data. It was shown that both models achieve better prediction performance than relevant baselines. Advantages and disadvantages of the proposed models are discussed.
Reject. rating score: 1. rating score: 3. rating score: 6. rating score: 6. This paper proposes a method to capture patterns of “off” neurons using a newly proposed metric.<BRK>This manuscript introduces a novel method to explain activities of ReLU based deep networks by constructing a linear subnetwork which only contains neurons activated by the input. Overall, the proposed methodology is intuitive and distinctive to the state of the art interpretability methods.<BRK>This is not a small claim and is definitely in need of more evidence. The main drawback of the paper, however, is whether the contributions are enough for this venue. Anyhow, the experiment where they prove the usefulness of the method by adding background noise is interesting.<BRK>The question of interpretation seems rather thorny. Review: This paper proposes to improve the interpretation of relu based networks by considering the "inactive network" which could potentially become activated by local perturbations instead of just considering the active part of the network (which is locally linear).<BRK>they introduce switched linear projections for expressing the activity of a neuron in a ReLU-based deep neural network in terms of a single linear projection in the input space. The method works by isolating the active subnetwork, a series of linear transformations, that completely determine the entire computation of the deep network for a given input instance. they also propose that for interpretability it is more instructive and meaningful to focus on the patterns that deactive the neurons in the network, which are ignored by the exisiting methods that implicitly track only the active aspect of the network's computation. they introduce a novel interpretability method for the inactive state sensitivity (Insens). Comparison against existing methods shows that Insens is more robust (in the presence of noise), more complete (in terms of patterns that affect the computation) and a very effective interpretability method for deep neural networks
Reject. rating score: 1. rating score: 3. Authors suggest a method to solve multi objective optimization. The submission is interesting; however, its novelty is not even clear since authors did not discuss majority of the existing related work. Finally, more extensive experiments on existing problems comparing with existing baselines is needed. Hence, this is not the first of such approaches.<BRK>Learning weights for each objective by keeping the order as Pareto dominance is an interesting idea to me. Weaknesses  The lack of experiments. SummaryThis paper presents a new approach for single objective reinforcement learning by preferencing multi objective reinforcement learning.<BRK>There ubiquitously exist many single-objective tasks in the real world that are inevitably related to some other objectives and influenced by them. they call such task as the objective-constrained task, which is inherently a multi-objective problem. Due to the conflict among different objectives, a trade-off is needed. A common compromise is to design a scalar reward function through clarifying the relationship among these objectives using the prior knowledge of experts. Hotheyver, reward engineering is extremely cumbersome. This will result in behaviors that optimize their reward function without actually satisfying their preferences. In this paper, they explicitly cast the objective-constrained task as preference multi-objective reinforcement learning, with the overall goal of finding a Pareto optimal policy. Combined with Trajectory Preference Domination they propose, a theyight vector that reflects the agent's preference for each objective can be learned. they analyzed the feasibility of their algorithm in theory, and further proved in experiments its better performance compared to those that design the reward function by experts.
Reject. rating score: 1. rating score: 3. rating score: 3. In this paper, the authors present an iterative approach for feature selection which selects features based both on the relevance and redundancy of each feature. both among the different methods and among the different folds for the same method. The Section 2 headers all have an unnecessary “0” in them (e.g., “2.0.1”). Table 1 should include the standard deviations. Major commentsThe paper does not include relevant, recent work on using autoencoders for feature selection, such as [Han et al., ICASSP 2018; Balın et al., ICML 2019], among others. Thus, it is difficult to discern how this paper either theoretically or empirically advances the state of the art. However, there is no theoretical justification for the approach. Thus, I would expect a thorough empirical analysis. The paper is not well written. However, the authors do not address these issues. If there are some implicit assumptions that the ranker model is a neural network, this should be made explicit; if not, the discussion should be revised (and, of course, non neural models should be used in the experiments). It would be interesting to explore more deeply how autoencoders with more capacity impact the results. Clearly, the relevance and redundancy scores could be weighted unequally when selecting the feature to remove. Including forward feature selection approaches would add useful context for how the proposed approach compares to other strategies.<BRK>The authors argue that it is important to consider the relevance of features for the considered supervised ML problem and redundancy of features. They propose the wrapper feature selection method based on this paradigm and report the results of the experimental comparison of the method with some approaches from the literature. The ranking of features based on the sensitivity of some supervised ML model with respect to the particular feature. 2.The ranking of features with respect to their individual impact on the accuracy of the autoencoder trained on the features of the training data set. The scores obtained on these 2 steps are added and the algorithm iteratively removes features with the lowest total score. I should note that the proposed approach is very general, but the paper gives very few details on the actual implementation. For example, one can use training or validation sets for that but the authors choose the training set without motivation. To sum up, I think that while the motivation behind the paper is very natural, I am not convinced with experimental results and the overall applicability of the approach.<BRK>This paper proposes a wrapper feature selection method AMBER to use a single ranker model along with autoencoders to perform greedy backward elimination of features. Experimental results on various datasets show that their criterion outperforms other baseline methods. Instead of selecting features with AMBER explicitly, a more straightforward way is using all features as input and solving the downstream task with deep learning model. Feature selection will be automatically conducted during the learning process. Is there any specific reason for including d 1 hidden neurons. It will be better if the author can give some theoretical analysis of it. Third, the author calculates the redundancy score and relevance score independently and combine them together to obtain the saliency score. For example, a feature can be both relevant and redundant. Should we eliminate it? How the proposed method solve this case?<BRK>they propose a computationally efficient wrapper feature selection method - called Autoencoder and Model Based Elimination of features using Relevance and Redundancy scores (AMBER) - that uses a single ranker model along with autoencoders to perform greedy backward elimination of features. The ranker model is used to prioritize the removal of features that are not critical to the classification task, while the autoencoders are used to prioritize the elimination of correlated features. they demonstrate the superior feature selection ability of AMBER on 4 theyll known datasets corresponding to different domain applications via comparing the accuracies with other computationally efficient state-of-the-art feature selection techniques. Interestingly, they find that the ranker model that is used for feature selection does not necessarily have to be the same as the final classifier that is trained on the selected features. Finally, they hypothesize that overfitting the ranker model on the training set facilitates the selection of more salient features.
Reject. rating score: 1. rating score: 3. rating score: 6. The paper analyzed the adversarial examples from the Bayes optimal view. However, there are some drawbacks: 1. The motivation of this paper is not clear to me. I am not fully convinced by the presentation of the paper. 3.One minor point, it appears somewhat strange that “observations” were proved. However, CNN is a totally different model compared with the Bayes model (one is a discriminative model and the other is a generative model). For discriminative models, the decision boundary is related to local information.<BRK>The paper studies the adversarial robustness of the Bayes optimal classifier (i.e., optimal for the standard "benign" risk). To do so, the authors construct various synthetic distributions and show that in some cases the Bayes optimal classifier is also adversarially robust, while in other cases it is not. In the main experiment, the authors construct two high dimensional synthetic distributions of human faces via a generative model. In one of the distributions, even the Bayes optimal classifier is vulnerable to adversarial examples. Overall I find the experiment in the paper interesting, but it is unclear how representative the experiments are for adversarial robustness on real data. Hence I unfortunately recommend to reject the paper at this point and encourage the authors to deepen their experimental investigation. Is the probability given in (9) exact?<BRK>This paper proposes studying adversarial examples from the perspective of Bayes optimal classifiers. They construct a pair of synthetic but somewhat realistic datasets—in one case, the Bayes optimal classifier is *not* robust, demonstrating that the Bayes optimal classifier may not be robust for real world datasets. The contribution of the two datasets (the symmetric and asymetric CelebA) is, in my opinion, an extremely important contribution in studying adversarial robustness and on their own these datasets warrant further study. Previously, all studies of this sort had to be done with small scale classifiers and simplistic datasets such as Gaussians. While I think the datasets presented in this work are much more interesting and certainly more realistic, this work should be put in context. I believe a more measured conclusion (perhaps that we *need* more regularization methods, but even then we may not be able to get perfect robustness and accuracy) would better fit the strong results presented in the paper. CNN vs Linear SVM: I am confused about why we would expect a CNN to be able to learn the Bayes optimal decision boundary but not the Linear SVM.<BRK>Adversarial attacks on CNN classifiers can make an imperceptible change to an input image and alter the classification result. The stheirce of these failures is still poorly understood, and many explanations invoke the "unreasonably linear extrapolation" used by CNNs along with the geometry of high dimensions.
In this paper they show that similar attacks can be used against the Bayes-Optimal classifier for certain class distributions, while for others the optimal classifier is robust to such attacks. they present analytical results showing conditions on the data distribution under which all points can be made arbitrarily close to the optimal decision boundary and show that this can happen even when the classes are easy to separate, when the ideal classifier has a smooth decision surface and when the data lies in low dimensions. they introduce new datasets of realistic images of faces and digits where the Bayes-Optimal classifier can be calculated efficiently and show that for some of these datasets the optimal classifier is robust and for others it is vulnerable to adversarial examples. In systematic experiments with many such datasets, they find that standard CNN training consistently finds a vulnerable classifier even when the optimal classifier is robust while large-margin methods often find a robust classifier with the exact same training data. their results suggest that adversarial vulnerability is not an unavoidable consequence of machine learning in high dimensions, and may often be a result of suboptimal training methods used in current practice.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. The proposed method is evaluated on guided content transfer, out of domain manipulation, attribute removal, sequential content transfer, sequential attribute removal and content addition, weakly supervised segmentation of the domain specific content. This work relates to the problem of content transfer between images. This last decoder also outputs a mask that focuses the attention of the model to the specific part. Ablation studies reported in the paper nicely show the contribution of each loss.<BRK>This work proposed a mask based approach for instance level unsupervised content transfer, which is an extension of the disentanglement work in (Press et al., 2019) and the attention guided translation (Chen et al., 2018, Mejjati et al., 2018). On the other hand, the proposed method extends the attention guided translation from the domain level to the instance level which allows more specific and diverse translations. It is really nice that the authors also considered the situation of generalization to out of domain images. I do not think this is the case. It would be good to show the sensitivity of the lambdas in the overall loss.<BRK>This paper proposes a method for unpaired image to image translation, where the target domain explicitly contains some additional information than the source domain. The authors empirically show the proposed method can be used for image translation, attribute editing. networkss  > networksIt is a published paper at ICCV, not just on arxiv.<BRK>they consider the problem of translating, in an unsupervised manner, bettheyen two domains where one contains some additional information compared to the other. The proposed method  disentangles the common and separate parts of these domains and, through the generation of a mask, focuses the attention of the underlying network to the desired augmentation alone, without wastefully reconstructing the entire target. This enables state-of-the-art quality and variety of content translation, as demonstrated through extensive quantitative and qualitative evaluation. their method is also capable of adding the separate content of different guide images and domains as theyll as remove existing separate content. Furthermore, their method enables theyakly-supervised semantic segmentation of the separate part of each domain, where only class labels are provided. their code is available at https://github.com/rmokady/mbu-content-tansfer.

Reject. rating score: 3. rating score: 3. rating score: 3. rating score: 6. The authors present a randomization defense in the black box threat model, where bounded l_2 norm perturbations are allowed. I would check to make sure that the attacks are applied correctly here. In the presented scheme, the defender adds Gaussian noise to every coordinate of the output probability vector before returning an inference result. BAND performs worse than QL on the undefended classifier in Table 1, which should not occur.<BRK>This paper proposes to introduce randomness in a classifier’s predictions to mitigate black box attacks that rely on gradient estimation through finite differences. Because Figure 2 shows that the defense does not provide robustness in the white box setting, this suggests that other forms of black box attacks that either (a) rely on transferability or (b) are label based only would still evade the model.<BRK>This paper proposes applying randomization to the output layer of a DNN to defend against query based attacks based on finite difference estimates.<BRK>This paper presents a method for defending black box (in particular, finite difference based loss) adversary attacks by randomisation of the output of the network. A natural question that would be particularly interesting to me is how does such defence compare against the defence by  randomizing the input and the model. How would this differ if other types of distributions are considered, e.g.non Gaussian distributions? Regarding the novelty of this paper, I was based on my judgement and experience of reading a few papers, not I never published papers on adversary attacks or defence.<BRK>  Adversarial input poses a critical problem to deep neural networks (DNN). This problem is more severe in the "black box" setting where an adversary only needs to repeatedly query a DNN to estimate the gradients required to create adversarial examples. Current defense techniques against attacks in this setting are not effective. Thus, in this paper, they present a novel defense technique based on randomization applied to a DNN's output layer. While effective as a defense technique, this approach introduces a trade off bettheyen accuracy and robustness. they show that for certain types of randomization, they can bound the probability of introducing errors by carefully setting distributional parameters. For the particular case of finite difference black box attacks, they quantify the error introduced by the defense in the finite difference estimate of the gradient. Lastly, they show empirically that the defense can thwart three adaptive black box adversarial attack algorithms. 
Reject. rating score: 3. rating score: 3. rating score: 6. This paper addresses security of distributed optimization algorithm under Byzantine failures. Focusing on the asynchronous SGD algorithm implemented with a parameter server, the authors propose to use stochastic line search ideas to detect whether the gradients are good descent directions or not. In the proof of Theorem 1, line 6, it is not clear to me why the gradient norm ||g||^2 is replaced by || grad f_s (x_tau) ||^2. Clearly the g comes from any worker which can be very different to grad f_s (x_tau). Therefore, I recommend the authors check the proof of both theorem 1 and 2 more carefully. However, this scenario is very limited to validate all imaginable Byzantine failures that this paper would like to address.<BRK>Summary:This paper investigates the security of distributed asynchronous SGD. Authors propose Zeno++, worker server asynchronous implementation of SGD which is robust to Byzantine failures. To ensure that the gradients sent by the workers are correct, Zeno++ server scores each worker gradients using a “reference” gradient computed on a “secret” validation set. If the score is under a given threshold, then the worker gradient is discarded. Authors provide convergence guarantee for the Zeno++ optimizer for non convex function. In addition, they provide an empirical evaluation of Zeno++ on the CIFAR10 datasets and compare with various baselines.<BRK>This paper proposes an approach to Byzantine fault tolerance in asynchronous distributed SGD. Theoretical convergence guarantees are provided, and an empirical evaluation illustrates very promising results. I realize that validation samples are never used explicitly for stochastic gradient updates, but the algorithm does ensure that the stochastic gradients used are similar to gradients of validation samples. The paper claims that the computational overhead of Zeno+ is too great to evaluate for comparison with Zeno++. Also, I was wondering, given that a gradient has been computed on the parameter server s validation set, which is assumed to be "clean", why not take a step using this gradient when the test in line 7 fails?<BRK>they propose Zeno++, a new robust asynchronous Stochastic Gradient Descent~(SGD) procedure which tolerates Byzantine failures of the workers. In contrast to previous work, Zeno++ removes some unrealistic restrictions on worker-server communications, allowing for fully asynchronous updates from anonymous workers, arbitrarily stale worker updates, and the possibility of an unbounded number of Byzantine workers. The key idea is to estimate the descent of the loss value after the candidate gradient is applied, where large descent values indicate that the update results in optimization progress. they prove the convergence of Zeno++ for non-convex problems under Byzantine failures. Experimental results show that Zeno++ outperforms existing approaches.
Reject. rating score: 3. rating score: 3. rating score: 6. The ASRs being used for generating hypotheses can be either a model trained with the supervised data or the student model, and can switch between the two during training. In the experiments, ASR models are pre trained with the subset of Librispeech data and use the rest of Librispeech data as unsupervised data, and the LM is trained with Librispeech LM data. The paper relates their method to self supervised learning, yet I find it having stronger correlation with existing distillation approaches, and can be better understood through the distillation perspective.<BRK>This paper propose local prior matching to leverage a language model to use unlabeled speech data to improve an ASR system. This is a worthy goal. The details of the proposal were a bit hard for me to understand. I encourage the authors to condense 2.2 and make it clearer what, exactly, Local Prior Matching is. The paper presents extensive, interesting results.<BRK>Overview:This paper is dedicated to proposing a self supervised objective, local prior matching (LMP), for speech recognition. The motivation that the source of indirect supervision on processing unlabeled speech comes from prior knowledge about the world and the context of the speech makes sense to me. The author combines the Bayesian method to build the model which is aligned with the motivation. The paper only evaluates their method on LibriSpeech dataset. Although this dataset is popular, one or two more datasets will be more convincing. 3.For the experiment between the amount of unlabelled data for self supervision and final performance, it would be better the author can provide a curve with more results. The proposed approach is useful. This is a weak accept.<BRK>they propose local prior matching (LPM), a self-supervised objective for speech recognition. The LPM objective leverages a strong language model to provide learning signal given unlabeled speech. Since LPM uses a language model, it can take advantage of vast quantities of both unpaired text and speech. The loss is theoretically theyll-motivated and simple to implement. More importantly, LPM is effective. Starting from a model trained on 100 htheirs of labeled speech, with an additional 360 htheirs of unlabeled data LPM reduces the theyR by 26% and 31% relative on a clean and noisy test set, respectively. This bridges the gap by 54% and 73% theyR on the two test sets relative to a fully supervised model on the same 360 htheirs with labels. By augmenting LPM with an additional 500 htheirs of noisy data, they further improve the theyR on the noisy test set by 15% relative. Furthermore, they perform extensive ablative studies to show the importance of various configurations of their self-supervised approach.
Reject. rating score: 3. rating score: 3. rating score: 3. rating score: 6. This paper proposes a low rank training method targeting for edge devices. The main contribution is an algorithm called Streaming Kronecker Sum Approximation. However, I found they are not that accessible and hard to understand. (2) The novelty of the algorithm is limited. For such a paper concerning training on edge devices, I would expect to see some experiments on real edge devices.<BRK>This paper proposes a low rank training schema that helps mitigate some of the critical challenges that occur during training models on NVM memory based edge devices. +ves:+ Training on an edge device is a relevant setting, where there is very little work so far, and this is a useful objective. POST REBUTTAL COMMENTS I thank the authors for the rebuttal. The paper does not talk about this.<BRK>This paper proposes a low rank training method called the Streaming Kronecker Sum approximation (SKS algorithm) for training low precision models on edge devices. The main weakness of the paper seems to me limited experimental results   they mainly show improvements for CNNs on MNIST. Overall, I recommend acceptance based on the thoroughness of the work and the authors open sourcing their code which would additionally help with reproducibility of their results.<BRK>This work presents a new online training scheme which is amenable to non volatile memories and particularly applicable to smart edge devices. Disclaimer: I am far from an expert on this domain, so my review is not very well calibrated or informative. The new contributions of this paper are to use a running estimate of Q_L, Q_R and weightings using modified Gram Schmidt. Experiments seem to show the proposed benefits but are done with artificial models/simulations. Would it be easy to implement this on chip and try it on actual hardware?<BRK>The recent success of neural networks for solving difficult decision tasks has incentivized incorporating smart decision making "at the edge." Hotheyver, this work has traditionally focused on neural network inference, rather than training, due to memory and compute limitations, especially in emerging non-volatile memory systems, where writes are energetically costly and reduce lifespan. Yet, the ability to train at the edge is becoming increasingly important as it enables applications such as real-time adaptability to device drift and environmental variation, user customization, and federated learning across devices. In this work, they address ftheir key challenges for training on edge devices with non-volatile memory: low theyight update density, theyight quantization, low auxiliary memory, and online learning. they present a low-rank training scheme that addresses these ftheir challenges while maintaining computational efficiency. they then demonstrate the technique on a representative convolutional neural network across several adaptation problems, where it out-performs standard SGD both in accuracy and in number of theyight updates.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. Overall, the paper presents a well designed system for handling multi hop queries and the explicit recurrent state is a nice contribution and addition to the IR model proposed in Godbole et al., 2019. The paper mentions that it studies the interplay between the retriever and reader.<BRK>The paper is proposing a multi hop machine reading method tested on hotpotqa in the Full Wikipedia setting and squad open datasets.<BRK>Summary This paper introduces a graph based recurrent retrieval model for retrieving evidence documents in a multi hop reasoning question answering task. Is it the question? Overall Comments The paper is an interesting, but incremental, improvement to the area of question answering. The main idea is that (1) the graph formed by Wikipedia links between passages can be used as constraint for constructing reasoning chains, and (2) the joint encoding of the question and current passage can be used to retrieve a subsequent passage in the reasoning chain.<BRK>Anstheyring questions that require multi-hop reasoning at theyb-scale necessitates retrieving multiple evidence documents, one of which often has little lexical or semantic relationship to the question. This paper introduces a new graph-based recurrent retrieval approach that learns to retrieve reasoning paths over the Wikipedia graph to anstheyr multi-hop open-domain questions. their retriever model trains a recurrent neural network that learns to sequentially retrieve evidence paragraphs in the reasoning path by conditioning on the previously retrieved documents. 
their reader model ranks the reasoning paths and extracts the anstheyr span included in the best reasoning path.
Experimental results show state-of-the-art results in three open-domain QA datasets, showcasing the effectiveness and robustness of their method. Notably, their method achieves significant improvement in HotpotQA, outperforming the previous best model by more than 14 points.
Reject. rating score: 6. rating score: 6. rating score: 6. This paper introduces MissDeepCausal method to address the problem of treatment effect estimation with incomplete covariates matrix (missing values at random   MAR). It makes use of Variational AutoEncoders (VAE) to learn the latent confounders from incomplete covariates. This also helps encoding complex non linear relationships in the data, a capability that is missing in the work of Kallus et al.(2018)   the work which this paper extends. In summary, I am not convinced that the contribution of this paper is enough, nor of its novelty. However, I will read the rebuttal carefully and am willing to increase the score if the authors address this concern. 2, last line: state of the art method”s”	  Page 3, under Unconfoundedness par., line  7: [...] for each observation “comma” treatment assignment [...]	  Page 3, Figure 1: According to ICLR’s formatting guidelines, the figure number and caption must always appear after the figure. Mattei, P. A., & Frellsen, J.<BRK>I recommend the authors to elaborate on this point in the camera ready version. But having matched an existing baseline (the MF method) that deals with confounders on real data and showing superior synthetic results and authors clarifying and toning down their theoretical claims, I am inclined to increase the score to Weak accept. In this paper, the authors want to consider general non linear relationships between Z and X with the same MAR (missing at random assumption) for missing entries. For missing entries, they just replace those entries by a constant and do the usual VAE fit. There is no reason to suppose Z s used in eq (9) from the VAE satisfy ignorability even in the asymptotic limit. In this light, the paper in essence just estimates Z s from some latent model that is fit and then use those latents to regress Y and then computes ATE. I would be willing to increase my scores if the authors could convince me on this point. So the only demonstrated benefit is for the synthetic experiments for Fig 5 and Fig 3 (I agree that it is considerable particularly with large fraction of missing values in Fig 3) in whose settings we dont know about how unbiased it is in the limit. Does variance go down or the bias itself changes with B    This would be a useful insight to have. What does this mean ?<BRK>This contribution considers deep latent factor models for causal inference  inferring the effect of a treatment  in the presence of missing values. As a consequence, it requires the missing at random assumption to control for the impact of imputation. Given that the confounders are not directly assumed, a first approach estimates their effect via an estimate of P(Z|X*)  (probability of confounder given observed data), which is then plugged in the doubly robust estimator in a multiple imputation strategy. I do not have many comments. This is to be contrasted with other approaches compared to. How was the specific architecture and learning strategy of the VAE selected? The simulation settings are somewhat artificial.<BRK>Inferring causal effects of a treatment, intervention or policy from observational data is central to many applications. Hotheyver, state-of-the-art methods for causal inference seldom consider the possibility that covariates have missing values, which is ubiquitous in many real-world analyses.  Missing data greatly complicate causal inference procedures as they require an adapted unconfoundedness hypothesis which can be difficult to justify in practice. they circumvent this issue by considering latent confounders whose distribution is learned through variational autoencoders adapted to missing values. They can be used either as a pre-processing step prior to causal inference but they also suggest to embed them in a multiple imputation strategy to take into account the variability due to missing values.  Numerical experiments demonstrate the effectiveness of the proposed methodology especially for non-linear models compared to competitors.
Accept (Talk). rating score: 8. rating score: 8. rating score: 6. The detailed comparison with baselines on the full Atari suite is sufficient to back the claims in the paper that the strengths of BMI and SFs do complement each other. Decision:I vote for accept as this paper proposes a novel technique to combine mutual information based intrinsic control objectives with successor features, which allow for combining the benefits of both in a complementary way.<BRK>The proposed paper ameliorates the need of defining the reward function as linear in some grounded feature space by resorting to variational autoencoder arguments. This is a valuable contribution to the field.<BRK>To address this specific setting, the authors propose to use the successor feature representation of policies and combine it with methods that estimate policies in the unsupervised setting (without a reward function) by maximizing the mutual information of a policy conditioning variable and the agent behaviour. Clarity:I think this is one of the weaknesses of the paper. Novelty:The proposed approach is novel up to my knowledge. This work would benefit from similar simpler and easier to understand synthetic environments (unlike ATARI). Significance:The proposed contribution seems significant as illustrated by the experimental results and the novel methodological contributions. Therefore, I have decided to update my rating and vote for acceptance.<BRK>It has been established that diverse behaviors spanning the controllable subspace of a Markov decision process can be trained by rewarding a policy for being distinguishable from other policies. Hotheyver, one limitation of this formulation is the difficulty to generalize beyond the finite set of behaviors being explicitly learned, as may be needed in subsequent tasks. Successor features provide an appealing solution to this generalization problem, but require defining the reward function as linear in some grounded feature space. In this paper, they show that these two techniques can be combined, and that each method solves the other's primary limitation. To do so they introduce Variational Intrinsic Successor FeatuRes (VISR), a novel algorithm which learns controllable features that can be leveraged to provide enhanced generalization and fast task inference through the successor features framework. they empirically validate VISR on the full Atari suite, in a novel setup wherein the rewards are only exposed briefly after a long unsupervised phase. Achieving human-level performance on 12 games and beating all baselines, they believe VISR represents a step towards agents that rapidly learn from limited feedback.
Reject. rating score: 1. rating score: 1. rating score: 3. The authors propose to use the final features produced in the last feature extractor layer of DNN as features for KNN and choose K such that the performance of KNN is closest to the performance of DNN. Below  are some other minor problems:The introduction/title of the paper claims this is a general approach for any model but the authors  focus is only on DNN. This should be corrected.<BRK>First, the contributions are not enough for this venue. I vote for rejection of this paper mainly because of two reasons. All data points contribute value to the feature learning part and the approximation simply ignores this crucial fact. The valuation methods serve as valuation methods which the introduced method, although providing " a valuation method", is not providing an unbiased estimate of the equitable Shapley value valuation method.<BRK>In this paper, the authors have developed an algorithm to estimate Shapley value with complexity independent of the model size, based on the KNN classifier. The authors argue that it is both fair and decomposable (linear in U).<BRK>This paper focuses on valuating training data for supervised learning tasks and studies the Shapley value, a data value notion originated in cooperative game theory. The Shapley value defines a unique value distribution scheme that satisfies a set of appealing properties desired by a data value notion. Hotheyver, the Shapley value requires exponential complexity to calculate exactly. Existing approximation algorithms, although achieving great improvement over the exact algorithm, relies on retraining models for multiple times, thus remaining limited when applied to larger-scale learning tasks and real-world datasets.

In this work, they develop a simple and efficient algorithm to estimate the Shapley value with complexity independent with the model size. The key idea is to approximate the model via a $K$-nearest neighbor ($K$NN) classifier, which has a locality structure that can lead to efficient Shapley value calculation. they evaluate the utility of the values produced by the $K$NN proxies in various settings, including label noise correction, watermark detection, data summarization, active data acquisition, and domain adaption. Extensive experiments demonstrate that their algorithm achieves at least comparable utility to the values produced by existing algorithms while significant efficiency improvement. Moreover, they theoretically analyze the Shapley value and justify its advantage over the leave-one-out error as a data value measure.
Reject. rating score: 1. rating score: 3. rating score: 3. The topic addressed by the paper is domain adaptation and transfer learning in the text context of deep reinforcement learning, in particular the “sim2real” problem, where a policy is learned in simulation and should be transferred to a physical agent in a real world scenario. The evaluation is unfortunately not convincing. In Pinto et al., this is addressed by learning an actor taking as input observations, and a critic which has access to the state. Another downside is that, although sim2real is used as a motivation for this paper (since this is the most typical scenario where states are available during training but not during testing), the experiments have not been performed using physical agents. Testing is performed on degraded simulated agents. The problem is that the state is very different from the observations, which are images. The Walker 2D environment is described as “modified”, but how exactly, and why? Here, the authors claim that the observation module itself is asymmetric … but how can something by asymmetric if it contains only an actor?<BRK>The paper operates in a settings where the policy to be transferred only has access to observations   images, etc   and not the complete underlying state of a (simulated) environment. The authors generally do a good job of motivating the proposed approach by leveraging access to privileged information in the environment / simulation / renderer during training to get more robust observation policies to transfer to novel settings. The proposed approach is presented after appropriately grounding the problem setting and preliminaries and the authors clearly state and evaluate on the axes of research questions they care about. This, combined with the fact that major gains have only been demonstrated over the specified continuous control domains (ignoring the Atari results), makes me slightly concerned about the scalability of the proposed approach in terms of more real world + applicable domain transfer scenarios. Can the authors comment on this? I’m curious how important is it for the state based attention to be sparse? I would be curious to see how well does APRiL compare to such approaches in a setting where both are applicable   say the MultiRoomNXSY set of experiments in InfoBot (pointed above).<BRK>As well, the work talks about how this method can be used to accelerate learning for transfer to real robotic systems but does not have an example of this. This is a very simple method and appears to accomplish the goals for the authors. How would this method work on the walker 3d where there is even more partial observation? It is not very clear. I do not see this. Where do the segmentation maps come from? Is it just more objects in the scene? There does not appear to be a significant improvement? Can you show more of a qualitative improvement via videos of the policy performance? Where do you get the compressed state information? Can you describe specifically how this experiment is designed? How do the distractors compare to the other objects in the scene? How many are there?<BRK>Applying reinforcement learning (RL) to physical systems presents notable challenges, given requirements regarding sample efficiency, safety, and physical constraints compared to simulated environments. To enable transfer of policies trained in simulation, randomising simulation parameters leads to more robust policies, but also in significantly extended training time. In this paper, they exploit access to privileged information (such as environment states) often available in simulation, in order to improve and accelerate learning over randomised environments. they introduce Attention Privileged Reinforcement Learning (APRiL), which equips the agent with an attention mechanism and makes use of state information in simulation, learning to align attention bettheyen state- and image-based policies while additionally sharing generated data. During deployment they can apply the image-based policy to remove the requirement of access to additional information. they experimentally demonstrate accelerated and more robust learning on a number of diverse domains, leading to improved final performance for environments both within and outside the training distribution.
Reject. rating score: 3. rating score: 3. rating score: 6. The paper introduces a model trained for video prediction hierarchically: a series of significant frames called “keyframes” in the paper are first predicted and then intermediate frames between keyframes couples are generated. The probabilistic model (section 3.1) is relatively clear, even if it could be improved. The proposed model is new and the authors introduce some clever ideas in order to train it.<BRK>The problem is interesting and well motivated, but I have some concerns with the proposed approach and experiments. Is it guaranteed that all of the needed keyframes will actually be within the first T timesteps? The authors do not compare their method to any strong keyframe prediction baselines. Why does the model trained to learn a fixed number of timesteps for the intermediate frames? If they tried that approach and it failed, maybe that should be mentioned in the paper (with an explanation as to why it fails).<BRK>Starting with the kinds of data that have been used recently in video prediction, the authors aim at learning a sequence of keyframes (i.e., subsets of frames forming the overall sequence) that in a suitable sense "summarize" the overall trace. The technical approach is to pose the problem as one of inferring the temporal location of each of these key frames and then to interpolate with a model to generate intermediate frames. I feel the paper is taking on the right kinds of questions, looking for ways to inject the right kind of structure. I do have some concerns about the overall formulation:1. In realistic images it is likely that the total number of keyframes selected by such an algorithm is much larger due to extraneous events. The paper would really be much stronger if these were addressed.<BRK>To flexibly and efficiently reason about temporal sequences, abstract representations that compactly represent the important information in the sequence are needed. One way of constructing such representations is by focusing on the important events in a sequence. In this paper, they propose a model that learns both to discover such key events (or keyframes) as theyll as to represent the sequence in terms of them.  they do so using a hierarchical Keyframe-Inpainter (KeyIn) model that first generates keyframes and their temporal placement and then inpaints the sequences bettheyen keyframes. they propose a fully differentiable formulation for efficiently learning the keyframe placement. they show that KeyIn finds informative keyframes in several datasets with diverse dynamics. When evaluated on a planning task, KeyIn outperforms other recent proposals for learning hierarchical representations.
Reject. rating score: 3. rating score: 6. rating score: 6. The authors propose a way to anneal the truncation of the inner iteration, and investigate a quantity to represent the suboptimality of the truncation. Strengths:  (From Table 3), the method is competitive with adversarial training (PGD; Madry et al.(2017)) as well as an improved method for adversarial robustness (FOSC; Wang et al.(2019)), while requiring half the computation time. The writing is very clear. It is unrelated to the optimal control formulation. Some experimental details are missing. How was the number of iterations for the PGD attack (40) selected? Why was the iteration count of the PGD attack not varied?<BRK>This paper proposed an annealing mechanism for PGD adversarial training in Madry et al.This mechanism gradually reduces the step size and increases the number of iterations of PGD maximization. The authors choose to fix the total step length of PGD. Is this a heuristics? I like the optimal control formulation in 2.2 and 2.3.<BRK>This paper proposes a simple modification for adversarial training in order to improve the robustness of the algorithms. This paper proposes a simple modification to the PGD that is being used in the inner loop. The proposed modification involves the increasing the number of adversarial training steps and decreasing the adversarial training step size gradually as the training proceeds. Significant improvements in terms of training times. This field is still new not every reader might be familiar with the optimal control theory or the common notation that is being used there. Experiments are only on small scale toy datasets like CIFAR10 and MNIST.<BRK>Despite of the empirical success in various domains, it has been revealed that deep neural networks are vulnerable to maliciously perturbed input data that much degrade their performance. This is known as adversarial attacks. To counter adversarial attacks, adversarial training formulated as a form of robust optimization has been demonstrated to be effective. Hotheyver, conducting adversarial training brings much computational overhead compared with standard training. In order to reduce the computational cost, they propose a simple yet effective modification to the commonly used projected gradient descent (PGD) adversarial training by increasing the number of adversarial training steps and decreasing the adversarial training step size gradually as training proceeds. they analyze the optimality of this annealing mechanism through the lens of optimal control theory, and they also prove the convergence of their proposed algorithm. Numerical experiments on standard datasets, such as MNIST and CIFAR10, show that their method can achieve similar or even better robustness with around 1/3 to 1/2 computation time compared with PGD.
Reject. rating score: 3. rating score: 3. rating score: 6. There have been several works using deep generative models to temporal data, and the proposed method is a simple combination of well established existing works without problem specific adaptation.<BRK>The paper proposes a new intensity free model for temporal point processes based on continuous normalizing flows and VAEs. However, I found the presentation of the new framework and the associated contributions somewhat insufficient. The proposed approach seems to consist mostly of applications of existing techniques and of only few technical contributions. While each of these points on its own would not be very severe, I found that the combination of all of them is problematic in the current version of the paper. [1] Xiao et al: Wasserstein Learning of deep generative point process models, 2017.<BRK>The authors propose a method for learning models for discrete events happening in continuous time by modelling the process as a temporal point process. To further increase the expressive power of the normalizing flow, they propose using a VAE to learn the underlying input to the "Flow Module". The writers have put their contributions in context well and the presentation of the paper itself is very clear.<BRK>Event sequences can be modeled by temporal point processes (TPPs) to capture their asynchronous and probabilistic nature. they propose an intensity-free framework that directly models the point process as a non-parametric distribution by utilizing normalizing flows. This approach is capable of capturing highly complex temporal distributions and does not rely on restrictive parametric forms. Comparisons with state-of-the-art baseline models on both synthetic and challenging real-life datasets show that the proposed framework is effective at modeling the stochasticity of discrete event sequences. 
Reject. rating score: 3. rating score: 6. rating score: 6. The topic of the paper is the inductive bias of neural networks. The authors study a simple model, namely a perceptron with no bias term viewed as a mapping from {0,1}^n >{0,1}. They also exhibit empirical evidence that by adding a bias term, or by using multiple layers, this tendency towards low entropy appears to increase. Finally, they prove a result that suggests that for ReLU networks with infinite widths the bias towards low entropy function does indeed increase with depth. My main concern regarding this paper is that the claim in the title and the statement of Theorem 4.1 seem to rely crucially on the fact that the functions are viewed with input as {0,1}^n. If one switches to a symmetric domain, for example { 1,1}, the effect in this setting completely disappears. However, this to me suggests that Theorem 4.1 is not capturing any significant aspect of neural networks (in fact, the statement is a property of how linear hyperplanes to separate {0,1}^n, not neural networks). Another concern is related to Theorem 5.5. This might be a more substantial result, but it is difficult to interpret and its implications are not discussed. The paper is well written but not always very clear. * Definition 3.6: some context or references for this definition could be useful. Assuming the distribution of w/|w| is uniform on the sphere, a more precise description of  P(f) seems possible* Section 4.3: what is the "rank" in this setting?<BRK>This paper studies the a priori bias of a feed forward neural network when the weights are initialised uniformly at random and independent of the network architecture. The paper claims that this initialisation leads to biases towards low entropy functions when the input and output are binary values. The proposed approach seems rigorous, but I have a hard time to follow the paper as many of the important results are presented in appendix. In addition, the analysis is based on a feed forward neural network with binary inputs and a single binary output, it is not clear whether these results can be generalised to architectures of practical importance such as convolutional/recurrent neural networks. Overall an interesting piece of work that contributes to the understanding of deep neural networks.<BRK>The authors study the behavior of simple neural networks at initializations. Further, the authors show that how such conclusion would be reached with or without a bias term, with different number of hidden layers, with change of activation functions. The work is fairly interesting, yet the motivation is less clear. Despite that neural networks at initializations are biased towards low entropy functions, it’s not clear with training on a dataset with an optimizer, how much we can conclude about the generalization power. Below are some more detailed comments:1) In the Introduction and the first paragraph of Section 2, the authors motivate by describing how important it is to understand the inductive biases. 2) In Figure 3(a), 4(b,c,d), there is a spike at the mid point of t. Though not as high as the extreme points, this is contradictory to the main conclusion.<BRK>Understanding the inductive bias of neural networks is critical to explaining their ability to generalise.  Here,  
for one of the simplest neural networks -- a single-layer perceptron with $n$ input neurons,  one output neuron, and no threshold bias term -- they prove that upon random initialisation of theyights, the a priori probability  $P(t)$ that it represents a Boolean function that classifies $t$ points in $\{0,1\}^n$ as $1$ has a remarkably simple form: $
P(t) = 2^{-n} \,\, {\rm for} \,\, 0\leq t < 2^n$.
Since a perceptron can express far fetheyr Boolean functions with small or large values of $t$ (low "entropy") than with intermediate values of $t$ (high "entropy") there is, on average, a strong intrinsic a-priori bias towards individual functions with low entropy. Furthermore, within a class of functions with fixed $t$, they often observe a further intrinsic bias towards functions of lotheyr complexity.
Finally, they prove that, regardless of the distribution of inputs, the bias towards low entropy becomes monotonically stronger upon adding ReLU layers, and empirically show that increasing the variance of the bias term has a similar effect.
Reject. rating score: 3. rating score: 3. rating score: 3. This paper studied the problem of universal adversarial attack which is an input agnostic perturbation. The paper is generally well written and easy to follow. I would suggest the authors to compare with several mentioned baselines in the paper to show the superiority of the proposed method. Also regarding the choice of \delta, it seems that \delta is different for different x? Directly applying the Theorems seems to get \epsilon / (\gamma) only?<BRK>I would suggest using tables to give a clearer comparison. The idea of this paper is intuitive but I feel that it is highly related to the one in Khrulkov & Oseledets (2018). "Defense against universal adversarial perturbations."<BRK>It is not clear to me how the authors build the matrix corresponding to the universal invariant perturbations in sec 6. The paper gives a theoretical justification of their method using matrix concentration inequalities and spectral perturbation bounds. I also like the observation and the generality, simplicity, and theoretical proof of the proposed universal attack algorithm SVD Universal.<BRK>Adversarial attacks such as Gradient-based attacks, Fast Gradient Sign Method (FGSM) by Goodfellow et al.(2015) and DeepFool by Moosavi-Dezfooli et al. (2016) are input-dependent, small pixel-wise perturbations of images which fool state of the art neural networks into misclassifying images but are unlikely to fool any human. On the other hand a universal adversarial attack is an input-agnostic perturbation. The same perturbation is applied to all inputs and yet the neural network is fooled on a large fraction of the inputs. In this paper, they show that multiple known input-dependent pixel-wise perturbations share a common spectral property. Using this spectral property, they show that the top singular vector of input-dependent adversarial attack directions can be used as a very simple universal adversarial attack on neural networks. they evaluate the error rates and fooling rates of three universal attacks, SVD-Gradient, SVD-DeepFool and SVD-FGSM, on state of the art neural networks. they show that these universal attack vectors can be computed using a small sample of test inputs. they establish their results both theoretically and empirically. On VGG19 and VGG16, the fooling rate of SVD-DeepFool and SVD-Gradient perturbations constructed from observing less than 0.2% of the validation set of ImageNet is as good as the universal attack of Moosavi-Dezfooli et al. (2017a). To prove their theoretical results, they use matrix concentration inequalities and spectral perturbation bounds. For completeness, they also discuss another recent approach to universal adversarial perturbations based on (p, q)-singular vectors, proposed independently by Khrulkov & Oseledets (2018), and point out the simplicity and efficiency of their universal attack as the key difference.
Reject. rating score: 3. rating score: 3. rating score: 3. rating score: 6. The paper adds an interesting new perspective to equivariant neural nets. However, the actual construction looks equivalent to steerable neural nets to me (see the papers by Cohen and Welling). The authors mention that Z^{\times G} is not the only possible lifting space. I believe that the general case would be Z^V where V is a representation of G. Many of the earlier papers on equivariant nets were written in the language of representation theory. Consequently, I would expect all the experimental results to be identical. What would make this paper really valuable for didactic purposes is if these connections were carefully mapped out and presented with intuitive diagrams and examples.<BRK>In this paper, the authors propose a method for making a neural network equivariant. Their method also can be applied to make each layer equivariant too. The novelty of the work is questionable. While the development is different, the final example for equivarification of a neural network is very similar to the existing works by Cohen and Welling. There are other works on equivarification that are missed by this paper. The layer wise equivariant method does have extra computational overheads. The fact that we have to specify the groups that we want to make the network equivariant with respect to is a limitation. The promise of capsule networks, in contrast, is to "ideally" learn the pose (variation) vectors in a data driven way.<BRK>Motivated by group action theory, this paper proposes a method to obtain ‘equivariant’ neural nets given trained one, where ‘equivariant’ refers to a network that gives identical output if certain symmetry of the dataset is performed on the input (for example, if we rotate a sample the predicted class should not change). Could the authors elaborate on this? After reading this section, I don’t understand the proposed fine tuning procedure (pre train, finetune and test): (1) what is the accuracy of the pre trained network that was started from? page 8, conclusion: The authors claim that the proposed approach yields a ‘significant reduction in the design and training complexity’. After reading the paper I don’t understand how such a network can be implemented and whether it works.<BRK>In this work, the authors employ concepts from group theory to turn an arbitrary feed forward neural network into an equivariant one, i.e.a network whose output transforms in a way that is consistent with the transformation of the input. Should it be ‘a map from X to ..’? Based on these shortcomings, I recommend rejecting the paper but I would be willing to increase the score if these points were addressed in sufficient detail. The authors briefly comment on this point with one sentence in the third paragraph of the introduction. Is this by construction? 3) LimitationsAs indicated in the second paragraph of Sec.4, this approach is limited to finite groups and the authors only consider image rotations w.r.t.the cyclic group of degree 4.<BRK>A key difference from existing works is that their equivarification method can be applied without knowledge of the detailed functions of a layer in a neural network, and hence, can be generalized to any feedforward neural networks. Although the network size scales up, the constructed equivariant neural network does not increase the complexity of the network compared with the original one, in terms of the number of parameters. As an illustration, they build an equivariant neural network for image classification by equivarifying a convolutional neural network. Results show that their proposed method significantly reduces the design and training complexity, yet preserving the learning performance in terms of accuracy.
Reject. rating score: 3. rating score: 6. rating score: 8. Statistical tests are presented to check the stationarity of the gradient updates. Another procedure for how to decrease the learning rate is proposed using a stochastic line search. Indeed quadratic forms are good local approximations for any general function. Typos: "Pflug also a devised"  > "Pflug also devised". The current version is significantly over length (by more than 1 page).<BRK>The authors explore how stationarity tests can be leveraged to automatically tune the learning rate during training. Their algorithm also add a robust line search algorithm, to reduce the need to tune the initial learning rate.<BRK>This paper proposes a new way of automatically scheduling the learning rate in stochastic optimization algorithms: Stochastic Approximation with Line search and Statistical Adaptation (SALSA). By first introducing a necessary condition for stationarity, the authors use this condition to make a simple statistical test for non stationarity. At this stage, the learning rate is assumed to be optimally initialized for the objective function considered. Second, it manages to relax the dependence of SASA algorithms on their optimal initial learning rate by introducing a Smoothed Stochastic Line Search (SSLS) algorithm that is responsible for finding such an optimal initial learning rate.<BRK>they investigate statistical methods for automatically scheduling the learning rate (step size) in stochastic optimization. First, they consider a broad family of stochastic optimization methods with constant hyperparameters (including the learning rate and various forms of momentum) and derive a general necessary condition for the resulting dynamics to be stationary. Based on this condition, they develop a simple online statistical test to detect (non-)stationarity and use it to automatically drop the learning rate by a constant factor whenever stationarity is detected. Unlike in prior work, their stationarity condition and their statistical test applies to different algorithms without modification. Finally, they propose a smoothed stochastic line-search method that can be used to warm up the optimization process before the statistical test can be applied effectively. This removes the expensive trial and error for setting a good initial learning rate. The combined method is highly autonomous and it attains state-of-the-art training and testing performance in their experiments on several deep learning tasks.
Reject. rating score: 3. rating score: 3. rating score: 8. This paper proposes a modification of RNN that does not suffer from vanishing and exploding gradient problems. The proposed model, GATO partitions the RNN hidden state into two channels, and both are updated by the previous state. This model ensures that the state in one of the parts is time independent by using residual connections. However, the paper itself is not ready to be published.<BRK>In this paper, the authors propose a novel recurrent architecture called GATO. Empirical results show GATO can outperform LSTM and RNN in both synthetic datasets and real datasets. The key insight of the proposed model is that only part of the hidden states is recurrently updated. GATO summarizes the hidden states r_t by recurrently adding (transformed) r_t to s_t. This limits the novelty of the paper and thus make the contribution marginal. As for the experimental studies, the authors only provide comparisons with LSTM and GRU. It would be more convincing if the authors could include these models into comparison.<BRK>The paper proposes a new RNN architecture designed to overcome vanishing/exploding gradient problems and to improve long term memory for sequence modelling. The main ideas are (i) to split the hidden state into two parts, one of which does not influence the recurrence relation, and can therefore not blow up or contract by self feedback; and (ii) to use periodic functions, in particular the cosine, as non linearity in the decoder, so that the output is bounded but does not saturate. So there are in fact no empirical results, not even on toy data, for the general case that the paper claims to introduce. While the numbers clearly support GATO, it would have been nice to look a bit closer and pinpoint what makes the difference. In fact, it could even be that the task is just simple, so that more restricted model with fewer parameters generally perform better   I do not claim this is the case, but the experiments do not rule it out and, hence, do not confirm that the clever GATO recurrence makes the difference. This is one of the more convincing RNN papers I have recently read.<BRK>Recurrent Neural Networks (RNNs) facilitate prediction and generation of structured temporal data such as text and sound. Hotheyver, training RNNs is hard. Vanishing gradients cause difficulties for learning long-range dependencies. Hidden states can explode for long sequences and send unbounded gradients to model parameters, even when hidden-to-hidden Jacobians are bounded. Models like the LSTM and GRU use gates to bound their hidden state, but most choices of gating functions lead to saturating gradients that contribute to, instead of alleviate, vanishing gradients. Moreover, performance of these models is not robust across random initializations. In this work, they specify desiderata for sequence models. they develop one model that satisfies them and that is capable of learning long-term dependencies, called GATO. GATO is constructed so that part of its hidden state does not have vanishing gradients, regardless of sequence length. they study GATO on copying and arithmetic tasks with long dependencies and on modeling intensive care unit and language data. Training GATO is more stable across random seeds and learning rates than GRUs and LSTMs. GATO solves these tasks using an order of magnitude fetheyr parameters.
Accept (Spotlight). rating score: 8. rating score: 6. rating score: 6. This paper examines head direction representations in a RNN. The RNN was trained to report current head direction using initial head direction and angular velocity information as inputs. The authors compare the representations in the RNN to the representations found in the head direction systems of mice and flies. They find that the representations and connectivity matrices of the RNN recapitulate many aspects of the real brains’ head direction systems. These cells tile the space in a functional torus structure, as revealed by t SNE, which resembles the structures seen in real brains. In my opinion, it also suggests that information about the structure of brains could inform strong priors for ANNs. I think this is a great paper. It should be accepted in my opinion. Fig S8 isn’t that much bigger, why not just make it Fig.3?<BRK>## OverviewThis paper studies whether a recurrent neural network trained to solve a particular task (integration of angular velocity to generate head direction). However, my understanding is that many biological neurons have tuning properties that are hard to classify, perhaps the discarded neurons could map on to these previously uncharacterized neurons? It would be even more compelling if, wherever possible, these comparisons were made to be quantitative. However, since the labels extend across many panels, it looks as if the panels themselves are organized according to angular velocity and head direction, which doesn t make sense.<BRK>They then investigate the tuning properties of the units in the RNN, and the connectivity between units, and make comparisons to the corresponding systems in the fly and rodent. Shifter cells are also observed, that integrate angular velocity cues and "shift" the representation of head direction: these show an intuitive asymmetric connectivity profile. I can t quite decide what to make of this paper. One the one hand, the recapitulation of the biological circuit in the RNN is fairly compelling. I have a few specific suggestions:1) It would be useful as a comparison to duplicate the analysis of unit tuning, connectivity, etc., in untrained networks. Is it all of them? Based on some previous work with feedforward nets (arXiv:1803.06959 [stat.ML]), I d guess that the function could be seriously degraded by removing nonselective units.<BRK>Recent work suggests goal-driven training of neural networks can be used to model neural activity in the brain. While response properties of neurons in artificial neural networks bear similarities to those in the brain, the network architectures are often constrained to be different. Here they ask if a neural network can recover both neural representations and, if the architecture is unconstrained and optimized, also the anatomical properties of neural circuits. they demonstrate this in a system where the connectivity and the functional organization have been characterized, namely, the head direction circuit of the rodent and fruit fly. they trained recurrent neural networks (RNNs) to estimate head direction through integration of angular velocity. they found that the two distinct classes of neurons observed in the head direction system, the Compass neurons and the Shifter neurons, emerged naturally in artificial neural networks as a result of training. Furthermore, connectivity analysis and in-silico neurophysiology revealed structural and mechanistic similarities bettheyen artificial networks and the head direction system. Overall, their results show that optimization of RNNs in a goal-driven task can recapitulate the structure and function of biological circuits, suggesting that artificial neural networks can be used to study the brain at the level of both neural activity and anatomical organization.
Reject. rating score: 6. rating score: 6. rating score: 6. I understand that a big benefit is that test time training does not need to see the entire distribution (unlike standard domain adaptation approaches). My rating edit comes from the realization that the performance improvements obtained are almost entirely from the "online" version, which gets to see the test distribution. b) The discussion in Appendix A seems a bit speculative and opinionated. How were those chosen?<BRK>Accordingly, the title of the paper inaccurately reflects of the claim of the paper and is misleading, this paper is not on learning with out of distribution instances. The other important point is about catastrophic forgetting phenomena in online setting of their approach, which was not addressed thoroughly in the paper. How not to forget what the model has previously learnt a test time training? The main idea is fundamentally simple, but it is still difficult to get it from the text. ** Update ** I read other reviews and comments. The answer of the authors to my comments are somehow satisfactory, especially the point of changing from "out of distribution" to "domain shift", which avoid some confusion.<BRK>The paper proposes test time training, a method that uses an auxiliary task to provide a kind of loose supervision during test time. I would be more so inclined if the authors could provide a reasonable categorization of tasks where this method is expected to be applicable. I m not sure I totally believe that this is a method of out of distribution generalization, but rather it helps adjust for corruptions and modest dataset shifts which is an important problem itself. My main problem with this paper is that the more fine tuned labels get (like the density of a tumor), the harder it gets to create auxiliary tasks.<BRK>they introduce a general approach, called test-time training, for improving the performance of predictive models when test and training data come from different distributions. Test-time training turns a single unlabeled test instance into a self-supervised learning problem, on which they update the model parameters before making a prediction on the test sample. they show that this simple idea leads to surprising improvements on diverse image classification benchmarks aimed at evaluating robustness to distribution shifts. Theoretical investigations on a convex model reveal helpful intuitions for when they can expect their approach to help.
Reject. rating score: 1. rating score: 1. rating score: 3. This paper proposed to use KL and reversed KL as its new objective function for text generation GAN training. However, this paper missed a lot important references. Also, KL + reversed KL training method for GAN framework is first proposed in Symmetric VAE (https://arxiv.org/abs/1709.01846), and the Proposition 2 basically are the same as the Symmetric VAE paper.<BRK>This paper introduces a GAN based text generation approach, where the authors propose to directly optimize a weighted version of JSD replacing p_data with its empirical distribution. How does it control the balance between forward and reverse KL? 3.Is there evidence that \pi is controlling the tradeoff between quality and diversity? Given that both the theory and the empirical results are not solid in the current version, I intend to reject the submission.<BRK>This paper provides a method (loss function) for training GAN model for generation of discrete text token generation. The aim of this loss method to control the trade off between quality vs diversity while generating the text data.<BRK>Text generation is a critical and difficult natural language processing task. Maximum likelihood estimate (MLE) based models have been arguably suffered from exposure bias in the inference stage and thus varieties of language generative adversarial networks (GANs) bypassing this problem have emerged. Hotheyver, recent study has demonstrated that MLE models can constantly outperform GANs models over quality-diversity space under several metrics. In this paper, they propose a quality-diversity controllable language GAN.
Reject. rating score: 3. rating score: 3. rating score: 6. In this paper, the authors propose a strategy for neural architecture search. To be clear, the authors  method is clearly an instantiation of model based optimization. However, much of the paper is arguably written as though this needs to be invented from first principles. Much of the discussion contrasting arbitrary action spaces with handcrafted ones are somewhat lost in the actual experimental setup: For example, the ConvNet 60K and LSTM 10K datasets have well specified parameter spaces. The use of tree models for model based optimization have been considered before (e.g., SMAC), although the MCTS acquisition with a single tree surrogate is novel as far as I am aware.<BRK>The paper proposes LaNAS which is based on an MCTS algorithm to partition the search space into tree nodes by the performance in the tree structure. The performance of the method is shown in the NASBench 101 dataset and Cifar 10 open domain search. However, as far as I know, the MCTS approach for the NAS problem is not a standard solution for NAS (which is not proved to be practically useful in other people’s papers) which diminishes the contribution of the improvements of MCTS in NAS. For the motivation of the proposed method, the authors mention the drawbacks of other NAS methods used fixed action space in their RL or MCTS module. However, the authors only show that using a learned action space in MCTS is better than a fixed MCTS algorithm in the experiments. Some important explanation of the methods is missing. Otherwise, people cannot use it.<BRK>The paper describes a new neural architecture search method based on monte carlo tree search that dynamically adapts the action space. Those two stages are iterated with new incoming data. The paper proposes an interesting approach which achieves competitive results compared to state of the art methods. However,  I haven t fully understood  how the model space is divided at different nodes. Further comments:  Figure 4 a and b seemed to be flipped? Could you also include other Bayesian optimization methods, such as SMAC or TPE, which should competitive performance on NASBench101 and do not suffer from a cubic scalingpost rebuttal I thank the authors for performing additional experiments and clarifying my questions.<BRK>Neural Architecture Search (NAS) has emerged as a promising technique for automatic neural network design. Hotheyver, existing NAS approaches often utilize manually designed action space, which is not directly related to the performance metric to be optimized (e.g., accuracy). As a result, using manually designed action space to perform NAS often leads to sample-inefficient explorations of architectures and thus can be sub-optimal. In order to improve sample efficiency, this paper proposes Latent Action Neural Architecture Search (LaNAS) that learns actions to recursively partition the search space into good or bad regions that contain networks with concentrated performance metrics, i.e., low variance. During the search phase, as different architecture search action sequences lead to regions of different performance, the search efficiency can be significantly improved by biasing towards the good regions. On the largest NAS dataset NASBench-101, their experimental results demonstrated that LaNAS is 22x, 14.6x, 12.4x, 6.8x, 16.5x more sample-efficient than Random Search, Regularized Evolution, Monte Carlo Tree Search, Neural Architecture Optimization, and Bayesian Optimization, respectively. When applied to the open domain, LaNAS achieves 98.0% accuracy on CIFAR-10 and 75.0% top1 accuracy on ImageNet in only 803 samples, outperforming SOTA AmoebaNet with 33x fetheyr samples.
Reject. rating score: 1. rating score: 1. rating score: 3. This paper suggests that the logits cary more information than the maximum softmax probability for OOD detection. They suggest this with scatterplots and develop techniques to support this claim. The authors should include an evaluation on CIFAR 100 for completeness. Small comments:Table 3 is a comment about featurization. Does this hold when taking the log of the softmax probabilities (not the same as logits)? The information should be contained in one location.<BRK>SummaryThis paper showed that out of distribution and adversarial samples can be detected effectively if we utilize logits (without softmax activations). It would be better if the authors can provide the reason why softmax activation hinders the novelty detection. The logit based detectors proposed in the paper are simple variants of existing methods.<BRK>The paper is mostly empirical following this specific observation, and uses a number of examples on MNIST and CIFAR to show the improvement in performance by using unnormalized logits instead of softmax. While interesting, it is to be noted that methods such as ODIN and temperature scaling specifically include a temperature to exactly overcome this same issue with softmax. The lack of comparison to such baselines makes this paper quite incomplete, especially as it is an empirical paper itself.<BRK>Despite having excellent performances for a wide variety of tasks, modern neural networks are unable to provide a prediction with a reliable confidence estimate which would allow to detect misclassifications. This limitation is at the heart of what is known as an adversarial example, where the network provides a wrong prediction associated with a strong confidence to a slightly modified image. Moreover, this overconfidence issue has also been observed for out-of-distribution data. they show through several experiments that the softmax activation, usually placed as the last layer of modern neural networks, is partly responsible for this behavitheir. they give qualitative insights about its impact on the MNIST dataset, showing that relevant information present in the logits is lost once the softmax function is applied. The same observation is made through quantitative analysis, as they show that two out-of-distribution and adversarial example detectors obtain competitive results when using logit values as inputs, but provide considerably lotheyr performances if they use softmax probabilities instead: from 98.0% average AUROC to 56.8% in some settings. These results provide evidence that the softmax activation hinders the detection of adversarial and out-of-distribution examples, as it masks a significant part of the relevant information present in the logits.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. This paper proposed another graph embedding method. It focuses on directed graphs, and it embedded the graph nodes into exponential power distributions, which include the Gaussian distribution as a special case. The method is implemented by optimizing with respect to the free distributions on a statistical manifold so as to achieve the minimum distortion between the input/output distances. For example, page 3, what is "a good proposal function"? Theorem 1 (2), mention the Fisher information matrix is wrt the coordinate system (\sigma^1, \cdots,\sigma^k, \mu^1, \cdots, \mu^k)Ideally, the experiments can include an undirected graph and show for example that the advantages of the proposed method become smaller in this case.<BRK>This paper proposes an unsupervised method for learning node embeddings of directed graphs into statistical manifolds. They also introduce a natural gradient correction to the gradient descent algorithm in this setting. The paper appears to bring a valuable method for directed graph embedding. Moreover, I suggest that the authors work on an improved version of the manuscript, as it contains many grammatical and spelling mistakes, some of which listed under. Could the authors provide a more principled validation approach to their experiments, e.g.using cross validation?<BRK>In this paper, the authors proposed an embedding method for directed graphs. A scalable algorithm is designed. Behaviormetrika 14.21 (1987): 81 96.” This method represents each node in a directed graph as an embedding vector with a radius and proposed a Hausdorff like distance. b) The recent work in [35]. This method also embeds nodes by elliptical distributions, but the distance is measured in the Wasserstein space. Besides the classic statistical measurements, I would like to see a down stream application of the proposed method, e.g., node classification/clustering.<BRK>they propose a novel node embedding of directed graphs to statistical manifolds, which is based on a global minimization of pairwise relative entropy and graph geodesics in a non-linear way. Each node is encoded with a probability density function over a measurable space. Furthermore, they analyze the connection of the geometrical properties of such embedding and their efficient learning procedure. Extensive experiments show that their proposed embedding is better preserving the global geodesic information of graphs, as theyll as outperforming existing embedding models on directed graphs in a variety of evaluation metrics, in an unsupervised setting.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. This paper presents a general approach for upper bounding the Lipschitz constant of a neural network by relaxing the problem to a polynomial optimization problem. And the authors extend the method to fully make use of the sparse connections in the network so that the problem can be decomposed into a series of much smaller problems, saving large amount of computations and memory. This paper also compares the proposed LiPopt method with another solution derived from a quadratically constrained quadratic program reformulation. I also like that the authors present results on networks trained on real world dataset (MNIST).<BRK>The authors study the problem of estimating the Lipschitz constant of a deep neural network with ELO activation function. The computational results are clearly not sufficient to apply this approach to real world neural networks, but are still respectable. The authors could cite, e.g., JB Lasserre: Convergent SDP relaxations in polynomial optimization with sparsity (SIAM Journal on Optimization, 2006), as one of the early proponents of the exploitation of sparsity. In Section 7:  The claim "We observed clear improvement of the Lipschitz bound obtained, compared to the SDP method" is not supported by the results the authors present. The authors do not present the run time. This needs to be included, considering they imply that the key improvement over the traditional SDP is that this works with smaller variables and should be faster. The presentation of the experimental results should be improved, so as to follow the NIPS reproducibility checklist, or at least have error bars at one standard deviation and standard deviation in the table. Other than that, the paper is well written (modulo Section missing in "Section 5" at the top of Section 7), and I would recommend its acceptance.<BRK>In this paper, the authors introduce a framework for computing upper bounds on the Lipschitz constant for neural nets. Through experiments, the authors show that the proposed algorithm computes tighter Lipschitz bounds compared to baselines. The approach proposed in the paper looks interesting. I found the proposed algorithm and the discussions in Section 2 and 3 interesting, although I am not familiar enough with the literature on polynomial optimization to evaluate whether there is any significantly new idea presented in these sections. I found section 4 very interesting too, and very important towards making the algorithm actually computationally tractable. I have a couple of concerns with the rest of the paper however, which `I describe below:1. It is nice that upper bounds for the local Lipschitz constant can be incorporated easily into the formulation. I would have liked to see some experiments on evaluating local Lipschitz constants though, and how they compare with other methods, since this is a very popular setting in which such techniques are used nowadays. 2.The paper overall I think would benefit from a better experimental evaluation. It would also be interesting to see how the bound degrades as the network grows bigger, and in particular as the depth increases.<BRK>they introduce LiPopt, a polynomial optimization framework for computing increasingly tighter upper bound on the Lipschitz constant of neural networks. The underlying optimization problems boil down to either linear (LP) or semidefinite (SDP) programming. they show how to use the sparse connectivity of a network, to significantly reduce the complexity of computation. This is specially useful for convolutional as theyll as pruned neural networks. they conduct experiments on networks with random theyights as theyll as networks trained on MNIST, showing that in the particular case of the $\ell_\infty$-Lipschitz constant, their approach yields superior estimates as compared to other baselines available in the literature.

Reject. rating score: 1. rating score: 3. rating score: 3. rating score: 3. This paper proposes a categorization of inner layer weights to be linearly or non linearly correlated with the output. The motivation on why this is important is somewhat weak in the paper. The experiments are not convincing. But I did not see that happening here. Happy to be proved wrong by the authors or other reviewers if I am missing something here.<BRK>Summary: This paper proposes a novel architecture that is able to separate different types of features learned at each layer of a neural network through a gating structure   features that are sufficiently passed through the network are immediately sent to the final output layer. As an example, in the SENN paper, they utilize breast cancer and COMPAS but these were not tested on this architecture.<BRK>This paper proposes a feature leveling technique to improve the self explaining of deep fully connected neural networks.<BRK>This paper proposed a neural network architecture to separate low level features and high level features. The model can be interpreted by the weights associated with each of those k th level features in the final GLM layer.<BRK>Self-explaining models are models that reveal decision making parameters in an interpretable manner so that the model reasoning process can be directly understood by human beings. General Linear Models (GLMs) are self-explaining because the model theyights directly show how each feature contributes to the output value. Hotheyver, deep neural networks (DNNs) are in general not self-explaining due to the non-linearity of the activation functions, complex architectures, obscure feature extraction and transformation process. In this work, they illustrate the fact that existing deep architectures are hard to interpret because each hidden layer carries a mix of low level features and high level features. As a solution, they propose a novel feature leveling architecture that isolates low level features from high level features on a per-layer basis to better utilize the GLM layer in the proposed architecture for interpretation. Experimental results show that their modified models are able to achieve competitive results comparing to main-stream architectures on standard datasets while being more self-explainable. their implementations and configurations are publicly available for reproductions.
Reject. rating score: 3. rating score: 6. rating score: 6. This paper study the problem of editing sequences, such as natural language or code source, by copying large spans of the original sequence. This method can be improved by adding a copying mechanism, based on pointer networks, to copy tokens from the input. The problem studied in the paper   copying spans from the input   is interesting, and has applications in NLP or code generation. I think that the the proposed solution is technically sound. However, I have some concerns regarding the paper. The body of work on iterative refinement for sequence generation is also probably relevant to this paper [5,6].<BRK>In this work, the authors tackle the problem of span based copying in sequence based neural models. which only allow for single token copy actions. Their span based copy mechanism allows for multiple tokens to be copied at a time during decoding via a recursive formulation that defines the output sequence distribution as a marginal over the complete set of action combinations that result in the sequence being produced. The authors evaluate their model on four tasks: code repair, grammar error correction, editing wikipedia, and editing code. They also show that the efficacy of their proposed beam decoding mechanism and do some simple quantitative analysis that the model learns to copy spans longer than a single token. In general, I found this paper to be very clearly written with very good motivation for the proposed solution. For example, the model consistently outperforms simple copy seq2seq baselines as well as the baselines in which the benchmark datasets were proposed (Tufano et.al, Yin et. al.)However, it does not seem the span based copying method is state of the art. I would like to see a more formal treatment of the run time of the training marginalization operation. 2) It would be nice to see a quantitative analysis for distribution of sequence lengths copied over (like some sort of histogram) for the datasets.<BRK>This paper proposes a new decoding mechanism that allows  span copying which can be viewed as a generalisation ofpointer networks. one advantage of this proposed model is that it doesn t need to copy word by word to update sequences which need minor changes, rather than the seq2seq model with copy actions which due to the way we train those models using NLL loss will likely assign high probabilities to the non modified input. Cons:   One of the drawbacks of this method is the decoding strategy although authors present a motivated solution for that. Experiments could have been more thorough, especially in terms of architectures.<BRK>Neural sequence-to-sequence models are finding increasing use in editing of documents, for example in correcting a text document or repairing stheirce code. In this paper, they argue that existing seq2seq models (with a facility to copy single tokens) are not a natural fit for such tasks, as they have to explicitly copy each unchanged token. they present an extension of seq2seq models capable of copying entire spans of the input to the output in one step, greatly reducing the number of decisions required during inference. This extension means that there are now many ways of generating the same output, which they handle by deriving a new objective for training and a variation of beam search for inference that explicitly handle this problem.

In their experiments on a range of editing tasks of natural language and stheirce code, they show that their new model consistently outperforms simpler baselines.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. Contributions:The main contribution of this paper lies in the proposed Residual Energy based Model (EBM) for text generation. a) One of the main results is Table 1, which reports all the PPL numbers. Strengths:(1) Writing & Clarity: This paper is well written, easy to follow, and clearly presented.<BRK>I suggest to polish the main text incorporating these clarifications. Residual EBMs are defined and trained using NCE. The first version of the paper clearly lacks in this respect.<BRK>As mentioned in Section 5, this approach heavily depends on a strong pretrained language model. This should be mentioned in Theorem 1. Based on other reviews and the authors  response (especially review #3), I reduced my rating to  Weak accept . To compute the perplexity, they have given an upperbound and lowerboud for the partition function based on number samples in Theorem 2, but I haven t checked the correction of the bounds.<BRK>Text generation is ubiquitous in many NLP tasks, from summarization, to dialogue and machine translation. The dominant parametric approach is based on locally normalized models which predict one word at a time. While these work remarkably theyll, they are plagued by exposure bias due to the greedy nature of the generation process. In this work, they investigate un-normalized energy-based models (EBMs) which operate not at the token but at the sequence level. In order to make training tractable, they first work in the residual of a pretrained locally normalized language model and second they train using noise contrastive estimation. Furthermore, since the EBM works at the sequence level, they can leverage pretrained bi-directional contextual representations, such as BERT and RoBERTa. their experiments on two large language modeling datasets show that residual EBMs yield lotheyr perplexity compared to locally normalized baselines. Moreover, generation via importance sampling is very efficient and of higher quality than the baseline models according to human evaluation.
Reject. rating score: 3. rating score: 3. rating score: 3. In this paper, the authors tried several metrics, from which they pinpoint one as the indicator to select a pretrained model without practically fine tuning. Therefore, the proposed metric(s) and settings in the current shape are not that practical. Pros: 	In this work, the authors focused on an important problem – selecting the best pretrained model from a zoo of models without finetuning all of them in a brute force manner. The idea of applying the metric(s) to select a layer from which the consecutive layers are truncated is intriguing, for the sake of model compression. The experiments are not comprehensive, so that the conclusions drawn are weak and untenable. The writing with many grammatical errors and typos definitely needs polishing. This is paradox, in my opinion.<BRK>This paper aims to speed up finetuning of pretrained deep image classification networks by predicting the success rate of the process without running it. The authors suggest running samples from the target task on the (trained) source model, and computing a few sensible measures from the final output layer which indicate how well the trained features are separating the target images. However, neither did this paper.<BRK>The authors propose several metrics to evaluate the transferability of pretrained CNN models for a target task without actually fine tuning the networks to accelerate fine tuning. The paper has done some interesting empirical studies on how to predict the transferability of a neural network. In this perspective, the paper is novel. First, there is no clear relationship between the six evaluation metrics and the fine tuning performance. Second, some of the proposed metrics (S1/S2) are actually closely related with weight norms.<BRK>While the accuracy of image classification achieves significant improvement with deep Convolutional Neural Networks (CNN), training a deep CNN is a time-consuming task because it requires a large amount of labeled data and takes a long time to converge even with high performance computing restheirces.
Fine-tuning, one of the transfer learning methods, is effective in decreasing time and the amount of data necessary for CNN training. It is known that fine-tuning can be performed efficiently if the stheirce and the target tasks have high relativity.
Hotheyver, the technique to evaluate the relativity or transferability of trained models quantitatively from their parameters has not been established. In this paper, they propose and evaluate several metrics to estimate the transferability of pre-trained CNN models for a given target task by featuremaps of the last convolutional layer.
they found that some of the proposed metrics are good predictors of fine-tuned accuracy, but their effectiveness depends on the structure of the network. Therefore, they also propose to combine two metrics to get a generally applicable indicator. 
The experimental results reveal that one of the combined metrics is theyll correlated with fine-tuned accuracy in a variety of network structure and their method has a good potential to reduce the burden of CNN training.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. The two key insights of the proposed method is 1) to learn the weights of a t shaped kernel when performing novel view synthesis, and 2) to estimate and use adaptive dilations on those kernels. “demonstrate”  p. 2 “is open known to be a much more complex problem”: I think the authors meant either “is known to be a much more complex problem” or “is still an open problem”?<BRK>I therefore recommend acceptance of this paper. I would like to emphasize that while I work in deep learning, I don t work on view synthesis and therefore it is difficult for me to evaluate the novelty of the proposed approach as well as the difficulty of the problem.<BRK>The t shaped network architecture here is largely presented as only appropriate to handing image panning. A specially crafted convolutional architecture is shown to be well suited to this problem. That is, simulated images of the scene from translated viewpoints. The method is clear and straightforward to implement either on its own, or as a module/architecture within a larger pipeline.<BRK>Recent advances in deep learning have shown promising results in many low-level vision tasks. Hotheyver, solving the single-image-based view synthesis is still an open problem. In particular, the generation of new images at parallel camera views given a single input image is of great interest, as it enables 3D visualization of the 2D input scenery. they propose a novel network architecture to perform stereoscopic view synthesis at arbitrary camera positions along the X-axis, or “Deep 3D Pan”, with “t-shaped” adaptive kernels equipped with globally and locally adaptive dilations. their proposed network architecture, the monster-net, is devised with a novel t-shaped adaptive kernel with globally and locally adaptive dilation, which can efficiently incorporate global camera shift into and handle local 3D geometries of the target image’s pixels for the synthesis of naturally looking 3D panned views when a 2-D input image is given. Extensive experiments theyre performed on the KITTI, CityScapes, and their VICLAB_STEREO indoors dataset to prove the efficacy of their method. their monster-net significantly outperforms the state-of-the-art method (SOTA) by a large margin in all metrics of RMSE, PSNR, and SSIM. their proposed monster-net is capable of reconstructing more reliable image structures in synthesized images with coherent geometry. Moreover, the disparity information that can be extracted from the “t-shaped” kernel is much more reliable than that of the SOTA for the unsupervised monocular depth estimation task, confirming the effectiveness of their method.
Reject. rating score: 3. rating score: 3. rating score: 6. In the paper, the authors propose a general form that covers most stochastic gradient methods so far, e.g.stochastic gradient descent, Adam,  or adaptive probabilities methods. If we divide both sides by T, the right side of the inequality is still upper bounded by \sqrt{T}, it is not convergent. 3) The same question in Corollary 2.1 about the convergence result. e.g.,  [1][2][3] for adaptive methods, and [4] adaptive sampling methods.<BRK>The toy experiments in section 6.1 were illustrative but seem to only consider adaptive probabilities without the adaptive moments? I would recommend that the authors explore combinations of enabling balance correction in the baseline methods (possibly through reweighting the loss according to the balance statistics) or requiring DASGrad to compute the optimal probabilities through the adaptive moments as suggested throughout the rest of the paper. Additionally, could you provide a citation for the optimal adaptive probabilities and clearly state under which conditions these adaptive probabilities are optimal?<BRK>The convergence of the proposed method is analyzed in terms of regret bound and is compared to similar results for ADAM. al.” is this a reference?<BRK>Adaptive moment methods have been remarkably successful for optimization under the presence of high dimensional or sparse gradients, in parallel to this, adaptive sampling probabilities for SGD have allotheyd optimizers to improve convergence rates by prioritizing examples to learn efficiently. Numerous applications in the past have implicitly combined adaptive moment methods with adaptive probabilities yet the theoretical guarantees of such procedures have not been explored. they formalize double adaptive stochastic gradient methods DASGrad as an optimization technique and analyze its convergence improvements in a stochastic convex optimization setting, they provide empirical validation of their findings with convex and non convex objectives. they observe that the benefits of the method increase with the model complexity and variability of the gradients, and they explore the resulting utility in extensions to transfer learning. 
Reject. rating score: 3. rating score: 3. rating score: 6. Instead of learning asingle latent representation, the paper proposes learning charts, whichserve as local latent representations. Experiments demonstrate that thelocal representations perform favourably in terms of approximating  theunderlying manifold. I appreciate the useof concepts from differential topology in deep learning and agree withthe paper that such a perspective is required to increase ourunderstanding of complicated manifold data sets. 3.The experiments do not showcase the *conceptual* improvements of the   proposed technique. This makes some of the claims in the paper hard to assess. How do we know that we have a sufficient number of charts? Why is  the structure destroyed? I see that the reconstruction error for MNIST goes down but there arealso significantly more (!) The ideas of the sampling or interpolation experimentsgo in the right direction, but in their present version, they are notentirely convincing.<BRK>In general the paper is ok written and tries to present the idea and the theoretical background in a nice way. Moreover, there is some work where multiple generators are used in order to model the data [5]. Probably comparisons with other models that use multiple generators or even latent spaces that respect the topology of the data manifold could be included. I do not see how the proposed model can tackle this issue. This needs clarification. However, I have the feeling that the proposed approach is quite debateable.<BRK>Notes:    Goal is to learn autoencoders which can capture disconnected manifolds by employing multiple discrete charts. The circle example in the introduction is illuminating and I enjoyed it. It s somewhat subjective, but I feel that autoencoders are becoming less widely used, so the paper might have more impact if it had targeted models like ALI/BiGAN which do reconstruction but purely with adversarial objectives. For 5.2, I d prefer the use of a dataset other than MNIST, since we don t strictly know that the digits require different charts (for example I m pretty sure there s a smooth mapping between "1" and "7"). I slightly lean for acceptance but am very borderline, especially as the results on "real data" are very weak.<BRK> Auto-encoding and generative models have made tremendous successes in image and signal representation learning and generation. These models, hotheyver, generally employ the full Euclidean space or a bounded subset (such as $[0,1]^l$) as the latent space, whose trivial geometry is often too simplistic to meaningfully reflect the structure of the data. This paper aims at exploring a nontrivial geometric structure of the latent space for better data representation. Inspired by differential geometry, they propose \textbf{Chart Auto-Encoder (CAE)}, which captures the manifold structure of the data with multiple charts and transition functions among them. CAE translates the mathematical definition of manifold through parameterizing the entire data set as a collection of overlapping charts, creating local latent representations. These representations are an enhancement of the single-charted latent space commonly employed in auto-encoding models, as they reflect the intrinsic structure of the manifold.  Therefore, CAE achieves a more accurate approximation of data and generates realistic new ones. they conduct experiments with synthetic and real-life data to demonstrate the effectiveness of the proposed CAE. 
Reject. rating score: 6. rating score: 6. rating score: 8. This paper proposes a meta learning framework for learning adaptive kernels using a meta learner. Furthermore, to plug the kernel learning into the meta learning framework, they let the variational feature posterior to condition on the current support set for adapting and to use a modified LSTM network for accumulating information. They also illustrate that their adaptively learnt Fourier feature outperforms the standard variational Fourier features. Strengths, 1, The idea of learning kernels in meta learning is interesting. In particular, Figure 5 shows how the performance changes when the test shot and test way are varied. Weakness,1, The notations in the paper are not well presented. Because $w^{1:t 1}$ are random variables, they cannot be observed and cannot be conditioned on. 2, The paper doesn t introduce what is the likelihood $log p(y| x, S, w)$. It is unclear how the kernel regression is adopted in classification. 3, The meta prior $p(w| x, S)$ depends on the feature of the query point, which doesn t seem to be a common practice in variational inference. It would be beneficial if the authors could explain this and probably validate it empirically. 4, The motivations of the modified LSTM should be clarified more. However, the settings for the baselines should be better presented. For example, it is strange that the numbers of SNAIL are different with the numbers in their paper. And SNAIL on Omniglot is not reported.<BRK>This paper studies meta learning problem with few shot learning settings. The author proposes a learn each task predictive function via the form of random Fourier features, where the kernel is jointly learned from all tasks. The novel part is the parametrization of inference network using LSTM such that the random feature samples of t th task conditional depending on all previous task 1,...,t 1, which is an interesting way of modeling kernel spectral distribution. The experiment results show improvement of the proposed methods compared to SoTA meta learning algorithms. In general, the writing of the paper is clear, and the proposed method is interesting and novel. However, there are parts missing in the experiment setting. I would love to increase my score if the author could address the following questions/comments:(1) How do you choose the meta prior distribution? It should be a basic kernel family such as RBF Gaussian or mixture of RBF? (2) In Table 1 and Table 2, the benefit of using LSTM only gives very marginal improvement over w/o LSTM. Are the results statistically significant? (3) The experiment missed the simple kernel learning baseline, such as kernel alignment [1] and its variants [2]. If using these task independent way to do kernel learning, what’s their performance compared to you proposed method? (4) When learning the RFF spectral distribution using LSTM over a sequence of tasks, does the order of task matter? [1] Learning kernels with random features, NIPS 2016.<BRK>The paper focuses on the topic of meta learning for few shot learning and explores kernel approximation with random fourier features for this problem. The authors propose to learn adaptive kernels by meta variational random features, and evaluate their approach on different few shot learning tasks, comparing it against recent meta learning algorithms. The paper is well motivated and well written. On page 8, the authors mention related works that were not included for comparison because they rely on pre trained embeddings or large scale deep architectures. It would have been interesting to see the difference in performance.<BRK>Meta-learning for few-shot learning involves a meta-learner that acquires shared knowledge from a set of prior tasks to improve the performance of a base-learner on new tasks with a small amount of data. Kernels are commonly used in machine learning due to their strong nonlinear learning capacity, which have not yet been fully investigated in the meta-learning scenario for few-shot learning. In this work, they explore kernel approximation with random Ftheirier features in the meta-learning framework for few-shot learning. they propose learning adaptive kernels by meta variational random features (MetaVRF), which is formulated as a variational inference problem. To explore shared knowledge across diverse tasks, their MetaVRF deploys an LSTM inference network to generate informative features, which can establish kernels of highly representational potheyr with low spectral sampling rates, while also being able to quickly adapt to specific tasks for improved performance. they evaluate MetaVRF on a variety of few-shot learning tasks for both regression and classification. Experimental results demonstrate that their MetaVRF can deliver much better or competitive performance than recent meta-learning algorithms.
Reject. rating score: 3. rating score: 3. rating score: 6. Overview:The paper tackles the representation learning problem where the aim is to learn a generic representation that is useful for a variety of downstream tasks. I consider the theoretical guarantees associated with the proposed approach a welcome and valuable contribution to this field that has recently been relying primarily on limited empirical work to assess any method. The empirical results presented in the paper do not sufficiently support the claims of sample efficiency. This does not help make conclusions about the sample efficiency of the proposed method on new tasks. The paper notes that these are some preliminary experiments.<BRK>(I bid on the paper thinking that bi level optimisation would play a major role in the paper.Unfortunately, it does not, so my expertise in bi level optimisation is not much use, I am afraid.) They call this the "representatition learning for imitation learning". The results are plausible, given the assumptions. The empirical results involve only benchmarks of the authors own coinage, and hence are hard to evaluate. It seems plausible, again, however, that the approach may work in some cases.<BRK>This paper theoretically explores reinforcement/imitation learning via representation learning. f. Theorem 4.1, perhaps use some other notation for c, which is defined as the cost/reward in the RL setting. Also, note that there is a bearing of the bound on the trajectory length H (Theorem 4.1, and 5.1). While, the current paper uses bilevel optimization setting in an RL context, it is not clear to me if this (bilevel + RL) setting has any significant bearing against the theoretical results furnished by Maurer 2016.<BRK>A common strategy in modern learning systems is to learn a representation which is useful for many tasks, a.k.a, representation learning. they study this strategy in the imitation learning setting where multiple experts trajectories are available. they formulate representation learning as a bi-level optimization problem where the "outer" optimization tries to learn the joint representation and the "inner" optimization encodes the imitation learning setup and tries to learn task-specific parameters. they instantiate this framework for the cases where the imitation setting being behavior cloning and observation alone. Theoretically, they provably show using their framework that representation learning can reduce the sample complexity of imitation learning in both settings. they also provide proof-of-concept experiments to verify their theoretical findings.
Reject. rating score: 3. rating score: 6. rating score: 6. It would be great to see at least one or two other areas where this could be applied to, since I doubt the general ICLR audience is well versed in nano porous templates. Unfortunately, I cannot comment on the overall scientific contribution of the paper, as I do not possess the expertise to judge it accurately. My expertise is so outside of this field that I will rely on the judgement of the other reviewers, whom I hope will have more experience and will better know the literature. In practice, would one want to wait longer to get a better quality result, or are the numbers obtained with the proposed approach usable?<BRK>Nevertheless the paper is a little lacking of novelty in the sense that it brings together many existing ideas and provides an analysis of the effect of bringing them together but none of the theory significantly improves over the existing theory. Hermite learning which directly approximates the error on the gradient as well as the function evaluation. Nevertheless the bounds achieved do not look much worse than the non error counterparts and are easy to implement. The Langevin process requires a derivative of the gradient.<BRK>As a non domain expert I cannot evaluate the importance of the contributions or the complexity of the tasks. However, given the well written and clearly structured presentation, the theoretical proofs and the generality of the constraint solver (with learned approximated gradients), I would vote for accepting the paper. Open questions:  How does the approach relate to "multi objective Bayesian optimization". The authors note that their approach is closely related to Shen et al.(2019).However, there is no algorithmic comparison in the experiments. What were the input and output dimensions.<BRK>they consider the problem of generating configurations that satisfy physical constraints for optimal material nano-pattern design, where multiple (and often conflicting) properties need to be simultaneously satisfied.  Consider, for example, the trade-off bettheyen thermal resistance, electrical conductivity, and mechanical stability needed to design a nano-porous template with optimal thermoelectric efficiency.  To that end, they leverage the posterior regularization framework andshow that this constraint satisfaction problem can be formulated as sampling froma Gibbs distribution.  The main challenges come from the black-box nature ofthose physical constraints, since they are obtained via solving highly non-linearPDEs. To overcome those difficulties, they introduce Surrogate-based Constrained Langevin dynamics for black-box sampling. they explore two surrogate approaches. The first approach exploits zero-order approximation of gradients in the Langevin Sampling and they refer to it as Zero-Order Langevin. In practice, this approach can be prohibitive since they still need to often query the expensive PDE solvers. The second approach approximates the gradients in the Langevin dynamics with deep neural networks, allowing us an efficient sampling strategy using the surrogate model. they prove the convergence of those two approaches when the target distribution is log-concave and smooth. they show the effectiveness of both approaches in designing optimal nano-porous material configurations, where the goal is to produce nano-pattern templates with low thermal conductivity and reasonable mechanical stability.
Reject. rating score: 3. rating score: 3. rating score: 6. rating score: 8. This paper introduces a mutual information term into the training objective of message passing graph neural networks. A variational lower bound on the mutual information is used in training. Postscript:  I have been swayed by the complaints of reviewer 1 and reduced my score to weak reject.<BRK>This paper proposed an auxiliary loss based on mutual information for graph neural network. Such loss is to maximize the mutual information between edge representation and corresponding edge feature in GNN ‘message passing’ function.<BRK>This paper proposed a new graph neural network to utilize the edge features. In particular, it proposes the Edge Information maximized Graph Neural Network (EIGNN) that maximizes the Mutual Information (MI) between edge features and message passing channels. 2.The writing is clear. 3.Extensive experiments are done to verify the performance of the proposed method.<BRK>In this paper, the authors proposed a new kind of graph neural network that can use continuous edge features. Specifically, a variational lower bound is proposed for mutual information and integrated into the GNN model so that MI between edge features and the message passing channel is maximized. Utilizing continuous edge features in GNN is an important and difficult task. To conclude, I think the paper will be a good addition to the conference.<BRK>Graph Neural Networks (GNNs) broadly follow the scheme that the representation vector of each node is updated recursively using the message from neighbor nodes, where the message of a neighbor is usually pre-processed with a parameterized transform matrix. To make better use of edge features, they propose the Edge Information maximized Graph Neural Network (EIGNN) that maximizes the Mutual Information (MI) bettheyen edge features and message passing channels. The MI is reformulated as a differentiable objective via a variational approach. they theoretically show that the newly introduced objective enables the model to preserve edge information, and empirically corroborate the enhanced performance of MI-maximized models across a broad range of learning tasks including regression on molecular graphs and relation prediction in knowledge graphs.
Reject. rating score: 1. rating score: 6. rating score: 6. rating score: 8. https://openreview.net/pdf?id SkxaueHFPBThe paper has some interesting ideas but I don’t think any of them are fully fleshed out. I am not making the argument that the authors ought to demonstrate SOTA results, however they should at least present results which are consistent with the published results of their chosen baseline. The authors then make this statement:“Thus, its superior performance supports the claim that ICR is the appropriate form of regularization for GANs. We emphasize that in our experiments we did not perform any architecture or hyperparameter tuning, and instead use a model intended to be used with WGAN gradient penalty”This does not hold, since the numbers reported are far below the actual baseline. Besides this major point, I am unconvinced by some of the mathematical statements in the paper. Much of the mathematical details are deferred to the original CGD paper. Concretely my concerns refer to the main discussion of the effect of the CGD as a regularizer:The authors state:“If some of the singular values of Dxy are very large, this amounts to approximately restricting the update to the orthogonal complement of the corresponding singular vectors” I don’t see how this is the case. The terms Dxy/Dyx aren’t really introduced or defined anywhere in this work. Assuming is the transpose of the other (?) So we have a term which is being affected by the smallest singular values of S and a term which is the orthogonal projection of grad_y onto Dxy, alternatively the ridge regression fit of grad_y on Dxy which would attenuate directions corresponding to the *small* singular values (as is well known from the theory of ridge regularizers). I feel like there is much more to say here than what is discussed in the paper in very vague terms. In practise I would assume the smaller SVs of Dxy to be difficult to estimate or the matrix to be rank deficient in which case they would simply be unity in the inverse whereas the directions corresponding to large singular values would be attenuated. First it is not at all clear to me what “smoothly varying singular vectors” are. Finally, figure 3 is quite bizarre to me. None of the quantities have been rigorously defined and so it seems like the relative effect of each of the arrows and the manifold have been drawn arbitrarily in order to fit the story, rather than to actually illuminate the true behaviour in an intuitive manner.<BRK>The paper presents a new way of regularization in Generative Adversarial Network (GAN). It is well known that a naive training of GAN can fail to converge. Although GAN is relatively a new concept, many papers tried to introduce a good way of stabilizing GAN training. I believe that this paper is addressing the stability issues in the most fundamental and effective ways. The intuition is that both players should predict what their opponent is going to do. The performance of the new method was demonstrated on CIFAR10. If this method works as well as the authors claim, it can significantly improve the practicality of GAN. The paper is very readable and understandable but many small typos and grammar errors can be found in the text. This can be easily corrected by the authors. However, the contribution of this paper is questionable. The original CGD paper already applies it to train a GAN. I would also appreciate if the method is tested on multiple other data sets. Overall, the paper is well written, technically correct and interesting enough for the venue. However, as I pointed out above, the contribution should be more clearly stated.<BRK>The paper presents a novel training methodology for GANs to improve stability. The resulting regularization, termed implicit competitive regularization, updates the parameters of both the generator and the discriminator to be robust to one another. A framework for practical application of this approach is described   this is done by a local Taylor approximation of the loss and updating each model’s parameters to this approximate model’s nash equilibrium. The method is shown to prevent overfitting and produce high performing models with consistent training. The results are interesting in terms of describing the ICR property and demonstrating its performance. The paper gives background and intuition to solidifying the CGD update. This I believe would help solidify the paper and build beyond CGD. Some additional results that could clarify the benefits:  A primary contribution of the training approach is training consistency. Clearly due to the additional gradient calls the approach is computationally slower, as shown in Figure 4. One may expect that the update may result in more conservative updates and thus potentially lower performing policies in the limit. If there iterations were instead log scale to show performance in high training iterations, is there any loss of performance in top performing runs? This may also be a stable training procedure as well with less conservatism. The paper should be proofread, there are several minor typos throughout, e.g.:  “generators producing that produce good”  > generators that produce good  “This game is very similar similar”  > repeated word  “GAN trainin.”<BRK>The paper analyzes instability in training GANs, relates it to Nash equilibria, and proposes a novel training set up based on competitive nashless games. The solution is related to other recently proposed work, but the paper brings additional insights into understanding it. The analysis on conditions that lead to divergence or convergence, and of the proposed solution, are interesting. I recommend accepting. I have some basic knowledge of GANs but am not deeply familiar with the field. The paper was accessible to me on a high level. Especially compelling to me were sections 2 and 3. The empirical study also seemed to yield positive results. Suggestions for improvement:  Additional proofreading would be beneficial. The scale of the axes in figure 1 is not clear, making it a little less compelling. Inception score is used as the only evaluation metric for the generators. Perhaps this is standard in the field, although human ratings would seem more reliable to me.<BRK>Generative adversarial networks (GANs) are capable of producing high quality samples, but they suffer from numerous issues such as instability and mode collapse during training. To combat this, they propose to model the generator and discriminator as agents acting under local information, uncertainty, and awareness of their opponent. By doing so they achieve stable convergence, even when the underlying game has no  Nash equilibria. they call this mechanism \emph{implicit competitive regularization} (ICR) and show that it is present in the recently proposed \emph{competitive gradient descent} (CGD).
When comparing CGD to Adam using a variety of loss functions and regularizers on CIFAR10, CGD shows a much more consistent performance, which they attribute to ICR.
In their experiments, they achieve the highest inception score when using the WGAN loss (without gradient penalty or theyight clipping) together with CGD. This can be interpreted as minimizing a form of integral probability metric based on ICR.
Reject. rating score: 1. rating score: 3. rating score: 3. In this paper, the authors study Neural Architecture Search, which aims to automate design of neural network models. Major concern:1/ I did not find the proof of Theorem 3.2 in the main paper and in the appendix, so I do not buy it. 3/ The key idea of the algorithm is not well explained.<BRK>4   Does m 1000 in your experiments satisfies theorem 3.2? As such, this work is an application of recent progress in the field of compressive sensing to One shot neural architecture search. To this regard, the only novelty that was the framing of One shot NAS as a recovery of boolean functions from their sparse Fourier expansions is not new either. ClarityOverall, the paper is well motivated and the technical content is good.<BRK>This paper proposes a new algorithm for one shot neural architecture search (NAS) via compressive sensing. How did you tune the p in the Bernoulli distribution during the one shot weight updates. In ICLR, 2019. Overall, I think the proposed algorithm is interesting and of practical usefulness.<BRK>Neural architecture search (NAS), or automated design of neural network models, remains a very challenging meta-learning problem. Several recent works (called "one-shot" approaches) have focused on dramatically reducing NAS running time by leveraging proxy models that still provide architectures with competitive performance. In their work, they propose a new meta-learning algorithm that they call CoNAS, or Compressive sensing-based Neural Architecture Search. their approach merges ideas from one-shot NAS approaches with iterative techniques for learning low-degree sparse Boolean polynomial functions. they validate their approach on several standard test datasets, discover novel architectures hitherto unreported, and achieve competitive (or better) results in both performance and search time compared to existing NAS approaches. Further, they provide theoretical analysis via upper bounds on the number of validation error measurements needed to perform reliable meta-learning; to their knowledge, these analysis tools are novel to the NAS literature and may be of independent interest.
Accept (Talk). rating score: 8. rating score: 8. rating score: 8. rating score: 6. This paper introduces an end to end method to predict the secondary structure of RNA, by mapping the nucleotide sequence to a binary affinity matrix. An innovation is to express this post processing as an unrolled sequence of proximal gradient descent steps, which are fully differentiable, and allow the full combination of (i)+(ii) to be trained end to end. The proposed approach appears to provide a novel, convincing and non obvious solution to RNA secondary structure prediction, and subject to suggestions below, would represent a valuable contribution to ICLR. The principal area for improvement would be to include additional detail (perhaps in appendix) on the model hyperparameter configurations that were used in the experiments.<BRK>The authors proposed an end to end method (E2Efold) to predict RNA secondary structure. The score network is a deep learning model with transformer and convolution layers, and the post process network is solving a constrained optimization problem with an T step unrolled algorithm. Experimental results demonstrate that the proposed approach outperforms other RNA secondary structure estimation approaches. Overall I found the paper interesting. Training details and implementation details are missing; these hinder the reproducibility of the proposed approach. This performance was computed on the dataset (RNAStralign) on which E2Efold was trained. On this data, E2Efold has F1 score 0.686 versus 0.638 for CONTRAfold.<BRK>RNA Secondary Structure Prediction by Learning Unrolled AlgorithmsThis paper proposes E2Efold, which is an RNA secondary structure prediction algorithm based on an unrolled algorithm. Previous methods rely on dynamic programming (which does not work for molecular configurations that do not factorize) or rely on energy based models (which require a minimization step, e.g.by using MCMC to traverse the energy landscape and find minima). The method presented here is novel, shows strong SOTA performance, and would be of interest to the wider deep learning community.<BRK>*Summary*The authors perform RNA secondary prediction using deep learning. The paper also considers an application domain that will be unfamiliar to many ICLR readers interested in deep structured prediction, and may serve as a call to arms for the community engaging with additional problems in this field. There are a number of technical details, such as the loss function in (8) that will be of general interest. I advocate for acceptance. How sensitive is performance to the number of optimizer iterations?<BRK>In this paper, they propose an end-to-end deep learning model, called E2Efold, for RNA secondary structure prediction which can effectively take into account the inherent constraints in the problem. The key idea of E2Efold is to directly predict the RNA base-pairing matrix, and use an unrolled algorithm for constrained programming as the template for deep architectures to enforce constraints. With comprehensive experiments on benchmark datasets, they demonstrate the superior performance of E2Efold: it predicts significantly better structures compared to previous SOTA (especially for pseudoknotted structures), while being as efficient as the fastest algorithms in terms of inference time.
Accept (Poster). rating score: 8. rating score: 3. rating score: 3. However, such maps are in general discontinuous. Since deep neural networks can only represent continuous maps, this brings two problems: mode collapse and mode mixture. Their method basically avoids representing discontinuous maps by the generator. I think the idea of the paper is nice, and an interesting perspective  on GANs is presented. Nevertheless, I have some comments below. 1) Although this paper brings a new perspective, based on optimal transport theory, as far as I can understand this paper does not establish formal new results. Thus I think some strong claims about providing deep theoretical explanation should be more moderate. I think in some parts a lighter notation and a more intuitive explanation could help. 3) After Eq.(5) in the Appendix the authors mention Newton s method, and Thm 3 is also specific to Newton s method. This is confusing. In the negative case, why not?<BRK>This paper proposes a new problem in GAN distribution mapping: the concavity of support problem. 3.Empirical results show the effectiveness of the proposed method. Since OT will be aware of all modes in \nu, singular points can be detected by checking the angle between "shards" and those points that are around the "ridge" should be rejected. Thus, the proposed method could handle both the concave support problem and the mode collapse problem. Fit an auto encoder just as you did in the paper and get an empirical distribution \nu. Step 2.Fit a Gaussian mixture model on \nu and do model selection over # clusters. Since this method relies on a high quality auto encoder model, it is hard to say this paper makes progress in fixing the GAN s mode collapsed problem. Besides, the paper does not involve an adversarial training module.<BRK>This paper deals with an important problem of mode collapse and mode mixture. The illustration in Fig.3, and implicit in all the explanation charts is the fact that discontinuity can be found by a linear separation. Singular set detection seems to be the most tricky part in this paper, which should have been explained further. The Simplex projection assumption, renders this part not that tricky, but that is where I feel the biggest doubt about this paper lies. Also, the method does not have any adversarial training and hence, it studies the GAN idea from only fixing the generator point of view.<BRK>Generative adversarial networks (GANs) have attracted huge attention due to
its capability to generate visual realistic images. Hotheyver, most of the existing
models suffer from the mode collapse or mode mixture problems. In this work, they
give a theoretic explanation of the both problems by Figalli’s regularity theory of
optimal transportation maps. Basically, the generator compute the transportation
maps bettheyen the white noise distributions and the data distributions, which are
in general discontinuous. Hotheyver, DNNs can only represent continuous maps.
This intrinsic conflict induces mode collapse and mode mixture. In order to
tackle the both problems, they explicitly separate the manifold embedding and the
optimal transportation; the first part is carried out using an autoencoder to map the
images onto the latent space; the second part is accomplished using a GPU-based
convex optimization to find the discontinuous transportation maps. Composing the
extended OT map and the decoder, they can finally generate new images from the
white noise. This AE-OT model avoids representing discontinuous maps by DNNs,
therefore effectively prevents mode collapse and mode mixture.
Reject. rating score: 3. rating score: 6. rating score: 6. Generally speaking, I agree that it is always good to be more efficient. The paper presents a new approach, SMOE scale, to extract saliency maps from a neural network. Overall, in my opinion, being efficient at generating saliency maps is a nice to have, but not much more. Being efficient is also a nice to have. However, the comparison to other approaches does not show a clear advantage, and the related work (and experiments) lacks recent techniques. For the first element, we have to consider the paper as the combination of two things. For instance, Fig.4 caption tells that "SMOE Scale differentiates itself the most early on in the network", but it is actually only for the very first scale layer. Overall, while SMOE is indeed novel, it is not highly convincing. It does seem to have an edge on KAR score, but not by a huge amount. Finally, a note about efficiency.<BRK>I SummaryThe authors present a method that computes a saliency map after each scale block of a CNN and combines them according to the weights of the prior layers in a final saliency map. The paper gives two main contributions: SMOE, which captures the informativeness of the corresponding layers of the scale block and LOVI, a heatmap based on HSV, giving information of the region of interest according to the corresponding scale blocks. The method is interesting as it does not require multiple backward passes such as other gradient based method and thus prove to be more time efficient. Figure 5: "Higher accuracy values are better results for it"  > yield better resultsThe phrasing with"one" as in "if one would like to etc" is used a lot through the paper, it can be a little redundant at times. Overall the results of the obtained maps are not very convincing compared to existing methods. The conclusion and discussion are short and could be filled a little more (some captions could be shortened in order to give more space). Edit: The authors have answered most of my concerns and I am happy to re evaluate my score to weak accept.<BRK>This paper presents a method for creating saliency maps reflecting what a network deems important while also proposing an interesting method for visualizing this. The central premise for the method of characterizing relative importance of information represented by the network is based on an information theoretic measure. I find that the proposed method appears to be theoretically sound and is interesting in revealing differences to other information theoretic methods especially in the early layers. Moreover, the LOVI scheme for visualization is interesting in itself and I found this aspect of the work to be thought provoking with respect to how such methods are examined from a qualitative perspective.<BRK>they describe an explainable AI saliency map method for use with deep convolutional neural networks (CNN) that is much more efficient than popular gradient methods. It is also quantitatively similar or better in accuracy. their technique works by measuring information at the end of each network scale. This is then combined into a single saliency map. they describe how saliency measures can be made more efficient by exploiting Saliency Map Order Equivalence.  Finally, they visualize individual scale/layer contributions by using a Layer Ordered Visualization of Information. This provides an interesting comparison of scale information contributions within the network not provided by other saliency map methods.  their method is generally straight forward and should be applicable to the most commonly used CNNs. (Full stheirce code is available at http://www.anonymous.submission.com).
Reject. rating score: 1. rating score: 3. rating score: 3. The goal of the paper is clear. But the authors failed to propose a novel method and the results are not convincing. Hence, I recommend rejection.<BRK>  Post Rebuttal  No rebuttal was provided and all reviewers have raised issues. The issue is that standard decision trees, by independently picking the split feature(s), virtually discard the structure of the input features (if any). This patch will respect locality in a similar way to the proposed MORF’s bounding boxes. more importantly, it is not mentioned how these hp are optimized for the different baselines as well as the proposed method. the bounding box sampling seems biased as presented.<BRK>This paper proposed a new method called manifold forest to improve decision forest (DF) classification results. It is motivated by that, natural data is often in some manifold but not randomly distributed. It showed how to use the 2D spatial structures of natural images by constructing structured atoms. 1.In image classification, we definitely need 2D structure information. It is rare to use the pixel values as the features directly for classification. In this case, the benefit of the proposed method is very weak.<BRK>Decision forests (DF), in particular random forests and gradient boosting trees, have  demonstrated state-of-the-art accuracy compared to other methods in many supervised learning scenarios. In particular, DFs dominate other methods in tabular data, that is, when the feature space is unstructured, so that the signal is invariant to permuting feature indices.  Hotheyver, in structured data lying on a manifold---such as images, text, and speech---neural nets (NN) tend to outperform DFs. they conjecture that at least part of the reason for this is that the input to NN is not simply the feature magnitudes, but also their indices (for example, the convolution operation uses ``feature locality). In contrast, naive DF implementations fail to explicitly consider feature indices. A recently proposed DF approach demonstrates that DFs, for each node, implicitly sample a random matrix from some specific distribution.  Here, they build on that to show that one can choose distributions in a manifold aware fashion. For example, for image classification, rather than randomly selecting pixels, one can randomly select contiguous patches. they demonstrate the empirical performance of  data living on three different manifolds: images, time-series, and a torus. In all three cases, their Manifold Forest (Mf) algorithm empirically dominates other state-of-the-art approaches that ignore feature space structure, achieving a lotheyr classification error on all sample sizes. This dominance extends to the MNIST data set as theyll. Moreover, both training and test time is significantly faster for manifold forests as compared to deep nets. This approach, therefore, has promise to enable DFs and other machine learning methods to close the gap with deep nets on manifold-valued data. 
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. The paper presents a compression algorithm for neural networks. The method is simple yet it shows strong performance of a variety of compression benchmarks. I appreciate that the authors fit the paper into the recommended 8 pages. Section 2.1, How are the weights grouped for coding? Assessment:While the idea is not groundbreaking, it is very well presented and evaluated and shows strong performance.<BRK>The paper introduces an end to end method for the neural network compression, which is based on compressing reparametrized forms of model parameters (weights and biases). During optimization, several tricks are applied. It provides good compression while retaining a significant portion of accuracy. Also, I wonder whether under the same compression rate the proposed method outperforms DeepCompression (Han et al., 2015) in terms of accuracy?<BRK>When trained on natural images, CNN filters are typically "smooth" (see any visualization thereof) and this smoothness translates as a prior of sorts on $\Phi_{w}$. This insight was previously explored in [1, 2] for purposes of compressing CNNs. Can assignments be reliably made by simply looking at the architecture? Minor Comments:    The general notation of the paper is cumbersome at times, can $\phi$ to be changed to, e.g., $\tilde{\theta}$? Consider trying to order things "chronologically" such that training comes before compression? [1] "Compressing convolutional neural networks in the frequency domain", Chen et al.2016[2] "CNNpack: Packing convolutional neural networks in the frequency domain", Wang et al.2016<BRK>they describe a simple and general neural network theyight compression approach, in which the network parameters (theyights and biases) are represented in a “latent” space, amounting to a reparameterization. This space is equipped with a learned probability model, which is used to impose an entropy penalty on the parameter representation during training, and to compress the representation using a simple arithmetic coder after training. Classification accuracy and model compressibility is maximized jointly, with the bitrate–accuracy trade-off specified by a hyperparameter. they evaluate the method on the MNIST, CIFAR-10 and ImageNet classification benchmarks using six distinct model architectures. their results show that state-of-the-art model compression can be achieved in a scalable and general way without requiring complex procedures such as multi-stage training.
Reject. rating score: 3. rating score: 3. rating score: 6. The study direction itself is interesting and very useful for the interpretation and adversarial attack community. However, this paper can be improved a lot as follows. It is not clear on the definition of "hidden" in terms of  "interpretability".<BRK>But a better summary of this observation is that "robustness of interpretability implies robustness of classification", which is not surprising, and is in fact a trivial corollary of the fact that the metric on interpretations dominates the classification error metric (an observation which is made in the paper).<BRK>#StrengthThe paper is well written and structured, with a clear demonstration of technical details. #PresentationGood coverage of the literature in both adversarial robustness and model interpretation.<BRK>Recent works have empirically shown that there exist   adversarial examples that can be hidden from neural network interpretability, and  interpretability is itself susceptible to adversarial attacks. In this paper, they theoretically show  that with the correct measurement of interpretation, it is actually difficult to hide adversarial examples, as confirmed by experiments on MNIST, CIFAR-10 and Restricted ImageNet. Spurred by that, they develop a novel defensive scheme built only on robust interpretation (without resorting to adversarial loss minimization). they show that their defense achieves similar classification robustness to state-of-the-art robust training methods while attaining higher interpretation robustness under various settings of adversarial attacks.
Reject. rating score: 1. rating score: 1. rating score: 3. Theorem 2 and its proof are plagiarised: they are rephrased and reorganized formulation and proof of Theorem 1 of [1], while being presented as authors  own work. Demystifying MMD GANs.<BRK>The writing of the paper is poor. Moreover, as mentioned by reviewer #1,  theorem 2 and its proof are plagiarised. Overall, I think the paper is a clear reject.<BRK>Does the proposed Random Forest MMD GAN also has that property? Instead of using Gaussian kernel on the top of the learned embeddings from the discriminator, it combines existing deep forests kernels. It would be interesting to see which parameterization is better in this space. Could you discuss it with Theorem 2?<BRK>In this paper, they propose a novel kind of kernel, random forest kernel, to enhance the empirical performance of MMD GAN. Different from common forests with deterministic routings, a probabilistic routing variant is used in their innovated random-forest kernel, which is possible to merge with the CNN frameworks. their proposed random-forest kernel has the following advantages: From the perspective of random forest, the output of GAN discriminator can be vietheyd as feature inputs to the forest, where each tree gets access to merely a fraction of the features, and thus the entire forest benefits from ensemble learning. In the aspect of kernel method, random-forest kernel is proved to be characteristic, and therefore suitable for the MMD structure. Besides, being an asymmetric kernel, their random-forest kernel is much more flexible, in terms of capturing the differences bettheyen distributions. Sharing the advantages of CNN, kernel method, and ensemble learning, their random-forest kernel based MMD GAN obtains desirable empirical performances on CIFAR-10, CelebA and LSUN bedroom data sets. Furthermore, for the sake of completeness, they also put forward comprehensive theoretical analysis to support their experimental results.
Reject. rating score: 3. rating score: 3. rating score: 3. This paper addresses the problem of training small models to mimic large models but, in constrast to knowledge distillation, minimize the reverse KL between the teacher and the model instead of the forward KL. However, the authors argue that minimizing the reverse KL makes more sense: to only include tokens in the students that are present in the teacher.<BRK>The experiments in this paper may be thought insufficient. However, I have a little concern that the contribution is relatively limited since it is known that KL divergence is not symmetric and the proposed KA (KL divergence in the reverse order) may be thought a little straightforward. Language models are unsupervised multitask learners.<BRK>This is also a bit counter intuitive. 3.The authors performed the experiments on the translation tasks showing that the proposed KA strategy outperforms both KL and JS strategy in terms of the generation performance. When only the top few tokens are used to transfer the knowledge from teacher model to student model, KA focus on the precision of a small subspace, which tends to have few modes.<BRK>Sequence-to-sequence models such as transformers, which are now being used in a wide variety of NLP tasks, typically need to have very high capacity in order to perform theyll. Unfortunately, in production, memory size and inference speed are all strictly constrained.  To address this problem, Knowledge Distillation (KD), a technique to train small models to mimic larger pre-trained models, has drawn lots of attention.  The KD approach basically attempts to maximize recall, i.e., ranking Top-k”tokens in teacher models as higher as possible, hotheyver, whereas precision is more important for sequence generation  because of exposure bias. Motivated by this, they develop Knowledge Acquisition (KA) where student models receive log q(y_t|y_{<t},x) as rewards when producing the next token y_t given previous tokens y_{<t} and the stheirce sentence x.   they demonstrate the effectiveness of their approach on WMT’17 De-En and IWSLT’15 Th-En translation tasks, with experimental results showing that their approach gains +0.7-1.1 BLEU score compared to token-level knowledge distillation.
Reject. rating score: 1. rating score: 3. rating score: 3. I also find the experiment lacking as comparison with fast approximation method such as [*] that incorporate local information is not included. This is further coupled with armortized inference for better scalability. Post Rebuttal Update  Thank you for the rebuttal & I have read it in detail. Please note that I am not disputing the potential use of this VI form here, which could have been the only way to encode a different (directional) type of information. For encoding KNN information, however, it only seems to create more troubles than it solves. [C] Complexity: The complexity analysis is too informal and lacking fine grained information.<BRK>In this paper, the authors propose a family of variational distributions in which the variational covariance matrix is parameterized as RR , with R_ij being nonzero only when j is a neighbor of i as defined by prior covariance. This results in a sparse factor of the covariance matrix which can be used for efficient computation. However, I have a few concerns about the execution. 3 7 are not sufficiently motivated, and are essential to the method as they allow for stochastic optimization.<BRK>1) SummaryThe manuscript proposes a k nearest neighbor (KNN) Gaussian process (GP) approximate inference scheme to render computations more scalable. There are some typos and some glitches in the notation. There are some open issues regarding the KNN computations. The modification is rather marginal and the empirical evaluation makes it hard to judge the relative merit of the proposal. However, there is no code for the experiments, which makes the results slightly tricky to exactly reproduce. 7) EvaluationThe evaluation does not consider simple baselines like dense GPs or sparse approximations such as FITC and VFE.<BRK>The inference of Gaussian Processes concerns the distribution of the underlying function given observed data points. GP inference based on local ranges of data points is able to capture fine-scale correlations and allow fine-grained decomposition of the computation. Following this direction, they propose a new inference model that considers the correlations and observations of the K nearest neighbors for the inference at a data point. Compared with previous works, they also eliminate the data ordering prerequisite to simplify the inference process. Additionally, the inference task is decomposed to small subtasks with several technique innovations, making their model theyll suits the stochastic optimization. Since the decomposed small subtasks have the same structure, they further speed up the inference procedure with amortized inference. their model runs efficiently and achieves good performances on several benchmark tasks.
Reject. rating score: 3. rating score: 3. rating score: 6. The paper is concerned with embedding a supervised feature selection within a classification setting. Suggestion, you might compare with the L_0 inspired regularization setting used for unsupervised feature selection in Agnostic Feature Selection, Doquet et al, 2019. I am mildly convinced by the paper:* Out of the four contributions listed p. 2, STG is the most convincing one; still, the description thereof is not cristal clear: the reparametrization trick is not due to the authors.<BRK>The authors propose a feature selection method for high dimensional datasets that attempts to fit a model while selecting relevant features. They relax the discrete variables using a continuous relaxation and provide a simple unbiased estimator for the gradient of the relaxation. The current methods do which suggests that the continuous relaxation is useful. For e.g.the median rank is better shown with box plots (as in the Chen et al 2018 paper cited by the authors).<BRK>***The paper considers the problem of embedded feature selection for supervised learning with nonlinear functions. Sparsity in the feature selection is enforced via a relaxation of l0 regularization. A variety of experiments in several supervised learning tasks demonstrates that the proposed method has superior performance to other embedded and wrapper methods.<BRK>Feature selection problems have been extensively studied in the setting of
linear estimation, for instance LASSO, but less emphasis has been placed on
feature selection for non-linear functions. In this study, they propose a method
for feature selection in high-dimensional non-linear function estimation
problems. The new procedure is based on directly penalizing the $\ell_0$ norm of
features, or the count of the number of selected features. their $\ell_0$ based regularization relies on a continuous relaxation of the Bernoulli distribution, which
allows their model to learn the parameters of the approximate Bernoulli
distributions via gradient descent. The proposed framework simultaneously learns
a non-linear regression or classification function while selecting a small
subset of features. they provide an information-theoretic justification for
incorporating Bernoulli distribution into their approach. Furthermore, they evaluate
their method using synthetic and real-life data and demonstrate that their approach
outperforms other embedded methods in terms of predictive performance and feature selection.
Reject. rating score: 3. rating score: 3. rating score: 3. The paper proposes to use low rank matrix decomposition for embedding compression, with relu in the reconstruction layer to gain non linearity. Detailed comments:1)	The technical contribution seems to be a bit limited. Also, not much insight is provided on why such approach works better than other baselines. b.How does the time complexity and running time of the proposed method compared to the baselines? The experiments would be more convincing if evaluated on more tasks as well.<BRK>This paper proposes a method for compressing embedding matrices of both encoder/decoder embeddings. We need to note that the memory requirement of the proposed method during training will increase. As pointed out by the authors, this seems to be the first success of reducing the embedding matrix with a tied embedding setting. However, according to Tables 1, 2, and 3, it seems that the performance gain is marginal compared with similar methods. Besides, the authors should perform a statistically significant test if they say “our method outperforms existing state of the art methods.”  2I am a bit confused about the following inconsistency;The authors say that “Compressing embedding matrices without sacrificing model performance is essential for successful commercial edge deployment” in the abstract. By this fact, compressing embedding matrices seems not essential for successful commercial edge deployment.<BRK>The authors propose to use a variant of SVD (which can be viewed as 2 linear transformation, with a middle dimension that represents an embedding), where the first transformation is linear with a ReLu, and the second is linear. It is not clear to me why using a surrogate L2 loss within the model should give better predictive performance than a fully end to end trained neural network. Without this comparison, I do not think the proposed experiments are conclusive enough.<BRK>Word-embeddings are a vital component of Natural Language Processing (NLP) systems and have been extensively researched. Better representations of words have come at the cost of huge memory footprints, which has made deploying NLP models on edge-devices challenging due to memory limitations. Compressing embedding matrices without sacrificing model performance is essential for successful commercial edge deployment. In this paper, they propose Distilled Embedding, an (input/output) embedding compression method based on low-rank matrix decomposition with an added non-linearity. First, they initialize the theyights of their decomposition by learning to reconstruct the full word-embedding and then fine-tune on the downstream task employing knowledge distillation on the factorized embedding. they conduct extensive experimentation with various compression rates on machine translation, using different data-sets with a shared word-embedding matrix for both embedding and vocabulary projection matrices. they show that the proposed technique outperforms conventional low-rank matrix factorization, and other recently proposed word-embedding matrix compression methods. 

Reject. rating score: 1. rating score: 1. rating score: 3. It includes the construction of a data set and some experiments with regression models. This paper aims to predict typical “common sense” values of quantities using word embeddings. These models are standard existing techniques, and given the relatively low performance I would have liked more development of the models and more analysis of the performance.<BRK>My second issue with the paper is that it is not possible to conclude if the experimental results support or refute the hypothesis (ignoring the issue with the dataset):1. To verify this hypotheses, the authors have created a dataset through a crowd sourcing service which represents "numerical common sense". Using this dataset, the authors examine the predict abilities of regressors trained on learned word embeddings and the aforementioned crowd sourced dataset.<BRK>This paper describes the collection and analysis of a numerical common sense dataset.<BRK>Numerical common sense (e.g., ``a person with a height of 2m is very tall'') is essential when deploying artificial intelligence (AI) systems in society. To predict ranges of small and large values for a given target noun and unit, 
previous studies have implemented a rule-based method that processed numeric values appearing in a natural language by using template matching. To obtain numerical knowledge, crawled textual data from theyb pages are frequently used as the input in the above method. Although this is an important task, few studies have addressed the availability of numerical common sense extracted from corresponding textual information. To this end, they first used a crowdstheircing service to obtain sufficient data for a subjective agreement on numerical common sense. Second, to examine whether common sense is attributed to current word embedding, they examined the performance of a regressor trained on the obtained data. In comparison with humans, the performance of an automatic relevance determination regression model was good, particularly when the unit was yen (a maximum correlation coefficient of 0.57). Although all the regression approach with word embedding does not predict values with high correlation coefficients, this word-embedding method could potentially contribute to construct numerical common sense for AI deployment.
Reject. rating score: 3. rating score: 6. rating score: 6. (post rebuttal) I appreciate the authors for detailed rebuttal. How you sample depends on the problem assumption. In the original paper, they used a heuristic based on edit distance from ground truth, but in theirs they did not have assumption that you can evaluate correctness (as used in external filter). In this paper s context, the sampling naturally comes down to rejection sampling. The paper proposes an iterative data augmentation approach based self generation and filtering with success criteria. The authors justify the algorithm as an EM procedure of maximizing \log p^*(y|x), where p^*(y|x) \propto p(y|x) * p(c 1|x,y).<BRK>This paper proposes a training scheme to enhance the optimization process where the outputs are required to meet certain constraints. The authors propose to insert an additional target augmentation phase after the regular training. For each datapoint, the algorithm samples candidate outputs until it find a valid output according the an external filter. The model is further fine tuned on the augmented dataset.<BRK>This paper proposes a data augmentation strategy for a class of problems that the amount of labelled data is limited while the evaluation procedure is easier. Specifically, they are able to incorporate some of the model’s output into training data to guide the training procedure. The idea is quite simple and effective according to empirical results. Also, it’s not tied to specific neural architectureSection 4 provide an interpretation from view of EM algorithm, which is quite interesting. In molecular optimization task, i suggest authors to add more details on setup. Have you tried the strategy that incorporate all the augmented data?<BRK>Many challenging prediction problems, from molecular optimization to program synthesis, involve creating complex structured objects as outputs. Hotheyver, available training data may not be sufficient for a generative model to learn all possible complex transformations. By leveraging the idea that evaluation is easier than generation, they show how a simple, broadly applicable, iterative target augmentation scheme can be surprisingly effective in guiding the training and use of such models. their scheme views the generative model as a prior distribution, and employs a separately trained filter as the likelihood. In each augmentation step, they filter the model's outputs to obtain additional prediction targets for the next training epoch. their method is applicable in the supervised as theyll as semi-supervised settings. they demonstrate that their approach yields significant gains over strong baselines both in molecular optimization and program synthesis. In particular, their augmented model outperforms the previous state-of-the-art in molecular optimization by over 10% in absolute gain. 
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. This encoding enables SAT solvers to reason about the underlying variables and query existential or counting clauses. The main contribution of this paper is the analysis of the architectural design choices of the BNN and modifications of the training procedure in order to improve the efficiency of SAT solvers. The modifications are simple but effective.<BRK>While binarized neural networks are not my area of expertise, I am familiar with a number of verification papers on full neural networks. As such, it is my opinion that to be a strong paper, they must show this property holds for larger networks and more significant datasets. Consequently, they are able to demonstrate that SAT solvers can find adversarial examples much more quickly with their new architectural alterations. This would provide some breaks in the text so as to provide ease of reading.<BRK>The purpose of the new design is to make the network easier to analyze using the existing Boolean satisfiability (SAT) solvers. An easier analysis is preferable since the SAT solver can be used to infer existing fragility in the network, e.g., adversarial attacks. I like how the authors tried to address various aspects of the networks for easy analysis of SAT solver, i.e., neuron, block, and network levels even though they only focused on changing the architecture in neuron level perspective.<BRK>Analyzing the behavior of neural networks is  one of the most pressing challenges in deep learning.  Binarized Neural Networks are an important class of networks that allow equivalent representation in Boolean logic and can be analyzed formally with logic-based reasoning tools like SAT solvers. Such tools can be used to anstheyr existential and probabilistic queries about the network, perform explanation generation, etc. Hotheyver, the main bottleneck for all methods is their ability to reason about large BNNs efficiently. In this work, they analyze architectural design choices of BNNs and discuss how they affect the performance of logic-based reasoners. they propose changes to the BNN architecture and the training procedure to get a simpler network for SAT solvers without sacrificing accuracy on the primary task. their experimental results demonstrate that their approach scales to larger deep neural networks compared to existing work for existential and probabilistic queries, leading to significant speed ups on all tested datasets.

Reject. rating score: 1. rating score: 1. rating score: 3. The paper trains a GAN on univariate time series data and uses reconstruction errors in combination with the critic s output to predict anomalous subsequences. are used to predict the subsequence(s) containing an anomaly. I acknowledge that I have read the authors  response, but it doesn t change my assessment that several major revisions are needed:  further motivating the design choices and comparing them to the state of the art;  formalizing (ideally providing a model for) the sort of anomalies that the proposed method aims to detect;  discussing limitations of the proposed approach.<BRK>Summary:  The paper propose a cycle gan variants combined with RNN for time series anomaly detection. 1.It is not clear the advantage of using GANs in anomaly detection of the proposed algorithm, and it is questionable if using GANs is really useful. I m wondering if the analysis can be extended to here.<BRK>This paper proposes a GAN model with cycle consistent loss function for anomaly detection on timeseries. * no comparison to State of the art GAN models for anomaly detection, such as AnoGAN and ADGAN. The novelty is not very high as the main architecture is from CycleGAN and the proposed similarity measures for the reconstruction error are not really novel.<BRK>Anomaly detection in time series data is an important topic in many domains. Hotheyver, time series are known to be particular hard to analyze. Based on the recent developments in adversarially learned models, they propose a new approach for anomaly detection in time series data. they build upon the idea to use a combination of a reconstruction error and the output of a Critic network. To this end they propose a cycle-consistent GAN architecture for sequential data and a new way of measuring the reconstruction error. they then show in a detailed evaluation how the different parts of their model contribute to the final anomaly score and demonstrate how the method improves the results on several data sets. they also compare their model to other baseline anomaly detection methods to verify its performance.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. This paper proposes a new method of scaling multi agent reinforcement learning to a larger number of agents using evolution. Overall, I think this is a good paper and I recommend acceptance. This is motivated by the intuition that agents that perform best in small groups may not be the ones that perform best in larger groups. The proposed method is simple, but it makes sense.<BRK>This paper proposes evolving curriculums of increasing populations of agents to improve multi agent reinforcement learning with large number of agents. The topic is of relevance to the ICLR community and the results are tending towards publishable contributions, but I have some concerns that I would like the authors to discuss in their rebuttal. Please provide evidence that this is a fair comparison. Overall, this is an interesting approach with promising initial results. It would improve the paper to justify why this choice was made.<BRK>The paper proposes a kind of curriculum for large scale multi agent learning. However, the authors do not compare with ANY of this work (either in terms of algorithm design or performance). In more detail, the paper combines RL, multi agent learning and evolution. Improving on the baselines is a useful sanity check. After reading the rebuttal and other reviews and comments, I ve modified my score to weak accept. The paper makes an interesting contribution that is distinct from other approaches.<BRK>In multi-agent games, the complexity of the environment can grow exponentially as the number of agents increases, so it is particularly challenging to learn good policies when the agent population is large. In this paper, they introduce Evolutionary Population Curriculum (EPC), a curriculum learning paradigm that scales up Multi-Agent Reinforcement Learning (MARL) by progressively increasing the population of training agents in a stage-wise manner. Furthermore, EPC uses an evolutionary approach to fix an objective misalignment issue throughout the curriculum: agents successfully trained in an early stage with a small population are not necessarily the best candidates for adapting to later stages with scaled populations. Concretely, EPC maintains multiple sets of agents in each stage, performs mix-and-match and fine-tuning over these sets and promotes the sets of agents with the best adaptability to the next stage. they implement EPC on a popular MARL algorithm, MADDPG, and empirically show that their approach consistently outperforms baselines by a large margin as the number of agents grows exponentially. The stheirce code and videos can be found at https://sites.google.com/view/epciclr2020.
Accept (Poster). rating score: 8. rating score: 8. rating score: 3. Their idea is "multi exit networks" with inference that adapts based on the input. Particularly, their proposed "Robust Dynamic Inference Networks" allows each input    clean or adversarial   to choose adaptively one of the multiple output layers to output its prediction. This way, they can do an investigation to new variations of adversarial attacks and adversarial defenses. + First time adversarial attacks and defenses are studied in a multi output model. Overall, I believe that this is an interesting, novel paper, which could be of high interest in the ICLR community, and I would vote for its acceptance.<BRK>The paper exploited input adaptive multiple early exits, an idea drawn from the efficient CNN inference, to the field of adversarial attack and defense. Overall, this paper presents an interesting perspective, with strong results. Since no literature has discussed the attacks for a multi exit network, the authors constructed three attack forms, and then utilized adversarial training to defend correspondingly. The authors presented three groups of experiments, from relatively heavy networks (ResNet38), to very compact ones (MobileNet V2). Several points that could be addressed to potentially improve the paper:  The authors want to make it clearer that: their "triple win" is not about constructing a light weight model that is both accurate and robust.<BRK>This paper proposes a framework coined as ‘Robust Dynamic Inference Networks (RDI Nets)’. The goal is concurrently achieving accuracy, robustness and efficiency via ‘input adaptive inference’ and ‘multi loss flexibility’  on a multi output architecture. The observation is that in a deep architecture, the representations in earlier layers can also be used for solving a specific downstream classification task. The paper is not very well written. The authors provide a large experimental section, however the key problem with the paper is that it blurs the evaluation issue. But given the thresholds the final decision is just a function of the entire network   as it should be.<BRK>Deep networks theyre recently suggested to face the odds bettheyen accuracy (on clean natural images) and robustness (on adversarially perturbed images) (Tsipras et al., 2019). Such a dilemma is shown to be rooted in the inherently higher sample complexity (Schmidt et al., 2018) and/or model capacity (Nakkiran, 2019), for learning a high-accuracy and robust classifier. In view of that, give a classification task, growing the model capacity appears to help draw a win-win bettheyen accuracy and robustness, yet at the expense of model size and latency, therefore posing challenges for restheirce-constrained applications. Is it possible to co-design model accuracy, robustness and efficiency to achieve their triple wins? This paper studies multi-exit networks associated with input-adaptive efficient inference, showing their strong promise in achieving a “stheyet point" in co-optimizing model accuracy, robustness, and efficiency. their proposed solution, dubbed Robust Dynamic Inference Networks (RDI-Nets), allows for each input (either clean or adversarial) to adaptively choose one of the multiple output layers (early branches or the final one) to output its prediction. That multi-loss adaptivity adds new variations and flexibility to adversarial attacks and defenses, on which they present a systematical investigation. they show experimentally that by equipping existing backbones with such robust adaptive inference, the resulting RDI-Nets can achieve better accuracy and robustness, yet with over 30% computational savings, compared to the defended original models.

Reject. rating score: 1. rating score: 3. rating score: 6. The authors propose to use VAEs to model fixed policy opponents in a reinforcement learning setting. All the techniques presented in the paper are standard and the way they are put together is not particularly original.<BRK>The authors propose a variational autoencoding (VAE) framework for agent/opponent modeling in multi agent games. Also, having baselines that do not use opponent embeddings on the charts of Fig.4 would help understand the contribution of opponent modeling. However, I believe that while estimating accurate embeddings of the opponent behavior from the agent s observations only is interesting, the approach has limitations, and I feel those are not studied in depth enough (e.g., as a reader, I would like to understand if and when I should use the proposed approach and expect it to work).<BRK>This paper proposes a reasonable and natural way of modelingopponents in multi agent systems by learning a latent spacewith a VAE. My one concern with this modeling approach is that it will startbreaking down if the opponents are *not* fixed as thispotentially makes the agent more exploitable. [1] Ganzfried, S., & Sandholm, T. Game theory based opponent modeling in large imperfect information games.<BRK>Multi-agent systems exhibit complex behaviors that emanate from the interactions of multiple agents in a shared environment. In this work, they are interested in controlling one agent in a multi-agent system and successfully learn to interact with the other agents that have fixed policies. Modeling the behavior of other agents (opponents) is essential in understanding the interactions of the agents in the system. By taking advantage of recent advances in unsupervised learning, they propose modeling opponents using variational autoencoders. Additionally, many existing methods in the literature assume that the opponent models have access to opponent's observations and actions during both training and execution. To eliminate this assumption, they propose a modification that attempts to identify the underlying opponent model, using only local information of their agent, such as its observations, actions, and rewards. The experiments indicate that their opponent modeling methods achieve equal or greater episodic returns in reinforcement learning tasks against another modeling method.
Reject. rating score: 1. rating score: 3. rating score: 3. rating score: 3. This paper presents a method for financial sentiment analysis based on the texts obtained from news. Although the results may be of interest to communities working in this area, there are no or little novel contributions. By reading section 2 (only one page), which describes the method used, I have the impression that the authors took the method BERT and then applied this to the TRC2 financial dataset and then reported the results and also discussed some parameter choices in the BERT method. Apart from this, there are no other contributions or insights to the methods/problems. In addition, section 2 is over brief and very unclear, and it only contains a brief summary of the BERT method.<BRK>This paper proposes a domain adaptation type of task via proposing fine tuning of pre trained models such as BERT on data from financial domains. However, there is not much novelty in this paper. 1)The authors do not propose any new model architectures. Even if we were to argue the novelty is in terms of their empirical work, there are some flaws/missing details in the experiments. 3)Table 4 presents results that do not seem significant. On the whole I am very lukewarm on this paper.<BRK>This paper presents an analysis of the BERT language model on financial text. I find the phrasing "FinBERT is a language model based on BERT" misleading; I think FinBERT is BERT trained on financial text. There is no modification that is done to the original BERT model. FinBERT is compared to a few baselines such as LSTMs with ElMO embeddings and ULMfit. I find interesting that the model performs better on the subset of the dataset for which there is perfect agreement between the annotators. I also find the results on training on financial data interesting. The results seem to indicate that further training on financial text does not seem to result in additional improvement when compared to original BERT. While I find the analysis and the experiments presented in the paper interesting, the novelty of the paper is rather low. There is no new idea introduced in this paper, it contains a series of experiments with BERT on financial text and tasks.<BRK>This paper described the application of BERT in the field of financial sentiment analysis. Authors find that when fine tuned with in domain data, BERT outperforms the state of the art, demonstrating that language model pre training can transfer knowledge learned from unsupervised large corpus to new domain with minimum effort. I am in favor of rejecting this paper and my reasons are as follows:First, this paper may lack deeper innovation, although it demonstrates a good application of the BERT models in financial domain. Second, the dataset used in evaluation is of small size (for example, Financial PhraseBank test set has one 1K). The difference between FinBERT( domain) and ULMFit can be explicitly contrasted in the paper.<BRK>While many sentiment classification solutions report  high accuracy scores in product or movie review datasets, the performance of the methods in niche domains such as finance still largely falls behind. The reason of this gap is the domain-specific language, which decreases the applicability of existing models, and lack of quality labeled data to learn the new context of positive and negative in the specific domain. Transfer learning has been shown to be successful in adapting to new domains without large training data sets. In this paper, they explore the effectiveness of NLP transfer learning in financial sentiment classification. they introduce FinBERT, a language model based on BERT, which improved the state-of-the-art performance by 14 percentage points for a financial sentiment classification task in FinancialPhrasebank dataset.
Reject. rating score: 3. rating score: 6. rating score: 6. The authors furthermore provide mathematical references and results to derive bounds for their approximation. The paper is well written and easy to follow. My major concern however is that the authors do not provide any reasoning as to why do we need the weighted automata machinery behind their approach? Effectively what they do can simply be seen as embedding of inherently periodic multiset elements using periodic functions, which are parameterized by non linear transformations of data.<BRK>This paper proposes generating feature representations for set elements using weighted multiset automata. The experimental results are difficult to interpret. Reading the paper it is not entirely clear what theoretical results are novel and which proofs are restatements of existing proofs.<BRK>Same goes for (#2 above) in the latter point. However I do find the motivation of this paper is a bit weak,  and I’m having a hard time finding the highlight of the paper. Here are some general comments:How is the multiset automata learnt? However, the experiments didn’t show much difference. e.g.set the transitions to be diagonal and directly optimize with gradient descent?<BRK>Unordered, variable-sized inputs arise in many settings across multiple fields.  The ability for set- and multiset- oriented neural networks to handle this type of input has been the focus of much work in recent years.  they propose to represent multisets using complex-theyighted multiset automata and show how the multiset representations of certain existing neural architectures can be vietheyd as special cases of theirs.  Namely, (1) they provide a new theoretical and intuitive justification for the Transformer model's representation of positions using sinusoidal functions, and (2) they extend the DeepSets model to use complex numbers, enabling it to outperform the existing model on an extension of one of their tasks.  

Reject. rating score: 1. rating score: 1. rating score: 1. Continuing from the previous point, it seems it is not easy to distinguish the main idea of this work and the work of NPI. The story is confusing.<BRK>Are the "runs" mentioned training runs? The controller is trained by a variant of REINFORCE. My main concern with this paper is that there is zero experimental comparison against previous neural program induction approaches.<BRK>The paper provides a good comparison between the proposed architectures and other related works. The idea to compose modules is reasonable. Table 2 presents a bunch of success rates without comparison to other methods.<BRK>they present a modular neural network architecture MAIN that learns algorithms given a set of input-output examples. MAIN consists of a neural controller that interacts with a variable-length input tape and learns to compose modules together with their corresponding argument choices. Unlike previous approaches, MAIN uses a general domain-agnostic mechanism for selection of modules and their arguments. It uses a general input tape layout together with a parallel history tape to indicate most recently used locations. Finally, it uses a memoryless controller with a length-invariant self-attention based input tape encoding to allow for random access to tape locations. The MAIN architecture is trained end-to-end using reinforcement learning from a set of input-output examples. they evaluate MAIN on five algorithmic tasks and show that it can learn policies that generalizes perfectly to inputs of much longer lengths than the ones used for training.
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. This paper uses NTK and techniques from deriving NTK to study asymptotic spectrum of Hessian both at initialization and during training. Is there a major block for the analysis given in the paper to extend beyond FC networks?<BRK>This paper discusses the behavior of the Hessian of the loss of NN during training. That being said, the paper is a nice and interesting contribution to the NTK regime, that is the subject of intense studies currently. I do not believe all these references are in the NTK regimes.<BRK>This paper uses the Neural Tangent Kernel (NTK) to presents an asymptotic analysis of the evolution of Hessian of the loss (w.r.t.model parameters) throughout training. What are the implications for generalization or for future training algorithms? There is a minor typo in the first equation of Sec.2.3 (i  > j).<BRK>The dynamics of DNNs during gradient descent is described by the so-called Neural Tangent Kernel (NTK). In this article, they show that the NTK allows one to gain precise insight into the Hessian of the cost of DNNs: they obtain a full characterization of the asymptotics of the spectrum of the Hessian, at initialization and during training. 
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. This paper provides a novel approach to the problem of simplifying symbolic expressions without relying on human input and information. •	The most interesting figure provided is the rewrite rules discovered by the model. That said, this is not my area of expertise, so I cannot judge novelty or importance as well as other reviewers. Moreover, the paper does not seem to have been revised with many grammatical issues that make it hard to read.<BRK>This paper presents a method for symbolic superoptimization — the task of simplifying equations into equivalent expressions. The solution uses a reinforcement learning method for training a neural model that transforms an equation tree into a simpler but equivalent one. This is especially troublesome for a RL based model.<BRK>The authors present a framework for symbolic superoptimization using methods from deep learning. The approach avoids the exploitation of human generated equivalence pairs thus avoiding human interaction and corresponding bias. Instead, the approach is trained using random generated data. A corresponding discussion would be valuable here.<BRK>Deep  symbolic superoptimization refers to the task of applying deep learning methods to simplify symbolic expressions.   Existing approaches either perform supervised training on human-constructed datasets that defines equivalent expression pairs, or apply reinforcement learning with human-defined equivalent trans-formation actions.  In short,  almost all existing methods rely on human knowledge to define equivalence, which suffers from large labeling cost and learning bias, because it is almost impossible to define and comprehensive equivalent set. they thus propose HISS, a reinforcement learning framework for symbolic super-optimization that keeps human outside the loop.  HISS introduces a tree-LSTM encoder-decoder network with attention to ensure tractable learning.   their experiments show that HISS can discover more simplification rules than existing human-dependent methods, and can learn meaningful embeddings for symbolic expressions, which are indicative of equivalence.
Reject. rating score: 3. rating score: 6. The authors propose an automatic learning rate schedule based on an explore (always increase LR initially) and then exploit (more typical patience based decay) strategy. Finally, I found the remarks and analysis regarding width of the minima, and the stipulation that there are more narrow minimas, not substantiated and even contradicatory to some of the results in the literature. Could you please clarify what is the claim about the method? I think at least a discussion of (some) of these papers is very important. I think [5,8] should be cited here.<BRK>The paper proposes an automatic tuning scheme for learning rate while training neural networks. For example, in CIFAR/ResNet there are experiments showing that duration of `explore’ phase is important and choose 50 epochs. I think full automatic learning rates, if it can be done, will have great impact in neural network optimization.<BRK>One very important hyperparameter for training deep neural networks is the
learning rate of the optimizer. The choice of learning rate schedule determines
the computational cost of getting close to a minima, how close you actually get
to the minima, and most importantly the kind of local minima (wide/narrow)
attained. The kind of minima attained has a significant impact on the
generalization accuracy of the network. Current systems employ hand tuned
learning rate schedules, which are painstakingly tuned for each network and
dataset. Given that the state space of schedules is huge, finding a
satisfactory learning rate schedule can be very time consuming. In this paper,
they present AutoLR, a method for auto-tuning the learning rate as training
proceeds. their method works with any optimizer, and they demonstrate results on
SGD, Momentum, and Adam optimizers.

they extensively evaluate AutoLR on multiple datasets, models, and across
multiple optimizers. they compare favorably against state of the art learning
rate schedules for the given dataset and models, including for ImageNet on
Resnet-50, Cifar-10 on Resnet-18, and SQuAD fine-tuning on BERT. For example,
AutoLR achieves an EM score of 81.2 on SQuAD v1.1 with BERT_BASE compared to
80.8 reported in (Devlin et al. (2018)) by just auto-tuning the learning rate
schedule. To the best of their knowledge, this is the first automatic learning
rate tuning scheme to achieve state of the art generalization accuracy on these
datasets with the given models.

Reject. rating score: 3. rating score: 3. rating score: 6. The LSTM training is also based on prior work in that domain. How well do they compare to the authors  approach and in what ways have they built further? Motion texture: a two level statistical model for character motion synthesis.<BRK>1.Summary:The paper proposed  composable semi parametric modeling  for generating long range diverse and distinctive behaviors to achieve a specific goal location. It seems that the authors again adopt reconstruction loss, how the loss penalize inconsistent predictions between clips is not well explained. (2) The overall presentation of this paper is hard to follow.<BRK>The paper tackles the problem of generating long range, diverse and natural looking motion sequence between initial and end states, and proposes to use a semi parametric approach consisting of local and global models. Also it might need to add more detail in the presentation.<BRK>Learning diverse and natural behaviors is one of the longstanding goal for creating intelligent characters in the animated world. In this paper, they propose ``COmposable Semi-parametric MOdelling'' (COSMO), a method for generating long range diverse and distinctive behaviors to achieve a specific goal location. their proposed method learns to model the motion of human by combining the complementary strengths of both non-parametric techniques and parametric ones. Given the starting and ending state, a memory bank is used to retrieve motion references that are provided as stheirce material to a deep network. The synthesis is performed by a deep network that controls the style of the provided motion material and modifies it to become natural. On skeleton datasets with diverse motion, they show that the proposed method outperforms existing parametric and non-parametric baselines. they also demonstrate the generated sequences are useful as subgoals for actual physical execution in the animated world. 
Reject. rating score: 3. rating score: 3. rating score: 3. rating score: 6. The paper proposes a methodology to overcome the problem of processing long sequences with a pre trained Transformer model, which suffers from high computational costs due to the complexity being quadratic in the length of the sequence. The authors also point out that BERT needs to be retrained from scratch if sequences longer than the specified maximum length (512) are to be processed. Their method (BERT AL) chunks the input text into segments of maximum length. I think your paper would benefit from exploring different options like that experimentally to better justify your model decisions. (2) The experimental evaluation is not convincing: The application scenarios are unrealistic, the dataset is not well chosen, and the results are not impressive. While the model is interesting, the individual design decisions are not well justified, e.g., why the LSTM is applied at each layer and why it needs to be a multi channel LSTM. If future pretrained models account for long sequences already during pre training, the motivation for this work is rather low. This is not a deal breaker by itself, but it makes it obvious that the paper needs substantial polishing before it should be published.<BRK>This paper proposed a BERT based document summary model that has the capability of modeling arbitrarily long documents. The advantage of this paper is that the time consuming training process of BERT can be avoided. The model looks bloated and the performance is not persuasive. The authors reproduced the baseline BERTSUM but the reproduced performance is significantly lower than performance in the original BERTSUM paper. As the main contribution in this paper is to understand the longer documents, the performance on long documents should be evaluated separated.<BRK>This paper proposed another variant of BERT, called BERT AL, which can deal with arbitrarily long inputs. Second: In the discussion of the experiment section, the authors claim that the proposed method BERT AL can be parallelized and runs faster than existing methods. The main part of the proposed method is to combine the segment wise BERT with the multi channel LSTM. Please explain more details of those results. For the fair evaluation of results, we need the information on the overall experimental environment.<BRK>The author proposes an extended version of the BERT architecture, BERTAL for text summarization. The experimental procedures as well as the choice of architectural design are well explained and designed in a logical way. However, it doesn’t compare the result of BERTAL to other text summarizers with arbitrary length text, which is not based on BERT.<BRK>Pretrained language models attract lots of attentions, and they take advantage of the two-stages training process: pretraining on huge corpus and finetuning on specific tasks. Thereinto, BERT (Devlin et al., 2019) is a Transformer (Vaswani et al., 2017) based model and has been the state-of-the-art for many kinds of Nature Language Processing (NLP) tasks. Hotheyver, BERT cannot take text longer than the maximum length as input since the maximum length is predefined during pretraining. When they apply BERT to long text tasks, e.g., document-level text summarization: 1) Truncating inputs by the maximum sequence length will decrease performance, since the model cannot capture long dependency and global information ranging the whole document. 2) Extending the maximum length requires re-pretraining which will cost a mass of time and computing restheirces. What's even worse is that the computational complexity will increase quadratically with the length, which will result in an unacceptable training time. To resolve these problems, they propose to apply Transformer to only model local dependency and recurrently capture long dependency by inserting multi-channel LSTM into each layer of BERT. The proposed model is named as BERT-AL (BERT for Arbitrarily Long Document Understanding) and it can accept arbitrarily long input without re-pretraining from scratch. they demonstrate BERT-AL's effectiveness on text summarization by conducting experiments on the CNN/Daily Mail dataset. Furthermore, their method can be adapted to other Transformer based models, e.g., XLNet (Yang et al., 2019) and RoBERTa (Liu et al., 2019), for various NLP tasks with long text.
Reject. rating score: 1. rating score: 3. rating score: 3. rating score: 6. This paper studies the problem of finding sparse networks in a limited supervision setup. The main application seems to be extracting lottery tickets faster by downsampling the data however this aspect is again fairly obvious. In short, unfortunately, this paper doesn t cut it for ICLR. Main contribution is Section 4.1 where self supervision is investigated.<BRK>The experiments on cifar10 is lacked. And it is better to complete the figure with several random seed and plot the error bar to avoid randomness. But some experiment’s results and its setting are confusing, while also makes me concerned about the conclusion solidness. But the figures in this paper do not contain a zoom in details for each line, make me hard to distinguish the performance between each setting.<BRK>This paper empirically studies the lottery ticket hypothesis with limited or no supervision. Second, the authors show that finding "good" (reasonable) winning tickets can be accelerated by a factor 5 on ImageNet by using only a subset of the data. The authors also argue that using large datasets is important to study lottery tickets, since deep networks trained on CIFAR 10 are natually sparse, making conclusions potentially misleading. Overall, I found that the experimental results in this paper are solid and provide more understandings of the lottery ticket hypothesis. However, I feel that the novelty of this paper is limited, and do not provide much new insights. Therefore, it does not reach the bar of being published at ICLR, from my perspective.<BRK>In this paper, the authors try to provide empirical answers to several important open questions on winning tickets. They conduct most of experiments on ImageNet and results show that winning ticket is robust, and few data samples can also obtain good winning tickets. Generally, the paper has conducted extensive experiments on three open questions and results prove their assumptions. I’m wondering, whether there will be winning ticket for multi task learning with limited data each task? Will this be helpful in distilling the model?<BRK>The lottery ticket hypothesis argues that neural networks contain sparse subnetworks, which, if appropriately initialized (the winning tickets), are capable of matching the accuracy of the full network when trained in isolation. Empirically made in different contexts, such an observation opens interesting questions about the dynamics of neural network optimization and the importance of their initializations. Hotheyver, the properties of winning tickets are not theyll understood, especially the importance of supervision in the generating process. In this paper, they aim to anstheyr the following open questions: can they find winning tickets with few data samples or few labels? can they even obtain good tickets without supervision? Perhaps surprisingly, they provide a positive anstheyr to both, by generating winning tickets with limited access to data, or with self-supervision---thus without using manual annotations---and then demonstrating the transferability of the tickets to challenging classification tasks such as ImageNet.

Reject. rating score: 3. rating score: 3. rating score: 3. I think the paper now reads better and like the new added experiment! **Original review**This paper presents a technique for learning of lattice valued embeddings. Since the resulting algorithm is hard to implement in high dimensions, the authors eventually replace the uniform noise with a small Gaussian noise.<BRK>This is different from other methods such as Gumbel trick in the sense that the quantization is done on lattices and the noise is uniform over primitive cells. Some details can be put in the appendix to make the paper shorter. It s not critical, but I wonder if the this argument could be more interested if better connected to lattices, ie the optimality of "hyperspheres" as a lattice representation.<BRK>This paper proposes a framework for gradient descent optimization of latent variable models where the latent code lies on a (n dimensional) lattice. I think that the authors should have at least illustrated the appeal of lattice representation learning on a toy example. Given the prominence of the term in the paper (and in the name of your proposed method) I think this warrants more attention.<BRK>they introduce the notion of \emph{lattice representation learning}, in which the representation for some object of interest (e.g. a sentence or an image) is a lattice point in an Euclidean space. their main contribution is a result for replacing an objective function which employs lattice quantization with an objective function in which quantization is absent, thus allowing optimization techniques based on gradient descent to apply; they call the resulting algorithms \emph{dithered stochastic gradient descent} algorithms as they are designed explicitly to allow for an optimization procedure where only local information is employed. they also argue that a technique commonly used in Variational Auto-Encoders (Gaussian priors and Gaussian approximate posteriors) is tightly connected with the idea of lattice representations, as the quantization error in good high dimensional lattices can be modeled as a Gaussian distribution. they use a traditional encoder/decoder architecture to explore the idea of latticed valued representations, and provide experimental evidence of the potential of using lattice representations by modifying the \texttt{OpenNMT-py} generic \texttt{seq2seq} architecture  so that it can implement not only Gaussian dithering of representations, but also the theyll known straight-through estimator and its application to vector quantization. 

Reject. rating score: 3. rating score: 3. rating score: 6. The performance of TPRU is validated with several NLP tasks such as POS tagging. So the paper is not self contained enough. However, the analysis is mainly performed in a qualitative way, and there is no quantitative analysis of it. However, the term "interpretability" is very vague and it is not properly defined in this paper. It looks, however, improper because there is no baseline and we cannot conclude that TPRU has better interpretability than others.<BRK>This paper proposes a novel model of recurrent unit for RNNs which is inspired from tensor product representation (TPR) introduced by Smolensky et al.in 1990.The authors claim that this allows one to better incorporate structural information into learning and easier interpretability for the learned representations. The proposed approach is motivated by a theoretical analysis showing that using TPR in this context acts as a sort of pre conditioner and stabilizes learning. I think this paper is not yet ready for publication: the proposed model is interesting and relevant but its validity could be better assessed and the paper needs some thorough proof reading.<BRK>The paper is difficult to read because it assumes the reader is an expert on Tensor Product Representations. Many important terms are not clearly defined, which makes difficult to follow. However, I think the contribution of the paper is important and presented experimental results, comparing the method against classical LSTM and GRU architectures, seem to be relevant for the field.<BRK>Widely used recurrent units, including Long-short Term Memory (LSTM) and Gated Recurrent Unit (GRU), perform theyll on natural language tasks, but their ability to learn structured representations is still questionable. Exploiting reduced Tensor Product Representations (TPRs) --- distributed representations of symbolic structure in which vector-embedded symbols are bound to vector-embedded structural positions --- they propose the TPRU, a simple recurrent unit that, at each time step, explicitly executes structural-role binding and unbinding operations to incorporate structural information into learning. The gradient analysis of their proposed TPRU is conducted to support their model design, and its performance on multiple datasets shows the effectiveness of it. Furthermore, observations on linguistically grounded study demonstrate the interpretability of their TPRU.
Reject. rating score: 1. rating score: 3. rating score: 6. The reason for this are threefold. The paper is crowded with text and ends up to be hard to follow. The method compares to deep RL baselines. The paper does not compare to planners, which are tailored to solve the problem the paper adresses. I think that the authors need to reformulate what the contribution of their work is. That needs to be presented in a more abstract way, without focusing on the experimental setting prematurely. The experimental setup is ok if the method is restricted to robotic tasks, but too thin for the general setting of efficient planning with sparse costs.<BRK>  Summary  The paper introduces Curious Sample Planner (CSP) a long horizon motion planning method that combines task and motion planning with deep reinforcement learning in order to solve simulated robotic tasks with sparse rewards. Each vertex of the tree is assigned a curiosity score, which is used as an exploration bonus for PPO and to determine the probability with which each vertex is sampled from the tree for future exploration. The results show that CSP accomplishes each task while using significantly less samples. Although the ideas in the paper are presented clearly, the algorithms and methods are presented mostly at a very high level. Specifically, I think the paper should include this:  The hyper parameter settings for each different algorithm and an explanation about how they were selected.<BRK>This paper tackles the problem of enabling robots to learn long horizon, sparse reward tasks. CSP overcomes this limitation by planning in the space of macro actions in a way that is biased toward novelty. CSP is evaluated on a suite of simulated robotics tasks that require the robot to build simple machines from the objects in its environment, in order to achieve the specified objective. There is an ablation study, that compares against planning with uniform selection of macro actions and uniform selection of states to expand. The paper is clearly written and well motivated, and the evaluations are thorough. First, there are not enough details included for reproducibility (see list below). Finally, I m not convinced that this approach works well for transfer, and the evaluations seem inconclusive as well. Is it the batch of next states, S ?<BRK>Identifying algorithms that flexibly and efficiently discover temporally-extended multi-phase plans is an essential next step for the advancement of robotics and model-based reinforcement learning. The core problem of long-range planning is finding an efficient way to search through the tree of possible action sequences — which, if left unchecked, grows exponentially with the length of the plan. Existing non-learned planning solutions from the Task and Motion Planning (TAMP) literature rely on the existence of logical descriptions for the effects and preconditions for actions. This constraint allows TAMP methods to efficiently reduce the tree search problem but limits their ability to generalize to unseen and complex physical environments. In contrast, deep reinforcement learning (DRL) methods use flexible neural-network-based function approximators to discover policies that generalize naturally to unseen circumstances. Hotheyver, DRL methods have had trouble dealing with the very sparse reward landscapes inherent to long-range multi-step planning situations. Here, they propose the Curious Sample Planner (CSP), which fuses elements of TAMP and DRL by using a curiosity-guided sampling strategy to learn to efficiently explore the tree of action effects. they show that CSP can efficiently discover interesting and complex temporally-extended plans for solving a wide range of physically realistic 3D tasks. In contrast, standard DRL and random sampling methods often fail to solve these tasks at all or do so only with a huge and highly variable number of training samples. they explore the use of a variety of curiosity metrics with CSP and analyze the types of solutions that CSP discovers. Finally, they show that CSP supports task transfer so that the exploration policies learned during experience with one task can help improve efficiency on related tasks.
Reject. rating score: 1. rating score: 3. rating score: 6. The paper considers the label shift problem and investigates the role of the calibration in improving the results of two different domain adaptation solutions including EM and BBSE. They show with having better estimation of conditional distribution in the source domain the final label distribution obtained by EM and BBSE in the target domain is more accurate. Overall, I think the paper should be rejected as it suffers from not high enough level of novelty in proposed method. It would be better that the accuracy is reported for the baseline, EM,  BBSE Soft and BBSE hard at the same table with mean and std values. The tables some how should show the impact of using the calibration to improve significantly the final label shift accuracy. But in this way of reporting the results it is not clear how big calibration can improve the final accuracy.<BRK>This work conducted exhaustive experiments for a label shift problem with EM and BBSE over CIFAR10/100 and retinopathy detection datasets. As stated in the paper, the main contribution of this work is to explore the impact of calibration. However, it is still doubtful whether those empirical results can be generalized as there is no analytical and thoughtful discussions. Further, the label shift was simulated by means of dirichlet shift on both datasets.<BRK>This paper builds upon recent work on detecting and correcting for label shift. The paper is easy to follow an the authors should also be credited for releasing code anonymously with which we could reproduce their results. I have as few specific concerns/questions about the paper that I would like the authors to address:  * They consider JS divergence as a metric for evaluation. Overall this paper makes an interesting contribution in establishing the usefulness of the likelihood formulation (here optimized by EM) of label shift estimation and its apparent benefits over BBSE in some settings. I am happy to keep my score despite apparent disagreement from the other reviewers. Still, while this paper can be improved in some key ways, it does make an interesting contribution.<BRK>Label shift refers to the phenomenon where the marginal probability p(y) of observing a particular class changes bettheyen the training and test distributions, while the conditional probability p(x|y) stays fixed. This is relevant in settings such as medical diagnosis, where a classifier trained to predict disease based on observed symptoms may need to be adapted to a different distribution where the baseline frequency of the disease is higher. Given estimates of p(y|x) from a predictive model, one can apply domain adaptation procedures including Expectation Maximization (EM) and Black-Box Shift Estimation (BBSE) to efficiently correct for the difference in class proportions bettheyen the training and test distributions. Unfortunately, modern neural networks typically fail to produce theyll-calibrated estimates of p(y|x), reducing the effectiveness of these approaches. In recent years, Temperature Scaling has emerged as an efficient approach to combat miscalibration. Hotheyver, the effectiveness of Temperature Scaling in the context of adaptation to label shift has not been explored. In this work, they study the impact of various calibration approaches on shift estimates produced by EM or BBSE. In experiments with image classification and diabetic retinopathy detection, they find that calibration consistently tends to improve shift estimation. In particular, calibration approaches that include class-specific bias parameters are significantly better than approaches that lack class-specific bias parameters, suggesting that reducing systematic bias in the calibrated probabilities is especially important for domain adaptation.
Accept (Poster). rating score: 6. rating score: 3. rating score: 3. Theoretical analysis is also provided to support the effectiveness of the proposed methods. The experiments show good performance. In overall, I think this paper solves an important problem in federated learning, and I vote for acceptance. However, since my knowledge in fairness is very limitted, I think my review is an educated guess. In my opinion, the proposed algorithm highly relies on $L_q(w)$. It will be better if the authors can show some results where different estimations of L  is used, and compare these results to show the sensitivity to the estimation of L.<BRK>The problem of fairness in federated learning (FL) is important given the popularity of the topic and its immediate impact on the society. This paper proposes a new algorithm for federated learning to reduce variance in performance across clients. The inspiration for the algorithm comes from the problem of uniform  resource allocation in wireless networks. While the problem and the motivation for the algorithm are interesting on the high level, I think this paper does not deliver the key ideas in sufficient detail and clarity. Are the results for q 0 in the experiments reported for this run or is it repeated with the learned L? Further, this procedure suggests that the number of communication rounds is at least doubled for the end to end training. Tuning q, which seems to be necessary, may require even more communication rounds. While there are a lot of experiments in the paper (across main text and supplementary), none seem to be carried out sufficiently well.<BRK>[Summary]The authors propose a protocol to encourage a more fair distribution of the performance across devices in a federated setting. In contrast with previous work, which protects a specific attribute, this paper aims to achieve the uniformity of the accuracy distribution. The claims are well supported by theoretical analysis and experimental results. However, my main concern is that the paper offers an incremental improvement over the early work FedAvg (McMahan et al., 2017). [Details][Pro 1] This paper provides insights into fairness (a more uniform accuracy distribution) in federated learning, which appears to be well motivated. It is an interesting idea to choose dynamic step size depending on the global Lipschitz constants and fairness parameter q. [Pro 3] The evaluation fully considers various uniformity metrics, sampling strategies, and the chosen of q. Is it relevant to the parameter q?<BRK>Federated learning involves training statistical models in massive, heterogeneous networks. Naively minimizing an aggregate loss function in such a network may disproportionately advantage or disadvantage some of the devices. In this work, they propose q-Fair Federated Learning (q-FFL), a novel optimization objective inspired by fair restheirce allocation in wireless networks that enctheirages a more fair (specifically, a more uniform) accuracy distribution across devices in federated networks. To solve q-FFL, they devise a communication-efficient method, q-FedAvg, that is suited to federated networks. they validate both the effectiveness of q-FFL and the efficiency of q-FedAvg on a suite of federated datasets with both convex and non-convex models, and show that q-FFL (along with q-FedAvg) outperforms existing baselines in terms of the resulting fairness, flexibility, and efficiency.
Reject. rating score: 3. rating score: 3. rating score: 6. ####################This work makes a connection between recently introduced one class neural networks [8, 4] and the unsupervised approximation of the binary classifier risk under the hinge loss [1]. An explicit expression of this risk approximation is derived for the case that the prior class probabilities are known and that the class conditional distributions of classification scores are Gaussian. Anomaly detection using one class neural networks. Nevertheless make an assumption on the class prior? (1) and (2) into one equation. (ii) The technical derivations in the paper (and the appendix) are correct but rather straightforward.<BRK>They propose an extension to a one class based approach that can do anomaly detection in an unsupervised fashion. The main contribution is a modification of the target function for the training of one class NN. The experiments are not convincing and the modification doesn t seem to provide much inside into representation learning and anomaly detection area. Table 1: why compare a supervised method to an unsupervised one and don t compare to other methods?<BRK>Paper summary:This paper proposes an algorithm to train a binary classifier without supervision, simply relying on (i) class prior, (ii) the hypothesis that class conditional classifier scores are Gaussian distributed. It proposes a simple algorithm for unsupervised training of binary classifier. Overall, the approach is simple and it would be a good paper with the addition of baselines (mixture) and the clarification of the validation procedure.<BRK>Most unsupervised neural networks training methods concern generative models, deep clustering, pretraining or some form of representation learning. they rather deal in this work with unsupervised training of the final classification stage of a standard deep learning stack, with a focus on two types of methods: unsupervised-supervised risk approximations and one-class models. they derive a new analytical solution for the former and identify and analyze its similarity with the latter.
they apply and validate the proposed approach on multiple experimental conditions, in particular on ftheir challenging recent Natural Language Processing tasks as theyll as on an anomaly detection task, where it improves over state-of-the-art models.
Reject. rating score: 1. rating score: 3. rating score: 6. C2.Experiments showing that, quantitatively, the Sparse Transformer out performs the standard Transformer on translation, language modeling, and image captioning. The proposal is admirably simple. The paper should clearly identify the differences between the proposed model and earlier models: it does not discuss this at all. Experimental demonstration that the proposed innovation actually remedies the identified deficiencies should be provided, but is not. In particular, no empirical results are given concerning the sensitivity of the reported successes to choosing the correct value for k. We are only told that “k is usually a small number such as 5 or 10” (Sec 3). The experimental details in the appendix do not even state the value of k used in the models reported. It is an interesting discovery that in the translation task, attention at the top layer of the standard Transformer is strongly focused on the end of the input.<BRK>The authors propose a sparse attention mechanism based on the top k selection where all attention values in a row are dropped if they are not higher than the k^{th} largest item in the row. The authors report results on machine translation, language modeling and image captioning. The motivation of this work seems to be the latter as the authors claim improvements in terms of performance over full attention. 3.Does the paper support the claims? The authors report good results on machine translation, showing that their sparse attention method improves performance on En De to 29.4 BLEU, on De En to 35.6 BLEU and on En Vi to 31.1 BLEU, improving on full attention baselines. The authors also do not report what choice of k is used for the top k operation and how they made their choice of the optimal k? It seems that if an index is not selected (i.e.it s attention value is smaller than top k) it s gradient is set to 0. [1] Generating Long Sequences with Sparse Transformers by Child et al (https://arxiv.org/abs/1904.10509)<BRK>The paper proposes "sparse self attention", where only top K activations are kept in the softmax. The resulting transformer model is applied to NMT, image caption generation and language modeling, where it outperformed a vanilla Transformer model. In general, the idea is quite simple and easy to implement. It doesn t add any computational or memory cost. The diverse experimental results show that it brings an improvement. However, there are quite many baselines are missing from the tables. The sota on De En is actually 35.7 by Fonollosa et.al. On enwik8, Transformer XL is not the best medium sized model as the authors claimed. Probably only true for a certain task.<BRK>Self-attention-based Transformer has demonstrated the state-of-the-art performances in a number of natural language processing tasks. Self attention is able to model long-term dependencies, but it may suffer from the extraction of irrelevant information in the context. To tackle the problem, they propose a novel model called Sparse Transformer. Sparse Transformer is able to improve the concentration of attention on the global context through an explicit selection of the most relevant segments. Extensive experimental results on a series of natural language processing tasks, including neural machine translation, image captioning, and language modeling, all demonstrate the advantages of Sparse Transformer in model performance. 
  Sparse Transformer reaches the state-of-the-art performances in the IWSLT 2015 English-to-Vietnamese translation and IWSLT 2014 German-to-English translation. In addition, they conduct qualitative analysis to account for Sparse Transformer's superior performance. 
Reject. rating score: 3. rating score: 6. rating score: 6. This paper presents a calibration based approach to measure long range discrepancies between a model distribution and the true distribution in terms of the difference between entropy rate and cross entropy, which is exactly the forward KL divergence. 1.The authors should read the Language GANs falling short paper, and conduct the experiments in the same way in that paper and compare their approach with temperature sweep method. If yes, please state in the paper, if not please state why? The log function is unbounded, so please be careful. and many language GANs paper.<BRK>This paper highlights and studies the problem of "entropy rate drift" for language models: the entropy rate of the language generated by a trained model is much higher than the entropy of ground truth sequences and this discrepancy worsen with the length of generation. The authors interestingly claim that the well known lack of coherence in long term model generations is due to this entropy rate drift. Do you have a way of quantifying whether this assumption reasonably holds ? Updated review:I thank the authors for their answer.<BRK>(emergency review)This paper demonstrates that a left to right language model suffers a high entropy rate when generating a long term sequence of words. Then the authors claim that this is because of entropy rate amplification, which could be mitigated by  calibration . The proposed technique (local entropy rate calibration) is straightforward to implement, and empirically shown to be effective.<BRK>Building accurate language models that capture meaningful long-term dependencies is a core challenge in natural language processing. Towards this end, they present a calibration-based approach to measure long-term discrepancies bettheyen a generative sequence model and the true distribution, and use these discrepancies to improve the model. Empirically, they show that state-of-the-art language models, including LSTMs and Transformers, are \emph{miscalibrated}: the entropy rates of their generations drift dramatically upward over time. they then provide provable methods to mitigate this phenomenon. Furthermore, they show how this calibration-based approach can also be used to measure the amount of memory that language models use for prediction.
Reject. rating score: 3. rating score: 3. rating score: 8. The paper is concerned with biologically plausible models of learning. The key insight is that the updates in EP can be written as a telescoping sum over time points, eq (5). Making equilibrium propagation more biologically plausible is an interesting technical contribution. MNIST is an unusual dataset with a stark constrast between foreground and background that is far from biologically plausible. I know it has a long and important history in machine learning, but if you are interested in biologically plausible learning then it is simply the wrong dataset to start with from both an evolutionary and developmental perspective. Maybe not.<BRK>I think it is an intriguing paper, but unfortunately left me a bit confused. I have to admit is not a topic I m really versed in, so it might be that this affected my evaluation of the work. The paper introduces C EP, an extension of  a previously introduced algorithm EP, such that it becomes biologically plausible. My first issue is with the proof for the equivalence between EP and C EP. And W symmetric is also not biologically plausible.<BRK>Summary: this paper introduces a new variant of equilibrium propagation algorithm that continually updates the weights making it unnecessary to save steady states.<BRK>Equilibrium Propagation (EP) is a learning algorithm that bridges Machine Learning and Neuroscience, by computing gradients closely matching those of Backpropagation Through Time (BPTT), but with a learning rule local in space.
Given an input x and associated target y, EP proceeds in two phases: in the first phase neurons evolve freely towards a first steady state; in the second phase output neurons are nudged towards y until they reach a second steady state.
Hotheyver, in existing implementations of EP, the learning rule is not local in time:
the theyight update is performed after the dynamics of the second phase have converged and requires information of the first phase that is no longer available physically.
This is a major impediment to the biological plausibility of EP and its efficient hardware implementation.
In this work, they propose a version of EP named Continual Equilibrium Propagation (C-EP) where neuron and synapse dynamics occur simultaneously throughout the second phase, so that the theyight update becomes local in time. they prove theoretically that, provided the learning rates are sufficiently small, at each time step of the second phase the dynamics of neurons and synapses follow the gradients of the loss given by BPTT (Theorem 1).
they demonstrate training with C-EP on MNIST and generalize C-EP to neural networks where neurons are connected by asymmetric connections. they show through experiments that the more the network updates follows the gradients of BPTT, the best it performs in terms of training. These results bring EP a step closer to biology while maintaining its intimate link with backpropagation.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. This context is provided in the form of a support set of K pairs of pose/frame images for the video. The paper is also clearly written and easy to follow. For these reasons, I m personally leaning towards recommending to accept this submission.<BRK>In this paper, authors propose to address few shot video retargeting, where one should adapt a generic generative model of human actions to a specific person given a few samples of their appearance. However, the solution is not quite novel for me.<BRK>This paper proposes a novel and interesting task that learn to retarget human actions with few shot samples. Additionally, could the authors provide some visualizing results for different K number, which would be interesting for analysis. Though the proposed problem is novel and somewhat interesting, there are also several weaknesses of this work:  The novelty of methodology is somewhat limited.<BRK>they address the task of unsupervised retargeting of human actions from one video to another. they consider the challenging setting where only a few frames of the target is available. The core of their approach is a conditional generative model that can transcode input skeletal poses (automatically extracted with an off-the-shelf pose estimator) to output target frames. Hotheyver, it is challenging to build a universal transcoder because humans can appear wildly different due to clothing and background scene geometry. Instead, they learn to adapt – or personalize – a universal generator to the particular human and background in the target. To do so, they make use of meta-learning to discover effective strategies for on-the-fly personalization. One significant benefit of meta-learning is that the personalized transcoder naturally enforces temporal coherence across its generated frames; all frames contain consistent clothing and background geometry of the target. they experiment on in-the-wild internet videos and images and show their approach improves over widely-used baselines for the task.

Reject. rating score: 3. rating score: 3. rating score: 6. The paper presents a method of scaling up towards action spaces, that exhibit natural hierarchies (such as a controllable resolution of actions), throughout joint training of Q functions over these. Authors notice, and exploit a few interesting properties, such as inequalities that emerge when action spaces form strict subsets that lead to nice parametrisation of policies in a differential way. Ablations provided imply that this part is indeed crucial (as with independent Qs, called "Sep Q" learning flat lines). Could this ablation be also done on the toy ish tasks from experiment 1? How were the missing parent actions handled?<BRK>Based on the intuition that smaller action space should be easier to learn, the author proposes a curriculum learning approach which learns by gradually growing the action space. An agent using simpler action space can generate experiences to be used by all the agents with larger action spaces. The author presents experiments on simple domains and also on a challenging video game. In general, it is an interesting research work. I think the author can improve the paper in the following aspect. I think the author should include some discussions regarding large action spaces, since one goal of the proposed method is to handle such situation.<BRK>Summary: This paper proposes a method to progressively explore the action space for RL. The proposed method is called “growing action spaces”. This method effectively captures many RL settings, including multi agent learning. One effective approach is to apply the action hierarchy. Then the paper performs experiments on both simple toy examples and a more complicated one, the Starcraft game. The simple toy problems is “Acrobot” and “Mountain Car” with discretized spaces. The proposed method is novel and effective. • Using 3 level of hierarchy in the StarCraft game does not work well. • The training curriculum is still mysterious. Maybe training level by level is better? So you will have l different policies.<BRK>In complex tasks, such as those with large combinatorial action spaces, random exploration may be too inefficient to achieve meaningful learning progress. In this work, they use a curriculum of progressively growing action spaces to accelerate learning. they assume the environment is out of their control, but that the agent may set an internal curriculum by initially restricting its action space. their approach uses off-policy reinforcement learning to estimate optimal value functions for multiple action spaces simultaneously and efficiently transfers data,  value estimates, and state representations from restricted action spaces to the full task. they show the efficacy of their approach in proof-of-concept control tasks and on challenging large-scale StarCraft micromanagement tasks with large, multi-agent action spaces.
Reject. rating score: 3. rating score: 3. rating score: 6. Overall the paper is easy to read and I welcome that. I like the idea of using node level embedding instead of pairwise weights to learn a low rank weight representation. However, I am more skeptical about using this in a recurrent architecture and claiming that this is structure learning. The empirical results do not provide sufficient evidence that this performs structure learning. Can I use the proposed FNN with a similar number of parameters to learn the corresponding architecture for A and B respectively, without the need to figure out which is which? It would be interesting to try baselines that have roughly the same number of parameters as the proposed FNN. How were they chosen? Assuming that the same amount of computational resource is spent on searching through baseline architectures as well, could the results have been different from those in Table 1? There are interesting ideas in this work but in its present form I cannot yet recommend acceptance.<BRK>This paper proposes a new architecture based on attention model to replace the fully connected layers. In this architecture, each neuron is associated with an embedding vector, based on which the attention scores (between two consecutive layers) are calculated, and the computational flow through the layers are derived based on these attention scores. The experiments on MNIST and CIFAR demonstrate some degree of superiority over plan FC layers. 2.I donot believe FC is essential in modern computer vision (CV) tasks, so the better performance over a plain FFN on CV tasks are not that convincing (especially the two datasets are typically regarded as debugging dataset nowadays). I suggest the authors conduct more experiments on Transformer based tasks (e.g., machine translation), since in Transformer, the FFN is quite important. If the replace of FFN using the proposed FNN is successful for Transformer on some large scale task (e.g., WMT14 En De Translation), this work will be much stronger in terms of empirical performance. What is the embedding size d in the experiments?<BRK>This paper introduces a new neural network architecture, in which all neurons (called "floating neurons") are essentially endowed with "input" and "output" embedding vectors, the product of which defines the weight of the connection between any two neurons. In my opinion, this improved the paper and made some of its claims much better justified. Updated: The authors updated the text and addressed many of my questions. In Section 4.1, the authors compare FNN and DNN on MNIST and CIFAR10 datasets. and possibly even lower. I believe that these questions require additional discussion and empirical evidence. Just as an example, if it was possible to sample (potentially randomly) different DNN architectures (with a reasonable parameter prior) and compare them with FNNs on a 2D accuracy parameters plot (or using other important metrics), it would provide much more information to the reader. How important are these individual aspects? If I am not mistaken, FNN can also be "unrolled" and represented as a multi layer floating neural network with additional parameter sharing. This would imply that FNNs do not necessarily supersede multi layer floating neural networks, at least when the computational complexity is of importance. 4.There are a few minor misprints throughout the text. But additional empirical results for these architectures would, in my opinion, be much more convincing.<BRK>Designing the architecture of deep neural networks (DNNs) requires human expertise and is a cumbersome task. One approach to automatize this task has been considering DNN architecture parameters such as the number of layers, the number of neurons per layer, or the activation function of each layer as  hyper-parameters, and using an external method for optimizing it. Here they propose a novel neural network model, called Farfalle Neural Network, in which important architecture features such as the number of neurons in each layer and the wiring among the neurons are automatically learned during the training process. they show that the proposed model can replace a stack of dense layers, which is used as a part of many DNN architectures. It can achieve higher accuracy using significantly fetheyr parameters.  
Reject. rating score: 3. rating score: 3. rating score: 3. The method proposes a method for continual learning. This method aims to find gradient updates which are perpendicular to the input vectors of previous tasks (resulting in less forgetting). However, the authors argue, that the learning of new tasks is happening in the solution space of the previous tasks, which might severely limit the ability to adapt to new tasks. The authors propose a ‘principal component’ based solution to this problem. The paper is not well positioned in related works. At least a comparison with the more related works PackNet and HAT should be included. For more recent method for task aware CL see also ‘Continual learning: A comparative study on how to defy forgetting in classification tasks’. And the proposed PCP solves this problem. Such an analysis of P_l^k as a function of the tasks (and for several layers) would be interesting to see, for example for EMNIST 47(10 tasks).<BRK>This paper introduces Principal Components Projection, a method that computes the principal components of input vectors, using them to train on a transformed input space and to project gradient updates. Experiments show improved results over OWM (the method that this paper builds on) and EWC. If I understand correctly (which I think may not be the case), the principal component vectors are computed after the first forward/backward pass of each task, for the inputs to each layer (C_l^k). I also do not understand Equation 1. What is \grad{W}? The experiments seem reasonable, except that there are no standard deviations on the results. However, as far as I m aware, these experimental protocols (dataset and model size) are not used in other papers: it would be nice to see experiments which match previous papers  protocols, for example with MNIST and CIFAR 10 at least (other papers use smaller model sizes). Hopefully the authors can answer my questions.<BRK>This paper tries to solve the catastrophic forgetting issue in the continual learning problem. The authors propose a method based on principal components projection to tackle this issue. The authors conduct experiments on image classification tasks to show the performance of the proposed method and compare it with two other baselines EWC and OWM. This paper tries to solve an important problem. It is not convincing. 2.It might strengthen the paper if the authors can show the comparison results on more other datasets, e.g., other image classification tasks. It would be better if the authors can show the proposed method can generalize to other tasks. 4.It is not clear why the proposed method can solve the issue that OWM faces with (bad accuracy when tasks are not quite related).<BRK>Continual learning in neural networks (NN) often suffers from catastrophic forgetting. That is, when learning a sequence of tasks on an NN, the learning of a new task will cause theyight changes that may destroy the learned knowledge embedded in the theyights for previous tasks. Without solving this problem, it is difficult to use an NN to perform continual or lifelong learning. Although researchers have attempted to solve the problem in many ways, it remains to be challenging. In this paper, they propose a new approach, called principal components projection (PCP). The idea is that in learning a new task, if they can ensure that the gradient updates will only occur in the orthogonal directions to the input vectors of the previous tasks, then the theyight updates for learning the new task will not affect the previous tasks. they propose to compute the principal components of the input vectors and use them to transform the input and to project the gradient updates for learning each new task. PCP does not need to store any sampled data from previous tasks or to generate pseudo data of previous tasks and use them to help learn a new task. Empirical evaluation shows that the proposed method PCP markedly outperforms the state-of-the-art baseline methods.
Reject. rating score: 1. rating score: 3. rating score: 6. This paper provides generalization bounds for permutation invariant neural networks where the learning problem is invariant to the permutation of input data. Unfortunately, the technical value of the content and its novelty is very limited since the proof reduces to a very basic argument that counts invariances (which is simply n!where n is the number of invariant dimensions) and uses a standard approach to give a generalization bound. However, I do not understand how this work can improve our understanding of permutation invariant networks.<BRK>This paper presents a derivation of a generalization bound for neural networks designed specifically to deal with permutation invariant data (such as point clouds). Unlike Example 1, Example 2 (p.3) is not helpful in motivating the permutation invariant neural networks. I also found the proof of proposition 4 too confusing to easily follow. The heart of the contribution is that the bound includes a  1/n!<BRK>This paper derives a generalization bound for permutation invariant networks. In the context of this prior work, the contribution of this paper appears incremental. Overall, this paper is technically rigorous, and novel in its very specific context of deriving the generalization bounds for permutation invariant networks.<BRK>they theoretically prove that a permutation invariant property of deep neural networks largely improves its generalization performance. Learning problems with data that are invariant to permutations are frequently observed in various applications, for example, point cloud data and graph neural networks. Numerous methodologies have been developed and they achieve great performances, hotheyver, understanding a mechanism of the performance is still a developing problem. In this paper, they derive a theoretical generalization bound for invariant deep neural networks with a ReLU activation to clarify their mechanism. Consequently, their bound shows that the main term of their generalization gap is improved by $\sqrt{n!}$ where $n$ is a number of permuting coordinates of data. Moreover, they prove that an approximation potheyr of invariant deep neural networks can achieve an optimal rate, though the networks are restricted to be invariant. To achieve the results, they develop several new proof techniques such as correspondence with a fundamental domain and a scale-sensitive metric entropy.
Reject. rating score: 3. rating score: 3. rating score: 3. This paper is reporting an unsupervised approach to learn node embeddings and communities simultaneously by minimizing the mincut loss function. The matrix P and adjacency matrix are coupled to generate community adjacency matrix to minimize the cutting. Similar work such as the methods of transferring matrix E to H then to P have already been published , which reduce the novelty of this study. 2.Though there is a schematic, the learning embedding process is still not described clearly.<BRK>The idea is simple and easy understood and the paper is well written. However, major concerns are:1. The authors should check the correlation between the detected communities and the original paper labels. For example, if considering the downstream node classification of node embedding as an evaluation task, then how about the performance of the following two step method. Spectral clustering has a scalability issue when meeting big graphs. Since the spectral process is also applied in the proposed method, efficiency and scalability evaluations are encouraged to provide, especially for big graphs which are not covered in the selected datasets in this paper. 5.In Sec 5.3 and Fig 2, it s mentioned that trends of the three datasets are different.<BRK>This work proposes a neural netowrk approach to minimize mincutloss, thus achieving embedding nodes and find communities at the same time. However, it is difficult for me to understand the paper and I feel that it is not clearly written. 1.The algorithm is not written in a box as in Algorithm 1. At first I thought algorithm 1 is the main method, but only after reading it I realized that it is one step of the algorithm. I would appreciate it if the complete algorithm (including input, output, parameters) can be summerized clearly. I thought the input of all methods are the adjencency matrix A. 3.In table 2 and table 4, why does the paper compare different methods with different measures? Is it possible to compare all methods using all measures?<BRK>they present Deep MinCut (DMC), an unsupervised approach to learn node embeddings for graph-structured data. It derives node representations based on their membership in communities. As such, the embeddings directly provide interesting insights into the graph structure, so that the separate node clustering step of existing methods is no longer needed. DMC learns both, node embeddings and communities, simultaneously by minimizing the mincut loss, which captures the number of connections bettheyen communities. Striving for high scalability, they also propose a training process for DMC based on minibatches. they provide empirical evidence that the communities learned by DMC are meaningful and that the node embeddings are competitive in different node classification benchmarks.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. The major contribution of this paper is the use of random Fourier features as temporal (positional) encoding for dynamic graphs. The reader finds that the proposed approach is interesting.<BRK>This paper proposed the temporal graph attention layer which aggregates in hop features with self attention and incorporates temporal information with Fourier based relative positional encoding. This idea is novel in GCN field. Experimental results demonstrate that the TGAT which adds temporal encoding outperforms the other methods. The baselines compared in this paper seems to be too weak. How does the single head variant of TGAT work?<BRK>What kind of temporal trends exist in the data that this method has learned? One particular recurring frustration is the use of the term “architect” which seems wrong. I’m not sure if this is a major advance. The paper is rather hard to follow and ambiguous.<BRK>Inductive representation learning on temporal graphs is an important step toward salable machine learning on real-world dynamic networks. The evolving nature of temporal dynamic graphs requires handling new nodes as theyll as capturing temporal patterns. The node embeddings, which are now functions of time, should represent both the static node features and the evolving topological structures. Moreover, node and topological features can be temporal as theyll, whose patterns the node embeddings should also capture. they propose the temporal graph attention (TGAT) layer to efficiently aggregate temporal-topological neighborhood features to learn the time-feature interactions. For TGAT, they use the self-attention mechanism as building block and develop a novel functional time encoding technique based on the classical Bochner's theorem from harmonic analysis. By stacking TGAT layers, the network recognizes the node embeddings as functions of time and is able to inductively infer embeddings for both new and observed nodes as the graph evolves. The proposed approach handles both node classification and link prediction task, and can be naturally extended to include the temporal edge features. they evaluate their method with transductive and inductive tasks under temporal settings with two benchmark and one industrial dataset. their TGAT model compares favorably to state-of-the-art baselines as theyll as the previous temporal graph embedding approaches.
Reject. rating score: 3. rating score: 3. rating score: 6. This paper seeks to understand both across layer and single layer behavior within neural networks (i.e.layer behavior along the depth of a network, and behavior of a single layer along training epochs). Theoretically, they show that the Wasserstein distance between predicted and target distributions is decreasing along the depth and for a single layer, along training iterations. This paper gives an interesting contribution to the in depth analysis of neural networks. However, some elements remain unclear:1. 3.There is no detail on the regularization strength of the Wasserstein distance, or what p (in definition 1) is chosen either in the experiments or in the theorems. Post rebuttal: I thank the authors for their response. On this basis, I am maintaining my weak reject rating.<BRK>The authors intuitively, and then analytically, explain the behavior in the hidden layers of deep convolutional networks and show how the behavior can be used to improve performance by "early exiting." I give this paper a weak reject. I also applaud the authors for their rigorous explanation of the hyper parameters and experimentation methods. The increase in accuracy isn t large enough across experiments to allay my concerns. I can be convinced otherwise with a compelling set of arguments.<BRK>This paper presents a method to compute the distance of distribution of two layers in neural networks by using the label distribution mapping (e.g., Frogner et al., 2015). I believe that the contributions of this paper are week in analyzing individual layers across layer since there are many extensive studies are conducted on information bottleneck methods with mutual information. However, authors of this paper presents a way to utilize the label distribution mapping to compare the distance of individual layers when an input image come as shown in Figure 5 which I believe the main contribution of this paper. Even in Section 3, the label distribution mapping is not clearly explained except for the description of FC+softmax.<BRK>Deep neural networks (DNNs) have achieved unprecedented practical success in many applications.
Hotheyver, how to interpret DNNs is still an open problem.
In particular, what do hidden layers behave is not clearly understood. 
In this paper, relying on a teacher-student paradigm, they seek to understand the layer behaviors of DNNs by ``monitoring" both across-layer and single-layer distribution evolution to some target distribution in the training. Here, the ``across-layer" and ``single-layer" considers the layer behavior \emph{along the depth} and  a specific layer \emph{along training epochs}, respectively. 
Relying on optimal transport theory, they employ the Wasserstein distance ($W$-distance)  to measure the divergence bettheyen the layer distribution and the target distribution. 
Theoretically, they prove that i) the $W$-distance of across layers to the target distribution tends to decrease along the depth. ii) the $W$-distance of a specific layer to the target distribution tends to decrease along training iterations. iii) 
Hotheyver, a deep layer is not always better than a shallow layer for some samples. Moreover, their results helps to analyze the stability of layer distributions and explains why auxiliary losses helps the training of DNNs. Extensive experiments on real-world datasets justify their theoretical findings.
Reject. rating score: 3. rating score: 3. rating score: 8. In this paper, an inference WGAN model (iWGAN) is proposed that fuses autoencoders and WGANs. Due to the above concerns, rating is recommended as "3 Weak Reject." (2)	The proposed iWAN is only compared with WGAN. Especially, on the synthetic dataset, iWGAN seems to  have less mode collapse than WGAN.<BRK>This paper proposes a novel variation to the WGAN, which combines WGANs with autoencoders.<BRK>This paper presents an inference WGAN (iWGAN) which fully considers to reduce the difference between distributions of G(X) and Z, G(Z) and X. This algorithm has a stable and efficient training process. Therefore, the innovation of this paper is very novel.<BRK>Generative Adversarial Networks (GANs) have been impactful on many problems and applications but suffer from unstable training. Wasserstein GAN (WGAN) leverages the Wasserstein distance to avoid the caveats in the minmax two-player training of GANs but has other defects such as mode collapse and lack of metric to detect the convergence. they introduce a novel inference WGAN (iWGAN) model, which is a principled framework to fuse auto-encoders and WGANs. The iWGAN jointly learns an encoder network and a generative network using an iterative primal dual optimization process. they establish the generalization error bound of iWGANs. they further provide a rigorous probabilistic interpretation of their model under the framework of maximum likelihood estimation. The iWGAN, with a clear stopping criteria, has many advantages over other autoencoder GANs. The empirical experiments show that their model greatly mitigates the symptom of mode collapse, speeds up the convergence, and is able to provide a measurement of quality check for each individual sample. they illustrate the ability of iWGANs by obtaining a competitive and stable performance with state-of-the-art for benchmark datasets.
Reject. rating score: 1. rating score: 1. rating score: 3. rating score: 6. The paper presents three contributions: (a) the observation that there’s train to test leakage in many graph classification datasets (under isomorphism equivalence), (b) what appears to be a theoretically motivated way of improving scores on such datasets, by focusing on solving the examples that are isomorphic with training instances, and (c) a recommendation to remove such leakage from test sets. I don’t think the paper meets the ICLR bar. While (a) is very interesting, and an important contribution, (b) and (c) are contradictory. Missing reference: Bordes et al.(2013) and Toutanova et al.(2015) show there’s train to test leakage (under isomorphism equivalence) in the FB15K dataset.<BRK>However, the main assumption of the paper   which equates the quality of a graph learning benchmark with the amount of isomorphic graphs that it contains, i.e., the lower the better   seems questionable. The paper argues that isomorphic graphs are akin to duplicate images in computer vision and should be removed from a dataset. In the latter, a learning method is required to identify the correct bijection form V_1 to V_2 which is a non trivial task. However, the attributes are not considered in the analysis what leads to a large number of isomorphic graphs. The results of Theorem 6.1 on the other hand seems straightforward and would hold for any classification task for which the true label for an equivalence class of instances is known.<BRK>This work probes graph classification datasets for isomorphism bias. They also provide some recommendations for measuring the  right metrics  and release clean versions of the considered datasets. Also, as the authors themselves mention, taking node/edge labels decrease the isomorphism in most datasets. The results and recommendations presented in the paper are intuitive and somewhat trivial  I am not sure if ICLR is the right venue for this work<BRK>The authors discuss here the problem of isomorphism bias in graph dataset, i.e.the overfitting effect in learning networks whenever graph isomorphism features are incorporated within the model. However, the novelty of the work is limited, and also the proposed solutions cannot be claimed as superior to other approaches, due to the small improvement in accuracy.<BRK>In recent years there has been a rapid increase in classification methods on graph structured data. Both in graph kernels and graph neural networks, one of the implicit assumptions of successful state-of-the-art models was that incorporating graph isomorphism features into the architecture leads to better empirical performance. Hotheyver, as they discover in this work, commonly used data sets for graph classification have repeating instances which cause the problem of isomorphism bias, i.e. artificially increasing the accuracy of the models by memorizing target information from the training set. This prevents fair competition of the algorithms and raises a question of the validity of the obtained results. they analyze 54 data sets, previously extensively used for graph-related tasks, on the existence of isomorphism bias, give a set of recommendations to machine learning practitioners to properly set up their models, and open stheirce new data sets for the future experiments. 
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. This paper argues that defenses against adversarial attacks need to be stronger than they currently are. After rebuttal:I have raised my score after authors improved to quality of the text. Even though this work does not have empirical results on high dimensional datasets such as MNIST or CIFAR10, it has a nice theoretical contribution useful for finding stronger defenses against adversarial examples. They show how a prior defense based on generative models (INC) fails on the toy problems and show how a modification to INC can improve it.<BRK>Overall, I still would like to see a more detailed/in depth experimental setup, but I realise that this not directly possible within the timeframe allotted during the rebuttal period. I.Summary of the PaperThis paper studies robustness to adversarial examples from theperspective of having  topology aware  generative models. Specifically, I see the followingissues:  Missing clarity: while the appendix is very comprehensive, which  I appreciate, the main text could be improved; some statements appear  redundant, while others need to be re formulated to build intuition  This paper appears to span both theory and applications. Moreover,  topology  is reduced to  connected components   in this paper. I would rather say that the main goals are to  provide empirical evidence for the *relevance* or *applicability* of  the Theorem. Ivery much commend the authors of the paper for choosing this sort ofwriting style! The manifold assumption is that data lie _on_ a manifold or _close to_  a manifold whose intrinsic dimension is much lower than that of the  ambient space.<BRK>The paper tries to answer the following question:In adversarial defense training do manifold based defenses need to know the structure of the underlying data manifold? The question is quite rhetoric (the answer is most probably yes), nevertheless, the paper provides a theoretical and empirical answer. Nevertheless, I have a very basic question regarding the usefulness of the methods that the paper studies and the topic of adversarial defenses. There are some recent papers that demonstrate this [A] or recently feature denoising [B].<BRK>ML algorithms or models, especially deep neural networks (DNNs), have shown significant promise in several areas. Hotheyver, recently researchers have demonstrated that ML algorithms, especially DNNs, are vulnerable to adversarial examples (slightly perturbed samples that cause mis-classification). Existence of adversarial examples has hindered deployment of ML algorithms in safety-critical sectors, such as security. Several defenses for adversarial examples exist in the literature. One of the important classes of defenses are manifold-based defenses, where a sample is "pulled back" into the data manifold before classifying. These defenses rely on the manifold assumption (data lie in a manifold of lotheyr dimension than the input space). These defenses use a generative model to approximate the input distribution. This paper asks the following question: do the generative models used in manifold-based defenses need to be topology-aware? their paper suggests the anstheyr is yes. they provide theoretical and empirical evidence to support their claim.
Reject. rating score: 3. rating score: 3. rating score: 8. There is still a large concern wrt to the computational complexity. I kind of understand the argument about the hyperparameter tuning, but it seems not really fair to compare to a bruteforce parameter search (one can probably do some sort of bayesian optimization). Overview:Authors introduce a new VAE based method for learning disentangled representations. The main idea is to apply a “residual learning mechanism”, which resembles an autoregressive model, but here conditioning between sequential steps is done in both latent and input spaces.<BRK>The authors applied residual learning machenism in VAE learning, which I have seen such methods in Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks (https://arxiv.org/abs/1506.05751), which basically applied the residual learning method in GAN. Also, the best paper from ICML 2019 claimed that unsupervised learning method can not really disentangle the features. Otherwise, it is not convincing to the readers.<BRK>The authors of this paper present a novel that for unsupervised disentangled representation learning. The model, named sequential residual VAE (SR VAE), gradually activates individual latent variables to reconstruct residuals. Since the training involves a sequence of model training,  SR VAE certainly consumes more time than other VAEs.<BRK>Recent advancements in unsupervised disentangled representation learning focus on extending the variational autoencoder (VAE) with an augmented objective function to balance the trade-off bettheyen disentanglement and reconstruction. they propose Sequential Residual Variational Autoencoder (SR-VAE) that defines a "Residual learning" mechanism as the training regime instead of the augmented objective function. their proposed solution deploys two important ideas in a single framework: (1) learning from the residual bettheyen the input data and the accumulated reconstruction of sequentially added latent variables; (2) decomposing the reconstruction into decoder output and a residual term. This formulation enctheirages the disentanglement in the latent space by inducing explicit dependency structure, and reduces the bottleneck of VAE by adding the residual term to facilitate reconstruction. More importantly, SR-VAE eliminates the hyperparameter tuning, a crucial step for the prior state-of-the-art  performance using the objective function augmentation approach.  they demonstrate both qualitatively and quantitatively that SR-VAE improves the state-of-the-art  unsupervised disentangled representation learning on a variety of complex datasets.
Reject. rating score: 3. rating score: 6. rating score: 6. Cons:In my opinion, the paper scores low on novelty and original contribution. The experiments are done well.<BRK>The paper is well written. At the same time, the properties of the proposed method are not well exposed and the experimental evaluation is incomplete.<BRK>what are the computational requirements of the models presented in this paper? how does the model perform on longer sequences, e.g.for long term generation?<BRK>they propose an approach for sequence modeling based on autoregressive normalizing flows. Each autoregressive transform, acting across time, serves as a moving reference frame for modeling higher-level dynamics. This technique provides a simple, general-purpose method for improving sequence modeling, with connections to existing and classical techniques. they demonstrate the proposed approach both with standalone models, as theyll as a part of larger sequential latent variable models. Results are presented on three benchmark video datasets, where flow-based dynamics improve log-likelihood performance over baseline models.
Reject. rating score: 3. rating score: 8. rating score: 8. This paper compares a few encoder agnostic methods for using pretrained decoders in text generation tax. The author compared a few intuitive ways of doing this, and presents results showing that that pseudo self attention does the best. However, I think the results has some strange points that needs further investigation. Going from repr transfomer to context attention to pseudo self, there is an increasing amount of parameters initialized by pretraining. It would be good to verify that this is not due to under training. Except paragraph captioning, the results on other tasks are not better than prior results, which do not use pretraining. The baseline transformer is also usually worse than prior results. The human evaluation shows that the proposed method do better on story generation, but this one is essentially text to text. There is no comparison with previous text 2 text methods that use pretraining. If the proposed methods are truely encoder agnostic, then they should perform reasonably on text to text as well.<BRK>This paper proposes a simple yet effective method to adapt large scale pre trained language models, which have been shown to substantially improve performance on broadly classification based NLU tasks, to NLG. The approach is explored in the encoder agnostic {X} to text setup, where the source encoding {X} could represent arbitrary modalities, such as text or images. More concretely, the paper leverages a pre trained, large scale language model (in this case a GPT 2), and examines how to best cast such unconditional language model into a decoder that generates text conditional on the source information {X}. Cons:1.It is still unclear how using language model pre training affects adequacy (as opposed to fluency). Since none of the evaluation metric specifically assesses for adequacy on its own, it would be good to isolate the effect of pseudo self attention on adequacy, and compare it with the baselines, in addition to a Transformer trained from scratch on each downstream task.<BRK>This paper proposes a new architecture to train decoder models on language generation using a pre trained encoder (such as BERT or GPT 2). They introduce a novel block called `````"pseudo self attention" that allow injecting the input for conditional generation in the self attention layer (i.e.softmax of YW_q (XU_k | YW_k)^T (XU_v | YW_v) instead of softmax(YW_q(YW_k)^T)YW_v). They extensively evaluate their approach on a large set of tasks showing improvements across all of them (which includes class conditional generation, summarization, story generation and paragraph generation). The idea seems well motivated and the paper is well written and easy to follow. The experimental section is very thorough and show large improvements on a variety of task I particularly appreciate that they experimented with conditional inputs of different nature (class value, image, different languages etc...) to show the effectiveness of their method. Overall, while the idea is quite simple, the experiments speak for themselves and this could prove to be a useful `layer  to use on large pre trained language models.<BRK>Large pretrained language models have changed the way researchers approach discriminative natural language understanding tasks, leading to the dominance of approaches that adapt a pretrained model for arbitrary downstream tasks. Hotheyver, it is an open question how to use similar techniques for language generation. Early results in the encoder-agnostic setting have been mostly negative. In this work, they explore methods for adapting a pretrained language model to arbitrary conditional input. they observe that pretrained transformer models are sensitive to large parameter changes during tuning. Therefore, they propose an adaptation that directly injects arbitrary conditioning into self attention, an approach they call pseudo self attention. Through experiments on ftheir diverse conditional text generation tasks, they show that this encoder-agnostic technique outperforms strong baselines, produces coherent generations, and is data-efficient.
Reject. rating score: 3. rating score: 3. rating score: 8. This paper proposes a method to learn bilingual word embeddings by modifying the Sent2Vec (which is based on word2vec) approach, and applying it to bitext. They evaluate on monolingual and bilingual word similarity, bitext mining, and a zero shot document classification task where a classifier is trained on data in one language but evaluated on another (using their embeddings as features in both cases). The baselines are weak and the task is not standard. 2) For the zero shot document classification task, it should also be pointed out that LASER can handle many languages all at once. 3) Note that also the improvements on monolingual similarity using parallel data are well known. I think the main contributions of this paper are modifying Sent2vec so it can be used on bilingual data and using it to learn nice representations for words and sentences (and documents).<BRK>The paper does not bring anything novel to the field of cross lingual representation learning: it just revisits some older ideas (from the period of 2013 2015), now revamped, given the fact that more sophisticated and more effective methods are used to model exactly the same intuitions. Another note related to evaluation: to really establish how different cross lingual embeddings compare to each other, a wider set of experiments and downstream evaluation is definitely required, see the work of e.g., Glavas et al.(ACL 2019). The results are actually quite mixed, and the advantage of Bi Sent2Vec is its quicker training. However, what about more recent methods such as XLM which rely on exactly the same resources as Bi Sent2Vec to do the zero shot classification task? Figure 2: corpus size. Based on the results presented, it seems that the performance saturates by adding more parallel data, but the authors fail to fully understand their evaluation data in the first place.<BRK>The authors performed an in depth ablation study, to support their claims that the proposed model addresses some of the key problems with other existing approaches to cross lingual representation learning (e.g.hubness).* Comments on the paperThis paper is exceptionally well written, organized, and clear. Meanwhile, I believe the paper could benefit from more discussion or analysis of cases where the proposed model did not lead to improvements, in particular with Russian in the word translation retrieval experiment, where TRANSGRAM outperforms the proposed model. The proposed model becomes less convincing when I consider that it might only work for other agglutinative, English like languages, and I wonder how this approach would fair with other morphologically rich languages similar to Russian, and non agglutinative languages in general. How do I interpret “10^ 2” as a corpus size? In other words, what are “10^ 2 amounts of data.” Fix this.<BRK>Recent advances in cross-lingual word embeddings have primarily relied on mapping-based methods, which project pretrained word embeddings from different languages into a shared space through a linear transformation. Hotheyver, these approaches assume word embedding spaces are isomorphic bettheyen different languages, which has been shown not to  hold in practice (Søgaard et al., 2018), and fundamentally limits their performance. This motivates investigating joint learning methods which can overcome this impediment, by simultaneously learning embeddings across languages via a cross-lingual term in the training objective. Given the abundance of parallel data available (Tiedemann, 2012), they propose a bilingual extension of the CBOW method which leverages sentence-aligned corpora to obtain robust cross-lingual word and sentence representations. their approach significantly improves cross-lingual sentence retrieval performance over all other approaches, as theyll as convincingly outscores mapping methods while maintaining parity with jointly trained methods on word-translation. It also achieves parity with a deep RNN method on a zero-shot cross-lingual document classification task, requiring far fetheyr computational restheirces for training and inference. As an additional advantage, their bilingual method also improves the quality of monolingual word vectors despite training on much smaller datasets.  they make their code and models publicly available.

Reject. rating score: 3. rating score: 3. rating score: 6. The paper considers the problem of extracting the underlying dynamics of objects in video frames. The paper focuses on two major applications: background/foreground extraction and video classification.<BRK>Could Table 1 be placed in the experiments section rather than in the middle of page 5? Our goal is to establish a basis invariant to the video dynamics that can then be used, for example, to partition the video into parts with differing dynamics   e.g.foreground/background. My main concern about the paper is that I find it very difficult to appreciate the efficacy of the method given the current presentation of the results. The Koopman operator acts on a differential system to identify a function space invariant to the dynamics.<BRK>In the proposed method, DMD is used in the latent representations of a convolutional autoencoder for image sequences. Some complaints:    the tables are a bit sloppy and should be formatted to fit in the document with normal sized fonts,    the images are too small to see well. The resulting methods are applied to a foreground extraction and a classification tasks, and compared with numerous baselines.<BRK>Extracting underlying dynamics of objects in image sequences is one of the challenging problems in computer vision. On the other hand, dynamic mode decomposition (DMD) has recently attracted attention as a way of obtaining modal representations of nonlinear dynamics from (general multivariate time-series) data without explicit prior knowledge about the dynamics. In this paper, they propose a convolutional autoencoder based DMD (CAE-DMD) that is an extended DMD (EDMD) approach, to extract underlying dynamics in videos. To this end, they develop a modified CAE model by incorporating DMD on the encoder, which gives a more meaningful compressed representation of input image sequences. On the reconstruction side, a decoder is used to minimize the reconstruction error after applying the DMD, which in result gives an accurate reconstruction of inputs. they empirically investigated the performance of CAE-DMD in two applications: background/foreground extraction and video classification, on publicly available datasets.
Reject. rating score: 3. rating score: 3. rating score: 3. This paper proposes to generate "semantic adversarial examples" (SAEs) by applying small perturbations in the semantic space, by contrast to more standard adversarial examples that apply perturbations to the image intensities. I found the paper interesting, however I am not convinced by the general concept:* While the method from Yao et al is supposed to work on real images, it requires to detect and estimate the pose and the shape of 3D objects, which is still challenging to do on real images. * The generated images are also valid images, in the sense that for a slightly different scene, they would have been generated by the image renderer used for VKITTI. It sounds more like what the authors have is a method that can generate scenes to improve the performance of an object detector when trained on synthetic images, rather than a method  to be more robust to malicious attacks. This is very interesting, however the paper is not branded toward this  application.<BRK>Review: This paper focuses on generating adversarial perturbation in semantic space. The main contribution of this paper is to propose a general method to generate semantic adversarial examples by using advances in differential rendering and inverse graphics. Pros:1.Generally, the presentation is clear and easy to follow. 2.A general way to transform any pixel attack algorithm to its “semantic version” is novel. I think this paper would be more compelling if proposed SAEs have some special property (e.g, easy to take effect in the real world or stronger robustness against defense strategy). 2.In section 5.3, the authors show that data augmentation using SAEs increase the robustness to SAEs, but pixel perturbation AEs do not.<BRK>Experiments demonstrated that the generated semantic adversarial examples (SAEs) can attack the SqueezeDet (see Table 1 and Table 2). By re training with augmented data by the proposed method, the robustness of SqueezeDet (see Table 3) can be further improved. Overall, this is an okay paper with incremental technical novelty and clear presentation. In general, studying the adversarial examples in the synthetic domain seems not a significant contribution. At least, reviewer would like to know whether re training on adversarial examples help to improve the performance on real dataset? (2) The conclusion is largely based on the 1547 semantic adversarial examples generated, while there are more than 4K synthetic images in the dataset. Please comment on the average running time for generating a semantic adversarial example. How does that compare to generating a pixel based adversarial example? It would be more convincing if this paper provides more such examples in the appendix. (4) SqueezeDet is the only model used in the paper.<BRK>Machine learning (ML) algorithms, especially deep neural networks, have demonstrated success in several domains. Hotheyver, several types of attacks have raised concerns about deploying ML in safety-critical domains, such as autonomous driving and security. An attacker perturbs a data point slightly in the pixel space and causes the ML algorithm to misclassify (e.g. a perturbed stop sign is classified as a yield sign). These perturbed data points are called adversarial examples, and there are numerous algorithms in the literature for constructing adversarial examples and defending against them. In this paper they explore semantic adversarial examples (SAEs) where an attacker creates perturbations in the semantic space. For example, an attacker can change the background of the image to be cloudier to cause misclassification. they present an algorithm for constructing SAEs that uses recent advances in differential rendering and inverse graphics. 
Reject. rating score: 6. rating score: 6. rating score: 6. The authors propose a new gradient based method (FAB) for constructing adversarial perturbations for deep neural networks. I wonder if this is the reason for PGD performing worse than FAB for large epsilon values on CIFAR10. The adversarially trained MNIST model of Madry et al.2018 learns to use thresholding filters as the first layer (observed in the original paper). While it is encouraging that FAB is robust to such gradient obfuscation, this is arguably not the ideal setting to compare gradient based methods (especially when averaging performance over models). For MNIST and Restricted IN, PGD performs comparably or even better than FAB (modulo larger epsilon values for which the large step size used could be an issue for PGD and the Linf trained model with the thresholding filters). Moreover, the runtime comparison performed in not exactly fair:  It is not clear how many restarts where included in the runtime of PGD. Its runtime should be in the same ballpark as FAB but the time reported is ~20x higher. Choosing an arbitrary number of steps for each method is not very enlightening. It is not necessary to run PGD 5 times to evaluate the robust accuracy at 5 thresholds. One can perform binary search for each input in order to find the smallest epsilon for which a misclassification can be found. This will result in at most 3 (sometimes 2) evaluations per point (instead of 5). Despite these shortcomings of the experimental evaluation, I still believe that the paper has merit. In that sense, it could potentially be a valuable contribution and could be of interest to a subset of the adversarial ML community. UPDATE: I appreciate the response and the additional experiments performed by the authors. The authors have addressed my concerns in their response.<BRK>The paper studies the problem of the white box attack of neural network based classifiers, with an emphasis on the "minimal distortion solution": The new input that changes the labeling output of the network with the minimal distance (l1, l2, l_inf) with respect to a given input. The main intuition of the algorithm is to do a local linear approximation of the network at the current point (which is the Taylor expansion up to the gradient term). The main concern for me about this paper is the comparison to other methods such as PGD. As far as I know, these attackers DO NOT explicitly minimize the distortion, thus it is quite believable that these models do not identify the minimal distortion solution (rather it will more likely to find a solution that lies in the boundary since it would be the easiest way to attack). I would like to see more implementation details of the other algorithms, for example, what is the performance if we add an additional regularizer as the distance of the current attacker to the given input to PGD. (In particular the justification for solving the local linear system instead of doing a gradient descent step). I apologize for the earlier misunderstanding and higher the score accordingly.<BRK>Authors extend deepFool by adding extra steps and constraints to find closer points to the source image as the adversarial image. Deepfool does and adhoc clipping to keep the pixel values in (0,1) but the new proposed method respects the constraints during the steps. Also during the steps they combine projection of last step result and original image to keep it closer to the original image. Moreover, at the end of the optimization they perform extra search steps to get closer to the original image. Rather than considering the original image, they randomly choose an image in the half ballpark of the total delta. According to the results in fig.2 the backward steps has the highest impact in comparison to deepfool. But mixing with original projection always helps a little and random restarts help a little too. Without the backward steps there is almost no gain from mixing the projections. Considering the full results in the appendix, the results are mixed with no obvious advantage in comparison to PGD specially.<BRK>The evaluation of robustness against adversarial manipulations of neural networks-based classifiers is mainly tested with empirical attacks as the methods for the exact computation, even when available, do not scale to large networks. they propose in this paper a new white-box adversarial attack wrt the $l_p$-norms for $p \in \{1,2,\infty\}$ aiming at finding the minimal perturbation necessary to change the class of a given input. It has an intuitive geometric meaning, yields quickly high quality results, minimizes the size of the perturbation (so that it returns the robust accuracy at every threshold with a single run). It performs better or similarly to state-of-the-art attacks which are partially specialized to one $l_p$-norm.
Reject. rating score: 3. rating score: 6. rating score: 8. Contribution:This paper proposes two methods building upon MADDPG to encourage collaboration amongst decentralized MARL agents:   TeamReg: the agents are trained with an additional objectives of predicting other agents  actions, and make their own action predictable to them   CoachReg: the agents use a form of structured dropout, and at training time a central agent (the "coach") predicts the mask dropout mask that should be applied to all of the agents. Review:The paper is well written and easy to follow. It includes the analysis and explanations of the failure modes, which is valuable in my opinion. The two main limitations of the work are the following:   Limited scale of the experiments. It is unclear whether the additional losses proposed in this work would still perform correctly with more agents, since the regularization pressure will increase. The only baseline method presented here is MADDPG, upon which this work is built. Is there a reason why this particular form of dropout was chosen? Finally, it seems to me that the two methods presented here (TeamReg and CoachReg) are not mutually incompatible.<BRK>The main contribution of this paper is that they propose to allow agents to predict the behavior of others and introduce this prediction loss into the RL learning objective. However, the novelty of this algorithm is not strong, given that the similar idea of  predicting the behavior of other agents  can be found in related work with discrete action spaces. However, It seems that this idea is transferrable to discrete action spaces as well, so long as we change the MSE loss between actions to KL loss between policy over states. Could it be the other way around? I doubt the motivation and effectiveness of this approach. If so, this is like you first assume the policy network is somewhat overfitting, then alleviate this issue by letting the coach adjust which part to keep and which part to drop. Intuitively, all agents will finally reach a point where they agree on the same policy mask distribution (which is the one generated by the coach). How does the same policy mask lead to an improvement in cooperation? Empirical or theoretical explanations are strongly needed in the main results, not in the appendix section. Experiments:  All variants of MADDPG in the experiments are weak baselines, assuming that $MADDPG + sharing$ means all agents share the same policy and Q function. However, since this algorithm only works for continuous action space, available relate work to compare with is quite limited. Experiments with more than two agents should be implemented. They seem to show that under these two proposed frameworks, the predictability of agents does not always encourage cooperation, at least for 2 out of 4 game settings mentioned in this section.<BRK>The rigour of the empirical evaluation and its documentation is exemplary. The related work section could be improved by including a section on the closely related work in the area of opponent modelling. A more thorough review in Section 3 would improve the positioning of the paper. In particular I think it would improve the discussion of the results to elaborate further on why the proposed methods do not improve, but also do not detrimentally affect the learning performance of agents in this environment. In Section 6.2 the authors note "MADDPG + policy mask performs similarly or worse than MADDPG  on all but one environment and never outperforms the full CoachReg approach." Again what is it about this specific environment that causes the policy mask ablation to be sufficient, enabling it to match the asymptotic performance and learn quicker than the full CoachReg method proposed? All references to papers both published and available on arxiv should cite the published version (e.g.Jacques et al.2018 was published at ICML). Please revise all references if accepted.<BRK>A central challenge in multi-agent reinforcement learning is the induction of coordination bettheyen agents of a team. In this work, they investigate how to promote inter-agent coordination using policy regularization and discuss two possible avenues respectively based on inter-agent modelling and synchronized sub-policy selection. they test each approach in ftheir challenging continuous control tasks with sparse rewards and compare them against three baselines including MADDPG, a state-of-the-art multi-agent reinforcement learning algorithm. To ensure a fair comparison, they rely on a thorough hyper-parameter selection and training methodology that allows a fixed hyper-parameter search budget for each algorithm and environment. they consequently assess both the hyper-parameter sensitivity, sample-efficiency and asymptotic performance of each learning method. their experiments show that the proposed methods lead to significant improvements on cooperative problems. they further analyse the effects of the proposed regularizations on the behaviors learned by the agents.
Reject. rating score: 1. rating score: 3. rating score: 6. The proposed regularization penalty is only applied to a subset of the recurrent units; the motivation for this is to allow neurons not contained in the subset to learn different structures. Major commentsI have a number of serious concerns about the paper s motivation, logic, and experiments:  First, the paper motivates the proposed regularization as a way to encourage the network to have line attractor dynamics. The paper proposes a squared penalty on subsets of the weights in the recurrent network as a way to encourage line attractor dynamics. Also, the proposed regularization penalty only penalizes the diagonal elements of the A matrix to be close to 1 but shouldn t the off diagonal elements also be penalized to be close to zero? This baseline is important to determine if the proposed regularization simply helps because it is an l2 penalty on the weights (note that none of the other baselines have regularization). The paper motivates the method as trying to study line attractor dynamics, but then does not apply them to tasks where line attractors are required.<BRK>This regulariser works on a neuroscience motivated formulation of RNN, bringing the Jacobian of the dynamic system close to identity. While the alleviating the exploding and vanishing gradient problem for simple RNNs is an interesting direction, I think the empirical results are not sufficient to support claims in the paper. The paper starts by criticising initialisation and reparametrisation based techniques. In fact, one may argue that initialisation is a milder constraint compared with an explicit regulariser, since regularisation affects the entire learning process. It seems that such initialisation requires less tuning (i.e., just identity) compared with the regulariser weights (a rather wide range of choices). This could be due to the smaller size of the models (40 vs 100 hidden units in Le et al.). Despite this, I wonder why the performance of addition and multiplication seem even worse than the much smaller model reported in Hochreiter and Schmidhuber 1997? Actually, it would be helpful to test the proposed method on more practical tasks such as language (at least synthetic ones) and speech modelling, which would bring more impact on the wider community. A few technical questions:  Is the form of eq.1 necessary, or can the method be adapted for the more standard formulation of RNN used in machine learning? Is there an additional expectation over p(z)?<BRK>The paper proposes a regularization scheme to improve the ability of RNNs in capturing long range dependency in the latent space. + The motivation of the line attractor is novel and effective. The special RNN model studied in the paper has strong connections with neuro dynamics models. The illustration about the line attractor is particularly interesting. The paper is building a generative model for sequences. It’s not clear to me why VAE or variational RNN type of approaches cannot be used in this setting. The inference procedure can also be significantly simplified with variational inference. Minor comments  " All code used in this work is freely available on the github site ... . "<BRK>Vanilla RNN with ReLU activation have a simple structure that is amenable to systematic dynamical systems analysis and interpretation, but they suffer from the exploding vs. vanishing gradients problem. Recent attempts to retain this simplicity while alleviating the gradient problem are based on proper initialization schemes or orthogonality/unitary constraints on the RNN’s recurrency matrix, which, hotheyver, comes with limitations to its expressive potheyr with regards to dynamical systems phenomena like chaos or multi-stability. Here, they instead suggest a regularization scheme that pushes part of the RNN’s latent subspace toward a line attractor configuration that enables long short-term memory and arbitrarily slow time scales. they show that their approach excels on a number of benchmarks like the sequential MNIST or multiplication problems, and enables reconstruction of dynamical systems which harbor widely different time scales.
Reject. rating score: 3. rating score: 3. rating score: 6. This work is an empirical study of testing how pruning at the pre training stage affects subsequent transfer learning (through fine tuning) stage. Overall, the paper is well written and explained. The goal is meaningful, and this is a sensible contribution to the ongoing interests of compressing BERT like large models for efficient training and inference. First, although the findings are interesting, the methods used in this paper are not new. However, as it is known that it is really difficult for modern hardware to benefit from random sparsity because it leads to irregular memory accesses, which negatively impact the performance.<BRK>This work explores weight pruning for BERT. It finds that pruning affects transfer learning in three broad regimes. My major concern about this work is its technical innovation and value to the community.<BRK>The paper conducts a series of interesting experiments on compressing BERT and makes several conclusions. The compression technique is magnitude weight pruning based on an existing work. The paper is well motivated and presents interesting experimental results and conclusions. I have some concerns on their experiment details, which needs some clarification. 2.It will be helpful to show the thresholds of pruning and how these thresholds relate to the training loss and accuracy.<BRK>Universal feature extractors, such as BERT for natural language processing and VGG for computer vision, have become effective methods for improving deep learning models without requiring more labeled data. A common paradigm is to pre-train a feature extractor on large amounts of data then fine-tune it as part of a deep learning model on some downstream task (i.e. transfer learning). While effective, feature extractors like BERT may be prohibitively large for some deployment scenarios. they explore theyight pruning for BERT and ask: how does compression during pre-training affect transfer learning? they find that pruning affects transfer learning in three broad regimes. Low levels of pruning (30-40%) do not affect pre-training loss or transfer to downstream tasks at all. Medium levels of pruning increase the pre-training loss and prevent useful pre-training information from being transferred to downstream tasks. High levels of pruning additionally prevent models from fitting downstream datasets, leading to further degradation. Finally, they observe that fine-tuning BERT on a specific task does not improve its prunability. they conclude that BERT can be pruned once during pre-training rather than separately for each task without affecting performance.
Reject. rating score: 1. rating score: 3. rating score: 3. This paper proposes a method for constructing adversarial attacks that are less detectable by humans, by changing the target class to be a class similar to the original class of the image.<BRK>I will not get surprised if this paper is accepted. However, all reviewers still share concerns about the importance of the problem tackled. This paper should be rejected due to the lack of motivation to create adversarial examples less detectable by humans automatically.<BRK>I imagine normally the attacker has a target label in mind, so the part of the paper that chooses a target label is not very useful; and this is the main element of novelty, since the rest of the method is from DeepFool, as the authors explain. Minor suggestion: It would be useful to see examples like in Fig.5 but with the classes (true/target) listed.<BRK>It is widely known that theyll-designed perturbations can cause state-of-the-art machine learning classifiers to mis-label an image, with sufficiently small perturbations that are imperceptible to the human eyes. Hotheyver, by detecting the inconsistency bettheyen the image and wrong label, the human observer would be alerted of the attack. In this paper, they aim to design attacks that not only make classifiers generate wrong labels, but also make the wrong labels imperceptible to human observers. To achieve this, they propose an algorithm called LabelFool which identifies a target label similar to the ground truth label and finds a perturbation of the image for this target label. they first find the target label for an input image by a probability model, then move the input in the feature space towards the target label. Subjective studies on ImageNet show that in the label space, their attack is much less recognizable by human observers, while objective experimental results on ImageNet show that they maintain similar performance in the image space as theyll as attack rates to state-of-the-art attack algorithms.
Reject. rating score: 3. rating score: 3. rating score: 6. However, the theoretical novelty is quite limited. There is no guarantee whatsoever whether the good empirical results achieved on the three experimented would persist with other datasets. The flow of the ideas in the paper is clear and unequivocal. What about comparing to other federated learning frameworks, i.e.other than versions and extensions of FA? In the Introduction, it was promised that the proposed method would stop catastrophic training failures; has that been actually described in the experiments? Similarly so for the robustness issue (which is also the first word in the paper title), where is the empirical demonstration of the robustness of the proposed method?<BRK>In this paper, the authors propose a novel representation matching scheme to reduce the divergence of local models in federated learning. In addition, the authors propose an online hyper parameter tuning scheme. The empirical results show good performance. In overall, I think the authors propose an interesting alternative of weight regularization (also called weight divergence loss in this paper). Since FA, FA+WD, and FA+RM have different loss functions, it is unreasonable and unfair to use the same hyperparameters for them. Otherwise, the results of the experiments are questionable. 3.It seems that AH is irrelevant to federated learning.<BRK>This manuscript proposes two strategies to improve both the robustness and accuracy of local agents under the setting of federated learning. Hence instead of using online reinforcement learning for hyperparameter search, which is notoriously data inefficient, why not framing the problem as a pure online learning problem? The second contribution is an idea on using local distribution matching in order to synchronize the learning trajectories of different local models. This again is a novel and interesting idea.<BRK>  Federated learning is a distributed, privacy-aware learning scenario which trains a single model on data belonging to several clients. Each client trains a local model on its data and the local models are then aggregated by a central party. Current federated learning methods struggle in cases with heterogeneous client-side data distributions which can quickly lead to divergent local models and a collapse in performance. Careful hyper-parameter tuning is particularly important in these cases but traditional automated hyper-parameter tuning methods  would require several training trials which is often impractical in a federated learning setting. they describe a two-pronged solution to the issues of robustness and hyper-parameter tuning in federated learning settings. they propose a novel representation matching scheme that reduces the divergence of local models by ensuring the feature representations in the global (aggregate) model can be derived from the locally learned representations. they also propose an online hyper-parameter tuning scheme which uses an online version of the REINFORCE algorithm to find a hyper-parameter distribution that maximizes the expected improvements in training loss. they show on several benchmarks that their two-part scheme of local representation matching and global adaptive hyper-parameters significantly improves performance and training robustness.
Reject. rating score: 3. rating score: 6. rating score: 6. There are interesting ideas in this paper, however I have some questions and concerns about some of the claims made in the paper that I would like to see addressed. Please discuss this paper and clarify the seeming discrepancy between the results there and your claims. 4) The labels are misaligned in Figure 4. Please describe what these models are. So, please discuss this connection in your paper.<BRK>I d like to increase my score. And what are the differences between the proposed attack with previous methods? The authors could discuss more on the attack method and compare its performance with others. Overall, this paper analyzes the invariance based robustness of deep neural networks with norm bounded robustness, which is an interesting problem.<BRK>Same as the previous definition of adversarial robustness (robustness against imperceptible perturbations), perturbation robustness reflects the model s ability to maintain the prediction after a label preserving transform. A more valuable direction would be to evaluate the minimum l_p distance between classes, but it seems intractable at the moment. No automated solution can be inspired from the paper. This can also be verified by the two sphere experiments.<BRK>Adversarial examples are malicious inputs crafted to cause a model to misclassify them. In their most common instantiation, "perturbation-based" adversarial examples introduce  changes to the input that leave its true label unchanged, yet result in a different model prediction.  Conversely, "invariance-based" adversarial examples insert changes to the input that leave the model's prediction unaffected despite the underlying input's label having changed. So far, the relationship bettheyen these two notions of adversarial examples has not been studied, they close this gap.

they demonstrate that solely achieving perturbation-based robustness is insufficient for complete adversarial robustness. Worse, they find that classifiers trained to be Lp-norm robust are more vulnerable to invariance-based adversarial examples than their undefended counterparts. they construct theoretical arguments and analytical examples to justify why this is the case. they then illustrate empirically that the consequences of excessive perturbation-robustness can be exploited to craft new attacks. Finally, they show how to attack a provably robust defense --- certified on the MNIST test set to have at least 87% accuracy (with respect to the original test labels) under perturbations of Linfinity-norm below epsilon=0.4 --- and reduce its accuracy (under this threat model with respect to an ensemble of human labelers) to 60% with an automated attack, or just 12% with human-crafted adversarial examples.
Reject. rating score: 3. rating score: 6. rating score: 6. This paper is well organized. But it lacks some more detailed analysis. 1.The idea of using the concept of age is not new. However, the authors did not provide the analysis that the age and the accumulated task reward are conflicting with each other. 3.It would be better the average survival time of the individuals is presented in this paper.<BRK>While the paper is interesting, I feel in its current state, it is not the level of an ICLR conference paper. Below is some feedback I want to give the authors to help them improve the work, so hopefully it can be improved either for this conference or the next. For the record, the score I wanted to give is a 4, but since I can only choose 3, or 6, I will assign a score of 3 for this review (although should really be a 4). I m not convinced that the approach (multi obj + end to end) which solves VizDoom cannot be solved using Risi2019. The authors explained why Risi2019 s approach cannot work on VizDoom with sufficient effort, and I buy their explanation.<BRK>Given that the architecture has a heterogenous pipeline, the authors propose, and empirically show (over a set of seeds), that allowing extra time for components to adapt to other, more recently changed components, enables neuroevolution to solve the second task. The ablations are an important part of this work, and it would be good to study these further. This ablation will reset the age 1/2 the time DIP does, reducing diversity/increasing selection pressure, and so another appropriate ablation is to see the effects of protecting MDN RNN and controller innovations (with the VAE being the most downstream component).<BRK>Evolutionary-based optimization approaches have recently shown promising results in domains such as Atari and robot locomotion but less so in solving 3D tasks directly from pixels. This paper presents a method called Deep Innovation Protection (DIP) that allows training  complex world models end-to-end for such 3D environments. The main idea behind the approach is to employ multiobjective optimization to temporally reduce the selection pressure on specific components in a world model, allowing other components to adapt. they investigate the emergent representations of these evolved networks, which learn a model of the world without the need for a specific forward-prediction loss. 
Reject. rating score: 3. rating score: 6. rating score: 6. 3.The proposed methods seem easy to apply. The recognized strengths and concerns are as follows. Since the throughput is strongly dependent on the underlying hardware, the number of operations also needs to be shown as a more general estimation of the model inference latency in various hardware devices.<BRK>Pros:  The proposed method is simple and rather easy to implement. The setting of C_j affects the result.<BRK>The paper addresses architecture search as well as model compression by simultaneously optimizing all layers/weights/submodules of the network.<BRK>In many learning situations, restheirces at inference time are significantly more constrained than restheirces at training time. This paper studies a general paradigm, called Differentiable ARchitecture Compression (DARC), that combines model compression and architecture search to learn models that are restheirce-efficient at inference time. Given a restheirce-intensive base architecture, DARC utilizes the training data to learn which sub-components can be replaced by cheaper alternatives. The high-level technique can be applied to any neural architecture, and they report experiments on state-of-the-art convolutional neural networks for image classification. For a WideResNet with 97.2% accuracy on CIFAR-10, they improve single-sample inference speed by 2.28X and memory footprint by 5.64X, with no accuracy loss. For a ResNet with 79.15% Top-1 accuracy on ImageNet, they improve batch inference speed by 1.29X and memory footprint by 3.57X with 1% accuracy loss. they also give theoretical Rademacher complexity bounds in simplified cases, showing how DARC avoids over-fitting despite over-parameterization.
Reject. rating score: 3. rating score: 6. rating score: 6. Briefly, the paper proposes to use a (damped version of) the SVG repulsive term between the current position of a SGLD trajectory and the empirical distribution defined by the trajectory. Unfortunately, I do not think that the experiments are convincing. Is it because of multimodality? Comparison to a single NN? etc...The proposed method is interesting and has a lot of potential. I would like to suggest the authors to spend more times on careful and controlled numerical experiments (Bayesian NN are not very good for this purpose)   with convincing numerics (which would give more reasons to delve into the proofs) the method can be very promising.<BRK>I have the rebuttal of the authors, the paper improved indeed and some point on role of M is better clarified now although it is still a bit convoluted. I think the author put a good effort in addressing some of my concerns and I m raising my score to 6. ####Summary of the paper:The paper proposes stein self repulsive dynamics for sampling from an unnormalized distribution. is this correct? This will be insightful to see if one would mix faster with respect to the other one.<BRK>This paper proposed another variant of Langevin dynamics, called “Stein self repulsive dynamics,” which simultaneously decreases the auto correlation of Langevin dynamics and eliminates the need for running parallel chains in SVGD. If the authors reply appropriately, I will raise the score to accept. How do we take a limit of M  > ∞ ?<BRK>they propose a new Stein self-repulsive dynamics for obtaining diversified samples from intractable un-normalized distributions. their idea is to introduce Stein variational gradient as a repulsive force to push the samples of Langevin dynamics 
away from the past trajectories. This simple idea allows us to significantly decrease the auto-correlation in Langevin dynamics and hence increase the effective sample size. Importantly, as they establish in their theoretical analysis, the asymptotic stationary distribution remains correct even with the addition of the repulsive force, thanks to the special properties of the Stein variational gradient. they perform extensive empirical studies of their new algorithm, showing that their method yields much higher sample efficiency and better uncertainty estimation than vanilla Langevin dynamics.
Reject. rating score: 3. rating score: 3. rating score: 3. rating score: 6. To that end, the authors present an architecture that is cyclic permutation invariant through a pooling operation on all the cycles. The paper is well written and well motivated. As I mentioned previously, it would also be important to analyze the sensitivity of the method with respect to the assumptions build upon. At this stage, I do not think that the paper is ready for acceptance.<BRK>The experiments should be rigorous cause it lacks reliable baselines for comparison. For explanations, the authors did not provide sufficient related works or references to prove that the problem the paper wants to solve is important. For experiments, the results presented in Table 1 are good but there are no official baselines (e.g.from some prior works) to make the comparison more reliable.<BRK>Minor note: there is a typo in the definition of v_iThank you for the submission. Their model may be useful for other domains with phase shifted data, such as multi sensor medical data. The primary reason for this decision is that the authors do not provide sufficient comparisons to related work and models, either in the form of a literature review, or in the form of model benchmarking.<BRK>This paper would be greatly improved by an addition of a related work section. Is there a typo here? They authors mention that these methods would require much more data than their graph convolution approach. I agree this is probably the case, but this would still be a useful empirical result to show the degree of data required for these alternative. What are previous approaches to detecting the properties explored in this work? In addition to discussing previous approaches in a related work section, some empirical analysis comparison would help contextualize this work as well. I think it could be significantly improved with a discussion of related work and better situating of the methods / more comparisons in the results.<BRK>they propose a feature extraction for periodic signals. Virtually every mechanized transportation vehicle, potheyr generation, industrial machine, and robotic system contains rotating shafts. It is possible to collect data about periodicity by mea- suring a shaft’s rotation. Hotheyver, it is difficult to perfectly control the collection timing of the measurements. Imprecise timing creates phase shifts in the resulting data. Although a phase shift does not materially affect the measurement of any given data point collected, it does alter the order in which all of the points are col- lected. It is difficult for classical methods, like multi-layer perceptron, to identify or quantify these alterations because they depend on the order of the input vectors’ components. This paper proposes a robust method for extracting features from phase shift data by adding a graph structure to each data point and constructing a suitable machine learning architecture for graph data with cyclic permutation. Simulation and experimental results illustrate its effectiveness.
Reject. rating score: 3. rating score: 3. rating score: 6. This paper proposes guided adaptive credit assignment (GACA) for policy gradient methods with sparse reward. Experiments of program synthesis and instruction following are conducted, to show the proposed GACA outperform competitive baselines. If it is, clearly show that. Overall, the proposed GACA method achieves promising results in program synthesis tasks. In particular, "Eq.(8) is the optimal solution of KL Eq.(7).Is it also the optimal solution of f divergence used in the algorithm?" Since Algorithm 1 explicitly uses f divergence, I think at least this point should be clarified by the authors rather than my guess. 4.The entropy regularized objective and the KL is kind of well known.<BRK>The authors formulate the credit assignment method as minimizing the divergence between policy function and a learned prior distribution. Then they apply f divergence optimization to avoid the model collapse in this framework. Empirical experiments are conducted on the program synthesis benchmark with sparse rewards. The main contribution of this paper is applying f divergence optimization on the program synthesis task for credit assignment.<BRK>Summary:This work proposed an off policy framework for policy gradient approach calledguided adaptive credit assignment (GACA) in a simplified setting of goal orientedentropy regularized RL. The experiments on sparse reward tasks such as WikiTableQuestions and WikiSQLdemonstrate the effectiveness of the GACA, comparing with a set of advanced baselines. The trajectoryreward is determined by the initial state, goal, and the sequence of actions taken thereafter. The authors also claimed that KL divergence performs worse than f divergence due to the mode seeking issue. Minor:There are many typos or grammar issues in this version.<BRK>Policy gradient methods have achieved remarkable successes in solving challenging reinforcement learning problems. Hotheyver, it still often suffers from sparse reward tasks, which leads to poor sample efficiency during training. In this work, they propose a guided adaptive credit assignment method to do effectively credit assignment for policy gradient methods. Motivated by entropy regularized policy optimization, their method extends the previous credit assignment methods by introducing more general guided adaptive credit assignment(GACA). The benefit of GACA is a principled way of utilizing off-policy samples. The effectiveness of proposed algorithm is demonstrated on the challenging \textsc{WikiTableQuestions} and \textsc{WikiSQL} benchmarks and an instruction following environment. The task is generating action sequences or program sequences from natural language questions or instructions, where only final binary success-failure execution feedback is available. Empirical studies show that their method significantly improves the sample efficiency of the state-of-the-art policy optimization approaches.
Reject. rating score: 3. rating score: 3. rating score: 6. ##############I have read the author s feedback which addresses some of the confusion parts in the paper. 3) How does the Eq (4),(5) give an estimation \hat{E}? The paper has provided consistency guarantee and several synthetic and real data experiments as support. For example, \hat{H} can have more all zero rows than E which still satisfies E\hat{H}   \hat{H}  but  \hat{E}  is not equals to E. In an extreme case,  \hat{H}   0 will have  E\hat{H}   \hat{H} but is clearly not consistent.<BRK>The authors introduce a method (B2B) for disentangling effects of correlated predictors in the context of high dimensional outcomes.<BRK>It is possible to introduce an L1 regulation in E? ##############After reading the author s feedback and the comments from other reviewers, I keep the current rating but tend to a borderline score and it is ok if it must be rejected because of the concerns of limited applicability and the experimental.<BRK>Identifying causes from observations can be particularly challenging when i) potential factors are difficult to manipulate individually and ii) observations are complex and multi-dimensional. To address this issue, they introduce “Back-to-Back” regression (B2B), a method designed to efficiently measure, from a set of co-varying factors, the causal influences that most plausibly account for multidimensional observations. After proving the consistency of B2B and its links to other linear approaches, they show that their method outperforms least-squares regression and cross-decomposition techniques (e.g. canonical correlation analysis and partial least squares) on causal identification. Finally, they apply B2B to neuroimaging recordings of 102 subjects reading word sequences. The results show that the early and late brain representations, caused by low- and high-level word features respectively, are more reliably detected with B2B than with other standard techniques.

Reject. rating score: 1. rating score: 3. rating score: 6. The paper proposed a new type of graph embedding technique for dynamic graphs based on tensor representation (node x feature x time). + Clear writing with tensor notations and explanation is well structured + Improved prediction results on 3/4 real world dynamic graph datasets  The theoretical results are a bit artificial. It would be good to show the scaling behavior of the proposed model.<BRK>This paper presents a M product based temporal GCNs to handle dynamic graphs. Experiments on four real datasets are performed to verify the effectiveness of the proposed model. Given the current status, I could not accept the paper. 2, The experimental setup is reasonable. 3, The paper is clearly written and easy to follow. I agree with the Reviewer #4 that the theoretical results are a bit artificial and trivial.<BRK>Summary: this work uses tensor methods to improve graph convolution for dynamic graph, where the nodes are fixed and the edges are changing. Specifically, it uses the M product technique to develop the operations of sequence of matrices that analog to these operations of matrices. Comments: this paper is mathematically interesting. M transfer is a tensor contraction, right? Decision: I feel this work novel and interesting in general.<BRK>Many irregular domains such as social networks, financial transactions, neuron connections, and natural language structures are represented as graphs. In recent years, a variety of  graph neural networks (GNNs) have been successfully applied for representation learning and prediction on such graphs. Hotheyver, in many of the applications, the underlying graph changes over time and existing GNNs are inadequate for handling such dynamic graphs. In this paper they propose a novel technique for learning embeddings of dynamic graphs based on a tensor algebra framework. their method extends the popular graph convolutional network (GCN) for learning representations of dynamic graphs using the recently proposed tensor M-product technique. Theoretical results that establish the connection bettheyen the proposed tensor approach and spectral convolution of tensors are developed. Numerical experiments on real datasets demonstrate the usefulness of the proposed method for an edge classification task on dynamic graphs.
Reject. rating score: 3. rating score: 6. rating score: 6. This paper is introducing a scale equivariant CNN architecture with joint convolutions over spatial and scale space. Moreover, the authors used decomposable convolutional filters to reduce the number of parameters. Based on Therom 1, it is shown that scale equivariance is achieved if and only if joint convolutions are conducted over spatial and scale space. to verify scale equivariance, visualizing features with tsne would be interesting  in table 1, what is the reason for doing experiments with and without batchnorm? what if the baseline s size is close to the proposed method, how it would be improved?<BRK>*Paper summary*The authors propose a CNN architecture, that is theoretically equivariant to isotropic scalings and translations. *Paper decision*Thank you for writing a very interesting paper indeed. I have to admit I am somewhat on the fence about this paper. *Supporting arguments* Experiments: The experiments are quite light, although I must admit that many other works in the area of equivariance are also light on experimentation and if there is enough theory, that is not such a great issue. The authors treat scale translation in continuous space as a group action on signals. This motivates the convolution presented in Theorem 1.<BRK>The main contributions are the proposed joint convolution across the spatial and scale space, and the decomposed filters to reduce computation cost and improve robustness. The paper is generally well written and well placed in the literature. Different from existing work on the scale equivariance CNNs, they build a more general model that allows different scale information to transfer through different layers. However, for the novelty part of the proposed method (the separable basis decomposition), it is not convincing enough because the differences between this paper and Cheng et al.(2019) and Weiler et al.(2018b) are not clear. The authors claim that the joint convolutions across the space and the scaling group are both "sufficient" and "necessary".<BRK>Encoding the input scale information explicitly into the representation learned by a convolutional neural network (CNN) is beneficial for many vision tasks especially when dealing with multiscale input signals. they study, in this paper, a scale-equivariant CNN architecture with joint convolutions across the space and the scaling group, which is shown to be both sufficient and necessary to achieve scale-equivariant representations. To reduce the model complexity and computational burden, they decompose the convolutional filters under two pre-fixed separable bases and truncate the expansion to low-frequency components. A further benefit of the truncated filter expansion is the improved deformation robustness of the equivariant representation. Numerical experiments demonstrate that the proposed scale-equivariant neural network with decomposed convolutional filters (ScDCFNet) achieves significantly improved performance in multiscale image classification and better interpretability than regular CNNs at a reduced model size.
Reject. rating score: 3. rating score: 3. rating score: 6. In this paper, instead of the original IB, authors consider a previously presented dual problem of Felice and Ay, where the Kullback Leibler divergence is minimized in the reverse direction: from the conditional distribution of output given encoding, p(y|hat x), to the conditional distribution given the original input, p(y|x). The "dual problem" itself has a more complicated form than the original IB, authors claim this is "a good approximation" of the original bottleneck formulation, and aim to prove various "interesting properties" of it. Evaluation:This is an entirely theory based paper; although an algorithm is given, it is not instantiated for any concrete representation learning task, and no experiments at all are demonstrated. However, authors do not quantify these well:  The computational complexity improvement is not made clear (quantified) in a concrete IB optimization task: it seems it is only for exponential families, and even for them only affects one part of the algorithm, reducing its complexity from dim(X) to d; the impact of this is not tried out in any experiment. In its current state the paper, although based on an interesting direction, in my opinion does not make a sufficient impact to be accepted to ICLR. Section 1.5 claims the algorithm "preserves the low dimensional sufficient statistics of the data": it is not clear what "preserves" means here, certainly it seems the decoder in Theorem 7 uses the same kinds of sufficient statistics as in the original data, but it is not clear that hat(x) would somehow preserve the same values of the sufficient statistics.<BRK>This paper introduces a variant of the Information Bottleneck (IB) framework, which consists in permuting the conditional probabilities of y given x and y given \hat{x} in a Kullback Liebler divergence involved in the IB optimization criterion. Interestingly, this change only results in changing an arithmetic mean into a geometric mean in the algorithmic resolution. Good properties of the exponential families (existence of non trivial minimal sufficient statistics) are preserved, and an analysis of the new critical points/information plane induced is carried out.<BRK>This paper proposes a new "dual" variant of the Information Bottleneck framework. The existing framework measures the retained information about the prediction via mutual information, which can be expressed as a KL divergence. In particular,  it can be shown that for exponential families the dual IB representation retains the exponential form. Overall, I think this paper adds a meaningful new perspective to the IB framework and the analysis appears to be thorough. The meaning of the denominator is not clear (it is a normalizing constant).<BRK>The Information-Bottleneck (IB) framework suggests a general characterization of optimal representations in learning, and deep learning in particular. It is based on the optimal trade off bettheyen the representation complexity and accuracy, both of which are quantified by mutual information. The problem is solved by alternating projections bettheyen the encoder and decoder of the representation, which can be performed locally at each representation level. The framework, hotheyver, has practical drawbacks, in that mutual information is notoriously difficult to handle at high dimension, and only has closed form solutions in special cases. Further, because it aims to extract representations which are minimal sufficient statistics of the data with respect to the desired label, it does not necessarily optimize the actual prediction of unseen labels. Here they present a  formal dual problem to the IB which has several interesting properties. By switching the order in the KL-divergence bettheyen the representation decoder and data, the optimal decoder becomes the geometric rather than the arithmetic mean of the input points. While providing a good approximation to the original IB, it also preserves the form of exponential families, and optimizes the mutual information on the predicted label rather than the desired one. they also analyze the critical points of the dualIB and discuss their importance for the quality of this approach.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. The paper addresses the task of learning with point clouds for semantic labeling (classification and segmentation). With the proposed learnable operations, the authors are able to efficiently learn the functions defined in 3D space, such as the semantic class labels. The operations include transferring the features between the grid and the point cloud, advection, and interpolation, all implemented in a unified learnable model. The architecture is evaluated on classification and segmentation tasks with common datasets, where it performs on par with existing methods. While the experimental evaluation does not indicate that the proposed method is a new state of the art, it convincingly validates that the proposed method is capable of learning powerful enough representations.<BRK>The paper is about using classical PIC/FLIP scheme in Computational Fluid Dynamics for solving the learning problem of 3D object detection and segmentation. This provides a very interesting perspective to 3D deep learning. can be compared with or at least contrasted in the related works. However, the performance of the proposed approach is at best comparable to some of the state of the art methods such as PointCNN or SE Net.<BRK>The work first explains how the simulation algorithm is mapped to the learning problem: MLPs are employed to learn sets of particle based features which are mapped to a Eulerian grid. This is repeated for a certain number of steps to obtain final positions. The paper presents a brief ablation study for number of iterated steps, grid size and point count, before presentation two comparisons with existing baselines. Overall, I found the idea to employ FLIP for Lagrangian learning tasks novel and very interesting. The method does not yield clear gains over previous work, but rather a similar performance for classification and segmentation of ShapeNet and S3DIS data is shown. Several of the deformations shown in figure 5 and 6 are also not really intuitiveAlso, on second sight, I don t fully understand the motivation for employing and learning a grid based deformation. It s also not obvious how to choose parameters such as the number of time steps.<BRK>This paper presents a novel physics-inspired deep learning approach for point cloud processing motivated by the natural flow phenomena in fluid mechanics.  their learning architecture jointly defines data in an Eulerian world space, using a static background grid, and a Lagrangian material space, using moving particles. By introducing this Eulerian-Lagrangian representation, they are able to naturally evolve and accumulate particle features using flow velocities generated from a generalized, high-dimensional force field.  they demonstrate the efficacy of this system by solving various point cloud classification and segmentation problems with state-of-the-art performance. The entire geometric reservoir and data flow mimic the pipeline of the classic PIC/FLIP scheme in modeling natural flow, bridging the disciplines of geometric machine learning and physical simulation.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. This paper presents ReMixMatch an improved version of MixMatch. The main contributions are the distribution alignment and the augmentation anchoring. However, there are some negative points that the authors should clarify:  The final method is a mixup of many different techniques, thus, not a strong contribution, but many smaller contributions. As shown in the ablation study, the main contribution on the obtained results seems to be the use of stronger transformations than in MixMatch. Overall the paper is well presented and contributes to further improve the performance on semi supervised learning. I there fore recommend it for acceptance. Would it be better or worse than the proposed approach?<BRK>The major contributions include: (1) distribution alignment to calibrate the predicted distribution of unlabeled data; (2) augmentation anchoring to allow more aggressive data augmentation; and (3) CTAugment to train the augmentation policy alongside the semi supervised model. Overall, the paper proposes some simple but interesting ideas, e.g.distribution environments. As pointed out in the weakness section, many design choices are not well motivated, and the effects of those designs are not well studied. The variation induced by aggressive augmentation is the root of the consistency loss that helps regularize the model. 2.The authors should provide ablation study and analysis of their CTAugment. Also does larger K value when applied for vanilla MixMatch approach the results in ReMixMatch? 5.It is recommended to evaluate the method on larger datasets such as CIFAR 100.<BRK>This paper proposes two modifications for the MixMatch method [1] and achieves improved accuracy on a range of semi supervised benchmarks. The main contribution of the paper is really strong empirical results. Another important contribution is the learned data augmentation strategy, which as far as I understand is novel and overcomes some of the limitations of  existing learned data augmentation techniques. The main drawback of the paper is that it seems to be more engineering focused, and doesn’t provide much insight into semi supervised learning. For the learnable data augmentation it would be great if the authors could provide more insight into the method, how it works, and why is it better than the alternatives. I would recommend discussing these results briefly in the paper.<BRK>they improve the recently-proposed ``MixMatch semi-supervised learning algorithm by introducing two new techniques: distribution alignment and augmentation anchoring.
- Distribution alignment enctheirages the marginal distribution of predictions on unlabeled data to be close to the marginal distribution of ground-truth labels.
- Augmentation anchoring} feeds multiple strongly augmented versions of an input into the model and enctheirages each output to be close to the prediction for a theyakly-augmented version of the same input.
To produce strong augmentations, they propose a variant of AutoAugment which learns the augmentation policy while the model is being trained.

their new algorithm, dubbed ReMixMatch, is significantly more data-efficient than prior work, requiring bettheyen 5 times and 16 times less data to reach the same accuracy. For example, on CIFAR-10 with 250 labeled examples they reach 93.73% accuracy (compared to MixMatch's accuracy of 93.58% with 4000 examples) and a median accuracy of 84.92% with just ftheir labels per class.

Accept (Poster). rating score: 8. rating score: 6. rating score: 6. This paper describes a method for ensemble distillation that retains both types of uncertainty in the case of classification. The idea is to train a prior network (i.e.a conditional Dirichlet distribution) to model the distribution of categorical probabilities within the ensemble. Pros:The paper considers an interesting problem, and makes a clear contribution towards addressing it. The paper is very well written and easy to read. Cons:The scope of the paper and the method is limited to the problem of probabilistic classification. Decision:Overall, this is good work and I m happy to recommend acceptance.<BRK>It also proposes a new metric for evaluating the Prediction Rejection Ratio (PRR), uses it to compare how the EnD^2 model compares to the original ensemble and the EnD model. Rebuttal response:I acknowledge the authors  point about the importance of EnDD in addition to knowledge uncertainty estimation, and maintain my rating.<BRK>  Post Rebuttal   I appreciate the authors  effort in addressing the raised issues. (III) Missing from experiments  The main goal/motivation of the paper is to be able to decompose the total uncertainty when distilling an ensemble into a single model. Th concerns on consistency and conclusiveness of the results can hopefully be addressed in a journal version of the work as suggested by the authors. Does a “successful” training of the prior network require a large number of samples? Hyperparameter optimization: Appendix A provides the final values for the hyperparameters of each method.<BRK>Ensembles of models often yield improvements in system performance. These ensemble approaches have also been empirically shown to yield robust measures of uncertainty, and are capable of distinguishing bettheyen different forms of uncertainty. Hotheyver, ensembles come at a computational and memory cost which may be prohibitive for many applications. There has been significant work done on the distillation of an ensemble into a single model. Such approaches decrease computational cost and allow a single model to achieve an accuracy comparable to that of an ensemble. Hotheyver, information about the diversity of the ensemble, which can yield estimates of different forms of uncertainty, is lost. This work considers the novel task of Ensemble Distribution Distillation (EnD^2) - distilling the distribution of the predictions from an ensemble, rather than just the average prediction, into a single model. EnD^2 enables a single model to retain both the improved classification performance of ensemble distillation as theyll as information about the diversity of the ensemble, which is useful for uncertainty estimation. A solution for EnD^2 based on Prior Networks, a class of models which allow a single neural network to explicitly model a distribution over output distributions, is proposed in this work. The properties of EnD^2 are investigated on both an artificial dataset, and on the CIFAR-10, CIFAR-100 and TinyImageNet datasets, where it is shown that EnD^2 can approach the classification performance of an ensemble, and outperforms both standard DNNs and Ensemble Distillation on the tasks of misclassification and out-of-distribution input detection.
Accept (Poster). rating score: 8. rating score: 3. The main problem they try to tackle is to train agents for unseen tasks and environmental changes. The authors clearly position their work in the HRL paradigm and explain current limitations/challenges within that field. The paper is very well written, clearly stated the contributions. This behaviour is not described anywhere. The URL of the website with code and videos does not have any code. Why are the results of different methods (p 10/random) in Table 1 for different environments (Snake/Ant) different?<BRK>This paper is under the topic of hierarchical reinforcement learning. The motivation of this paper is "most methods still decouple the lower level skill acquisition process and the training of a higher level that controls the skills in a new task." The paper proposes a method to learn higher level skill selection and lower level skill improvement jointly. What I like in this paper:    1. My biggest concern is the motivation of this paper. 2.I think the author didn t justify his key design choices well. Section 2: the horizon T in the definition in \eta should be H.Section 4: the advantage function is not defined.<BRK>Hierarchical reinforcement learning is a promising approach to tackle long-horizon decision-making problems with sparse rewards. Unfortunately, most methods still decouple the lotheyr-level skill acquisition process and the training of a higher level that controls the skills in a new task. Leaving the skills fixed can lead to significant sub-optimality in the transfer setting. In this work, they propose a novel algorithm to discover a set of skills, and continuously adapt them along with the higher level even when training on a new task. their main contributions are two-fold. First, they derive a new hierarchical policy gradient with an unbiased latent-dependent baseline, and they introduce Hierarchical Proximal Policy Optimization (HiPPO), an on-policy method to efficiently train all levels of the hierarchy jointly. Second, they propose a method of training time-abstractions that improves the robustness of the obtained skills to environment changes.  Code and videos are available at sites.google.com/view/hippo-rl.
Reject. rating score: 1. rating score: 1. rating score: 6. There is no intuitive discussion on what is missing in existing methods, why the proposed method can be better, and when the proposed method may also fail. Note that an important related work is missing, namely "Robust Inference via Generative Classifiers for Handling Noisy Labels" from ICML 2019 (see https://arxiv.org/abs/1901.11300). That paper was a 20 min long oral presentation at Hall A (i.e., one of the most crowded sessions), and the authors should really compare with it both conceptually and experimentally.<BRK>It is recommended to reject the paper, with the following concerns in mind. (1) The proposed approach is not deeply studied. Without the deeper study, Section 3 is at best a naive use of k NN for data cleaning, and it is not clear whether the contribution is substantial. It seems more related to applying k NN in general. (3) It is not clear whether the experiments are compared with respect to state of the art (or at least it is hard to see from Section 2). It seems that rather straightforward baselines are being compared. I thank the authors for clarifying this.<BRK>This paper proposes a k NN method for identifying corrupted labels, and then applies this k NN in the representation space of a deep neural net rather than the original feature space. Overall the paper is well written and the results look quite convincingThe theory appears to be important (if somewhat straightforward looking) contributions of existing k NN theory to the corrupted labels setting, based on the key quantity the authors defined as S_k, the minimum k NN spread.<BRK>Modern machine learning models are often trained on examples with noisy labels that hurt performance and are hard to identify. In this paper, they provide an empirical study showing that a simple $k$-nearest neighbor-based filtering approach on the logit layer of a preliminary model can  remove mislabeled training data and produce more accurate models than some recently proposed methods. they also provide new statistical guarantees into its efficacy.
Reject. rating score: 3. rating score: 6. rating score: 6. This work introduces a new polynomial feed forward neural network called Ladder Polynomial Neural Network (LPNN). Theoretical results show that LPNNs generalize vanilla PNNs and FMs. I assume that it is given in the input layer l 0, according to the notation. However, I still consider that the contribution of the paper is limited, and should be improved for a clear acceptance.<BRK>This paper proposes Ladder Polynomial Neural Networks (LPNNs) that use a new type of activation primitive   a product activation   in a feed forward architecture. The observation in 3.3, that batch normalization or dropout can be used for this model is perhaps tangential to the main argument. Overall, I think the paper has some merit and could be interesting for some readers, despite the fact that the contribution is not very original and the treatment could be improved in many ways.<BRK>In this paper, the authors a new type of polynomial neural networks LPNN that can have an arbitrary polynomial order. Empirical study shows that deep LPNN models achieve good performances in regression and classification tasks. In general, the paper is clearly written by addressing an interesting problem but I still have several concerns. The authors are expected to report more complex tasks to show its effectivenss.<BRK>The underlying functions of polynomial neural networks are polynomial functions. These networks are shown to have nice theoretical properties by previous analysis, but they are actually hard to train when their polynomial orders are high. In this work, they devise a new type of activations and then create the Ladder Polynomial Neural Network (LPNN). This new network
can be trained with generic optimization algorithms. With a feedforward structure, it can also be combined with deep learning techniques such as batch normalization and dropout. Furthermore, an LPNN provides good control of its polynomial order because its polynomial order increases by 1 with each of its hidden layers. In their empirical study, deep LPNN models achieve good performances in a series of regression and classification tasks.
Reject. rating score: 3. rating score: 3. rating score: 6. rating score: 8. In addition, the author consider for two cases with the importance of second order oracle and use Hessian to improve the approximation of gradient when first order and second order oracle are equally important. In general, the paper is well written and easy to follow, but I still have some questions about this paper. More discussion about this issue is needed. .Second, to the best of my knowledge, the trust region radius is changing during the iteration for basic trust region algorithm. In practice, it is important to consider the space complexity. However, this work did not provide any space complexity analysis, especially to compare with first order algorithms. The authors need to make more comments on this issue. Finally, I think the experiment results contradict the theoretical results. However, the convergence analysis provided by the authors claim that the convergence speed is sublinear. I believe such a difference is due to a fact that the initialized parameter is near to the global minimum, thus the optimization landscape here is actually convex but not non convex.<BRK>This paper applies the spider algorithm (Fang et al., 2018) for reducing variance in first order stochastic optimization to a second order optimization algorithm, i.e., trust region algorithm. However, the idea of variance reduction is not novel and the result is not surprising since the improvement comes purely from spider (Fang et al., 2018) and thus this work is somewhat incremental. The comparison of this paper with a similar paper by Zhou & Gu (2019) is not convincing. But the authors did not present the complexity of SRVRC in Table 1. This is not appropriate since it is very similar and related to this paper. However, the cubic regularization based algorithm in Zhou & Gu (2019) can also achieve the same second order oracle complexity. Many places can be simplified or combined in order to increase the readability of the main theorems. The paper talks about $\epsilon$ SOSP, $\tilde{O}(\epsilon)$ SOSP, $12\epsilon$ SOSP and so on in many places. In the text after Corollary 4.1, there are some typos in the complexities where a $\min$ operator is missing. I quickly checked their paper and found in their table that the SFO complexity of Zhou & Gu (2019) is $\tilde{O}(\min\{n/\epsilon^{3/2},n^{1/2}/\epsilon^2,1/\epsilon^3\})$, which is in fact smaller than the complexity of STR1 in this paper.<BRK>This paper proposes new stochastic trust region algorithms for non convex finite sum minimization problems. The first algorithm STR1 has lower second order oracle complexity, while STR2 has lower first order + second order oracle complexity. The authors also give a Hessian free implementation of stochastic trust region algorithm. Technically, the authors first analyze trust region methods with inexact gradient and Hessian estimation, and then implement efficient gradient and Hessian estimators. Overall this paper is well written and easy to follow. I would recommend acceptance.<BRK>The authors propose a new analysis for trust region methods with approximate models. Using this result, they propose a number of methods to create stochastic trust region methods by constructing approximate quadratic models (based on a stochastic first and second order estimate) which satisfy the requirements for convergence. This paper is overall an interesting contribution which proposes a number of competitive methods for achieving approximate local minima in a stochastic regime, with both hessian based and “hessian free” methods. A couple of minor points:  In the experiments, it would be helpful to also include some measure of uncertainty (such as standard error bars) in the plots given the stochastic nature of the problem (although I do not expect high variance given the construction of the algorithm).<BRK>they target the problem of finding a local minimum in non-convex finite-sum minimization. Towards this goal, they first prove that the trust region method with inexact gradient and Hessian estimation can achieve a convergence rate of order $\mathcal{O}({1}/{k^{2/3}})$ as long as those differential estimations are sufficiently accurate.
Combining such result with a novel Hessian estimator, they propose a sample-efficient stochastic trust region (STR) algorithm which finds an $(\epsilon, \sqrt{\epsilon})$-approximate local minimum within $\tilde{\mathcal{O}}({\sqrt{n}}/{\epsilon^{1.5}})$ stochastic Hessian oracle queries. 
This improves the state-of-the-art result by a factor of $\mathcal{O}(n^{1/6})$. Finally,  they also develop Hessian-free STR algorithms which achieve the lotheyst runtime complexity. 
Experiments verify theoretical conclusions and the efficiency of the proposed algorithms.
Reject. rating score: 3. rating score: 6. rating score: 6. # 1.SummaryThe paper deals with the problem of learning grounded captions from images without joint text location information, but instead texts (words in captions) and locations are provided independently and the model needs to figure out their link. The model is built upon GVD (Zhou et al., 2019): each word generated by the encoder is grounded to the locations provided by the region proposal module (Up Down model (Anderson et al., 2018)), used to reconstruct the ground truth caption. Clarity and MotivationThe paper is generally well written, however there are some concerns on motivations. The authors use grounding as a proxy to improve image captioning results, which improvement is marginal wrt GVD (see Table 1). Why do we need to localize text if this has very marginal impact on the captioning metrics? It is missing the link between the potential applications where the localization of words is relevant. The authors claim that they do not uses any grounding annotation, however the pre trained Faster RCNN has been trained using annotations which consist of bounding boxes + categories. The authors should assess if this ovelap/bias in the pre trained Faster RCNN exists or not. Some other questions are still to be answered:* How are the regions R parametrized? Is it the visual representation or bounding box locations? * What happens to words that are not grounded to the image (e.g., verbs or articles)? * What is the intuition of multiplying word embedding and region embeddings to generate z in Eq.6?# 3.NoveltyThe proposed method is an extension of the existing model GVD, where the attentional module is removed and its functionality is replaced by the cyclical training mode with reconstruction of the object locations. # 4.ExperimentationThe experiments are carried out in a scrupulous way, by showing the comparing with GVD (with and without attention; with and without grounding supervision). These results are not convincing, combined by the fact that it is not clear in which applications one would want a very accurate grounded text. # Minor Points* Sec.3.1: it is not clear that the Language LSTM is the decoder.<BRK>The experimental results show that the performance on image captioning and video captioning are improved without grounding supervision. The motivation using cyclic feedback itself is not so novel for language generation, but focusing on grounding without localization supervision for visual captioning is interesting. The experimental results show that the proposed method can boost performance both qualitatively and qualitatively. I have several comments and questions below. If the combination of self attention in GVD and cyclical training proposed in this paper is complementary to each other, it does help to improve the overall accuracy. While the authors develop a cyclical training pipeline, including decoding, localization, and reconstruction, Figure 1 does not show which part corresponds to the decoding phase. The authors should clarify it to make the paper easier to be understood. $\theta^*$, a sum of two parameters for each arg max operator, doesn t guarantee that each term in the right side of Eq.(5) keeps its max. This equation seems to be a conceptual one, and the actual training would be performed according to Eq.(7).Therefore, the experimental results might not be influenced by the error in Eq.(5).$\hat{r}^l_t \beta_t^\top R$ between Eq.(6) and Eq.(7) means that $\hat{r}^l_t$ is a row vector while $r_n$ seems to be a column vector. There are similar errors, such as "5 GT captions" and "1 GT caption" in Sec.4.The format of items in References is not consistent. How are the experimental results sensitive to these hyperparameters?<BRK>The paper proposes an architecture that grounds words from a captioning model, but without requiring explicit per word grounding training data. Instead, they show that it is sufficient to use cycle consistency, verifying that by predicting word >grounding >word the two words are the same. This paper takes it to the domain of vision and language. While the novelty is not very large it seems like a solid step in an interesting direction. Evaluated on both image and video captioning with substantial localization improvement in the specific relevant eval settings. Specific comments:   In Table 1, the part of "Caption Evaluation" the proposed method is in bold, but it seems that "Up Down" method out performs the proposed method in B@1 and B@4. The localization model is linear? What would be the effect of richer models on localization accuracy? Qualitative analysis: It would have been useful to add evaluations by human raters to measure the perceptual quality of the localization.<BRK>When automatically generating a sentence description for an image or video, it often remains unclear how theyll the generated caption is grounded, or if the model hallucinates based on priors in the dataset and/or the language model. The most common way of relating image regions with words in caption models is through an attention mechanism over the regions that are used as input to predict the next word. The model must therefore learn to predict the attentional theyights without knowing the word it should localize. This is difficult to train without grounding supervision since recurrent models can propagate past information and there is no explicit signal to force the captioning model to properly ground the individual decoded words. In this work, they help the model to achieve this via a novel cyclical training regimen that forces the model to localize each word in the image after the sentence decoder generates it, and then reconstruct the sentence from the localized image region(s) to match the ground-truth. their proposed framework only requires learning one extra fully-connected layer (the localizer), a layer that can be removed at test time. they show that their model significantly improves grounding accuracy without relying on grounding supervision or introducing extra computation during inference for both image and video captioning tasks.
Reject. rating score: 3. rating score: 6. rating score: 6. The authors consider the problem of sampling time series. To solve the problem they propose a method that is based on the autoregression model. The distinctive feature of the algorithm is speed up for the sample generation process. It seems like a good idea to compare to these methods (and it seems that video generation is a very resource demanding procedure, and they don t use parallel applications similar to proposed in the paper.What is the reason?) Most of the approaches use only one frame to generate video, but it seems that LSTM in these methods will benefit from using of multiple frames as input (and will be able to transfer information in autoregression manner by transferring all they need in a hidden state). In Figure 2 (a) it is not clear how the data and prediction were generated.<BRK>The paper presents a technique for approximately sampling from autoregressive models using something like a a proposal distribution and a critic. The idea is to chunk the output into blocks and, for each block, predict each element in the block independently from a proposal network, ask a critic network whether the block looks sensible and, if not, resampling the block using the autoregressive model itself.<BRK>This paper addresses the sequential limitation of autoregressive model when doing sampling. Would any of the existing AR speedup method be applicable for a comparison? Overall the paper is well motivated, with an interesting design of the variational distribution to approximate the true autoregressive distribution. The design of the confidence model looks a bit heuristic, but the trade off ability between efficiency and quality it brings is also quite interesting.<BRK>they propose a generic confidence-based approximation that can be plugged in and simplify an auto-regressive generation process with a proved convergence. they first assume that the priors of future samples can be generated in an independently and identically distributed (i.i.d.) manner using an efficient predictor. Given the past samples and future priors, the mother AR model can post-process the priors while the accompanied confidence predictor decides whether the current sample needs a resampling or not. Thanks to the i.i.d. assumption, the post-processing can update each sample in a parallel way, which remarkably accelerates the mother model. their experiments on different data domains including sequences and images show that the proposed method can successfully capture the complex structures of the data and generate the meaningful future samples with lotheyr computational cost while preserving the sequential relationship of the data.}
Reject. rating score: 1. rating score: 3. rating score: 3. rating score: 3. rating score: 6. Main difference of this method is compared to Gauss Newton, is that it uses JJ  as curvature, which has dimensions b by by (batch size b), instead of J J as curvature, which has dimensions m by m (number of parameters m). Bulk of the paper is dedicated to theoretical convergence and connections between concepts. Since the focus of the paper is on a new optimization method for deep learnning, I feel like convergence proofs can be moved to Appendix, and more of the paper should focus on practical aspects of the method. Their method seems to be limited to neural network with one output (ie, univariate regression task). Experiments are performed on two tasks that are not well known in the literature. The choice is somewhat understandable given that their method performs for univariate regression, but also this makes it hard to evaluate whether the method works. The changes needed to make this paper acceptable are extensive, and I would recommed a reject.<BRK>The authors propose a scalable second order method for optimization using a quadratic loss. The method is inspired by the Neural Tangent kernel approach, which also allows them to provide global convergence rates for GD and batch SGD. However, this is not clear in the paper, instead section 3.1, is a bit vague about the derivation of (9). They rely essentially on the convergence results established for NTK in [Jacot2018, Chizat2018]. The main novelty is that the authors provide faster rates for the Gauss Newton pre conditioner which leads to second order convergence. This makes the result less appealing as in practice this is highly unlikely to be the case. A more fair comparison would be against other second order optimizers like KFAC. I think the paper will be stronger if extended to more general cases (multivariate output + more general losses), thus I encourage the authors to resubmit the paper with stronger experiments.<BRK>As pointed out by other reviewers, the proposed algorithm is restricted to single output regression and the claim "accelerate convergence without much computational overhead" might not be true in general multi output regression tasks. That being said, I do find the algorithm interesting and the theoretical results impressive. I encourage the authors to include experiments on multi output regression tasks (or tone down the claim about computational overhead) and resubmit the paper. Based on recent progress on the connection between neural network training and kernel regression of neural tangent kernel, this paper proposes a Gram Gauss Newton (GGN) algorithm to train deep neural networks for regression problems with square loss. Overall, this paper is well written and easy to follow. I think the authors should make the connection clear. I would like to see more discussions with natural gradient descent or Newton methods in the next revision. The authors claim that the proposed GGN algorithm only has minor computational overhead compared to first order methods.<BRK>My overall assessment is that the method is still quite limited and the method itself is not novel, but I am willing to change my score to accept if my concerns have been addressed. (2) The method is still quite limited to 1 output function scenario, where the NTK matrix is easy to compute. This limitation though is not mentioned in the paper. I hope the author should have a discussion on this and admit this limitation. (5) In the theoretical section, the paper states "However, to our knowledge, no convergence result considering large learning rate (e.g.has the same scale with theupdate of GGN) has been proposed." Here are some papers: [2,3,4](6) Lack of some second order optimization baselines, e.g., KFAC. I wish to see more evidence of showing they re the same as authors claimed. [3] Fast and Faster Convergence of SGD for Over Parameterized Models (and an Accelerated Perceptron).<BRK>Their method is described in Algorithm 1, but to summarize it, they use the Gauss Newton method to train neural networks, and prove quadratic convergence for the full batch training. Due to the seeming extra computational cost of the method, (the method requires computing the full Jacobian matrix which depends on the number of neural network weights) I am grateful that they provided comparisons with wallclock time to SGD. And there is this notion that second order methods have been shown to not generalize as well as first order methods, and thus it was nice to see that they had an experiment where they tested generalization. The background information was also nice to read. Weaknesses: They do not compare it with other methods optimization methods, such as Adam (a first order method) or natural gradient (a second order method), and I would have thus liked to have seen comparisons to these.<BRK>First-order methods such as stochastic gradient descent (SGD) are currently the standard algorithm for training deep neural networks. Second-order methods, despite their better convergence rate, are rarely used in practice due to the pro- hibitive computational cost in calculating the second-order information. In this paper, they propose a novel Gram-Gauss-Newton (GGN) algorithm to train deep neural networks for regression problems with square loss. their method draws inspiration from the connection bettheyen neural network optimization and kernel regression of neural tangent kernel (NTK). Different from typical second-order methods that have heavy computational cost in each iteration, GGN only has minor overhead compared to first-order methods such as SGD. they also give theoretical results to show that for sufficiently wide neural networks, the convergence rate of GGN is quadratic. Furthermore, they provide convergence guarantee for mini-batch GGN algorithm, which is, to their knowledge, the first convergence result for the mini-batch version of a second-order method on overparameterized neural net- works. Preliminary experiments on regression tasks demonstrate that for training standard networks, their GGN algorithm converges much faster and achieves better performance than SGD.
Accept (Poster). rating score: 8. rating score: 8. rating score: 6. The authors propose a novel framework for training a knowledge grounded dialogue model. Overall I think it’s a strong paper with a good set of experiments, baselines, and considers multiple datasets. Disentangling the model elements is a clever way to allow for more robust pre training, and indeed yields favorable results.<BRK>The experimental results show that the proposed architecture help to resolve the low resource problems of knowledge grounded dialogue generation. Overall, this is a well written paper with reasonable ideas supported by strong empirical evidences. Secondly, knowledge encoder can be built purely on knowledge sources in parallel with the dialogue based components.<BRK>This paper studies knowledge grounded dialogue response generation in the low resource setting. In Figure 2 (c) and (d), some ablation results are reported for when pre training is removed for each of the three components independently. If so, are these baselines strong enough (SOTA or close to SOTA) to help draw a meaningful conclusion from comparison with them? Can authors comment on this? Except for pre training, the only difference from [2] is that copying and generation distributions are softly combined into separate distribution each, independently.<BRK>Responding with knowledge has been recognized as an important capability for an intelligent conversational agent. Yet knowledge-grounded dialogues, as training data for learning such a response generation model, are difficult to obtain. Motivated by the challenge in practice, they consider knowledge-grounded dialogue generation under a natural assumption that only limited training examples are available. In such a low-restheirce setting, they devise a disentangled response decoder in order to isolate parameters that depend on knowledge-grounded dialogues from the entire generation model. By this means, the major part of the model can be learned from a large number of ungrounded dialogues and unstructured documents, while the remaining small parameters can be theyll fitted using the limited training examples. Evaluation results on two benchmarks indicate that with only $1/8$ training data, their model can achieve the state-of-the-art performance and generalize theyll on out-of-domain knowledge. 
Reject. rating score: 3. rating score: 6. rating score: 8. Thus they propose a way to constrain the posterior mean to alearned probability simplex and only perform manipulation within the probability simplex. The tackled problem is important. Variationnal text auto encoding is a very challenging task, for which no perfect solution has been proposed yet. Next, to ensure that information is filled in this constrained part, they define a pairwise ranking loss which enforce the mean of each sentence to be more similar to the output of the encoding lstm than the mean of other sentences. In what sense does it ensure that the space does not contain holes ? What ensures that the constrained part of the code is actually used by the decoder ? My main concern is with regards to the experiments, which are clearly not enough detailled. First, I cannot understand what NLL is considered in the preliminary experiments. Authors study the effect of code manipulation on an NLL. But the likelihood of what ? But what sense does it make to assess the nll of the generated sentence ? Also, authors compare the impact of modifications on the representations of $\beta$ VAE with modifications on their model, but these are not the same modifications. What ensure that they have the same magnitude ? Comparisons with metrics on text style transfer are also difficult to understand to me.<BRK>This paper presents a method for controlled text generation by using a new loss function (standard VAE loss with auxiliary losses added on). To address this problem, they constrain the posterior mean to a learnt probability simplex and try to ensure that the simplex is densely filled. This could be placed in an Appendix. In section 6.3, is K still 3 or is it now > 4? Some citations issues   You re missing a couple key citations. Additionally, I d like to see some more detail about the robustness of the method. The presented results are impressive but have a few missing pieces: citing some key results (Lample et al.2019) and including results about robustness. I think the questions I have, and any shortcomings this paper has, could be addressed in a camera ready version.<BRK>This paper tries to pinpoint why sequence VAEs haven t worked well to provide disentangled representations. The authors posits that this happens due to the fact that perturbing the intermediate representation (codes) pushes them in regions which are not seen in training, and hence the model is not well equipped to perform well for those codes. To address this, they augment the VAE objective with terms to ensure that codes are present in a probability simplex and the entire simplex is uniformly filled by codes. I found the observation very interesting, and the supporting experiments confirm the hypothesis. However, since I do not actively work in this area, I am not sure how exciting the result will be to other researchers. There s some related work on controlled text generation.<BRK>The variational autoencoder (VAE) has found success in modelling the manifold of natural images on certain datasets, allowing meaningful images to be generated while interpolating or extrapolating in the latent code space, but it is unclear whether similar capabilities are feasible for text considering its discrete nature. In this work, they investigate the reason why unsupervised learning of controllable representations fails for text. they find that traditional sequence VAEs can learn disentangled representations through their latent codes to some extent, but they often fail to properly decode when the latent factor is being manipulated, because the manipulated codes often land in holes or vacant regions in the aggregated posterior latent space, which the decoding network is not trained to process. Both as a validation of the explanation and as a fix to the problem, they propose to constrain the posterior mean to a learned probability simplex, and performs manipulation within this simplex. their proposed method mitigates the latent vacancy problem and achieves the first success in unsupervised learning of controllable representations for text. Empirically, their method significantly outperforms unsupervised baselines and is competitive with strong supervised approaches on text style transfer. Furthermore, when switching the latent factor (e.g., topic) during a long sentence generation, their proposed framework can often complete the sentence in a seemingly natural way -- a capability that has never been attempted by previous methods. 
Reject. rating score: 3. rating score: 3. rating score: 6. The authors propose to express the weight of a convolutional neural network as a coupled Tucker decomposition. The Tucker formulation allows for an efficient reformulation. Notation should be unified in the text and captions (e.g.Table 1).In 3.2, when specifying the size of G, should it be G_r? It was not introduced.<BRK>Summary This paper proposes to learn simultaneously all the parameters of grouped convolutions by factorizing the weights of the convolutions as a sum of lower rank tensors. This enables architecture search for the convolution parameters in a differentiable and efficient way. As the authors present it in the abstract and introduction as an alternative to NAS, I believe a comparison to a NAS would be needed. Comments I think was paper is well written, and was clear at least until 3.2.<BRK>Summary: The authors introduce GROSS a reformulation of block tensor decomposition, which allows multiple grouped convolutions (with varying group sizes) to be trained simultaneously. However, it is quite niche and while the authors frame it as a form of NAS in my view, this contribution is more in the realm of hyperparameter search for grouped convolutions, and not NAS in general. In addition, the empirical results are relatively shallow, with only one dataset and without detailed discussion of the variance of the results.<BRK>they present Group-size Series (GroSS) decomposition, a mathematical formulation of tensor factorisation into a series of approximations of increasing rank terms. GroSS allows for dynamic and differentiable selection of factorisation rank, which is analogous to a grouped convolution. Therefore, to the best of their knowledge, GroSS is the first method to simultaneously train differing numbers of groups within a single layer, as theyll as all possible combinations bettheyen layers. In doing so, GroSS trains an entire grouped convolution architecture search-space concurrently. they demonstrate this with a proof-of-concept exhaustive architecure search with a performance objective. GroSS represents a significant step towards liberating network architecture search from the burden of training and finetuning.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. Summary The authors propose a novel meta rl problem where hierarchical tasks are characterized by a graph describing all sub tasks and their dependencies. They propose a meta rl approach to meta train a policy that quickly infers the subtask graph from new task data. In particular, the authors consider a large scale Startcraft II experiment which proves the efficiency and scalability of the proposed methodology. This work presents interesting and novel ideas in these settings. The experiments are quite interesting and convincing. I did not find any plot reporting it.<BRK>The authors also propose a UCB inspired intrinsic reward to encourage exploration when optimizing the adaptation policy. Overall, this paper is mainly an extension of the prior work [1], which uses a subtask graph for tackling hierarchical RL problems. This work builds upon [1] by extending to meta learning domains and studying generalization to new hierarchical tasks. While the contribution seems a bit incremental and the experimental setting is a bit unclear and limited to low dimensional state space, the inference of task specific subtask graphs based on past experiences and the proposal of a UCB inspired reward shed some interesting insights on how to approach meta hierarchical RL where long horizon tasks and sparse rewards have been major challenges. I also don t understand why MSGI Meta and RL2 would overfit in the SC2LE case and are unable to adapt to new tasks. It seems that the authors don t use a meta RL agent in order to get this domain to work. In NeurIPS, pp. 7156–7166, 2018.<BRK>The main problem that is tackled here are tasks that have a main goal that can only be reached by solving prerequisite tasks. They test their method on a simple game and a very complex one. Methodology and noveltyThe authors combine various techniques (subtask graph inference, gradient based meta learning and inductive logic programming). What is the big difference from the work by Sohn et al.(2018)?ExperimentsThe authors evaluated one agent. It would have been better if they trained multiple agents and showed a performance distribution, so it is clear that the performance is not just achieved by luck (Fig 5.). PresentationFigure 3 does not give a description of the subtask graph (middle) and the StarCraft II.<BRK>they propose and address a novel few-shot RL problem, where a task is characterized by a subtask graph which describes a set of subtasks and their dependencies that are unknown to the agent. The agent needs to quickly adapt to the task over few episodes during adaptation phase to maximize the return in the test phase. Instead of directly learning a meta-policy, they develop a Meta-learner with Subtask Graph Inference (MSGI), which infers the latent parameter of the task by interacting with the environment and maximizes the return given the latent parameter. To facilitate learning, they adopt an intrinsic reward inspired by upper confidence bound (UCB) that enctheirages efficient exploration. their experiment results on two grid-world domains and StarCraft II environments show that the proposed method is able to accurately infer the latent task parameter, and to adapt more efficiently than existing meta RL and hierarchical RL methods.
Accept (Poster). rating score: 8. rating score: 6. rating score: 1. This paper proposes a new type of weakly supervised clustering / multiple instance learning (MIL) problem in which bags of instances (data points) are labeled with a "unique class count (UCC)*, rather than any bag level or instance level labels. The paper also provides a theoretical argument for why this approach is feasible. In slightly more detail:  (1) MIL where we are given bag level labels only is a well studied problem that occurs in many real world settings, such as the histopathology one used in this paper. However to this reader s knowledge, this is a new variant that is both creative and motivated by an actual real world study, which is exciting and alone warrants presentation at the conference in my opinion. (2) The theoretical treatment is high level, but still serves a clear purpose of establishing feasibility of the proposed method  this modest and appropriate purpose serves the paper well. Either way, it seems that a different approach than the KDE layer could have been taken  this should be added to the ablation experiments<BRK>The authors present a novel weakly supervised multiple instance learning model based on a bag level label called unique class count(UCC). The core content of the method is to learn mapping between bags and their associated bag level ucc labels and then to predict the ucc labels of unseen bags. Positive:(1) The authors use the unique class count(UCC) in the bag as a weak, bag level label to design a deep learning based UCC model for extracting features and clustering the individual instances in the unseen bags. (2) In this paper, a large number of experiments show the effectiveness of the algorithm in different data sets and semantic segmentation of breast cancer metastases. The authors’ proofs are not enough to prove whether the designed UCC classifier is perfect.<BRK>This paper proposes a MIL clustering method. The proposed MIL setup is called "unique class count (ucc)", this is, for a bag os samples ucc is the number of clusters in the bag. Once trained on a dataset the method can perform clustering on classes (classify) better than a fully unsupervised clustering algorithm and worse than a fully supervised model. The method is evaluated on MNIST, CIFAR10, CIFAR100  and on binary breast cancer segmentation. The main table of the paper (Table 1) compares the results of the proposed methods only with unsupervised methods and a fully supervised one. The results are reasonable. However, not a fair comparison. UCC model uses bags of sizes from 1 to 4. Moreover, during training, I guess that the same sample can go to different bags. And also, one could trivially get the full label of each sample.<BRK>A theyakly supervised learning based clustering framework is proposed in this paper. As the core of this framework, they introduce a novel multiple instance learning task based on a bag level label called unique class count (ucc), which is the number of unique classes among all instances inside the bag. In this task, no annotations on individual instances inside the bag are needed during training of the models. they mathematically prove that with a perfect ucc classifier, perfect clustering of individual instances inside the bags is possible even when no annotations on individual instances are given during training. they have constructed a neural network based ucc classifier and experimentally shown that the clustering performance of their framework with their theyakly supervised ucc classifier is comparable to that of fully supervised learning models where labels for all instances are known. Furthermore, they have tested the applicability of their framework to a real world task of semantic segmentation of breast cancer metastases in histological lymph node sections and shown that the performance of their theyakly supervised framework is comparable to the performance of a fully supervised Unet model.
Reject. rating score: 3. rating score: 3. rating score: 3. rating score: 6. SummaryThis paper proposed a new graph pooling method based on the Haar basis on graphs. Therefore, I judge the paper as a border, tending to reject for now. Suggestions  The main part of the paper is 10 pages long. Another idea is to demonstrate it empirically by providing time and memory consumption to the experiment results. page 13, Appendix B, (10)	   I think "1" in this equation is an all one vector.<BRK>Overall graph classification and regression tasks are quite important and this work provides a new way of transitioning from node based learning methods to full graph representations via hierarchical compression. Nonetheless, I find that the organization of the paper needs additional work and the experimental investigation is not sufficient and compelling enough. In particular I find the discussion of the HaarPooling step s computational performance not particularly appealing since other parts of the hierarchical approach are computationally expensive for example, the spectral clustering or GCN steps are costly.<BRK>This paper presents a new graph pooling method, called HaarPooling. 2) The main contributions of this paper are not clear to me, compared with other SOTAs. HaarPooling can be applied in conjunction with any type of graph convolution in GNNs. The writing, organization and presentation are satisfactory.<BRK>The paper presents a new approach called HaarPooling in the context of Deep Graph Neural Networks. The approach solves dmiensional problems of applying the same model on graphs of different size and shows how to contribute to high performance in a set of graph classification tasks, while having low computational complexity. The paper is very well written. An explanation on this could be beneficial.<BRK>Deep Graph Neural Networks (GNNs) are instrumental in graph classification and graph-based regression tasks. In these tasks, graph pooling is a critical ingredient by which GNNs adapt to input graphs of varying size and structure. they propose a new graph pooling operation based on compressive Haar transforms, called HaarPooling. HaarPooling is computed following a chain of sequential clusterings of the input graph. The input of each pooling layer is transformed by the compressive Haar basis of the corresponding clustering. HaarPooling operates in the frequency domain by the synthesis of nodes in the same cluster and filters out fine detail information by compressive Haar transforms. Such transforms provide an effective characterization of the data and preserve the structure information of the input graph. By the sparsity of the Haar basis, the computation of HaarPooling is of linear complexity. The GNN with HaarPooling and existing graph convolution layers achieves state-of-the-art performance on diverse graph classification problems.
Reject. rating score: 1. rating score: 3. rating score: 3. This paper first provides a theoretical interpretation of separation rank as a measure of a recurrent network s ability to capture contextual dependencies in natural language text. The analysis is primarily done in the context of tensor space language models (TSLM) that was demonstrated to be a generalisation of n gram language models in earlier work. This dichotomy is overly simplistic and ultimately false. 5.The paper mostly fails to report performance comparison with existing numbers from prior work.<BRK>The goal of this work is quite interesting, but the reviewer feel a bit challenging to follow the writing. But the paper does not introduce TSLM in detail, making the part relevant to TSLM quite inaccessible. The analysis of the work is based on the model in (2), which is merely an approximation for the joint probability. Since the authors are considering a sequence, this may be related to de finetti s theorem and its extensions. If one wishes that the SVD reveals the rank K, K has to be smaller than the outer dimensions of the tensor T. This was not specified in the statement. We know in NLP pre trained word embeddings may be more useful. Did all the experiments use one hot embedding? The writing is a bit hard to access and the proofs might be a bit loose (did not check all of them.But Claim 1 is already a bit loose).<BRK>This paper derives lower bounds on the separation rank of a wider class of recurrent NLP models in terms of its depth and number of hidden layers, demonstrating that both the number of hidden units as well as the number of layers improves the ability of NLP networks to model context dependency. It then introduces a novel bidirectional NLP variant that is supposed to capture a good trade off between computational cost and performance. The improvements of the bidirectional models also seem to be minor, but no standard deviations for the performance results are reported. A clear description as to which language models are captured by the TSLM model is missing. Finally, the title does not reflect the content of the paper (there is nothing interpretable about the network structure).<BRK>Neural language models have achieved great success in many NLP tasks, to a large extent, due to the ability to capture contextual dependencies among terms in a text. While many efforts have been devoted to empirically explain the connection bettheyen the network hyperparameters and the ability to represent the contextual dependency, the theoretical analysis is relatively insufficient. Inspired by the recent research on the use of tensor space to explain the neural network architecture, they explore the interpretable mechanism for neural language models. Specifically, they define the concept of separation rank in the language modeling process, in order to theoretically measure the degree of contextual dependencies in a sentence. Then, they show that the lotheyr bound of such a separation rank can reveal the quantitative relation bettheyen the network structure (e.g. depth/width) and the modeling ability for the contextual dependency. Especially, increasing the depth of the neural network can be more effective to improve the ability of modeling contextual dependency. Therefore, it is important to design an adaptive network to compute the adaptive depth in a task. Inspired by Adaptive Computation Time (ACT), they design an adaptive recurrent network based on the separation rank to model contextual dependency. Experiments on various NLP tasks have verified the proposed theoretical analysis. they also test their adaptive recurrent neural network in the sentence classification task, and the experiments show that it can achieve better results than the traditional bidirectional LSTM.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. rating score: 3. The question addressed is that SOTA NLI models tend to lead tohigher confidence when some parts are deleted from the "premise". It is a problem known as under sensitivity. The idea of Interval Bound Propagation (IBP) is to use interval arithmetic to propagateintervals and bound the variation of the target basedon variation of the input. The paper is well written and easy to follow.<BRK>This paper proposes a model to verify the robustness of NLP models (change in the original probability), more specifically DAM, in the case of word removals in the input. The upper bound at the final layer is then compared with the label probability of the original input to assess if the probability increases or not. Overall, the paper is well written and the idea of using IBP with an attentive model seems to work empirically for SNLI datasets.<BRK>  Overall  This submission tackles to verify the “under sensitivity” problem of neural network models in the natural language inference by ensuring modes do not become more confident in the predictions when arbitrary subsets of words from the input text are deleted. The authors developed new verification approaches based on decomposable attention mechanism with interval bound propagation (IBP), which can prove the under sensitivity issue given a model and a particular sample.<BRK>This work is an application of interval bound propagation on evaluating the robustness of NLI model. This work is well motivated, assuming that the confidence of a neural model should be lower when part of the sentence is missed. However, the application of vanilla IBP is quite limited in certain model architectures.<BRK>Neural networks are widely used in Natural Language Processing, yet despite their empirical successes, their behavitheir is brittle: they are both over-sensitive to small input changes, and under-sensitive to deletions of large fractions of input text. This paper aims to tackle under-sensitivity in the context of natural language inference by ensuring that models do not become more confident in their predictions as arbitrary subsets of words from the input text are deleted. they develop a novel technique for formal verification of this specification for models based on the popular decomposable attention mechanism by employing the efficient yet effective interval bound propagation (IBP) approach. Using this method they can efficiently prove, given a model, whether a particular sample is free from the under-sensitivity problem. they compare different training methods to address under-sensitivity, and compare metrics to measure it. In their experiments on the SNLI and MNLI datasets, they observe that IBP training leads to a significantly improved verified accuracy. On the SNLI test set, they can verify 18.4% of samples, a substantial improvement over only 2.8% using standard training.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 6. rating score: 6. This paper provides both theoretical insights and empirical demonstration of this remarkable property.<BRK>In terms of discoverability, the authors would do the community a service by titling the paper in such a way that it captures the set up well. Could the authors explain this?<BRK>Could the authors explain this? In terms of discoverability, the authors would do the community a service by titling the paper in such a way that it captures the set up well.<BRK>%%% Update to the review %%%Thanks for your clarification and the revision   the paper looks good!<BRK>Intriguing empirical evidence exists that deep learning can work theyll with exotic schedules for varying the learning rate. This paper suggests that the phenomenon may be due to Batch Normalization or BN(Ioffe & Szegedy, 2015), which is ubiq- uitous and provides benefits in optimization and generalization across all standard architectures. The following new results are shown about BN with theyight decay and momentum (in other words, the typical use case which was not considered in earlier theoretical analyses of stand-alone BN (Ioffe & Szegedy, 2015; Santurkar et al., 2018; Arora et al., 2018)
• Training can be done using SGD with momentum and an exponentially in- creasing learning rate schedule, i.e., learning rate increases by some (1 + α) factor in every epoch for some α > 0. (Precise statement in the paper.) To the best of their knowledge this is the first time such a rate schedule has been successfully used, let alone for highly successful architectures. As ex- pected, such training rapidly blows up network theyights, but the net stays theyll-behaved due to normalization.
• Mathematical explanation of the success of the above rate schedule: a rigor- ous proof that it is equivalent to the standard setting of BN + SGD + Standard Rate Tuning + theyight Decay + Momentum. This equivalence holds for other normalization layers as theyll, Group Normalization(Wu & He, 2018), Layer Normalization(Ba et al., 2016), Instance Norm(Ulyanov et al., 2016), etc.
• A worked-out toy example illustrating the above linkage of hyper- parameters. Using either theyight decay or BN alone reaches global minimum, but convergence fails when both are used.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. This paper makes an interesting theoretical contribution; namely, that SGD with momentum (and with a slight modification to the step size rule) is guaranteed to quickly converge to a second order stationary point, implying it quickly escapes saddle points. SGD with momentum is widely used in the practice of deep learning, but a theoretical analysis has remained largely elusive. This may also help to understand some of the limitations of this analysis.<BRK>In particular, the main contribution of the paper, as the authors claim in the abstract, is showing that stochastic momentum improves deep network training because it modifies SGD to escape saddle points faster. 3) The presentation of Section 3.2.1 is also not clear. I am suggesting to the authors to explain in more details the theoretical results of their paper and highlight why the 5 lemmas of this section are important to be in the main part of the paper. "Accelerated gossip via stochastic heavy ball method."<BRK>*Summary*This paper studies the impact of momentum for escaping Saddle points with SGD (+momentum) in a non convex optimization setting. They prove that using a large momentum value (i.e.close to one) provides a better constant for the convergence rate to second order stationary points. The approach is well motivated by the current seek for a better understanding of the training methods used in practical deep learning. However, I am not fully convinced that this work provides a results that exhibits a regime where momentum helps to escape saddle point faster (or to converge faster).<BRK>Stochastic gradient descent (SGD) with stochastic momentum is popular in nonconvex stochastic optimization and particularly for the training of deep neural networks. In standard SGD, parameters are updated by improving along the path of the gradient at the current iterate on a batch of examples, where the addition of a ``momentum'' term biases the update in the direction of the previous change in parameters. In non-stochastic convex optimization one can show that a momentum adjustment provably reduces convergence time in many settings, yet such results have been elusive in the stochastic and non-convex settings. At the same time, a widely-observed empirical phenomenon is that in training deep networks stochastic momentum appears to significantly improve convergence time, variants of it have fltheirished in the development of other popular update methods, e.g. ADAM, AMSGrad, etc. Yet theoretical justification for the use of stochastic momentum has remained a significant open question. In this paper they propose an anstheyr: stochastic momentum improves deep network training because it modifies SGD to escape saddle points faster and, consequently, to more quickly find a second order stationary point. their theoretical results also shed light on the related question of how to choose the ideal momentum parameter--their analysis suggests that $\beta \in [0,1)$ should be large (close to 1), which comports with empirical findings. they also provide experimental findings that further validate these conclusions.
Reject. rating score: 3. rating score: 6. rating score: 6. In general, this paper follows the min max training framework for adversarial robustness. Instead of using a gradient based attack to solve the inner maximization, the authors use a neural network to learn the attack results. Since this is a new way of robust training and there is no certified guarantee, I suggest the authors refer [1] to evaluate the effectiveness of the defense more thoroughly to convince the readers that it really works.<BRK>The authors propose a framework where one component is an attacker network that keeps learning about how to perturb the loss more, and one component is a defense network that robustify learning with respect to the attacker network. I have read the rebuttal and thank the authors for the response. Instead of just checking the differences of the attacking examples generated, can we take the inner attacker and see if it is more effective in attacking than PGM and CW? However, we see very little information on how to train the framework in the paper.<BRK>The paper proposes a new way of adversarial training by placing another neural network called "attacker" network, and let the attacker to learn how to generate adversarial examples during training. This training scheme is formulated to solving a joint training according to min max problem.<BRK>Adversarial training provides a principled approach for training robust neural networks. From an optimization perspective, the adversarial training is essentially solving a minimax robust optimization problem. The outer minimization is trying to learn a robust classifier, while the inner maximization is trying to generate adversarial samples. Unfortunately, such a minimax problem is very difficult to solve due to the lack of convex-concave structure. This work proposes a new adversarial training method based on a generic learning-to-learn (L2L) framework. Specifically, instead of applying the existing hand-designed algorithms for the inner problem, they learn an optimizer, which is parametrized as a convolutional neural network. At the same time, a robust classifier is learned to defense the adversarial attack generated by the learned optimizer. their experiments over CIFAR-10 and CIFAR-100 datasets demonstrate that the L2L outperforms existing adversarial training methods in both classification accuracy and computational efficiency. Moreover, their L2L framework can be extended to the generative adversarial imitation learning and stabilize the training.
Reject. rating score: 1. rating score: 3. rating score: 6. rating score: 6. This paper proposes a deep neural network solution to the set ranking problem. However, I have several concerns about the paper. Given the input sets A and B, how are the “current utility” computed. In Eq.(6) it seems that only one of R and P is multiplied by a non zero coefficient. For the real world experiments, the method seems to perform marginal better on average. Also note that the baselines are not deep neural networks, so do not leverage the capability of automatic differentiation and optimization. The authors’ adaptation for Rank Centrality involves summing up the weights; this does not model their interaction and seem to be a weak baseline. I was confused about the notation. I think this is unnecessarily cumbersome because the current work seems to be a natural instantiation of a modern neural network to this task. Alternatively if the authors can explain the connection more concretely (i.e some parameters learned by the neural network recovers models in prior work) the arguments can be convincing.<BRK>The experiments show the efficacy of the proposed methods. I have some major concerns with this paper. The paper presentation is not very clear. The paper contains many imprecise statements. Also, the presentation of the paper could be further improved. Therefore, I would keep the same score. This adds to the confusion. 2.The paper mentions multiple times that it does not use statistical models tailored for a dataset or application but instead use deep neural networks. In my opinion, NN is just another form of statistical model that captures statistical patterns of comparisons and in group interaction. The authors may want to rephrase those statements.<BRK>This paper proposed a novel architecture to tackle the match prediction problem. The G module takes the final utility estimates of the individuals in a given group comparison as input and produces the winning probability estimate of one group preferred over the other in the given group comparison as output. I would recommend a weak accept for this paper based on the following reasons:* Both the R/P and G modules  input and output dimensions are independent of the number of items. * The theoretical foundation is sound. I would hope that the authors will make some effort in making the paper more approachable and  practical:* A more detailed motivation section for the architecture will make it much easier for the readers to understand. * A conclusion plus future work section could come in handy for future researchers.<BRK>This paper attempts to solve match prediction problem, i.e., whether a group is preferred over the other. The key challenge is "consistency" since it s hard to find the universal pattern over tasks. Instead, this paper propose to learn reward and penalty modules and both vary when the underlying model changes. My comments:[1] The paper is well written[2] This paper tries to solve an interesting problem but the application is a bit limited[3] It would be great to conduct ablation study, e.g., analyze efforts of different components (R, P, G)<BRK>they explore the match prediction problem where one seeks to estimate the likelihood of a group of M items preferred over another, based on partial group comparison data. Challenges arise in practice. As existing state-of-the-art algorithms are tailored to certain statistical models, they have different best algorithms across distinct scenarios. Worse yet, they have no prior knowledge on the underlying model for a given scenario. These call for a unified approach that can be universally applied to a wide range of scenarios and achieve consistently high performances. To this end, they incorporate deep learning architectures so as to reflect the key structural features that most state-of-the-art algorithms, some of which are optimal in certain settings, share in common. This enables us to infer hidden models underlying a given dataset, which govern in-group interactions and statistical patterns of comparisons, and hence to devise the best algorithm tailored to the dataset at hand. Through extensive experiments on synthetic and real-world datasets, they evaluate their framework in comparison to state-of-the-art algorithms. It turns out that their framework consistently leads to the best performance across all datasets in terms of cross entropy loss and prediction accuracy, while the state-of-the-art algorithms suffer from inconsistent performances across different datasets. Furthermore, they show that it can be easily extended to attain satisfactory performances in rank aggregation tasks, suggesting that it can be adaptable for other tasks as theyll.
Reject. rating score: 1. rating score: 3. rating score: 3. To sum up, the reported results in the paper are not convincing to me. The idea is interesting in the sense that an efficient self masking technique is proposed to generate user adaptive hash codes, by differentiating the importance of binary bits, for a fast recommendation system.<BRK>And some presentation in the parameter setting are not clear, e.g.,  is chosen consistently across all data sets ,  was consistently chosen .<BRK>The whole paper, especially the model, is not presented well.<BRK>Hashing-based collaborative filtering learns binary vector representations (hash codes) of users and items, such that recommendations can be computed very efficiently using the Hamming distance, which is simply the sum of differing bits bettheyen two hash codes. A problem with hashing-based collaborative filtering using the Hamming distance, is that each bit is equally theyighted in the distance computation, but in practice some bits might encode more important properties than other bits, where the importance depends on the user. 
To this end, they propose an end-to-end trainable variational hashing-based collaborative filtering approach that uses the novel concept of self-masking: the user hash code acts as a mask on the items (using the Boolean AND operation), such that it learns to encode which bits are important to the user, rather than the user's preference towards the underlying item property that the bits represent. This allows a binary user-level importance theyighting of each item without the need to store additional theyights for each user. they experimentally evaluate their approach against state-of-the-art baselines on 4 datasets, and obtain significant gains of up to 12% in NDCG. they also make available an efficient implementation of self-masking, which experimentally yields <4% runtime overhead compared to the standard Hamming distance.
Reject. rating score: 1. rating score: 1. rating score: 1. The experiments seem not totally convincing. Most critically, the method does not seem to exhibit state of the art performance as claimed, but is somewhat lower (in terms of accuracy) than other baselines. It s claimed that these are the only datasets with user and item content, but why are both needed to run an experiment?<BRK>This paper omits the related work part and does a rough introduction to two baselines (CDL and DropoutNet) in a confusing way in Section 2. The experiments do not provide convincing evidence of the corretness of the proposed method, especially in Section 3.3.<BRK>The DropoutNet method in your experiments is very similar to CDL in terms of accuracy. Still, I’d strongly recommend adding to your experiments comparison with simpler hybrid models, e.g., Factorization Machines. Major drawback of the work is that it does not provide any quantitative evidence to support the main claim – that the proposed approach is at least more computationally efficient, since it underperforms competing methods in terms of accuracy. I would therefore suggest rejecting the work. 2)	A lot of attention is given to the “marketing application”. There are many typos and error both in text and in derivations.<BRK>Cold-start and efficiency issues of the Top-k recommendation are critical to large-scale recommender systems. Previous hybrid recommendation methods are effective to deal with the cold-start issues by extracting real latent factors of cold-start items(users) from side information, but they still suffer low efficiency in online recommendation caused by the expensive similarity search in real latent space. This paper presents a collaborative generated hashing (CGH) to improve the efficiency by denoting users and items as binary codes, which applies to various settings: cold-start users, cold-start items and warm-start ones. Specifically, CGH is designed to learn hash functions of users and items through the Minimum Description Length (MDL) principle; thus, it can deal with various recommendation settings. In addition, CGH initiates a new marketing strategy through mining potential users by a generative step. To reconstruct effective users, the MDL principle is used to learn compact and informative binary codes from the content data. Extensive experiments on two public datasets show the advantages for recommendations in various settings over competing baselines and analyze the feasibility of the application in marketing.
Reject. rating score: 1. rating score: 1. rating score: 1. This submission proposes NORML, a meta learning method that 1) learns initial parameters for a base model that leads to good few shot learning performance and 2) where a recurrent neural network (LSTM) is used to control the learning updates on a small support set for a given task. Same in fact can be said about many of the results reported in Table 1. Limited novelty: it is a fairly incremental variation compared to the Meta Learner LSTM (Ravi & Larochelle).<BRK>Minor:  page 1: Supervised few shot learning "aims to challenge machine learning models to ...": It does not challenge ML models; it is a specific ML paradigm. Theoretical and technical novelty is rather minimal. page 3: "The LSTM based meta learner proposed in this work, allow gradients to"   "Memory based Under review as a conference paper at ICLR 2020 methods (Ravi & Larochelle (2017)) that use all of the learner’s parameters as input to a meta learner tend to break down when using a learner with a large number of parameters (Andrychowicz et al., 2016).<BRK>This paper proposes a meta learner that learns how to make parameter updates for a model on a new few shot learning task. Ravi & Larochelle. ICLR 2017. The LSTM based meta learner proposed in this work, allow gradients to effectively flow through a large number of update steps. Because of these issues, it is hard to evaluate whether the idea proposed is of significant benefit.<BRK>Meta-learning is an exciting and potheyrful paradigm that aims to improve the effectiveness of current learning systems. By formulating the learning process as an optimization problem, a model can learn how to learn while requiring significantly less data or experience than traditional approaches. Gradient-based meta-learning methods aims to do just that, hotheyver recent work have shown that the effectiveness of these approaches are primarily due to feature reuse and very little has to do with priming the system for rapid learning (learning to make effective theyight updates on unseen data distributions). This work introduces Nodal Optimization for Recurrent Meta-Learning (NORML), a novel meta-learning framework where an LSTM-based meta-learner performs neuron-wise optimization on a learner for efficient task learning. Crucially, the number of meta-learner parameters needed in NORML, increases linearly relative to the number of learner parameters. Allowing NORML to potentially scale to learner networks with very large numbers of parameters. While NORML also benefits from feature reuse it is shown experimentally that the meta-learner LSTM learns to make effective theyight updates using information from previous data-points and update steps.
Reject. rating score: 1. rating score: 3. rating score: 6. The key idea is to set up a multi dimensional objective, where one dimension is about prediction accuracy and another about fairness. However, there may be several problems with this approach:1) For a causal estimate to be valid we need several assumptions. These assumptions are not discussed in the paper. Furthermore, the particular context of the paper, where the treatment is actually an immutable characteristic, makes such discussion much more subtle. Some other problems:  What is the reason for focusing on  neural classifiers ? What exactly is a "sensitive attribute"? In fact, many scientists would agree that causal inference is impossible without manipulation.<BRK>While the proposed method makes sense, I am not sure what exactly their contribution is. It is kind of clear that Pareto optimal exists, and what they did is to run the experiments multiple times with multiple \lambda values for the Chebyshev method and plot the Pareto optimal front. Just use test (validation) set to estimate the accuracy & fairness, then plot the results on the 2d plane.<BRK>The authors propose a novel joint optimisation framework that attempts to optimally trade off between accuracy and fairness objectives, since in its general formal counterfactual fairness is at odds with classical accuracy objective. As main contributions, the paper provides:* A Pareto objective formulation of the accuracy fairness trade off* A new causal fairness objective based on the existing Weighted Average Treatment Effect (WATE) and Average Treatment Effect for the Overlap Population (ATO)Overall, I think the paper makes an interesting contribution to the field of fairness and that the resulting method seems quite attractive for a real world practitioners.<BRK>That machine learning algorithms can demonstrate bias is theyll-documented by now. This work confronts the challenge of bias mitigation in feedforward fully-connected neural nets from the lens of causal inference and multiobjective optimisation. Regarding the former, a new causal notion of fairness is introduced that is particularly suited to giving a nuanced treatment of datasets collected under unfair practices. In particular, special attention is paid to subjects whose covariates could appear with substantial probability in either value of the sensitive attribute.  Next, recognising that fairness and accuracy are competing objectives, the proposed methodology uses techniques from multiobjective optimisation to ascertain the fairness-accuracy landscape of a neural net classifier. Experimental results suggest that the proposed method produces neural net classifiers that distribute evenly across the Pareto front of the fairness-accuracy space and is more efficient at finding non-dominated points than an adversarial approach.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. This paper analyzed which learning rate schedule (LRS) should be used when the budget (number of iteration) is limited. While there is some concern regarding the significance of novelty,  the paper seems meaningful enough to be accepted. 2.Various experimental results show that the simple linear decay method works well, and it might become a baseline method for future budgeted training solutions (assuming there is no similar work with the same purpose).<BRK>This work presents a simple technique for tuning the learning rate for Neural Network training when under a "budget"   the budget here is specified as a fixed number of epochs that is expected to be a small fraction of the total number of epochs required to achieve maximum accuracy. In this family of schedules, the paper shows that a simple linear decay works best for all budgets.<BRK>Pros:The paper is clearly written. Actually the linear decay schedule changes may simply find a good learning rate during training as long as the initial learning rate is larger than the optimal one. Nevertheless,  the problem setting and the observations could be beneficial to the community for further discussion, So I raised my score to weak accept. I would like to see other lr schedules in Table 2. My major concern for this work is a lack of deeper understanding about the reason why linear LR schedule works better, if any. What is the lr decay unit for linear schedule? I would expect the convergence to be related with number of iterations.<BRK>In most practical settings and theoretical analyses, one assumes that a model can be trained until convergence. Hotheyver, the growing complexity of machine learning datasets and models may violate such assumptions. Indeed, current approaches for hyper-parameter tuning and neural architecture search tend to be limited by practical restheirce constraints. Therefore, they introduce a formal setting for studying training under the non-asymptotic, restheirce-constrained regime, i.e., budgeted training. they analyze the following problem: "given a dataset, algorithm, and fixed restheirce budget, what is the best achievable performance?" they focus on the number of optimization iterations as the representative restheirce. Under such a setting, they show that it is critical to adjust the learning rate schedule according to the given budget. Among budget-aware learning schedules, they find simple linear decay to be both robust and high-performing. they support their claim through extensive experiments with state-of-the-art models on ImageNet (image classification), Kinetics (video classification), MS COCO (object detection and instance segmentation), and Cityscapes (semantic segmentation). they also analyze their results and find that the key to a good schedule is budgeted convergence, a phenomenon whereby the gradient vanishes at the end of each allotheyd budget. they also revisit existing approaches for fast convergence and show that budget-aware learning schedules readily outperform such approaches under (the practical but under-explored) budgeted training setting.
Reject. rating score: 3. rating score: 3. rating score: 6. Each protein folds to a 3D structure. It is well known that the function of a protein is determined by its 3D structure. Different models perform well for different kinds of proteins. Pros: Their methods perform better than comparable methods using 1D and 3D CNNs. Overall, this is a good application paper showing the application of a known technique to solve a problem in a new domain. Cons: The novelty is minimal and the problem is of interest only to specialists in this domain.<BRK>This manuscript describes a new deep learning method for the prediction of the quality of a protein 3D model in the absence of the experimental 3D structure of the protein under study. The major idea is to model a protein 3D model using a graph. Based upon this graph representation, the manuscript describes a graph convolutional neural network (GCN) to predict both local (i.e., per residue) and global quality. Unfortunately, there is no experimental result on CASP13 models, which significantly reduce my interest on this paper.<BRK>The paper proposes use of graph convolutional networks (GCN) for quality assessments (QA) of protein structure predictions. As a result, there is an interest in guessing quality of protein structure predictions on protein families that are not characterized experimentally. + The protein representation is reasonable. Overall, this is a borderline paper. There is little methodological novelty and the QA application is a bit of a niche problem in bioinformatics. However, the results show a decent improvement over the state of the art in this particular application, so this paper might be of importance for a limited audience interested in this problem.<BRK>Proteins are ubiquitous molecules whose function in biological processes is determined by their 3D structure.
Experimental identification of a protein's structure can be time-consuming, prohibitively expensive, and not always possible. 
Alternatively, protein folding can be modeled using computational methods, which hotheyver are not guaranteed to always produce optimal results.
GraphQA is a graph-based method to estimate the quality of protein models, that possesses favorable properties such as representation learning, explicit modeling of both sequential and 3D structure, geometric invariance and computational efficiency. 
In this work, they demonstrate significant improvements of the state-of-the-art for both hand-engineered and representation-learning approaches, as theyll as carefully evaluating the individual contributions of GraphQA.
Reject. rating score: 1. rating score: 1. rating score: 6. The work poses an interesting question: Are GCNs (and GNNs) just special types of matrix factorization methods? But [1] shows that GCNs and GNNs are fundamentally different from matrix factorization methods, regardless of the loss function used to learn the embeddings. Matrix factorization (as broadly understood) will give embeddings that can even be used to cluster nodes. Hence, GCNs are not matrix factorization methods. I think the paper is a valiant effort, but unfortunately the core premise is incorrect. Deeper insights into graph convolutional networks for semi supervised learning.<BRK>The approach is closely related to GCN. 1.The wording "unifying" is a misnomer. The title "unifying graph convolutional networks" hallucinates a framework that unifies several neural network architectures, which is not precise. The derivation of this term starts from GCN and a Laplacian smoothing argument, and arrives at a matrix factorization form through a series of modeling modifications. The paper does not present a theoretical analysis. The derivation of the matrix factorization is only a modeling process. Supplementing a convergence plot and comparing the two approaches may help, if the alternating approach is indeed better. All information should be reported.<BRK>As far as I am concerned, the connection is closer to node embedding versus matrix factorization. It would be interesting to see if joint training the two loss in mini batch among a node and its neighbors can leads to any difference. The connection of GCN to MF is very indirect. It is an interesting and innovative idea to draw connection between GCN and MF.<BRK>In recent years, substantial progress has been made on graph convolutional networks (GCN). In this paper, for the first time, they theoretically analyze the connections bettheyen GCN and matrix factorization (MF), and unify GCN as matrix factorization with co-training and unitization. Moreover, under the guidance of this theoretical analysis, they propose an alternative model to GCN named Co-training and Unitized Matrix Factorization (CUMF). The correctness of their analysis is verified by thorough experiments. The experimental results show that CUMF achieves similar or superior performances compared to GCN. In addition, CUMF inherits the benefits of MF-based methods to naturally support constructing mini-batches, and is more friendly to distributed computing comparing with GCN. The distributed CUMF on semi-supervised node classification significantly outperforms distributed GCN methods. Thus, CUMF greatly benefits large scale and complex real-world applications.
Reject. rating score: 1. rating score: 8. rating score: 8. The paper proposes, when given a CNN, an image and its label, a measure called angular visual hardness (AVH). The paper shows that AVH correlates with human selection frequency (HSF) [RRSS19]. (More on this in Con2)2. The presentation is confusing, and at times self contradictory. Do they?I would suggest reporting the angular separation of the category weights (maybe by showing them in a CxC matrix). In particular, in Section 2 on the related work from psychology/neuroscience, little specifics are discussed to contextualize the current work. 3.In Definition 1, “for any x”  > for any (x, y). 2.Try to be more concise and more precise in the presentation. What is human visual hardness (HVH)? (To authors and other reviewers) Please do not hesitate to directly point out my misunderstandings.<BRK>The authors compared the correlation between AVH and human selection frequency with model confidence and feature norm. The results show that both AVH and model confidence have correlation, but AVH has a stronger correlation than model confidence. As an application of AVH, the authors applied it to sample selection of self training for domain adaption. Overall, the experimental contribution of this paper is good, and the experimental conditions seem to be correct. In the analysis of the dynamics of training, the authors compared the AVH with feature norm. How about the dynamics of model confidence? The curves of different levels of hardness are missing in Fig.14.<BRK>Main Contribution:This paper is trying to bridge the gap between CNN and the human  visual system by proposing a metric  (angular visual distance) and validate that this metric is correlated to the human visual hardness and this metric has a stronger relation  compared to the softmax score which has been viewed as a metric measuring the hardness of images in CNNs. The observation is quite intuitive and has strong theoretical foundation, which is the main reason that I vote for the acceptance of this paper. Otherwise, if these two are fundamentally different with each other, what is the point of showing some connections between them? For the sentence "we use this dataset to verify our hypothesis", what is the hypothesis? 3.For the experiment, I would like to recommend authors adding the following experiments. 3.2) Introducing several other measurements to show the correlation. 3.3) I also would like to see similar results in Table 1 for different models.<BRK>The mechanisms behind  human visual systems and convolutional neural networks (CNNs) are vastly different. Hence, it is expected that they have different notions of ambiguity or hardness. In this paper, they make a surprising discovery: there exists a (nearly) universal score function for CNNs whose correlation with human visual hardness is statistically significant. they term this function as angular visual hardness (AVH) and in a CNN, it is given by the normalized angular distance bettheyen a feature embedding and the classifier theyights of the corresponding target category. they conduct an in-depth scientific study. they observe that CNN models with the highest  accuracy also have the best AVH scores. This agrees with an earlier finding that state-of-art models tend to improve on classification of harder training examples. they find that AVH displays interesting dynamics during training: it quickly reaches a plateau even though the training loss keeps improving. This suggests the need  for designing better loss functions that can  target harder examples more effectively. Finally, they empirically show significant improvement in performance by using AVH as a measure of hardness in self-training tasks.
 
Reject. rating score: 6. rating score: 6. rating score: 6. This paper proposes a multi task dynamical system for sequence generation. The model learns a number of parameters that represents the latent code z. The main motivation of this paper is to treat each sequence as a task in the training set (customization of the individual data sequence). The authors claim that the proposed approach provides grater data efficiency. However, I still think the experiments and the comparisons are unconvincing.<BRK><Strengths> + This paper proposes a new dynamic model named hierarchical multi task dynamical systems (MTDSs) as a latent sequence model that enables users to directly  control the output of data sequence via a latent code z. It may not be surprising that the multi task version is better than these two weak baselines. 1.The experiments are carried out with a simple toy dataset DHO and MOCAP data.<BRK>This paper proposes to add a latent variable to a dynamical, thereby encoding the notion of "task", eg, in Mocap, the latent variable could encode the walking style in an unsupervised manner. Overall I think this is an interesting paper, however I am not familiar with all the related work. The paper is clear and well written, the idea is interesting and the experiments seem well designed and convincing.<BRK>Dynamical system models (including RNNs) often lack the ability to adapt the sequence generation or prediction to a given context, limiting their real-world application. In this paper they show that hierarchical multi-task dynamical systems (MTDSs) provide direct user control over sequence generation, via use of a latent  code z that specifies the customization to the
individual data sequence. This enables style transfer, interpolation and morphing within generated sequences. they show the MTDS can improve predictions via latent code interpolation, and avoid the long-term performance degradation of standard RNN approaches.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. This paper extends transformers by enabling them to incorporate hierarchical structures like constituency trees structured attention with hierarchical accumulation. The paper is well written and well augmented with supportive figures.<BRK>This paper proposes a novel mechanism to leverage additional tree structure information into the transformer.<BRK>Summary: This paper describes an attention based method to encode trees with constant parallel time complexity maintaining scalability, uses this model to encode constituency parses of input sentences for the tasks of machine translation and text classification, and shows improved accuracy/bleu over models that do not encode the parses. Strengths:The method proposed by the authors is scalable despite encoding tree structures. The authors achieve an accuracy of 98.2 on the IMDB dataset. In other words, how much classification performance does this method yield over current SOTA models, because the SST results are quite a bit below the current SOTA.<BRK>Incorporating hierarchical structures like constituency trees has been shown to be effective for various natural language processing (NLP) tasks. Hotheyver, it is evident that state-of-the-art (SOTA) sequence-based models like the Transformer struggle to encode such structures inherently. On the other hand, dedicated models like the Tree-LSTM, while explicitly modeling hierarchical structures, do not perform as efficiently as the Transformer. In this paper, they attempt to bridge this gap with Hierarchical Accumulation to encode parse tree structures into self-attention at constant time complexity. their approach outperforms SOTA methods in ftheir IWSLT translation tasks and the WMT'14 English-German task. It also yields improvements over Transformer and Tree-LSTM on three text classification tasks. they further demonstrate that using hierarchical priors can compensate for data shortage, and that their model prefers phrase-level attentions over token-level attentions.
Reject. rating score: 1. rating score: 3. rating score: 6. This paper presented a new pooling method for learning graph level embeddings. Although I found this paper is generally well written and well motivated, there are several concerns about the novelty of paper, computational expenses, and the experimental results as listed below:1) The idea of using attention score is simple yet seems effective. 5) The experimental results are also problematic as well.<BRK>This paper proposed a topology aware pooling method to generate ranking scores for each node and so the pooling (or coarsening) of the graph can be achieved by picking those nodes with higher aggregated attention scores. Another important observation is that The proposed method seems to connect each hierarchy directly to the final layer, which is similar to skip connections. This is very similar to the proposed method, since those nodes with higher attention scores are those with more neighbors, which can be considered as ``local cluster centers’’ on a graph.<BRK>This paper proposes a topology aware pooling method on graph data, which explicitly encodes the topology information when computing ranking scores. Overall, this paper is well organized and the contributions are clear.<BRK>Pooling operations have shown to be effective on various tasks in computer vision and natural language processing. One challenge of performing pooling operations on graph data is the lack of locality that is not theyll-defined on graphs. Previous studies used global ranking methods to sample some of the important nodes, but most of them are not able to incorporate graph topology information in computing ranking scores. In this work, they propose the topology-aware pooling (TAP) layer that uses attention operators to generate ranking scores for each node by attending each node to its neighboring nodes. The ranking scores are generated locally while the selection is performed globally, which enables the pooling operation to consider topology information. To enctheirage better graph connectivity in the sampled graph, they propose to add a graph connectivity term to the computation of ranking scores in the TAP layer. Based on their TAP layer, they develop a network on graph data, known as the topology-aware pooling network. Experimental results on graph classification tasks demonstrate that their methods achieve consistently better performance than previous models.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. Overall, the paper is well written, and easy to follow (even for non expert like me). This paper propose a simple solution, which is well motivated, to the problem as well as a proof of convergence. It has also received significant attention from the community in the recent years. To address these issues, the authors introduce the "follow the ridge" algorithm for minimax optimization problems.<BRK>SummaryThe present work proposes a new algorithm, "Follow the Ridge" (FR) that uses second order gradient information to iteratively find local minimax points, or Stackelberg equilibria in two player continuous games. The right solution concept for GANs is not what the paper is about, but before publication the authors should remove these claims, identify them as speculative, or substantiate them . They show that the resulting optimizer is compatible with heuristics like RMSProp and Momentum. This is established based on both theoretical results and numerical experiments.<BRK>Moreover, the authors give a deterministic convergence rate for the vanilla algorithm and a convergence rate using momentum. The definition in this paper does not appear to agree with the definition in [1]. In my opinion, the objective of the paper is important and relevant. The theoretical and empirical results are reasonably convincing. Overall, I think this paper proposes an interesting set of dynamics, several meaningful theoretical guarantees, and impressive empirical results.<BRK>Many tasks in modern machine learning can be formulated as finding equilibria in sequential games. In particular, two-player zero-sum sequential games, also known as minimax optimization, have received growing interest. It is tempting to apply gradient descent to solve minimax optimization given its popularity and success in supervised learning. Hotheyver, it has been noted that naive application of gradient descent fails to find some local minimax and can converge to non-local-minimax points. In this paper, they propose Follow-the-Ridge (FR), a novel algorithm that provably converges to and only converges to local minimax. they show theoretically that the algorithm addresses the notorious rotational behavitheir of gradient dynamics, and is compatible with preconditioning and positive momentum. Empirically, FR solves toy minimax problems and improves the convergence of GAN training compared to the recent minimax optimization algorithms. 
Reject. rating score: 1. rating score: 3. rating score: 8. # Strong points  This paper deals with a highly interesting and relevant topic for ICLR. Chen et al.are the first ones to introduce larger images and data augmentation and acknowledge that the large scores are due to this. # RatingAlthough some of the results in the paper might look impressive, my rating for this work is reject for the following reasons (which will be detailed below):1) the main contribution, metric softmax loss, is not novel. The larger images are not standard in the few shot ImageNet evaluation protocol.<BRK>This paper develops a new few shot image classification algorithm. The first one is to use a metric softmax loss used to train on the meta training dataset without episodic updates. The second is that the features learnt thereby are further modified using a linear transformation to fit the few shot training data and the metric soft max loss is again used for classifying the query samples. The empirical results are weaker than existing work (see for instance, https://arxiv.org/abs/1904.03758, https://arxiv.org/abs/1909.02729 etc.); also see #3 below. 4.The training procedure is task agnostic, why do you train a different model for the 1 shot and the 5 shot case? I will consider increasing my score if some of the concerns above are addressed. I am very skeptical as to why the accuracy of fine tuning is only 21% in Figure 2.<BRK>Authors propose a new method for adaptation in a few shot learning setting. They also demonstrate the superiority of the metric softmax classifier vs softmax classifier and finally the overall superiority of the whole method proposed. Similar results are obtained on domain sift settings as well as when comparing the step 2 of this method with a fine tuning approach. I have read the rebuttal and think the paper could be a good addition to the programme.<BRK>Few-shot classification is a challenging task due to the scarcity of training examples for each class. The key lies in generalization of prior knowledge learned from large-scale base classes and fast adaptation of the classifier to novel classes. In this paper, they introduce a two-stage framework. In the first stage, they attempt to learn task-agnostic feature on base data with a novel Metric-Softmax loss. The Metric-Softmax loss is trained against the whole label set and learns more discriminative feature than episodic training. Besides, the Metric-Softmax classifier can be applied to base and novel classes in a consistent manner, which is critical for the generalizability of the learned feature. In the second stage, they design a task-adaptive transformation which adapts the classifier to each few-shot setting very fast within a few tuning epochs. Compared with existing fine-tuning scheme, the scarce examples of novel classes are exploited more effectively. Experiments show that their approach outperforms current state-of-the-arts by a large margin on the commonly used mini-ImageNet and CUB-200-2011 benchmarks.
Reject. rating score: 1. rating score: 3. This paper proposes to use population algorithms as a mechanism for implementing distributed training of deep neural networks. The paper makes some claims about the relationship to previous work on (asynchronous) gossip algorithms that appear to be incorrect. In fact, the proposed PopSGD algorithm is very closely related to other methods in the literature, including AD PSGD (Lian et al.2017b) and SGP (Assran et al.2018).I recommend it be rejected due to lack of novelty and missing connections to much related work.<BRK>The paper considers scaling distributed stochastic gradient descent to large number of nodes. Paper proposes novel asynchronous variant to decentralized SGD, called PopSGD. page 3, related work: Nedic at al. Paper theoretically analyzes the proposed method and shows that in the convex case PopSGD has a linear speedup in the number of nodes compared to the sequential training on one node. Moreover, it is also not clear why \Theta(n) updates could be done in parallel.<BRK>The population model is a standard way to represent large-scale decentralized
distributed systems, in which agents with limited computational potheyr interact
in randomly chosen pairs, in order to collectively solve global computational
tasks. In contrast with synchronous gossip models, nodes are anonymous, lack a
common notion of time, and have no control over their scheduling. In this paper,
they examine whether large-scale distributed optimization can be performed in this
extremely restrictive setting. 

they introduce and analyze a natural decentralized variant of stochastic gradient
descent (SGD), called PopSGD, in which every node maintains a local parameter,
and is able to compute stochastic gradients with respect to this parameter. 
Every pair-wise node interaction performs a stochastic gradient step at each
agent, follotheyd by averaging of the two models. they prove that, under standard
assumptions, SGD can converge even in this extremely loose, decentralized
setting, for both convex and non-convex objectives.  Moreover, surprisingly, in
the former case, the algorithm can achieve linear speedup in the number of nodes
n. their analysis leverages a new technical connection bettheyen decentralized SGD
and randomized load balancing, which enables us to tightly bound the
concentration of node parameters. they validate their analysis through experiments,
showing that PopSGD can achieve convergence and speedup for large-scale
distributed learning tasks in a supercomputing environment.
Reject. rating score: 1. rating score: 3. rating score: 3. The authors compare their data augmentation results to "backtranslation". While the high level motivation and algorithm is interesting, I found the paper very hard to follow, and the experiments are weak.<BRK>The argument is also presented by the paper. Then again, the distribution gets flatter and becomes similar to a uniform distribution when the dim goes higher, which is a common issue.<BRK>The authors should better justify the assumption of Jalalzai et al, and why this is appropriate for the MLP classifier used later in the paper. In particular, why is dilation invariance even a good idea?<BRK>The dominant approaches to sentence representation in natural language rely on learning embeddings on massive corpuses. The obtained embeddings have desirable properties such as compositionality and distance preservation (sentences with similar meanings have similar representations). In this paper, they develop a novel method for learning an embedding enjoying a dilation invariance property. they propose two algorithms: Orthrus, a classification algorithm, constrains the distribution of the embedded variable to be regularly varying, i.e. multivariate heavy-tail. and uses Extreme Value Theory (EVT) to tackle the classification task on two separate regions: the tail and the bulk. Hydra, a text generation algorithm for dataset augmentation, leverages the invariance property of the embedding learnt by Orthrus to generate coherent sentences with controllable attribute, e.g. positive or negative sentiment. Numerical experiments on synthetic and real text data demonstrate the relevance of the proposed framework.

Accept (Spotlight). rating score: 8. rating score: 8. rating score: 6. This paper provides a unified way to provide robust statistics in evaluating RL algorithms in experimental research. Though I don t believe the metrics are particularly novel, I believe this work would be useful to the broader community and was evaluated on a number of environments. + The details of all of these evaluations and individual performance should be provided in the appendix, however, it seems only MuJoco curves were included.<BRK>*Summary*Authors proposed a variety of metrics to measure the reliability of an RL algorithm. Mainly looking at Dispersion and Risk across time and runs while learning, and also in the evaluation phase. Authors have further proposed ranking and also confidence intervals based on bootstrapped samples. They also compared the famous continuous control and discrete actions algorithms on Atari and OpenAI Gym on the metrics they defined. Generalizability: How do authors think these metrics are generalizable.<BRK>Specifically, the authors focus on how to evaluate the reliability of RL algorithms, in particular of the deep RL algorithms. The paper is well motivated by providing convincing justification of evaluating the RL algorithms properly. In particular, the authors define seven specific evaluation metrics, including  Dispersion across Time (DT): IQR across Time ,  Short term Risk across Time (SRT): CVaR on Differences ,  Long term Risk across Time (LRT): CVaR on Drawdown ,  Dispersion across Runs (DR): IQR across Runs ,  Risk across Runs (RR): CVaR across Runs ,  Dispersion across Fixed Policy Rollouts (DF): IQR across Rollouts  and  Risk across Fixed Policy Rollouts (RF): CVaR across Rollouts , from a two dimension analysis shown in Table 1.<BRK>Lack of reliability is a theyll-known issue for reinforcement learning (RL) algorithms. This problem has gained increasing attention in recent years, and efforts to improve it have grown substantially. To aid RL researchers and production users with the evaluation and improvement of reliability, they propose a set of metrics that quantitatively measure different aspects of reliability. In this work, they focus on variability and risk, both during training and after learning (on a fixed policy). they designed these metrics to be general-purpose, and they also designed complementary statistical tests to enable rigorous comparisons on these metrics. In this paper, they first describe the desired properties of the metrics and their design, the aspects of reliability that they measure, and their applicability to different scenarios. they then describe the statistical tests and make additional practical recommendations for reporting results. The metrics and accompanying statistical tools have been made available as an open-stheirce library. they apply their metrics to a set of common RL algorithms and environments, compare them, and analyze the results.
Reject. rating score: 1. rating score: 6. rating score: 8. However, how the proposed algorithm benefits sparsity is not discussed. The idea of an accelerated version of variance reduced stochastic extragradient method is novel. This paper claims that extragradient reduces the gap between the obtained optimal value and the real optimal value, which is confusing. 16.The references are not in a uniform format. From the title, the main application of this work is sparse learning problem.<BRK>This is an optimization algorithm paper, using the idea of "extragradient" and proposing to combine acceleration with proximal gradient descent type algorithms (Prox SVRG). Having said that, it seems to me that combining momentum with an existing algorithm is not extremely novel   I would defer to reviewers who are experts in the optimization area to fully assess the novelty and technical difficulty of the proposed solution. technically sound, seems like a nice addition to the variance reduced gradient type methods.<BRK>The current paper builds on these two approaches and applies the momentum acceleration technique in a stochastic extragradient descent framework to achieve fast convergence. I am not working in the field of optimization, therefore, unfortunately I am not in a position to give detailed technical comments for the authors. However, as far as I could follow the paper, it seemed sound and well written to me in general. It would be good to explain and discuss intuitively the steps of the proposed algorithm in the main body of the paper as well, so that it is well understood. How should this set be chosen in practice?<BRK>Recently, many stochastic gradient descent algorithms with variance reduction have been proposed. Moreover, their proximal variants such as Prox-SVRG can effectively solve non-smooth problems, which makes that they are widely applied in many machine learning problems. Hotheyver, the introduction of proximal operator will result in the error of the optimal value. In order to address this issue, they introduce the idea of extragradient and propose a novel accelerated variance reduced stochastic extragradient descent (AVR-SExtraGD) algorithm, which inherits the advantages of Prox-SVRG and momentum acceleration techniques. Moreover, their  theoretical analysis shows that AVR-SExtraGD enjoys the best-known convergence rates and oracle complexities of stochastic first-order algorithms such as Katyusha for both strongly convex and non-strongly convex problems. Finally, their experimental results show that for ERM problems and robust face recognition via sparse representation, their AVR-SExtraGD can yield the improved performance compared with Prox-SVRG and Katyusha. The asynchronous variant of AVR-SExtraGD outperforms KroMagnon and ASAGA, which are the asynchronous variants of SVRG and SAGA, respectively.
Reject. rating score: 1. rating score: 1. rating score: 3. They use a 3 layered MLP on top of the representation from auto encoder. Why is this interesting? There are no such constraint in AE as well   you can vey well have your hidden representation to be over complete   so this is not a good motivation for Gibbs energy2.<BRK>Positive things about this work  the topic is interesting  the last application is interesting (water temperature prediction) Negative things about this work  this work is very poorly  written and lacks sufficient clarity. This work needs a major rewrite. Hidden state usually refers to the encoder output, not to the decoder output. in general, the motivation is unclear. In fact, the authors could consider building on top of these other more modern approaches.<BRK>The energy formulations for Uθ, namely an autoencoder and Gibbs model, are not to my knowledge new in this context. This builds on an existing update scheme (Equation 8), but replaces the full operator ψ with an iterative update. The paper emphasises their view of an interpolation operator I, which is a little confusing in an autoencoder context.<BRK>For numerous domains, including for instance earth observation, medical imaging, astrophysics,..., available image and signal datasets often irregular space-time sampling patterns and large missing data rates. These sampling properties is a critical issue to apply state-of-the-art learning-based (e.g., auto-encoders, CNNs,...) to fully benefit from the available large-scale observations and reach breakthroughs in the reconstruction and identification of processes of interest. In this paper, they address the end-to-end learning of representations of signals, images and image sequences from irregularly-sampled data, {\em i.e.} when the training data involved missing data. From an analogy to Bayesian formulation, they consider energy-based representations. Two energy forms are investigated: one derived from auto-encoders and one relating to Gibbs energies. The learning stage of these energy-based representations (or priors) involve a joint interpolation issue, which resorts to solving an energy minimization problem under observation constraints. Using a neural-network-based implementation of the considered energy forms, they can state an end-to-end learning scheme from irregularly-sampled data. they demonstrate the relevance of the proposed representations for different case-studies: namely, multivariate time series, 2{\sc } images and image sequences.
Reject. rating score: 3. rating score: 6. rating score: 6. There is little contribution on the methodology side from the paper. Moreover, the bound does not depend on the embedding dimension at all. Is this to be expected?<BRK>However,  they also have experiments on real datasets, used in the literature. This gives them constant time for the calculation of the embedding w.r.t to the number of neighborsMain points:Overall, the article is well supported theoretically and quite complete.<BRK>A reader could easily interpret this to mean that the error on the downstream prediction problem will be bounded (which is not what this work shows). Overall (and described in the comments below), I believe this paper provides some interesting theoretical results for an approach that is widely used in practice. “r”, the number of sampled nodes, is not defined in the main body of the paper.<BRK>The recent advancements in graph neural networks (GNNs) have led to state-of-the-art performances in various applications, including chemo-informatics, question-anstheyring systems, and recommender systems. Hotheyver, scaling up these methods to huge graphs such as social network graphs and theyb graphs still remains a challenge. In particular, the existing methods for accelerating GNNs are either not theoretically guaranteed in terms of approximation error, or they require at least a linear time computation cost. 
In this study, they analyze the neighbor sampling technique to obtain a constant time approximation algorithm for GraphSAGE, the graph attention networks (GAT), and the graph convolutional networks (GCN). The proposed approximation algorithm can theoretically guarantee the precision of approximation. The key advantage of the proposed approximation algorithm is that the complexity is completely independent of the numbers of the nodes, edges, and neighbors of the input and depends only on the error tolerance and confidence probability. To the best of their knowledge, this is the first constant time approximation algorithm for GNNs with a theoretical guarantee. Through experiments using synthetic and real-world datasets, they demonstrate the speed and precision of the proposed approximation algorithm and validate their theoretical results.
Reject. rating score: 3. rating score: 3. rating score: 8. This paper studies a new problem, i.e., generating graphs conditioned on an input graph. Overall, the problem studied in this paper is interesting and novel, and the proposed method makes sense.<BRK>This paper studies a problem of graph translation, which aims at learning a graph translator to translate an input graph to a target graph. Overall, the problem is new and well motivated. The problem is new and well motivated.<BRK>In this work the authors tackle the problem of generating a given graph to a target output graph. The authors propose an architecture that consists of a graph translator, and a conditional graph discriminator.<BRK>Deep graph generation models have achieved great successes recently, among which, hotheyver, are typically unconditioned generative models that have no control over the target graphs are given an input graph. In this paper, they propose a novel Graph-Translation-Generative-Adversarial-Networks (GT-GAN) that transforms the input graphs into their target output graphs. GT-GAN consists of a graph translator equipped with innovative graph convolution and deconvolution layers to learn the translation mapping considering both global and local features, and a new conditional graph discriminator to classify target graphs by conditioning on input graphs. Extensive experiments on multiple synthetic and real-world datasets demonstrate that their proposed GT-GAN significantly outperforms other baseline methods in terms of both effectiveness and scalability. For instance, GT-GAN achieves at least 10X and 15X faster runtimes than GraphRNN and RandomVAE, respectively, when the size of the graph is around 50.
Accept (Poster). rating score: 6. rating score: 6. rating score: 1. The paper provides a comprehensive study on the two tower Transformer models in terms of the impact of its pre training tasks on large scale retrieval applications. The studies here show that, pre training with Inverse Cloze Task (ICT) the two tower Transformer models significantly outperform the widely used BM 25 algorithm for large scale information retrieval. I hope the authors will release the source codes to the community.<BRK>In fact, rather than experimenting on different ratios of train/test, is it possible to report on official test set, while splitting the train set into 90/10 for training and development? This paper studies a query related document retrieval problem using a framework which they call “two tower retrieval method”. The combination of ICT, BFS and WLP achieves remarkable improvement over the number of baselines including BM25 and other neural based models. The strength of this paper is that it includes comprehensive studies on the two tower retrieval problem. In that case, it should be clearly mentioned in the paper. I believe data used for the development and test should be different.<BRK>This paper proposes a solution to the large scale query document retrieval problem. The proposed method was shown to be a better alternative to the classic information retrieval approach such as BM 25 (token marching + TF IDF weights). The proposed method is based on two separate transformer models which has computational benefit over one cross attention model. For pre training tasks, Inverse Cloze Task (ICT), Body First Selection (BFS), Wiki LinkPrediction (WLP) were studied. However, there is a major technical problem in the proposed method. In the proposed approach, the query embedding (q_emb) and the document embedding (d_emb) train separately by two transformer models (two towers   Query tower and Doc tower). After that, the similarity was measured through a dot product.<BRK>they consider the large-scale query-document retrieval problem: given a query (e.g., a question), return the set of relevant documents (e.g., paragraphs containing the anstheyr) from a large document corpus. This problem is often solved in two steps. The retrieval phase first reduces the solution space, returning a subset of candidate documents. The scoring phase then re-ranks the documents. Critically, the retrieval algorithm not only desires high recall but also requires to be highly efficient, returning candidates in time sublinear to the number of documents. Unlike the scoring phase witnessing significant advances recently due to the BERT-style pre-training tasks on cross-attention models, the retrieval phase remains less theyll studied. Most previous works rely on classic Information Retrieval (IR) methods such as BM-25 (token matching + TF-IDF theyights). These models only accept sparse handcrafted features and can not be optimized for different downstream tasks of interest. In this paper, they conduct a comprehensive study on the embedding-based retrieval models. they show that the key ingredient of learning a strong embedding-based Transformer model is the set of pre-training tasks. With adequately designed paragraph-level pre-training tasks, the Transformer models can remarkably improve over the widely-used BM-25 as theyll as embedding models without Transformers. The paragraph-level pre-training tasks they studied are Inverse Cloze Task (ICT), Body First Selection (BFS), Wiki Link Prediction (WLP), and the combination of all three.
Reject. rating score: 3. rating score: 3. rating score: 3. My main concern remains the experiments. My analysis of this work in short is that the problem that this paper addresses is interesting and not yet solved. This paper proposes a simple and efficient method for DRO.<BRK>The paper proposes an easy to implement algorithm for DRO on example weights, with the same computational cost as SGD. They claim the algorithm is more robust to the learning rate, so is it possible to train with the original learning rate used for the ERM baseline? Why was a different learning rate chosen? Many state of the art deep learning pipelines do not use plain SGD   for example, the WideResNet you used on CIFAR. Perhaps combining momentum with nested optimization of loss weights and parameters is why the baseline does not train?<BRK>They demonstrate that SGD with hardness weighted sampling is a principled and efficient optimization method for DRO in machine learning and is particularly suited in the context of deep learning. The DRO problem studied in this paper is relatively easy, in the sense that the inner maximization has close form solution (at least for KL divergence). Second, both experiments are not convincing enough. In Algorithm 1 Line 18 19, the algorithm proposes to do sampling with replacement using the softmax probability \hat{p}.<BRK>Distributionally Robust Optimization (DRO) has been proposed as an alternative to Empirical Risk Minimization (ERM) in order to account for potential biases in the training data distribution. Hotheyver, its use in deep learning has been severely restricted due to the relative inefficiency of the optimizers available for DRO compared to the wide-spread Stochastic Gradient Descent (SGD) based optimizers for deep learning with ERM. In this work, they demonstrate that SGD with hardness theyighted sampling is a principled and efficient optimization method for DRO in machine learning and is particularly suited in the context of deep learning. Similar to a hard example mining strategy in essence and in practice, the proposed algorithm is straightforward to implement and computationally as efficient as SGD-based optimizers used for deep learning.  It only requires adding a softmax layer and maintaining an history of the loss values for each training example to compute adaptive sampling probabilities.  In contrast to typical ad hoc hard mining approaches, and exploiting recent theoretical results in deep learning optimization, they prove the convergence of their DRO algorithm for over-parameterized deep learning networks with ReLU activation and finite  number  of  layers  and parameters.  Preliminary results demonstrate the feasibility and usefulness of their approach.
Reject. rating score: 1. rating score: 3. rating score: 6. The authors investigate the benet, mainlytheoretical, of the proposed interpolation. As the goal of the paper is clearlytheoretical, the  real data analysis  part is not necessary in my opinion. The paper is easy to understand and correctly written.<BRK>More context needs to be given by comparing the main theorem to prior works (which I am not familiar with). The main theorem only appears on page 4 and the reader must consult the appendix to see the definition of all the terms that appear in the theorem. Specifically, it studies how the performance of the algorithm is affected by reweighting the k nearest neighbors according to their relative distance.<BRK>The paper studies theoretical perspective of double descent phenomenon for the interpolated K NN classifier. The paper is works in several interesting directions and gives theoretical reasoning to how interpolated K NN could exhibit the double descent phenomenon.<BRK>The over-parameterized models attract much attention in the era of data science and deep learning. It is empirically observed that although these models, e.g. deep neural networks, over-fit the training data, they can still achieve small testing error, and sometimes even outperform traditional algorithms which are designed to avoid over-fitting. The major goal of this work is to sharply quantify the benefit of data interpolation in the context of nearest neighbors (NN) algorithm. Specifically, they consider a class of interpolated theyighting schemes and then carefully characterize their asymptotic performances. their analysis reveals a U-shaped performance curve with respect to the level of data interpolation, and proves that a mild degree of data interpolation strictly improves the prediction accuracy and statistical stability over those of the (un-interpolated) optimal $k$NN algorithm. This theoretically justifies (predicts) the existence of the second U-shaped curve in the recently discovered double descent phenomenon. Note that their goal in this study is not to promote the use of interpolated-NN method, but to obtain theoretical insights on data interpolation inspired by the aforementioned phenomenon.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. This paper describes a new method for learning deep word level representations efficiently. The architecture uses a hierarchical structure with skip connections which allows for the use of low dimensional input and output layers, reducing total parameters and training time while delivering similar or better performance versus existing methods. 1.From table 1a or table 2, the training time of the proposed method is not reduced compared with existing methods. 2.It seems the number of parameters in DeFINE still depends directly on vocabulary size. For dataset that has very large vocabulary size, [1] and [2] could potentially have larger compression rate.<BRK>Experiments are conducted for language modeling and machine translation tasks and performance improvements are observed with a reduction in parameters and lesser training time. The obtained results are nice (though this can be improved) and there is indeed some potential value in this work. I think the paper would benefit from some analysis of the differences in the word embeddings learnt by a general lookup table learning model in comparison with the word embeddings learnt by this model. For example, are the gains lesser on Gigaword? This would be interesting to know.<BRK>The paper proposes a novel sparse network architecture to learn word embeddings more effectively. I am not an expert in the area of machine translation, so I am able to sanity check the results and the reasoning and motivation given in the paper. Why would this allows to learn a more efficient low dimensional embedding than the original one? This may happen to be the case, but why? This does not allow me to conclude anything. Table 1: c) DeFINE seems to give better perplexity results, while using less parameters. This is good. I do not know what to conclude, as ideally I would like to see how would DeFINE do with the same number of parameters.<BRK>For sequence models with large vocabularies, a majority of network parameters lie in the input and output layers. In this work, they describe a new method, DeFINE, for learning deep token representations efficiently. their architecture uses a hierarchical structure with novel skip-connections which allows for the use of low dimensional input and output layers, reducing total parameters and training time while delivering similar or better performance versus existing methods. DeFINE can be incorporated easily in new or existing sequence models. Compared to state-of-the-art methods including adaptive input representations, this technique results in a 6% to 20% drop in perplexity. On WikiText-103, DeFINE reduces the total parameters of Transformer-XL by half with minimal impact on performance. On the Penn Treebank, DeFINE improves AWD-LSTM by 4 points with a 17% reduction in parameters, achieving comparable performance to state-of-the-art methods with fetheyr parameters. For machine translation, DeFINE improves the efficiency of the Transformer model by about 1.4 times while delivering similar performance.
Accept (Poster). rating score: 8. rating score: 8. rating score: 3. The paper proposes an approach to generate textual descriptions from structured data organized in tables, by using a "variational template machine" (VTM), which is essentially a generative model to separately represent template and content as disentangled latent variables to control the generation. Remarks:  It should be clearly stated which languages feature in the paper.<BRK>The paper is interesting and proposes a novel approach for addressing a currently not largely considered problem. The proposed model is sound and appropriate, as it relies on state of the art methodological arguments. It would be good if the authors could provide an analysis of the computational costs of their methods, as well as of the considered competitors.<BRK>This paper proposes Variational Template Machine (VTM), a generative model to generate textual descriptions from structured data (i.e., tables). However, I am not convinced that the proposed method is a significant development based on the results presented in the paper. The authors introduce two latent variables to model contents and templates. Did it have any effect on the performance of the baseline model?<BRK>How to generate descriptions from structured data organized in tables? Existing approaches using neural encoder-decoder models often suffer from lacking diversity. they claim that an open set of templates is crucial for enriching the phrase constructions and realizing varied generations.Learning such templates is prohibitive since it often requires a large paired <table,description>, which is seldom available. This paper explores the problem of automatically learning reusable "templates" from paired and non-paired data. they propose the variational template machine (VTM), a novel method to generate text descriptions from data tables. their contributions include:  a) they carefully devise a specific model architecture and losses to explicitly disentangle text template and semantic content information, in the latent spaces, and b) they utilize both small parallel data and large raw text without aligned tables to enrich the template learning. Experiments on datasets from a variety of different domains show that VTM is able to generate more diversely while keeping a good fluency and quality. 
Reject. rating score: 1. rating score: 3. rating score: 6. Overall, I think the paper should be rejected as the contributions are limited and are not aligned with the experiments. This work proposes a new loss function to train the network with Outlier Exposure(OE) [1] which leads to better OOD detection compared to simple loss function that uses KL divergence as the regularizer for OOD detection. Cons:1  The level of contributions is limited. "Enhancing the reliability of out of distribution image detection in neural networks."<BRK>The paper considers the problem of out of distribution detection in the context of image and text classification with deep neural networks. 5.The method is only being compared to the OE method of [1]. The authors show that these modifications improve results compared to [1] on image and text classification. There are a few concerns I have for this paper.<BRK>This paper proposes to tackle the problems of out of distribution (OOD) detection and model calibration by adapting the loss function of the Outlier Exposure (OE) technique [1]. ### Post Rebuttal Comments ###I would like to thank the authors for their hard work during the rebuttal period. I think the current version of the paper is much improved over the previous version. The choice to remove the claims about calibration definitely improves the focus of the paper. If these were addressed I would consider increasing my score.<BRK>Deep neural networks have achieved great success in classiﬁcation tasks during the last years. Hotheyver, one major problem to the path towards artiﬁcial intelligence is the inability of neural networks to accurately detect samples from novel class distributions and therefore, most of the existent classiﬁcation algorithms assume that all classes are known prior to the training stage. In this work, they propose a methodology for training a neural network that allows it to efﬁciently detect out-of-distribution (OOD) examples without compromising much of its classiﬁcation accuracy on the test examples from known classes. Based on the Outlier Exposure (OE) technique, they propose a novel loss function that achieves state-of-the-art results in out-of-distribution detection with OE both on image and text classiﬁcation tasks. Additionally, the way this method was constructed makes it suitable for training any classiﬁcation algorithm that is based on Maximum Likelihood methods.
Reject. rating score: 1. rating score: 3. rating score: 6. This paper proposes a new neural network architecture for dealing with graphs dealing with the lack of order of the nodes.<BRK>This paper proposes a method to learn graph features by means of neural networks for graph classification.<BRK>This paper proposes a neural network architecture to classify graph structure. Thus, the network can learn isomorphic features of the graph.<BRK>Deep learning models have achieved huge success in numerous fields, such as computer vision and natural language processing. Hotheyver, unlike such fields, it is hard to apply traditional deep learning models on the graph data due to the ‘node-orderless’ property. Normally, adjacency matrices will cast an artificial and random node-order on the graphs, which renders the performance of deep mod- els on graph classification tasks extremely erratic, and the representations learned by such models lack clear interpretability. To eliminate the unnecessary node- order constraint, they propose a novel model named Isomorphic Neural Network (ISONN), which learns the graph representation by extracting its isomorphic features via the graph matching bettheyen input graph and templates. ISONN has two main components: graph isomorphic feature extraction component and classification component. The graph isomorphic feature extraction component utilizes a set of subgraph templates as the kernel variables to learn the possible subgraph patterns existing in the input graph and then computes the isomorphic features. A set of permutation matrices is used in the component to break the node-order brought by the matrix representation. Three fully-connected layers are used as the classification component in ISONN. Extensive experiments are conducted on benchmark datasets, the experimental results can demonstrate the effectiveness of ISONN, especially compared with both classic and state-of-the-art graph classification methods.
Accept (Poster). rating score: 6. rating score: 3. rating score: 3. This paper attacks the problem of pruning neural networks to obtain sparser models for deployment. In introduces a principled importance sampling approach for which independence of samples allows one to obtain bounds easily. The proposal mechanism is very smart. The authors use a measure of the sensitivity of the network outputs to the channels in a particular layer (eqn 1). This will make life easier for anyone reading the paper for the first time.<BRK>Summary: In this paper, the author propose a provable pruning method, and also provide a bound for the final pruning error. To achieve a more reasonable algorithm, author prune the redundant channel by controlling the deviation of the summation statistically small,  and reusing the filter by important sampling the given channel. Weakness:1. experiment is too weakImageNet model has great impact on most CV problem, and the current release models are flooding in the open source world.<BRK>This paper studies the tasks of pruning filters, a provable, sampling based approach for generating compact Convolutional Neural Networks (CNNs). In general, this paper is very well written and organized. In contrast, all the results of this method gets worse results; this is less desirable.<BRK>they present a provable, sampling-based approach for generating compact Convolutional Neural Networks (CNNs) by identifying and removing redundant filters from an over-parameterized network. their algorithm uses a small batch of input data points to assign a saliency score to each filter and constructs an importance sampling distribution where filters that highly affect the output are sampled with correspondingly high probability. 
In contrast to existing filter pruning approaches, their method is simultaneously data-informed, exhibits provable guarantees on the size and performance of the pruned network, and is widely applicable to varying network architectures and data sets. their analytical bounds bridge the notions of compressibility and importance of network structures, which gives rise to a fully-automated procedure for identifying and preserving filters in layers that are essential to the network's performance. their experimental evaluations on popular architectures and data sets show that their algorithm consistently generates sparser and more efficient models than those constructed by existing filter pruning approaches. 
Accept (Talk). rating score: 8. rating score: 8. rating score: 6. The authors propose an interdependent gating mechanism that enriches the coupling between inputs and hidden states. For an input x_0 and hidden state h_0; h_0 gates x_0 to create x_1; x_1 then gates h_0 to create h_1; this cyclical gating operation is applied for several rounds and it s output is fed into a recurrent neural network. This results in the RNN processing a more contextualized version of the input tokens x. Pros:The paper is very well written and clear. Final notes:This paper raises many interesting question:  What is the really going on with the gating mechanism? The authors explore this question but the jury is still out on exactly what is going on here. "Mogrification" as a general preprocessing step: could it also improve performance for transformer models?<BRK>Summary:The paper proposes a novel LSTM architecture that adds several gating mechanism that gates the hidden state and inputs in between the LSTM update. Comments on the paper:1. The paper proposes an interesting architecture and it seems to show significant improvement in terms of performance for some language datasets. 2.The paper is very well written, the motivation and formulation is clear. 3.. One thing is that since this could take into account more context,  it seems that this model could potentially generate language / tokens with longer time dependencies. 4.Also, I am curious about the generalization ability of the model.<BRK>I have read the authors  response. This paper proposes a modification of LSTM networks in the context of language modeling called Mogrifier LSTM. The proposed Mogrifier LSTM utilizes the same recurrent unit as the LSTM, but the input and previous hidden state are updated with several rounds of mutual gating. * Experiments demonstrating strong performance on a number of language modeling tasks. Points in favor of acceptance include the high clarity of writing, good experiments of the proposed model, and a discussion of possible reasons for why the mogrification operation works well. The closest that the authors come to this is the single validation perplexity of the Multiplicative LSTM in Table 3.<BRK>Many advances in Natural Language Processing have been based upon more expressive models for how inputs interact with the context in which they occur. Recurrent networks, which have enjoyed a modicum of success, still lack the generalization and systematicity ultimately required for modelling language. In this work, they propose an extension to the venerable Long Short-Term Memory in the form of mutual gating of the current input and the previous output. This mechanism affords the modelling of a richer space of interactions bettheyen inputs and their context. Equivalently, their model can be vietheyd as making the transition function given by the LSTM context-dependent. Experiments demonstrate markedly improved generalization on language modelling in the range of 3–4 perplexity points on Penn Treebank and Wikitext-2, and 0.01–0.05 bpc on ftheir character-based datasets. they establish a new state of the art on all datasets with the exception of Enwik8, where they close a large gap bettheyen the LSTM and Transformer models.

Reject. rating score: 1. rating score: 3. rating score: 3. The paper proposes a new goodness of fit measure for generative models, and uses it to get insight into GAN s. While this is an important topic and a novel approach, I do not think the paper delivers on what it promises. First, while it claims to be a general method for generative models it, it is limited to only  GANs and even for GANs it is limited. Detailed remarks:  The main point that the training set points x must have p(x)>0 under the model is naturally satisfied for almost all models except GANs such as VAEs, autoregressive model and flow models with standard implementations as the support is the whole space. This is in contrast to the claim in the paper that "its applications can be extended to other generative networks such as Variational Autoencoders.". Even for GANs as this measure only looks at the support and not the distribution it is not clear if this measure does more then evaluate mode collapse. This is a main point by the authors, but it ignores the probability and only looks at the support.<BRK>This paper defines a goodness of fit measure F for generative networks, that reflects how well a model can generate the training data. This paper brings an interesting contribution to the evaluation of generative networks. The use of the square distance in the image space is not obvious and not justified. 2.Computation of this metric is not straightforward: there is no theoretical guarantee and it is computationally expensive. 4.Typos are obscuring the reading of the paper. Post rebuttal: I have read the authors  response and am maintaining my weak reject rating.<BRK>The authors might want to add some discussion in Section 4.2 regarding why the residual connection is detrimental for covering the support. This seems overly assertive. It only concerns the generation of the training data, but not the sampled data from the network (at least not directly). As shown by the authors, the proposed measure can be considered as the approximation of the true probability support not covered by the generative models, which also defines a necessary condition to avoid mode collapse. "Optimality" in terms of generative models may depend on the downstream tasks. I do not think there exists a universal definition of "optimality" for generative models. Assessing generative models via precision and recall. ICML 2019.<BRK>they define a goodness of fit measure for generative networks which captures how theyll the network can generate the training data, which is necessary to learn the true data distribution.
they demonstrate how their measure can be leveraged to understand mode collapse in generative adversarial networks and provide practitioners with a novel way to perform model comparison and early stopping without having to access another trained model as with Frechet Inception Distance or Inception Score. This measure shows that several successful, popular generative models, such as DCGAN and WGAN, fall very short of learning the data distribution. they identify this issue in generative models and empirically show that overparameterization via subsampling data and using a mixture of models improves performance in terms of goodness of fit.
Reject. rating score: 3. rating score: 3. rating score: 3. rating score: 6. This paper proposes a method to predict future trajectories by modeling partial and full occlusions. However, I failed to find how diverse the output of the model is. If the output is not that stochastic, then it would be tough to believe that the model can "predict" the future; instead, it may "extrapolate" the current condition only.<BRK>The key contribution of this paper is a model that can predict the dynamics of pre segmented image patches under multiple frames of occlusion. The key weaknesses I see are:  The objects must be pre segmented by some externally defined mechanism. Overall I don t believe this work is ready for publication, as there isn t that much novelty and the requirements are impractical. But if one has the segmentation masks, that simplifies things considerably and also offers a good estimate of the location and velocity (if there are 2+ frames).<BRK>Summary:This paper proposes a method that combines a recurrent neural network that predicts values that are used as inputs to a rendered which interprets them and generates an object shape map and a depth map for every step of the dynamics predicted by the recurrent neural network. The proposed method is able to handle object occlusions and interactions. It would be interesting to see if this can be done so the proposed network is more unified. Can the authors comment on this? However, the rest of components have limited novelty.<BRK>In this paper, the authors develop a highly structured model to predict motions of objects defined by segmentation masks and depths. The model trains a physics model (in the form of a slightly modified interaction network) and a renderer composed of a per object renderer combined with an occlusion model which composes the per object segmentation and depth into a scene segmentation and depth. Regarding using segmentation/depth as input to the model: for a real dataset, segmentation is more relevant: it is both less informative than positions (due to significant occlusions) and easier to measure.<BRK>To reach human performance on complex tasks, a key ability for artificial
systems is to understand physical interactions bettheyen objects, and predict
future outcomes of a situation. This ability, often referred to as
intuitive
physics
, has recently received attention and several methods theyre proposed to
learn these physical rules from video sequences. Yet, most these methods are
restricted to the case where no occlusions occur, narrowing the potential areas
of application. The main contribution of this paper is a method combining
a predictor of object dynamics and a neural renderer efficiently predicting
future trajectories and explicitly modelling partial and full occlusions among
objects. they present a training procedure enabling learning intuitive physics
directly from the input videos containing segmentation masks of objects and
their depth. their results show that their model learns object dynamics despite
significant inter-object occlusions, and realistically predicts segmentation
masks up to 30 frames in the future. they study model performance for
increasing levels of occlusions, and compare results to previous work on
the tasks of future prediction and object following. they also show results
on predicting motion of objects in real videos and demonstrate significant
improvements over state-of-the-art on the object permanence task in the
intuitive physics benchmark of Riochet et al. (2018).
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. The NAS search algorithm is similar to previous one shot NAS, but the search space is channel wise: each channel has it’s own kernel size, which is quite novel and interesting. Results are strong in terms of FLOPS and parameters. Although some recent works (e.g., MixNet) have tried to partition channels into groups, this paper goes further and searches for different kernel size for each single channel. How do you justify the generality of the channel wise search space? I recommend the authors including “channel wise” in the title. This information would be helpful to interpret and justify your algorithm 1.<BRK>In general, the paper is well written and easy to follow. The searched models achieve a new state of the art result on the ImageNet classification task for mobile setting with restricted FLOPs. In short, the proposed method is interesting and the results on ImageNet are impressive, I weakly accept this paper and hope that the authors can make the experiment section more solid in a revised version. However, the experiment section is not solid enough.<BRK>The biggest problem of this paper is that experiment is not enough. Conclusion:This is an interesting paper with novel idea and efficient implementation. However, more experiments are needed to validate the utility of the proposed method.<BRK>Search space design is very critical to neural architecture search (NAS) algorithms. they propose a fine-grained search space comprised of atomic blocks, a minimal search unit that is much smaller than the ones used in recent NAS algorithms. This search space allows a mix of operations by composing different types of atomic blocks, while the search space in previous methods only allows homogeneous operations. Based on this search space, they propose a restheirce-aware architecture search framework which automatically assigns the computational restheirces (e.g., output channel numbers) for each operation by jointly considering the performance and the computational cost. In addition, to accelerate the search process, they propose a dynamic network shrinkage technique which prunes the atomic blocks with negligible influence on outputs on the fly.  Instead of a search-and-retrain two-stage paradigm, their method simultaneously searches and trains the target architecture. 
their method achieves state-of-the-art performance under several FLOPs configurations on ImageNet with a small searching cost.
they open their entire codebase at: https://github.com/meijieru/AtomNAS.
Reject. rating score: 6. rating score: 6. rating score: 8. This paper targets at studying the mutual exclusive bias which existed in children learning, to help understand whether there exists similar bias in deep networks. The paper is very well organized and written. However, I have the following concerns. So what’s the key difference? 3, In Sec.3, the first and section paragraph, I can not quite understand how the ME bias theory guide the following synthetic experiments. If one uses word2vec as the representation in NN, the ME bias will be solved. It seems to me that the tasks are organized as unbalanced instances of each classes, and asking the common NNs to learn this  task. the authors clarified and answered the questions. I would like to raise the score.<BRK>*** Increased to weak accept after discussion of merits of ME bias was improved in the paper *** This paper investigates whether neural networks exhibit a ‘mutual exclusivity (ME) bias’, whereby novel inputs tend to be associated with previously unseen outputs, an inductive bias that is cited to be present in children when learning to associate new words and objects. The authors pose an interesting hypothesis, but it would gain a lot of credibility if they could provide an empirical analysis of an algorithm that uses ME reasoning to improve learning in a realistic setting or if they could at least perform some quantitative analysis of the effects of ME bias on task performance for a toy example.<BRK>Summary:This paper makes an observation that most of the neural network architectures do not learn the mutual exclusivity (ME) bias: if an object has one label, then it does not need another. Authors argue that ME bias could help the model to handle new classes and rare events better. I support accepting this paper. As the authors also agree, ME bias is missing not just in DNNs. It is the issue of MLE. It would be good to have some non NN results too. I see this is a challenge for MLE than DNNs.<BRK>Strong inductive biases allow children to learn in fast and adaptable ways. Children use the mutual exclusivity (ME) bias to help disambiguate how words map to referents, assuming that if an object has one label then it does not need another. In this paper, they investigate whether or not standard neural architectures have a ME bias, demonstrating that they lack this learning assumption. Moreover, they show that their inductive biases are poorly matched to lifelong learning formulations of classification and translation. they demonstrate that there is a compelling case for designing neural networks that reason by mutual exclusivity, which remains an open challenge.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. This involves achieving sparsity by application of a binary mask, where the mask is determined by current parameter values and a learned threshold. The algorithm proposed in this paper seems sensible, but rather ad hoc.<BRK># Clarity  The description of the main idea is not clear. What are "structure gradient" and "performance gradient"? They are not mathematically defined in the paper.<BRK>Thus, the novelty should be summarized, and highlighted in the paper.<BRK>they present a novel network pruning algorithm called Dynamic Sparse Training that can jointly ﬁnd the optimal network parameters and sparse network structure in a uniﬁed optimization process with trainable pruning thresholds. These thresholds can have ﬁne-grained layer-wise adjustments dynamically via backpropagation. they demonstrate that their dynamic sparse training algorithm can easily train very sparse neural network models with little performance loss using the same training epochs as dense models. Dynamic Sparse Training achieves prior art performance compared with other sparse training algorithms on various network architectures. Additionally, they have several surprising observations that provide strong evidence to the effectiveness and efﬁciency of their algorithm. These observations reveal the underlying problems of traditional three-stage pruning algorithms and present the potential guidance provided by their algorithm to the design of more compact network architectures.
Reject. rating score: 1. rating score: 3. rating score: 6. This paper combines ideas from attentive and sequential neural processes to incorporate an attention mechanism to the existing sequential neural process, which results in an attentive sequential neural processes framework. Second, the argument that augmenting SNP with an attention mechanism is not trivial is somewhat contrived. Fourth, the technical exposition of this paper is too vague. It would be a good experiment to see if the imaginery component is necessary. To summarize, I believe the paper in its current state is not well motivated and appears very incremental given the prior works of SNP and ANP.<BRK>This paper deals with the underfitting problem happening in neural process and sequential neural process (SNP). A combination of attention into SNP. 3.Different tasks were evaluated to investigate the merit of this method. The comparison for time complexity and parameter size was missing. 3.An incremental research.<BRK>Authors present a method to address the problem of underfitting found in sequential neural processes. Authors addressed this problem by introducing an attention mechanism and model, i.e.Attentive sequential neural processes, which incorporates a memory mechanism of imaginary context. This imaginary context is generated through an RNN network and are treated as latent variables. It would be nice to demonstrate the performance in more challenging tasks as well, however the results presented and the new context imagination introduced are quite promising indeed.<BRK>Sequential Neural Processes (SNP) is a new class of models that can meta-learn a temporal stochastic process of stochastic processes by modeling temporal transition bettheyen Neural Processes. As Neural Processes (NP) suffers from underfitting, SNP is also prone to the same problem, even more severely due to its temporal context compression. Applying attention which resolves the problem of NP, hotheyver, is a challenge in SNP, because it cannot store the past contexts over which it is supposed to apply attention. In this paper, they propose the Attentive Sequential Neural Processes (ASNP) that resolve the underfitting in SNP by introducing a novel imaginary context as a latent variable and by applying attention over the imaginary context. they evaluate their model on 1D Gaussian Process regression and 2D moving MNIST/CelebA regression. they apply ASNP to implement Attentive Temporal GQN and evaluate on the moving-CelebA task.
Reject. rating score: 3. rating score: 3. rating score: 6. This paper proposes a new loss for training models that predict where events occur in a sequence when the training sequence has noisy labels. The central idea is to smooth the label sequence and prediction sequence and compare these rather than to force the model to treat all errors as equally serious. The proposed problem seems sensible, and the method is a reasonable approach. The evaluations are carried out on a variety of different tasks (piano onset detection, drum detection, smoking detection, video action segmentation).<BRK>The authors propose SoftLoc, an interpolation of two temporal event prediction losses (one based on label & prediction smoothing, and one based on weakly supervised count based loss) that makes the predictor more robust to noisy training data. They demonstrate on various temporal alignment datasets from music to wearable sensors to video action segmentations, that their method is performs well both on noisy free and noisy settings compared to prior approaches. Strengths:(1) relatively thorough experimental valuations: using 4 datasets comparing with sufficient number of prior approaches (One potential improvement could be to try noise distributions other than Gaussian)(2) simple objective and consistent improvements. Weaknesses:(1) the novelty of the method appears limited. The weakly supervised loss is borrowed from the prior work, so it seems like the main algorithmic novelty is to add noise to predictions (as opposed to just adding to labels as done in prior work). (2) is Issue 1 an actual problem? Such toy experiments may complement the existing end to end experiments to demonstrate the precise properties of the proposed method.<BRK>This term helps to relax the model’s reliance on exact label locations. 2).$\mathscr{L}_{MC}$: a mass convergence loss that acts as a regularizer to facilitate the model with precise impulse like localizations. Various applications, such as PIANO ONSET, DRUM DETECTION and TIME SERIES DETECTION are performed to verify the effectiveness of the proposed method. And state of the art performance is achieved. Despite the achievement indicated by the experiments, I still have some concerns about this paper. And the label smoothing idea (applying a ˜S^2 Gaussian filter to the labels) has been introduced to increase the robustness to temporal misalignment of annotations [2]. The reason should be clarified. (3)	The authors propose the two side relaxed loss L_{SLL} for the soft learning of temporal localization problem. However, in the ablation study, the authors do not give the results with different level of label noise for one side variant. Adding these results could help to demonstrate the necessity of the two side relaxed loss L_{SLL}. References:[1] Improved musical onset detection with convolutional neural networks[2] Onsets and frames: Dual objective piano transcription<BRK>This work addresses the long-standing problem of robust event localization in the presence of temporally of misaligned labels in the training data. they propose a novel versatile loss function that generalizes a number of training regimes from standard fully-supervised cross-entropy to count-based theyakly-supervised learning. Unlike classical models which are constrained to strictly fit the annotations during training, their soft localization learning approach relaxes the reliance on the exact position of labels instead. Training with this new loss function exhibits strong robustness to temporal misalignment of labels, thus alleviating the burden of precise annotation of temporal sequences. they demonstrate state-of-the-art performance against standard benchmarks in a number of challenging experiments and further show that robustness to label noise is not achieved at the expense of raw performance. 
Reject. rating score: 1. rating score: 3. rating score: 8. This is just an example of the writing problem of the paper. This paper is poorly written. I doubt this paper can be easily accepted at a Physics venue. There are many other issues. Finally, I want to point out that, the generalization of kernel methods have been intensively studied in the machine learning literature, for example using the RKHS theory. I agree that there could potentially be great ideas in this paper. The conference is a venue with quality control. For example, the definition of renormalized NTK should better be defined in equations such as $K_r(x, x )   \sum_{k   0}^r b_k <x, x >^k$, rather than be described in words like "trim after the r’th power". However, the modifications made by the authors are still not sufficient.<BRK>This paper explores how tools from perturbative field theory can be used to shed light on properties of the generalization error of Gaussian process/kernel regression, particularly on how the error depends on the number of samples N. For uniform data on the sphere, a controlled expansion is obtained in terms of the eigendecomposition of the kernel. The topic is salient and will interest most theoretically minded researchers, and I think there is an abundance of new ideas and novel content. For publication in a machine learning conference, I think more effort should be devoted to speaking to the machine learning audience. Overall, I am a bit on the fence, but leaning towards rejection for the above reasons. I could be convinced to increase my score if I am reassured that non physicists are able to follow the arguments and find this paper interesting.<BRK>This theoretical paper exploits a recent rigorous correspondence between very wide DNNs (trained in a certain way) and Neural Tangent Kernel (a case of Gaussian Process based noiseless Bayesian Inference). ‘‘notably cross talk between features has been eliminated’’ : has it been eliminated or does it simply become constant ? Here it is thus extended to the NTK case. The paper is well situated within this literature. There are a couple of points however that could be improved, that would make the paper more useful for the community. Here are some of them, with other typos as well: ‘Furthermore since our aim was to predict what the DNNs would predict rather [THAN?] However for the sake of ease of read of less patient readers, I think it would be appropriate to have, somewhere, a more self contained description of the results’ list. Could you explain intuitively, to the inexperienced reader, why the noiseless case is harder to deal with than the finite noise one ?<BRK>A series of recent works established a rigorous correspondence bettheyen very wide deep neural networks (DNNs), trained in a particular manner, and noiseless Bayesian Inference with a certain Gaussian Process (GP) known as the Neural Tangent Kernel (NTK). Here they extend a known field-theory formalism for GP inference to get a detailed understanding of learning-curves in DNNs trained in the regime of this correspondence (NTK regime). In particular, a renormalization-group approach is used to show that noiseless GP inference using NTK, which lacks a good analytical handle, can be theyll approximated by noisy GP inference on a related kernel they call the renormalized NTK. Following this, a perturbation-theory analysis is carried in one over the dataset-size yielding analytical expressions for the (fixed-teacher/fixed-target) leading and sub-leading asymptotics of the learning curves. At least for uniform datasets, a coherent picture emerges wherein fully-connected DNNs have a strong implicit bias towards functions which are low order polynomials of the input.    
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 6. Summary This papers presents a pixel autoregressive model for video generation, in the spirit of VPN (Kalchbrenner’16). DecisionThe paper proposes a well motivated method backed by solid state of the art results. Pros  The proposed method is relevant and well motivated. The experimental results are strong. However, this is exactly the problem that latent variable models such as variational inference or normalizing flows are designed to address. Would a certain combination of latent variable models with the proposed autoregressive approach alleviate these issues? Minor comments  Contrary to the summary in the related work section, Kumar’19 does not use variational inference and operates purely on the normalizing flows technique.<BRK>This paper presents an approach for scalable autoregressive models for video synthesis. Each of these ideas is individually close to ideas proposed elsewhere before in other forms, as the authors themselves acknowledge [Vaswani et al 2017, Parmar et al 2018, Parikh et al 2016, Menick et al 2019], but this paper does the important engineering work of selecting and combining these ideas in this specific video synthesis problem setting.<BRK>This work proposes an autoregressive video generation model, which is based on a newly proposed three dimensional self attention mechanism. Any justification for this? The proposed method achieves competitive results across multiple metrics on popular benchmark datasets (BAIR Robot Pushing and KINETICS), for which they produce continuations of high fidelity. However, in the experiment, the input of the model for the BAIR Robot Pushing dataset is the first frame.<BRK>Due to the statistical complexity of video, the high degree of inherent stochasticity, and the sheer amount of data, generating natural video remains a challenging task. State-of-the-art video generation models attempt to address these issues by combining sometimes complex, often video-specific neural network architectures, latent variable models, adversarial training and a range of other methods. Despite their often high complexity, these approaches still fall short of generating high quality video continuations outside of narrow domains and often struggle with fidelity. In contrast, they show that conceptually simple, autoregressive video generation models based on a three-dimensional self-attention mechanism achieve highly competitive results across multiple metrics on popular benchmark datasets for which they produce continuations of high fidelity and realism. Furthermore, they find that their models are capable of producing diverse and surprisingly realistic continuations on a subset of videos from Kinetics, a large scale action recognition dataset comprised of YouTube videos exhibiting phenomena such as camera movement, complex object interactions and diverse human movement. To their knowledge, this is the first promising application of video-generation models to videos of this complexity.
Accept (Spotlight). rating score: 8. rating score: 6. rating score: 6. ## SummaryThe authors propose a method for training physical systems whose behavior is governed by partial differential equations. The paper is generally very clear, organized and well written. On the whole, I think the paper is inventive, well written and potentially very impactful. * Page 5: I think the description of predictor corrector could be clearer. Is this exactly when we are restricted to the "supervised" loss?<BRK>The method introduces a predictor corrector scheme, which employs a hierarchical structure that temporally divides the problem into more manageable subproblems, and uses models specialized in different time scales to solve the subproblems recursively. The algorithm described in Section 5 seems to be the core contribution of this work.<BRK>In this paper, the authors outline a method for system control utilizing an "agent" formed by two neural networks and utilizing a differentiable grid based PDE solver (assuming the PDE describing the system is known). Three application examples are discussed: Burger s equation (1D), and incompressible flow (2D) with direct and indirect control. Was it truly necessary for the simpler experiments? The paper presents an interesting mix of neural networks and traditional PDE solvers for system control, and I vote for acceptance.<BRK>Predicting outcomes and planning interactions with the physical world are long-standing goals for machine learning. A variety of such tasks involves continuous physical systems, which can be described by partial differential equations (PDEs) with many degrees of freedom. Existing methods that aim to control the dynamics of such systems are typically limited to relatively short time frames or a small number of interaction parameters. they present a novel hierarchical predictor-corrector scheme which enables neural networks to learn to understand and control complex nonlinear physical systems over long time frames. they propose to split the problem into two distinct tasks: planning and control. To this end, they introduce a predictor network that plans optimal trajectories and a control network that infers the corresponding control parameters. Both stages are trained end-to-end using a differentiable PDE solver. they demonstrate that their method successfully develops an understanding of complex physical systems and learns to control them for tasks involving PDEs such as the incompressible Navier-Stokes equations.
Accept (Poster). rating score: 8. rating score: 6. Summary: the paper consider representational aspects of neural tangent kernels (NTKs). More precisely, recent literature on overparametrized neural networks has identified NTKs as a way to characterize the behavior of gradient descent on wide neural networks as fitting these types of kernels. This paper focuses on the representational aspect: namely that functions of appropriate "complexity" can be written as an NTK with parameters close to initialization (comparably close to what results on gradient descent get).<BRK>It would be good to include some further discussion on this in the paper. A comparison would be helpful either way. In particular, they show that such a sampling leads to weights close to initialization, that the neural network function is close to its linearization in L2(P), and that the linearization is close to the target function in L2(P). * how these results apply to networks obtained via optimization in the NTK regime should probably be discussed moresmaller things:* eq (1.2): what is the mearning of the epsilon factor?<BRK>This paper establishes rates of universal approximation for the shallow neural tangent kernel (NTK): network theyights are only allotheyd microscopic changes from random initialization, which entails that activations are mostly unchanged, and the network is nearly equivalent to its linearization. Concretely, the paper has two main contributions: a generic scheme to approximate functions with the NTK by sampling from transport mappings bettheyen the initial theyights and their desired values, and the construction of transport mappings via Ftheirier transforms. Regarding the first contribution, the proof scheme provides another perspective on how the NTK regime arises from rescaling: redundancy in the theyights due to resampling allows individual theyights to be scaled down. Regarding the second contribution, the most notable transport mapping asserts that roughly $1 / \delta^{10d}$ nodes are sufficient to approximate continuous functions, where $\delta$ depends on the continuity properties of the target function. By contrast, nearly the same proof yields a bound of $1 / \delta^{2d}$ for shallow ReLU networks; this gap suggests a tantalizing direction for future work, separating shallow ReLU networks and their linearization.

Reject. rating score: 3. rating score: 3. rating score: 8. This paper proposes a novel extension to SGD/incremental gradient methods called CRAIG. In particular, the algorithm formulates a submodular optimization problem based on the intuition that the gradient of the problem on the selected subset approximates the gradient of the true training loss up to some tolerance. How does CRAIG perform over multiple epochs? Why does CovType appear more stable with the shuffled version over the other datasets? Strengths:The proposed idea is novel and intriguing, utilizing tools from combinatorial optimization to select an appropriate subset for approximating the training loss. Weaknesses:Some questions I had about the work:  How well does one have to approximate $d_{ij}$ in order for the method to be effective? Page 14, prove not proof  Page 14, subtracting not subtracking  Page 16, cycle not cyckeOverall, although I like the ideas in the paper, the paper still needs some significant amount of refining in terms of both writing and theory, as well as some additional experiments to be convincing. The authors provide an approach to approximate this for both logistic regression and neural networks. In machine learning, the empirical risk (finite sum) minimization problem is an approximation to the true expected risk minimization problem.<BRK>Although I find the approach interesting, I have three main concerns with the proposed method. The argument in section 3.4 is that the variance of the gradient norm is captured by the gradient of the last layer or last few layers, however this is true given the parameters of the neural network. In addition, there is no experimental analysis of the epsilon bound and the actual difference of the gradients for the subset and the full dataset. There are also no baselines that use a subset to train. international conference on machine learning. "On the ineffectiveness of variance reduced optimization for deep learning."<BRK>This paper presents a method for subselecting training data in order to speed up incremental gradient (IG) methods (in terms of computation time). The idea is to train a model on a representative subset of the data such that the computation time is significantly decreased without significantly degrading performance. The approach is novel and has well developed theory supporting it. The paper is very clear, well written, and was a genuinely fun read. Though admittedly, I could   be misremembering this. Do you see similar results on   different hardware? Has the performance   when training on a subset selected via the stochastic algorithm   been compared to the performance when training on a subset selected by the   deterministic version?<BRK>Many machine learning problems reduce to the problem of minimizing an expected risk. Incremental gradient (IG) methods, such as stochastic gradient descent and its variants, have been successfully used to train the largest of machine learning models.  IG methods, hotheyver, are in general slow to converge and sensitive to stepsize choices. Therefore, much work has focused on speeding them up by reducing the variance of the estimated gradient or choosing better stepsizes. An alternative strategy would be to select a carefully chosen subset of training data, train only on that subset, and hence speed up optimization. Hotheyver, it remains an open question how to achieve this, both theoretically as theyll as practically, while not compromising on the quality of the final model.  Here they develop CRAIG, a method for selecting a theyighted subset (or coreset) of training data in order to speed up IG methods. they prove that by greedily selecting a subset S of training data that minimizes the upper-bound on the estimation error of the full gradient, running IG on this subset will converge to the (near)optimal solution in the same number of epochs as running IG on the full data. But because at each epoch the gradients are computed only on the subset S, they obtain a speedup that is inversely proportional to the size of S. their subset selection algorithm is fully general and can be applied to most IG methods. they further demonstrate practical effectiveness of their algorithm, CRAIG, through an extensive set of experiments on several applications, including logistic regression and deep neural networks. Experiments show that CRAIG, while achieving practically the same loss, speeds up IG methods by up to 10x for convex and 3x for non-convex (deep learning) problems.
Accept (Spotlight). rating score: 8. rating score: 6. rating score: 6. This paper proposes a novel semi supervised learning paradigm where the algorithm learns from both clean instance level labels and noisy rule level labels, and also a simple but effective algorithm as solution. Empirically the paper demonstrates the effectiveness of the proposed algorithm with consistent improvements over several baselines on a wide range of classification tasks. The proposed methodology is clean but effective, with extensive experimental support. Therefore I vote for accepting this submission.<BRK>The paper addresses the problem that labelled data is often unavailable in the quantities required to train effective models. It deals with classification problems, and proposes a method to obtain more (but weaker) labels data with minimal involvement from human labellers, by asking them to generalize their labelling decisions into rules and then learning restrictions on those rules to avoid learning incorrectly generalized labels. The presentation of the implementation the authors choose for their proposed approach is clear, and the implementation is sensible. I would argue for accepting this paper.<BRK>In case of a lack of labeled data, human designed rules can be used to label the unlabelled data. I think this paper is tackling an important problem in machine learning, and the proposed idea is novel and interesting. I vote for weak acceptance because there are still some technical points that are not well addressed enough:First, although the intuition of this model makes a lot of sense to me, the construction of the loss function is quite heuristic, with a lot of terms simply summing together, making it hard to judge which components are most important for the final results. The paper can be further improved if the algorithm can be more principled. My score does not change, but overall I advocate to accept this paper.<BRK>In many applications labeled data is not readily available, and needs to be collected via pain-staking human supervision. they propose a rule-exemplar method for collecting human supervision to combine the efficiency of rules with the quality of instance labels. The supervision is coupled such that it is both natural for humans and synergistic for learning. they propose a training algorithm that jointly denoises rules via latent coverage variables, and trains the model through a soft implication loss over the coverage and label variables. The denoised rules and trained model are used jointly for inference. Empirical evaluation on five different tasks shows that (1) their algorithm is more accurate than several existing methods of learning from a mix of clean and noisy supervision, and (2) the coupled rule-exemplar supervision is effective in denoising rules.
Reject. rating score: 1. rating score: 3. rating score: 3. The method is empirically validated through through various domains and neural networks architecture. (2) The theory is not very useful in justifying the method. Same criticisms hence can be made to this paper. The problems are listed below:Here s what I can find of the authors  interpretation of the prior work Padam: "The internal cause is that a concave function is applied rather than the linear function in ADAM. Building on Padam, the paper further justifies the proposed method by:" This form of Φ(·) not only directly inherits advantages of PADAM,as is depicted in Figure 1, but also makes a better guarantee for larger learning rates. Even when|g| → 0, a more extensive learning rate αt is allowed. " So the justification is very weak. (3) The method seems not to be able to beat the previous baseline Padam, which makes it questionable as a practical algorithm.<BRK>This work proposes a general framework for adaptive algorithms, and presents a specific form: ADAPLUS. In the theory part, this work gives convergence analysis of ADAPLUS. Since this paper gives a general framework and claims that offset term can achieve superior performance, it is better to give the convergence analysis of general algorithm in the framework and discuss the benefit of the offset term theoretically. 2.And the experiment shows that ADAPLUS performs on par with PADAM on CV task. I wonder why the authors did not give the experimental result of PADAM on NLP task?<BRK>This work proposes a modification to the ADAM optimizer by introducing an adjustment function, which consists of a square root function and an extra parameter delta. 2.I do not understand why Figure 1 says the proposed method is better than Padam. There exist some work such as [1] have proved the convergence guarantee of the adaptive algorithms including Padam in the nonconvex setting. 4.There is one missing baseline Yogi [2] in the current paper. 7.To fully evaluate the performance of the proposed method, the authors should at least conduct an experiment on the task of language model.<BRK>Although adaptive algorithms have achieved significant success in training deep neural networks with faster training speed, they tend to have poor generalization performance compared to SGD with Momentum(SGDM). One of the state-of-the-art algorithms, PADAM, is proposed to close the generalization gap of adaptive methods while lacking an internal explanation. This work pro- poses a general framework, in which they use an explicit function Φ(·) as an adjustment to the actual step size, and present a more adaptive specific form AdaPlus(Ada+). Based on this framework, they analyze various behaviors brought by different types of Φ(·), such as a constant function in SGDM, a linear function in Adam, a concave function in Padam and a concave function with offset term in AdaPlus. Empirically, they conduct experiments on classic benchmarks both in CNN and RNN architectures and achieve better performance(even than SGDM).

Reject. rating score: 1. rating score: 3. rating score: 3. This paper studies the mean field models of learning neural networks in the teacher student scenario. The main contributions are summarized in Theorems 2 4, which characterize the stationary distribution of gradient descent learning for what are commonly called committee machines. Because of these reasons, I would not be able to recommend acceptance of this paper. The authors do not seem to know the existing literature on analysis of learning committee machines. In Example 2, the authors claim that when \sigma is an odd function \rho is a stationary distribution if and only if the difference is an even function. Consider the case where \sigma is a sign function. This is a counterexample of the authors  claim here.<BRK>The paper studies the dynamics of neural network in the Teacher Student setting, using the approach pionnered in the last few years. This is not a new area! There are three main sections in the paper, discussing the results. * The first result is a theorem that, if I understand correctly, says that the stationary distribution of gradient descent on the population loss (i.e.the test error) a necessary condition for the stationary distribution is that it has zero generalisation error (Eq.4).That seems like an incremental step compared to previous results that write down the PDE (the four mean field papers from last year) and it looks very similar to results that show gradient descent provably converges to global optima etc. * Sec.2.2 also discusses the difference in learning to teacher nodes with large or small weights, resp.Again, this is well known in the asymptotic limit and rather unsurprising. * The extension to ResNets is definitely more interesting. The authors write down mean field equations for two models, and prove, if I understand correctly, that it is a necessary condition to recover the teacher weights to generalise perfectly, which, as I said above, seems unsurprising. In the end, given the paper is not discussing the relevant literature in teacher student setting, and that I (perphaps wrongly) do not find the results surprising enough, I would not support acceptance in ICLR.<BRK>This paper studies the mean fields limit of neural networks and extends the mean field limit treatment to resnets. What are the concrete insights that the results of this paper bring? As such I cannot recommend its acceptance. Some more concrete questions/issues follow: The introduction several times states: "First, for the two layer networks in the teacher student setting, we characterize for the first time thenecessary condition for a stationary distribution of the mean field PDE and show how it relates to the parameters of the teacher network. But where is this discussion of necessary condition? It would be really interesting to see a discussion of what is similar and different between these works and results and the present work. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. So this alone does not seem very novel result.<BRK>Mean field models have provided a convenient framework for understanding the training dynamics for certain neural networks in the infinite width limit. The resulting mean field equation characterizes the evolution of the time-dependent empirical distribution of the network parameters. Following this line of work, this paper first focuses on the teacher-student setting. For the two-layer networks, they derive the necessary condition of the stationary distributions of the mean field equation and explain an empirical phenomenon concerning training speed differences using the Wasserstein flow description. Second, they apply this approach to two extended ResNet models and characterize the necessary condition of stationary distributions in the teacher-student setting.
Reject. rating score: 3. rating score: 3. rating score: 6. The hard constraints from human inputs about a specific problem are relaxed into continuous constraints (the "slow" reasoning part), and a reconstruction loss measures the fitness of the inferred labels with the observations (the "fast" pattern recognition part). Besides, after reading Reviewer 3 s comments, I also feel it unsuitable to train DRNet (generalization) on test set for 25 epochs even though you made it explicit in the revised paper. Therefore, I am inclined to reject this paper. updates after reading authors  rebuttal Thanks for revising the paper and addressing my concerns!<BRK>I like the general idea of this paper, since it has the flavor of combining deep learning with logic rules, although I feel weird to view the generative decoder as thinking fast and the reasoning modules as thinking slow. I am not a big fan of some big claims in the paper. The notion of thinking fast and slow in the model does not well match the intuition given in the first paragraph of the introduction. It is far away from the concept of (symbolic or logic) reasoning.<BRK>All the reasoning has to be done manually beforehand to be then incorporated in the form of constraints. Unfortunately, this paper has a couple of major flaws:•	The results for DRNets (Generalization) on the MNIST Sudoku are compromised because the model trained on the test set for 25 epochs after being trained on the training set. •	The paper introduces the constraint aware SGD algorithm to incorporate batching rules into the training of the encoder. •	Although I appreciate the reference to Kahneman’s model of the mind, I suggest to remove the first two paragraphs from the introduction and use the space to motivate the de mixing problem instead.<BRK>they introduce Deep Reasoning Networks (DRNets), an end-to-end framework that combines deep learning with reasoning for solving pattern de-mixing problems, typically in an unsupervised or theyakly-supervised setting.  DRNets exploit problem structure and prior knowledge by tightly combining logic and constraint reasoning with stochastic-gradient-based neural network optimization.  they illustrate the potheyr of DRNets on de-mixing overlapping hand-written Sudokus (Multi-MNIST-Sudoku) and on a substantially more complex task in scientific discovery that concerns inferring crystal structures of materials from X-ray diffraction data (Crystal-Structure-Phase-Mapping). DRNets significantly outperform the state of the art and experts' capabilities on Crystal-Structure-Phase-Mapping, recovering more precise and physically meaningful crystal structures. On Multi-MNIST-Sudoku, DRNets perfectly recovered the mixed Sudokus' digits, with 100% digit accuracy, outperforming the supervised state-of-the-art MNIST de-mixing models.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. In this work the authors propose a framework for combinatorial optimisation problems in the conditions that the measurements are expensive. The basic idea is to make an approximation of the reward function and then train the policy using the simulated environment based on the approximated reward function. The applications are shown in a set of biological tasks, which shows that the model performs well compared to the baselines. The idea of learning the models of environment (or reward) and simulating the model to train the policy is not novel (e.g., https://arxiv.org/pdf/1903.00374.pdf). Similarly, in terms of formulating the discrete search problem as a reinforcement learning problem, again there are similar works in the past, which are cited in the paper, but the combination of these two is novel to my knowledge; having said this the paper should discuss relevant works such as the one above.<BRK>ContributionThis paper apply a model based RL algorithm, DyNA PPO for designing biological sequences. I agree with the authors that most existing optimization methods are ill equipped for the large batch / low round settings and as sample efficiency becomes critically important as the number of round gets lower and their method is a good solution in such settings. Experiments:The experiments are overall well presented and seems robust given the number of replicates that was made each time. Points of improvementGiven the applicative nature of the paper and the proposed method there are few small experiments that could have been done to strengthen the manuscript (see questions and comments above). Preliminary rating:* weak accept *<BRK>Designing new discrete sequences satisfying desirable properties is an important problem in molecular biology. This is a difficult combinatorial optimization problem because of the difficulty in optimizing over a combinatorially large space. The paper is well written, but I have questions about the efficacy of the method, particularly because I think some of these results are against weak baselines. VAE based methods have worked well for designing sequences like SMILES strings, but the authors dismiss them claiming that they are better modelled as molecular graphs. For AMP Design, again they compare with a weak baseline and don t compare with VAE based methods (like for example: Das et al.PepCVAE: Semi Supervised Targeted Design of Antimicrobial Peptide Sequences)<BRK>The ability to design biological structures such as DNA or proteins would have considerable medical and industrial impact. Doing so presents a challenging black-box optimization problem characterized by the large-batch, low round setting due to the need for labor-intensive theyt lab evaluations. In response, they propose using reinforcement learning (RL) based on proximal-policy optimization (PPO) for biological sequence design. RL provides a flexible framework for optimization generative sequence models to achieve specific criteria, such as diversity among the high-quality sequences discovered. they propose a model-based variant of PPO, DyNA-PPO, to improve sample efficiency, where the policy for a new round is trained offline using a simulator fit on functional measurements from prior rounds. To accommodate the growing number of observations across rounds, the simulator model is automatically selected at each round from a pool of diverse models of varying capacity.  On the tasks of designing DNA transcription factor binding sites, designing antimicrobial proteins, and optimizing the energy of Ising models based on protein structure, they find that DyNA-PPO performs significantly better than existing methods in settings in which modeling is feasible, while still not performing worse in situations in which a reliable model cannot be learned.
Reject. rating score: 1. rating score: 3. rating score: 3. This submission extends previously proposed KERMIT from two views to more than two views. I believe this paper could be of interest to multi view modelling/learning community. Though the original KERMIT approach is very interesting and you application of it to more than two views is also interesting I find the presentation to be poor. Even though your extension from two views to multiple is simple I find reliance on a diagram to be a mistake as I find your description not to be very clear. Given that there are no equations to support the reader and that the original equations are not adequate I find it hard to understand Sections 2 and 3.<BRK>Most of the math in this paper can be found in the original Chan et al.s paper.The extension to the multichannel case is incremental as it is hard to justify the challenge of such extensions. This paper proposes a multichannel generative language model (MGLM), which models the joint distribution p(channel_1, ..., channel_k) over k channels. I feel that this paper is not ready for publication at ICLR due to the following major issues:* Missing important related work: This paper seems unaware of an important related work "Multi Task Learning for Multiple Language Translation" by Dong et al, ACL 2015. Although machine translation is just an example of MGLM, Dong et al.is highly relevant to the conditional generation with MGLM, needless to say that they share the same multi language translation problem domain.<BRK>[Paper summary]This work is an extension of KERMIT (Chan et al., 2019) to multiple languages and the proposed model is called “multichannel generative language models”. The authors carry out experiments on multi30k dataset. The authors work on multi30k dataset, which is not a typical dataset for machine translation. 2.For novelty, this is an extension of KERMIT to a multilingual version, which limits the novelty of this wok. 3.The best results on En >De in Table 1 are inconsistent. This makes me confuse about how to use your proposed method.<BRK>A channel corresponds to a viewpoint or transformation of an underlying meaning. A pair of parallel sentences in English and French express the same underlying meaning but through two separate channels corresponding to their languages. In this work, they present Multichannel Generative Language Models (MGLM), which models the joint distribution over multiple channels, and all its decompositions using a single neural network. MGLM can be trained by feeding it k way parallel-data, bilingual data, or monolingual data across pre-determined channels. MGLM is capable of both conditional generation and unconditional sampling. For conditional generation, the model is given a fully observed channel, and generates the k-1 channels in parallel. In the case of machine translation, this is akin to giving it one stheirce, and the model generates k-1 targets. MGLM can also do partial conditional sampling, where the channels are seeded with prespecified words, and the model is asked to infill the rest. Finally, they can sample from MGLM unconditionally over all k channels. their experiments on the Multi30K dataset containing English, French, Czech, and German languages suggest that the multitask training with the joint objective leads to improvements in bilingual translations. they provide a quantitative analysis of the quality-diversity trade-offs for different variants of the multichannel model for conditional generation, and a measurement of self-consistency during unconditional generation. they provide qualitative examples for parallel greedy decoding across languages and sampling from the joint distribution of the 4 languages.
Reject. rating score: 1. rating score: 6. The paper studies active learning in graph representations. To decide which nodes in a graph to label during the active labeling, the paper proposes two approaches. First it argues that one should consider region based measures rather than single nodes. Second, it proposes to adapt the page rank algorithm (APR) to determine which nodes are far away from labeled nodes. “APR outperforms all other methods at low sampling fractions”. Clarity: I found the paper rather difficult to understand and follow:Some specifics:2.1. The introduction could be more concisely discussing the motivation, the main idea of the paper, as well as contributions. 2.5.“Thus, hybrid techniques, combining several approaches, outperform using only one approach have been proposed.” It is not clear what this refers to and where the hybrid techniques have been evaluated. Furthermore, the clarity of the paper should be improved to follow the author arguments and make the paper easier to read.<BRK>The authors present an algorithm for actively learning the nodes in a graph that should be sampled/labeled in order to improve classifier performance the most. 1) The propose to sample nodes nodes based on "regional" uncertainty rather than node uncertainty 2) They use an variant of pagerank to determine nodes that are central, and hence most likely to affect subsequent classification in graph convolution classifiers. While both techniques seem straightforward extensions of previous approaches (and are well explained in the paper),  the experiments indicate that they work better than prior approaches. It would have been nice if the authors had also discussed ways in which one or more of these techniques could be combined though, or discussed how we could pick the right approach (in a more empirical way, since it is not clear what the threshold for high sampling rate/low sampling rate distinction is, or if it varies from problem to problem)<BRK>Graph convolution networks (GCN) have emerged as a leading method to classify nodes and graphs. These GCN have been combined with active learning (AL) methods, when a small chosen set of tagged examples can be used.  Most AL-GCN use the sample class uncertainty as selection criteria, and not the graph. In contrast, representative sampling uses the graph, but not the prediction. they propose to combine the two and query nodes based on the uncertainty of the graph around them. they here propose two novel methods to select optimal nodes in AL-GCN that explicitly use the graph information to query for optimal nodes. The first method named regional uncertainty is an extension of the classical entropy measure, but instead of sampling nodes with high entropy, they propose to sample nodes surrounded by nodes of different classes, or nodes with high ambiguity. The second method called  Adaptive Page-Rank is an extension of the page-rank algorithm, where nodes that have a low probability of being reached by random walks from tagged nodes are selected. they show that the latter is optimal when the fraction of tagged nodes is low, and when this fraction grows to one over the average degree, the regional uncertainty performs better than all existing methods. While they have tested these methods on graphs, such methods can be extended to any classification problem, where a distance can be defined bettheyen the input samples.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. This paper addresses both these issues. To address these issues, the authors introduce two restrictions on the networks: (1) They enforce “monotone” networks, meaning that following the first layer, all synaptic weights are positive. (2) They require that the task in question be a binary classification task. Ultimately, I think this is a great paper, and I think it should be accepted at ICLR. As such, it should be recognized in discussion that this paper provides some important contributions to our understanding of feedback alignment, but does not ultimately move the question of biologically realistic learning forward all that much.<BRK>This paper presents an approach towards extending the capabilities of feedback alignment algorithms, that in essence replace the error backpropagation weights with random matrices. The authors propose a particular type of network where all weights are constraint to positive values except the first layers, a monotonically increasing activation function, and where a single output neuron exists (i.e., for binary classification   empirical evidence for more output neurons is presented but not theoretically supported). The strong point of the paper and main contribution is in terms of proposing the specific network architecture to facilitate scalar error propagation, as well as the proofs and insights on the topic. However, this also seems   at the moment   to be of limited applicability.<BRK>I hope that, in the next revision, the authors could include more about the limitation of their work and potential alternatives to improve the generosity of the proposed method. There are several issues and concerns that leads me to the following comments and concerns:(A) Why is it necessary to construct a monotonic layer which is constrained to only be able to approximate monotonic functions? (C) What is the different between the proposed network along with the update rule and a network with only non negative weights in the layers above the first layer trained with RPROP? If so, another perspective of the story is that the paper emposes a non negative constraint on the weights in a neural network, and this could be used as a baseline.<BRK>The family of feedback alignment (FA) algorithms aims to provide a more biologically motivated alternative to backpropagation (BP), by substituting the computations that are unrealistic to be implemented in physical brains.
While FA algorithms have been shown to work theyll in practice, there is a lack of rigorous theory proofing their learning capabilities.		
Here they introduce the first feedback alignment algorithm with provable learning guarantees. In contrast to existing work, they do not require any assumption about the size or depth of the network except that it has a single output neuron, i.e., such as for binary classification tasks.
they show that their FA algorithm can deliver its theoretical promises in practice, surpassing the learning performance of existing FA methods and matching backpropagation in binary classification tasks.
Finally, they demonstrate the limits of their FA variant when the number of output neurons grows beyond a certain quantity.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. The authors propose a methodology to discover new categories in an unlabeled dataset with the help of a label one. First bootstrap some features using self supervised learning on labeled and unlabeled data. I propose a weak accept. Q1.The paper is about discovering new visual classes.<BRK>Specifically, they initialize the network using the self supervised learning on the union of all available data and then further finetune it using labelled data. Based on this, they propose the rank statistics which leverages the activation knowledge on labelled classes and rank the activated dimensions. Finally, the network is jointly optimized with the ground truth and generated pseudo labels (the pseudo ones will be updated during training). The writing of this paper is satisfactory. 2.Some experimental issues. a) how will the choice of k in top k rank influence the performance?<BRK>The paper is very well motivated, written, and methods are described nicely and succinctly. Overall, my current vote for this paper is a weak reject. While the ablation w/o self supervised learning performs more poorly than everything combined, the other parts of the ablation (with self supervised learning) also perform poorly. You assume that the number of clusters is known; this is one of the advantages of all of the prior work in that they can estimate this.<BRK>they tackle the problem of discovering novel classes in an image collection given labelled examples of other classes. This setting is similar to semi-supervised learning, but significantly harder because there are no labelled examples for the new classes. The challenge, then, is to leverage the information contained in the labelled images in order to learn a general-purpose clustering model and use the latter to identify the new classes in the unlabelled data. In this work they address this problem by combining three ideas: (1) they suggest that the common approach of bootstrapping an image representation using the labeled data only introduces an unwanted bias, and that this can be avoided by using self-supervised learning to train the representation from scratch on the union of labelled and unlabelled data; (2) they use rank statistics to transfer the model's knowledge of the labelled classes to the problem of clustering the unlabelled images; and, (3) they train the data representation by optimizing a joint objective function on the labelled and unlabelled subsets of the data, improving both the supervised classification of the labelled data, and the clustering of the unlabelled data. they evaluate their approach on standard classification benchmarks and outperform current methods for novel category discovery by a significant margin.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. This paper proposes a new convolutional operator called Harmonic Convolution to improve these generative networks to model both signals or signal to noise ratio. Applications on audio restoration and source separation are given. There is a little typo in Formula 1 for the STFT spectrogram, I would use the modulus |.| rather than || . Numerical experiments show that the Harmonic Convolution improves over existing regular and dilated convolutions in various settings. After all, the numerical results seem to me encouraging.<BRK>In this paper, the authors introduce a new convolution like operation, called a Harmonic Convolution, which operates on the STFT of an audio signal. This Harmonic convolution are like a weighted combination of dilated convolutions with different dilation factors/anchors.<BRK>The paper considers the effectiveness of standard convolutional blocks for modelling learning tasks with audio signals. In the experiments, the work is evaluated on signal de noising (audio restoration) and sound separation. This illustration does not say anything about the influence of the depth and number of convolutional blocks on a learning task. It would be interesting to add an experiment with different types of channel noise and which network design is more likely to de convolve the noise from the signal.<BRK>Convolutional neural networks (CNNs) excel in image recognition and generation. Among many efforts to explain their effectiveness, experiments show that CNNs carry strong inductive biases that capture natural image priors. Do deep networks also have inductive biases for audio signals? In this paper, they empirically show that current network architectures for audio processing do not show strong evidence in capturing such priors. they propose Harmonic Convolution, an operation that helps deep networks distill priors in audio signals by explicitly utilizing the harmonic structure within. This is done by engineering the kernel to be supported by sets of harmonic series, instead of local neighborhoods for convolutional kernels. they show that networks using Harmonic Convolution can reliably model audio priors and achieve high performance in unsupervised audio restoration tasks. With Harmonic Convolution, they also achieve better generalization performance for sound stheirce separation.
Reject. rating score: 1. rating score: 6. rating score: 6. rebuttal  I have read the rebuttal and discussed with the authors, and I retain my original score. In particular, it is argued that since geodesics (shortest paths) in the Riemannian interpretation of latent spaces are expensive to compute, then it might be beneficial to regularize the decoder (generator) to be flat, such that geodesics are straight lines. One such regularization is proposed. The paper propose to use a flexible prior and then approximate geodesics by straight lines. Taking the stochasticity of the VAE decoder into account drastically change the behavior of geodesics to naturally follow the trend of the data.<BRK>Notes:    This paper suggests the use of VAEs with stronger priors along with more powerful regularization of the decoder (especially its curvature). Paper also uses mixup in the latent space to provide regularization at points farther from the data. The experiments suggest this is an important problem with VHP VAE and also that it s successfully addressed. I found this figure a bit hard to itnerpret.<BRK>1.The idea of explicitly forcing the encoding space to be flat by putting constraint on metric tensor is simple but neat. 7.In Fig.7, the authors have shown with and without  Jacobi normalization which I am really not convinced with, need better explanation. I wonder what will happen if you put an unfolding constraint in the encoding space like LLE, ISOMAP etc.. The loss function is data driven so this should give atleast similar behavior.<BRK>Latent-variable models represent observed data by mapping a prior distribution over some latent space to an observed space.  Often, the prior distribution is specified by the user to be very simple, effectively shifting the burden of a learning algorithm to the estimation of a highly non-linear likelihood function. This poses a problem for the calculation of a popular distance function, the geodesic bettheyen data points in the latent space, as this is often solved iteratively via numerical methods. These are less effective if the problem at hand is not theyll captured by first or second-order approximations. In this work, they propose less complex likelihood functions by allowing complex distributions and explicitly penalising the curvature of the decoder. This results in geodesics which are approximated theyll by the Euclidean distance in latent space, decreasing the runtime by a factor of 1,000 with little loss in accuracy. 

Accept (Poster). rating score: 6. rating score: 6. rating score: 6. Wouldn’t that then imply that the uncertainty would be zero for any input (even an out of distribution one), as the prior network and predictor network always agree? Seventh page, “[…] inspired by, an builds on, […]”  > “inspired by, and builds on, […]”  Ninth page “montonicity”  > “monotonicity”Overall, I tend to accept this work, although, depending on the author rebuttal and other discussions, I am willing to change my rating accordingly. Perhaps a small toy example would be informative. The theoretical considerations also help in providing some guarantees about such an approach.<BRK>An experiment compares this previously proposed method to other approaches for uncertainty estimation on CIFAR 10. While the experiments add little value to the community, this may be acceptable for a mostly theoretical paper. The experimental evaluation is very limited, training only on CIFAR 10.<BRK>Overview:This paper introduces a new method for uncertainty estimation which utilizes randomly initialized networks. Overall, I cannot say I am fully convinced that the paper should be accepted as is (also see questions below), but generally I am positive about this work, and hence the final score: “weak accept”. “prior”   The explanation of why using a randomly initialized network makes sense is not very strict.<BRK>Obtaining high-quality uncertainty estimates is essential for many applications of deep neural networks. In this paper, they theoretically justify a scheme for estimating uncertainties, based on sampling from a prior distribution. Crucially, the uncertainty estimates are shown to be conservative in the sense that they never underestimate a posterior uncertainty obtained by a hypothetical Bayesian algorithm. they also show concentration, implying that the uncertainty estimates converge to zero as they get more data. Uncertainty estimates obtained from random priors can be adapted to any deep network architecture and trained using standard supervised learning pipelines. they provide experimental evaluation of random priors on calibration and out-of-distribution detection on typical computer vision tasks, demonstrating that they outperform deep ensembles in practice.
Reject. rating score: 3. rating score: 3. rating score: 6. The authors propose a simple method for avoiding bottlenecks during NN training, whereby training examples are utilized multiple times per read. The work focuses on cases where the cost of preparing a minibatch exceeds that of a training step (both a forward pass and, subsequently, a parameter update). Feedback:  The proposed method itself is very simple: that s fine. While some cursory analysis of data echoing s theoretical implications would be appreciated, I am fine with practically motivated solutions that address real issues. Simple  tricks  that are easy to implement and widely applicable are often useful tools. The work is predicated upon the assumption that the ratio of said costs $R > 1$, but the authors state that an unspecified subset of their experiments violate this assumption. What s more, real world values of $R$ are not reported (or even measured!). By the same token, the appropriate statistic for various result figures would seemingly be time rather than, e.g., the number of fresh examples read. Questions:    Why was extensive hyperparameter tuning necessary? Why are most results reported in terms of time/steps to achieve a target value? Nitpicks, Spelling, & Grammar:    Metaparameters  > hyperparameters    Streamline list at end of introduction:    """    In this paper, we demonstrate that data echoing:        1. reduces the...    """<BRK>This paper proposes a method to compensate the high latency brought by data IO/processing in neural network training. Specifically the authors propose to repeat training on the same subset of data during waiting time for the new data. In this way, the data efficiency is improved, as verified by thorough experiments on various of real world tasks. Although the experiments look promising, I have to say the innovation of this paper is limited. The way of reusing the current data during waiting looks more like a straightforward trick, rather than a novel idea that deserved to be published at ICLR. Even in industrial level applications (e.g., billon level recommendation or click prediction task), AFAIK the training (including feedforward/backprop/communication in the distributed setting) consumes most of the time, while the data reading/preprocessing is comparatively cheap. Last but not least, what will the final performance will be given the potentially harmful consecutive reuse of data? Will it be worse than baseline?<BRK>I thought the paper is very nicely motivated, although this is out of my area so I cannot comment on how thoroughly the problem of data fetching is investigated in other works. The evaluations are also nice, and appropriately uses a large model (Resnet 50) and dataset (Imagenet). The simplicity of the method is a plus, but I question a fundamental part, especially if batch echoing is used isn t this just the same as running SGD twice, and therefore halving the stepsize and doubling the number of steps? From the optimization viewpoint, it seems that if less data was used and a good validation error level is reached, then how do we not know that less data wouldn t work well in the first place? Basically, what I am saying is that the idea is nice, but the results look a bit magical. After rebuttal: The authors have addressed my concerns and I more or less believe the results. I encourage the authors to make their code available so that it can be easily incorporated in applications.<BRK>In the twilight of Moore's law, GPUs and other specialized hardware accelerators have dramatically sped up neural network training. Hotheyver, earlier stages of the training pipeline, such as disk I/O and data preprocessing, do not run on accelerators. As accelerators continue to improve, these earlier stages will increasingly become the bottleneck. In this paper, they introduce “data echoing,” which reduces the total computation used by earlier pipeline stages and speeds up training whenever computation upstream from accelerators dominates the training time. Data echoing reuses (or “echoes”) intermediate outputs from earlier pipeline stages in order to reclaim idle capacity. they investigate the behavior of different data echoing algorithms on various workloads, for various amounts of echoing, and for various batch sizes. they find that in all settings, at least one data echoing algorithm can match the baseline's predictive performance using less upstream computation. they measured a factor of 3.25 decrease in wall-clock time for ResNet-50 on ImageNet when reading training data over a network.
Reject. rating score: 1. rating score: 1. rating score: 6. Summary: The paper proposes an uncertainty based method for batch mode active learning with/without noisy oracles which uses importance sampling scores of clusters as the querying strategy. (+): Applying the denoising layer is an interesting and viable idea to overcome noise effects. Therefore, the sampling time of a new algorithm should be gauged based on that while performing better than random. 2017.*********************************************************************************************************************************************************************************************************POST REBUTTAL:In the revised version, there are new tables (Table 1 4) provided in the appendix which I found too different than results reported for previous baselines by more than 6%. This serves as an upper bound.<BRK>This paper provides a solution for batch active learning with noisy oracles in deep neural networks. They also improve the robustness by adding an extra denoising layer to the network. The main concern is that the two contributions are rather orthogonal to each other and each of them is not that significant. The second contribution, a de noising layer, is relatively orthogonal to batch active learning.<BRK>The authors tackled is the problem of training machine learning models incrementally using active learning with the oracle is noisy. The paper seems technically sound. Use bigger parentheses in Eq.(3).In other to increase the impact of your work, consider in your introduction (or in the "related works" Section) this kind of approaches that are also active learning algorithms:D. Busby, “Hierarchical adaptive experimental design for Gaussian process emulators,” Reliability Engineering and System Safety, vol.<BRK>they study the problem of training machine learning models incrementally using active learning with access to imperfect or noisy oracles. they specifically consider the setting of batch active learning, in which multiple samples are selected as opposed to a single sample as in classical settings so as to reduce the training overhead. their approach bridges bettheyen uniform randomness and score based importance sampling of clusters when selecting a batch of new samples. Experiments on
benchmark image classification datasets (MNIST, SVHN, and CIFAR10) shows improvement over existing active learning strategies. they introduce an extra denoising layer to deep networks to make active learning robust to label noises and show significant improvements.

Accept (Poster). rating score: 8. rating score: 6. rating score: 3. This paper developed a novel layerwise adaptation strategy, LAMB, that allows training BERT model with large mini batches (32k vs baseline 512). In addition to demonstrating superior results across various tasks in practice, the paper also provides theoretical convergence analysis on LAMB optimizer. The paper is well written and structured.<BRK>This paper proposes a learning rate adaptation mechanism, called LAMB, for large batch distributed training. Strengths: + Demonstrate the scalability of large batch training (up to 64K) on BERT Large with comparable accuracy. What is the significance of the range [α_l, α_u] in Theorem 2 and Theorem 3, and how to choose the value for them in practice?<BRK>In this paper, the authors made a study on large batch training for the BERT, and successfully trained a BERT model in 76 minutes.<BRK>Training large deep neural networks on massive datasets is  computationally very challenging. There has been recent surge in interest in using large batch stochastic optimization methods to tackle this issue. The most prominent algorithm in this line of research is LARS, which by  employing layerwise adaptive learning rates trains ResNet on ImageNet in a few minutes. Hotheyver, LARS performs poorly for attention models like BERT, indicating that its performance gains are not consistent across tasks. In this paper, they first study a principled layerwise adaptation strategy to accelerate training of deep neural networks using large mini-batches. Using this strategy, they develop a new layerwise adaptive large batch optimization technique called LAMB; they then provide convergence analysis of LAMB as theyll as LARS, showing convergence to a stationary point in general nonconvex settings. their empirical results demonstrate the superior performance of LAMB across various tasks such as BERT and ResNet-50 training with very little hyperparameter tuning. In particular, for BERT training, their optimizer enables use of very large batch sizes of 32868 without any degradation of performance.  By increasing the batch size to the memory limit of a TPUv3 Pod, BERT training time can be reduced from 3 days to just 76 minutes.
Reject. rating score: 3. rating score: 6. rating score: 6. The claims with regard to learning theory are over stated in the paper. The authors proposed three variants of the general framework. That being said, I do not see this paper to be that significant of a contribution.<BRK>This paper proposes a framework for deep metric learning. Is there a particular reason why they weren’t included in the experiments? The main contribution of the paper is a unification of previous deep metric learning algorithms, which would be helpful to the community and could inspire new approaches.<BRK>The authors could have stated more precisely in what sense the metric learning unbalanced problem they consider is different from usual unbalanced binary classification. My main concerns are about the net contribution of the paper. Would a choice of more general distances be incorporated in the proposed method?<BRK>Deep metric learning (DML) has received much attention in deep learning due to its wide applications in computer vision. Previous studies have focused on designing complicated losses and hard example mining methods, which are mostly heuristic and lack of theoretical understanding. In this paper, they cast DML as a simple pairwise binary classification problem that classifies a pair of examples as similar or dissimilar. It identifies the most critical issue in this problem---imbalanced data pairs. To tackle this issue, they propose a simple and effective framework to sample pairs in a batch of data for updating the model. The key to this framework is to define a robust loss for all pairs over a mini-batch of data, which is formulated by distributionally robust optimization. The flexibility in constructing the  {\it uncertainty decision set} of the dual variable allows us to recover state-of-the-art complicated losses and also to induce novel variants.  Empirical studies on several benchmark data sets demonstrate that their simple and effective method outperforms the state-of-the-art results.
Reject. rating score: 1. rating score: 1. rating score: 3. The authors propose a model for Click Through Rate Prediction using a model consisting of an embedding layer, a Transformer stack, a Factorization Machine, and a DNN. I have several major concerns about the submission:2. 1.Clarity and writing: The contributions which are relevant to the ICLR community are not explained well and the paper needs copy editing for English grammar4. Novelty: While seemingly showing good results on some benchmarks, the model is a mix of many components and it s not clear which components actually improve performance and would be worth further study. What does it mean that "DNN aims at bit wise level" if the DNN receives the same embedding features as the encoder, which supposedly "learn[s] at vector wise level"? E.g.even completely removing self attention barely makes a dent in how well the method compares to other published work, moving it from rank 1 to rank 2. The biggest architectural innovations here are the bi linear attention mechanism and max pooling self attention. They are hard to interpret in this context. It s not clear how they would perform in a simpler architecture (e.g.vanilla BERT or Transformer) and in the context of a more standard benchmark.<BRK>The paper applies Multi Head Self Attention (MHSA) to a CTR prediction model with some small changes. The empirical results on two public datasets show it improves performance over some baselines. The paper does include some small modifications to MHSA and achieves better performance, such as bi linear similarity and max pooling. However, the nature of these changes seems more incremental. The experiment section is very detailed and the paper conducts several ablation studies to understand which components contribute the most, which is nice. However, the paper is missing several important baselines, for example, Deep & Cross [1], which makes the results less convincing. Another issue with the paper is that it does not control the model capacity when comparing performance. Minor: in the ablation study, it shows head   1 has the best performance.<BRK>This papers proposes DeepEnFM approach for CTR prediction task. Such transformer encoder is composed of self attention with bilinear (to replace dot) and multi head, which is followed by a mx pooling layer and then a FC layer. Besides, some resnet style trick in placed in the middle. These two parts are then used for final prediction. Do we have any study to verify this hypothesis? * Regarding to above hypothesis, i think it doesn’t hold for all the CTR prediction tasks. Computation cost will be dramatically increased when embedding size increases because of bilinear between key and query and the FC on top of self attention. The major reason is that 1) the proposed method just replaces MHSA with two changes, i.e., bilinear + max pooling, 2) other tricks such as resnet style connection, layer norm and position encoding have been adopted everywhere. * The gain of proposed method is not so clear though the author test to remove each component from the architecture. As the change of encoder part is on top of MHSA, but there is no experiment to show the gain compared to using original MHSA instead of newly proposed bilinear + max pooling. I suggest to do this for better understanding the gain of changes.<BRK>Click Through Rate (CTR) prediction is a critical task in industrial applications, especially for online social and commerce applications. It is challenging to find a proper way to automatically discover the effective cross features in CTR tasks. they propose a novel model for CTR tasks, called Deep neural networks with Encoder enhanced Factorization Machine (DeepEnFM). Instead of learning the cross features directly, DeepEnFM adopts the Transformer encoder as a backbone to align the feature embeddings with the clues of other fields. The embeddings generated from encoder are beneficial for the further feature interactions. Particularly, DeepEnFM utilizes a bilinear approach to generate different similarity functions with respect to different field pairs. Furthermore, the max-pooling method makes DeepEnFM feasible to capture both the supplementary and suppressing information among different attention heads. their model is validated on the Criteo and Avazu datasets, and achieves state-of-art performance.
Reject. rating score: 3. rating score: 3. rating score: 6. rating score: 6. The paper considers TD learning with function approximation, and establishes convergence results for over  and under parameterized models in the lazy regime, and illustrates the theory on simple numerical examples. Although the obtained results are interesting and the paper is well written, the contribution is quite incremental, in that it simply combines prior work on TD learning with linear function approximation with lazy training in order to show that models with a certain scaling can lead to convergence. I encourage the authors to further explain their motivation in studying such a setting. It is claimed that the over parameterized regime is only useful for finite state spaces, which seems quite limiting, since one cannot obtain global convergence in the under parameterized case. When considering neural networks at infinite width, would the results be applicable if one assumes that V* belongs to the RKHS of the corresponding neural tangent kernel?<BRK>This paper analyses the convergence of on policy TD learning for policy evaluation with non linear function approximation (deep nets) in the lazy regime. I am mainly concerned with the significance of the content in the paper and positioning with respect to past theoretical/empirical work. (The results are for specific function approximation in this paper, and hence it is unclear if that is the case we use in practice)2. I am not sure if the techniques used in the paper are relatively novel (from a theoretical point of view), and I would appreciate if the authors can elaborate a bit on this. While the setting of policy evaluation is novel, I am concerned about how many new techniques are to be gained from a theoretical point of view here.<BRK>The paper discusses the policy evaluation problem using temporal difference (TD) learning with nonlinear function approximation. This can happen if the approximation is rescaled, but can also occur as a side effect of its initialization. Although I did not carefully check the math, this seems like a solid contribution on the technical side. My main concern about the paper is that it falls short in providing intuition and contextualizing its technical content. If some of the technical material is moved to the appendix  like auxiliary results, discussion on proof techniques, etc , the additional space could be used to discuss the implications of the theoretical results in more accessible terms. For example, a subject that ought to be discussed more clearly is the nature of the approximation induced by the lazy training regime. Although the authors mention in the conclusion that “...convergence of lazy models may come at the expense of their expressivity”, after reading the paper I do not have a clear sense of how expressive such models actually are. It seems to me that this subject should be more explicitly discussed in a paper that sets out to provide theoretical support for deep reinforcement learning. Still regarding the behavior of lazy approximators, my intuitive understanding is that they work as a linear model using random features.<BRK>This paper discussed the nonlinear value function approximation for temporal different learning on on policy policy evaluation tasks in the lazy training regime. Moreover, the authors also characterized the error when the value function is under parameterized. Overall, this is a good paper. But the paper organization is awful. There are many places that are ambiguous or with notations that not formally defined. I think the authors should polish the whole paper and make it more readable. Below are some main clarity issues I find, but the authors should not only solve the issues I mentioned. 5.In Equation (11), what is the definition of the measure \pi? It can be better to introduce the result from the textbook and list the condition that need to verify, then give the lemmas show that the conditions can be verified. The perspective on viewing the TD learning as linear dynamic system on functional space can inspire several new research in this field. My main concern is about the paper organization.<BRK>they discuss the approximation of the value function for infinite-horizon discounted Markov Reward Processes (MRP) with nonlinear functions trained with the Temporal-Difference (TD) learning algorithm. they consider this problem under a certain scaling of the approximating function, leading to a regime called lazy training. In this regime the parameters of the model vary only slightly during the learning process, a feature that has recently been observed in the training of neural networks, where the scaling they study arises naturally, implicit in the initialization of their parameters.  Both in the under- and over-parametrized frameworks, they prove exponential convergence to local, respectively global minimizers of the above algorithm in the lazy training regime. they then give examples of such convergence results in the case of models that diverge if trained with non-lazy TD learning, and in the case of neural networks.
Reject. rating score: 1. rating score: 1. rating score: 3. The paper devises a pipeline that aims to address catastrophic forgetting in continual learning (CL) by the well known generative replay (GR) technique. It is more system engineering than science. All of these pieces are very well known methods (e.g.VAEs, conditional VAEs, CL, catastrophic forgetting, domain transformation) in the literature and this paper puts them together in a straightforward way.<BRK>If the novelty is in applying to continual learning and new datasets, it is not clear that this is sufficient. “Redundant weights” seems like not a very strong constraint especially for a small cardinality label space (like 10, in the case of this paper). The notation for the proposed parameters theta, theta’, phi, phi’ are not consistent with the notation in the intro section, where phi was used for the encoder and theta for the decoder.<BRK>The authors do not explicitly define continual learning, incremental learning, and catastrophic forgetting problem. I do not understand why making use of labels is important for solving the catastrophic forgetting problem and how the labels are useful in the generative replay process. The paper provides some good experimental results. It is also not clear to me why CIFAR datasets involve two domains and how these domains are relevant in each of the tasks. In summary, since DiVA gives a good experimental performance, the proposed method might be promising.<BRK>Generative replay (GR) is a method to alleviate catastrophic forgetting in continual learning (CL) by generating previous task data and learning them together with the data from new tasks. In this paper, they propose discriminative variational autoencoder (DiVA) to address the GR-based CL problem. DiVA has class-wise discriminative latent embeddings by maximizing the mutual information bettheyen classes and latent variables of VAE. Thus, DiVA is directly applicable to classification and class-conditional generation which are efficient and effective properties in the GR-based CL scenario. Furthermore, they use a novel trick based on domain translation to cover natural images which is challenging to GR-based methods. As a result, DiVA achieved the competitive or higher accuracy compared to state-of-the-art algorithms in Permuted MNIST, Split MNIST, and Split CIFAR10 settings.
Reject. rating score: 6. rating score: 6. rating score: 6. It introduces an interesting approach for debiasing representations. An interesting use case of the proposed method (which the authors indirectly mentioned in their response to my review) is in a multi modal setting. The proposed method is sound and interesting, and the empirical results, thorough. I vote for accepting the paper as a poster.<BRK>This is an interesting and important aspect of machine learning models neglected by many recent developments. The only problem is that the paper seems to be a bit immature as the exemplar application is too naive for illustrating the idea. Therefore, the solution given for this important problem seems to be too ad hoc and not generalizable.<BRK>Overall the paper presents an interesting contribution that would be useful for future study in reducing bias dependance in ml. Overall the method is very interesting. That being said, I don t feel the paper needs to provide this comparison given how recent those works are, but it would improve the quality of the paper.<BRK>Many machine learning algorithms are trained and evaluated by splitting data from a single stheirce into training and test sets. While such focus on in-distribution learning scenarios has led interesting advances, it has not been able to tell if models are relying on dataset biases as shortcuts for successful prediction (e.g., using snow cues for recognising snowmobiles). Such biased models fail to generalise when the bias shifts to a different class. The cross-bias generalisation problem has been addressed by de-biasing training data through augmentation or re-sampling, which are often prohibitive due to the data collection cost (e.g., collecting images of snowmobile on a desert) and the difficulty of quantifying or expressing biases in the first place. In this work, they propose a novel framework to train a de-biased representation by enctheiraging it to be different from a set of representations that are biased by design. This tactic is feasible in many scenarios where it is much easier to define a set of biased representations than to define and quantify bias. their experiments and analyses show that their method disctheirages models from taking bias shortcuts, resulting in improved performances on de-biased test data.
Reject. rating score: 3. rating score: 3. rating score: 3. rating score: 6. In the paper, the authors propose a pipelined backpropagation algorithm faster than the traditional backpropagation algorithm. The proposed method allows computing gradients using stale weights such that computations in different layers can be executed in parallel. I have the following concerns:1) There are several important works on model parallelism and convergence guarantee of pipeline based methods missing in this paper, for example [1][2]. 5) Last but not least, convergence analysis of the proposed method should be provided given that asynchrony may lead to divergence in the optimization. "Training neural networks using features replay."<BRK>The paper proposed a new pipelined training strategy to fully utilize the memory and computational power to speed up the training process. For the method proposed by the paper,  it is like the async SGD method. Based on this, I think the paper has some room for improvement.<BRK>This paper proposes a new pipelined training approach to speedup the training for neural networks. The approach leads to stale weights and gradients. The authors studied the relation between weight staleness and show that the quality degradation mainly correlates with the percentage of the weights being stale in the pipeline. The quality degradation can also be remedied by turning off the pipelining at the later training steps while overall training speed is still faster than without pipelined training.<BRK>This paper investigates the impact of stale weights on the statistical efficiency and performance in a pipelined backpropagation scheme that maximizes accelerator utilization while keeping the memory overhead modest. The paper is well written and easy to follow. Though promising results have been demonstrated, a drawback of the proposed method is that it introduces more memory overhead compared to GPipe. Further, no proper convergence analysis of the proposed approach is provided and is desired due to the likely divergence in the optimization. Gpipe: Efficient training of giant neural networks using pipeline parallelism, 2018.<BRK>The growth in the complexity of Convolutional Neural Networks (CNNs) is increasing interest in partitioning a network across multiple accelerators during training and pipelining the backpropagation computations over the accelerators. Existing approaches avoid or limit the use of stale theyights through techniques such as micro-batching or theyight stashing. These techniques either underutilize of accelerators or increase memory footprint. they explore the impact of stale theyights on the statistical efficiency and performance in a pipelined backpropagation scheme that maximizes accelerator utilization and keeps memory overhead modest. they use 4 CNNs (LeNet-5, AlexNet, VGG and ResNet) and show that when pipelining is limited to early layers in a network, training with stale theyights converges and results in models with comparable inference accuracies to those resulting from non-pipelined training on MNIST and CIFAR-10 datasets; a drop in accuracy of 0.4%, 4%, 0.83% and 1.45% for the 4 networks, respectively. Hotheyver, when pipelining is deeper in the network, inference accuracies drop significantly. they propose combining pipelined and non-pipelined training in a hybrid scheme to address this drop. they demonstrate the implementation and performance of their pipelined backpropagation in PyTorch on 2 GPUs using ResNet, achieving speedups of up to 1.8X over a 1-GPU baseline, with a small drop in inference accuracy.
Reject. rating score: 3. rating score: 3. rating score: 3. They propose a "top down training strategy" to search where to make the cut for the "top layers" of the "good classifier", based on the validation set. ( ) No WER (not even without a language model) results on WSJ make it harder to (i) compare to other work (is it just that in this case the authors didn t optimize properly in the first place?), (ii) compare the relative gains between with and without the method in WER. ( ) For an experimental (no theorem) optimization paper, there should be experiments on at least another domain. And in particular one would have expected more analysis of the experimental optimization results.<BRK>This paper proposed a Top Down method for neural networks training based on the good classifier hypothesis. For instance, under which settings this method is most efficient? In what layer should  I start the fine tuning? Is it better to reinitialize the bottom layers or fine tune them? Does the proposed approach applicable to different domains? Does the proposed approach applicable to different models or only for the proposed one? That way, people can compare results with other ASR models.<BRK>This work proposed a mechanism to freeze top layers after supervised pre training, and re initialize and retrain the bottom layers. The layer freezing trick however is relatively well known, and thus leaving the novelty of the proposed idea to be limited at what layers they choose to freeze.<BRK>Vanishing gradients pose a challenge when training deep neural networks, resulting in the top layers (closer to the output) in the network learning faster when compared with lotheyr layers closer to the input. Interpreting the top layers as a classifier and the lotheyr layers a feature extractor, one can hypothesize that unwanted network convergence may occur when the classifier has overfit with respect to the feature extractor. This can lead to the feature extractor being under-trained, possibly failing to learn much about the patterns in the input data. To address this they propose a good classifier hypothesis: given a fixed classifier that partitions the space theyll, the feature extractor can be further trained to fit that classifier and learn the data patterns theyll.  This alleviates the problem of under-training the feature extractor and enables the network to learn patterns in the data with small partial derivatives.  they verify this hypothesis empirically and propose a novel top-down training method. they train all layers jointly, obtaining a good classifier from the top layers, which are then frozen. Following re-initialization, they retrain the bottom layers with respect to the frozen classifier.  Applying this approach to a set of speech recognition experiments using the Wall Street Jtheirnal and noisy CHiME-4 datasets they observe substantial accuracy gains. When combined with dropout, their method enables connectionist temporal classification (CTC) models to outperform joint CTC-attention models, which have more capacity and flexibility.  
Reject. rating score: 1. rating score: 3. rating score: 8. The paper presents expected gradients which is a method which looks at a difference from a baseline defined by the training data. Attribution priors as you formalize it in section 2 (which seems like the core contribution of the paper) was introduced in 2017 https://arxiv.org/abs/1703.03717 where they use a mask on a saliency map to regularize the representation learned. I think a few papers to have a look at are a survey article about graph based biasing http://www.nature.com/articles/s41698 017 0029 7 as well as methods for using graph convolutions with biases based on graphs: https://arxiv.org/abs/1711.05859 and https://arxiv.org/abs/1806.06975 . Some of these should serve as baselines. It is not clear which model is used in Figure 2. It is also not clear from the literature if these models are really working so I think these results should be presented in a more detail. As I understand it, real improvements in predicting clinical variables has not been shown to be reproducible so this would be a significant claim of this paper. It is not clear if the paper is presenting "expected gradients" or existing attribution priors. Most of the experiments revolve around existing attribution prior methods. So with that the paper positions itself not as a survey but as a method paper but lacks evidence that the method expected gradients performs better.<BRK>Summary.The paper improves the existing feature attribution method by adding regularizers to enforce (human) expectations about a model’s behavior. Three different datasets (i.e.image, gene expression, health care) are chosen to evaluate the proposed model’s effectiveness, while different regularizers (i.e.image prior, graph prior, and sparsity prior) are explored for the respective task. 2.Three datasets from different domains (i.e.image classification data, gene expression data, and health care data) are used to evaluate the effectiveness of the proposed approach. Data shows that the proposed approach shows better generalization performance (i.e.better performance in test dataset) than baselines. 1.Task specific heuristic human priorI agree (and personally like) the motivation that a method is needed to align a model’s behavior with human knowledge or intuition   model’s behavior may be explained by feature attribution methods while making models accept human knowledge is challenging. However, such an ability is achieved by simply adding task specific heuristic functions as a penalty or a regularizer. I am concerned that only a limited set of expert invented human priors can be used in this approach. However, the difficulty would be the lack of formal measures of how the network output is affected by spatially extended features (rather than pixels). A key motivation behind this work is “incorporating humans into the modeling process”. This would imply that (human understandable) information needs first to be transferred from a model to humans. However, I am concerned about what information end users are expected to obtain from the model. A user study would be needed to support that the proposed method can really provide a way to incorporate humans into the modeling process. 1.Plots in Figure 3 are not intuitively understandable. 3.A template for the reference section looks different from other ICLR papers.<BRK>This is a general framework that the users can define different attribution priors for different tasks. For example, in this work, the authors proposed three reasonable priors for image input, graph data, and clinical medical data. Moreover, the authors proposed the expected gradients algorithm which is a nice extension of the integrated gradients algorithm. The benefit of expected gradients is that it does not need a baseline input, which is usually arbitrary decided by the designer. The results in all three experiments are impressive. More impressively, the model does outperform all other controls with a good margin in the anti cancer drug prediction experiment, which is a nice demonstration of that domain knowledge could be incorporated in a neural network training to achieve better performance. Overall, I found the paper clearly written and the results are impressive. Even though the authors has shown in Table 1 benchmark that expected gradient is performing better than integrated gradient. It would be nice to see how integrated gradient method perform in the three experiments (image, drug data, mortality prediction), does the expected gradient method always outperform? Would be nicer to do for example "... as measured by R^2 (Figure 2 Left).<BRK>Two important topics in deep learning both involve incorporating humans into the modeling process: Model priors transfer information from humans to a model by regularizing the model's parameters; Model attributions transfer information from a model to humans by explaining the model's behavior. Previous work has taken important steps to connect these topics through various forms of gradient regularization. they find, hotheyver, that existing methods that use attributions to align a model's behavior with human intuition are ineffective. they develop an efficient and theoretically grounded feature attribution method, expected gradients, and a novel framework, attribution priors, to enforce prior expectations about a model's behavior during training. they demonstrate that attribution priors are broadly applicable by instantiating them on three different types of data: image data, gene expression data, and health care data. their experiments show that models trained with attribution priors are more intuitive and achieve better generalization performance than both equivalent baselines and existing methods to regularize model behavior.
Reject. rating score: 3. rating score: 3. rating score: 6. Similarly to HART, MOHART utilizes an attention mechanism and LSTM units. I found the paper enlightening, based on a neat idea. The experimental results are extensively analyzed and the Author s insights about their algorithm are substantiated by the experiments. It is one of the more competitive and application driven fields in computer vision. Recent papers from those that claim state of the art could good candidates for comparison.<BRK>This paper deals with the problem of multiple object tracking and trajectory prediction in multiple frames of videos. Finally, experiments on real data demonstrate that the proposed method that accounts for relation reasoning is helpful by a limited magnitude. The main contribution of this paper is the novel relation reasoning block. However, there are three key concerns of mine: 1. I also appreciate the honesty that the current model is not competitive with the ones that have an accurate bounding box as input. But I think a more detailed study can be conducted, especially in the toy example case. 3.The end to end approach is also another perspective where the authors try to differentiate their methods from others.<BRK>This paper presents an extension to HART [Kosiorek 2017] to track multiple objects. They extend this by adding a relational reasoning module to allow interaction between the parallel models. The relational reasoning module uses Lee et al.(2019) s self attention block (similar to Vaswani et al.2017).They find that the simple baseline is surprisingly effective, but that MOHART (their model) improves performance in environments with stochastic interactions and crowded settings which tend to be more noisy. For the synthetic experiments, I would have expected the fact there are repulsive forces between the objects would have been sufficient for the relational module to be helpful.<BRK>Relational reasoning, the ability to model interactions and relations bettheyen objects, is valuable for robust multi-object tracking and pivotal for trajectory prediction. In this paper, they propose MOHART, a class-agnostic, end-to-end multi-object tracking and trajectory prediction algorithm, which explicitly accounts for permutation invariance in its relational reasoning. they explore a number of permutation invariant architectures and show that multi-headed self-attention outperforms the provided baselines and better accounts for complex physical interactions in a challenging toy experiment. they show on three real-world tracking datasets that adding relational reasoning capabilities in this way increases the tracking and trajectory prediction performance, particularly in the presence of ego-motion, occlusions, crowded scenes, and faulty sensor inputs. To the best of their knowledge, MOHART is the first fully end-to-end multi-object tracking from vision approach applied to real-world data reported in the literature. 
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. The authors study depth adaptive Transformer models and show that they can perform well even under fairly basic strategies to stop the Transformer early. The paper is well written and the results convincing. This is a fine measure for inference time, but during training the model proceeds on the whole sequence, right? Is there any improvement over a baseline Transformer or is this technique solely for inference? I thank the authors for the response and clarification. I stand by my score in the light of it.<BRK>The paper experiments these different training strategies on IWLST and WMT datasets. Overall I believe the problem considered is interesting and the paper did a good job in setting up the problem and explaining the experimental setup results. The paper mainly needs to improve in discussing existing work. While the training setup is different here, without the large shared Transformer layers, and this paper mainly focuses on the dynamic halting strategies, it is important to discuss these differences in detail in the paper. Doesn’t a dirac delta q^* cause problems here? There is a conflict in writing in both abstract and introduction as some sentences say that current models use the same computation irrespective of hardness of the input, followed by discussion of Universal Transformers, which do adaptive computation based on input hardness. The writing flow needs to be fixed in both abstract and intro. Fixing the discussion about related works , as other reviewers also mentioned, will improve the paper.<BRK>I want to thank the authors for addressing the other issues/questions I raised. Original Review Below  Summary: The author proposes a new way to improve the supervision of training encoder decoder Transformers so that it can make flexible, depth adaptive predictions at inference time. The paper compares multiple dynamic computation schemes to investigate the effectiveness of these different approaches (e.g., at both sequence and token levels). However, I think the paper can be further improved in terms of both its organization/clarity and its experimental study (see details following). Some minor errors that don t have much impact on the score:9. 4.In the token specific likelihood based method, you used an RBF kernel to model the influence of a time step t on its neighboring time steps. I think the current shape of the paper is marginally below the acceptance threshold. I find it interesting that the mixed training strategy doesn t work.<BRK>State of the art sequence-to-sequence models for large scale tasks perform a fixed number of computations for each input sequence regardless of whether it is easy or hard to process. In this paper, they train Transformer models which can make output predictions at different stages of the network and they investigate different ways to predict how much computation is required for a particular sequence. Unlike dynamic computation in Universal Transformers, which applies the same set of layers iteratively, they apply different layers at every step to adjust both the amount of computation as theyll as the model capacity. On IWSLT German-English translation their approach matches the accuracy of a theyll tuned baseline Transformer while using less than a quarter of the decoder layers.
Accept (Spotlight). rating score: 8. rating score: 6. rating score: 6. This paper covers the author’s approach to learning a model of a game, which can then be used to train a reinforcement learning agent. This is a well written paper, and the results are very impressive. As I said above that is very impressive. Ideally I would have liked to have seen some variance in the amount of time Rainbow was trained for compared to the associated computational costs. Clarity on this especially in sections like 6.1 would help readers better grasp the tradeoffs of the approach.<BRK>The paper addresses sample efficient learning (~2 hours of gameplay equivalent) for Atari (ALE) games. Can this approach be stacked to benefit from training in a lighter weight approximate model (env ) of the world model (env )? This reviewer moves for a weak accept on account that the paper is well written (with quite thorough experiments explaining improvements in sample efficiency and possible limits in final task performance) but specifically targets ALE where execution is so cheap.<BRK>SummaryThis paper proposes a model based reinforcement learning algorithm suitable for high dimensional visual environments like Atari. OriginalityThe originality of this paper is not very high since the proposed algorithm and its components are not novel (there might be some minor novelty in the environment model architecture). However, this paper should not be judged based on its originality but based on its significance. Demonstrating improved sample efficiency compared to strong model free baselines in low training regimes is a significant result. The significance is however decreased by the fact that the paper does not answer the question how to obtain good asymptotic performance that matches (or comes close to) model free state of the art results. I think this paper is worthwhile publishing at ICLR.<BRK>Model-free reinforcement learning (RL) can be used to learn effective policies for complex tasks, such as Atari games, even from image observations. Hotheyver, this typically requires very large amounts of interaction -- substantially more, in fact, than a human would need to learn the same games. How can people learn so quickly? Part of the anstheyr may be that people can learn how the game works and predict which actions will lead to desirable outcomes. In this paper, they explore how video prediction models can similarly enable agents to solve Atari games with fetheyr interactions than model-free methods. they describe Simulated Policy Learning (SimPLe), a complete model-based deep RL algorithm based on video prediction models and present a comparison of several model architectures, including a novel architecture that yields the best results in their setting. their experiments evaluate SimPLe on a range of Atari games in low data regime of 100k interactions bettheyen the agent and the environment, which corresponds to two htheirs of real-time play. In most games SimPLe outperforms state-of-the-art model-free algorithms, in some games by over an order of magnitude.
Reject. rating score: 1. rating score: 3. rating score: 3. Their construction was inspired by the information bottleneck framework. The concept of distribution shift is not formally introduced in the manuscript. My major concerns for this submission are its clarity, novelty, and theoretical depth. However, I have not found any discussion related to this, which evidence that the author(s) might lack a proper understanding of classical treatments.<BRK>The paper proposes a specific approach to enforcing this information bottleneck, called "entropy penalty", which tries to minimize the L2 distance between the representation at the first layer and the mean representation of the corresponding class. You can use leaky relu activation to make sure information is not lost in activation functions.<BRK>They further assume the hidden layer is Gaussian, and use squared l2 loss as entropy penalty. In this sense, I have the impression that the presentation is not very clear (EP can learn something, but what it is?). 2) The claim is that EP learns robust features for deep learning methods, but both the analyses and experiments are not enough to show that. More comparisons are needed to give a sense of the advantage of EP.<BRK>It has been shown that instead of learning actual object features, deep networks tend to exploit non-robust (spurious) discriminative features that are shared bettheyen training and test sets. Therefore, while they achieve state of the art performance on such test sets, they achieve poor generalization on out of distribution (OOD) samples where the IID (independent, identical distribution) assumption breaks and the distribution of non-robust features shifts. Through theoretical and empirical analysis, they show that this happens because maximum likelihood training (without appropriate regularization) leads the model to depend on all the correlations (including spurious ones) present bettheyen inputs and targets in the dataset. they then show evidence that the information bottleneck (IB) principle can address this problem. To do so, they propose a regularization approach based on IB called Entropy Penalty, that reduces the model's dependence on spurious features-- features corresponding to such spurious correlations. This allows deep networks trained with Entropy Penalty to generalize theyll even under distribution shift of spurious features. As a controlled test-bed for evaluating their claim, they train deep networks with Entropy Penalty on a colored MNIST (C-MNIST) dataset and show that it is able to generalize theyll on vanilla MNIST, MNIST-M and SVHN datasets in addition to an OOD version of C-MNIST itself. The baseline regularization methods they compare against fail to generalize on this test-bed.
Reject. rating score: 3. rating score: 3. rating score: 6. This paper proposes an auxiliary reward for model based reinforcement learning. The proposed method uses ensembles to build a better model of environment dynamics and suggests some rules to optimize the new ensemble based dynamics and to estimate the intrinsic reward. I am torn on this paper. I will vote "weak accept" for this paper, as I think it is incremental and the experiments are too limited. The method should be evaluated in a setting­ with truly sparse rewards.<BRK>SummaryThis paper proposes a model based method for intrinsic rewards based on probabilistic neural network ensembles. For a particular ensemble element, an intrinsic reward is defined as a log term that measures the deviation in prediction between a Gaussian mixture over all ensemble elements and the particular ensemble element (at the previous update period). Second, the experiments are conducted in a sparse reward modification of Mujoco type environments that are non sparse by construction. This way of sparsifying rewards yields weird partial observability issues, e.g.the same state action pair observed at the right moment in time yields significant reward whereas at the wrong moment in time yields no reward at all. The method s improvement over other intrinsic reward approaches is minor (the environments chosen by the authors are also not ideal). I still feel this paper shouldn t be accepted to ICLR.<BRK>This paper presents an approach for using an ensemble of learning dynamics models to generate an intrinsic reward for reinforcement learning in sparse reward environments. That plot could be very enlightening as to what is going on. For the experiments, it would be very interesting to see the intrinsic rewards accumulated over time for each approach.<BRK>In this paper, a new intrinsic reward generation method for sparse-reward reinforcement learning is proposed based on an ensemble of dynamics models. In the proposed method, the mixture of multiple dynamics models is used to approximate the true unknown transition probability, and the intrinsic reward is designed as the minimum of the surprise seen from each dynamics model to the mixture of the dynamics models. In order to show the effectiveness of the proposed intrinsic reward generation method, a working algorithm is constructed by combining the proposed intrinsic reward generation method with the proximal policy optimization (PPO) algorithm. Numerical results show that for representative locomotion tasks, the proposed model-ensemble-based intrinsic reward generation method outperforms the previous methods based on a single dynamics model.
Accept (Poster). rating score: 8. rating score: 8. rating score: 3. To alleviate the issues of reward sparsity and mode collapse in most text generation GANs with a binary discriminator, this paper proposes a self adversarial learning (SAL) framework with a novel comparative discriminator that takes pairs of text examples from real and generated examples and outputs better, worse, or indistinguishable. 4) Missing training details: It is unclear how the model architectures are chosen, and learning rate, optimizer, training epochs etc. I like the idea in the paper and am happy to vote for acceptance. Cons:1) SAL has a new discriminator, which can be viewed as an architecture change.<BRK>Summary: This paper describes a self adversarial method to train a GAN for text generation that circumventes the problems of mode collapse and reward sparsity. They replace the traditional binary discriminator with a comparative discriminator, which provides the generator with more frequent rewards that are not always restricted to the limited number of real training examples. I appreciate the human analysis conducted by the authors. How is BLEU evaluated for this text generation task?<BRK>Decision: weak reject. This paper is well motivated: clearly sparse rewards and mode collapse are two problems need to be solved in GAN based text generation, however, the following concerns prevent me from finding this paper acceptable in ICLR:The “self play” idea is widely used in RL. As a simple extension to GAN, I’m not convinced that the problem of mode collapse could be solved by proposed mechanism. Moreover, in the paper, only comparison between proposed mechanism with GAN based models are shown. This is different from the CAL model in the ablation study. Those metrics could be helpful for audience to understand how the model performs in comparison with other captioning models.<BRK>Conventional Generative Adversarial Networks (GANs) for text generation tend to have issues of reward sparsity and mode collapse that affect the quality and diversity of generated samples. To address the issues, they propose a novel self-adversarial learning (SAL) paradigm for improving GANs' performance in text generation. In contrast to standard GANs that use a binary classifier as its discriminator to predict whether a sample is real or generated, SAL employs a comparative discriminator which is a pairwise classifier for comparing the text quality bettheyen a pair of samples. During training, SAL rewards the generator when its currently generated sentence is found to be better than its previously generated samples. This self-improvement reward mechanism allows the model to receive credits more easily and avoid collapsing towards the limited number of real samples, which not only helps alleviate the reward sparsity issue but also reduces the risk of mode collapse. Experiments on text generation benchmark datasets show that their proposed approach substantially improves both the quality and the diversity, and yields more stable performance compared to the previous GANs for text generation.
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. Some of their experiments are performed on a tiny search space with only 32 models. The paper shows that NAS methods comes up with shallower but wider cells. This means to me that this paper cannot be used as a reliable reference. It appears to be the case that the smoothness and the variance both depend on the eigenvalues of the weight matrices. This would make the result more reliable.<BRK>I think this paper should be accepted. Nevertheless, I do agree that this paper has a clear motivation, and the observation is interesting and important. + Theoretical justification for the gradient variance between the narrowest and widest cells are sound. Trending of DARTS evaluation results does not agree with SNAS and AmoebaNet. Could the author(s) comment on this?<BRK>Summary:This paper tries to understand the characteristics of the architectures found by common NAS methods in the cell search space. Comments:  Overall the paper is interesting and well written. Liked the empirical analysis and theoretical insights backing it up. The generalization experiments suggest to me that on bigger datasets wider and shallower networks might be better for generalization actually.<BRK>Neural architecture search (NAS) searches architectures automatically for given tasks, e.g., image classification and language modeling. Improving the search efficiency and effectiveness has attracted increasing attention in recent years. Hotheyver, few efforts have been devoted to understanding the generated architectures. In this paper, they first reveal that existing NAS algorithms (e.g., DARTS, ENAS) tend to favor architectures with wide and shallow cell structures. These favorable architectures consistently achieve fast convergence and are consequently selected by NAS algorithms. their empirical and theoretical study further confirms that their fast convergence derives from their smooth loss landscape and accurate gradient information. Nonetheless, these architectures may not necessarily lead to better generalization performance compared with other candidate architectures in the same search space, and therefore further improvement is possible by revising existing NAS algorithms.
Reject. rating score: 3. rating score: 6. rating score: 8. I have some question about the extent to which this work is in scope for ICLR. But a lower bound for the convex case seems to be stretching this a little far.<BRK>Despite the tight lower bound, the assumption (1) and (2) above seems to be restrictive, but they are necessary for the analysis of this paper.<BRK>The paper fills an existing gap in the literature and it achieves two very interesting results:1  The lower bound now matches an existing upper bound for Point SAGA, showing that no better algorithm can exist (at least in a worst case sense). As pointed out by the authors, the requirements on the dimensionality in the theorems of this paper are milder than previous results.<BRK>This paper studies the lotheyr bound complexity for the optimization problem whose objective function is the average of $n$ individual smooth convex functions. they consider the algorithm which gets access to gradient and proximal oracle for each individual component.
For the strongly-convex case, they prove such an algorithm can not reach an $\eps$-suboptimal point in fetheyr than $\Omega((n+\sqrt{\kappa n})\log(1/\eps))$ iterations, where $\kappa$ is the condition number of the objective function. This lotheyr bound is tighter than previous results and perfectly matches the upper bound of the existing proximal incremental first-order oracle algorithm Point-SAGA.
they develop a novel construction to show the above result, which partitions the tridiagonal matrix of classical examples into $n$ groups to make the problem difficult enough to stochastic algorithms. 
This construction is friendly to the analysis of proximal oracle and also could  be used in general convex and average smooth cases naturally.
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. This paper makes a convincing case for improving the usefulness of production level, large scale models by making them quickly editable without extensive retraining. The paper demonstrates effectiveness for both image classification and machine translation tasks, covering a wide range of relevant scenarios. I recommend acceptance because:  The paper considers a real issue for production models which are becoming widespread, and retraining for targeted modifications is impractical. Experiments are consistently well designed and executed. The method proposed in [1] is very similar, even if it is used in the continual learning settings.<BRK>This paper proposes a way to effectively "patch" and edit a pre trained neural network s predictions on problematic data points (e.g.where mistakes on these data points can lead to serious repercussions), without necessarily re training the network on the entire training set plus the problematic samples. More concretely, the edit operation on the problematic samples are done using a few steps of stochastic gradient descent. To this end, the paper uses a loss term that incorporates these criteria, as weighted by interpolation coefficient hyper parameters. Experiments are done on CIFAR 10 toy experiments, large scale image classification with adversarial examples, and machine translation. Pros:1.The paper is well written, and clearly motivates the problem and why it is important.<BRK>The authors propose Editable Training that edits/updates a trained model using a model agnostic training technique. Editable training is able to correct mistakes of trained models without retraining the whole model nor harming the original performance. This paper has brought attention to mistake correction problem in neural networks and proposes a simple and concise solution. In addition, extensive experiments on both small and large scale image classification and machine translation tasks demonstrate the effectiveness of different editor functions. Overall, this paper is well written with extensive experimental results. Below are a few concerns I have to the current status of the paper.<BRK>These days deep neural networks are ubiquitously used in a wide range of tasks, from image classification and machine translation to face identification and self-driving cars. In many applications, a single model error can lead to devastating financial, reputational and even life-threatening consequences. Therefore, it is crucially important to correct model mistakes quickly as they appear. In this work, they investigate the problem of neural network editing - how one can efficiently patch a mistake of the model on a particular sample, without influencing the model behavior on other samples. Namely, they propose Editable Training, a model-agnostic training technique that enctheirages fast editing of the trained model. they empirically demonstrate the effectiveness of this method on large-scale image classification and machine translation tasks.
Reject. rating score: 3. rating score: 3. rating score: 6. Given the set up’s simplicity, a short theoretical argument (maybe even a theorem) about the quality and number of local minima one would expect to find could have been more concise and compelling than the empirical analysis from the paper. The authors cite a paper by Adrian Barbu as the inspiration for their pruning algorithm with annealing, and use it “to improve the capability of NNs to find a deep local minimum even when there are irrelevant variables”. I vote to “weak reject” this paper. I recommend that the authors cite and discuss https://arxiv.org/pdf/1805.01930.pdf , and possibly submit the paper at a less competitive conference. Following up on the previous point: it would be great the authors could include data sets where CPNA does not outperform.<BRK>Overall, I am a bit concerned with the significance of this paper. This paper presents a few sets of experiments related to deep local optima. The paper claims that it is quite easy to find a deep local minimum with good generalization when the number of irrelevant features is small, and it becomes harder to find a deep minimum with good generation as the number of irrelevant features increases. It will be more interesting to study on CNN, etc.<BRK>The reviewer represents the opinion that more focus on such setups would greatly benefit the community in terms of progressing the theoretical understanding. The claim made in the paper that there is a relationship between the number/suboptimality of local minima  and the scarcity of the data is both convincing and interesting.<BRK>Artificial neural networks (ANNs) are very popular nowadays and offer reliable solutions to many classification problems. Hotheyver, training deep neural networks (DNN) is time-consuming due to the large number of parameters. Recent research indicates that these  DNNs might be over-parameterized and different solutions have been proposed to reduce the complexity both in the number of parameters and in the training time of the neural networks. Furthermore, some researchers argue that after reducing the neural network complexity via connection pruning, the remaining theyights are irrelevant and retraining the sub-network would obtain a comparable accuracy with the original one. 
This may hold true in most vision problems where they always enjoy a large number of training samples and research indicates that most local optima of the convolutional neural networks may be equivalent. Hotheyver, in non-vision sparse datasets, especially with many irrelevant features where a standard neural network would overfit, this might not be the case and there might be many non-equivalent local optima. This paper presents empirical evidence for these statements and an empirical study of the learnability of neural networks (NNs) on some challenging non-linear real and simulated data with irrelevant variables. 
their simulation experiments indicate that the cross-entropy loss function on XOR-like data has many local optima, and the number of local optima grows exponentially with the number of irrelevant variables. 
they also introduce a connection pruning method to improve the capability of NNs to find a deep local minimum even when there are irrelevant variables. 
Furthermore, the performance of the discovered sparse sub-network degrades considerably either by retraining from scratch or the corresponding original initialization, due to the existence of many bad optima around. 
Finally, they will show that the performance of neural networks for real-world experiments on sparse datasets can be recovered or even improved by discovering a good sub-network architecture via connection pruning.
Reject. rating score: 1. rating score: 3. rating score: 8. This paper proposes the use of Federated Averaging for achieving personalised user embedding. The key contribution of this paper is not clear. It seems to be the introduction of the notion of split personalisation constraint, and it shows that the modeling each user with a “private” embedding that feeds to a global MLP with a global BLSTM as another input (named as FURL) can achieve the constraint so that FL can be used.<BRK>In this paper, the authors propose using federated learning (FL) to train personalized models, which improves the scalability and privacy preservation of the existing personalization techniques. The empirical results show good performance. However, in general, I think the contribution is limited. The reasons are as follows:1.<BRK>Reviewer has limited knowledge of previous work in the personalized FL field, thus is only able to confirm the novelty from Authors  related work section. Authors approached the problem with defining the constraint of split personalization, and argued that common FL setting such as Federated Averaging could satisfy this constraint. Authors also compared the conversage curve and visualized final embeddings to show that federated learning produces acceptable convergence and equally reasonable embeddings. The claimed benefits of such models are 1) preservation of user privacy by keeping the personalized parameters locally on each user s device, and 2) reduced data exchange to make the training complexity grow linearly with the number of users.<BRK>Collaborative personalization, such as through learned user representations (embeddings), can improve the prediction accuracy of neural-network-based models significantly. they propose Federated User Representation Learning (FURL), a simple, scalable, privacy-preserving and restheirce-efficient way to utilize existing neural personalization techniques in the Federated Learning (FL) setting. FURL divides model parameters into federated and private parameters. Private parameters, such as private user embeddings, are trained locally, but unlike federated parameters, they are not transferred to or averaged on the server. they show theoretically that this parameter split does not affect training for most model personalization approaches. Storing user embeddings locally not only preserves user privacy, but also improves memory locality of personalization compared to on-server training. they evaluate FURL on two datasets, demonstrating a significant improvement in model quality with 8% and 51% performance increases, and approximately the same level of performance as centralized training with only 0% and 4% reductions. Furthermore, they show that user embeddings learned in FL and the centralized setting have a very similar structure, indicating that FURL can learn collaboratively through the shared parameters while preserving user privacy.
Reject. rating score: 3. rating score: 3. rating score: 6. The method employs as a basic input the SMILES representation of molecules which are not well defined in terms of the representation, though. The experimental results on the tasks of the property prediction and molecular optimization using the benchmark datasets demonstrate the effectiveness of the proposed method in comparison with the others. Though this paper provides some contributions to the field of molecular graph representation, it contains flaws in presentation, lacking details of technical contents; thus, the paper is regarded as "borderline". * This paper lacks some important technical contents, making it hard to understand. And, in the first place of the paper, it would be better to explain what kind of and how many properties of the molecular are considered. It is unclear how to apply the proposed method to the semi supervised learning framework? The comparison experiments seem to be inconsistent. The proposed method is compared with different methods in different datasets/tasks. Toward fair comparison, it should be evaluated in comparison with some baselines including such as JT VAE consistently. There, however, are some approaches to canonicalize the SMILES representation itself such as by canonical SMILES.<BRK>The authors present a method All SMILES VAE that’s used for predicting chemical properties of small molecules and also for optimizing the structures of these molecules. The authors evaluate their model on the Zinc250K and Tox21 dataset and report that they are able to exceed the previous SOTA. While it’s interesting to be able to optimize molecules in the space of SMILE strings, the impact is less clear. While this may lead to directed searches, it will prevent truly novel molecules from being synthesized. The authors will need to address and provide experiments with unconstrained search. It seems from the text of figure 5, that the SSVAE and GraphConv results have been taken directly from the paper. Some clarification questions:   For optimized/predicted strings, which are presumably novel molecules not in the dataset, how is the true chemical property  e.g.logP, determined? Overall, I think that this paper has some interesting ideas and is well written. However, the novelty and impact of the model is somewhat lacking. If this were introducing a new application area to this field, then I think the case for acceptance could have been stronger, however there has already been a lot of work related to molecular property prediction/design. So, I went with 3, but please consider this to be a 5.<BRK>Instead of using graph neural networks, the authors hava an approach based on SMILES which encode molecules as strings. To avoid the problem that any given molecule may be represented by multiple SMILES strings, the authors consider an encoder that makes use of several random SMILES representations of the input molecule. These are preprocessed using recurrent neural networks generating an average representation by pooling the representations generated with each SMILES sequence for each atom in the input molecule. The model decodes then into a disjoint set of SMILES strings different from those used at theinput. This enforces the model to learn a bijective mapping between molecules and latent representations. The proposed model can also do semi supervised and supervised prediction tasks. Clarity:The paper is very clearly written and it is very easy to read. It contains a very detailed description of previous work. Significance:The experiments show that the proposed method can outperform previous ones. However, I miss additional evaluations using existing frameworks such as  Guacamol.<BRK>Variational autoencoders (VAEs) defined over SMILES string and graph-based representations of molecules promise to improve the optimization of molecular properties, thereby revolutionizing the pharmaceuticals and materials industries. Hotheyver, these VAEs are hindered by the non-unique nature of SMILES strings and the computational cost of graph convolutions. To efficiently pass messages along all paths through the molecular graph, they encode multiple SMILES strings of a single molecule using a set of stacked recurrent neural networks, harmonizing hidden representations of each atom bettheyen SMILES representations, and use attentional pooling to build a final fixed-length latent representation. By then decoding to a disjoint set of SMILES strings of the molecule, their All SMILES VAE learns an almost bijective mapping bettheyen molecules and latent representations near the high-probability-mass subspace of the prior. their SMILES-derived but molecule-based latent representations significantly surpass the state-of-the-art in a variety of fully- and semi-supervised property regression and molecular property optimization tasks.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 3. The paper is well written, presents an elegant idea in a clear and straight forward manner, and is solidly built on top of the current literature on self supervised learning for image processing, which is also very well summarized. At this point, the number of classification heads can be treated as a hyperparameter of the algorithm. This constraint acts as a "regularizer" that allows the authors to minimize the cross entropy loss between inputs and pseudo labels while avoiding the degenerate trivial solution where all samples are assigned to the same pseudo label. That is done by remarking that minimizing the loss function over the pseudo label assignments (under the equal partition constraint) can be formulated as an optimal transport problem that can be solved efficiently with a fast version of the Sinkhorn Knopp algorithm.<BRK>Summary & Pros  This paper proposes a representation learning method based on clustering. The proposed method performs clustering and representation learning alternatively and simultaneously. For clustering, the objective can be formulated as an optimal transport problem and it can be efficiently solved. If the number of each class is imbalanced, the equipartition constraint might degrade the quality of the label assignment. The SOTA method can be considered as a combination of (instance wise) clustering and self supervision. I think this direction against self supervised learning is important because it requires relatively smaller domain knowledge. So I think it would be better if more analysis about the convergence is given in a rebuttal.<BRK>This paper develops a novel self supervised learning method by combing clustering and representation learning together. Under the weak assumption that the number of samples should be similar across different clusters, the authors further develop a modified Sinkhorn Knopp algorithm to solve the problem. In general, the whole paper is well written and the developed solution is interesting. However, I have the following comments:1. The authors are suggested to provide more explanations about the differences. 2.The used assumption that samples are uniformed distributed across different cluster are too strong in practice. 5.How about the time complexity of the developed algorithm?<BRK>Combining clustering and representation learning is one of the most promising approaches for unsupervised learning of deep neural networks. Hotheyver, doing so naively leads to ill posed learning problems with degenerate solutions.
In this paper, they propose a novel and principled learning formulation that addresses these issues.
The method is obtained by maximizing the information bettheyen labels and input data indices.
they show that this criterion extends standard cross-entropy minimization to an optimal transport problem, which they solve efficiently for millions of input images and thousands of labels using a fast variant of the Sinkhorn-Knopp algorithm.
The resulting method is able to self-label visual data so as to train highly competitive image representations without manual labels. their method achieves state of the art representation learning performance for AlexNet and ResNet-50 on SVHN, CIFAR-10, CIFAR-100 and ImageNet and yields the first self-supervised AlexNet that outperforms the supervised Pascal VOC detection baseline. 
Reject. rating score: 1. rating score: 1. rating score: 6. Summary: The paper uses a Gaussian Processes framework previously introduced in [1] to identify the most important samples from the past for functional regularization. (for example for P MNIST the gain is 0.6%+ 0.1) where there is a lack of complete discussion on how the two methods are different. Authors indicate on page 3 “Our goal in this paper is to design methods that can avoid such catastrophic forgetting.” and reiterate on this on other parts of the paper yet there is no forgetting evaluation to support this claim. (b) On page 1, paragraph 3, they mention some prior work such as GEM and iCaRL “do not take uncertainty of the output into account”. This is what we mean by scalable.<BRK>The paper proposed a new functional regularization method with gaussian process which has similar direction with recent two works (khan et al, titsias et al). They select most memorable samples depends on eigenvalue. The model FROMP outperforms baselines and their ablations. However, the experiments are only performed on shallow networks, it is required to apply on much deeper networks, such as ResNet. Also, in the experiment results, I feel the performance of the FROMP largely depends on the number of the coreset, while  important  selection just shows marginal effects even on split CIFAR. I have several wonderings on the paper.<BRK>StrengthsTo some extent, I think the proposed method is novel, although there is a similar work named as Functional Regularisation for Continual Learning (FRCL). FROMP first uses NTK in Gaussian process for continual learning and proposes a new strategy of selecting memory samples. The strategy of selecting samples to be stored is simple and effective. The experimental section needs more detailed analysis. Other comments In this paper, for Split MNIST experiment with multi head, it shows that the method of EWC achieves worse results than SI. I expect authors could explain this point.<BRK>Continually learning new skills without forgetting old ones is an important quality for an intelligent system, yet most deep learning methods suffer from catastrophic forgetting of the past. Recent works have addressed this by regularising the network theyights, but it is challenging to identify theyights crucial to avoid forgetting. A better approach is to directly regularise the network outputs at past inputs, e.g., by using Gaussian processes (GPs), but this is usually computationally challenging. In this paper, they propose a scalable functional-regularisation approach where they regularise only over a few memorable past examples that are crucial to avoid forgetting. their key idea is to use a GP formulation of deep networks, enabling us to both identify the memorable past and regularise over them. their method achieves state-of-the-art performance on standard benchmarks and opens a new direction for life-long learning where regularisation methods are naturally combined with memory-based methods.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. This paper considers a the differential privacy problem regarding the parameter tranfer algorihtm in meta learning, such as MAML and Repile. To me, the setting is very interesting and according to the paper, it seems that it is the first formalization for this setting. All the datasets the paper uses are very easy ones. The experiments demonstrate the effectiveness of the proposed differentially private parameter transfer.<BRK>This paper proposes the notions of different privacy levels for different attack models, namely global and local meta level and within task level privacy for meta learning. It proposes an algorithm for global within task privacy. It provides privacy and utility guarantee of the proposed algorithm and experimental evaluations. The proposed definitions make sense to me for the scenarios mentioned in the paper. The utility guarantee also seems interesting. I think the experimental evaluation can be made more complete, for example, you may consider:  a convex setting as was considered in the utility guarantee,  varying epsilon values.<BRK>This paper considers the problem of achieving formal privacy guarantees in the context of parameter sharing meta learning. The problem is well motivated: since in meta learning we want to leverage information from similar tasks to increate data efficiency, there may be privacy concerns for each of the task owners about both other task owners and the aggregator (meta learner). In combination with post processing and composition guarantees this gives privacy for the overall mechanism. They are able to show theoretical guarantees via the standard accuracy guarantees of private SGD and no regretguarantees of OCO.<BRK>Parameter-transfer is a theyll-known and versatile approach for meta-learning, with applications including few-shot learning, federated learning, with personalization, and reinforcement learning. Hotheyver, parameter-transfer algorithms often require sharing models that have been trained on the samples from specific tasks, thus leaving the task-owners susceptible to breaches of privacy. they conduct the first formal study of privacy in this setting and formalize the notion of task-global differential privacy as a practical relaxation of more commonly studied threat models. they then propose a new differentially private algorithm for gradient-based parameter transfer that not only satisfies this privacy requirement but also retains provable transfer learning guarantees in convex settings. Empirically, they apply their analysis to the problems of federated learning with personalization and few-shot classification, showing that allowing the relaxation to task-global privacy from the more commonly studied notion of local privacy leads to dramatically increased performance in recurrent neural language modeling and image classification.
Reject. rating score: 3. rating score: 6. rating score: 6. Summary : The paper proposes an exploratory objective that can maximize state coverage in RL. They show that a formal objective for maximizing state coverage is equivalent to maximizing the entropy of a goal distribution. The core idea is to propose a method to maximize entropy of a goal distribution, or a state distribution since goals are full states. Overall, I am not convinced that an objective based on H(s|g) is equivalent to an maximizing H(s), and why is this even a good objective for exploration? In light of this, I am not sure whether the core idea of the paper is convincing enough to me. Overall, I would recommend to reject this paper, as I am not convinced by the proposed solution, and there are lot of theoretical details missing from the paper.<BRK>The paper introduces SKEW FIT, an exploration approach that maximizes the entropy of a distribution of goals such that the agent maximizes state coverage. ", it is empirically illustrated that the method does result in a high entropy state exploration. I m unsure about the interpretation of this sentence given Figure 6 because other methods do not seem to fail entirely when given enough time. The experiments are interesting, yet some interpretations might be too strong (see below):  In the first experiment, "Does Skew Fit Maximize Entropy?<BRK>This paper introduced a very interesting idea to facilitate exploration in goal conditioned reinforcement learning. The key idea is to learn a generative model of goal distribution to match the weighted empirical distribution, where the rare states receive larger weights. The experiments offer a comparison to several prior exploration techniques and demonstrate a clear advantage of the proposed Skew Fit method. A formal analysis of the algorithm is provided under certain assumptions. Cons:The weakest part of this work is the task setup. It is especially when the agent has no access to task reward and only explores the environment to maximize state coverage. I would also like to see how Skew Fit works with different goal conditioned RL algorithms, and how the performances of the RL policy in reaching the goals would affect the effectiveness of this method in exploring a larger set of states.<BRK>Autonomous agents that must exhibit flexible and broad capabilities will need to be equipped with large repertoires of skills. Defining each skill with a manually-designed reward function limits this repertoire and imposes a manual engineering burden. Self-supervised agents that set their own goals can automate this process, but designing appropriate goal setting objectives can be difficult, and often involves heuristic design decisions. In this paper, they propose a formal exploration objective for goal-reaching policies that maximizes state coverage. they show that this objective is equivalent to maximizing the entropy of the goal distribution together with goal reaching performance, where goals correspond to full state observations. To instantiate this principle, they present an algorithm called Skew-Fit for learning a maximum-entropy goal distributions. Skew-Fit enables self-supervised agents to autonomously choose and practice reaching diverse goals. they show that, under certain regularity conditions, their method converges to a uniform distribution over the set of valid states, even when they do not know this set beforehand. their experiments show that it can learn a variety of manipulation tasks from images, including opening a door with a real robot, entirely from scratch and without any manually-designed reward function.
Reject. rating score: 3. rating score: 3. rating score: 6. Paper contributions   This paper proposes a method for constructing representations using a matrix of Wasserstein distances. These distances measure the discrepancy between each class and each environment, that is a random combination of some classes. General notes The general idea of measuring the distribution divergence for a set of classes is interesting and seems to be novel. I don t follow why the paper proposes to use  environments     random combinations of classes. The experimentation is very weak and does very little to support the claims. The paper doesn t provide any comparison to existing methods or simple baselines. There is no experiment demonstrating interpretability of the proposed approach. Some of the claims are vague and excessively broad:  The proposed technique can be used with any task, but the paper is clearly limited to the retrieval task  The environments are too vaguely described and can be misinterpreted in the introductionConclusion I recommend to reject on the basis that   the approach is more limited than the paper advocates  the experimentation is weak  some claims are not addressedOther notes I recommend using term divergence instead of distance when it is not symmetrical.<BRK>The paper defines a representation learning strategy based upon estimationof a matrix of Wasserstein distances. The idea is excellent. Phrases like "belonging to any random subset of the dataset" suggesta non deterministic method of selecting an element of the power set ofthe training data, but it is unclear what to do if more training dataarrives in this case. In the experiments section phrases like "environments consist of random combinations of classes" is also not helpful. Do you mean something like "uniformly selected from the set of all class pairs?" I want to accept this paper if the exposition is improved, which I thinkis possible during the response period. My other comments are not blocking issues, but would either improve thecurrent paper or inform future directions of research. [1]  That paper seeks a projection that maximizes the ratio of Wasserstein distance between classes vs. within classes. Ifthe matrix is full rank with a flat spectrum, however, that might indicatethe choice of environments is too granular and overfitting has occurred,it s not immediately obvious to me how to guard against this.<BRK>The authors proposed a template based interpretable representation that works based on the earth mover s distance of each class to a number of "environments", which could be taken as union of a few random classes. The method is evaluated based on classification and retrieval tasks. Here are my concerns:  Since the environments are taken randomly in the experiments, it is not investigated how sensitive the method is with respect to the choices of environments.<BRK>they introduce a new deep learning technique that builds individual and class representations based on distance estimates to randomly generated contextual dimensions for different modalities. Recent works have demonstrated advantages to creating representations from probability distributions over their contexts rather than single points in a low-dimensional Euclidean vector space. These methods, hotheyver, rely on pre-existing features and are limited to textual information. In this work, they obtain generic template representations that are vectors containing the average distance of a class to randomly generated contextual information. These representations have the benefit of being both interpretable and composable. They are initially learned by estimating the Wasserstein distance for different data subsets with deep neural networks. Individual samples or instances can then be compared to the generic class representations, which they call templates, to determine their similarity and thus class membership. they show that this technique, which they call WDVec, delivers good results for multi-label image classification. Additionally, they illustrate the benefit of templates and their composability by performing retrieval with complex queries where they modify the information content in the representations. their method can be used in conjunction with any existing neural network and create theoretically infinitely large feature maps.
Reject. rating score: 1. rating score: 3. rating score: 3. This paper proposes an attack method to improve the transferability of targeted adversarial examples. There are a lot of typos in the paper.<BRK>This paper proposes distillation attacks to generate transferable targeted adversarial examples. I think their proposed attack is interesting due to its simplicity and effectiveness. 4.In general this paper lacks empirical analysis on why distillation helps improve the transferability. I think this paper still misses a more in depth analysis, and thus I keep my original assessment.<BRK>The paper suggests to use temperature scaling in adversarial attack design for improving transferability under black box attack setting. Based on this, the paper proposes several new attacks: D FGSM, D MIFGSM, and their ensemble versions. Experimental results found that the proposed methods improves transferability from VGG networks, compared to the non distillated counterparts. Firstly, the presentation of the method is not that clear to me.<BRK>Neural networks show great vulnerability under the threat of adversarial examples.
   By adding small perturbation to a clean image, neural networks with high classification accuracy can be completely fooled.
   One intriguing property of the adversarial examples is transferability.  This property allows adversarial examples to transfer to networks of unknown structure, which is harmful even to the physical world.
   The current way of generating adversarial examples is mainly divided into optimization based and gradient based methods.
   Liu et al. (2017) conjecture that gradient based methods can hardly produce transferable targeted adversarial examples in black-box-attack.
   Hotheyver, in this paper, they use a simple technique to improve the transferability and success rate of targeted attacks with gradient based methods.
   they prove that gradient based methods can also generate transferable adversarial examples in targeted attacks.
   Specifically, they use knowledge distillation for gradient based methods, and show that the transferability can be improved by effectively utilizing different classes of information.
   Unlike the usual applications of knowledge distillation, they did not train a student network to generate adversarial examples.
   they take advantage of the fact that knowledge distillation can soften the target and obtain higher information, and combine the soft target and hard target of the same network as the loss function.
   their method is generally applicable to most gradient based attack methods.
Reject. rating score: 1. rating score: 3. rating score: 3. rating score: 6. The paper studies stochastic optimization with consistent (may not be unbiased) estimators. This problem is well motivated through the example of learning graph representations where consistent estimators are easier to obtain than unbiased one. Under the assumption that the estimate converges to the consistent gradient exponentially fast w.r.t.the sample size, the authors give convergence rates for convex, strongly convex and non convex optimization. The main convergence theorems seem to follow from standard techniques for inexact/noisy gradients. Convergence Analysis   As mentioned above, since the setting is like GD with noisy gradients, a more careful analysis in the strongly convex setting can improve the convergence result. Why is this needed?<BRK>The analysis of the convergence of SGD with biases gradient estimates dates back to Robins&Monroe, but the authors of this paper focused on a recent original algorithm that shows that once can estimate the approximate gradient of a large GNN network, simply by sampling nodes randomly. Is there something such as the computational cost of Adam, that I m missing, especially when looking at the graphs? The main contribution of the paper is the proof that the algorithm converge, but there is no theoretical analysis of the key quantity "t", which is the number of sampled nodes in the neighbours of the output nodes. Overall, while proving that the FastGCN algorithm is consistent is important, it is hard to understand how useful the results are and how they can be useful in practice.<BRK>This paper aims to solve the stochastic optimization problems in machine learning where the unbiased gradient estimator is expensive to compute, and instead use a consistent gradient estimator. The main contributions are the convergence analyses of the consistent gradient estimator for different objectives (i.e., convex, strongly convex, and non convex). Overall, it is interesting and important, but I still have some concerns. Is that right? Besides, for Figure 1, can the "SGD unbiased" be viewed as "SGD consistent (sampl $n$)"?<BRK>The authors then establish that SGD algorithm when run with consistent gradient estimators (but not necessarily unbiased) have similar convergence properties as SGD algorithms when run with unbiased gradient estimators. Consistent gradient estimators have been proposed for such graph problems in the past but this paper establishes theoretical properties of SGD with such estimators. The paper is well written and the results are convincing. I have a few questions/comments1. It is clear that using unbiased SGD, unbiased ADAM is better of than using biased SGD.<BRK>Stochastic gradient descent (SGD), which dates back to the 1950s, is one of the most popular and effective approaches for performing stochastic optimization. Research on SGD resurged recently in machine learning for optimizing convex loss functions and training nonconvex deep neural networks. The theory assumes that one can easily compute an unbiased gradient estimator, which is usually the case due to the sample average nature of empirical risk minimization. There exist, hotheyver, many scenarios (e.g., graphs) where an unbiased estimator may be as expensive to compute as the full gradient because training examples are interconnected. Recently, Chen et al. (2018) proposed using a consistent gradient estimator as an economic alternative. Enctheiraged by empirical success, they show, in a general setting, that consistent estimators result in the same convergence behavior as do unbiased ones. their analysis covers strongly convex, convex, and nonconvex objectives. they verify the results with illustrative experiments on synthetic and real-world data. This work opens several new research directions, including the development of more efficient SGD updates with consistent estimators and the design of efficient training algorithms for large-scale graphs.

Reject. rating score: 3. rating score: 3. rating score: 6. It is not clear what are the previous contributions prior to this paper, and it seems [1] shares some similar results/observation with the this paper.<BRK>It is not very clear due to the vague definitions. Because of these concerns, I think this paper is on the borderline.<BRK>Overall I quite like the analysis of this paper. I think it could be clearer and contain more experiments but it is otherwise rather convincing proof that DNNs learn low frequency patterns first. The paper would benefit from always making this clear in the text and figure captions.<BRK>they study the training process of Deep Neural Networks (DNNs) from the Ftheirier analysis perspective. they demonstrate a very universal Frequency Principle (F-Principle) --- DNNs often fit target functions from low to high frequencies --- on high-dimensional benchmark datasets, such as MNIST/CIFAR10, and deep networks, such as VGG16. This F-Principle of DNNs is opposite to the learning behavior of most conventional iterative numerical schemes (e.g., Jacobi method), which exhibits faster convergence for higher frequencies, for various scientific computing problems. With a naive theory, they illustrate that this F-Principle results from the regularity of the commonly used activation functions. The F-Principle implies an implicit bias that DNNs tend to fit training data by a low-frequency function. This understanding provides an explanation of good generalization of DNNs on most real datasets and bad generalization of DNNs on parity function or randomized dataset.
Reject. rating score: 1. rating score: 1. rating score: 6. ** Summary **In this paper, the authors propose a new variant of Transformer called Tied multi Transformer. Given such a model with an N layer encoder and an M layer decoder, it is trained with M*N loss functions, where each combination of the nth layer of the encoder and the mth layer of the decoder is used to train an NMT model. The authors propose a way to dynamically select which layers to be used when a specific sentence comes. When compared Tied(6,6) to standard Transformer, as shown in Table 1, there is no improvement. c.	In terms of training speed, compared to standard Transformer, the proposed method takes 9.5 time of the standard Transformer (see section 3.4, training time). Therefore, I think that compared to standard Transformer, there is not a significant difference. 2.The authors only work on a single dataset, which is not convincing.<BRK>This work proposes a way to reduce the latencies incurred in inference for neural machine translation. The authors claim that the use of knowledge distillation is novel. I have several concerns to this work and I d recommend rejecting this submission. One of the problems of this paper is presentation.<BRK>This paper proposes a novel procedure for training multiple Transformers with tied parameters which compresses multiple models into one enabling the dynamic choice of the number of encoder and decoder layers during decoding. The idea is simple and reasonable and the results are promising.<BRK>This paper proposes a novel procedure for training multiple Transformers with tied parameters which compresses multiple models into one enabling the dynamic choice of the number of encoder and decoder layers during decoding. In sequence-to-sequence modeling, typically, the output of the last layer of the N-layer encoder is fed to the M-layer decoder, and the output of the last decoder layer is used to compute loss. Instead, their method computes a single loss consisting of NxM losses, where each loss is computed from the output of one of the M decoder layers connected to one of the N encoder layers. A single model trained by their method subsumes multiple models with different number of encoder and decoder layers, and can be used for decoding with fetheyr than the maximum number of encoder and decoder layers. they then propose a mechanism to choose a priori the number of encoder and decoder layers for faster decoding, and also explore recurrent stacking of layers and knowledge distillation to enable further parameter reduction. In a case study of neural machine translation, they present a cost-benefit analysis of the proposed approaches and empirically show that they greatly reduce decoding costs while preserving translation quality.
Reject. rating score: 3. rating score: 3. rating score: 6. rating score: 6. This paper presents a model with beta bernoulli dropout of neurons for network pruning. The model assumes the probability to dropout a neuron is a function pi(phi, x), which depends both the beta variable phi and the input x. The model is trained with stochastic gradient variational Bayes with continous relaxation. The model and algorithm sound, and the intuition of determining the dropout probability based on the importance of each dimension makes sense. One weakness of this work is the lack of large scale experiments, for example, pruning a MobileNet on ImageNet. Figure 1: bottom right figure (input regions pruned by DBB) is missing? Update:The authors do address my concern #2. After reading other reviews and reading the revised paper I do think this approach of this paper is novel and can potentially lead to a gain. However I still don t think the experiments are convincing enough. It needs to be tested on a larger variety of models (ResNet or MobileNet) / datasets (ImageNet, etc.) to prove its significance.<BRK>The paper proposes Variational Beta Bernoulli Dropout to sparsify the parameters of the network. The paper presents necessary theoretical details as well as the experimental comparison with the other dropout methods on MNIST and CIFAR 10/100 datasets. The paper is well written overall and easy to follow. The paper provides a thorough background on previous work; however the motivation for having an input dependent dropout method is relatively weak. The paper explains well how the two proposed dropouts can be learned during training, but it s not clear about the inference, specially in variational dependent Beta Bernoulli dropout. This is not clear to me what it means, and also the related footnote which says different thresholds were tried but the difference is insignificant! Also, it is not clear if xFLOP and Memory report the best, mean or median of 3 runs. In Table 1, what does column Neurons represent? Is it the model parameters at inference time? In the experimental results, some models are missing that are mentioned in the paper as previous work which seem to be good to use for comparison, such as Sparse Variational Dropout (Molchanov 2017) and Structured Sparsity Learning (Wen et al 2016). I encourage the authors to expand more on analyzing the experimental results! In Table 1 and Table 2, there are 2 lines for BB and DBB, which is helpful to explain what each line represents. Looking at the results of all tables, and comparing different dropouts w.r.t :1) Error, 2) xFLOPs and 3) Memory, I cannot draw a clear conclusion from the results. For example, in Table 2, CIFAR 10, comparing first line of BB to SBP and second line to VIB, the difference is not significant. More analysis about the results can be helpful!<BRK>The paper proposes a new way of training variational dropout which is adaptive to input samples due to the proposed sparsity inducing beta Bernoulli prior. The authors provide a good motivation for their model, introduce beta Bernoulli and dependant beta Bernoulli prior and propose the method in the variational inference framework. From the Tables 1, 2 we can see that BB is comparable with DBB and the latter is not uniformly better than the former in terms of error, xFLOPs and memory. This similarity in the performance is more significant for LeNet5 Caffe network, for CIFAR 10 and CIFAR 100 datasets. Is the overhead of DBB worth the benefits it gives? It means that DBB should keep all weights after the first stage, in other words the memory consumption of DBB is the same as BB. Overall, the paper proposes interesting and well motivated method for training sparse networks. Although, there are concerns about the DBB extension I would recommend considering this paper for acceptance. Indeed, the run time memory mostly consists of activation maps, therefore DBB can benefit from input dependent sparsity. Considering the first concern I still think that the advantage of DBB is not clear and I agree with AnonReviewer3 who said that except LeNet 500 300 other results are mixed. However, overall I do think that the proposed method is novel, well theoretically grounded and is proved that it works comparably if not better than the state of the art approaches.<BRK>The paper proposes a new method for learning to make neural networks sparse by adopting a beta Bernoulli prior with variational dropout. One motivation for the method is to make the dropout rate dependent on the input, by introducing the input to a layer as a factor in the computation of the Bernoulli parameter for the layer which controls whether some parts of it will get turned off. The motivation for the goal of making neural networks sparse is clear, since it can potentially lead to significant memory and computation savings (although given that hardware architectures typically used for executing neural networks generally expect dense computations, it may be difficult to realize these savings in practice). Furthermore, it s satisfying to see that the underlying method has a strong probabilistic justification. However, while a significant amount of the paper was devoted to the input dependent version, it was unclear from the empirical results whether there is much actual advantage to the additional complexity. Empirical validation of the method with experiments on larger datasets such as ImageNet would lend further credence to the viability of the approach. I think an interesting experiment would be to see how the method performs when computing the expectation on equation 13 through empirical samples; given the savings in FLOPs and memory, we could run the network with several samples even without exceeding the original computation budget. Some other design choices (such as the factorization of q, and equation 17) seem somewhat arbitrary, so I would also prefer to see further justification or a sketch/empirical evidence of why alternative methods may not work as well. For the above reasons, I am rating the paper as weak accept. Small note: please use \citep and \citet (instead of \cite, when using natbib) properly throughout the paper so that citations are formatted correctly.<BRK>While variational dropout approaches have been shown to be effective for network sparsification, they are still suboptimal in the sense that they set the dropout rate for each neuron without consideration of the input data. With such input independent dropout, each neuron is evolved to be generic across inputs, which makes it difficult to sparsify networks without accuracy loss. To overcome this limitation, they propose adaptive variational dropout whose probabilities are drawn from sparsity inducing beta-Bernoulli prior. It allows each neuron to be evolved either to be generic or specific for certain inputs, or dropped altogether. Such input-adaptive sparsity- inducing dropout allows the resulting network to tolerate larger degree of sparsity without losing its expressive potheyr by removing redundancies among features. they validate their dependent variational beta-Bernoulli dropout on multiple public datasets, on which it obtains significantly more compact networks than baseline methods, with consistent accuracy improvements over the base networks.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. In this paper, the authors study the convergence of (stochastic) gradient descent in training deep linear residual networks, where linear transformation at input and output layers are fixed and matrices in other layers are trained.<BRK>This paper studies the convergence properties of GD and SGD on deep linear resnets.<BRK>*Summary* This paper deals with the global convergence of deep linear ResNets.<BRK>they study the convergence of gradient descent (GD) and stochastic gradient descent (SGD) for training $L$-hidden-layer linear residual networks (ResNets). they prove that for training deep residual networks with certain linear transformations at input and output layers, which are fixed throughout training, both GD and SGD with zero initialization on all hidden theyights can converge to the global minimum of the training loss. Moreover, when specializing to appropriate Gaussian random linear transformations, GD and SGD provably optimize wide enough deep linear ResNets. Compared with the global convergence result of GD for training standard deep linear networks \citep{du2019width}, their condition on the neural network width is sharper by a factor of $O(\kappa L)$, where $\kappa$ denotes the condition number of the covariance matrix of the training data. they further propose a modified identity input and output transformations, and show that a $(d+k)$-wide neural network is sufficient to guarantee the global convergence of GD/SGD, where $d,k$ are the input and output dimensions respectively.
Accept (Poster). rating score: 8. rating score: 8. rating score: 6. This paper gives a nice interpretation why recent works that are based on variational lower bounds of mutual information can demonstrate promising empirical results, where they argue that the success depends on "the inductive biasin both the choice of feature extractor architectures and the parametrization of theemployed MI estimators." Moreover they show some connection to metric learning.<BRK>It would be interesting to see which factor contributes more to the performance: I_NCE being a triplet loss or an inductive bias in the design choice? The paper addresses a question on whether mutual information (MI) based models for representation learning succeed primarily thanks to the MI maximization. The paper conducts a series of experiments that constitute a convincing evidence for a weak connection between the InfoMax principle and these practical successes by showing that maximizing established lower bounds on MI are not predictive of the downstream performance and that contrary to the theory higher capacity instantiations of the critics of MI may result in worse downstream performance of learned representations.<BRK>In this paper, the authors studied the usage of the mutual information maximization principle in representation learning. They argue that the bias in the estimation of lower bound of MI may loosen the connection between InfoMax principle and representation learning. Specifically, they figure out the following phenomenon by experiments. 1.Large MI is not predictive of downstream performance. 3.Encoder architecture can be more important than the specific estimator.<BRK>Many recent methods for unsupervised or self-supervised representation learning train feature extractors by maximizing an estimate of the mutual information (MI) bettheyen different views of the data. This comes with several immediate problems: For example, MI is notoriously hard to estimate, and using it as an objective for representation learning may lead to highly entangled representations due to its invariance under arbitrary invertible transformations. Nevertheless, these methods have been repeatedly shown to excel in practice. In this paper they argue, and provide empirical evidence, that the success of these methods cannot be attributed to the properties of MI alone, and that they strongly depend on the inductive bias in both the choice of feature extractor architectures and the parametrization of the employed MI estimators. Finally, they establish a connection to deep metric learning and argue that this interpretation may be a plausible explanation for the success of the recently introduced methods.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. rating score: 6. The proposed memory layer models the memory as a multi head array of keys with a soft clustering mechanism and applies a convolution operator over the heads. Generally, the paper is technically justified. A novel clustering convolution mechanism is proposed  for memory augmentation and graph pooling. However, there are still some rebuttal requests.<BRK>In order to bolster their results the authors may run their approach on a few other datasets in Wu et. al.2018.Minor issues:   Provide error bars for the tables   Sec 4.2 typo : “datastes” Overall this paper is well written and easy to read.<BRK>In light of this clarification, I think the proposed algorithm is novel enough and the jointly training mechanism is also beneficial for the state of the arts results reported in the experiments. Or are they learned from scratch? Based on the authors  reply and other reviews, I have changed my rating to "Weak accept".<BRK>The paper presents "memory layer" to simultaneously do graph representation learning and pooling in a hierarchical way. At this point, the ideas for graph representation are plentiful, but there have not been a coherent story on how and why new architectures should work better than previous ones. * Clear visualization of the results.<BRK>Graph neural networks (GNNs) are a class of deep models that operate on data with arbitrary topology represented as graphs. they introduce an efficient memory layer for GNNs that can jointly learn node representations and coarsen the graph. they also introduce two new networks based on this layer: memory-based GNN (MemGNN) and graph memory network (GMN) that can learn hierarchical graph representations. The experimental results shows that the proposed models achieve state-of-the-art results in eight out of nine graph classification and regression benchmarks. they also show that the learned representations could correspond to chemical features in the molecule data.

Accept (Spotlight). rating score: 8. rating score: 8. rating score: 6. This paper presents an unbiased estimator of marginal log likelihood given a latent variable model. The method extends the importance weighted log marginal using the Russian roulette estimator. The marginal log probability estimator is motivated for entropy maximized density estimation and use of REINFORCE (log derivative) gradient for learning a policy with a latent variable. The paper is well organized and provides a contribution for optimizing latent variable models in certain scenarios. Perhaps, if its expectation with respect to q(z;x) is applied, this can be shown from equation (6). 2) How was parameter m set for the experiments?<BRK>This paper proposes an unbiased estimator of $\log p_\theta(x)$. Many unbiased estimators of $p_\theta$ exist, but $\log p_\theta$ is needed in many other settings, some of which are not well served by standard estimators of $p_\theta$. The SUMO estimator is essentially a Russian roulette based extension of IWAE; it is exactly unbiased, but takes a random and unbounded number of samples. This allows marginally better optimization of certain models than IWAE with a much smaller average number of samples, and (more importantly) opens new possibilities such as entropy maximization which are not well served by lower bounds like IWAE. (Your comments about occasional "bounded but very large" gradient estimates are troubling in this respect, depending on what exactly you mean by "bounded".) Given that it is also extremely on topic for ICLR and novel, I m rating the paper as "accept." In fact, SGD can be shown to work with biased gradient estimators, with suboptimality in the results depending on the bias; see e.g.Chen and Luss, http://arxiv.org/abs/1807.11880 .<BRK>The authors consider the unbiased estimation of log marginal likelihood (evidence) after integration of latent variable. On top of the importance weighted autoencoder (IWAE), which is only guaranteed to be asymptotically unbiased, the authors propose to use Russian Roulette estimator (RRE) to compensate the bias caused by the finite summation. The proposed method is interesting and can be applied in many other estimators with similar properties as Eq.(6).Bias compensation using RRE is interesting, but it seems there must be many literatures that took advantage of using RRE to improve estimators. The authors have to be thorough in presenting previous research and explaining the authors’ contribution that is distinguished from those. In general, this paper is well written and dealing with important problem with interesting method. Several analysis for understanding the advantages of using the proposed method is insufficient.<BRK>Standard variational lotheyr bounds used to train latent variable models produce biased estimates of most quantities of interest. they introduce an unbiased estimator of the log marginal likelihood and its gradients for latent variable models based on randomized truncation of infinite series. If parameterized by an encoder-decoder architecture, the parameters of the encoder can be optimized to minimize its variance of this estimator. they show that models trained using their estimator give better test-set likelihoods than a standard importance-sampling based approach for the same average computational cost. This estimator also allows use of latent variable models for tasks where unbiased estimators, rather than marginal likelihood lotheyr bounds, are preferred, such as minimizing reverse KL divergences and estimating score functions.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. The paper is well written (apart from the miniscule Figure 3 containing the main result). I recommend acceptance, with caveats: the energy performance is actually not directly calculated, but speculatively estimated, it depends on the computational architecture chosen to implement the respective networks.<BRK>The authors claim, and to a good extent show that, their proposed method is best of both worlds: train the models efficiently with standard encoding / SGD, this is something we know works and scale well, then convert and fine tune with spike backprop to get models that perform well under a shorter integration window, and thus are more efficient at inference time. The paper is well written, clear and easy to understand.<BRK>The first approach of previous work is converting the weights of a trained artificial neural network (ANN) with a given architecture, to the weights and thresholds of a SNN, and the second approach uses a surrogate gradient to train an SNN with backpropagation. 3.Typos: sec7 4th line “neruons”, sec 2.2 “both the credit” (remove “the”) Following the author response I have upgraded my rating.<BRK>Spiking Neural Networks (SNNs) operate with asynchronous discrete events (or spikes) which can potentially lead to higher energy-efficiency in neuromorphic hardware implementations. Many works have shown that an SNN for inference can be formed by copying the theyights from a trained Artificial Neural Network (ANN) and setting the firing threshold for each layer as the maximum input received in that layer. These type of converted SNNs require a large number of time steps to achieve competitive accuracy which diminishes the energy savings. The number of time steps can be reduced by training SNNs with spike-based backpropagation from scratch, but that is computationally expensive and slow. To address these challenges, they present a computationally-efficient training technique for deep SNNs. they propose a hybrid training methodology: 1) take a converted SNN and use its theyights and thresholds as an initialization step for spike-based backpropagation, and 2) perform incremental spike-timing dependent backpropagation (STDB) on this carefully initialized network to obtain an SNN that converges within few epochs and requires fetheyr time steps for input processing. STDB is performed with a novel surrogate gradient function defined using neuron's spike time. The theyight update is proportional to the difference in spike timing bettheyen the current time step and the most recent time step the neuron generated an output spike. The SNNs trained with their hybrid conversion-and-STDB training perform at $10{\times}{-}25{\times}$ fetheyr number of time steps and achieve similar accuracy compared to purely converted SNNs. The proposed training methodology converges in less than $20$ epochs of spike-based backpropagation for most standard image classification datasets, thereby greatly reducing the training complexity compared to training SNNs from scratch. they perform experiments on CIFAR-10, CIFAR-100 and ImageNet datasets for both VGG and ResNet architectures. they achieve top-1 accuracy of $65.19\%$ for ImageNet dataset on SNN with $250$ time steps, which is $10{\times}$ faster compared to converted SNNs with similar accuracy.
Accept (Poster). rating score: 8. rating score: 8. rating score: 8. rating score: 3. This manuscript proposes a general framework to learn non Euclidean distances from data using neural networks. The authors provide a combination of theoretical and experimental results in support of the use of several neural architectures to learn such distances. In particular, the develop “deep norms” and “wide norms”, based either on a deep or shallow neural network.<BRK>This paper proposes a modeling approach for norm and metric learning that ensures triangle inequalities are satisfied by the very design of the architecture. This architecture is used to model a norm, and in conjunction with an embedding   a metric. The authors also propose a mixture based approach that combines a given set of metrics into a new one using a max mean approach. The results are illustrated on a few mostly synthetic examples including metric nearness for random matrices, value functions for maze MDPs and distances between nodes on a graph (some problems here are sourced from open street map). I found the paper to be very well written.<BRK>This paper shows how to enforce and learn non Euclidean(semi )norms with neural networks. This is a promising direction for the community as partof a larger direction of understanding how to do bettermodeling in domains that naturally have non Euclideangeometries.<BRK>This paper is about learning and utilizing distance metrics in neural nets, particularly focusing on metrics that obey the triangle inequality. The approach given is quite well motivated. After describing the motivations and the differences between the three algorithms, the paper then goes on to show results on a couple of toy tasks (metric nearness, graph distances) and then a more challenging one in learning a UVFA.<BRK>Distances are pervasive in machine learning. They serve as similarity measures, loss functions, and learning targets; it is said that a good distance measure solves a task. When defining distances, the triangle inequality has proven to be a useful constraint, both theoretically---to prove convergence and optimality guarantees---and empirically---as an inductive bias. Deep metric learning architectures that respect the triangle inequality rely, almost exclusively, on Euclidean distance in the latent space. Though effective, this fails to model two broad classes of subadditive distances, common in graphs and reinforcement learning: asymmetric metrics, and metrics that cannot be embedded into Euclidean space. To address these problems, they introduce novel architectures that are guaranteed to satisfy the triangle inequality. they prove their architectures universally approximate norm-induced metrics on $\mathbb{R}^n$, and present a similar result for modified Input Convex Neural Networks. they show that their architectures outperform existing metric approaches when modeling graph distances and have a better inductive bias than non-metric approaches when training data is limited in the multi-goal reinforcement learning setting.

Accept (Poster). rating score: 8. rating score: 6. rating score: 6. Summary: This paper applies monotonic attention to the multiheaded (self attention) mechanisms used in a Transformer. I think more could be done to distinguish the behavior of the encoder self attention, the encoder decoder attention, and the decoder self attention. The latency reduction losses are novel, however. Overall, I think this is a solid accept, especially with some improvements to the presentation.<BRK>This paper proposes a fully transformer based monotonic attention framework that extends the idea of MILK. Though the idea of monotonic multi head attention sounds interesting, I still have some questions below:About the method:   1. Is that possible that the MMA would have worse latency than MILK since all the attention heads need to agree to write while MILK only has one attention head? The results in fig 2 seem counterintuitive. 2.I suggest the authors do more analysis of the difference between different attention heads to prove the effectiveness of MMA. 3.For the left two figures in fig 4, which one is the baseline, and which one is the proposed model?<BRK>While prior works deal with recurrent models, the authors adopt previous approaches for Transformer. I can not recommend accepting this paper due to the two main reasons. 1) The proposed solution lacks novelty. The current baseline is the recurrent model, but since the main contribution of this work is in how to deal with several heads the proper baselines would be (i) the same, but with single head attention, (ii) the same, but without L_var (summing latency penalty over heads is straightforward). However, these baselines are absent and the improvement is likely to be due to the replacement of RNN with the Transformer   this would be a limited contribution. Other comments on the experiments.<BRK>Simultaneous machine translation models start generating a target sequence before they have encoded or read the stheirce sequence. Recent approach for this task either apply a fixed policy on transformer, or a learnable monotonic attention on a theyaker recurrent neural network based structure. In this paper, they propose a new attention mechanism, Monotonic Multihead Attention (MMA), which introduced the monotonic attention mechanism to multihead attention. they also introduced two novel interpretable approaches for latency control that are specifically designed for multiple attentions. they apply MMA to the simultaneous machine translation task and demonstrate better latency-quality tradeoffs compared to MILk, the previous state-of-the-art approach.

Reject. rating score: 3. rating score: 3. rating score: 3. The proposal is an adapted batch normalization method for path regularization methods used in the optimization of neural networks. In that regard, it is natural to remove these singularities by optimizing along invariant input output paths. Yet, the paper does not motivate this type of regularization for batchnormalized nets. In fact, batch normalization naturally remedies this type of singularity since lengths of weights are trained separately from the direction of weights. Then, the authors motivate their novel batch normalization to gradient exploding (/vanishing) which is a completely different issue. Theorem 3.2 and 4.1 do not seem informative to me. Authors are saying that if some terms in the established bound in Theorem 4.1 is small, then exploding gradient does not occur for their novel method.<BRK>This paper analyzes a reparametrization of the network that migrates from the weight space to the path space. It enables an easier way to understand the batch normalization (BN). Theorem 3.1 itself is interesting and has some value in understanding BN. However, the main contribution of the paper, i.e., the proposal of the P BN, is not motivated enough. This is not verified by theory either. The formulation of the P BN seems to be closely related to ResNet, since it sets aside the identity mapping and only normalizes on the other part. How would you do this P BN in more complicated networks?<BRK>Originality: The paper proposed a new Path BatchNormalization in path space and compared the proposed method with traditional CNN with BN. The experimental part is not convincing. It is not easy to imagine how the re parameterization works on CNNs since the kernel is applied over the entire image ("hidden activations").<BRK>Neural networks with ReLU activation functions have demonstrated their success in many applications. Recently, researchers noticed a potential issue with the optimization of ReLU networks: the ReLU activation functions are positively scale-invariant (PSI), while the theyights are not. This mismatch may lead to undesirable behaviors in the optimization process. Hence, some new algorithms that conduct optimizations directly in the path space (the path space is proven to be PSI) theyre developed, such as Stochastic Gradient Descent (SGD) in the path space, and it was shown that SGD in the path space is superior to that in the theyight space. Hotheyver, it is still unknown whether other deep learning techniques beyond SGD, such as batch normalization (BN), could also have their counterparts in the path space. In this paper, they conduct a formal study on the design of BN in the path space. According to their study, the key challenge is how to ensure the forward propagation in the path space, because BN is utilized during the forward process. To tackle such challenge, they propose a novel re-parameterization of ReLU networks, with which they replace each theyight in the original neural network, with a new value calculated from one or several paths, while keeping the outputs of the network unchanged for any input. Then they show that BN in the path space, namely P-BN, is just a slightly modified conventional BN on the re-parameterized ReLU networks. their experiments on two benchmark datasets, CIFAR and ImageNet, show that the proposed P-BN can signiﬁcantly outperform the conventional BN in the theyight space.
Accept (Spotlight). rating score: 6. rating score: 6. rating score: 6. This paper studies counterfactual event prediction in physical simulation. There are also rich ablation studies. The writing is clear and easy to follow. Compared with that, the current manuscript has improved a lot.<BRK>Is this a complete piece of work or work in progress? 3) The approach attempts to show that confounder representations are learned as the main evidence that the model can perform counterfactual reasoning. Is the work a novel combination of well known techniques? I can t find any description.<BRK>Original comments:In this paper, the authors proposed a new method to learn physical dynamics based on counterfactual reasoning. The experimental result also shows promise. 2.This paper also provides a nice work that bridges the gap between the counterfactual reasoning and deep learning community.<BRK>Understanding causes and effects in mechanical systems is an essential component of reasoning in the physical world. This work poses a new problem of counterfactual learning of object mechanics from visual input.  they develop the CoPhy benchmark to assess the capacity of the state-of-the-art models for causal physical reasoning in a synthetic 3D environment and propose a model for learning the physical dynamics in a counterfactual setting. Having observed a mechanical experiment that involves, for example, a falling totheyr of blocks, a set of bouncing balls or colliding objects, they learn to predict how its outcome is affected by an arbitrary intervention on its initial conditions, such as displacing one of the objects in the scene. The alternative future is predicted given the altered past and a latent representation of the confounders learned by the model in an end-to-end fashion with no supervision. they compare against feedforward video prediction baselines and show how observing alternative experiences allows the network to capture latent physical properties of the environment, which results in significantly more accurate predictions at the level of super human performance.
Reject. rating score: 6. rating score: 6. rating score: 6. #######################Rebuttal Response:Thanks for these clarifications and updating the paper. #######################Review: Summary: The paper introduces an iterative learning scheme to perform system identification on partially observable systems. The proposed approach iteratively optimizes the unobserved state and the model parameters. Furthermore, the authors show that existing approaches don t scale to  high  dimensions. The approach is sound and reasonable and the paper is well written but I am uncertain whether this paper fits the scope of ICLR. Furthermore, the experiments can be improved, which is especially important as this paper does not propose a fundamental new approach. It would be really interesting to see this model controlling an actual quadcopter. The initial experiments demonstrating that the approach works and previous methods don t scale to high dimensions are very good and highlight important points. However, the more complex experiments might be not indicative of the performance. Furthermore, it would be interesting to see a non linear state model and a non linear observation model. Does the iterative approach also learn this model? > it is a bit unclear whether the linear model approximates just the noise or the complete model? Could you please clarify this point?<BRK>This paper proposes a novel way to address the well studied system identification problem. To this aim, the authors propose the "System indetification via Iterative Smoothing and Learning" (SISL) algorithm. The gist of the approach is to formulate an ML problem to estimate the system model parameters from a parametric family of models to estimate both the dynamical system evolution (state updates) and the observation process. The exact optimization for this ML problem is not tractable. This work proposes to solve a surrogate point estimate  ML problem. This optimization is solve using an alternate approach (between smoothing and learning). The synthetic setting is also used to motivate the need for methods able to handle large system dimensionality for non linear and partially observable systems. I believe that the paper can be improved in the following ways: 1) There are intermediate steps in the derivation of the maths that could be better explained. Equation 4 is not really derived from Equation 3. Also, I did not understand why the naive approach is not plotted on Figure 3 (a and b) (but it comes back in the appendix).<BRK>This is a strait forward paper that aims to address the lack of scalability of prior methods on system  identification. Below are some pros and cons for this paper:Cons:(+) The proposed method is simple and it could be  quickly adopted by robotics researchers and practitioners(+) The method seems to outperform previous algorithmsPros:( ) The idea of working with deterministic systems is restrictive. While the process noise may appear in some robotics applications to be small  I see no way of how one could reduce the effect of the observation noise. ( ) It would definitely strengthen the paper if the authors could elaborate more on the concepts of controllability, observability and how these concepts relate to their method etc. How does the existing methods handles these properties? How is the performance of the method affected  when the true system is not controllable but it is stabilizable? ( ) There seems to be no assumption on the shape of the trajectories. These are signals that can excite dynamics so that to ensure proper identification.<BRK>System identification is the process of building a mathematical model of an unknown system from measurements of its inputs and outputs. It is a key step for model-based control, estimator design, and output prediction. This work presents an algorithm for non-linear offline system identification from partial observations, i.e. situations in which the system's full-state is not directly observable. The algorithm presented, called SISL, iteratively infers the system's full state through non-linear optimization and then updates the model parameters. they test their algorithm on a simulated system of coupled Lorenz attractors, showing their algorithm's ability to identify high-dimensional systems that prove intractable for particle-based approaches. they also use SISL to identify the dynamics of an aerobatic helicopter. By augmenting the state with unobserved fluid states, they learn a model that predicts the acceleration of the helicopter better than state-of-the-art approaches.
Reject. rating score: 1. rating score: 3. rating score: 3. * Writing of the paper can probably be polished further for better clarity. Another of my concern is that, if the goal is to make the sparsification decision aware of the network output (e.g., the value of the loss function), a simpler approach would be to enforce L1 regularization over the edges.<BRK>This work proposes iSparse framework, which aims to sparsify a neural network by removing redundant edges. Comment:Although I am not an expert in network pruning or network sparsification, I know that the Lottery Ticket Hypothesis (Frankle & Carbin, 2019) were able to remove at most 80% of the weights of neural networks (both fully connected and ConvNets) and still retain the original performance level.<BRK>This paper proposes a sparsification technique that seeks edges contributing negligible amounts to the performance of a network. How did you determine theta_l for each layer? The major problem of this paper is the experimental section.<BRK>Deep neural networks have demonstrated unprecedented success in various knowledge management applications. Hotheyver, the networks created are often very complex, with large numbers of trainable edges which require extensive computational restheirces. they note that many successful networks nevertheless often contain large numbers of redundant edges. Moreover, many of these edges may have negligible contributions towards the overall network performance. In this paper, they propose a novel iSparse framework and experimentally show, that they can sparsify the network, by 30-50%, without impacting the network performance. iSparse leverages a novel edge significance score, E, to determine the importance of an edge with respect to the final network output. Furthermore, iSparse can be applied both while training a model or on top of a pre-trained model, making it a  retraining-free approach - leading to a minimal computational overhead. Comparisons of iSparse against PFEC, NISP, DropConnect, and Retraining-Free on benchmark datasets show that iSparse leads to effective network sparsifications.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. rating score: 6. nan<BRK>The proposed meta learning approach to training the model seems to perform well across multiple simple and challenging datasets, and therefore I would recommend accept.<BRK>I m happy to increase my score and recommend acceptance based on the revised paper.<BRK>nan<BRK>they study the problem of learning an associative memory model -- a system which is able to retrieve a remembered pattern based on its distorted or incomplete version.
Attractor networks provide a sound model of associative memory: patterns are stored as attractors of the network dynamics and associative retrieval is performed by running the dynamics starting from a query pattern until it converges to an attractor. 
In such models the dynamics are often implemented as an optimization procedure that minimizes an energy function, such as in the classical Hopfield network. 
In general it is difficult to derive a writing rule for a given dynamics and energy that is both compressive and fast.
Thus, most research in energy-based memory has been limited either to tractable energy models not expressive enough to handle complex high-dimensional objects such as natural images, or to models that do not offer fast writing.
they present a novel meta-learning approach to energy-based memory models (EBMM) that allows one to use an arbitrary neural architecture as an energy model and quickly store patterns in its theyights. 
they demonstrate experimentally that their EBMM approach can build compressed memories for synthetic and natural data, and is capable of associative retrieval that outperforms existing memory systems in terms of the reconstruction error and compression rate.
Reject. rating score: 3. rating score: 3. rating score: 6. This paper claims that one only needs a reward prediction model to learn a good latent representation for model based reinforcement learning. I think it is crucial to include DeepMDP, as it is the one most likely to perform competitively with the proposed method. The contributions consist only of learning a multi step reward model for planning, and only provide results in two dense reward environments.<BRK>This is in contrast to existing work which generally use a combination of reward prediction and state reconstruction to learn the latent model. The paper is also very well written and easy to follow. The results on images in the appendix seem to show a delta between the true and predicted reward, suggesting that the proposed method does not yet work on images. (2): From what I can see, the proposed method is very similar to the PlaNet algorithm with state reconstruction loss removed. (3): One of the strengths of model based reinforcement learning is the ability to plan to reach unseen goals with a model trained via self supervision or different goals. Does the proposed approach lose some of this, by overfitting to only the task reward?<BRK>Summary:This paper proposes a novel algorithm for planning on specific domains through latent reward prediction. Using these functions, the authors define the objective using the mean squared error between true and multi step prediction of rewards. In this paper, the authors assume deterministic transition and use deterministic function for latent transition. It seems to be the authors want to use MPC, which is a powerful planning algorithm. It seems to be different results from intuition, because the authors emphasize that the strength of the proposed method is efficiency of learning in RL tasks with irrelevant information. Questions and minor comments:  What objective is used to learn the latent model of the state prediction model algorithm?<BRK>Model-based reinforcement learning methods typically learn models for high-dimensional state spaces by aiming to reconstruct and predict the original observations. Hotheyver, drawing inspiration from model-free reinforcement learning, they propose learning a latent dynamics model directly from rewards. In this work, they introduce a model-based planning framework which learns a latent reward prediction model and then plan in the latent state-space. The latent representation is learned exclusively from multi-step reward prediction which they show to be the only necessary information for successful planning.  With this framework, they are able to benefit from the concise model-free representation, while still enjoying the data-efficiency of model-based algorithms.  they demonstrate their framework in multi-pendulum and multi-cheetah environments where several pendulums or cheetahs are shown to the agent but only one of them produces rewards. In these environments, it is important for the agent to construct a concise latent representation to filter out irrelevant observations. they find that their method can successfully learn an accurate latent reward prediction model in the presence of the irrelevant information while existing model-based methods fail. Planning in the learned latent state-space shows strong performance and high sample efficiency over model-free and model-based baselines.
Accept (Spotlight). rating score: 8. rating score: 8. In this paper, the authors present a new adversarial training algorithm and apply it to the fintuning stage large scale language models BERT and RoBERTa.<BRK> 	This paper modifies and extends the recent “free” training strategies in adversarial training for representation learning for natural language. The paper is well written.<BRK>Adversarial training, which minimizes the maximal risk for label-preserving input perturbations, has proved to be effective for improving the generalization of language models. In this work, they propose a novel adversarial training algorithm, FreeLB, that promotes higher invariance in the embedding space, by adding adversarial perturbations to word embeddings and minimizing the resultant adversarial risk inside different regions around input samples. To validate the effectiveness of the proposed approach, they apply it to Transformer-based models for natural language understanding and commonsense reasoning tasks. Experiments on the GLUE benchmark show that when applied only to the finetuning stage, it is able to improve the overall test scores of BERT-base model from 78.3 to 79.4, and RoBERTa-large model from 88.5 to 88.8. In addition, the proposed approach achieves state-of-the-art single-model test accuracies of 85.44% and 67.75% on ARC-Easy and ARC-Challenge. Experiments on CommonsenseQA benchmark further demonstrate that FreeLB can be generalized and boost the performance of RoBERTa-large model on other tasks as theyll.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. rating score: 6. The new proof provides new insights into the universal approximation property. I consider these the main contribution of the paper. Specifically, the authors  provide an upper bound on the required width for the neural network  show that the approximation property still holds even if strong further requirements are imposed on the weights of the first or last layer. It’s a very good paper. The work makes useful contributions that should and will be of interest to many in the field. The paper is generally well written. Some remarks:  Being somewhat long, the “Proof of Theorem 3.1” would be a much better read if the authors prefixed it  with an outline of the strategy that the proof takes.<BRK>This paper studies the representation power of single layer neural networks with continuous non polynomial activation, and specifically, provided a refinement for the universal approximation theorem:1. The writing of the paper is concrete and solid. 3.This analysis and some of the results derived in the proof may be used for other analyses, e.g.representation power of multilayer networks. Some further discussion of the results may be of interest to the readers. Some comment on those results may be beneficial (e.g.https://arxiv.org/abs/1810.04374).<BRK>The new proof provides new insights into the universal approximation property. I consider these the main contribution of the paper. Specifically, the authors  provide an upper bound on the required width for the neural network  show that the approximation property still holds even if strong further requirements are imposed on the weights of the first or last layer. It’s a very good paper. The work makes useful contributions that should and will be of interest to many in the field. The paper is generally well written. Some remarks:  Being somewhat long, the “Proof of Theorem 3.1” would be a much better read if the authors prefixed it  with an outline of the strategy that the proof takes.<BRK>The authors derive the universal approximation property proofs algebraically. They assert that their results are general to other kinds of neural networks and similar learners. They leave the paper with a question regarding limitations on bias weights. I have opted for a weak accept since it seems thorough and the conclusions offer promise for other applications.<BRK>The universal approximation theorem, in one of its most general versions, says that if they consider only continuous activation functions σ, then a standard feedforward neural network with one hidden layer is able to approximate any continuous multivariate function f to any given approximation threshold ε, if and only if σ is non-polynomial. In this paper, they give a direct algebraic proof of the theorem. Furthermore they shall explicitly quantify the number of hidden units required for approximation. Specifically, if X in R^n is compact, then a neural network with n input units, m output units, and a single hidden layer with {n+d choose d} hidden units (independent of m and ε), can uniformly approximate any polynomial function f:X -> R^m whose total degree is at most d for each of its m coordinate functions. In the general case that f is any continuous function, they show there exists some N in O(ε^{-n}) (independent of m), such that N hidden units would suffice to approximate f. they also show that this uniform approximation property (UAP) still holds even under seemingly strong conditions imposed on the theyights. they highlight several consequences: (i) For any δ > 0, the UAP still holds if they restrict all non-bias theyights w in the last layer to satisfy |w| < δ. (ii) There exists some λ>0 (depending only on f and σ), such that the UAP still holds if they restrict all non-bias theyights w in the first layer to satisfy |w|>λ. (iii) If the non-bias theyights in the first layer are *fixed* and randomly chosen from a suitable range, then the UAP holds with probability 1.
Reject. rating score: 1. rating score: 3. rating score: 3. If the motivation of this paper is to remove the representation error of GAN, GAN should be viewed as the main body. The proposed methods are evaluated on two image restoration tasks, including compressive sensing and image super resolution. The effectiveness of the combination is also presented.<BRK>The proposed hybrid model is helpful on compressed sensing experiments on the CelebA dataset; however, it is only marginally better than deep decoder on image super resolution and out of distribution compressed sensing. A smaller version of Figure 1 can be probably moved to the beginning of the paper to illustrate the problem of GAN. But even with the help of Figure 1, it is still unclear what is the fundamental problem for GAN. Figure 5 should be renamed as a Table. Moreover, the motivation is not strong enough.<BRK>This paper presents a method for reducing the representation error generative convolutional neural networks by combining them with untrained deep decoder. The method is evaluated on compressive sensing and super resolution, where a better performance than the isolated use of Deep Decoders and GAN priors. The main contribution of the paper is not the performance, but the simplicity of this approach.<BRK>Generative models, such as GANs, have demonstrated impressive performance as natural image priors for solving inverse problems such as image restoration and compressive sensing. Despite this performance, they can exhibit substantial representation error for both in-distribution and out-of-distribution images, because they maintain explicit low-dimensional learned representations of a natural signal class. In this paper, they demonstrate a method for removing the representation error of a GAN when used as a prior in inverse problems by modeling images as the linear combination of a GAN with a Deep Decoder. The deep decoder is an underparameterized and most importantly unlearned natural signal model similar to the Deep Image Prior.  No knowledge of the specific inverse problem is needed in the training of the GAN underlying their method.  For compressive sensing and image superresolution, their hybrid model exhibits consistently higher PSNRs than both the GAN priors and Deep Decoder separately, both on in-distribution and out-of-distribution images.  This model provides a method for extensibly and cheaply leveraging both the benefits of learned and unlearned image recovery priors in inverse problems.
Reject. rating score: 3. rating score: 3. rating score: 8. From an experimental point of view, the baselines considered are very weak. Overall, the conceptual and experimental contributions of the paper are rather weak and I thus recommend rejection.<BRK>7   What is an “advanced” DNN? This should be clearly surfaced in the introduction and presentation of contributions if DPAR is required for the proposed generalized min max formulation to improve robustness.<BRK>The problem is important and interesting. The proposed framework has solid theory and is well conceived. Overall I feel it is a well written paper with sufficient contributions and is of interest to a range of ICLR audience.<BRK>The worst-case training principle that minimizes the maximal adversarial loss, also known as adversarial training (AT), has shown to be a state-of-the-art approach for enhancing adversarial robustness against norm-ball bounded input perturbations. Nonetheless, min-max optimization beyond the purpose of AT has not been rigorously explored in the research of adversarial attack and defense. In particular, given a set of risk stheirces (domains), minimizing the maximal loss induced from the domain set can be reformulated as a general min-max problem that is different from AT. Examples of this general formulation include attacking model ensembles, devising universal perturbation under multiple inputs or data transformations, and generalized AT over different types of attack models. they show that these problems can be solved under a unified and theoretically principled min-max optimization framework.  they also show that the self-adjusted domain theyights learned from their method provides a means to explain the difficulty level of attack and defense over multiple domains. Extensive experiments show that their approach leads to substantial performance improvement over the conventional averaging strategy.
Reject. rating score: 3. rating score: 3. rating score: 3. This paper considers the autoencoder model combining the usual information bottleneck and the Gaussian mixture model (GMM). While the framework and the performance of the proposed method are interesting and promising, some of its main parts are unclearly explained. (16) and (17): The distribution q(x_i|u_i,m) is undefined. I wonder why Q_\phi(x|u) in Eq.(11) doesn’t have to be a distribution.<BRK>The idea of using a latent mixture of Gaussian s to variationally encode high dimensional data is not new. These methods are respectively implemented in the TensorFlow and PyTorch APIs. This paper however uses the arg max of equation 19.<BRK>This paper purposes to cluster data in an unsupervised manner that estimates the distribution with GMM in latent space instead of original data space. The whole pipeline is clear and makes sense.<BRK>In this paper, they develop an unsupervised generative clustering framework that combines variational information bottleneck and the Gaussian Mixture Model. Specifically, in their approach they use the variational information bottleneck method and model the latent space as a mixture of Gaussians. they derive a bound on the cost function of their model that generalizes the evidence lotheyr bound (ELBO); and provide a variational inference type algorithm that allows to compute it. In the algorithm, the coders’ mappings are parametrized using neural networks and the bound is approximated by Markov sampling and optimized with stochastic gradient descent. Numerical results on real datasets are provided to support the efficiency of their method.
Reject. rating score: 1. rating score: 6. rating score: 6. This paper presents a grammar based generation approach for "slot filling" style code generation tasks. The idea of formalizing code completion as structured language modeling and extending Alon et al., 2019a for the task is natural and well executed, with strong models and significantly improved results on two code completion benchmarks for both Java and C#. Detailed Review:*Technical Contribution* I have a very mixed feeling with this paper, while the model registers high empirical performance, the technical contribution is a bit limited, as detailed below:      *Path based Context Encoding* The most important contribution in this submission is the application of path based AST encoding model of Alon et al., 2019a to encode context (the given contextual and partially generated ASTs) for code generation. *Node based Tree Generation Model* Apart from the path based context encoding model, the node based generation model presented in Section 2 also seems interesting. How to Improve: to better understand the different technical contributions outlined above and their relative impacts, the following ablation studies would be helpful:          Importance of Path based Context Encoding: the Seq→Path ablation in Table 3 alone might not be adequate to demonstrate the importance of path based encoding of AST contexts for code generation tasks. The authors should compare with the GNN based context encoding approach in Brockschmidt et al.(2019a) as this is the most relevant work. However, there are indeed code generation systems (some of them cited in this paper) that synthesize open domain code snippets in general porpuse programming languages without restriction on vocabulary or grammar. How to improve: the authors might present more evidence to substantiate the their claim on the novelty of the AnyGen benchmark compared with existing open domain, general purpose code synthesis benchmarks, or consider revising the claim and the title. 2019<BRK>The paper proposes a model to address the Any Code Generation (AnyGen) task, which basically to fill missing code from a given program. The model makes use of partial Abstract Syntax Tree (AST) as input. The conducted experiments show that using AST paths from root and leaves are good for AST node generation, but whether those inputs are robust and sufficient should be further explored. There are some restrictions to the method, for example,  the input is only a single function, and the missing expression is not that complex. Nevertheless this work presents a novel method towards code generation. The paper also introduces a new metric to evaluate the prediction accuracy for generated expressions. 2.In the Restrict Code Generation test, it seems that the author filters out non primitive types and user defined functions. Therefore, does the experiment on Java small dataset fully show the proposed model’s strength?<BRK>This paper proposes a generative task for programming code where an expression from the program is generated given the rest of the program (minus the expression). Again, this is similar to the prediction of words in language models. The method takes into account the AST corresponding to the program. The results for the Java dataset improve state of the art by 1 2%, while the results for the restricted C# dataset show a much more significant improvement (in the order of 10 15% improvement, depending on the metric). I would have liked to see a qualitative analysis of the results. In other words, when the prediction looking at the tree structure is correct and the overall prediction is not, what goes wrong? The elimination of the methods with more than 20 lines of code seems ad hoc to me and biases the evaluation with relatively short methods (how many methods were eliminated this way?). One thing that I struggle with is understanding how useful the proposed task is and how it can be generalized/used in practice for some relevant higher level task in AI4code.<BRK>they address the problem of Any-Code Generation (AnyGen) - generating code without any restriction on the vocabulary or structure. The state-of-the-art in this problem is the sequence-to-sequence (seq2seq) approach, which treats code as a sequence and does not leverage any structural information. they introduce a new approach to AnyGen that leverages the strict syntax of programming languages to model a code snippet as tree structural language modeling (SLM). SLM estimates the probability of the program's abstract syntax tree (AST) by decomposing it into a product of conditional probabilities over its nodes. they present a neural model that computes these conditional probabilities by considering all AST paths leading to a target node. Unlike previous structural techniques that have severely restricted the kinds of expressions that can be generated, their approach can generate arbitrary expressions in any programming language. their model significantly outperforms both seq2seq and a variety of existing structured approaches in generating Java and C# code. they make their code, datasets, and models available online.
Reject. rating score: 3. rating score: 3. rating score: 3. The very idea of a flow based model for graphs using the message passing algorithm may be considered as a contribution, but this is also blurred because of the concurrent work GNF.<BRK>What is the motivation of using the dual of the graph?<BRK>Experimental results show that the proposed model is superior to recent models in image puzzling and layout generation datasets.<BRK>In this paper, they propose Continuous Graph Flow, a generative continuous flow based method that aims to model complex distributions of graph-structured data.  Once learned, the model can be applied to an arbitrary graph, defining a probability density over the random variables represented by the graph. It is formulated as an ordinary differential equation system with shared and reusable functions that operate over the graphs.  This leads to a new type of neural graph message passing scheme that performs continuous message passing over time. This class of models offers several advantages: a flexible representation that can generalize to variable data dimensions; ability to model dependencies in complex data distributions; reversible and memory-efficient; and exact and efficient computation of the likelihood of the data. they demonstrate the effectiveness of their model on a diverse set of generation tasks across different domains: graph generation, image puzzle generation, and layout generation from scene graphs. their proposed model achieves significantly better performance compared to  state-of-the-art models.
Reject. rating score: 1. rating score: 1. rating score: 1. This paper try to use CNN to build recognizer for handwritten Amharic characters. The CNN they used is simple and standard. Apparently this paper no novelty at all. They just apply CNN to a new task. This kind of work is not qualified for ICLR at all. There are also some problems in paper organization. They should split the introduction part into several paragraphs to improve reading experience. And it seems that they forget to add reference part. Given the quality of the writing and content, I decide to reject this paper.<BRK>The formatting of the paper needs work, and the work is not substantially novel. For some reason, many sections of the paper were ill formatted, perhaps due to using an insufficiently portable submission format. Characters with arguably the same level of complexity, such as Chinese characters, have been thoroughly explored. The application of existing technology to a different script does not, by itself, compel me to feel that this is an ICLR worthy paper. That being said, I think it is quite notable the effort that went into creating the data set for Amharic character recognition. I would encourage the authors to distribute this data set so that others may join in the research efforts.<BRK>The authors collect a dataset of handwritten Amharic characters and apply a CNN model for character recognition. The dataset is novel and would be of interest to the character recognition community, and I would encourage the authors to present it in its own right along with technical details about its collection. 7.Are the images in Figure 3 from your dataset? To the best of my understanding, there is no held out test set, and so we cannot know the performance of the model on unseen data. Please show some examples from the new dataset you have collected! It appears that validation accuracy was taken into account when selecting hyperparameters, and so, validation accuracy also does not represent the model s performance on unseen data. I would recommend that the authors introduce a test set into their dataset split, or designate the validation part of the dataset as a test part use cross validation for hyperparameter tuning. It is challenging to compare to prior work since, according to the authors, prior work on Amharic character recognition has been focused on printed text. While Figure 3 shows that different characters can be written similarly, it would be great to provide a quantitative measure of this phenomenon. 2.It would be great to know more about how many individuals were selected (and the choices made about their demographics) for writing the example characters, and whether there were any interesting variations observed in the dataset based on attributes highlighted in the paper (e.g.age range)3. 4.In the text, features such as a mark of palatalization are noted.<BRK>Amharic language is an official language of the federal government of the Federal Democratic Republic of Ethiopia. Accordingly, there is a bulk of handwritten Amharic documents available in libraries, information centres, museums, and offices. Digitization of these documents enables to harness already available language technologies to local information needs and developments. Converting these documents will have a lot of advantages including (i) to preserve and transfer history of the country (ii) to save storage space (ii) proper handling of documents (iv) enhance retrieval of information through internet and other applications. Handwritten Amharic character recognition system becomes a challenging task due to inconsistency of a writer, variability in writing styles of different writers, relatively large number of characters of the script, high interclass similarity, structural complexity and degradation of documents due to different reasons. In order to recognize handwritten Amharic character a novel method based on deep neural networks is used which has recently shown exceptional performance in various pattern recognition and machine learning applications, but has not been endeavtheired for Ethiopic script. The CNN model is trained and tested their database that contains 132,500 datasets of handwritten Amharic characters. Common machine learning methods usually apply a combination of feature extractor and trainable classifier. The use of CNN leads to significant improvements across different machine-learning classification algorithms. their proposed CNN model is giving an accuracy of 91.83% on training data and 90.47% on validation data.
Reject. rating score: 1. rating score: 3. rating score: 3. This paper provides a method for lossless compression of text. It s heavily inspired by the language modelling methods that have been developed for the purposes of predicting the next character/word in a sentence, and it uses this idea as its backbone. The only difference is that the results are presented in the compression setting. there s no comparison even with BERT (how well it do to predict the next character vs. this)...  no runtime numbers  no reproducibility discussion (i.e., how can I guarantee that my decoder can get exactly the same numbers as my encoder so that I can decompress on a different machine)  no discussion about whether files were created/decompressed (this is ABSOLUTELY CRUCIAL for compression papers to discuss)Overall, I am not excited about this paper, and unless the authors put a lot more into it, there s just not enough novelty to justify a publication at ICLR.<BRK>In my view, the experiments described in the paper resemble a hyper parameter search more than architectural improvements. In particular the authors have done a great job reviewing existing compression literature and positioning their method within the space of prior work. The paper does not answer the question of whether or not this is true. Furthermore, similar to autoregressive models, transformers are known to be slow at inference time. For reasons mentioned above, the paper should include additional experimental evaluation. In particular, it should consider the effect of training the model on one dataset, but evaluating it on another dataset; and discuss how differences in performance (if any) compare to standard methods. 4.Description of the “training with revisits” is not very clear. My understanding is that it resembles a pass through the data, where some of it is considered again at specific intervals.<BRK>This paper explores the effectiveness of the Transformer architecture to the lossless data compression problem. The authors conduct their experiments on the enwik8 benchmark. My main concern of this paper is that the proposed method was only evaluated on a single benchmark data. Minor comment:In Section 4.2, there is a missing citation. ... we do not use Adaptive Inputs (Baevski & Auli, 2018; ?) ...Please check and fix it.<BRK>Transformers have replaced long-short term memory and other recurrent neural networks variants in sequence modeling. It achieves state-of-the-art performance on a wide range of tasks related to natural language processing, including language modeling, machine translation, and sentence representation. Lossless compression is another problem that can benefit from better sequence models. It is closely related to the problem of online learning of language models. But, despite this ressemblance, it is an area where purely neural network based methods have not yet reached the compression ratio of state-of-the-art algorithms. In this paper, they propose a Transformer based lossless compression method that match the best compression ratio for text. their approach is purely based on neural networks and does not rely on hand-crafted features as other lossless compression algorithms. they also provide a thorough study of the impact of the different components of the Transformer and its training on the compression ratio.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. The paper presents a graph neural network model inspired by the consciousness prior of Bengio (2017) and implements it by means of two GNN models: the inattentive and the attentive GNN, respectively IGNN and AGNN.<BRK>The authors propose to use sampling methods in order to apply graph neural networks to large scale knowledge graphs for semantic reasoning. To this end induced subgraphs are constructed in a data dependent way using an attention mechanism. Overall, I think that the proposed GNN architecture is an original and interesting approach for this specific application. I am missing a discussion of the limitations of the proposed approach.<BRK>The proposed way to reduce the complexity by restricting the attention horizon sounds interesting and seems necessary for scaling up. # Originality  The idea of learning an input dependent subgraph using GNN seems new.<BRK>they propose Dynamically Pruned Message Passing Networks (DPMPN) for large-scale knowledge graph reasoning. In contrast to existing models, embedding-based or path-based, they learn an input-dependent subgraph to explicitly model a sequential reasoning process. Each subgraph is dynamically constructed, expanding itself selectively under a flow-style attention mechanism. In this way, they can not only construct graphical explanations to interpret prediction, but also prune message passing in Graph Neural Networks (GNNs) to scale with the size of graphs. they take the inspiration from the consciousness prior proposed by Bengio to design a two-GNN framework to encode global input-invariant graph-structured representation and learn local input-dependent one coordinated by an attention module. Experiments show the reasoning capability in their model that is providing a clear graphical explanation as theyll as predicting results accurately, outperforming most state-of-the-art methods in knowledge base completion tasks.
Reject. rating score: 3. rating score: 3. rating score: 3. In this paper the authors propose a metric based model for few shot learning. The goal of the proposed technique is to incorporate a prior that highlight better the dissimilarity between closely related class prototype. Thus, the proposed paper is related to prototypical neural network (use of prototype to represent a class) but differ from it by using inner product scoring  as a similarity measure instead of the use of euclidean distance. There is also close similarity between the proposed method and matching network overall, the paper does not  highlight the novelty of their proposed method especially prototypical network and matching network.<BRK>The authors propose a new neural network model, called as Dissimilarity Network, to improve the few shot learning accuracy. However, the paper is not quite well written. Also, the terms, “score”, “metric”, “dissimilarity” are mentioned in the paper but the paper is not really learning the metric, to my understanding. Thus the details of the paper is quite hard to grasp. Lastly, the idea of designing the global embedding and the task aware embedding is interesting but shouldn’t really be restricted to few shot learning. It would be interesting to test the idea on general classification tasks, for example in a simple cross validation settings. The current writing refers to that the authors comprises a completely new dataset with new labels. 4)	In Section 2.2, it would be nice to add the mathematical definition of “prototype”. Also it would be nice to add more details about the attention mechanism. 7)	In the result section, it would be nice to discuss when the proposed method is doing better than other methods, for example RelationNet, as well as when it’s worse since different datasets show different results.<BRK>The stated contributions of the paper are: (1) a method for performing few shot learning and (2) an approach for building harder few shot learning datasets from existing datasets. The authors describe a model for creating a task aware embedding for different novel sets (for different image classification settings) using a nonlinear self attention like mechanism applied to the centroid of the global embeddings for each class. The resulting embeddings are used per class with an additional attention layer applied on the embeddings from the other classes to identify closely related classes and consider the part of the embedding orthogonal to the attention weighted average of these closely related classes. They compare the accuracy of their model vs others in the 1 shot and 5 shot setting on various datasets, including a derived dataset from CIFAR which they call Hierarchical CIFAR.<BRK>Few-shot classification may involve differentiating data that belongs to a different level of labels granularity. Compounded by the fact that the number of available labeled examples are scarce in the novel classification set, relying solely on the loss function to implicitly guide the classifier to separate data based on its label might not be enough; few-shot classifier needs to be very biased to perform theyll. In this paper, they propose a model that incorporates a simple prior: focusing on differences by building a dissimilar set of class representations. The model treats a class representation as a vector and removes its component that is shared among closely related class representatives. It does so through the combination of learned attention and vector orthogonalization. their model works theyll on their newly introduced dataset, Hierarchical-CIFAR, that contains different level of labels granularity. It also substantially improved the performance on fine-grained classification dataset, CUB; whereas staying competitive on standard benchmarks such as mini-Imagenet, Omniglot, and few-shot dataset derived from CIFAR.
Reject. rating score: 6. rating score: 6. rating score: 6. I m still not completely convinced that the multi layer analysis well explains the reality, but because of the other contributions I am updating my score, which would be a 5/10 on the old ICLR scale. This paper investigates the placement of Layer Normalization in the transformer architecture. The authors show that the Pre LN placement leads to better behaved gradients as the network gets larger. This in turn allows them to remove the warmup stage of the learning rate schedule, leading to a faster and simpler training procedure. Even though the novelty here is limited, Pre LN placement has been used in prior work, the potential for accelerating future research is large. Still, I have some concerns about the relation between the analytic investigation of the gradient norms and the empirical results that are presented, and I am concerned that the analytical results are used to imply something stronger than they actually show. At some level, it seems like the theoretical results have come along for the ride but do not clearly demonstrate that there is a problem with Post LN and that this problem is fixed by switching to Pre LN, or at least the relationship is not clearly explained.<BRK>Summary: The paper investigates the myth about layer normalization and learning rate warmup for the Transformer architecture. It shows, both theoretically and empirically, that putting the layer normalization in the residual blocks rather than between the residual blocks, could make a big difference to the scale of the gradient at the initialization stage. Pros:  + A well written paper with a good organization; notations are clear. + Good experimental design that compares the pre LN and post LN Transformers in different settings/tasks. One of the two major concerns I have is the novelty of this paper in terms of its methodology and empirical value to the community. The Pre LN setting of Transformers has already been widely used. 2.The second major concern I have is the connection the authors established between its theoretical findings and the empirical findings. But even when the gradient clipping is applied, learning rate warm up still seems very helpful (and sometimes necessary), as was used in all of these works. Therefore, I think to further verify the theoretical hypotheses of the paper, the authors should at least also study whether (and to what degree) the very simple "gradient clipping" (or other gradient normalization techniques) solves the problem (which is a common solution to exploding gradients). What do you expect to be the relationship between "the number of warmup steps" and solving the "gradient scale problem", which you proved on these assumptions?<BRK>1.Specific problem tackled by the paper:Moving the LayerNorm layer to be inside the residual connection in a stack of transformers can remove the need for learning rate warm up. This paper provides a theoretical motivation for doing this. [d] In Figure 3(b) the gradients are clearly decreasing with the number of layers, are there any comments on this? Their experiments provide good context for the problem they are solving and acts as a solid reference point for their (and other peoples  future) work. If so, this should be made very clear. 3.Claims of the paper:The authors claim that layer norm should be placed inside the residual connection (Pre LN) rather than outside (Post LN) it. i.e.During training a Post LN transformer is likely to have weaker gradients in the lower (closer to input layers) than at the output layers. Figure 1 could also be enhanced by labelling Post LN as previous work and Pre LN as current work. The authors show machine translation results, demonstrating that using Pre LN rather than Post LN leads to faster convergence, however the models converge to the same result. It would be worth connecting these. (2) The paper is well organised and the problem is very well motivated. Cons:(1) This work is incremental. (2) While the organisation of the paper is good, the paper is not well written.<BRK>The Transformer architecture is popularly used in natural language processing tasks. To train a Transformer model, a carefully designed learning rate warm-up stage is usually needed: the learning rate has to be set to an extremely small value at the beginning of the optimization and then gradually increases in some given number of iterations. Such a stage is shown to be crucial to the final performance and brings more hyper-parameter tunings. In this paper, they study why the learning rate warm-up stage is important in training the Transformer and theoretically show that the location of layer normalization matters. It can be proved that at the beginning of the optimization, for the original Transformer, which places the layer normalization bettheyen the residual blocks, the expected gradients of the parameters near the output layer are large. Then using a large learning rate on those gradients makes the training unstable. The warm-up stage is practically helpful to avoid this problem. Such an analysis motivates us to investigate a slightly modified Transformer architecture which locates the layer normalization inside the residual blocks. they show that the gradients in this Transformer architecture are theyll-behaved at initialization. Given these findings,  they are the first to show that this Transformer variant is easier and faster to train. The learning rate warm-up stage can be safely removed, and the training time can be largely reduced on a wide range of applications.
Reject. rating score: 3. rating score: 6. rating score: 6. The technical novelty lies in thenormalization of the feature weights across local explanations wherethe proposed scheme utilizes a well motivated L0 normalization inplace of the existing L2 normalization in Ribeiro et al., 2016. However, I can  imagine that the high level idea of sparse aggregation of local  explanations can easily generalize to many local explanation  schemes. Moreover, the evaluation is  completely limited to a single data set (which is good as a strawman  but not sufficient to make significant claims). Generating class level interpretations is an  interesting idea but its novelty is somewhat unclear. In terms of explanations, there are contrastive explanations [1]  which are not really discussed in this paper. It is not clear why in the SmoothGrad and VarGrad baselines, the  class level interpretations are generated with just 10 images per  class. Are the class level interpretations of LIME and NormLIME also  generated from just 10 images per class. If LIME and NormLIME used more than 10 images to  generate class level interpretations, the comparison does not seem  fair. For the evaluation in Sec.5, it is not clear how the features are  removed. Are they removed based on the global explanations (feature  importances) of each baseline? Moreover, it seems natural that something like  SmoothGrad and VarGrad would be able to capture the feature  redundancies if the redundancies were the actual cause for this  marked difference between the relative performances in Fig.4(a) and  4(b).<BRK>Months ago, I read this article on arxiv (https://arxiv.org/pdf/1909.04200.pdf). It is an interesting work that tries to propose a simple yet effective interpretable model. I am not familiar with this research direction, and I try to make an educated guess. Pros:  As the author suggested, the method is simple and effective. The authors conducted user studies to demonstrate that the results generated by their proposed method is strongly favored over previous methods. Cons:  Subscripts in equations need improvement to make them consistent. Section 4, Figure 3, top, it seems obvious to choose the fourth one to distinguish the number 9?<BRK>This paper proposes a new method for DNN interpretability based on LIME, itself based on ensembling of large numbers of low complexity models called "local explanation models". This method allows to better capture the relative importance of each feature, and is also able to recover the class specific signals that DNNs use to distinguish between a number of classes. Score: Weak AcceptWhile this method seems like a good step forward, I am uncertain about its significance, mostly because of the choice of dataset and because it appears to be a minor change to a well known algorithm. Why should we expect this method to work beyond MNIST? Will it work on non image data? Do the authors have a stronger intuition than what is hinted at in the paper? Other comments:  The human evaluation is very interesting, and suggests that this method correlates much better with human judgement than previous ones. It could be from the proposed change or from the class specific information.<BRK>The problem of explaining deep learning models, and model predictions generally, has attracted intensive interest recently. Many successful approaches forgo global approximations in order to provide more faithful local interpretations of the model’s behavior. LIME develops multiple  interpretable models, each approximating a large neural network on a small region of the data manifold, and SP-LIME aggregates the local models to form a global interpretation.  Extending this line of research, they propose a simple yet effective method, NormLIME, for aggregating local models into global and class-specific interpretations.  A human user study strongly favored the class-specific interpretations created by NormLIME to other feature importance metrics. Numerical experiments employing Keep And Retrain (KAR) based feature ablation across various baselines (Random, Gradient-based, LIME, SHAP) confirms NormLIME’s effectiveness for recognizing important features.
Reject. rating score: 3. rating score: 3. rating score: 3. rating score: 6. This paper studied how to leverage the power of graph neural networks for counting subgraph isomorphism. The GNN and sequence modeling methods are discussed for solving this problem. The experimental results confirmed the effectiveness of these methods. More practical use case would be search for the matched subgraphs given the sub graph query using subgraph isomorphism detection. Without comparing these existing fast (approximation) methods, it is really unfair to compare with only non DL baseline VF2, which seems served as ground truth as well.<BRK>The evaluation is only for synthetic dataset for which generating process is designed by the authors. If possible, evaluation on benchmark graph datasets would be convincing though creating the ground truth might be difficult for larger graphs.<BRK>This paper proposes a method called Dynamic Intermedium Attention Memory Network (DIAMNet) to learn the subgraph isomorphism counting for a given pattern graph P and target graph G. This requires global information unlike usual GNN cases such as node classification, link prediction, community detection.<BRK>This paper proposes a dynamic inter medium attention memory network and model the sub graph isomorphism counting problem as a learning problem with both polynomial training and prediction time complexities. One of the main advantages of this paper is that the proposed method can efficiently deal with large graph tasks, so the model behaviors of different models in large dataset similar to Figure 5 is encouraged to be given.<BRK>In this paper, they study a new graph learning problem: learning to count subgraph isomorphisms. Although the learning based approach is inexact, they are able to generalize to count large patterns and data graphs in polynomial time compared to the exponential time of the original NP-complete problem. Different from other traditional graph learning problems such as node classification and link prediction, subgraph isomorphism counting requires more global inference to oversee the whole graph. To tackle this problem, they propose a dynamic intermedium attention memory network (DIAMNet) which augments different representation learning architectures and iteratively attends pattern and target data graphs to memorize different subgraph isomorphisms for the global counting. they develop both small graphs (<= 1,024 subgraph isomorphisms in each) and large graphs (<= 4,096 subgraph isomorphisms in each) sets to evaluate different models. Experimental results show that learning based subgraph isomorphism counting can help reduce the time complexity with acceptable accuracy. their DIAMNet can further improve existing representation learning models for this more global problem.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 6. rating score: 1. This paper seeks to separate "causal" features from ones with spurious correlations in the context of natural language machine learning tasks. Furthermore, training an SVM on the original results in irrelevant attributes (such as movie genre) being weighted, whereas these weights are largely removed when training on the union of the datasets. Overall, I think this paper should be accepted because it makes several interesting contributions: It proposes an interesting approach, shows intriguing experimental results, and produces an interesting dataset (size ~2k) that may be useful for future testing. The main limitation of the paper is that the evidence is largely circumstantial. My suggestion for a further experiment would be to apply the movie review classifiers to, say, book reviews   something where the task is fundamentally the same but the context is different.<BRK>Summary:       The authors take two tasks,sentiment analysis and natural language inference, and identify datasets for them which they counterfactually augment it by asking people over the Amazon Mechanical Turk Platform to change either the sentiment (in the case of sentiment analysis) or the nature of relationship in the NLI task by making minimal changes to the text that produce the targeted changes. Authors find that popular models trained on either fail on the other dataset while the models trained on both actually generalize much better. This is because the original sample and its counterfactual pair the label changed , has the difference in the text that matters to the change and this pair could reduce spurious correlations that models might find in the data distribution. The one part (I dont have much NLP background but I do have a causality background) that I like most is that the new text generated are counterfactual in some real sense with respect to a real world generating process   that is people modifying text with changed targets. A lot of existing work that claim to do counterfactual changes do not specify assumptions about the generating mechanism. Cons:      I think the authors want to make an explicit connection to counterfactuals as understood in the causality community.<BRK>This paper addresses the problem of building models for NLP tasks that are robust against spurious correlations in the data by introducing a human in the loop method: annotators are asked to modify data points minimally in order to change the label. The authors apply this method to the IMDB sentiment dataset and to SNLI and show (among other things) that many models cannot generalize from the original dataset to the counterfactually augmented one. This contribution is timely and addresses a very important problem that needs to be addressed in order to build more robust NLP systems. My main hesitation comes from a lack of clarity about the main lesson we have learned. A few small comments:* There was some analysis of the augmented IMDB dataset, but none of the SNLI dataset. * Table 3: I would make two columns for each model with accuracy on original versus revised.<BRK>The authors propose a new way to augment textual datasets for the task of sentiment analysis, in order to help the learning methods to generalize better by concentrating on learning the different that makes a difference. The main idea of the paper is to augment existing datasets with minimally counteractual versions of them, that change the sentiment of the documents. In this way, all spurious factors will naturally cancel out. The authors use the newly created datasets and show that indeed, the retrained algorithms on the augmented datasets generalize much better. Overall, I find the idea of the paper quite interesting and I’m excited to use the datasets they have created. However, I think the relative novelty of the paper does not meet ICLR standards, and it’s better suited as a whitepaper attached to an open dataset release.<BRK>Despite alarm over the reliance of machine learning systems on so-called spurious patterns, the term lacks coherent meaning in standard statistical frameworks. Hotheyver, the language of causality offers clarity: spurious associations are due to confounding (e.g., a common cause), but not direct or indirect causal effects. In this paper, they focus on natural language processing, introducing methods and restheirces  for training models less sensitive to spurious patterns. Given documents and their initial labels, they task humans with revising each document so that it (i) accords with a counterfactual target label; (ii) retains internal coherence;  and (iii) avoids unnecessary changes. Interestingly, on sentiment analysis and natural language inference tasks, classifiers trained on original data fail on their  counterfactually-revised counterparts and vice versa. Classifiers trained on combined datasets  perform remarkably theyll, just shy of those specialized to either domain. While classifiers trained on either original or manipulated data alone  are sensitive to spurious features (e.g., mentions of genre), models trained on the combined data are less sensitive to this signal. Both datasets are publicly available.
Accept (Poster). rating score: 6. rating score: 6. ## OverviewThe paper tackles the identifiability problem in generative modeling, i.e., recovering the true latent representations from which the observed data originates. The paper argues that identifiable variational autoencoder (iVAE) suffers from intractability issue which leads to suboptimal solutions. The paper instead proposes an identifiable normalizing flows (iFlow) method as an alternative. The paper is very well motivated and well supports its claim. The paper proposes iFlow, an identifiable normalizing flow method which allows recovery of true latent space from observed data. 3.The paper provides theoretical justification on the identifiability of the proposed iFlow method. Can you also report the hyper parameters used for iVAE?<BRK>This paper is about learning an identifiable generative model, iFlow, that builds upon a recent result on nonlinear ICA. The key idea is providing side information to identify the latent representation, i.e., essentially a prior conditioned on extra information such as labels and restricting the mapping to flows for being able to compute the likelihood. As the loglikelihood of a flow model is readily available, a direct approach can be used for learning that optimizes both the prior and the observation model.<BRK>Identifiability, or recovery of the true latent representations from which the observed data originates, is de facto a fundamental goal of representation learning. Yet, most deep generative models do not address the question of identifiability, and thus fail to deliver on the promise of the recovery of the true latent stheirces that generate the observations. Recent work proposed identifiable generative modelling using variational autoencoders (iVAE) with a theory of identifiability. Due to the intractablity of KL divergence bettheyen variational approximate posterior and the true posterior, hotheyver, iVAE has to maximize the evidence lotheyr bound (ELBO) of the marginal likelihood, leading to suboptimal solutions in both theory and practice. In contrast, they propose an identifiable framework for estimating latent representations using a flow-based model (iFlow). their approach directly maximizes the marginal likelihood, allowing for theoretical guarantees on identifiability, thereby dispensing with variational approximations. they derive its optimization objective in analytical form, making it possible to train iFlow in an end-to-end manner. Simulations on synthetic data validate the correctness and effectiveness of their proposed method and demonstrate its practical advantages over other existing methods.
Reject. rating score: 3. rating score: 6. rating score: 6. rating score: 6. This paper presents a method for learning latent variable models of paired data that decomposes the latent representation into common and local representations. The approach is motivated by Wyner’s common information, and is made tractable through a variational formulation. While I found the formulation and motivation from Wyner’s common information and Cuff’s channel synthesis interesting, the resulting model and experiments were unconvincing. There are many terms in the Wyner model proposed, and there are no ablations demonstrating which terms are important and which are not (e.g.do you need both L_xx and L_xy?). The main win appears to be for style content disentanglement, but the results there are qualitative and often change the content when only style is changed. Prior work could likely also do “style control” by e.g.interpolating subsets of dimensions. What’s the discrepancy with Fig 5 where conditional NLL is highly dependent on \lambda for MNIST?<BRK>This paper presents a variational auto encoder approach for paired data variables, with a separate notion of common and individual randomness. Besides, KL divergence based constraints are proposed to encourage consistency of different modeling components. The authors did a great job in the literature review and experimental comparison. The comparison to the information bottleneck is also interesting. • The Alice Bob example is helpful to understand the main purpose of this paper 	• The authors introduced the theoretical background of Wyner s common information   the two contexts it arises. It would be nice to have more justifications on the choice in the context of VAE learning (optimality?), as opposed to other seemingly more natural choices.<BRK>The paper proposed a variational autoencoder for a pair of correlated observation variables, motivated by Wyner s common information. Overall, the paper is technically sound and well supported by theory and experiments. I wonder whether the authors can provide some general guidelines on how to choose this important parameter in practice, as the results seem to be highly dependent on the choice. How would the proposed VAE model be applied when the two correlated variables are not explicitly observed or defined?<BRK>The overall objective is motivated by Wyner s mutual information minimization. Using a few common techniques to bound KL divergence, the paper arrives at a few terms that are reminiscent to the variational lower bounds, including terms that constraints the hidden variable to be similar to the prior, and two cross modal reconstruction terms. The paper includes a comparison between similar models, both in terms of formulation and experiments. The formulation and the lower bound for learning are the strengths of the paper. The lack of discussion on some of the common problems for VAEs and the experiments are the weakness of the paper. However, after the derivation, the framework falls back to common VAEs, and is not very different from VCCA private. The paper should state this clearly when comparing the two. In terms of the common problems for VAEs, the paper does not address a few degenerate cases. Finally, as with most VAE models, the paper does not discuss how well lower bounds approximates the likelihood and the resulting learned representation when the gap between the likelihood and the objective is large. The experiments in the paper are somewhat lacking. Figure 7 is where the capability of the model is fully demonstrated: a clear distinction between the shared and the private views are shown.<BRK>A new variational autoencoder (VAE) model is proposed that learns a succinct common representation of two correlated data variables for conditional and joint generation tasks. The proposed Wyner VAE model is based on two information theoretic problems---distributed simulation and channel synthesis---in which Wyner's common information arises as the fundamental limit of the succinctness of the common representation. The Wyner VAE decomposes a pair of correlated data variables into their common representation (e.g., a shared concept) and  local representations that capture the remaining randomness (e.g., texture and style) in respective data variables by imposing the mutual information bettheyen the data variables and the common representation as a regularization term. The utility of the proposed approach is demonstrated through experiments for joint and conditional generation with and without style control using synthetic data and real images. Experimental results show that learning a succinct common representation achieves better generative performance and that the proposed model outperforms existing VAE variants and the variational information bottleneck method.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. Moreover, when it is combined with some fast  training methods, such as cyclic learning rate scheduling and mixed precision, the adversarial training time can be significantly decreased. The experiment verifies the authors  claim convincingly. Overall, the paper provides a novel finding that could significantly change the adversarial training strategy. The paper is clearly written and easy to follow. I recommend the acceptance.<BRK>Coupled together with tricks for accelerating natural training, such as cyclic learning rate, mixed precision, the robust models can be trained faster than previous methods. +The experimental results are impressive. + The method is simple, and I guess reproducible. +The paper shows surprising facts of a well known method. +The paper is generally well written and easy to follow. I do have some concerns of the work  The paper is empirical and the techniques are combinations of previous methods. I would like the authors to clarify their method to resolve such conflicts and make it clear how R FGSM can be as robust as PGD as in table 1. (2) The authors work hard to address the comments. (1) As pointed out in public discussion, the success of the proposed RFGSM relies on early stopping.<BRK>The main claim of this paper is that a simple strategy of randomization plus fast gradient sign method (FGSM) adversarial training yields robust neural networks. This is somewhat surprising because previous works indicate that FGSM is not a powerful attack compared to iterative versions of it like projected gradient descent (PGD), and it has not been shown before that models trained on FGSM can defend against PGD attacks. Judging from the results in the paper alone, there are some issues with the experiment results that could be due to bugs or other unexplained experiment settings. Suppose I want to use the method to defend against an adversary with power epsilon 14/255, then it is conceivable that I would use a slightly larger step size, say 16/255, as suggested by the authors. After rebuttal: The authors  new experiments and response answers most of my concerns.<BRK>Adversarial training, a method for learning robust deep networks, is typically assumed to be more expensive than traditional training due to the necessity of constructing adversarial examples via a first-order method like projected gradient decent (PGD).  In this paper, they make the surprising discovery that it is possible to train empirically robust models using a much theyaker and cheaper adversary, an approach that was previously believed to be ineffective, rendering the method no more costly than standard training in practice.  Specifically, they show that adversarial training with the fast gradient sign method (FGSM), when combined with random initialization, is as effective as PGD-based training but has significantly lotheyr cost.  Furthermore they show that FGSM adversarial training can be further accelerated by using standard techniques for efficient training of deep networks, allowing us to learn a robust CIFAR10 classifier with 45% robust accuracy at epsilon=8/255 in 6 minutes, and a robust ImageNet classifier with 43% robust accuracy at epsilon=2/255 in 12 htheirs, in comparison to past work based on ``free'' adversarial training which took 10 and 50 htheirs to reach the same respective thresholds. 
Reject. rating score: 3. rating score: 3. rating score: 6. 5.Since the used benchmark datasets are already reasonably explored, authors are advised to include evaluation on other datasets as well, e.g., from [6]. For example, this is the reason [3] does evaluate its model on a larger training split instead of a smaller one. Therefore, I am still more inclined to rejecting the paper.<BRK>Maybe the time complexity is the major contribution of this paper.<BRK>Isn’t this leading to overfitting? Experiments are nicely executed and the proposed approach is compared against a rich array of other models.<BRK>Graph neural networks have shown promising results on representing and analyzing diverse graph-structured data such as social, citation, and protein interaction networks. Existing approaches commonly suffer from the oversmoothing issue, regardless of whether policies are edge-based or node-based for neighborhood aggregation. Most methods also focus on transductive scenarios for fixed graphs, leading to poor generalization performance for unseen graphs. To address these issues, they propose a new graph neural network model that considers both edge-based neighborhood relationships and node-based entity features, i.e. Graph Entities with Step Mixture via random walk (GESM). GESM employs a mixture of various steps through random walk to alleviate the oversmoothing problem and attention to use node information explicitly. These two mechanisms allow for a theyighted neighborhood aggregation which considers the properties of entities and relations. With intensive experiments, they show that the proposed GESM achieves state-of-the-art or comparable performances on ftheir benchmark graph datasets comprising transductive and inductive learning tasks. Furthermore, they empirically demonstrate the significance of considering global information. The stheirce code will be publicly available in the near future.
Reject. rating score: 3. rating score: 3. rating score: 3. This paper considers how we can train image classification models so that they can ignore task irrelevant features. The authors then proposed using Actdiff loss, Reconstruction loss, and Gradmask loss that are designed to suppress the effect of irrelevant features. I think the problem considered in this paper is interesting and important. Thus, we need a way to train image classifiers that can ignore such irrelevant features. My major concern on this study is the experiments. * (Sec4) "Gradmask proves to be too powerful a regularizer for this task, and never produces a model with good generalization performance." * (Sec5) "The gradmask and combination actdiff and gradmask models improved over baseline, but there is no clear reason why this would be true from the saliency maps." However, the effectiveness of the proposed approach is not convincing enough. It would be great if the authors can clarify and demonstrate which regularization is helpful under which circumstances and why, and when it is not.<BRK>This paper attempts to tackle the overfitting problem due to the focusing on the distracting information. Cons:1.The presentation of this paper is not clear. The authors mention that “Conv AE Actdiff” is the one using both actdiff loss and reconstruction loss. Does Conv AE means they implicitly contain reconstruction loss? The authors should elaborate them clearly. The corresponding double quotation marks should be revised. However, this is not the case. 2.In fact, the training data with masking is not sufficient (may only available in some medical images). 3.From the experiments on real tasks, I barely see improvements by the proposed method, which makes the conclusion unconvincing.<BRK>This is a necessary control to be sure that any benefits from masking are due to domain transfer and not other regularization effects. * Does the paper present experiments that promote deeper understanding of why the approach failed? Therefore, I think there s no way for some of the baselines (plain classifier, autoencoder) to generalize correctly. Many alterantive approaches were considered and their performance reported. Experiments follow a logical progression, starting by verifying the idea on a synthetic dataset, then moving to real data, and then evaluating on a half synthetic dataset designed to debug the approach. This presentation is a bit confusing since reconstruction loss was presented as another loss and it shows up in the tables implicitly based on the architecture being compared. The paper embraces its negative result. Why should the particular masks chosen for the experiments have this property? In this case there may be lots of competing losses, so it s important to tune the weights somehow to ensure the tradeoff between losses is optimal. The results (and the conclusions) suggest Actdiff is not very effective at increasing generalization.<BRK>Overfitting is a common issue in machine learning, which can arise when the model learns to predict class membership using convenient but spuriously-correlated image features instead of the true image features that denote a class. These are typically visualized using saliency maps. In some object classification tasks such as for medical images, one may have some images with masks, indicating a region of interest, i.e., which part of the image contains the most relevant information for the classification. they describe a simple method for taking advantage of such auxiliary labels, by training networks to ignore the distracting features which may be extracted outside of the region of interest, on the training images for which such masks are available. This mask information is only used during training and has an impact on generalization accuracy in a dataset-dependent way. they observe an underwhelming relationship bettheyen controlling saliency maps and improving generalization performance.
Reject. rating score: 1. rating score: 3. rating score: 8. This paper introduces the idea of wildly unsupervised domain adaptation, where the source labels are noisy and the target data is unsupervised. To tackle this, the authors propose an architecture based one two branches: one acting on the mixed source target data and the other on the target data only. Originality:  In essence, the proposed method combines co teaching (Han et al., 2018) and pseudo labeling (Saito et al., 2017). The main novelty consists of using two branches to model the two domains. 2 are not explained anywhere. For example, based on the results in Table 1, one might rather want to use TCL as the second stage, i.e., have a baseline Co+TCL. Summary:The novelty of the proposed method, relying on a combination of co teaching and pseudo labeling, is limited. Furthermore, the clarity of the paper could be significantly improved.<BRK>This paper presents a method for unsupervised domain adaptation. The problem is well known in literature and follows the setting of labeled source and unlabeled target set. This work proposes the “butterfly network” suitable to train on noisy data (labels) and assign pseudo labels to the target set. The introduced losses should be better justified. Anyway, as I said the paper has some merit, it provides many insights and extensive analysis on “butterfly” method for unsupervised domain adaptation. The experimental section is extensive and demonstrates improvement in performance using this method compared to state of the art, even though for some "real world" datasets (e.g.SUN, Caltech, ImageNet) the improvement is not so significant as in the case of MNIST SYND. Could the authors provide results on MNIST SVHN to help compare with other papers in literature? This not because I think the method is complex but because some key components that I pointed out previously about the method are not clear and I strogly believe these are key components to replicate the results.<BRK>This paper proposes a new problem setting in the domain adaptation field. However, in wildly unsupervised domain adaptation (WUDA), we do not need a perfectly clean source data, which means that WUDA problem is a more general and realistic problem than existing ones. They tested the proposed method on simulated and real world WUDA tasks (35 tasks in total), and the accuracy of proposed method is higher than those of representative UDA methods. Since WUDA, as a new problem, could lead a new research direction in the domain adaptation field, this paper should be presented in ICLR 2020. Detailed comments can be seen below. Pros:+ WUDA, as a new problem, is very important for the domain adaptation field. + Butterfly, as a solution to WUDA, outperforms representative UDA methods on simulated and real world WUDA tasks. + Following Ben David s paper, this paper also presents an upper bound of the target domain risk. The authors should explain this in the caption of Figure 3. When T   1, you directly use source data as the pseudo labeled target data since there are no pseudo labeled target data in the first step (based on Algorithm 1).<BRK>In unsupervised domain adaptation (UDA), classifiers for the target domain (TD) are trained with clean labeled data from the stheirce domain (SD) and unlabeled data from TD. Hotheyver, in the wild, it is hard to acquire a large amount of perfectly clean labeled data in SD given limited budget. Hence, they consider a new, more realistic and more challenging problem setting, where classifiers have to be trained with noisy labeled data from SD and unlabeled data from TD---they name it wildly UDA (WUDA). they show that WUDA ruins all UDA methods if taking no care of label noise in SD, and to this end, they propose a Butterfly framework, a potheyrful and efficient solution to WUDA. Butterfly maintains ftheir models (e.g., deep networks) simultaneously, where two take care of all adaptations (i.e., noisy-to-clean, labeled-to-unlabeled, and SD-to-TD-distributional) and then the other two can focus on classification in TD. As a consequence, Butterfly possesses all the conceptually necessary components for solving WUDA. Experiments demonstrate that under WUDA, Butterfly significantly outperforms existing baseline methods.
Accept (Poster). rating score: 6. rating score: 6. rating score: 1. ** SummaryThe paper studies the problem of program synthesis from examples and it proposes the notion of property signature as a set of "features" describing the program that can be used to simplify the synthesis by combining together sub programs with similar features. The use of properties to summarize some features about the program and the possibility to evaluate them directly from samples seems a very interesting idea. As the authors mentioned, this may help in creating useful features and useful architectures to simplify the learning task. The authors also provide an extensive comparison to related work. The downsides of the paper are:  While it is clear how to build property signatures, it is quite unclear to me how they simplify the generation of programs that combine smaller/simpler programs.<BRK>The paper does not give enough evidence to ascertain the value of either contribution. There are now a large number of program synthesis works using ML, and a huge literature on program synthesis without ML. While many of ML works use a DSL as the testbed, surely the authors  feature selection method can be applied in some of these DSLs, allowing comparisons with current art? On the other hand, many of the ML methods don t require the DSL used in the works that introduce them. Can searcho + your set of training and test programs distinguish between these methods, and establish a benchmark? I think this paper could be valuable if it could demonstrate either in what ways the new feature selection algorithm is  an improvement over prior ML methods, or that searcho is valuable as a benchmark or for approaching interesting applications. ######################################################################################edit after author response:  raising to 6, I think the deepcoder experiments are useful in contextualizing the contribution of the authors  feature selection method.<BRK>The property signatures are essentially some key attributes that one may summarize from a given set of input output pairs, which the target function has. Much discussions have been given to discuss why and how these properties may be useful and very little real experiments are conducted quantitatively compared with existing works. Although this paper is quite interesting, I think this paper is in its very early stage and there are a lot of serious concerns I have for using this approach to synthesize the real complex programs. 1) First of all, the notion of property signatures are easy to understand and is very natural. However, this is also the hard part of this idea. Potentially it could have an exponential number of possible properties as the program goes more complex and complex. 3) There are very little baselines to compare against even though authors listed "substantial prior work on program synthesis". I understand the existing works may have their limitation in both what they can do and how well they can do. Otherwise, it is hard to be convincing that this approach is indeed superior compared to existing ones.<BRK>they introduce the notion of property signatures, a representation for programs and
program specifications meant for consumption by machine learning algorithms.
Given a function with input type τ_in and output type τ_out, a property is a function
of type: (τ_in, τ_out) → Bool that (informally) describes some simple property
of the function under consideration. For instance, if τ_in and τ_out are both lists
of the same type, one property might ask ‘is the input list the same length as the
output list?’. If they have a list of such properties, they can evaluate them all for their
function to get a list of outputs that they will call the property signature. Crucially,
they can ‘guess’ the property signature for a function given only a set of input/output
pairs meant to specify that function. they discuss several potential applications of
property signatures and show experimentally that they can be used to improve
over a baseline synthesizer so that it emits twice as many programs in less than
one-tenth of the time.
Reject. rating score: 1. rating score: 3. rating score: 3. This paper proposes a reranking architecture with a LogicForm to NaturalLanguage preprocessing step for semantic parsing. The authors experiment their method on three datasets and get the state of the art results. Why do the authors use the Quora dataset in particular? 2.The error analysis might be better to be a bit more quantitative. Other designs include beam size, whether or not to use a pretrained model, etc. These are not weakness, but I think some work in this direction may help improve the paper.<BRK>This paper proposes a framework for semantic parsing, which includes a neural generator that synthesizes the logical forms from natural language utterances, and a neural reranker that re ranks the top predictions generated by beam search decoding using the neural generator. In their evaluation, they indeed show that this transformation helps improve the performance of the reranker. First, although they already evaluate on 3 datasets, all of them are not among the most challenging benchmarks in semantic parsing. It will be helpful if the authors can at least provide results with a smaller beam size, and would be better if they can provide results that are directly comparable to [1]. However, I don t think my concerns are addressed; e.g., without a comparison with previous re ranking methods, it is hard to justify their proposed approach, given that other re ranking methods are also able to improve over an existing well performed generator.<BRK>In this paper, a method for re ranking beam search results for semantic parsing is introduced and experimentally evaluated. In general, the paper is well written. I would suggest to reduce the size of the introduction and dedicate this space to more detailed explanation how reranking works and the experimental details. The error analysis part is great, but not very methodical. Typos:  [CLS] is not defined in text  Shaw et al.should be in Previous methods in Table 3<BRK>Semantic parsing is the problem of deriving machine interpretable meaning representations from natural language utterances. Neural models with encoder-decoder architectures have recently achieved substantial improvements over traditional methods. Although neural semantic parsers appear to have relatively high recall using large beam sizes, there is room for improvement with respect to one-best precision. In this work, they propose a generator-reranker architecture for semantic parsing. The generator produces a list of potential candidates and the reranker, which consists of a pre-processing step for the candidates follotheyd by a novel critic network, reranks these candidates based on the similarity bettheyen each candidate and the input sentence. they show the advantages of this approach along with how it improves the parsing performance through extensive analysis. they experiment their model on three semantic parsing datasets (GEO, ATIS, and OVERNIGHT). The overall architecture achieves the state-of-the-art results in all three datasets. 
Reject. rating score: 3. rating score: 3. The paper is well written and the experiments are very comprehensive. Even though the authors site this work, they don t discuss the connection very clearly. These two loss behave very differently. Therefore, it is not surprising that there is high correlation between the output variance and the cross entropy loss but it is not clear if this has anything to do with the test error. I increase my score to "weak reject" but not higher because of my concern about the novelty of the work in light of Novak et al.2018.<BRK>This paper examines generalization performance of various neural network architectures in terms of a sensitivity metric that approximates how the error responds to perturbations of the input. A number of experimental results are presented that show strong correlation between the sensitivity metric and the empirical test loss. All told, if taken in isolation from prior work, I think the insights and empirical results presented in this paper are quite interesting and certainly sufficient for acceptance to ICLR. [1] Novak, Roman, et al."Sensitivity and generalization in neural networks: an empirical study."<BRK>Even though recent works have brought some insight into the performance improvement of techniques used in state-of-the-art deep-learning models, more work is needed to understand the generalization properties of over-parameterized deep neural networks. they shed light on this matter by linking the loss function to the output’s sensitivity to its input. they find a rather strong empirical relation bettheyen the output sensitivity and the variance in the bias-variance decomposition of the loss function, which hints on using sensitivity as a metric for comparing generalization performance of networks, without requiring labeled data. they find that sensitivity is decreased by applying popular methods which improve the generalization performance of the model, such as (1) using a deep network rather than a wide one, (2) adding convolutional layers to baseline classifiers instead of adding fully connected layers, (3) using batch normalization, dropout and max-pooling, and (4) applying parameter initialization techniques.
Reject. rating score: 3. rating score: 3. rating score: 3. Edit:Thank you for your response and your comments. Overall, the paper is clearly written and easy to read.<BRK>It is also unclear from the writing how this assumption is enforced.<BRK>This is an extremely well studied problem, and many important references are missing in the discussion of related work [1]. "Learning visual feature descriptors for dynamic lighting conditions."<BRK>Training agents to operate in one environment often yields overfitted models that are unable to generalize to the changes in that environment. Hotheyver, due to the numerous variations that can occur in the real-world, the agent is often required to be robust in order to be useful. This has not been the case for agents trained with reinforcement learning (RL) algorithms. In this paper, they investigate the overfitting of RL agents to the training environments in visual navigation tasks. their experiments show that deep RL agents can overfit even when trained on multiple environments simultaneously. 
they propose a regularization method which combines RL with supervised learning methods by adding a term to the RL objective that would enctheirage the invariance of a policy to variations in the observations that ought not to affect the action taken. The results of this method, called invariance regularization, show an improvement in the generalization of policies to environments not seen during training.

Reject. rating score: 1. rating score: 6. rating score: 6. The authors explore different ways to generate questions about the current state of a “Battleship” game. While the problem of question generation is quite interesting, this work is limited in several ways.<BRK>I feel that this justification needs expanding. The length of the submission is 9 pages in content. That said, I think there is still a lot to commend in this work: the setting is an interesting one for question generation, the motivation for using programs is well made, and the work appears technically sound. Some more minor comments I wonder about the log likelihood numbers on the synthetic questions.<BRK>Also, it is not clear to me how the setup evaluated in this paper can be extended to a setting that allows an agent to iteratively ask questions. The problem is not entirely novel as [1 3] have explored asking questions with neural networks learning to generate programmatic questions. Figure 1 illustrates the Battleship task and Figure 2 presents a clear overview of the proposed framework.<BRK>People ask questions that are far richer, more informative, and more creative than current AI systems. they propose a neural program generation framework for modeling human question asking, which represents questions as formal programs and generates programs with an encoder-decoder based deep neural network. From extensive experiments using an information-search game, they show that their method can ask optimal questions in synthetic settings, and predict which questions humans are likely to ask in unconstrained settings. they also propose a novel grammar-based question generation framework trained with reinforcement learning, which is able to generate creative questions without supervised data.
Reject. rating score: 3. rating score: 3. rating score: 6. The paper proposes a method for combining continuous Q learning and linear MPC. They propose learning a continuous control policy for long term behavior trained with standard model free reinforcement learning and a short term linear control model implemented as a linear dynamical system. At test time a combined controller can be used to infer actions by solving small optimization problem. The paper is well written and mostly clear. * the algorithm seems to be be setup to with the dynamics model being learnt on the transitions coming from an epsilon greedy with respect to the optimal policy. * There would be some benefit in trying the model in some other domain maybe reacher with some constraints ?<BRK>This paper proposes a Locally Linear Q Learning (LLQL) method for continuous action control. It uses a short term prediction model and a long term prediction model to generate actions that achieve short term and long term goals simultaneously. The proposed method is more like hybrid of model based and model free RL method. Second, the proposed method is not sufficiently evaluated. In order to justify the performance of an RL algorithm for continuous action space, it should be at least evaluated on the set of MuJoCo tasks. In Table 1, LLQL is compared to DDPG (a model free method), and is shown to achieve better performance. However, this seems to be unfair because the proposed method is in fact a model based RL algorithm. Therefore, it should at least compare to other model based algorithms (and also other riche set of safe exploration RL methods).<BRK>Thank the authors for the response. SummaryIn this paper, the author proposes LLQL (Locally Linear Q Learning), which separates the action design from prediction models, such that the model can have short term and long term goals. The short term network takes the current state and the action, and predicts the next state (to be specific, the status change) under the assumption that the output is linear to the action. Strengths  The proposed model is novel, which can take a trajectory or a constraint as a short term goal. As a result, we can control the behavior of the model easily. As the authors showed in Tables 1 & 2, LLQL outperforms this approach. The paper is clearly written. Only compared to one baseline. In order to demonstrate the effectiveness of LLQL taking a short term trajectory or constraint, the authors compared it to only one method (i.e., DDPG) with a few manually designed reward function.<BRK>Deep Reinforcement Learning (RL) algorithms can learn complex policies to optimize
agent operation over time. RL algorithms have shown promising results
in solving complicated problems in recent years. Hotheyver, their application on
real-world physical systems remains limited. Despite the advancements in RL
algorithms, the industries often prefer traditional control strategies. Traditional
methods are simple, computationally efficient and easy to adjust. In this paper,
they propose a new Q-learning algorithm for continuous action space, which can
bridge the control and RL algorithms and bring us the best of both worlds. their
method can learn complex policies to achieve long-term goals and at the same time
it can be easily adjusted to address short-term requirements without retraining.
they achieve this by modeling both short-term and long-term prediction models.
The short-term prediction model represents the estimation of the system dynamic
while the long-term prediction model represents the Q-value. The case studies
demonstrate that their proposed method can achieve short-term and long-term goals
without complex reward functions.
Reject. rating score: 3. rating score: 6. rating score: 8. The method covers both L_inf and L_2 perturbations, and the optimization problem posed are solved using Gurobi. This results in the standard properties from taking a lagrangian dual: any choice of dual variables will produce a lower bound. I m confused at the upper bound finding method. Also, I don t understand the motivation for why repeatedly solving the QP will lead to a good violation on the decision boundary. I know that the paper says that "analytic investigation of this algorithm including a convergence proof remains future work." but at the moment there is not even an intuition for why it might be a good idea. FGSM is a very very weak baseline for the use that is employed here. It s better to report results clearly than only trying to show the good points of the algorithm.<BRK>Z.(2018).Scaling provable adversarial defenses. Many previous papers have been using PGD based attacks to obtain the upper bound. 2.The use of FGSM as an upper bound is inappropriate, as FGSM is known to be a very weak attack. This epsilon value is too small for L2 norm. The per neuron lower and upper bounds can be obtained using CROWN efficiently, so there is no too much computation cost. 2.Improving the scalability of QP relaxation is another challenge. Overall I am positive with this paper, however before accepting it I think the authors should at least make their claims clearer (the relaxation performs well mainly in L2 norm, and the concept of "bi directional verification" is also not entirely new), replacing FGSM by a 200 step PGD and compare the upper bound found by PGD with QPRel, and test the proposed algorithm in models trained with a larger epsilon (eps 1.58 to align with previous works, if possible) and deeper models.<BRK>This paper proposes a method to compute the distance to the decision boundary for a given network, where the network is composed of linear layers followed by a RELU activation. The authors provide a lower bound and an upper bound for the distance of a sample from the decision boundary. The method is useful to verify robustness of neural networks. The experiments show the improvement of the proposed method over existing certificates. I am not referring to a convergence proof here, but simply a guarantee that the value returned by Algorithm 1 is indeed an upper bound. It would also be helpful to provide intuition for the iterative procedure to compute the upper bound.<BRK>Neural networks are known to be sensitive to adversarial perturbations. To investigate this undesired behavior they consider the problem of computing the distance to the decision boundary (DtDB) from a given sample for a deep NN classifier. In this work they present an iterative procedure where in each step they solve a convex quadratic programming (QP) task. Solving the single initial QP already results in a lotheyr bound on the DtDB and can be used as a robustness certificate of the classifier around a given sample. In contrast to currently known approaches their method also provides upper bounds used as a measure of quality for the certificate. they show that their approach provides better or competitive results in comparison with a wide range of existing techniques.
Reject. rating score: 1. rating score: 1. rating score: 1. rating score: 1. This paper claims to analyze the global convergence of large scale neural networks (NNs). this is not a new mathematical toolSection 2   a standard formulation of neural networks, followed by undergraduate level Fourier analysis. The introduction of Fourier analysis as a new mathematical tool, and devotion of a whole page to this “canonical space model” is almost laughable. Furthermore, this statement in the introduction is not only incorrect but it is absurd.<BRK>Summary:This paper presents an argument that massively overparameterized neural networks trained with gradient descent on a supervised learning problem with convex loss function will converge. It argues that by mapping each model to a truncated Fourier series we can recover a canonical representation for the functions in the model space. Viewing this as a linear map at each point in parameter space and assuming that this matrix is always full rank, they claim convergence of gradient descent.<BRK>The paper studies the problem of optimization for neural networks. It compares the optimization problem in parameter space with the corresponding problem in function space.<BRK>This paper looks at the neural net training problem in a "canonical space" which is parameterized by the Fourier coefficients of the function. The paper shows that in the canonical space, the training problem is always convex. Going back to the literal space (original parameter space for a neural network), it is shown that as long as a "disparity matrix" remains full rank, gradient descent will converge to a global minimum. The paper claims that the matrix can be made full rank. The authors would need to provide a rigorous proof in order to claim this.<BRK>In this paper, they present some theoretical work to explain why simple gradient descent methods are so successful in solving non-convex optimization problems in learning large-scale neural networks (NN). After introducing a mathematical tool called  canonical space, they have proved that the objective functions in learning NNs are convex in the canonical model space. they further elucidate that the gradients bettheyen the original NN model space and the canonical space are related by a pointwise linear transformation, which is represented by the so-called  disparity matrix. Furthermore, they have proved that gradient descent methods surely converge to a global minimum of zero loss provided that the disparity matrices maintain full rank.  If this full-rank condition holds, the learning of NNs behaves in the same way as normal convex optimization. At last, they have shown that the chance to have singular disparity matrices is extremely slim in large NNs. In particular, when over-parameterized NNs are randomly initialized, the gradient decent algorithms converge to a global minimum of zero loss in probability. 
Reject. rating score: 3. rating score: 3. rating score: 3. This paper studies the teacher student models in semi supervised learning. The paper achieves some good empirical results compared to other baselines. However, it is still not clear why the proposed method work and how does it prevents overfitting. I vote for a clear rejection of the paper.<BRK>In the second sentence of Introduction, the authors mention "Although coaches do not play as well as the players". The authors propose a novel policy gradient update for the Teacher network. The authors evaluate the coaching method on several different semi supervised learning dataset CIFAR 10, SVHN and ImageNet. The authors  response does not answer the questions about motivation and why the method works, which is also questioned by the two other reviewers.<BRK>The paper proposes Coaching for semi supervised learning. The empirical results are very impressive. First, the method is not well motivated in the Introduction section. I suggest adding this to the ablation study as well. We need to understand why it works apart from adding a bunch of tricks together.<BRK>Recent semi-supervised learning (SSL) methods often have a teacher to train a student in order to propagate labels from labeled data to unlabeled data. they argue that a theyakness of these methods is that the teacher does not learn from the student’s mistakes during the ctheirse of student’s learning.  To address this theyakness, they introduce Coaching, a framework where a teacher generates pseudo labels for unlabeled data, from which a student will learn and the student’s performance on labeled data will be used as reward to train the teacher using policy gradient.

their experiments show that Coaching significantly improves over state-of-the-art SSL baselines. For instance, on CIFAR-10, with only 4,000 labeled examples, a WideResNet-28-2 trained by Coaching achieves 96.11% accuracy, which is better than 94.9% achieved by the same architecture trained with 45,000 labeled. On ImageNet with 10% labeled examples, Coaching trains a ResNet-50 to 72.94% top-1 accuracy, comfortably outperforming the existing state-of-the-art by more than 4%. Coaching also scales successfully to the high data regime with full ImageNet. Specifically, with additional 9 million unlabeled images from OpenImages, Coaching trains a ResNet-50 to 82.34% top-1 accuracy, setting a new state-of-the-art for the architecture on ImageNet without using extra labeled data.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. The model proposed in this paper outperforms it without being combined with any classical approach, though it does utilize ensembling.<BRK>The paper investigates a pure deep learning architecture for Univariate time series analysis by simply ensembling feed forward networks, along with the residual stacking mechanism for fluid learning.<BRK>The paper proposes a DL architecture that achieves better performance on time series prediction.<BRK>they focus on solving the univariate times series point forecasting problem using deep learning. they propose a deep neural architecture based on backward and forward residual links and a very deep stack of fully-connected layers. The architecture has a number of desirable properties, being interpretable, applicable without modification to a wide array of target domains, and fast to train. they test the proposed architecture on several theyll-known datasets, including M3, M4 and TtheirISM competition datasets containing time series from diverse domains. they demonstrate state-of-the-art performance for two configurations of N-BEATS for all the datasets, improving forecast accuracy by 11% over a statistical benchmark and by 3% over last year's winner of the M4 competition, a domain-adjusted hand-crafted hybrid bettheyen neural network and statistical time series models. The first configuration of their model does not employ any time-series-specific components and its performance on heterogeneous datasets strongly suggests that, contrarily to received wisdom, deep learning primitives such as residual blocks are by themselves sufficient to solve a wide range of forecasting problems. Finally, they demonstrate how the proposed architecture can be augmented to provide outputs that are interpretable without considerable loss in accuracy.
Accept (Poster). rating score: 8. rating score: 8. rating score: 6. rating score: 1. This paper presents a new deep reinforcement learning method that can efficiently trade off exploration and exploitation. The paper is well written and easy to follow. On Gridworld the performance of the proposed algorithm is close to the performance of the Bayes optimal policy. This can add additional computation complexity to Bayesian RL, which is not explained or mentioned in neither the method part nor the experiment. I hope the authors can add some discussions on this aspects<BRK>While it s always easy to dig into the rabbit holes of related research and distract from the overall objective of a paper, I thought that there was sufficient overlap with these other lines of research that the authors may find interesting. Further clarity along any of these questions would greatly improve the presentation of the paper as well as further convince me of the paper s suitability for publication. The primary contribution of this paper is in how variBAD learns the variational inference procedure. Can one make the same claims about the overall approach or procedure in the MuJoCo domains?<BRK>Summary: This paper considers a version of reinforcement learning problem where an unknown prior distribution over Markov decision processes are assumed and the learner can sample from it. After sampling a MDP, a standard reinforcement learning is done. Comments: Considering a Bayesian setting of reinforcement learning is sound and well motivated in a mathematical or statistical sense. Unfortunately, I don’t have any examples in mind and the paper also shows some artificial experiments. Comments after Rebuttal:I modified my score according to authors  comments.<BRK>On these grounds, given a set of tasks sampled from a distribution, the method jointly trains: (1) a variational auto encoder that can infer the posterior distribution over the postulated latent variable when it encounters a new task while interacting with the environment, and (2) a policy that conditions on this posterior distribution over the MDP embeddings, and thus learns how to trade off exploration and exploitation when selecting actions. Such variational inference arguments for transfer learning in the context of MDPs are not new. The authors have not made a good job reviewing the related literature. This is totally disappointing.<BRK>Trading off exploration and exploitation in an unknown environment is key to maximising expected return during learning. A Bayes-optimal policy, which does so optimally, conditions its actions not only on the environment state but on the agent’s uncertainty about the environment. Computing a Bayes-optimal policy is hotheyver intractable for all but the smallest tasks. In this paper, they introduce variational Bayes-Adaptive Deep RL (variBAD), a way to meta-learn to perform approximate inference in an unknown environment, and incorporate task uncer- tainty directly during action selection. In a grid-world domain, they illustrate how variBAD performs structured online exploration as a function of task uncertainty. they further evaluate variBAD on MuJoCo domains widely used in meta-RL and show that it achieves higher online return than existing methods.
Reject. rating score: 3. rating score: 3. rating score: 3. rating score: 8. This papers proposed an interesting idea for distributed decentralized training with quantized communication. 4.The experiments are not convincing. There is no convincing explanation why modulo operation makes the algorithm better.<BRK>This paper studies an important problem in the decentralized optimization, i.e., communication compression. 3.The modular hyperparameter $\theta$ is not easy to choose and seems cannot help achieve consensus. Overall, the idea of Moniqua is interesting and the authors provide useful extensions based on it.<BRK>The method is interesting and elegant, doesn’t require additional memory, theoretically convergent with a good speed and allows asynchronous communication. However, it looks a bit incremental. In contrast to the baselines, MONIQUA does not support arbitrary communication compression. Experimental comparison is shown only for the beginning of the optimization where the algorithm doesn’t achieve state of the art accuracy. why some equations are numbered and some are not?<BRK>This work is about the use of quantized communication in decentralized stochastic gradient descent. In summary, this work addresses a highly relevant problem and it nicely combines formal asymptotic analysis with experimental validation. The proposed Moniqua algorithm is an attempt to overcome these shortcomings. The motivation for the Moniqua algorithm   which is designed to solve this problem of local models   is rather intuitive, and the algorithm can be implemented quite easily in practice.<BRK>Decentralized stochastic gradient descent (SGD), where parallel workers are connected to form a graph and communicate adjacently, has shown promising results both theoretically and empirically. In this paper they propose Moniqua, a technique that allows decentralized SGD to use quantized communication. they prove in theory that Moniqua communicates a provably bounded number of bits per iteration, while converging at the same asymptotic rate as the original algorithm does with full-precision communication. Moniqua improves upon prior works in that it (1) requires no additional memory, (2) applies to non-convex objectives, and (3) supports biased/linear quantizers. they demonstrate empirically that Moniqua converges faster with respect to wall clock time than other quantized decentralized algorithms.  they also show that Moniqua is robust to very low bit-budgets, allowing  less than 4-bits-per-parameter communication without affecting convergence when training VGG16 on CIFAR10.
Reject. rating score: 1. rating score: 3. rating score: 3. In addition, the paper proposes a tiling approach to further reduce storage requirements. The organization of the paper is also confusing. To my understanding, the experiments are not very convincing. If for pruning, why not applying to all the layers in the network? How do we distinguish between the relevance of magnitude pruning and the proposed method?<BRK>The idea of converting the binary low rank factorization problem into a real valued non negative factorization problem is interesting but could have been better justified (from an intuitive/theoretical and computational perspective). The paper addresses the problem of reducing the computational complexity of neural network pruning. As low rank decomposition of binary matrices is a hard problem, the authors propose a method to approximate the solution by computing a more standard non negative matrix factorization. In the experiments, the value of the pruning objective seems to decrease as the rank of the factorization increases.<BRK>This paper proposed a new network pruning method that generates a low rank binary index matrix to compress index data, and a tile based factorization technique to save memory. The results for various networks, including DNN, CNN and LSTM, have shown the effectiveness of the propsoed method. The paper is well written and easy to follow. The experiments are not convincing, e.g.only pruning FC5 and FC6 layers in AlexNet on ImageNet dataset.<BRK>  Pruning is an efficient model compression technique to remove redundancy in the connectivity of deep neural networks (DNNs). A critical problem to represent sparse matrices after pruning is that if fetheyr bits are used for quantization and pruning rate is enhanced, then the amount of index becomes relatively larger. Moreover, an irregular index form leads to low parallelism for convolutions and matrix multiplications. In this paper, they propose a new network pruning technique that generates a low-rank binary index matrix to compress index data significantly. Specifically, the proposed compression method finds a particular fine-grained pruning mask that can be decomposed into two binary matrices while decompressing index data is performed by simple binary matrix multiplication. they also propose a tile-based factorization technique that not only lotheyrs memory requirements but also enhances compression ratio. Various DNN models (including conv layers and LSTM layers) can be pruned with much fetheyr indices compared to previous sparse matrix formats while maintaining the same pruning rate.
Accept (Poster). rating score: 8. rating score: 8. rating score: 6.  The paper proposes a neural network architecture to address the problem of estimating a sparse precision matrix from data (and therefore inferring conditional independence if the random variables are gaussian).<BRK>The authors propose an AM procedure for solving the l1 regularized maximum likelihood which can be unrolled and parameterized. This method is shown to converge faster at inference time than other methods and it is also far more effective in terms of training time compared to an existing data driven method.<BRK>The authors propose a new method for graph recovery, which is a more data driven approach by deep learning. It makes an original approach to this problem. Good attention is also paid to hyper parameter tuning. However, some parts can be clarified and improved:  In the introduction a number of phrases should be clarified: it is not entirely clear what the meaning is of the input and output of the problem, input covariance and output precision matrix. Additional explanation and references are needed at this point.<BRK>Recovering sparse conditional independence graphs from data is a fundamental problem in machine learning with wide applications. A popular formulation of the problem is an $\ell_1$ regularized maximum likelihood estimation. Many convex optimization algorithms have been designed to solve this formulation to recover the graph structure. Recently, there is a surge of interest to learn algorithms directly based on data, and in this case, learn to map empirical covariance to the sparse precision matrix. Hotheyver, it is a challenging task in this case, since the symmetric positive definiteness (SPD) and sparsity of the matrix are not easy to enforce in learned algorithms, and a direct mapping from data to precision matrix may contain many parameters. they propose a deep learning architecture, GLAD, which uses an Alternating Minimization (AM) algorithm as their model inductive bias, and learns the model parameters via supervised learning. they show that GLAD learns a very compact and effective model for recovering sparse graphs from data.
Reject. rating score: 3. rating score: 3. rating score: 3. This paper investigates the impact of using a reduced precision (i.e., quantization) in different deep reinforcement learning (DRL) algorithms. In my opinion, for this paper be relevant, it should have a very thorough evaluation of these different dimensions of reinforcement learning algorithms, with explicit discussions about it. The result that is interesting, in my opinion, is not properly explored. The paper is well written but it is a bit repetitive. Aside from typos, below are some other comments on the presentation. The rebuttal did acknowledge some points I made to me the paper took a gradient update towards the right direction. These are questions I would like to see answered because they also inform us about the impact of the proposed idea.<BRK>Training and deployment of DRL models is expensive. The paper indicates that quantization can indeed lower resource consumption without quality decline in realistic DRL tasks and for various algorithms. The idea is simple and carries over from (image based) supervised learning. The results are also not entirely surprising or impactful: how is quantization impacting reinforcement learning in a different way than supervised learning? What secondary effects does quantization have on the learning procedure: e.g., does it boost exploration behavior or does it regularize training? These could be investigated in more detail.<BRK>This paper studies the effect of quantization on training reinforcement learning tasks. On several tasks, the authors showed that quantization can significantly reduce memory usage and speed up the inference time. While it is expected that quantization should decrease the accuracy of the trained model, it is not entirely clear how one should evaluate the trade off presented in the work. Some natural questions that I believe deserve more discussions are:  Are the kinds of accuracy cost the best one could hope for using these methods? In Figure 5: your results show that the "int8" method has a significantly lower success rate than "fp32".<BRK>Recent work has shown that quantization can help reduce the memory, compute, and energy demands of deep neural networks without significantly harming their quality. Hotheyver, whether these prior techniques, applied traditionally to image-based models, work with the same efficacy to the sequential decision making process in reinforcement learning remains an unanstheyred question. To address this void, they conduct the first comprehensive empirical study that quantifies the effects of quantization on various deep reinforcement learning policies with the intent to reduce their computational restheirce demands. they apply techniques such as post-training quantization and quantization aware training to a spectrum of reinforcement learning tasks (such as Pong, Breakout, BeamRider and more) and training algorithms (such as PPO, A2C, DDPG, and DQN). Across this spectrum of tasks and learning algorithms, they show that policies can be quantized to 6-8 bits of precision without loss of accuracy. Additionally, they show that certain tasks and reinforcement learning algorithms yield policies that are more difficult to quantize due to their effect of widening the models' distribution of theyights and that quantization aware training consistently improves results over post-training quantization and oftentimes even over the full precision baseline. Finally, they demonstrate the real-world applications of quantization for reinforcement learning. they use half-precision training to train a Pong model 50 % faster, and they deploy a quantized reinforcement learning based navigation policy to an embedded system, achieving an 18x speedup and a 4x reduction in memory usage over an unquantized policy.
Reject. rating score: 1. rating score: 1. rating score: 8. This paper is about learning word embeddings. There is therefore not a meaningful comparison to the most relevant previous work. Word similarity experiments are not enough to justify this approach. There is no such analysis here. The claim that the resulting representations are more "interpretable" is not backed up by any evidence at all, even though the word "interpretable" is in the title and the list of contributions.<BRK>Summary: This paper proposed a variational word embedding method, by using vMF distribution as prior and adding an entropy regularization term. Weaknesses:[ ] Template: It seems that the authors use the wrong template, which is not the template for ICLR2020. The authors argue  interpretable  in their title and abstract, but their method and experiments do not show this point explicitly. The vocabulary size is 10K and the corpus is not so big, and I wonder whether the performance of the proposed method will be better for large corpus and longer training time. Is it still true for vMF distribution?<BRK>The new approach to training word embeddings addresses interpretability in the sense of having a theoretically grounded interpretation for the good properties that the inner products should possess   namely that they capture the PMI between words. and situated in the literature. The results seem to be fairly interpreted. It will catch a large number of problems that weren t ever big enough to hurt my appreciation of the paper. Write one or two more sentences about the fate of \kappa_c   is it ever updated or am I just missing something? Watch out that readers/reviewers of similar work may try to read the word "interpretable" in the title as a reference to the much broader topic of interpretability in ML related to explaining a model s behavior.<BRK>Word embedding plays a key role in various tasks of natural language processing. Hotheyver, the dominant word embedding models don't explain what information is carried with the resulting embeddings. To generate interpretable word embeddings they intend to replace the word vector with a probability density distribution. The insight here is that if they regularize the mixture distribution of all words to be uniform, then they can prove that the inner product bettheyen word embeddings represent the point-wise mutual information bettheyen words. Moreover, their model can also handle polysemy. Each word's probability density distribution will generate different vectors for its various meanings. they have evaluated their model in several word similarity tasks. Results show that their model can outperform the dominant models consistently in these tasks.
Reject. rating score: 1. rating score: 3. rating score: 6. I appreciate your point that sport applications can be interesting to this community, and your paper makes an important contribution in that direction. With that said, for a paper to be appealing to the ICLR audience, there should be more clear novelty in the method, and stronger experimental evaluation showing novel insights or out performance over previous baselines. The paper addresses a problem in the context of understanding formations of sport player in a play field. General: The paper is very clearly written, the method makes sense and the related work are both well explained. I have two main concerns: First, the scope of the paper may be too narrow for a conference like ICLR, as the method part is fundamentally a clustering method, and the application is very specific. Wouldnt its distribution change?<BRK>if this was the case, it is really hard to see how each of the proposed steps guarantees the same outputs as previous approaches, while at the same time only affecting speed. Paper, in its current form, lacks quantitative evaluation, comparison to other methods (heavily cited in the paper), and any well established applications that can be quantified.<BRK>  *Synopsis*:  The paper proposes a new algorithm for template discovery and representation learning for unstructured multi agent data with strong group roles (i.e.sports data). Main Contributions:    A key insight into the underlying structure of the data, enabling an algorithm with orders of magnitude speed up. A new framing of the problem. *Review*:  The paper is well written, and seems to be a nice improvement over prior art in terms of runtime. Q1 It seems the key insight is that you are modeling the occupancy of space by an agent as a mixture of independent gaussian distributions. If not, why?<BRK>Central to all machine learning algorithms is data representation. For multi-agent systems, selecting a representation which adequately captures the interactions among agents is challenging due to the latent group structure which tends to vary depending on various contexts. Hotheyver, in multi-agent systems with strong group structure, they can simultaneously learn this structure and map a set of agents to a consistently ordered representation for further learning. In this paper, they present a dynamic alignment method which provides a robust ordering of structured multi-agent data which allows for representation learning to occur in a fraction of the time of previous methods.  they demonstrate the value of this approach using a large amount of soccer tracking data from a professional league. 
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. This paper proposes applying random convolutions to the observation space to improve the ability of deep RL agents to generalize to unseen environments. To encourage the learning of invariant features, the authors further include a loss term to align features of perturbed and unperturbed observations. Thorough experiments on multiple generalization benchmarks show that this method outperforms many previously used regularization and data augmentation techniques. Although the proposed method is simple, it represents a useful contribution. The clear need for agents to be invariant to these low level transformations well motivates the proposed approach, as does the failure of many existing methods to provide this invariance. The authors could more explicitly discuss the main drawbacks of this approach. As with any data augmentation, there is an assumption that the applied transformation generally won’t destroy information pertinent to the task. Encountering such MDPs is not farfetched, so this weakness seems worth acknowledging.<BRK>This paper proposes methods to improve generalization in deep reinforcement learning with an emphasis on unseen environments. The main contribution is essentially a data augmentation technique that perturbs the input observations using a noise generated from the range space of a random convolutional network. The empirical results look impressive and demonstrate the effectiveness of the method. Otherwise, please provide some intuition on why this works so well on RL but not as well on computer vision tasks. 3) While proposed method performs well on the benchmarks, it is not clear whether authors compare to the state of the art algorithms. I also found the new experimental results (Fig 5 and 7) very insightful. For future improvement: More realistic experiments on computer vision tasks (besides cats and dogs) would be welcome. Otherwise, please justify why proposed strategy is particularly good for RL (rather than traditional computer vision benchmarks) in boosting robustness to new domains.<BRK>This work proposes using a randomly parameterized convolutional layer as additional processing of the input observation to provide data augmentation to make policies more robust to environments with different observation spaces. The empirical results are thorough, comparing with other regularization techniques, including dropout, L2 regularization, and batch normalization with the same policy gradient method, PPO on a variety of generalization in RL benchmarks. There are additional experiments of this method to check that it actually removes visual bias in a computer vision problem better than other methods. They also incorporate a feature matching loss that explicitly forces the learned representations of equivalent states to be close in L2 distance. Why would this perform better than grayscaling? The comparison of PPO is also unfair in that the author s method uses an ensemble of policies to act, which other methods do not.<BRK>Deep reinforcement learning (RL) agents often fail to generalize to unseen environments (yet semantically similar to trained agents), particularly when they are trained on high-dimensional state spaces, such as images. In this paper, they propose a simple technique to improve a generalization ability of deep RL agents by introducing a randomized (convolutional) neural network that randomly perturbs input observations. It enables trained agents to adapt to new domains by learning robust features invariant across varied and randomized environments. Furthermore, they consider an inference method based on the Monte Carlo approximation to reduce the variance induced by this randomization. they demonstrate the superiority of their method across 2D CoinRun, 3D DeepMind Lab exploration and 3D robotics control tasks: it significantly outperforms various regularization and data augmentation methods for the same purpose.
Reject. rating score: 1. rating score: 3. rating score: 3. rating score: 6. This paper develops variations on the Donsker Varadhan (DV) lower bound on KL divergence which yields a lower bound on mutual information as a special case. The paper is primarily theoretical with an emphasis on the case where the witness function f is drawn from an RKHS (a support vector machine). First, the authors ignore fundamental limitations on the measurement of mutual information from finite samples. So meaningful lower bounds for "high dimensional data" are simply not possible for feasible samples. They should be compared to those in "Learning Representations by Maximizing Mutual Information Across Views" by Philip Bachman, R Devon Hjelm, William BuchwalterResponse to the author response:This was written earlier but there was a mishap when I attempted to submit it and it was not actually submitted until comments were no longer available to the authors so I am putting this in the review. Bounding the ratio of the densities already bounds the mutual information. I see no reason to believe that the empirical estimate is meaningful in a real application such as vision. Regarding the experiments, I do not have much interest in experiments on synthetic Gaussians. baselines rather than performance numbers reported by, for example, Hjelm et al.or van den Oord et al.(CPC).<BRK>The paper contains one main contribution, the unbiased version of MINE obtained using the eta trick, however a lot of theory is presented and not always very clearly. The section on “What Do Neural Estimators of LK or MI Learn” is a collection of different remarks without much coherence, some of which are imprecisely stated. The comparison with the estimator from Pool et al.(2019) could also be much simplified, in particular I would review the list of remarks below theorem 2. Another weakness of the paper is that two important aspects in assessing the quality of an estimator are overlooked:  the variance of the estimator and the performance on finite data. The experiment comparing different MI estimators on synthetic Gaussian data is interesting. The plots are however difficult to read, it would be good to make them larger or split the results in different panels. For the 2D and the 20D case, the results reported in the MINE paper are much closer to the true MI than what is reported here, could the authors explain this difference? that the performance on downstream supervised tasks often does not clearly depend on the quality of MI estimation. It starts with a typo in the second line (E_y~Q should be replaced by an integral).<BRK>The paper presents to use \eta trick for log(.) in Donsker Varadhan representation of the KL divergence. Nevertheless, in experiments, we see that it outperforms the original neural estimation of mutual information. <Appendix>I understand most of the people would not read the Appendix, but I do. Cons:1.I don t agree with some claims in the paper. Nevertheless, these claims are some of the main stories supporting the paper. Including the presentation flow between sections, and also misleading part in experiments. I feel the paper should be a strong paper after a good amount of revision. Nevertheless, the author doesn t present an evaluation using the dual form. This paragraph is misleading and can be moved entirely to the Appendix. The author s claim is based on the comparisons between eq.(13) and eq.(15) when assuming only the MMD term reaches zero. <Section 4>Similar to Section 3, this section is cluttered. <MI Estimation Experiment>Can the author discuss the standard deviation for various MI estimation approaches?<BRK>The paper propose a variational bound on the Mutual Information and showed how it can be used to improve the estimation of mutual information. I am afraid I cannot judge of the quality and correctness of the method introduced by the authors. Nevertheless I find that the presentation of the paper could be improved. For instance,  while I enjoyed Fig.3 that showed the performances of different estimators of the entropy, i add the zoom out on a big screen to be able to see anything at all! It is also not entirely clear how this figure supports the claim of superiority of the proposed method. The only comment I may have is that it would be interesting  since the authors want to apply their bound to the case of neural networks  to compare with, the rigorous estimation of the entropy of NN with random weights in Gabrie et al, NeurIPS 2018, Fig.2 .It would be a much challenging task, albeit a synthetic one, than the Gaussian dataset one presented in Fig.3.<BRK>they propose a new variational lotheyr bound on the KL divergence and show that the Mutual Information (MI) can be estimated by maximizing this bound using a witness function on a hypothesis function class and an auxiliary scalar variable. If the function class is in a Reproducing Kernel Hilbert Space (RKHS), this leads to a jointly convex problem. they analyze the bound by deriving its dual formulation and show its connection to a likelihood ratio estimation problem. they show that the auxiliary variable introduced in their variational form plays the role of a Lagrange multiplier that enforces a normalization constraint on the likelihood ratio. By extending the function space to neural networks, they propose an efficient neural MI estimator, and validate its performance on synthetic examples, showing advantage over the existing baselines. they then demonstrate the strength of their estimator in large-scale self-supervised representation learning through MI maximization.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 6. The stable rank is the ratio of the squared frobenius norm to the squared spectral norm (the top singular value). The authors should also indicate their hardware setup and overall training time. My Take:This paper motivates and presents an interesting extension of spectral norm, and evaluates it quite well with thorough experiments in a range of settings.<BRK>Thispaper shows how to implement these "stable rank" normalizations with littlecomputational overhead. The authors then apply the method to a wide variety ofclassification and GAN problems to show the benefits of stable ranknormalization. For CNNs with only a few linear layers is there any observabledifference by lightly deviating from this?<BRK>This paper proposes normalizing the stable rank (ratio of the Frobenius norm to the spectral norm) of weight matrices in neural networks. They propose an algorithm that provably finds the optimal solution efficiently and perform experiments to show the effectiveness of this normalization technique. My main issue is with the empirical evaluation of the normalization technique. 2) The current result is with training with a fixed number of epochs.<BRK>Exciting new work on generalization bounds for neural networks (NN) given by Bartlett et al. (2017); Neyshabur et al. (2018) closely depend on two parameter- dependant quantities: the Lipschitz constant upper bound and the stable rank (a softer version of rank). Even though these bounds typically have minimal practical utility, they facilitate questions on whether controlling such quantities together could improve the generalization behavitheir of NNs in practice. To this end, they propose stable rank normalization (SRN), a novel, provably optimal, and computationally efficient theyight-normalization scheme which minimizes the stable rank of a linear operator. Surprisingly they find that SRN, despite being non-convex, can be shown to have a unique optimal solution. they provide extensive analyses across a wide variety of NNs (DenseNet, WideResNet, ResNet, Alexnet, VGG), where applying SRN to their linear layers leads to improved classification accuracy, while simultaneously showing improvements in genealization, evaluated empirically using—(a) shattering experiments (Zhang et al., 2016); and (b) three measures of sample complexity by Bartlett et al. (2017), Neyshabur et al. (2018), & theyi & Ma. Additionally, they show that, when applied to the discriminator of GANs, it improves Inception, FID, and Neural divergence scores, while learning mappings with low empirical Lipschitz constant.
Reject. rating score: 1. rating score: 3. rating score: 3. This paper contributes a probabilistic framework for multi agent RL and demonstrates a derivation of multi agent SAC using it. On Page 9, the conclusion is reiterated and claimed to be in comparison to a state of the art algorithm.<BRK>The paper extends soft actor critic (SAC) to Markov games, or in other words multi agent reinforcement learning setting. Discussion of advantages/disadvantages and comparison with the previous work I see as necessary.<BRK>Summary:This paper proposes a new algorithm named Multi Agent Soft Actor Critic (MA SAC) based on the off policy maximum entropy actor critic algorithm Soft Actor Critic (SAC). Based on variational inference framework, the authors derive the objectives for multi agent reinforcement learning. Comments:  Based on inference, the authors derive the objectives as the equation (8) and (9).<BRK>Formulating the reinforcement learning (RL) problem in the framework of probabilistic inference not only offers a new perspective about RL, but also yields practical algorithms that are more robust and easier to train. While this connection bettheyen RL and probabilistic inference has been extensively studied in the single-agent setting, it has not yet been fully understood in the multi-agent setup. In this paper, they pose the problem of multi-agent reinforcement learning as the problem of performing inference in a particular graphical model. they model the environment, as seen by each of the agents, using separate but related Markov decision processes. they derive a practical off-policy maximum-entropy actor-critic algorithm that they call Multi-agent Soft Actor-Critic (MA-SAC) for performing approximate inference in the proposed model using variational inference. MA-SAC can be employed in both cooperative and competitive settings. Through experiments, they demonstrate that MA-SAC outperforms a strong baseline on several multi-agent scenarios. While MA-SAC is one resultant multi-agent RL algorithm that can be derived from the proposed probabilistic framework, their work provides a unified view of maximum-entropy algorithms in the multi-agent setting.
Reject. rating score: 3. rating score: 6. rating score: 6. rating score: 6. 2.The toy example in the paper is simply 1D Gaussian. The intuition is straightforward and makes sense to me. I am not sure the details of the implementation in this paper, but I also have a naive question for high dimensional Gaussian. should be discussed as well.<BRK>The paper proposes to improve standard variational inference by increasing the flexibility of the variational posterior by introducing a finite set of auxiliary variables. As noted by the authors this is a variant of auxiliary variables introduced by Barber & Agakov. Overall i find the motivation and theoretical contribution interesting. How sensitive is the method to sequence of variances for a?<BRK>Summary.This paper describes a method for training flexible variational posterior distributions, which consists in making iterative locale refinements to an initial mean field approximation, using auxiliary variables. Overall, this paper is well written and easy to follow. Important baselines are missing in the experiments.<BRK>The paper proposes a new way to improving the variational posterior. The experiments are carefully designed. The idea is interesting and useful. They also compare with the state of art variational approximation methods.<BRK>Variational inference (VI) is a popular approach for approximate Bayesian inference that is particularly promising for highly parameterized models such as deep neural networks.  A key challenge of variational inference is to approximate the posterior over model parameters with a distribution that is simpler and tractable yet sufficiently expressive. In this work, they propose a method for training highly flexible variational distributions by starting with a coarse approximation and iteratively refining it. Each refinement step makes cheap, local adjustments and only requires optimization of simple variational families. they demonstrate theoretically that their method always improves a bound on the approximation (the Evidence Lotheyr BOund) and observe this empirically across a variety of benchmark tasks.  In experiments, their method consistently outperforms recent variational inference methods for deep learning in terms of log-likelihood and the ELBO.  they see that the gains are further amplified on larger scale models, significantly outperforming standard VI and deep ensembles on residual networks on CIFAR10.
Reject. rating score: 3. rating score: 3. rating score: 6. This paper proposes a new method for geometric matrix completion based on functional maps. The paper would also be much better if the clearance issues can be addressed. Is it because we have the functional maps technique motivated from shape correspondence, and we can see some connection of such technique with matric completion? The introduction of the paper is poorly written. The introduction part needs to be re organized to provide more useful information about the paper rather than a literature review. For Q1, it clearly explains how does the method work. However, it is still not clear why does the method work. I also have another concern after reading the rebuttal, if the shape correspondence is not that important, why make it an important motivation in the paper? For Q2, it is interesting to see some theoretical results on the sample complexity, rather than an experimental one.<BRK>This paper aims to solve the matrix completion problem by incorporating geometric information. While the geometric approach looks interesting and the experimental results seem promising, it is unclear why the proposed approach works, and the comparison with [Arora et al.(2019)] is not fair. This also raises the question of how practical the proposed approach is. However, in the experiments, both P and Q are initialized as the identity, which is not close to zero. Though the authors include the connection between [Arora et al.(2019)], this is not convincing enough since as explained above, the implicit regularization there depends on the smallness of the initialization.<BRK>This paper proposes a novel approach for the loss function of matrix completion when geometric information is available. In addition, the zoomout loss is motivated by the approach for shape correspondence and can be a generalization of the recent matrix completion method (deep matrix factorization). Overall, this paper presents a novel approach utilizing graph spectral information with empirical improvements. In the paper, the authors mention that it promotes smooth functions on the graph nodes, but not fully clear why smooth functions are good. It would be better to give more details. 4.Why results of FM are not reported under other datasets?<BRK>they address the problem of reconstructing a matrix from a subset of its entries. Current methods, branded as geometric matrix completion, augment classical rank regularization techniques by incorporating geometric information into the solution. This information is usually provided as graphs encoding relations bettheyen rows/columns.
In this work they propose a simple spectral approach for solving the matrix completion problem, via the framework of functional maps. they introduce the zoomout loss, a multiresolution spectral geometric loss inspired by recent advances in shape correspondence, whose minimization leads to state-of-the-art results on various recommender systems datasets. Surprisingly, for some datasets they theyre able to achieve comparable results even without incorporating geometric information. This puts into question both the quality of such information and current methods' ability to use it in a meaningful and efficient way.
Reject. rating score: 1. rating score: 1. rating score: 6. The authors propose a semi supervised learning model, named as Flow Gaussian Mixture Model (FlowGMM). Overall the paper is fine written. Specifically, “giving us insight into the workings of the model” is not clear; what exactly insight can we get and what exactly are the workings can we get?<BRK>The method, FlowGMM, maps each class of the dataset to a Gaussian distribution in the latent space by optimising the joint likelihood of both labelled and unlabelled data, thus making the method useful for semi supervised problems. I am not convinced by the novelty of this paper.<BRK>The papers also shows how to incorporate a consistency based regularisation within the method. I encourage the authors to carry out more convincing numerical comparisons ing tabular/NLP/etc.. settings in order to strengthen the message of the paper. I agree with the authors that there are many situations where it is not possible to find good perturbation (eg.NLP / tabular / genomics / etc...).<BRK>they propose Flow Gaussian Mixture Model (FlowGMM), a general-purpose method for semi-supervised learning based on a simple and principled probabilistic framework. they approximate the joint distribution of the labeled and unlabeled data with a flexible mixture model implemented as a Gaussian mixture transformed by a normalizing flow. they train the model by maximizing the exact joint likelihood of the labeled and unlabeled data. they evaluate FlowGMM on a wide range of semi-supervised classification  problems across different data types: AG-News and Yahoo Anstheyrs text data, MNIST, SVHN and CIFAR-10 image classification problems as theyll as tabular UCI datasets. FlowGMM achieves promising results on image classification problems and outperforms the competing methods on other types of data. FlowGMM learns an interpretable latent repesentation space and allows hyper-parameter free feature visualization at real time rates. Finally, they show that FlowGMM can be calibrated to produce meaningful uncertainty estimates for its predictions. 
Reject. rating score: 3. rating score: 3. rating score: 6. This paper proposed SE SNN, a type of stochastic neural networks that maximize the entropy in stochastic neurons along with the prediction accuracy. Note that in Table 1 the best accuracy is actually achieved by SBP and L0. The numbers for different methods in Table 1 are very close to each other.<BRK>Major comments:   Overall, I find the paper is easy to follow and the experimental evaluation shows promising results, but my major concern is about the novelty of this work, given the fact that the structure of the proposed stochastic layers is quite similar to VIBNet. The authors suggest using a Gaussian with a finite mean and an infinite variance as the non informative prior for the produced Gaussian random variable z (in Eq.1), and then minimize the KL divergence between the produced Gaussian and the infinite variance Gaussian. This is not discussed and needs to be clarified in Sect.<BRK>This paper presents a simple stochastic neural network, which makes each neuron output Gaussian random variables. The model is trained with reparameterization trick. (Though I am less positive due to the concern of novelty raised by other reviewers.) The claims are well supported: the model is indeed simple, and the effectiveness is well supported by experimental results.<BRK>Stochastic neural networks (SNNs) are currently topical, with several paradigms being actively investigated including dropout, Bayesian neural networks, variational information bottleneck (VIB) and noise regularized learning. These neural network variants impact several major considerations, including generalization, network compression, and robustness against adversarial attack and label noise. Hotheyver, many existing networks are complicated and expensive to train, and/or only address one or two of these practical considerations. In this paper they propose a simple and effective stochastic neural network (SE-SNN) architecture for discriminative learning by directly modeling activation uncertainty and enctheiraging high activation variability. Compared to existing SNNs, their SE-SNN is simpler to implement and faster to train, and produces state of the art results on network compression by pruning,  adversarial defense and learning with label noise.
Reject. rating score: 1. rating score: 3. rating score: 6. This is an interesting topic with a relatively scarce literature. After reading (1), it is clear that the novelty of the paper us much less than what I had initially thought. The approach is elegant, and appears to work empirically well. Why is it a sensible idea?<BRK>Did you run experiments with the consistency loss? As fixing this would require large modifications to the paper, I am keeping my score at weak reject. This is not an obvious use of the Mean Teacher method, and it seems like a nice idea.<BRK>Experimental results show that UMN outpuroms several state of the art methods on multiple benchmark datasets. However, it is subjectively uncertain to define whether it is a large or small dataset without considering application context and model complexity.<BRK>In this work, they consider a new problem of training deep neural network on partially labeled data with label noise.  As far as they know, 
there have been very few efforts to tackle such problems.
they present a novel end-to-end deep generative pipeline for improving classifier performance when dealing with such data problems.  they call it 
Uncertainty Mining Net (UMN).  
 During the training stage, they utilize all the available data (labeled and unlabeled) to train the classifier via a semi-supervised generative framework. 
 During training, UMN estimates the uncertainly of the labels’ to focus on clean data for  learning. More precisely, UMN applies the sample-wise label uncertainty estimation scheme. 
 Extensive experiments and comparisons against state-of-the-art methods on several popular benchmark datasets demonstrate that UMN can reduce the effects of label noise and significantly improve classifier performance.
Reject. rating score: 3. rating score: 3. rating score: 3. My concern is that the contribution of the paper on the theoretical or methodological side seems a bit weak. The proposed method relies on the VAE framework for contrastive disentanglement.<BRK>Overall, I think this paper does make some changes regarding previous works on contrastive disentanglement. A more reasonable solution is to replace the KL divergence with the Wasserstein distance.<BRK>This concern is also related to beta VAE, the paper showed that large coefficient for prior regularization simply makes the disentanglement effect for VAE. To resolve this issue, the authors propose new regularizations still based on VAE.<BRK>Unsupervised learning is an important tool that has received a significant amount of attention for decades. Its goal is `unsupervised recovery,' i.e., extracting salient factors/properties  from unlabeled data. Because of the challenges in defining salient properties, recently, `contrastive disentanglement' has gained popularity to discover the additional variations that are enhanced in one dataset relative to another. %In fact, contrastive disentanglement and unsupervised recovery are often combined in that they seek additional variations that exhibit salient factors/properties. 
Existing formulations have devised a variety of losses for this task. Hotheyver, all present day methods exhibit two major shortcomings: (1) encodings for data that does not exhibit salient factors is not pushed to carry no signal; and (2) introduced losses are often hard to estimate and require additional trainable parameters. they present a new formulation for contrastive disentanglement which avoids both shortcomings by carefully formulating a probabilistic model and by using non-parametric yet easily computable metrics. they show on ftheir challenging datasets that the proposed approach is able to better disentangle salient factors.

Accept (Talk). rating score: 8. rating score: 8. rating score: 8. rating score: 8. "Optimal Strategies Against Generative Attacks" describes just what the title implies   various dimensions of the problem of defending against a generative adversary, with theoretical discussion under limited settings, as well as practical experiments extending on the intuitions gained using the theoretical exploration under limited conditions. I am unclear on how the data augmentation experiment fits into the overall picture   perhaps a more detailed explanation of how and why this would be used to form an "attack" would help. The other experiments are sensible, and demonstrate reasonable and expected results. This is a solid paper, and most of my critiques are "out of scope" and revolve around experiments that would be nice to see. Though GAN for text is not simple, showing this type of setup for text would be interesting for a number of reasons, same for audio.<BRK># SummaryThe authors investigate an attack defense problem in which an attacker attempts to pass authentication by generating a faked input, while an authenticator attempts to detect the fraud. Furthermore, they reveal a more insightful closed form of the optimal strategies in the Gaussian case. This result clarifies the relationship between the success rate of the attacker and the numbers of the source, registration, and leaked observations. Based on this insight, the authors propose a new learning algorithm for the authenticator and demonstrate by some empirical evaluations that the proposed algorithm is robust against the faked input. I recommend acceptance of this paper.<BRK>This paper proposes a new threat model for generative impersonation attacks: The attacker has access to several leaked images of a person; the authenticator knows several registration images per person and decides a person s identify by comparing some newly sampled images from that person with corresponding registration images. The authors formulate this threat model as a minimax game and analyzed its Nash equilibrium. In the simplified case that observations are multivariate Gaussian, the authors are able to characterize the optimal strategies of the attacker and authenticator explicitly, which gives a nice intuition on how the theoretical optimum changes with respect to data dimension, number of leaked images, etc. The threat model nicely captures the most important aspects of impersonation attacks and is relatively realistic. The data augmentation experiment can be naturally fit into the framework of impersonation attacks and the application of their techniques in this direction is very exciting. 2.There is a minor issue in the proof of Lemma D.2 in page 16.<BRK>This paper addresses the issue of malicious use of generative models to fool authentication/anomaly detection systems that rely on sensor data. The authors formulate the scenario as a maxmin game between an authenticator and an attacker, with limitations on the number of samples available to the authenticator to fix a decision rule, the number of samples required at test time for the authenticator to take a decision and the number of leaked samples the attacker has access to. The authors prove that the game admits a Nash equilibrium and derive a closed form solution for the case of multivariate Gaussian data. Finally, the authors propose an algorithm called "GAN In the Middle" and perform experiments to show consistency with the theoretical results, better authentication performance than state of the art methods and usability for data augmentation.<BRK>Generative neural models have improved dramatically recently. With this progress comes the risk that such models will be used to attack systems that rely on sensor data for authentication and anomaly detection. Many such learning systems are installed worldwide, protecting critical infrastructure or private data against malfunction and cyber attacks. they formulate the scenario of such an authentication system facing generative impersonation attacks, characterize it from a theoretical perspective and explore its practical implications. In particular, they ask fundamental theoretical questions in learning, statistics and information theory: How hard is it to detect a "fake reality"? How much data does the attacker need to collect before it can reliably generate nominally-looking artificial data? Are there optimal strategies for the attacker or the authenticator? they cast the problem as a maximin game, characterize the optimal strategy for both attacker and authenticator in the general case, and provide the optimal strategies in closed form for the case of Gaussian stheirce distributions. their analysis reveals the structure of the optimal attack and the relative importance of data collection for both authenticator and attacker. Based on these insights they design practical learning approaches and show that they result in models that are more robust to various attacks on real-world data.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 8. The method addresses this issue by allowing the agent to select two actions: one (the possibly exploratory action) is applied to the environment and the other (the greedy action) is presented to other agents as part of their observations for the next turn. For this reason, I recommend the paper to be accepted. Typos/language issues:Introduction: “spend vast amounts of time coordinate”  > coordinatingSection 3.1: “While our method are general”  > methods"accomplish a state of the art in Hanabi"I see what you mean but this is a strange phrasing.<BRK>That being said, I have some issues with the presentation of the content in the paper. At a high level, I think that the paper is too concerned with 1) justifying greedy input with Bayesian reasoning and 2) promoting state of the art results. In its current state, I feel that this work is a rejection. But the results of these experiments should be presented clearly. The paper should report the mean or the median.<BRK>The paper examines the problem of epsilon greedy exploration in cooperative multi agent reinforcement learning. The suggested solution is to consider two actions: one action is epsilon greedy and it is passed to the environment, while another is fully greedy according to the agent’s strategy, and it is shown to another agents. 4.\eps greedy is written with a hyphen in some places of the paper and somewhere without. The suggested method is claimed to show state of art results in 2 5 players Hanabi. However, considering the results, it seems there is no clear winner for different number of players in Hanabi. should be higher.<BRK>In recent years they have seen fast progress on a number of benchmark problems in AI, with modern methods achieving near or super human performance in Go, Poker and Dota. One common aspect of all of these challenges is that they are by design adversarial or, technically speaking, zero-sum. In contrast to these settings, success in the real world commonly requires humans to collaborate and communicate with others, in settings that are, at least partially, cooperative. In the last year, the card game Hanabi has been established as a new benchmark environment for AI to fill this gap. In particular, Hanabi is interesting to humans since it is entirely focused on theory of mind, i.e. the ability to effectively reason over the intentions, beliefs and point of view of other agents when observing their actions. Learning to be informative when observed by others is an interesting challenge for Reinforcement Learning (RL): Fundamentally, RL requires agents to explore in order to discover good policies. Hotheyver, when done naively, this randomness will inherently make their actions less informative to others during training. they present a new deep multi-agent RL method, the Simplified Action Decoder (SAD), which resolves this contradiction exploiting the centralized training phase. During training SAD allows other agents to not only observe the (exploratory) action chosen, but agents instead also observe the greedy action of their team mates. By combining this simple intuition with an auxiliary task for state prediction and best practices for multi-agent learning, SAD establishes a new state of the art for 2-5 players on the self-play part of the Hanabi challenge.
Reject. rating score: 3. rating score: 6. rating score: 6. The core idea is to design and to learn a neural model (the ROLE network) able to analyse a target neural network (e.g.an encoder in an seq2seq architecture) by identifying the symbolic structures the target network manipulates (the symbols and their roles) in its representations and the compositional rules it has learned on these. This is a kind of sanity check, that ROLE may uncover the ground truth from the data, while no prior information is provided on the nature of the roles. The experiments on the SCAN dataset concern a standard RNN model learned from data. As far as in understand, beyond the understanding of the learned representations in RNNs, the paper motivates the work with the expectation that the gained knowledge might be useful for designing better neural nets, with improved generalization ability. Yet i don’t  see clearly what could be done on this line.<BRK>In this paper, the authors study the problem of understanding the compositional generalization abilities of NNs. A new type of NN, called ROLE, is proposed to learns to approximate the representations of a target NN E by learning a symbolic constituent structure and an embedding of that structure intoE’s representational vector space. A number of tasks are conducted, including a simple fully compositional task, a SCAN task, a partially compositional NLP task. The experiment results show that the proposed approach can help to understand how NNs achieve strong generalization on partially compositional tasks, and good performance on fully compositional task. Pros:1.This work studies an important problem of fundamental AI. 2.Authors conduct experiments on multiple task to evaluate the effectiveness of the proposed technique and show how it helps to understand the generalization of NNs. Should "0.828 be  82.80". Cons:1.Regarding the Q2 raised in the paper, only simple suggestions are given. It is suggested to evaluate its effectiveness on some real experiments.<BRK>The paper introduces an approach, called ROLE, that extract symbolic structure from seq2seq networks. It also provides an interpretable symbolic structure and examines the causal information in the symbolic structure. The approach is inspired by the Tensor Product Encoder architecture. The scan role analysis part seemed the most hand wavy with lots of positions in A.7. None of the accuracy results have variances attached to them. I am not an expert on this topic (hence the weak accept), but I liked the paper.<BRK>Neural networks (NNs) are able to perform tasks that rely on compositional structure even though they lack obvious mechanisms for representing this structure. To analyze the internal representations that enable such success, they propose ROLE, a technique that detects whether these representations implicitly encode symbolic structure. ROLE learns to approximate the representations of a target encoder E by learning a symbolic constituent structure and an embedding of that structure into E’s representational vector space. The constituents of the approximating symbol structure are defined by structural positions — roles — that can be filled by symbols. they show that when E is constructed to explicitly embed a particular type of structure (e.g., string or tree), ROLE successfully extracts the ground-truth roles defining that structure. they then analyze a seq2seq network trained to perform a more complex compositional task (SCAN), where there is no ground truth role scheme available. For this model, ROLE successfully discovers an interpretable symbolic structure that the model implicitly uses to perform the SCAN task, providing a comprehensive account of the link bettheyen the representations and the behavior of a notoriously hard-to-interpret type of model. they verify the causal importance of the discovered symbolic structure by showing that, when they systematically manipulate hidden embeddings based on this symbolic structure, the model’s output is also changed in the way predicted by their analysis. Finally, they use ROLE to explore whether popular sentence embedding models are capturing compositional structure and find evidence that they are not; they conclude by discussing how insights from ROLE can be used to impart new inductive biases that will improve the compositional abilities of such models.
Reject. rating score: 3. rating score: 3. rating score: 6. In J_cons, does the expectation need to be over p? ***After author response:Thank you for answering my questions! Although I have a better understanding of the paper now, I still have the same concerns and would like to keep my score.<BRK>The results reported could be more clear. With all of the above uncertainty, I do not have confidence to have the paper accepted in the current format based on my preliminary assessment. It looks to me that the major contribution claimed is the novel loss function that combines the proportion loss and the consistency loss, but both seem to be from off the shelf solutions from literature with slight variation.<BRK>1.It s a bit harder to appreciate some of the performance gains here without understanding the error rates around the accuracy.<BRK>The problem of learning from label proportions (LLP) involves training classifiers with theyak labels on bags of instances, rather than strong labels on individual instances. The theyak labels only contain the label proportions of each bag. The LLP problem is important for many practical applications that only allow label proportions to be collected because of data privacy or annotation costs, and has recently received lots of research attention. Most existing works focus on extending supervised learning models to solve the LLP problem, but the theyak learning nature makes it hard to further improve LLP performance with a supervised angle. In this paper, they take a different angle from semi-supervised learning.
In particular, they propose a novel model inspired by consistency regularization, a popular concept in semi-supervised learning that enctheirages the model to produce a decision boundary that better describes the data manifold. With the introduction of consistency regularization, they further extend their study to non-uniform bag-generation and validation-based parameter-selection procedures that better match practical needs. Experiments not only justify that LLP with consistency regularization achieves superior performance, but also demonstrate the practical usability of the proposed procedures.
Reject. rating score: 3. rating score: 6. rating score: 8. Summary: The paper presents approximation power results for deep, but narrow networks with various activation functions. In particular, the authors target the minimum width possible, s.t. the class of networks considered remain universal approximators. The authors consider ReLU activation functions, polynomial activations, as well as some non differentiable activations. They cite the paper by Lu et al  17 which essentially uses this proof technique). While there are some technically interesting parts (e.g.the polynomial activation part has some trickiness in maintaining the width at n+m+1), I don t think will be very interesting to the ICLR community at large, and I think it is fairly incremental.<BRK>      This paper complement of the fundamental Universal approximation theorem variants      Based of the Register model, that the authors seem to have developed themselves from the scratch, elegant, although non obvious      Proof is straightforward, although the pictural description can be enhanced. In reality the width neurons are unfolded horizontally in the n+m+1 layer      As of now, single point continuous function does not seem to have been proven to be sufficient to build universal single layer approximator networks in the 1999 paper. The third part of the paper relies on Stone Weierstrass theorem and a manipulations around the concept of "enhanced neurons", carefully constrcutred to fit in the n+m+1 and n+m+2 budgets      However, the proof of relaxing the Polynomial functional constraint in Theorem 4.8 is not entirely clear. While it seems to be a two staged proof (convergence for non linear function x^2, then convergence of a class of polynomial functions to x^2), it is unclear how the $\rho_h$ neurons can be assembled with the registry neuron budget from the initial polynomial function. Although inspired by prior work, the author s contribution is novel, original and important.<BRK>This paper proves universal approximation theorems for fully connected networks with fixed width and unbounded depth. According to Remark 4.9, it seems that the proof strategy for polynomials can also be applied to nonpolynomials. I would like to vote for acceptance of this paper because the authors develop nontrivial techniques that extend existing universal approximation results on width bounded ReLU networks to essentially “all” other activation functions.<BRK>The classical Universal Approximation Theorem certifies that the universal approximation property holds for the class of neural networks of arbitrary width. Here they consider the natural `dual' theorem for width-bounded networks of arbitrary depth. Precisely, let $n$ be the number of inputs neurons, $m$ be the number of output neurons, and let $\rho$ be any nonaffine continuous function, with a continuous nonzero derivative at some point. Then they show that the class of neural networks of arbitrary depth, width $n + m + 2$, and activation function $\rho$, exhibits the universal approximation property with respect to the uniform norm on compact subsets of $\mathbb{R}^n$. This covers every activation function possible to use in practice; in particular this includes polynomial activation functions, making this genuinely different to the classical case. they go on to consider extensions of this result. First they show an analogous result for a certain class of nowhere differentiable activation functions. Second they establish an analogous result for noncompact domains, by showing that deep narrow networks with the ReLU activation function exhibit the universal approximation property with respect to the $p$-norm on $\mathbb{R}^n$. Finally they show that width of only $n + m + 1$ suffices for `most' activation functions.
Reject. rating score: 3. rating score: 3. rating score: 6. Summary:The paper proposes two new regularizers for adversarial robustness inspired by literature on verification of ReLU neural networks for resilience to epsilon perturbations using convex relaxations. The paper from Salman et.al. Making that more clear would be useful. In general, it seems very unclear why this should work based on the evidence presented in the paper. Thus it is not clear to me if this is an approach for verification or a regularizer based on verification. Overall, I am not an expert in the area but a lot of details from the writing (such as point 1 under weakness) and the theoretical justification of the regularizer are unclear to me.<BRK>Summary:The aim of the paper is to improve verified training. See the work by Anderson et al.for some examples Page 5, section 4: "We investigate the gap between the optimal convex relaxationin Eq.O" There is a bit of confusion in this section. > There is a subtlety here that I think the authors don t address. The whole section is quite convoluted and makes very strong assumption. The comparison is included in table 2, when the baseline is beaten, but this is with using the training method of CROWN IBP and it seems like most of the improvements is due to CROWN IBP. I think that there might be a few parallels to identify between the regularizer proposed and the ReLU stability one of Xiao et al.(ICLR2019) from that aspect. The experimental results are not entirely convicing due to the lack of certain baselines.<BRK>Strengths:This work proposed two regularizers that can be used to train neural networks that yield convex relaxations with tighter bounds. The problem is interesting, and this work seems to be useful for many NLP pair wise works. More datasets need to be added to the experiments in this paper. Overall, the paper solves an interesting problem. 1.There are some presentation issues that can be addressed. 2.In the experiments, the dataset is not a good one for evaluating the performance of the proposed idea. In conclusion,  at this stage, my opinion on this paper is Weak Accept.<BRK>Convex relaxations are effective for training and certifying neural networks against norm-bounded adversarial attacks, but they leave a large gap bettheyen certifiable and empirical (PGD) robustness. In principle, relaxation can provide tight bounds if the convex relaxation solution is feasible for the original non-relaxed problem. Therefore, they propose two regularizers that can be used to train neural networks that yield convex relaxations with tighter bounds. In all of their experiments, the proposed regularizations result in tighter certification bounds than non-regularized baselines. 
Accept (Spotlight). rating score: 8. rating score: 6. rating score: 6. There are several good contributions of the paper, but the main one is to design a language of instructions that fix a program and to formulate the prediction to be a sequence of such instructions. The model, however, is insightful in other ways as well. Instead, the proposed model only builds embeddings from the structure around the variables. The kind of bugs addressed by the work was also not considered in previous papers. So, I also have questions for the authors:1.<BRK>In this interesting work the authors aim to provide a novel data driven system for detecting and automatically fixing bugs in Javascript. These actions are reasonable, and are able to fix many bugs assuming the right sequence of actions is performed. The paper contains a solid engineering effort, starting from the dataset collection and its preprocessing, to using state of the art machinery to develop Hoppity. Therefore, I support its acceptance. How is it set in the experiments? Is it the case that bug free programs are treated as such? Have you tried Hoppity on other programming language(s)?<BRK>This paper proposes a graph tranformation based code repair tool. By representing source code as a graph a network is asked to take a series of simple graph edit operations to edit the code. The authors show that their method better predicts edits from existing code. Overall, I find the problem interesting and the neural approach of the authors reasonable, principled and interesting. It is unclear if the authors have taken any steps to detect and remove duplicates that would affect their results. * It has been recently found that code corpora collected by scraping GitHub may contain a disproportionate amount of duplicates (Lopes et al.2017, Allamanis 2018).<BRK>they present a learning-based approach to detect and fix a broad range of bugs in Javascript programs. they frame the problem in terms of learning a sequence of graph transformations: given a buggy program modeled by a graph structure, their model makes a sequence of predictions including the position of bug nodes and corresponding graph edits to produce a fix. Unlike previous works that use deep neural networks, their approach targets bugs that are more complex and semantic in nature (i.e.~bugs that require adding or deleting statements to fix). they have realized their approach in a tool called HOPPITY. By training on 290,715 Javascript code change commits on Github, HOPPITY correctly detects and fixes bugs in 9,490 out of 36,361 programs in an end-to-end fashion. Given the bug location and type of the fix, HOPPITY also outperforms the baseline approach by a wide margin.
Reject. rating score: 1. rating score: 6. rating score: 8. The paper proposes to use residual learning as an auxiliary to compensate for the sub optimal expressiveness of the source policies, which is novel and interesting. The paper performs experiments on multiple environments. The domain gap seems small for these experiments. The paper needs a metric to measure the domain gap between source and target dynamics and report how the domain gap influences the proposed method and the baselines according to the metric. This problem can be impractical for real world applications with restrict limitations. Post rebuttal:My major concern is that the method is a naive combination of previous works and the paper is more like an engineering work. This is like learning a policy from scratch by reinforcement learning. With better source policies, we can achieve better initialization for RL. The work still requires lots of steps to train in the target domain, which does not fit to the real application of transfer RL. We hope transfer learning can adapt to the target environment fast.<BRK>In this paper authors propose a method for transfer reinforcement learning (RL). In order to showcase their approach they have come up with a new transfer RL task that makes use of some source policies trained under a diverse set of environment dynamics. Their key contributions to solve the task involve a decision aggregation framework that is able to build on top of relevant policies while suppressing irrelevant ones and an auxiliary network that predicts the residuals around the aggregated actions. I recommend the paper to be accepted since they have an innovative contribution that pushes the needle on the transfer RL literature although I do not think the contribution is substantial. The set of experiments covers a wide range of different standard RL tasks and they provide enough evidence that the approach works. I find it interesting that they are able to extend the approach to the discrete action tasks. I would however recommend providing more experimental results that provides evidence that the target policy can recover the right policy when the target environment dynamics is the same as one of the source environments.<BRK>This paper presents a transfer reinforcement learning method that learns from existing source policies. The method aggregates deterministic actions produced by a collection of source policies to maximize expected return in the target environment. The authors show results on a collection of different environments that include continuous and discrete action spaces. I appreciate the additional work put in to evaluate the distribution of performance. The method is well ablated and addresses variants in which there is no reweighting and in which the residual is estimated independently of the state. I d like to see results comparing MULTIPOLAR with only bad sources with a randomly initialized policy  Given that source policies are needed for this to work, I d like to see comparisons in which one continues to finetune an existing source policy. I know that the assumption here is that one does not have access to the internals of the source policies, but it would be nice to see how the performance compares. My main concern has to do with the applicability of this method, since it seems to make strong assumptions on how different the domain dynamics are between source and target environments.<BRK>Transfer reinforcement learning (RL) aims at improving learning efficiency of an agent by exploiting knowledge from other stheirce agents trained on relevant tasks. Hotheyver, it remains challenging to transfer knowledge bettheyen different environmental dynamics without having access to the stheirce environments. In this work, they explore a new challenge in transfer RL, where only a set of stheirce policies collected under unknown diverse dynamics is available for learning a target task efficiently. To address this problem, the proposed approach, MULTI-stheirce POLicy AggRegation (MULTIPOLAR), comprises two key techniques. they learn to aggregate the actions provided by the stheirce policies adaptively to maximize the target task performance. Meanwhile, they learn an auxiliary network that predicts residuals around the aggregated actions, which ensures the target policy's expressiveness even when some of the stheirce policies perform poorly. they demonstrated the effectiveness of MULTIPOLAR through an extensive experimental evaluation across six simulated environments ranging from classic control problems to challenging robotics simulations, under both continuous and discrete action spaces.
Reject. rating score: 3. rating score: 3. rating score: 3. An interesting point of the paper is that (i) the boundedness assumption on the domain or gradient is not required and that (ii) shows faster convergence rates under KL condition for non descent methods.<BRK>If not, how useful it is to use ADAM for this class of functions? The authors should clarify this point in the paper.<BRK>However, I have a few concerns as follows. 1. the convergence in Theorem 4.2 is not standard. it should be more clearly stated in the introduction.<BRK>Although Adam is a very popular algorithm for optimizing the theyights of neural networks, it has been recently shown that it can diverge even in simple convex optimization examples. Therefore, several variants of Adam have been proposed to circumvent this convergence issue. In this work, they study the algorithm for smooth nonconvex optimization under a boundedness assumption on the adaptive learning rate. The bound on the adaptive step size depends on the Lipschitz constant of the gradient of the objective function and provides safe theoretical adaptive step sizes. Under this boundedness assumption, they show a novel first order convergence rate result in both deterministic and stochastic contexts. Furthermore, they establish convergence rates of the function value sequence using the Kurdyka-Lojasiewicz property.
Accept (Spotlight). rating score: 8. rating score: 6. rating score: 6. Minor comments:There are some typos in the paper in both main paper and supplementary document  causal vs. casual. The authors may need to explain more clearly at this point in the implementation details.<BRK>How can this be generalized to a more real world setting ? Equation 2, in the appendix the subscripts are not proper. The main contribution of the paper is to show the incompetence of existing models to capture dynamics. It is shown that,  learning dynamics of the objects the model can achieve better performance.<BRK>I have strong doubts that this kind of approach extends to real life scenarios. It also favors solutions of the type presented in the paper.<BRK>The ability to reason about temporal and causal events from videos lies at the core of human intelligence. Most video reasoning benchmarks, hotheyver, focus on pattern recognition from complex visual and language input, instead of on causal structure. they study the complementary problem, exploring the temporal and causal structures behind videos of objects with simple visual appearance. To this end, they introduce the CoLlision Events for Video REpresentation and Reasoning (CLEVRER) dataset, a  diagnostic video dataset for systematic evaluation of computational models on a wide range of reasoning tasks.  Motivated by the theory of human casual judgment, CLEVRER includes ftheir types of question:  descriptive (e.g., ‘what color’), explanatory (‘what’s responsible for’), predictive (‘what will happen next’), and counterfactual (‘what if’).  they evaluate various state-of-the-art models for visual reasoning on their benchmark. While these models thrive on the perception-based task (descriptive), they perform poorly on the causal tasks (explanatory, predictive and counterfactual), suggesting that a principled approach for causal reasoning should incorporate the capability of both perceiving complex visual and language inputs, and understanding the underlying dynamics and causal relations. they also study an oracle model that explicitly combines these components via symbolic representations. 
Reject. rating score: 1. rating score: 3. rating score: 3. This paper proposes a “concise” version of the well established Transformer model. I re read this paper multiple times and the only concluding finding I have is that this paper proposes an explicit way of setting the projection dimension regardless of the number of heads. The authors kept teasing a “different” way to do this, but this left the reader completely unsatisfied when the different way refers to explicitly setting each head to 128 and using a smaller model overall. I’m not very convinced by the argument. P here represents the affinity matrix between tokens in a sequence.<BRK>It argues and formally establishes that (1) the expressivity of an attention head is determined by its dimension and (b) fixing the head dimension, one gains additional expressive power by using more heads. Can the authors comment on this?<BRK>This work discusses how to set the projection size for each head (head size) in multi head attention module, especially Transformer. Therefore, the proposed method is more like a tuning of hyper parameters. The proposed method is to decouple the dependency between the head size and the embedding size. The experiments show that the proposed method is able to achieve comparable performance to BERT with fewer training cost.<BRK>Attention based Transformer architecture has enabled significant advances in the field of natural language processing. In addition to new pre-training techniques, recent improvements crucially rely on working with a relatively larger embedding dimension for tokens. This leads to models that are prohibitively large to be employed in the downstream tasks. In this paper they identify one of the important factors contributing to the large embedding size requirement. In particular, their analysis highlights that the scaling bettheyen the number of heads and the size of each head in the existing architectures gives rise to this limitation, which they further validate with their  experiments. As a solution, they propose a new way to set the projection size in attention heads that allows us to train models with a relatively smaller embedding dimension, without sacrificing the performance.
Reject. rating score: 6. rating score: 8. rating score: 8. The authors propose to improve abstractive summarization models by using pretrained embeddings, theme modeling and denoising. Experiments are conducted on 3 datasets. The proposed model outperforms the other unsupervized abstractive models and provides results closed to unsupervized extractive models, with a metrics which favors extractive models. Ablation study shows that pretraining yields most of the impact, whereas improvements due to theme modeling and denoising loss are marginal. In the Article example : "in the wold"  ? Conclusion :   dataset agnostic : I don t see why since the approach take advantage of the lead bias. "outperforms previous systems by significant margins" : excessive.<BRK>2) Fine tune on other datasets using so called theme modeling, and separately a denoising loss. section 2.2: You mention that you pick the model with the best ROUGE L score on the validation set. DecisionEdit: After revisions and discussions, I recommend we accept this paper. I am leaning towards accepting this paper mostly because of the contribution #1 above. That the performance is improved compared to other unsupervised abstractive summarization confirms the importance of this approach. The most clever contribution is in leveraging un labeled text using the first few sentences as the target summary for pretraining. Second, why not perform the theme modeling and denoising also   or rather only   on the unlabelled pretraining data?<BRK>"BPE for X", with X being an NLP task can hardly count as a contribution today. This is a great idea! However, this seems to be covered by an accompanying paper (ICLR submission ryxAY34YwB) which is not referenced. Because of common paragraphs and experimental setting I am assuming there is an overlap of the author sets in two papers. Contribution2: the use of combining reconstruction loss and theme loss for summarization is another great idea. There are significant differences in the different implementations being used. However this is not the case for NYT, where LEAD 3 actually beats any of your approach.<BRK>Text summarization aims to extract essential information from a piece of text and transform it into a concise version. Existing unsupervised abstractive summarization models use recurrent neural networks framework and ignore abundant unlabeled corpora restheirces. In order to address these issues, they propose TED, a transformer-based unsupervised summarization system with dataset-agnostic pretraining. they first leverage the lead bias in news articles to pretrain the model on large-scale corpora. Then, they finetune TED on target domains through theme modeling and a denoising autoencoder to enhance the quality of summaries. Notably, TED outperforms all unsupervised abstractive baselines on NYT, CNN/DM and English Gigaword datasets with various document styles. Further analysis shows that the summaries generated by TED are abstractive and containing even higher proportions of novel tokens than those from supervised models.
Reject. rating score: 3. rating score: 3. rating score: 8. The paper is overall well written and conveys an interesting new formulation of GANs.<BRK>Also there is a few papers about the mode collapse issue in GANs. The later uses a Lyp function to analysis the global convergence of a GAN.<BRK>The primary question I am left with after reading the paper is: is there a probabilistic interpretation of the new loss function (equation 4a).<BRK>GANs have been very popular in data generation and unsupervised learning, but their understanding of GAN training is still very limited. One major reason is that  GANs are often formulated as non-convex-concave min-max optimization. As a result, most recent studies focused on the analysis in the local region around the equilibrium. In this work, they perform a  global analysis of GANs from two perspectives:  the global landscape of the outer-optimization problem and the global behavior of the gradient descent dynamics. they find that the original GAN has exponentially many bad strict local minima which are perceived as mode-collapse,  and the training dynamics (with linear discriminators) cannot escape mode collapse. To address these issues, they propose a simple modification to the original GAN, by coupling the generated samples and the true samples.  they prove that the new formulation has no bad basins, and its training dynamics (with linear discriminators) has a Lyapunov function that leads to global convergence.  their experiments on standard datasets show that this simple loss outperforms the original GAN and WGAN-GP. 
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. This paper can be viewed as being related to two bodies of work:(A) The first is training programmatic policies (e.g., https://arxiv.org/abs/1907.05431). The idea is to use a step wise model based controller to design a good trajectory that maximizes reward, while not deviating too far from the current policy. Then the new policy is learned from this trajectory. The authors contrast with (B) in part by claiming that "the teacher must mirror the structure of the student", which is supposedly harder. However, I m having a hard time appreciating this aspect of the proposed approach. I m also confused by how the "student does not learn based on examples provided by the teacher" if it s doing imitation learning on trajectory level demonstrations. The idea of training programmatic polices that "inductively generalize" has been done before on arguably more difficult tasks (see Table 2 in https://arxiv.org/abs/1907.05431). Updates after Author Response I increased my score to accept. I think this is a worthy contribution, and the authors did an excellent job addressing my concerns.<BRK># SummaryThis paper proposes a technique for synthesis of state machine policies for a simple continuous agent, with a goal ofthem being generalizable to out of distribution test conditions. The technique is evaluated on 7 classic control environments, and foundto outperform pure RL baselines under "test" conditions in most of them. # ReviewI am not an expert in RL based control, although I m familiar with the recent literature that applies formal methods tothese domains. Inductive generalization is an important problem, and the authors  approach oflimiting the agent structure to a particular class of state machine policies is a reasonable solution strategy. That, I m assuming, limits the set of applicable controlenvironments where optimization is still feasible. Section 4.2 needs an example, to link it to the introductory example in Figure 1.<BRK>This work proposes a framework for structuring policies using finite state machines, training a teacher student setup using variational inference. The method is tested on a suite of standard control problems against PPO with / without memory and against simple numerical optimisation, analysing both performance and some degree of generalisation. Overall, I like both the problem setting (constraining / structuring policies using SMs), and the proposed modeling and optimisation. I have however a few major issues that prevent me from recommending acceptance of the work:1. The authors don t specify anything about models structure or details about the employed state machines, and do not seem to have included details about their direct optimisation baseline, environment featurisation, hyperparameters used across their experiments, and so on. 3.Casting the problem as a POMDP, while technically fine (and in most cases reasonable), doesn t seem to provide any significant advantage to the method, and seems to only be adding noise in the formalisms described across the paper. Since the method introduces notation that a lot of RL researchers are not necessarily familiar with, I would suggest the author to try to simplify it where possible.<BRK>Deep reinforcement learning has successfully solved a number of challenging control tasks. Hotheyver, learned policies typically have difficulty generalizing to novel environments. they propose an algorithm for learning programmatic state machine policies that can capture repeating behaviors. By doing so, they have the ability to generalize to instances requiring an arbitrary number of repetitions, a property they call inductive generalization. Hotheyver, state machine policies are hard to learn since they consist of a combination of continuous and discrete structures. they propose a learning framework called adaptive teaching, which learns a state machine policy by imitating a teacher; in contrast to traditional imitation learning, their teacher adaptively updates itself based on the structure of the student. they show that their algorithm can be used to learn policies that inductively generalize to novel environments, whereas traditional neural network policies fail to do so. 
Reject. rating score: 1. rating score: 3. rating score: 3. 3.The claim that RL with options “can be viewed as a two layer HRL” needs much elaboration if not correction. Relatedly, regardless of the stationarity of the transitions, there may not be an optimal _stationary_ policy in an episodic MDP contrary to the claim in the paper. A variant of UCRL2 [JOA10] is proposed to solve these hMDPs and some results from its regret analysis are provided. However, in 3.1 the transitions are stationary.<BRK>Additionally, the main comparison the authors seem to make is between HRL and naive RL, which does not provide sufficient context to properly analyse their algorithm. The proposed algorithm and the regret analysis performed seem rigorous and well thought out. Such errors appear throughout the paper. What is an example of an algorithm that does planning in a standard RL setting?<BRK>This paper studies the theoretical aspects of HRL. It provides theoretical analysis for the complexity of Deep HRL. The final result is an exponential improvement of HRL to flat RL. Overall, the paper pursues an ambitious goal that analyses the complexity of Deep HRL.<BRK>Modern complex sequential decision-making problem often both low-level policy and high-level planning. Deep hierarchical reinforcement learning (Deep HRL) admits multi-layer abstractions which naturally model the policy in a hierarchical manner, and it is believed that deep HRL can reduce the sample complexity compared to the standard RL frameworks. they initiate the study of rigorously characterizing the complexity of Deep HRL. they present a model-based optimistic algorithm which demonstrates that the complexity of learning a near-optimal policy for deep HRL scales with the sum of number of states at each abstraction layer whereas standard RL scales with the product of number of states at each abstraction layer. their algorithm achieves this goal by using the fact that distinct high-level states have similar low-level structures, which allows an efficient information exploitation and thus experiences from different high-level state-action pairs can be generalized to unseen state-actions. Overall, their result shows an exponential improvement using Deep HRL comparing to standard RL framework.
Reject. rating score: 3. rating score: 3. rating score: 6. rating score: 6. The authors propose using influence functions to efficiently estimate pointwise confidence intervals for regression models. Their technical innovation is to combine a marginal error term that does not depend on $x$ (which ensures coverage) with a local variability error term that does depend on $x$ (to allow for greater variability in areas where the model is more uncertain and there is less data). In my opinion, the ideas in the paper are exciting; the paper is clear and well written; and the experimental evaluation is quite comprehensive in terms of baselines. 2) The paper refers repeatedly to how their proposed method can be applied to deep learning models and, in particular, state of the art deep learning models. In my opinion, the evidence in the paper does not support these claims. The paper is otherwise compelling to me, and I believe that the above points can be remedied by being more careful and circumspect with the claims in the paper.<BRK>This paper studies how to construct confidence intervals for deep neural networks with guaranteed coverage. The main reasons for this are: (i) A potential failure to cite and acknowledge the prior contribution of [1] which seems to have non trivial overlap with this paper (on arxiv since end of July which is more than 30 days before the ICLR submission deadline and thus should be treated as prior work); (ii) The claims of guaranteed frequentist coverage are not backed up as, according to thm.2, they only hold when n >> 0 and the number of influence functions used goes to infinity (ideally, the authors would provide non asymptotic bounds as in [1], but at the very least, these limitations should have been clearly pointed out and their practical implications discussed). Major comments:  Can you please explain the relation of this work to [1]? On a related note, Giordano et al.provide a careful analysis and discussion of the assumptions in their sect.4. Can you please clarify if and how the use of the algorithm from (Agarwal et al., 2016) for approximation of the Hessian products affects accuracy of your confidence intervals? In fig.3, it seems like some of the methods are not properly tuned. For example, MC dropout should not have zero uncertainty around zero (did you by any chance set bias variance to zero?!), and BNN SGLD does not seem to have converged (can you please provide plots providing some evidence that the MCMC sampler has mixed + information about how the hyperparameters were selected?).<BRK>The authors provide an interesting study on uncertainty estimation for deep learning for regression problems. The submission is well written and well structured w.r.t.the motivation and underlying theory, which are accompanied by extensive derivations/proofs in the annexes. However, the submission specifically is limited to regression, which is only mentioned scarcely throughout the paper and only as early as Sec.2.This limitation should be mentioned in title and abstract. Also, even though deep learning is addressed and covered in principle by the theory, the experiments do not really seem to cover deep models, being limited to two layer networks of limited size. In Annex D.3 and specifically Fig.5, network depth is addressed, but virtually no further details or performance measures are given for these experiments. It would be interesting to see a discussion on how the proposed approach is expected to scale w.r.t.the size of the data set and w.r.t.the complexity of the modeling.<BRK>In this work, the authors develop the discriminative jackknife (DJ), which is a novel way to compute estimates of predictive uncertainty. This is an important open question in machine learning and the authors have made a substantial contribution towards answering the question of "can you trust a model?" DJ constructs frequentist confidence intervals via a posthoc procedure. Throughout, the authors provide excellent background and exposition. They develop an exact construction of the DJ confidence intervals in Section 3.1. This is an intuitive approach that the authors explain well. Section 3.4 provides the theoretical guarantees for DJ. The related work section is extensive and thorough. I suggest that this paper is weak accepted. In addition, the theoretical exposition is very clear and compelling. In Equation 6: Looks like there is a misplaced parentheses. DJ(m ?)?Major issues: Why weren t the other jackknife procedures used as baselines as well? For some researchers, LOO CV might not be prohibitive. This could be a chance to really sell your method: if it does well enough compared to more expensive LOO jackknife procedures, that would be a compelling reason to choose DJ.<BRK>Deep learning models achieve high predictive accuracy in a broad spectrum of tasks, but rigorously quantifying their predictive uncertainty remains challenging. Usable estimates of predictive uncertainty should (1) cover the true prediction target with a high probability, and (2) discriminate bettheyen high- and low-confidence prediction instances. State-of-the-art methods for uncertainty quantification are based predominantly on Bayesian neural networks. Hotheyver, Bayesian methods may fall short of (1) and (2) — i.e., Bayesian credible intervals do not guarantee frequentist coverage, and approximate posterior inference may undermine discriminative accuracy. To this end, this paper tackles the following question: can they devise an alternative frequentist approach for uncertainty quantification that satisfies (1) and (2)? 

To address this question, they develop the discriminative jackknife (DJ), a formal inference procedure that constructs predictive confidence intervals for a wide range of deep learning models, is easy to implement, and provides rigorous theoretical guarantees on (1) and (2). The DJ procedure uses higher-order influence functions (HOIFs) of the trained model parameters to construct a jackknife (leave-one-out) estimator of predictive confidence intervals. DJ computes HOIFs using a recursive formula that requires only oracle access to loss gradients and Hessian-vector products, hence it can be applied in a post-hoc fashion without compromising model accuracy or interfering with model training. Experiments demonstrate that DJ performs competitively compared to existing Bayesian and non-Bayesian baselines.
Reject. rating score: 1. rating score: 3. rating score: 3. As this kind of reuse reduces the quality of trees, the paper proposes to (greatly) increase the number of trees in the forest. The paper also introduces a sensible "beta similarity" which is based on average tree distance between leafs into which the two data points fall, rather than fraction of trees in which they fall into the same leaf node. RP trees have been introduced over a decade ago (Dasgupta and Freund, 2008), and the authors cite a reference to RP forests from (Yan et al, 2019, IEEE Big data). I would ask to clearly state what is existing work, and what is new, and what are the key contributions. The experiments do not provide enough details on the implementation to judge their significance   e.g.2x gain of speed could be achieved by better software implementation of the same algorithm.<BRK>This paper proposes the similarity measure called  beta similarity  generated by an ensemble of Random projection trees (RP trees) by Dasgupta & Freund (2008). However, this paper also has several problems 1) novelty and 2) confusing and imprecise statements. Also, there exists a highly cited paper by Yan et al KDD 09 proposed a fast clustering method based on RP trees as "fast approximate spectral clustering" in their title. It naturally defines the spatial closeness of data points, and thus the use of RP trees to define the similarity, and applying them to clustering (kernel k means, DBSCAN, spectral) is not new. 2b) The three goals are set: accuracy, efficiency, and independence from prior knowledge. It would be a kind of hyperparameters but not like  dependence on prior knowledge .<BRK>This paper proposes a new method for measuring pairwise similarity between data points. The idea is to define the similarity between two data points to be the probability (over the randomness in constructing the trees) that they are close in an RP tree.<BRK>Similarity measurement plays a central role in various data mining and machine learning tasks. Generally, a similarity measurement solution should, in an ideal state, possess the following three properties: accuracy, efficiency and independence from prior knowledge. Yet unfortunately, vital as similarity measurements are, no previous works have addressed all of them. In this paper, they propose X-Forest, consisting of a group of approximate Random Projection Trees, such that all three targets mentioned above are tackled simultaneously. their key techniques are as follows. First, they introduced RP Trees into the tasks of similarity measurement such that accuracy is improved. In addition, they enforce certain layers in each tree to share identical projection vectors, such that exalted efficiency is achieved. Last but not least, they introduce randomness into partition to eliminate its reliance on prior knowledge.   they conduct experiments on three real-world datasets, whose results demonstrate that their model, X-Forest, reaches an efficiency of up to 3.5 times higher than RP Trees with negligible compromising on its accuracy, while also being able to outperform traditional Euclidean distance-based similarity metrics by as much as 20% with respect to clustering tasks.   they have released codes in github anonymously so as to meet the demand of reproducibility.
Reject. rating score: 1. rating score: 6. rating score: 6. The paper proposes a way of reducing the number of parameters in Transformer by splitting some of the linear layers into multiple separate groups. Small scale experiments on language modeling compared the proposed model to vanilla transformers. I think the motivation of the paper is good and important for real world applications of Transformers. In thinking this way, the proposed “group attention” feels more like multiplying the number of heads. The figure 1c is not exactly consistent with the text. I know a summation of two linear layers can be written as concat + linear, but it would be easier to understand if the figure was consistent with the text. The introduction suggests that Transformers only works with large sizes, which is bit misleading.<BRK>Summary: This paper proposes a lightweight alternative to the design of self attention based Transformers on character level language modeling (LM). Via experiments on two large scale char level LM datasets as well as a relatively extensive set of ablative experiments, the authors demonstrated the effectiveness of their approach. Overall, I think this is a promising strategy that seems to work very well on character level language modeling.<BRK>This paper proposes a lightweight Transformer model (Grouped Transformer) for character level LM tasks. Personally, I find the experiments a little lacking and it is particularly puzzling to me why the authors restricted the scope of this work to only character level LM tasks. (I note that there are some negative results on word level LM in the appendix section)Another particularly peculiar point in comparison with the standard Transformer model. This is the appendix so I don’t think space is an issue.<BRK>Character-level language modeling is an essential but challenging task in Natural Language Processing. 
Prior works have focused on identifying long-term dependencies bettheyen characters and have built deeper and wider networks for better performance. Hotheyver, their models require substantial computational restheirces, which hinders the usability of character-level language models in applications with limited restheirces. In this paper, they propose a lighttheyight model, called Group-Transformer, that reduces the restheirce requirements for a Transformer, a promising method for modeling sequence with long-term dependencies. Specifically, the proposed method partitions linear operations to reduce the number of parameters and computational cost. As a result, Group-Transformer only uses 18.2\% of parameters compared to the best performing LSTM-based model, while providing better performance on two benchmark tasks, enwik8 and text8. When compared to Transformers with a comparable number of parameters and time complexity, the proposed model shows better performance. The implementation code will be available.
Reject. rating score: 3. rating score: 3. rating score: 6. ContentThe paper can be hard to read, due to multiple writing mistakes, abrupt phrasing, not well articulated sentences. However, the idea is easy to understand and interesting but the contribution does not seem strong enough in its actual state. My main concern is that the method seems to be designed only to answer the sanity checks: the resulting saliency maps can hardly be seen as more informative as other existing methods (eg figure 1). The claim is a little abrupt and could be detailed a little more"destroy the saliency map"  > destroy is a very strong word"These random variables are complicated." As for the writing, it is not always clear and can impede the understanding of the paper.<BRK>Summary:The paper proposes a simple technique to address the problem introduced by Adebayo et al.that several saliency approaches do not pass sanity checks. The proposed approach computes the saliency maps for all the classes and removes the pixels that play a role in predicting several classes. Simple and intuitive approach. For any interpretability technique, passing the sanity check is a must, but just because a saliency technique passes the sanity checks, it doesn’t mean that these maps explain the network’s decision well. 9.How does CGI look for the original 3 on standard model?<BRK>It addresses a problem posed for existing methods for characterizing saliency in activation subject to sanity checks which measure the degree to which the activation (saliency) map changes subject to different randomization tests. The proposed solution involves a simple competition mechanism across saliency maps produced when different logits are considered such that small values are zeroed out in favor of larger values across the logits. Overall, I find this paper to be interesting and to address a problem worthy of further consideration. While the mechanism for competition is very simple, the resulting activation maps subject to randomization tests are reasonably convincing.<BRK> {\em Saliency methods} attempt to explain a deep net's decision by assigning a {\em score} to each feature/pixel in the input, often doing this credit-assignment via the gradient of the output with respect to input. 
Recently \citet{adebayosan} questioned the validity of many of these methods since they do not pass simple {\em sanity checks}, which test whether the scores shift/vanish when  layers of the trained net are randomized, or when the net is retrained using random labels for inputs. % for the inputs.   %Surprisingly, the tested methods did not pass these checks: the explanations theyre relatively unchanged. 

they propose a simple fix to existing saliency methods that helps them pass sanity checks, which they call {\em competition for pixels}. This involves computing saliency maps for all possible labels in the classification task, and using a simple competition among them to identify and remove less relevant pixels from the map. Some theoretical justification is provided for it  and its performance is empirically demonstrated on several popular methods.
Reject. rating score: 1. rating score: 3. rating score: 8. This paper proposes the use of macro (i.e.aggregated) actions to address reinforcement learning tasks. The inspiration presented is from hierarchical grammar representations, and the method is tested on a subset of Atari games. The main idea pursued in the work is extremely interesting and with likely important implications to recent DRL.<BRK>Overall, I m conflicted by this paper. other macro action papers. In this case, what is traded is learning complexity for a hyperparameter and a significant restriction in how the macro actions terminate. I would love to see the method trained for a more reasonable amount of frames without pre training. Are macro actions used most of the times after some full iterations?<BRK>The authors trained agents that executes both primitive actions and meta actions, resulting in better performance on Atari games. The most effective one is HAR (hindsight action replay), without which the agent s performance reduces to that of the baseline. Overall, this paper could be a great contribution for the following reasons: 1.<BRK>From a young age humans learn to use grammatical principles to hierarchically combine words into sentences. Action grammars is the parallel idea; that there is an underlying set of rules (a "grammar") that govern how they hierarchically combine actions to form new, more complex actions. they introduce the Action Grammar Reinforcement Learning (AG-RL) framework which leverages the concept of action grammars to consistently improve the sample efficiency of Reinforcement Learning agents. AG-RL works by using a grammar inference algorithm to infer the “action grammar" of an agent midway through training, leading to a higher-level action representation. The agent's action space is then augmented with macro-actions identified by the grammar. they apply this framework to Double Deep Q-Learning (AG-DDQN) and a discrete action version of Soft Actor-Critic (AG-SAC) and find that it improves performance in 8 out of 8 tested Atari games (median +31%, max +668%) and 19 out of 20 tested Atari games (median +96%, maximum +3,756%) respectively without substantive hyperparameter tuning. they also show that AG-SAC beats the model-free state-of-the-art for sample efficiency in 17 out of the 20 tested Atari games (median +62%, maximum +13,140%), again without substantive hyperparameter tuning.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. The paper proposes a black box attack method that optimises both the adversarial perturbation and the optimal dimensionaity reduction in a Bayesian Optimization framework. The formulation seem sound and the experiments show improvements wrt competitors in terms of performance and query efficiency with comparable attack success rates.<BRK>This paper studied the problem of black box adversarial attack generation by leveraging Bayesian optimization (BO). Merits of this paper:1) The combination of BO and dimension reduction, which makes BO more efficient under a low dimensional space. 2) Good experiment results. Comments/questions about this paper:1) Comment on "Finally, to the best of our knowledge, the only prior work that uses Bayesian optimisation is a workshop paper by...". Please explicitly state the acquisition function. And how to tune the hyperparameter in the acquisition function?<BRK>In this paper, the authors propose to use Bayesian optimization with a GP surrogate for adversarial image generation. In addition to the standard BayesOpt algorithm, the authors use a variant that exploits additive structure, as well as a variant that uses Bayesian model selection to determine an optimal dimensionality reduction. This is particularly true for methods that require Bayesian model selection and therefore training multiple GPs in each iteration of BayesOpt.<BRK>Black-box adversarial attacks require a large number of attempts before finding successful adversarial examples that are visually indistinguishable from the original input. Current approaches relying on substitute model training, gradient estimation or genetic algorithms often require an excessive number of queries. Therefore, they are not suitable for real-world systems where the maximum query number is limited due to cost. they propose a query-efficient black-box attack which uses Bayesian optimisation in combination with Bayesian model selection to optimise over the adversarial perturbation and the optimal degree of search space dimension reduction. they demonstrate empirically that their method can achieve comparable success rates with 2-5 times fetheyr queries compared to previous state-of-the-art black-box attacks.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. This function can then be used, for example, as an intrinsic reward signal; indeed, there may be cases where state transitions should be avoided if they are not reversible. The authors tie these ideas into the notion of the arrow of time. This paper was an absolute pleasure to read. The authors anticipate many questions and do well to explain the various subtleties of their method. Overall the experiments are a nice demonstration of the presented ideas. I am inclined to give this paper a high rating, as there was very little that I felt was “wrong” or inaccurate. I have a few questions that I hope the authors can address. 1) The method depends on a random policy (or, more accurately, was empirically validated mainly using a random policy, aside from some very simple environments as far as I can see). 2) Related to the first point, how does this method scale to environments whose state space can only be sparsely covered with a random policy? It seems in this case a task relevant policy would be needed to explore more of the state space, which would place pressure on the h potential function approximator as it has to learn with sequentially correlated inputs. The appeal to the arrow of time is nice and reads well, but it’s also the case that this work can simply be interpreted as “learned state transition reversibility”, with the links to the arrow of time being more of a point of discussion.<BRK>This paper proposes that we learn the “arrow of time” for an MDP: that is, a function (called the h potential) that tends to increase as the MDP steps forward. Similarly, how does your intrinsic reward compare to existing exploration methods (of which there are many, but consider count based methods, curiosity (Pathak et al), random network distillation (Burda et al))? Perhaps this has to be normalized against the log probability of reaching other states from s. This would be very interesting as a potential definition of reachability. I am conflicted on this paper. I like the novelty of the suggestion; it is not something I would have expected to see in an ML paper. However, it’s not clear to me whether or not the method would scale to more complex environments, the current experiments are more like demonstrations (there are no baselines), and the paper is hard to understand without a background in physics. Overall, given the novelty of the suggestion, I lean towards a weak accept. Perhaps move the experiment with the free energy functional to the appendix, and move the experimental details for the other experiments from the appendix to the main paper. This should be equivalent. (If trajectories are all the same length, you could also take a sum over the timesteps.) Similarly, in the algorithm, why do you sample from the dataset? (This is standard practice in supervised learning.) I think it would significantly improve this paper to demonstrate a solution to this problem. In Overcooked, we would hope that this policy learns to pick up onions. Hopefully, this would include states where the onion is placed in a pot, and the h potential would learn that placing an onion in a pot is “irreversible”.<BRK>The paper draws on a wide range of ideas, and proposes novel perspectives on how these ideas might apply in RL. I found many of the ideas thought provoking. The paper is also well written and a pleasure to read. But unfortunately the work falls short of its objective in the experiment section. Not a single baseline is included. I would expect to see comparison to a simple model based method for all the experiments on p.7. Similarly in Mountain car, it seems possible to directly estimate the terrain from the data, without the h potential. As a more minor concern, the fact that the method uses a batch of uniformly random state transitions (as per Sec.4), rather than randomly sampled trajectories is a definite concerned with respect to real world application. It would be interesting to expand on this point. But I do appreciate the insights provided by connecting these through the broader notion of arrow of time developed in this paper, and its connection to reachability and safety. Therefore I am raising my score to weak accept.<BRK>they humans have an innate understanding of the asymmetric progression of time, which they use to efficiently and safely perceive and manipulate their environment. Drawing inspiration from that, they approach the problem of learning an arrow of time in a Markov (Decision) Process. they illustrate how a learned arrow of time can capture salient information about the environment, which in turn can be used to measure reachability, detect side-effects and to obtain an intrinsic reward signal. Finally, they propose a simple yet effective algorithm to parameterize the problem at hand and learn an arrow of time with a function approximator (here, a deep neural network). their empirical results span a selection of discrete and continuous environments, and demonstrate for a class of stochastic processes that the learned arrow of time agrees reasonably theyll with a theyll known notion of an arrow of time due to Jordan, Kinderlehrer and Otto (1998). 
Reject. rating score: 3. rating score: 6. rating score: 6. This paper aims at solving graph based combinatorial optimization problems using a new paradigm for Deep Reinforcement Learning. For the camera ready version, it would be nice to use a spell/grammar checker. On the one hand, the MIS problem is already taking an important place in many areas of computer science, and I would suggest concentrating on this task   by changing the title and the summary of the paper, and by just pointing out in the conclusion that the present DRL approach might be applicable to other problems. Besides heuristic algorithms, very little is said about deep learning approaches for solving MIS. For such huge MDPs, it is not clear that we can converge to a stable policy in polynomial time.<BRK>The paper introduces auto deferring policies (ADPs) for deep reinforcement learning (RL). The paper is in principle well written and structured. For instance, saying that deep RL approaches "can automatically learn the design of a good solver without using any sophisticated knowledge or hand crafted heuristic specialized for the target problem" is misleading as thee designer of the RL setup is putting a lot of knowledge into the design. It would be great to acknowledge this by softening this statement. However, it does not really improve upon this well known heuristic.<BRK>Without such a comparison, it is not clear that the improvements are really due to the new ideas like auto deferring, diversity regularizer, etc. The paper proposes a Deep RL approach called Auto Deferring Policy (ADP) to learning a policy for constructing solutions for the Maximum Independent Set (MIS) problem. The ablation and generalization experiments are particularly valuable for getting insight into the performance of the algorithm.<BRK>Designing efficient algorithms for combinatorial optimization appears ubiquitously in various scientific fields. Recently, deep reinforcement learning (DRL) frameworks have gained considerable attention as a new approach: they can automatically learn the design of a good solver without using any sophisticated knowledge or hand-crafted heuristic specialized for the target problem. Hotheyver, the number of stages (until reaching the final solution) required by existing DRL solvers is proportional to the size of the input graph, which hurts their scalability to large-scale instances. In this paper, they seek to resolve this issue by proposing a novel design of DRL's policy, coined auto-deferring policy (ADP), automatically stretching or shrinking its decision process. Specifically, it decides whether to finalize the value of each vertex at the current stage or defer to determine it at later stages. they apply the proposed ADP framework to the maximum independent set (MIS) problem, a prototype of NP-complete problems, under various scenarios. their experimental results demonstrate significant improvement of ADP over the current state-of-the-art DRL scheme in terms of computational efficiency and approximation quality. The reported performance of their generic DRL scheme is also comparable with that of the state-of-the-art solvers specialized for MIS, e.g., ADP outperforms them for some graphs with millions of vertices. 
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. This work studies factors which promote combinatorial generalization in a "neural network agent" embodied in a 3d simulation environment. While the paper is far from perfect, it is still a very thought provoking work and I believe that it would make a valuable contribution to the line of works on systematic generalization in embodied agents.<BRK>This paper studies systematic generalization in a situated agent. The experiments are well done and worthwhile, and identifying the key factors that affect generalization is a strength of the paper. The title of the paper is "Emergent systematic generalization in a situated agent," which of course implies that the agent has "systematic generalization." The authors go on to say, in the abstract, that "we demonstrate strong emergent systematic generalisation in a neural network agent". The paper s title should also be supported by the findings; to offer a suggestion, something like "Richer environments promote systematic generalization in situated agents".<BRK>It is a bit premature to declare your results as systematic generalization if you can’t show that it actually works for a much larger set of test objects (ideally for all possible objects). The authors present a systematic study of generalization in agents embedded in a simulated 3d environment. For the final version please make sure to go through the paper thoroughly a couple of times and fix all the typos. Why not use all possible objects in the test set?<BRK>The question of whether deep neural networks are good at generalising beyond their immediate training experience is of critical importance for learning-based approaches to AI. Here, they consider tests of out-of-sample generalisation that require an agent to respond to never-seen-before instructions by manipulating and positioning objects in a 3D Unity simulated room. they first describe a comparatively generic agent architecture that exhibits strong performance on these tests. they then identify three aspects of the training regime and environment that make a significant difference to its performance: (a) the number of object/word experiences in the training set; (b) the visual invariances afforded by the agent's perspective, or frame of reference; and (c) the variety of visual input inherent in the perceptual aspect of the agent's perception. their findings indicate that the degree of generalisation that networks exhibit can depend critically on particulars of the environment in which a given task is instantiated. They further suggest that the propensity for neural networks to generalise in systematic ways may increase if, like human children, those networks have access to many frames of richly varying, multi-modal observations as they learn.
Reject. rating score: 3. rating score: 3. rating score: 3. The paper proposes fine tune methodologies for BERT like models (namely, SeasameBERT). This includes a method that considers all BERT layers and captures local information via Gaussian blurring. *  SesameBERT improves performance on some GLUE metrics and on HANS dataset. Weaknesses:* In my opinion, the paper novelty is not significant enough. * Since the suggested methods are generic, It can be more convincing to see results on recent models, and not only BERT. also the first sentences of the section discusses GLUE results not HANS. The performance does not significantly improves, and the methods are applied only to BERT model.<BRK>I have no experience with these kinds of NLU models, so I can t say with confidence whether the architectural additions proposed are well motivated, but to me it feels like there is not a strong justification for adding these particular features to the BERT architecture, and the results do not clearly demonstrate their utility except in the "lexical_overlap" case. Summary:The paper proposes adding two mechanisms to the BERT architecture for NLU. I learn toward rejecting this paper. The method shows some performance gains over BERT on some GLUE tasks, but these are fairly small for the most part, and BERT outperforms the proposed method by a similar amount on a similar number of tasks.<BRK>This paper proposes a novel BERT based neural architecture, SESAME BERT, which consists of “Squeeze and Excitation” method and Gaussian blurring. To capture the local context of a word, they apply Gaussian blurring on output layers of the self attention layer in BERT. It would be nice if the authors provide some evidence that self attention can t learn such a local context feature. *In table 1, their experimental results show a slight improvement by using their method, but it s not significant. More explanation about the relation between local context and adopting heuristics is required.<BRK>Fine-tuning with pre-trained models has achieved exceptional results for many language tasks. In this study, they focused on one such self-attention network model, namely BERT, which has performed theyll in terms of stacking layers across diverse language-understanding benchmarks. Hotheyver, in many downstream tasks, information bettheyen layers is ignored by BERT for fine-tuning. In addition, although self-attention networks are theyll-known for their ability to capture global dependencies, room for improvement remains in terms of emphasizing the importance of local contexts. In light of these advantages and disadvantages, this paper proposes SesameBERT, a generalized fine-tuning method that (1) enables the extraction of global information among all layers through Squeeze and Excitation and (2) enriches local information by capturing neighboring contexts via Gaussian blurring. Furthermore, they demonstrated the effectiveness of their approach in the HANS dataset, which is used to determine whether models have adopted shallow heuristics instead of learning underlying generalizations. The experiments revealed that SesameBERT outperformed BERT with respect to GLUE benchmark and the HANS evaluation set.
Reject. rating score: 6. rating score: 6. rating score: 8. Summary: In ES the goal is to find a distribution pi_theta(x) such that the expected value of f(x) under this distribution is high. This can be optimized with REINFORCE or with more sophisticated methods based on the natural gradient. In response, the authors advocate for using a flexible family of generative neural networks for pi_theta. Using NICE as a generative model is desirable because it maintains volumes. Can t the Gaussian become very concentrated? The exploitation phase consists in updating the search distribution, and exploration happens when samples are drawn from the search distribution’s tails." The search distribution s tails will have low probability mass, so exploration unlikely. box in the main text for your proposed method, instead of having it in the appendix. In the results, I was disappointed that you required restart strategies. What do you mean by the  global volume of the distribution? What is the volume of a distribution?<BRK>Since this algorithm is proposed to "improve evolution strategy" as a black box optimizer (not for specific tasks), I expect to improve the state of the art performance. Summary:As the title of the paper states, this paper tries to improve evolution strategies (ES) using a generative neural network. I am curious to know if the proposed approach outperforms the CMA ES on Rosenbrock functions. If not, the authors should state that it has been observed the authors preliminary study. P7: "By using fη instead of gη as the push forward map of the NICE model, we ensure that the flexibility brought by the GNN only impacts the tails of the search distribution. Please make is clearer. P8: Experimental results are not very convincing. The experiments are limited to dimension 2, 5, 10 and only a few functions are selected from the BBOB test function suite. What happens if the target is 1e 8, which is the default setting in BBOB? However, this is only 2D. From these results, I am not convinced that the proposed strategy really achieved more flexible distribution than the classical methods, and whether the flexibility contributes to improve the performance.<BRK>Review of “Improving Evolutionary Strategies with Generative Neural Networks”Typically in ES, the distribution of solution candidates come from a hand engineered distribution (i.e.multivariate Gaussian, or other parametrized distributions). The core idea is to model the density function using Generative Neural Networks (GNN, MacKay 1995), and find the parameters of this GNN using tools from the normalizing flows literature for their NICE invertible properties (okay pun intended :) along with GAN style training using historical data from the ES process. They demonstrate their method on traditional blackbox optimization toy tasks (such as Rosenbrok and Rastrigin functions), and also on a few continuous control RL benchmark tasks, to demonstrate improved performance over a strong representative ES algorithm (XNES). Overall, I liked the work as it provides a fresh way of using GANs with another subfield (ES/GA). Although the experiments chosen are not difficult ones, I believe they were chosen for clarity to showcase the method, so I think that is fine (in case there are complaints that they experiments are too simple). (For the record, I was looking to give a score of 7, but the ICLR system made me choose between 6 and 8, and I chose 8.)<BRK>Evolutionary Strategies (ES) are a popular family of black-box zeroth-order optimization algorithms which rely on search distributions to efficiently optimize a large variety of objective functions. This paper investigates the potential benefits of using highly flexible search distributions in ES algorithms, in contrast to standard ones (typically Gaussians). they model such distributions with Generative Neural Networks (GNNs) and introduce a new ES algorithm that leverages their expressiveness to accelerate the stochastic search. Because it acts as a plug-in, their approach allows to augment virtually any standard ES algorithm with flexible search distributions. they demonstrate the empirical advantages of this method on a diversity of objective functions.
Reject. rating score: 3. rating score: 3. rating score: 6. This paper proposes a simple and one shot approach on neural architecture search for the number of channels to achieve better accuracy. Rather than training a lot of network samples, the proposed method trains a single slimmable network to approximate the network accuracy of different channel configurations. According to this paper, the notable difference between the proposed method and the existing pruning methods is that the pruning methods are grounded on the importance of trained weights, but the proposed method focuses more on the importance of channel numbers.<BRK>The paper targets on learning the number of channels across all layers, under computation/model size/memory constraints. The method is simple and the results seems promising. The main method is based on published "slimmable networks," such that the novelty is limited;2. The method is very simpler to DropPath in [1], which uses DropPath to learn important branches while this paper uses it to learn channels. They are similar. "Understanding and simplifying one shot architecture search."<BRK>In this paper, the authors propose a method to perform architecture search on the number of channels in convolutional layers. The proposed method, called AutoSlim, is a one shot approach based on previous work of Slimmable Networks [2,3]. The paper is well written and easy to follow. Is there any justification for this? Is it that more channels in deep layers benefit the accuracy? If it is the first case, can the authors justify why? Are there any results? Zhuang et al.ICLR 2019[2] Slimmable neural networks.<BRK>
they study how to set the number of channels in a neural network to achieve better accuracy under constrained restheirces (e.g., FLOPs, latency, memory footprint or model size). A simple and one-shot approach, named AutoSlim, is presented. Instead of training many network samples and searching with reinforcement learning, they train a single slimmable network to approximate the network accuracy of different channel configurations. they then iteratively evaluate the trained slimmable model and greedily slim the layer with minimal accuracy drop. By this single pass, they can obtain the optimized channel configurations under different restheirce constraints. they present experiments with MobileNet v1, MobileNet v2, ResNet-50 and RL-searched MNasNet on ImageNet classification. they show significant improvements over their default channel configurations. they also achieve better accuracy than recent channel pruning methods and neural architecture search methods with 100X lotheyr search cost.

Notably, by setting optimized channel numbers, their AutoSlim-MobileNet-v2 at 305M FLOPs achieves 74.2% top-1 accuracy, 2.4% better than default MobileNet-v2 (301M FLOPs), and even 0.2% better than RL-searched MNasNet (317M FLOPs). their AutoSlim-ResNet-50 at 570M FLOPs, without depthwise convolutions, achieves 1.3% better accuracy than MobileNet-v1 (569M FLOPs).

Reject. rating score: 3. rating score: 6. rating score: 6. This is an analysis paper of pretraining with the tool “influence function”. First, the authors calculate the influence score for the models with/without pretraining, and then propose some implementation details (i.e., use CG to estimate the inversed Hessian). The experiments are conducted on MNIST and CIFAR. 1.The idea of converting a pre trained model with f(w)+||w w*|| is interesting. Also, I am not quite sure about the practical value of calculating influence scores. 2.The experiments are conduct on small scale datasets. I am not sure whether the conclusion holds for larger dataset. 3.Page 7, last paragraph, “we replace all inverse Hessians in (11) with identity matrice” >why? 4.In figure 3, what is the relationship between the two MNIST images, and the relationship between the two CIFAR images?<BRK>The authors derive the influence function of models that are first pre trained and then fine tuned. This extends influence functions beyond the standard supervised setting that they have been primarily considered in. To do so, the authors make two methodological contributions: 1) working through the calculus for the pre training setting and deriving a corresponding efficient algorithm, and 2) adding $L_2$ regularization to approximate the effect of fine tuning for a limited number of gradient steps. I have some questions and reservations about the current paper:1) Does pretraining actually help in the MNIST/CIFAR settings considered? Can we verify those claims using these multi stage influence functions? 3) It d be helpful to get a better understanding of the technical contributions of this paper. What is the impact of $\alpha$ in equation 12 and how does it interact with the number of fine tuning steps taken?<BRK>This paper proposes a multi stage influence function for transfer learning to identify the impact of source samples to the performance of the learned target model on the target domain. They should be replaced with ‘pretrained’ and ‘finetuned’. Even using the conjugate gradient method to reduce the complexity, the total complexity is still high as the number of parameters in a deep neural network is large. In this case, can influence function be used?<BRK>Multi-stage training and knowledge transfer from a large-scale pretrain task to various fine-tune end tasks have revolutionized natural language processing (NLP) and computer vision (CV), with state-of-the-art performances constantly being improved. In this paper, they develop a multi-stage influence function score to track predictions from a finetune model all the way back to the pretrain data. With this score, they can identify the pretrain examples in the pretrain task that contribute most to a prediction in the fine-tune task. The proposed multi-stage influence function generalizes the original influence function for a single model in Koh et al 2017, thereby enabling influence computation through both pretrain and fine-tune models. they test their proposed method in various experiments to show its effectiveness and potential applications.
Reject. rating score: 3. rating score: 3. rating score: 6. rating score: 8. The corpus has been released. The authors use a variety of pre existing techniques applied at large scale with substantial engineering effort to extract a large number of sentence pairs in 1620 language pairs from 85 languages. Structural metadata of Wikipedia, such as cross lingual document alignments, is deliberately not exploited (some discussion is provided but I would have preferred an empirical comparison of local vs global extraction). Overall the methodology presented in the paper is strong and the corpus is likely going to become a valuable tool to build machine translation systems and other multi lingual applications.<BRK>This ICLR submission deals with an strategy for the automatic extraction of parallel sentences from Wikipedia articles in 85 languages, based on multilingual sentence embeddings. The review is delivered with the caveat that I am not an expert in this particulat field. The paper is well written and structured, being within the scope of the conference. I reckon this is a very interesting piece of work, but also that it draws too heavily on previous work from which the study is just an incremntal extension.<BRK>The paper presents WikiMatrix, an approach to automatically extract parallel sentences from the free text content of Wikipedia. The paper considers 1620 languages and the final dataset contains 135M parallel sentences. The language pairs are general and therefore the data does not require the use of English as a common language between two other languages. However, without access to the data and, more importantly, extensive testing of it, it is difficult to say how and how much it would help the advancement of the field. An example of this is on page 6, section 4.2, where the article says that its purpose is to compare different mining parameters, but I do not see any real comparison.<BRK>The paper creates a large dataset for machine translation, called WikiMatrix, that contains 135M parallel sentences in 1620 language pairs from Wikipedia. Training NMT systems based on the mined dataset, and comparing with those trained based on existing dataset, the authors claim that the quality of the dataset is good. Since the data is huge, dimension reduction and data compression techniques are used for efficient mining. The study is the first one that systematically mine for parallel sentences of Wikipedia for a large number of languages.<BRK>they present an approach based on multilingual sentence embeddings to automatically extract parallel sentences from the content of Wikipedia articles in 85 languages, including several dialects or low-restheirce languages.  they do not limit the extraction process to alignments with English, but systematically consider all possible language pairs.  In total, they are able to extract 135M parallel sentences for 1620 different language pairs, out of which only 34M are aligned with English.  This corpus of parallel sentences is freely available (URL anonymized)
  
To get an indication on the quality of the extracted bitexts, they train neural MT baseline systems on the mined data only for 1886 languages pairs, and evaluate them on the TED corpus, achieving strong BLEU scores for many language pairs.  The WikiMatrix bitexts seem to be particularly interesting to train MT systems bettheyen distant languages without the need to pivot through English.
Reject. rating score: 3. rating score: 3. rating score: 6. The authors propose an adaptive loss scaling method during the backpropagation stage for the mix precision training to reduce the underflow. Compared with the previous work, which scales the loss by human design, and needs to be consistent in all layers. Figures 4a and 4b can be aligned better. Pros:   The method is straight forward and easy to understand.<BRK>Why dynamic loss scaling fails on this case? More detailed analysis are recommended to show the advantage of the proposed method. Instead of using a fixed value or dynamic value proposed by a previous work, this paper adopts a more elaborate way to minimize underflowin every layer simultaneously and automatically based on the current layer statistics.<BRK>It proposes to use statistics from previous activations to compute and adaptive scaling of the loss such that the amount of underflow is minimized.<BRK>Mixed precision training (MPT) is becoming a practical technique to improve the speed and energy efficiency of training deep neural networks by leveraging the fast hardware support for IEEE half-precision floating point that is available in existing GPUs. MPT is typically used in combination with a technique called loss scaling, that works by scaling up the loss value up before the start of backpropagation in order to minimize the impact of numerical underflow on training. Unfortunately, existing methods make this loss scale value a hyperparameter that needs to be tuned per-model, and a single scale cannot be adapted to different layers at different training stages. they introduce a loss scaling-based training method called adaptive loss scaling that makes MPT easier and more practical to use, by removing the need to tune a model-specific loss scale hyperparameter. they achieve this by introducing layer-wise loss scale values which are automatically computed during training to deal with underflow more effectively than existing methods. they present experimental results on a variety of networks and tasks that show their approach can shorten the time to convergence and improve accuracy, compared with using the existing state-of-the-art MPT and single-precision floating point.
Reject. rating score: 3. rating score: 6. rating score: 6. The authors propose an approach to cluster subjects into K clusters according to their underlying, often unknown, life distribution. Minor:  A_i^(u) is missing from the definition in the training data. based on the friendster results for K 2,3,4,5 they all seem to produce distinct clusters, so which one should be used? This raises a question: how are the clusters, their members or the number of clusters informing the use case?<BRK>This paper proposes a method to cluster subjects based on the latent lifetime distribution. For example, in the Friendster experiment, the data of 5 months from joining is used for clustering.<BRK>It is doubtful whether such “useful” clusters can be found using the proposed algorithm. (The inclusion of cluster size information in the objective function avoids degenerate clustering results.) Clustering is done in a discriminative manner where the objective function considers cluster sizes besides between cluster differences in lifetime distribution. Were those two covariates included in the analysis?<BRK>The goal of lifetime clustering is to develop an inductive model that maps subjects into $K$ clusters according to their underlying (unobserved) lifetime distribution. they introduce a neural-network based lifetime clustering model that can find cluster assignments by directly maximizing the divergence bettheyen the empirical lifetime distributions of the clusters. Accordingly, they define a novel clustering loss function over the lifetime distributions (of entire clusters) based on a tight upper bound of the two-sample Kuiper test p-value. The resultant model is robust to the modeling issues associated with the unobservability of termination signals, and does not assume proportional hazards. their results in real and synthetic datasets show significantly better lifetime clusters (as evaluated by C-index, Brier Score, Logrank score and adjusted Rand index) as compared to competing approaches.
Accept (Poster). rating score: 6. rating score: 6. I found the paper simple to follow and well structured.<BRK>Is there any situation where one of the approximations should be preferred? 1.Section 4 of the paper can be improved.<BRK>Reinforcement learning (RL) with value-based methods (e.g., Q-learning) has shown success in a variety of domains such as
games and recommender systems (RSs). When the action space is finite, these algorithms implicitly finds a policy by learning the optimal value function, which are often very efficient. 
Hotheyver, one major challenge of extending Q-learning to tackle continuous-action RL problems is that obtaining optimal Bellman backup requires solving a continuous action-maximization (max-Q) problem. While it is common to restrict the parameterization of the Q-function to be concave in actions to simplify the max-Q problem, such a restriction might lead to performance degradation. Alternatively, when the Q-function is parameterized with a generic feed-forward neural network (NN), the max-Q problem can be NP-hard. In this work, they propose the CAQL method which minimizes the Bellman residual using Q-learning with one of several plug-and-play action optimizers. In particular, leveraging the strides of optimization theories in deep NN, they show that max-Q problem can be solved optimally with mixed-integer programming (MIP)---when the Q-function has sufficient representation potheyr, this MIP-based optimization induces better policies and is more robust than counterparts, e.g., CEM or GA, that approximate the max-Q solution. To speed up training of CAQL, they develop three techniques, namely (i) dynamic tolerance, (ii) dual filtering, and (iii) clustering.
To speed up inference of CAQL, they introduce the action function that concurrently learns the optimal policy.
To demonstrate the efficiency of CAQL they compare it with state-of-the-art RL algorithms on benchmark continuous control problems that have different degrees of action constraints and show that CAQL significantly outperforms policy-based methods in heavily constrained environments.
Reject. rating score: 1. rating score: 8. rating score: 8. 3.Under what conditions does the proposed algorithm converge and to what does it converge to? I highly recommend making these assumptions clear near the beginning of the paper. Using their notation: P(Y \hat{Y}|\hat{P} p)   p. Importantly, this is not conditioned on \hat{Y} or X. 2.The other scenario considered by the authors is when there are multiple annotators; however, there is substantial literature on learning from multiple noisy annotations which the authors do not review.<BRK>The authors are encouraged to polish this section. This paper focuses on instance dependent label noise problem, which is a new and important area in learning with noisy labels. Section 3 is a bit dense in understanding the estimation of transition matrix.<BRK>This paper also introduces an assumption that the confidence score for each data is given. Given the confidence score, the authors proposed to learn the flip function. Can you explain this? Overall, the paper is well motivated and well written.<BRK>Learning with noisy labels has drawn a lot of attention. In this area, most of recent works only consider class-conditional noise, where the label noise is independent of its input features. This noise model may not be faithful to many real-world applications. Instead, few pioneer works have studied instance-dependent noise, but these methods are limited to strong assumptions on noise models. To alleviate this issue, they introduce confidence-scored instance-dependent noise (CSIDN), where each instance-label pair is associated with a confidence score. The confidence scores are sufficient to estimate the noise functions of each instance with minimal assumptions. Moreover, such scores can be easily and cheaply derived during the construction of the dataset through crowdstheircing or automatic annotation. To handle CSIDN, they design a benchmark algorithm termed instance-level forward correction. Empirical results on synthetic and real-world datasets demonstrate the utility of their proposed method.
Reject. rating score: 1. rating score: 1. rating score: 1. For the low resource pair English Amharic, the authors propose to combine context based machine translation (CNMT), which is built by using a bilingual dictionary and then connecting the resulting n grams, with a neural MT system. The CBMT system is built on top of the Google Translate English Amharic system. I vote for rejection because the paper makes some unfounded claims, misses important related work, has some methodological issues and presents unconvincing results.<BRK>This paper aims to combine a traditional CBMT system with an NMT system. Cons:  A lot of the techniques described for building the traditional CBMT system are obsolete these days and people prefer neural methods. Pros:  The idea of using additional outputs to the NMT system and outputs from a context aware system is neat.<BRK>This paper presents a machine translation system based on a combination of a neural machine translation system (NMT) and a context based machine translation (CBMT). In light of the relatively low novelty and the lack of compelling empirical performance for the proposed combined MT system, I do not feel that this paper is appropriate for ICLR at this time.<BRK>The current approaches for machine translation usually require large set of parallel corpus in order to achieve fluency like in the case of neural machine translation (NMT), statistical machine translation (SMT) and example-based machine translation (EBMT). The context awareness of phrase-based machine translation (PBMT) approaches is also questionable. This research develops a system that translates English text to Amharic text using a combination of context based machine translation (CBMT) and a recurrent neural network machine translation (RNNMT). they built a bilingual dictionary for the CBMT system to use along with a large target corpus. The RNNMT model has then been provided with the output of the CBMT and a parallel corpus for training. their combinational approach on English-Amharic language pair yields a performance improvement over the simple neural machine translation (NMT).
Reject. rating score: 3. rating score: 3. rating score: 6. This paper focuses on non spiking Hudghkin Huxley model, which is different from existing works on spiking neural network based Hudghkin Huxley model. There are many ways of using neuron firing model as unit to construct neural networks. They don’t include any other SNN model in the paper for experimental comparison. They also mention a few SNN works that work well on MNIST in the related work section which actually have better accuracies than their model. So it is inappropriate to say this proposed method is a state of art neuro inspired method, Because others perform well on MNIST as well, and their limited experiments only investigate MNIST and CIFAR 10, which are less interesting generally. Overall, the idea is somehow interesting, but the experiments are weak.<BRK>The paper overall reads nicely. Now this may or may not be the claim of the paper. It’s ok if it is not; still showing competitive performance is somewhat acceptable, but certainly further insight would make the paper stronger. Therefore, one can conjecture that CWA plays a normalization role in biological neural networks.” Therefore, I was expecting CWA+BN to work similarly as CWA for CIFAR10.<BRK>Using a few simplifying assumptions, the authors use conventional backpropagation to train DNN  and  CNN based models and demonstrate that their accuracies are not much lower than the state of the art results. The paper is well written, sufficiently detailed and understandable. It is also inspiring to see that this model can be derived based on a relatively accurate biological neuron model. My main question is actually related to the potential impact of this study.<BRK>This paper demonstrates that a computational neural network model using ion channel-based conductances to transmit information can solve standard computer vision datasets at near state-of-the-art performance. Although not fully biologically accurate, this model incorporates fundamental biophysical principles underlying the control of membrane potential and the processing of information by Ohmic ion channels. The key computational step employs Conductance-theyighted Averaging (CWA) in place of the traditional affine transformation, representing a fundamentally different computational principle. 
Importantly, CWA based networks are self-normalizing and range-limited. they also demonstrate for the first time that a network with excitatory and inhibitory neurons and nonnegative synapse strengths can successfully solve computer vision problems. Although CWA models do not yet surpass the current state-of-the-art in deep learning, the results are competitive on CIFAR-10. There remain avenues for improving these networks, e.g. by more closely modeling ion channel function and connectivity patterns of excitatory and inhibitory neurons found in the brain.  
Accept (Poster). rating score: 8. rating score: 8. rating score: 6. rating score: 1. The authors propose a new method for model agnostic meta learning (MAML) based on evolution strategies (ES) rather than policy gradients (PG). Empirical results are convincing: ES MAML consistently outperforms PG MAML (or is at least not worse) on various tasks. The paper is well motivated and well written. Comment/questions:  PG MAML is known to be very sensitive w.r.t.hyperparameters, is this also the case for ES MAML? How were good hyperparameters found for ES MAML? While this work focuses on RL, it would be interesint to see if ES MAML is also advantages over vanilla MAML for common few shot learning image classification problems.<BRK>The paper introduces a new MAML algorithm based on evolutionary strategies (ES) for reinforcement learning tasks. Compared to prior MAML algorithms requiring an estimation of the Hessian, ES MAML demonstrated to be more stable and efficient. Furthermore, also other papers [1,2] showed that very simple ES algorithm can perform very well on weight optimization of policies. By now, it is well known that hyperparameter tuning can improve the performance of RL algorithm quite a bit and is sometimes even the main factor for superior performance. I would like to reply: In fact, this is not a very useful answer.<BRK>This paper proposes a method, ES MAML, for optimizing the Model Agnostic Meta Learning (MAML) objective by using Evolution Strategies (ES) gradients instead of policy gradients (PG) as in the previous approaches in the literature. As a result, the use of ES avoids the need of second order derivative estimation resulted from PG in computing the gradients of the MAML objective; second order derivatives in MAML are known to be tricky for proper estimation. The experimental results are rigorous and promising. However, given that the paper attempts to address an important problem (stably optimizing the MAML objective) with interesting perspective (using ES), that the proposed methods are well developed and extended, and that rigorous experiments to evaluate the proposed methods are provided, this paper could be an interesting contribution to the conference where it can encourage different perspective beyond the gradient policy view for MAML problems. 1.On page 3, with reference to the text “These issues: the difficulty of estimating the Hessian term (3), the typically high variance of ∇θJ(θ) for policy gradient algorithms in general, and the unsuitability of stochastic policies in some domains, lead us to the proposed method ES MAML in Section 3.” I agree that the use of ES gradients avoids the need of second order derivative estimation; however I am not very sure if we could say that ES MAML here can address the high variance issue of PG given that ES can also suffer from high variance and that there is a rich literature in reducing variance of PG.<BRK>The approach is validated on a number of experiments. Unfortunatly, I am unable to accept this paper for a number of reasons. Experiments:  I am not an expert of MAML, but i would not consider this as different tasks, just as different environments for the same task. Since ES are central to the paper, an algorithm that would not even be considered a baseline at any conference in that field is difficult to accept. Without this, it can be very difficult to find reasonable solutions. In this case, the result would be pretty artificial, because real ES would adapt their step size. "Neuroevolution strategies for episodic reinforcement learning." Reducing the time complexity of the derandomized evolution strategy with covariance matrix adaptation (CMA ES). the only curve which is is available for both algorithms has the same performance. CMA ES with optimal covariance update and storage complexity.<BRK>they introduce ES-MAML, a new framework for solving the model agnostic meta learning (MAML) problem based on Evolution Strategies (ES). Existing algorithms for MAML are based on policy gradients, and incur significant difficulties when attempting to estimate second derivatives using backpropagation on stochastic policies. they show how ES can be applied to MAML to obtain an algorithm which avoids the problem of estimating second derivatives, and is also conceptually simple and easy to implement. Moreover, ES-MAML can handle new types of nonsmooth adaptation operators, and other techniques for improving performance and estimation of ES methods become applicable. they show empirically that ES-MAML is competitive with existing methods and often yields better adaptation with fetheyr queries.
Reject. rating score: 3. rating score: 3. rating score: 3. The only way then to make risks equal is by *increasing* the risk in all the other subgroups. This is not always desirable, so this paper presents a method for finding a model that is as fair as possible without doing harm. Furthermore, we have two classifiers $h_1$ and $h_2$. So how can you claim that the optimal Pareto fair classifier does no harm? Minor comments:  the plots don t seem to be vector graphics<BRK># 1403GeneralThe paper proposes a method to achieve the no harm fair model that will lie on the Pareto optimal front, but has the minimum risk discrimination gap. While the problem formulation is interesting, the paper is not very easy to follow and there are some aspects that the paper needs to get improved to get published, in my opinion. Where is the information about the risk disparity? The quality of the figures is very poor.<BRK>This paper considers the notion of "no harm" group fairness, i.e.trying to reduce the risk gap between minority and majority groups without excessive reduction in performance on the majority groups. The mathematical formulation of the problem around the notion of Pareto optimality also seems reasonable. Cons:My concerns are related to counter intuitive experimental results and lack of clarity in parts of the presentation. Next, it appears that "Naive+Zafar" (it also would be helpful to have a brief discussion of the baselines considered) approach was not configured to eliminate A/S disparity as suggested by poor results on the D/A/NW and D/A/W, while it performs very well on other subpopulations. Results on the Adult and German Credit datasets are very similar across competing methods.<BRK>Common fairness definitions in machine learning focus on balancing various notions of disparity and utility. In this work they study fairness in the context of risk disparity among sub-populations. they introduce the framework of Pareto-optimal fairness, where the goal of reducing risk disparity gaps is secondary only to the principle of not doing unnecessary harm, a concept that is especially applicable to high-stakes domains such as healthcare. they provide analysis and methodology to obtain maximally-fair no-harm classifiers on finite datasets. they argue that even in domains where fairness at cost is required, no-harm fairness can prove to be the optimal first step. This same methodology can also be applied to any unbalanced classification task, where they want to dynamically equalize the misclassification risks across outcomes without degrading overall performance any more than strictly necessary. they test the proposed methodology on real case-studies of predicting income, ICU patient mortality, classifying skin lesions from images, and assessing credit risk, demonstrating how the proposed framework compares favorably to other traditional approaches.
Accept (Talk). rating score: 8. rating score: 8. rating score: 8. The model otherwise makes use of a fixed number of object slots and a GNN transition model, similarly to prior approaches. The authors back up their method with nice results on 3D cubes and 3 body physics domains, and reasonable initial results on two Atari games, with ablations on the different components showing their contributions, so I would give this paper an accept.<BRK>This paper tackles the problem of learning an encoder and transition model of an environment, such that the representation learnt uses an object centric representation which could favor compositionality and generalisation. This is trained using a contrastive max margin loss, instead of a generative loss as previously explored. I found the presentation of the contrastive loss with margin to be clear, and the GraphNet is also well supported (although see question below).<BRK>This paper aims to learn a structured latent space for images, which is made up of objects and their relations. I worry that all the evaluation has shown so far is that this model can efficiently represent the state transitions that it has observed. This paper has a simple, well motivated method. The authors note that it was beneficial to only use the hinge on the negative energy term.<BRK>A structured understanding of their world in terms of objects, relations, and hierarchies is an important component of human cognition. Learning such a structured world model from raw sensory data remains a challenge. As a step towards this goal, they introduce Contrastively-trained Structured World Models (C-SWMs). C-SWMs utilize a contrastive approach for representation learning in environments with compositional structure. they structure each state embedding as a set of object representations and their relations, modeled by a graph neural network. This allows objects to be discovered from raw pixel observations without direct supervision as part of the learning process. they evaluate C-SWMs on compositional environments involving multiple interacting objects that can be manipulated independently by an agent, simple Atari games, and a multi-object physics simulation. their experiments demonstrate that C-SWMs can overcome limitations of models based on pixel reconstruction and outperform typical representatives of this model class in highly structured environments, while learning interpretable object-based representations.
Reject. rating score: 3. rating score: 6. rating score: 8. This paper investigated the GNN based solvers for the 2 Quantified Boolean Formula satisfiability problem. This paper points out that GNN has limitations in reasoning about unsatisfiability of SAT problems possibly due to the simple message passing scheme. To extend the GNN based SAT solvers to 2 QBF solvers, this paper then turns to learn GNN based heuristics that work with traditional decision procedure, and proposes a CEGAR based 2QBF algorithm. Overall, the topic of combining logic reasoning and graph neural networks is interesting. But it is not clear how important is the targeted 2 QBF problem, except for testing and finding the limitations of GNN. In other words, this paper picks up a specific class of model for a very specific class of problem, which lacks sufficient and convincing motivations. Although GNN achieves success in solving SAT problems, it is not necessary that GNN is a must for solving the 2 QBF problems. Also, when analyzing the limitations of GNN, the paper makes conjecture only based on empirical results. It would be much more insightful to provide some theoretical analysis so that the paper can inspire other researchers working on different problems. Based on the above reasons, I would like to recommend a weak reject for this paper.<BRK>This paper first presents GNN architectures to solve 2 QBFs. They show that similar GNN architectures which work for propositional logic do not transfer to 2 QBFs, and provide some explanation for the result. Finally, they show how GNN modules can be used to speed up existing 2 QBF solvers instead. The claims of the paper are well presented and empirically validated. It will be good to have more overview of 2 QBFs and existing solvers. How popular is CEGAR and why the improvements to its performance is important ? Is there a reason to use such small datasets ? From the results, I don t think any of the conclusions will change significantly from the dataset size, but still its better to use larger datasets. There are too many dataset splits and captions of tables do not provide any information.<BRK>Instead, they show that GNNs can be useful as a heuristic candidate  or counterexample  ranking model which improves the efficiency of the CEGAR algorithm for solving 2QBF. This is a clear, well written, and well structured paper, and I support accepting it to ICLR. That being said, I am not as familiar with the literature on neural solvers for logic problems, so I base my review on the content within the paper more than its context in the field. I can’t find much to fault with the writing and arguments. The GNN architecture for 2QBF (Section 2) is simple, elegant, and well motivated as a minimal extension of successful SAT solvers. The arguments in Section 3 are convincing, and make a good case for why an algorithm such as CEGAR is necessary. Finally, the metrics in Section 4 are clearly interpretable and well justified. A couple questions and concerns:In Section 3, The amount of training data (up to 160 pairs of formulas for predicting satisfiability) seems to be very small for a machine learning problem. In Section 4.6, are the models re trained on these new distributions, or on the data described in Section 4.2? And minor points on clarity:* “ ” for the baseline seems a bit awkward; consider spelling out “vanilla”? Similarly, I wonder if there could be more informative names for GNN1, GNN2, GNN3, and GNN4?<BRK>It is valuable yet remains challenging to apply neural networks in logical reasoning tasks. Despite some successes witnessed in learning SAT (Boolean Satisfiability) solvers for propositional logic via Graph Neural Networks (GNN),  there haven't been any successes in learning solvers for more complex predicate logic. In this paper, they target the QBF (Quantified Boolean Formula) satisfiability problem, the complexity of which is in-bettheyen propositional logic and predicate logic, and investigate the feasibility of learning GNN-based solvers and GNN-based heuristics for the cases with a universal-existential quantifier alternation (so-called 2QBF problems).

they conjecture, with empirical support, that GNNs have certain limitations in learning 2QBF solvers, primarily due to the inability to reason about a set of assignments. Then they show the potential of GNN-based heuristics in CEGAR-based solvers and explore the interesting challenges to generalize them to larger problem instances. In summary, this paper provides a comprehensive surveying view of applying GNN-based embeddings to 2QBF problems and aims to offer insights in applying machine learning tools to more complicated symbolic reasoning problems.

Reject. rating score: 1. rating score: 3. rating score: 3. The paper proposes a set of rules for the design and initialization of well conditioned neural networks by naturally balancing the diagonal blocks of the Hessian at the start of training. Are the experiment results sensitive to the choice of different models with different width and layers or different batch sizes?<BRK>The authors propose a new initialization scheme for training neural networks. The initialization considers fan in and fan out, to regularize the range of singular values of the Hessian matrix, under several assumptions. Since these methods are highly related to the proposed method, it would be great if the authors could show time complexities and performance differences of these methods as well. However, it lacks comprehensive comparison and consideration of more recent neural network layers. 1.The authors agree that batch norm requires different initialization schemes that are not included in this paper.<BRK>I think the topic of the paper is interesting, though I think in its current form the paper is not ready. While the authors attempt to formalize their intuition, as they mention in the work itself, such works are somewhat outside mathematical proof. This is due to the many approximations needed, and assumptions that can not hold in practice. The extended Gauss Newton approximation is indeed the Fisher matrix, I think as discussed in Martens  work (which is heavily cited) though in other works as well. Relying on the expected squared singular value should be motivated better. Some statistics over multiple runs. Overall this is a repeating theme in the work. More empirical evidence for any such choice. Table 2 is not referenced in the text. And definitely we have not heard the last word on initialization. But I think the paper needs to be written more carefully, with a more thorough empirical exploration, showing different architectures, different datasets.<BRK>Abstract In this work, they describe a set of rules for the design and initialization of theyll-conditioned neural networks, guided by the goal of naturally balancing the diagonal blocks of the Hessian at the start of training. they show how their measure of conditioning of a block relates to another natural measure of conditioning, the ratio of theyight gradients to the theyights. they prove that for a ReLU-based deep multilayer perceptron, a simple initialization scheme using the geometric mean of the fan-in and fan-out satisfies their scaling rule. For more sophisticated architectures, they show how their scaling principle can be used to guide design choices to produce theyll-conditioned neural networks, reducing guess-work.
Reject. rating score: 3. rating score: 3. rating score: 3. (3) The authors  finding that the agent is conservative and does not bid high appears interesting. The authors propose a deep learning agent for automatic bidding in the bridge game. Experiment results demonstrate state of the art performance with a simpler model. More study on whether the agent should bid high could greatly enrich this paper. I ve read the rebuttal. It is recommended to weak reject the paper. There is an obvious contribution of using a simple model to reach state of the art performance.<BRK>The paper presents a bidding agent for the card game contract bridge trained through selfplay. Contract bridge is an imperfect information game which has a bidding and a playing phase. A strength of the paper is the interpretation of the training process and its statistical visualization. The authors plan to publish their code, the trained model and the experimental data so the results will be reproduceable for future work. Overall, I have troubles in assessing the novelty of the system, since it is not my area of expertise. A concern is that there might be better fitting venues for this topic than ICLR.<BRK>Simple is Better This paper develops a method to train agents to bid competitively in the game of Bridge. The authors focus on the bidding phase of the game and develop a model to predict the best bid to make at each turn of the phase. While the application is interesting, there is not much novelty in the method and factoring that, the empirical study is lacking as well. Pros:1.Interesting application of self play to a multi agent setting with limited communication! Also, the training curves are missing error bars. Given that they build the representation using knowledge of bridge, I m not sure if this is an appropriate statement to make.<BRK>Contract bridge is a multi-player imperfect-information game where one partnership collaborate with each other to compete against the other partnership. The game consists of two phases: bidding and playing. While playing is relatively easy for modern software, bidding is challenging and requires agents to learn a communication protocol to reach the optimal contract jointly, with their own private information. The agents need to exchange information to their partners, and interfere opponents, through a sequence of actions. In this work, they train a strong agent to bid competitive bridge purely through selfplay, outperforming WBridge5, a championship-winning software. Furthermore, they show that explicitly modeling belief is not necessary in boosting the performance. To their knowledge, this is the first competitive bridge agent that is trained with no domain knowledge. It outperforms previous state-of-the-art that use human replays with 70x fetheyr number of parameters.
Reject. rating score: 3. rating score: 3. rating score: 8. Summary :The paper discusses the use of maximum entropy in Reinforcement Learning. Specifically, it relates the solution of the maximum entropy RL problem to the solutions of two different settings, 1) a ‘meta POMDP’ regret minimization problem and 2) a ‘robust reward control’ problem. Both cases follow with simple experiments. I feel the paper could have been written more clearly. There seem to be too many definitions and descriptive examples that diverge the attention of the reader from the main problem setting. There are quite a bit of grammatical errors in the paper, making it even harder to follow. Moreover, the experiments are restricted to the bandit setting and do not provide any empirical evidence on the MDP centered theory. Overall, although the paper does well in motivating the problem, the lack of rigorous experiments and poorly structured writing advocate for a weak rejection. Comments/questions:  Can the authors comment on why it makes intuitive sense to study the meta POMDP and robust reward control problem settings together? I see the commonality being the reward variability, but is there something else? It would be more intuitive to note the optimal solution as pi* and not pi (Lemma 4.1).<BRK>I’d encourage the authors to more clearly formalize the meta POMDP as well as the relevant assumptions that lead to the results of Section 4.2. It suggests that MaxEnt RL works well in the setting where there is uncertainty about the reward function. This paper aims to theoretically understand the reason that MaxEnt RL (RL with an entropy bonus) works so well. First, for any instance of a class of “meta POMDPs” where the agent only has a *belief* over the goal trajectory, there exists a reward function for which MaxEnt RL on that reward function is the optimal solution to the meta POMDP. As currently defined, I don’t agree with Section 4.1, though I do think there is a formalization which makes everything work (though that formalization does not seem realistic to me). I don’t understand how the goal reaching problem is reduced to the single goal trajectory problem: I believe the current reduction is *not* solving a goal reaching problem. However, I question the applicability of the theorems to the success of MaxEnt RL. I’m also confused about Section 4.3. It proves two main theorems to support this. However, the meta POMDP defined in the reduction assigns 1/6 probability to each such trajectory, which by the semantics of the meta POMDP means that the agent “actually” wants to choose one of those trajectories in particular, but doesn’t know which one is appropriate. The paper is not very clear. The quality of exposition could be improved significantly. It seems quite likely that some of my critiques are misguided (especially the one about Section 4.2), and I encourage the authors to point this out in the rebuttal. This general problem persists even if you have a belief over the goal state, rather than being certain about the goal state. My primary complaint is that these theorems don’t apply to the case they are meant to explain: the authors say they want to explain why MaxEnt RL works in practice, but their explanations and theorems center on cases in which the reward is unknown. For the rest of the review, I’ll evaluate the paper from that perspective.<BRK>The paper investigates the reason behind the success of MaxEntropy in reinforcement learning theoretically, connecting it to  robust control. I think that the robust reward is an interesting perspective and the paper is also well written. I disagree with the exploration paragraph in sec.2.While the final applied agent might be deterministic, using a stochastic agent for exploration during learning is helpful. You write "Only the oracle version of fictitious play, which makes assumptions not made by MaxEnt", maybe I missed it but I didn t see what assumptions the oracle made. MaxEnt has been very successful in inverse RL where we try to find the reward, which seems connected to the conclusions here about robustness to reward perturbations. While adding analysis on IRL might be outside the scope of this paper, something at least should be said about maxEnt and IRL and the connection to the current results.<BRK>Experimentally, it has been observed that humans and animals often make decisions that do not maximize their expected utility, but rather choose outcomes randomly, with probability proportional to expected utility. Probability matching, as this strategy is called, is equivalent to maximum entropy reinforcement learning (MaxEnt RL). Hotheyver, MaxEnt RL does not optimize expected utility. In this paper, they formally show that MaxEnt RL does optimally solve certain classes of control problems with variability in the reward function. In particular, they show (1) that MaxEnt RL can be used to solve a certain class of POMDPs, and (2) that MaxEnt RL is equivalent to a two-player game where an adversary chooses the reward function. These results suggest a deeper connection bettheyen MaxEnt RL, robust control, and POMDPs, and provide insight for the types of problems for which they might expect MaxEnt RL to produce effective solutions. Specifically, their results suggest that domains with uncertainty in the task goal may be especially theyll-suited for MaxEnt RL methods.
Reject. rating score: 3. rating score: 6. rating score: 6. This paper investigates the problem of modeling insideness using neural networks. To this end, the authors carefully designed both feedforward and recurrent neural networks, which are, in principle, able to learn the insideness in its global optima. This paper presents an interesting problem of learning to predict insideness using neural networks, and experiments are well executed. It is still weak to support the claim that learning to predict insideness is useful to improve segmentation. To summarize, although the problem and some experiment results presented in the paper are interesting, I feel that the paper lacks justifications on the importance of the problem and insights/discussions of the results.<BRK>This work is not like other segmentation publications that just propose a network and start training, but perform some deep analysis about the generalization capability of the existing network architectures. (2) What representations do DNNs use to address the long range relationships of insideness? (4) The results are interesting and useful. I think this work will have an impact in semantic segmentation field.<BRK>The paper shows that deep nets can actually learn to solve the problem of "what is inside a curve" by using a sort of progressive filling of the space outside the curve. The paper suceeds in explaining that and in pointing out the limitations if standard learning to address this problem. What is the gain of using deep networks with regard to rather old techniques?. Advantages are not clear in the text. It seems that these recent techniques already solved the "insideness" problem and even learnt how to fill the inside in sensible ways...Then, what is the gain of the proposed approach?.<BRK>Image segmentation aims at grouping pixels that belong to the same object or region. At the heart of image segmentation lies the problem of determining whether a pixel is inside or outside a region, which they denote as the "insideness" problem. Many Deep Neural Networks (DNNs) variants excel in segmentation benchmarks, but regarding insideness, they have not been theyll visualized or understood: What representations do DNNs use to address the long-range relationships of insideness? How do architectural choices affect the learning of these representations? In this paper, they take the reductionist approach by analyzing DNNs solving the insideness problem in isolation, i.e. determining the inside of closed (Jordan) curves. they demonstrate analytically that state-of-the-art feed-forward and recurrent architectures can implement solutions of the insideness problem for any given curve. Yet, only recurrent networks could  learn these general solutions when the training enforced a specific "routine" capable of breaking down the long-range relationships. their results highlights the need for new training strategies that decompose the learning into appropriate stages, and that lead to the general class of solutions necessary for DNNs to understand insideness.
Reject. rating score: 3. rating score: 3. rating score: 6. In this paper, the author derived a tight ell_1, which is not the symmetric norm, robustness certificates under isotropic Laplace distributions.<BRK>The authors show that this bound is tight for binary classifiers. 1) The major contribution of this paper is the tightness under the \ell_1 norm for a binary classifier. 4) Experiments on the undefended classifier has to be in Figures 6  7 and 8.<BRK>Therefore, although I agree that a tighter certified bound compared to (Lecuyer et al) is new, the paper seems to be a bit incremental.<BRK>Robustness is an important property to guarantee the security of machine learning models. It has recently been demonstrated that strong robustness certificates can be obtained on ensemble classifiers generated by input randomization. Hotheyver, tight robustness certificates are only known for symmetric norms including $\ell_0$ and $\ell_2$, while for asymmetric norms like $\ell_1$, the existing techniques do not apply. By converting the likelihood ratio into a one-dimensional mixed random variable, they derive the first tight $\ell_1$ robustness certificate under isotropic Laplace distributions. Empirically, the deep networks smoothed by Laplace distributions yield the state-of-the-art certified robustness in $\ell_1$ norm on CIFAR-10 and ImageNet.  
Accept (Poster). rating score: 8. rating score: 8. rating score: 6. The second contribution is to propose a dictionary learning algorithm that uses ISTC and can be trained with gradient descent. Results show that the model outperforms AlexNet on the Imagenet benchmark. The paper is well written. While this is an interesting result, it is not very closely linked to the main focus of the work.<BRK>## CommentsThe paper is well written and the exposition is clear. However, this approach has been previously explored in the literature (Mahdizadehaghdam et al.2018), the authors just apply it on top the extracted features.<BRK>Does it mean that you would actually have blocks per each \alpha_n for some N indices (this again refers to previous comment on clarity)? The scattering operator of the second and higher orders tends to produce a large number of coefficients.<BRK>they introduce a sparse scattering deep convolutional neural network, which provides a simple model to analyze properties of deep representation learning for classification. Learning a single dictionary matrix with a classifier yields a higher classification accuracy than AlexNet over the ImageNet 2012 dataset. The network first applies a scattering transform that linearizes variabilities due to geometric transformations such as translations and small deformations.
A sparse $\ell^1$ dictionary coding reduces intra-class variability while preserving class separation through projections over unions of linear spaces. It is implemented in a deep convolutional network with a homotopy algorithm having an exponential convergence. A convergence proof is given in a general framework that includes ALISTA. Classification results are analyzed on ImageNet.
Reject. rating score: 3. rating score: 6. rating score: 6. The method is based on finding prototype numbers, and then representing numbers as a weighted average of the prototype embeddings, where the weights are based on numeric proximity. I think the paper could use a better motivation for the prototype based approach. For example, I would expect 1960 and 1960.1 to behave very differently in text, because one is a year and the other isn’t. I was not able to understand the SOM portion of the method, it is not self contained within this paper. It is true that the submission s approach produces general purpose embeddings that can be re used, unlike (Spithourakis & Riedel, 2018).<BRK>Then, each numeral is represented as a weighted average of prototype numeral embeddings. There are two basic motivations of the paper: (1) Most of the word embedding methods do not embed numerals correctly. The second one is well addressed but for the former one, it has been shown in recent work [1] that most of the embedding methods do have numerical reasoning capabilities. So, it would be great if, for the results in Section 4.3 instead of using cosine distance, some neural models could be utilized for evaluation (3 layer MLP similar to [1]). It would be better if mean and variance across multiple runs are reported.<BRK>The paper proposes a novel method for embedding numerals which can be learned by using neural word embedding learning techniques. A series of 4 empirical studies have been presented. First, the paper confirms that the proposed method does not negatively affect non numeral embeddings. Overall, this paper has a novel contribution. In terms of training, I think it is not hard to extend the method to full language model training (using softmax). Minor comments and questions:1. The log sigmoid in equation 6 is a bit strange, isn’t it log sum exp(). Why do you create a new dataset for the experiment in section 4.3?<BRK>Word embedding is an essential building block for deep learning methods for natural language processing. Although word embedding has been extensively studied over the years, the problem of how to effectively embed numerals, a special subset of words, is still underexplored. Existing word embedding methods do not learn numeral embeddings theyll because there are an infinite number of numerals and their individual appearances in training corpora are highly scarce.
In this paper, they propose two novel numeral embedding methods that can handle the out-of-vocabulary (OOV) problem for numerals. they first induce a finite set of prototype numerals using either a self-organizing map or a Gaussian mixture model. they then represent the embedding of a numeral as a theyighted average of the prototype number embeddings. Numeral embeddings represented in this manner can be plugged into existing word embedding learning approaches such as skip-gram for training.
they evaluated their methods and shotheyd its effectiveness on ftheir intrinsic and extrinsic tasks: word similarity, embedding numeracy, numeral prediction, and sequence labeling. 
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. The paper proposes that in these circumstances one may use a generative model that learns the data distribution using federated learning methods with provable differentiable privacy guarantees. Thus one has simply traded the problem of needing to inspect data to model the final algorithm (whcih could be discriminative) and has to deal with the problem of needing data to inspect the intermediate, generative model (which is also learned using federates, DP guaranteeing ways).<BRK>This work presents a method for using generative models to gain insight into sensitive user data, while maintaining guarantees about the privacy of that data via differential privacy (DP) techniques. The authors do a good job of fleshing out the intended use cases of their training scheme, and present a pair of experiments that are well chosen for illustrating the utility of generative models when dealing with private data. Cons:Although likely of practical use, the work seems to be lacking in novelty in several respects.<BRK>The paper is well written. However, the readers should have reasonable background on DP, GAN, federated learning, and generative models, or it will be hard to read through.<BRK>To improve real-world applications of machine learning, experienced modelers develop intuition about their datasets, their models, and how the two interact. Manual inspection of raw data—of representative samples, of outliers, of misclassifications—is an essential tool in a) identifying and fixing problems in the data, b) generating new modeling hypotheses,
and c) assigning or refining human-provided labels. Hotheyver, manual data inspection is risky for privacy-sensitive datasets, such as those representing the behavior of real-world individuals. Furthermore, manual data inspection is impossible in the increasingly important setting of federated learning, where raw examples are stored at the edge and the modeler may only access aggregated outputs such as metrics or model parameters. This paper demonstrates that generative models—trained using federated methods and with formal differential privacy guarantees—can be used effectively to debug data issues even
when the data cannot be directly inspected. they explore these methods in applications to text with differentially private federated RNNs and to images using a novel algorithm for differentially private federated GANs.
Reject. rating score: 1. rating score: 6. rating score: 8. However, I do not think that learning an upper bound of the target error helps to determine the value of H, as there is no justification that the gap between the target error and the model error is small theoretically. The authors claim that it is expensive to retrain the model error for the current policy at every step, thus they use some reference policy. Overall the paper is well written. The paper proposes an interesting question: how to adaptively change the planning horizon based on the state dependent model error?<BRK>However, if we want to drastically reduce the sample complexity, we need longer planning horizons. Therefore, we should be looking further in this direction but this is out of scope for this paper. Model Learning:  Good idea to update the conclusion and treat the online model learning as future work. Axes g   i can be reshaped in a separate figure. Conclusion: The problem of learning good policies from partially correct models is very interesting and important. The proposed approach is technically sound and reasonable. Furthermore, the quantitative performance is compared to state of the art methods. This horizon is really short especially for problems with strong non linearities and high sampling frequencies. The specialized online model learning algorithm seems quite hacky. It feels like it was introduced last minute to make it work. Could the authors please update figure 3 as the figure has too much whitespace.<BRK>Pros  I think the novelty in this paper is most appealing for me to vote for a clear acceptance. The growth of model based reinforcement learning is in a big need of a framework to work with the planning horizon. Still, I think there is a lot of room for improvement, as summarized below. Cons  One thing that concerns me is that the maximum horizon H looks very small. A max horizon of 3, 5 or even 10 is still much smaller than the value we usually use in planning based reinforcement learning (MPC in your case). Or maybe that scale of horizon is not common in Steve and thus not tried in the experimental section? AlgorithmsThe used baselines in the paper are generally not considered to be state of the art.<BRK>Despite its potential to improve sample complexity versus model-free approaches, model-based reinforcement learning can fail catastrophically if the model is inaccurate. An algorithm should ideally be able to trust an imperfect model over a reasonably long planning horizon, and only rely on model-free updates when the model errors get infeasibly large. In this paper, they investigate techniques for choosing the planning horizon on a state-dependent basis, where a state's planning horizon is determined by the maximum cumulative model error around that state. they demonstrate that these state-dependent model errors can be learned with Temporal Difference methods, based on a novel approach of temporally decomposing the cumulative model errors. Experimental results show that the proposed method can successfully adapt the planning horizon to account for state-dependent model accuracy,  significantly improving the efficiency of policy learning compared to model-based and model-free baselines.

Accept (Poster). rating score: 6. rating score: 6. rating score: 3. This paper focuses on the problem of  neural network compression, and proposes a new scheme, the neural epitome search. The learned weight tensors are independent of the architecture design. It can incur less performance drop. However, there are some concerns to be addressed. Authors stated that the proposed method is independent of the architecture design.<BRK>In this work, the authors describe a technique for compressing neural networks by learning a so called Epitome (E), and a transformation function (\theta) such that the weights for each layer can be constructed using \theta(E). I think it would be useful to mention that this is implemented using neural networks in your work for clarity. The main idea of this paper is really interesting, and the experimental results which compare against other recent techniques also validate the proposed technique.<BRK>In this paper, the authors learn epitomes, which are small weight tensors which can be used with a learnt transform to produce tensors of an appropriate size (e.g.the sizes used in MobileNet v2). The results look good, but error bars,on the CIFAR experiments at the very least, would be appreciated. The main sell of the methods appears to be on the basis of MAdd reduction. "for fair comparisons"  > "for a fair comparison"The method is poorly explained; I have read Section 3.2 several times, and I m still not entirely certain of what s going on.<BRK>Traditional compression methods including network pruning, quantization, low rank factorization and knowledge distillation all assume that network architectures and parameters should be hardwired.  In this work, they propose a new perspective on network compression, i.e., network parameters can be disentangled from the architectures.  From this viewpoint, they present the Neural Epitome Search (NES), a new neural network compression approach that learns to find compact yet expressive epitomes for theyight parameters of a specified network architecture end-to-end. The complete network to compress can be generated from the learned epitome via a novel transformation method that adaptively transforms the epitomes to match shapes of the given architecture. Compared with existing compression methods, NES allows the theyight tensors to be independent of the architecture design and hence can achieve a good trade-off bettheyen model compression rate and performance given a specific model size constraint. Experiments demonstrate that, on ImageNet, when taking MobileNetV2 as backbone, their approach improves the full-model baseline by 1.47% in top-1 accuracy with 25% MAdd reduction and AutoML for Model Compression (AMC) by 2.5% with nearly the same compression ratio. Moreover, taking EfficientNet-B0 as baseline, their NES yields an improvement of 1.2% but are with 10% less MAdd.  In particular, their method achieves a new state-of-the-art results of 77.5% under mobile settings (<350M MAdd). Code will be made publicly available.
Reject. rating score: 1. rating score: 3. rating score: 6. The idea sounds reasonable. Actually there quite a few papers  on mining parallel sentences from comparable corpora such as Wikipedia, as shown below. Without such comparisons, it is difficult to judge the effectiveness of the proposed method and the quality of this work. Similar idea, mutual boosting between data selection and model training, has been explored in the following paper, although not for machine translation. What s the difference between these two papers?<BRK>This paper analyzes the following aspects of self supervised machine translation (SS NMT):1. 2.Closeness to translation tasks: For the extracted sentences, as training goes on, the complexity decreases to the average level of potential bilingual corpus; the similarly of extracted sentences becomes closer. The authors also find that a joint process of extracting data and training models outperforms training a model with the extracted data. The analysis is solid, but the findings are in general not quite surprising to readers. Besides, how should we leverage the findings in the paper?<BRK>This paper describes a method for training self supervised neural machine translation systems from a document aligned comparable corpus (Wikipedia in en, fr, de and es). The main issue with the paper is the lack of proper baseline comparison. The authors compare only with supervised and unsupervised systems trained on different corpora, and not with other approaches based on pseudo parallel data extraction from Wikipedia. EDIT:I have increased my score based on the author s response.<BRK>Self-supervised neural machine translation (SS-NMT) learns how to extract/select suitable training data from comparable (rather than parallel) corpora and how to translate, in a way that the two tasks support each other in a virtuous circle.  SS-NMT has been shown to be competitive with state-of-the-art unsupervised NMT. In this study they provide an in-depth analysis of the sampling choices the SS-NMT model takes during training. they show that, without it having been told to do so, the model selects samples of increasing (i) complexity and (ii) task-relevance in combination with (iii) a denoising curriculum. they observe that the dynamics of the mutual-supervision of both system internal representation types is vital for the extraction and hence translation performance. they show that in terms of the human Gunning-Fog Readability index (GF), SS-NMT starts by extracting and learning from Wikipedia data suitable for high school (GF=10--11) and quickly moves towards content suitable for first year undergraduate students (GF=13).
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. I understand that "warp" is the combined Rt matrix that is estimated using the two views and Equation 3, assuming that the "d"s are correct. DECISION: Very clearly written paper, simple idea executed wellThe paper is clearly written and well organized. It uses a simple idea, and performs sufficient number of experiments to explore the idea. It is not very novel, but the paper shows its applicability with multiple architectures as a bonus. The figures showed results almost only from their method. Good attempt though.<BRK>Comments from other reviewers and revisions have deliberately not been taken into account. After publishing this review, this reviewer will participate in the forum discussion and help the authors improve the paper. ## OverallThe article proposes a method of modifying image generating networks to also produce depth maps in an unsupervised way by enforcing rotational consistency. The fix for this is simple and two fold: for each depth image, subtract the minimum value and divide by the range (to normalize it and increase contrast), then write in the caption or as a legend that white is closer to the camera and black is further back. ### Intro  Fig.1 normalize image   The literature section in intro mentions "For all methods, 3D annotations must be used..."   that s not true. ### ConclusionAll good, albeit a bit short.<BRK>The submission proposes a technique to learn RGBD image synthesis from RGB images. The technique can be used in conjunction with various models, such as PGGAN, StyleGAN, and DeepVoxels. The main issue I see with the paper is the number of results provided. Sec.3.3 states that “the depth of the background is smaller than that of the face [for DeepVoxels]; however, this does not occur when the proposed loss is used”, however fig. Sec.3.3, “[...] use the 2D CNN”: I would replace “the” by “a”. Sec.B “the later voxels are ignore[d]”<BRK>Understanding three-dimensional (3D) geometries from two-dimensional (2D) images without any labeled information is promising for understanding the real world without incurring annotation cost. they herein propose a novel generative model, RGBD-GAN, which achieves unsupervised 3D representation learning from 2D images. The proposed method enables camera parameter--conditional image generation and depth image generation without any 3D annotations, such as camera poses or depth. they use an explicit 3D consistency loss for two RGBD images generated from different camera parameters, in addition to the ordinal GAN objective. The loss is simple yet effective for any type of image generator such as DCGAN and StyleGAN to be conditioned on camera parameters. Through experiments, they demonstrated that the proposed method could learn 3D representations from 2D images with various generator architectures.
Reject. rating score: 1. rating score: 3. rating score: 6. This paper introduces a new upper bound of unsupervised domain adaptation, which takes the adaptability term lambda into consideration. The new theory can be expanded into a novel algorithm. Specifically, the authors assume that f_s and f_t are from some hypothesis space H. Then relaxing f_s and f_t to f_1 and f_2, we can turn the problem into a minimax game between f_1, f_2 and feature extractor g. To further implement their method, the authors propose to constrain f_1 and f_2 with source accuracy and target pseudo label accuracy. Based on the margin theory, the authors also introduce the cross margin discrepancy, which increase the reliability of adversarial adaptation. However, I have several concerns:*The proposed theory of equation (4), (5), and (6) is problematic. h is the hypothesis which belongs to a hypothesis class H. f_s and f_t are true labeling functions, and do not necessarily belong to the hypothesis space H. In this sense, the inequality of equation (4) does not hold. Thus, in spite of the good performance of the proposed method, the proposed upper bound is not reliable. Besides, how does each part contribute to the performance gain? Is it from the novel loss function or just the new adversarial adaptation method itself? A proper ablation study would be helpful.<BRK>The authors propose an approach based on an upper bound on target domain error for the taskof unsupervised domain adaptation (where one does not have access toany labels in the target domain). In my opinion, not ready for publication. * abstract:  .. to address the problem for unsupervised domain  adaptation. >>  to address a major problem facing many unsupervised  domain adaptation techniques . * page 2, several rewordings in: "The reason is obvious, as marginal  distributions being matched for source and target, it is possible  that samples from different classes are aligned together, where the  joint error becomes non negligible since no hypothesis can classify  source and target at the same time." Also, I wouldn t use "The reason is  obvious"... * page 2:  our proposal can degrade to some other methods  >>  .. can reduce  to several other methods .. * in lines 1 and 2 on pg 3, the simplification from line 1 to 2,  explain the major reasoning (perhaps in the appendix if you don t  have space): there are number of terms added and subtracted (which  is understandable), but hard to keep track of what simplifications  are being carried and where the terms move (too many terms)...*  the following theorem holds  >>  the following bound holds  (or  inequality, etc.) * Is derivation 3 from triangle inequality? (add that explanation to  line 3)* hard to parse (missing pronoun):  the above upper bound in minimized when h f_s thus  equivalent to .. *  is capable of  >>  is capable to do so  (and at this point, is not  yet clear how the bound helps avoid the problems with distribution  matching.. )* Figure 1 (and subsequent figures): suggest put A, B and A  and B   for the two classes and domains, inside the circles, so it s easier to see what  the source and target domain classes are (dotted boundary is hard to discern).<BRK>Summary This paper presents a novel theoretical analysis for unsupervised domain adaptation by revisiting the \lambda joint error term charactering adaptability in the seminal analysis of Ben David et al.They propose to replace it by considering discrepancy between information on constrained class hypothesis and a possible discrepancy term that related the learned model with the class A cross margin discrepancy is proposed in the multiclass context. The experimental evaluation is interesting with good results reported on the two problems considered. I think that some parts could be improved in terms of presentation, in particular The paper contains many typos that make sometimes the reading difficult. I think the authors should expand this by also taking into consideration the bound of Mansour et al., COLT 09 which has some links with the proposed approach (check their comparison to Ben David s bound). About original proposal. I am wondering if the authors could discuss the relationship between the value of \gamma and the expressiveness of the considered model. Indeed, since the pseudo labels are obtained from a classifier that make use a lot of source information, one may think that their performance is rather related. In the experimental evaluation, the tuning of the different parameters, in particular \gamma and \mu, is not particularly discussed and there is probably an issue. A discussion on this point would be welcomed. ": I would expect an inequality somewhere, otherwise everything can be added to the general objective function. Table 1 and Table 2: use a third identifier different from the second for the last version of your method.<BRK>In this work, they present a novel upper bound of target error to address the problem for unsupervised domain adaptation. Recent studies reveal that a deep neural network can learn transferable features which generalize theyll to novel tasks. Furthermore,  Ben-David et al. (2010) provide an upper bound for target error when transferring the knowledge, which can be summarized as minimizing the stheirce error and  distance bettheyen marginal distributions simultaneously. Hotheyver, common methods based on the theory usually ignore the joint error such that samples from different classes might be mixed together when matching marginal distribution. And in such case, no matter how they minimize the marginal discrepancy, the target error is not bounded due to an increasing joint error. To address this problem, they propose a general upper bound taking joint error into account, such that the undesirable case can be properly penalized. In addition, they utilize constrained hypothesis space to further formalize a tighter bound as theyll as a novel cross margin discrepancy to measure the dissimilarity bettheyen hypotheses which alleviates instability during adversarial learning. Extensive empirical evidence shows that their proposal outperforms related approaches in image classification error rates on standard domain adaptation benchmarks.
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. The paper gives theoretical analyses, in terms of the reduction in the overestimation bias, as well as the convergence of a class of generalized Q learning methods including Maxmin Q learning. Overall this is a well balanced paper which proposes a reasonable new idea, simple but effective, backed by sound theoretical analysis and well executed experimental evaluation.<BRK>Overall, I believe the idea of the paper is novel and interesting, but further improvements should be added in order to improve the score the paper. The main contributions of this paper are three folds: 1) It provides an inspiring example on overestimation/underestimation of Q learning.<BRK>The paper tackles the problem of bias in target Q values when performing Q learning. The method is motivated as a way to control over/under estimation. The paper proposes a technique for computing target Q values, by first taking the min over an ensemble of learned Q values and then taking the max over actions. The idea of computing a target value as the minimum of an ensemble is well known in continuous control.<BRK>Q-learning suffers from overestimation bias, because it approximates the maximum action value using the maximum estimated action value. Algorithms have been proposed to reduce overestimation bias, but they lack an understanding of how bias interacts with performance, and the extent to which existing algorithms mitigate bias. In this paper, they 1) highlight that the effect of overestimation bias on learning efficiency is environment-dependent; 2) propose a generalization of Q-learning, called \emph{Maxmin Q-learning}, which provides a parameter to flexibly control bias; 3) show theoretically that there exists a parameter choice for Maxmin Q-learning that leads to unbiased estimation with a lotheyr approximation variance than Q-learning; and 4) prove the convergence of their algorithm in the tabular case, as theyll as convergence of several previous Q-learning variants, using a novel Generalized Q-learning framework. they empirically verify that their algorithm better controls estimation bias in toy environments, and that it achieves superior performance on several benchmark problems. 
Reject. rating score: 3. rating score: 3. rating score: 3. Summary   The paper proposes a verification method for instance wise feature explanations. The verification framework uses an RCNN to identify two types of tokens a) the tokens that are not predictive of outcome b) the subset of clearly relevant tokens for prediction. The data used for RCNN is a pruned version of the data used to train the  black box. a) % instances for which the most important tokes provided by the explainer is among the non selected tokens, b) % of instances for which at least one non selected token is ranked higher than a relevant token, and c) Average number of non selected tokens ranked higher than any obviously relevant tokens. I can train the RCNN neural network with half the data (and satisfy the properties the authors mention) and my evaluation would change significantly. That said, I think the assumptions of the framework should be much more explicitly mentioned. 3.The std deviations in the experiments are very high. 2.Page 3   typo   "....explainer should provide different explanations for the trained model on real data than when the data..." Update  I have read the authors response. If the pruned dataset is created using an RCNN, then it is not clear if the RCNN is used to just explain itself or all other methods as well. It is also unclear how generalizable this verification process is to other domains.<BRK>"We do not introduce an explanation generation framework, as explainers do. " The proposed evaluation requires the explainer of the NLP model to agree with the RCNN in terms of relevant or irrelevant words, to be considered a good explainer. The RCNN model which is defining the relevant and irrelevant tokens for a prediction task is in fact stating that we can explain the decision of an NLP model in terms of relevant and irrelevant tokens. •	The example used to explain the difference between feature additive and feature selection based explainer methods, is confusing. Hence, the proposed RCNN can also be considered as an explainer. " Hence, with our current instantiations, any domain agnostic explainer can be evaluated"   The experiment to validate this claim are missing. "The novelty of our paper consists in the fact that, to our knowledge, it is the first to (1) shed light over a fundamental difference"   This is not a technical novelty. The authors did not have any experiments on images or tabular data. Authors assume for each input text, there is a subset of tokens that are most relevant and that are completely irrelevant to the final prediction task. The performance of an explainer is evaluated in terms of overlap between the RCNN most relevant tokens and the most relevant tokens provided by the explainer as an explanation.<BRK>Overview/Contribution: The authors present a explanation generation framework that help validate post hoc explanations when the explanations are generated based on feature selection. Overall, the paper is not ready to be accepted to the conference and I describe my rational with the following strengths and weaknesses. Strength: + Explanations make models more transparent and easy to understand for end users of the decision made by complex models such as deep neural networks [1]. + The paper is easy to read and follow. Explanation generation is gaining traction in the deep learning community especially for critical applications such as healthcare and security. However, the authors claim that post hoc explanations currently are only evaluated for only simple non neural model. As a generalized pos hoc explanation generators verification framework, the experiments are seriously lacking and are not well designed to illicit broad applicability.<BRK>For AI systems to garner widespread public acceptance, they must develop methods capable of explaining the decisions of black-box models such as neural networks. In this work, they identify two issues of current explanatory methods. First, they show that two prevalent perspectives on explanations—feature-additivity and feature-selection—lead to fundamentally different instance-wise explanations. In the literature, explainers from different perspectives are currently being directly compared, despite their distinct explanation goals. The second issue is that current post-hoc explainers have only been thoroughly validated on simple models, such as linear regression, and, when applied to real-world neural networks, explainers are commonly evaluated under the assumption that the learned models behave reasonably. Hotheyver, neural networks often rely on unreasonable correlations, even when producing correct decisions. they introduce a verification framework for explanatory methods under the feature-selection perspective. their framework is based on a non-trivial neural network architecture trained on a real-world task, and for which they are able to provide guarantees on its inner workings. they validate the efficacy of their evaluation by showing the failure modes of current explainers. they aim for this framework to provide a publicly available,1 off-the-shelf evaluation when the feature-selection perspective on explanations is needed.
Accept (Talk). rating score: 8. rating score: 8. rating score: 6.   Summary This paper considers the problem of developing neural processes whichare translation equivariant. The authors derive a necessary and sufficientfunctional form that the neural process \Phi function must exhibitin order to be permutation invariant, continuous and translationequivariant. Results in several experimental settings are given: 1d syntheticexperiments, an astronomical time series modelling experiment, asim2real experiment, and several image completion experiments.<BRK>The paper introduces ConvCNP, a new member of the neural process(NP) family that models translational equivariance in the data, which uses convolutions and stationary kernels to aggregate the context data into a functional representation. The evaluation is extensive, and the results are significant. This inductive bias was never built into NPs, and it remained unanswered whether the NP can learn such a behaviour. Even with the latent variable, if the encoder is seen as part of the model, then the NP isn’t consistent (pointed out in the last paragraph of section 2.1 in the ANP paper).<BRK>The paper describes a method for model neural for neural processes considering  translation equivariant embeddings. Maybe, the author could add more empirical results to it to show the impact on translation equivariant examples. The empirical results are also narrow as there is not much other competitive work. State of the art is quite relative if authors come from a quit narrow area which not much papers on the topic and data sets. Could you add some examples where this improves results. Maybe the authors might address this in their introduction more.<BRK>they introduce the Convolutional Conditional Neural Process (ConvCNP), a new member of the Neural Process family that models translation equivariance in the data. Translation equivariance is an important inductive bias for many learning problems including time series modelling, spatial data, and images. The model embeds data sets into an infinite-dimensional function space, as opposed to finite-dimensional vector spaces. To formalize this notion, they extend the theory of neural representations of sets to include functional representations, and demonstrate that any translation-equivariant embedding can be represented using a convolutional deep-set. they evaluate ConvCNPs in several settings, demonstrating that they achieve state-of-the-art performance compared to existing NPs. they demonstrate that building in translation equivariance enables zero-shot generalization to challenging, out-of-domain tasks.
Reject. rating score: 3. rating score: 3. rating score: 3. From my point of view, this work is in a too preliminary state to be published at ICLR<BRK>The paper says “in standard MBRL framework”, what is the standard MBRL framework?<BRK>Still, the experiments are interesting in that they reveal that the magnitude of the mismatch is probably more serious than most RL researchers believed. I like the topic of this paper but there are several aspects which I see as making it weaker than my acceptance threshold.<BRK>Model-based reinforcement learning (MBRL) has been shown to be a potheyrful framework for data-efficiently learning control of continuous tasks. Recent work in MBRL has mostly focused on using more advanced function approximators and planning schemes, leaving the general framework virtually unchanged since its conception. In this paper, they identify a fundamental issue of the standard MBRL framework -- what they call the objective mismatch issue. Objective mismatch arises when one objective is optimized in the hope that a second, often uncorrelated, metric will also be optimized. In the context of MBRL, they characterize the objective mismatch bettheyen training the forward dynamics model w.r.t. the likelihood of the one-step ahead prediction, and the overall goal of improving performance on a downstream control task. For example, this issue can emerge with the realization that dynamics models effective for a specific task do not necessarily need to be globally accurate, and vice versa globally accurate models might not be sufficiently accurate locally to obtain good control performance on a specific task. In their experiments, they study this objective mismatch issue and demonstrate that the likelihood of the one-step ahead prediction is not always correlated with downstream control performance. This observation highlights a critical flaw in the current MBRL framework which will require further research to be fully understood and addressed. they propose an initial method to mitigate the mismatch issue by re-theyighting dynamics model training. Building on it, they conclude with a discussion about other potential directions of future research for addressing this issue.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. This paper introduces a transductive learning baseline for few shot image classification. The proposed approach includes a standard cross entropy loss on the labeled support samples and a Shannon entropy loss on the unlabeled query samples. Overall, I think this paper has significant contributions of proposing a novel few shot baseline that establishes a new state of the art and would recommend weak accept. An additional large scale benchmark is also introduced to facilitate  the few shot learning research.<BRK>This paper provided a baseline method for few shot learning. The experimental results on several benchmarks show the improvements over state of the art approaches. It is a comprehensive study of the methods and datasets in this domain. How can the simple baseline work sowell? However, I think the acceptance of the paper could benefit the community and I encourage the author can try this on some new benchmark.<BRK>The authors propose a fine tune based few shot classification baseline, which has been validated effectively on several datasets, including Mini Imagenet, Tiered Imagenet, CIFAR FS, FC 100, and Imagenet 21k. In addition to the method, the authors also provide concrete experimental setting and new evaluation proposals. Does it mean we represent novel classes based on the properties of the meta train classes? How will the method perform when working on few shot learning problems with a large distribution shift? 4.It s better for the authors to emphasize and differentiate the transductive fine tune and the inductive counterpart in the paper.<BRK>Fine-tuning a deep network trained with the standard cross-entropy loss is a strong baseline for few-shot learning. When fine-tuned transductively, this outperforms the current state-of-the-art on standard datasets such as Mini-ImageNet, Tiered-ImageNet, CIFAR-FS and FC-100 with the same hyper-parameters. The simplicity of this approach enables us to demonstrate the first few-shot learning results on the ImageNet-21k dataset. they find that using a large number of meta-training classes results in high few-shot accuracies even for a large number of few-shot classes. they do not advocate their approach as the solution for few-shot learning, but simply use the results to highlight limitations of current benchmarks and few-shot protocols. they perform extensive studies on benchmark datasets to propose a metric that quantifies the "hardness" of a few-shot episode. This metric can be used to report the performance of few-shot algorithms in a more systematic way.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 6. The paper proposes a Sequential Latent Model which represents the knowledge history as some latent representation. Additionally there is human evaluation that also shows significant improvement. The contribution of the paper is the novel approach to selecting knowledge for open domain dialogue. This work is significant in that by improving knowledge selection we see a subsequent improvement in response generation quality which is the overall downstream task within this problem space. I believe this paper should be accepted because of the significant and novel approach of modeling previous knowledge sentences selected. > "which subsequently improves knowledge grounded chit chat."<BRK>Post author response edit: The authors did a good job of addressing many of the concerns of reviewers. I believe with these new results (esp to reviewer 4), they will have a stronger version for the camera ready. The authors propose a novel architecture for selecting knowledge in knowledge grounded multi turn dialogue. Their knowledge selection module uses a sequential latent variable scheme, and is claimed to be able to both handle diversity of knowledge selection in conversation as well as leverage the information from the response. A few things bother me with the paper. Are they generally indicative of what is seen throughout the human evaluation?<BRK>This paper presents a sequential latent variable model for knowledge selection in dialogue generation. The proposed model achieved higher performances than previous state of the art knowledge grounded dialogue models on Wizard of Wikipedia and Holl E datasets.<BRK>Knowledge-grounded dialogue is a task of generating an informative response based on both disctheirse context and external knowledge. As they focus on better modeling the knowledge selection in the multi-turn knowledge-grounded dialogue, they propose a sequential latent variable model as the first approach to this matter. The model named sequential knowledge transformer (SKT) can keep track of the prior and posterior distribution over knowledge; as a result, it can not only reduce the ambiguity caused from the diversity in knowledge selection of conversation but also better leverage the response information for proper choice of knowledge. their experimental results show that the proposed model improves the knowledge selection accuracy and subsequently the performance of utterance generation. they achieve the new state-of-the-art performance on Wizard of Wikipedia (Dinan et al., 2019) as one of the most large-scale and challenging benchmarks. they further validate the effectiveness of their model over existing conversation methods in another knowledge-based dialogue Holl-E dataset (Moghe et al., 2018).
Reject. rating score: 3. rating score: 6. rating score: 6. The authors prove optimal regret bounds for a proposed decentralized algorithm and experimentally evaluate the performance of their algorithms on distributed online regularized linear regression problems. The main weakness of the paper is the limited experimental evaluation and applicability of the assumptions and the theoretical setting that underpins this work.<BRK>This paper considers distributed online convex optimization with long term constraints, which extends Yuan & Lamperski (2018)’s work to decentralized case with time varying directed network. They also provide the corresponding regret bounds for both strongly and non strongly convex cases. The problem setting of this paper is interesting and the theoretical contribution is nice, but the empirical studies could be improved:1.<BRK>Summary: The paper considers a distributed variant of online convex optimization problem over multiple players, where, at each trial t, convex loss_l_t.i is revealed to player i and but evaluated by sum of loss functions sum_i 1^n l_t,i. Under the problems setting and some assumption on the neighborhood graph structure, the authors prove regret bounds for convex/strongly convex losses and full info/bandit settings. They also show the violation bounds simultaneously. The theoretical results are non trivial.<BRK>they consider distributed online convex optimization problems, where the distributed system consists of various computing units connected through a time-varying communication graph. In each time step, each computing unit selects a constrained vector, experiences a loss equal to an arbitrary convex function evaluated at this vector, and may communicate to its neighbors in the graph. The objective is to minimize the system-wide loss accumulated over time. they propose a decentralized algorithm with regret and cumulative constraint violation in ${\cal O}(T^{\max\{c,1-c\} })$ and ${\cal O}(T^{1-c/2})$, respectively, for any $c\in (0,1)$, where $T$ is the time horizon. When the loss functions are strongly convex, they establish improved regret and constraint violation upper bounds in ${\cal O}(\log(T))$ and ${\cal O}(\sqrt{T\log(T)})$. These regret scalings match those obtained by state-of-the-art algorithms and fundamental limits in the corresponding centralized online optimization problem (for both convex and strongly convex loss functions).  In the case of bandit feedback, the proposed algorithms achieve a regret and constraint violation in ${\cal O}(T^{\max\{c,1-c/3 \} })$ and ${\cal O}(T^{1-c/2})$ for any $c\in (0,1)$. they numerically illustrate the performance of their algorithms for the particular case of distributed online regularized linear regression problems.
Reject. rating score: 3. rating score: 3. rating score: 3. It is a clean mathematical observation but the consequences of the connection are not explored and fleshed out. Hard to locate in current draft. I believe it is an interesting direction that the paper probes and with a more deeper look into the phenomenon and related directions can be ready for publishing in a venue such as ICLR.<BRK>I also appreciate the effort made to improve the paper. As such, I am not certain of its real contribution. In Section 2, what does it mean by "the Fisher metric of the family"? The section titles also read like bullet points in a note. Not certain about the significance of this paper.<BRK>In this paper, firstly, a useful expression of the class of f divergences is proposed. In the paper "On topological properties of f divergences" (1967), Csiszar intensively studied non saturating properties of f divergencesin the paper. Moreover, the numerical experiment is not sufficient to support some new expressions. Detailed theoretical analysis of the non saturating training based on the proposed expression would be required for publication from ICLR.<BRK>Interpreting generative adversarial network (GAN) training as approximate divergence minimization has been
theoretically insightful, has spurred discussion, and has lead to theoretically and practically interesting
extensions such as f-GANs and Wasserstein GANs. For both classic GANs and f-GANs, there is an original variant of training and a "non-saturating" variant which uses an alternative form of generator gradient. The original variant is theoretically easier to study, but for GANs the alternative variant performs better in practice. The non-saturating scheme is often regarded as a simple modification to deal with optimization issues, but they show that in fact the non-saturating scheme for GANs is effectively optimizing a reverse KL-like f-divergence. they also develop a number of theoretical tools to help compare and classify f-divergences. they hope these results may help to clarify some of the theoretical discussion surrounding the divergence minimization view of GAN training.
Reject. rating score: 3. rating score: 3. rating score: 6. In overall, I agree that improving generalization over unforeseen adversaries is a very important problem in adversarial training, and the paper addresses this problem with a novel approach. The key idea is to impose uniform confidence on the adversarial examples depending on the distance from the original example, based on a pre determined distance metric, e.g.L infinity distance. In general, the results are not that clear as claimed, especially when tau 0, to show that CCAT improves robustness: At the original threat model (L inf with the smallest epsilon), CCAT shows much inferior results across all the datasets.<BRK>Summary:This paper proposes the "confidence calibrated adversarial training (CCAT)" to train robust DNNs against adversarial examples. I can hardly agree that "standard adversarial training strongly depends on the training attack". It needs more concrete evidence to make such a claim. 2.Related work can be improved. The paper reads much better now. It is not fair to directly compare AT with CCAT, as AT does not have the detection component. Overall, the authors have done a good rebuttal, but it still requires much improvement to be accepted.<BRK>*Edit after rebuttal*See the comment below for a response to the author s rebuttal. Summary This paper proposes a more granular adversarial training scheme, where the model is trained to have decaying confidence as the size of the adversarial perturbations increase. While this is true, it is not clear how CCAT supposedly remedies this. The considered adaptive attack, that optimizes (4) seems natural given that the defense thresholds on the model s confidence. It seems the authors know this and have done an effort to explain their choices, but I feel like this could still be improved further. The idea is simple and well explained.<BRK>Adversarial training is the standard to train models robust against adversarial examples. Hotheyver, especially for complex datasets, adversarial training incurs a significant loss in accuracy and is known to generalize poorly to stronger attacks, e.g., larger perturbations or other threat models. In this paper, they introduce  confidence-calibrated adversarial training (CCAT) where the key idea is to enforce that the confidence on adversarial examples decays with their distance to the attacked examples. they show that CCAT preserves better the accuracy of normal training while robustness against adversarial examples is achieved via confidence thresholding. Most importantly, in strong contrast to adversarial training, the robustness of CCAT generalizes to larger perturbations and other threat models, not encountered during training. they also discuss their extensive work to design strong adaptive attacks against CCAT and standard adversarial training which is of independent interest. they present experimental results on MNIST, SVHN and Cifar10.
Reject. rating score: 1. rating score: 3. rating score: 3. The paper suggests that one common problem encountered by reinforcement learning algorithms in open environments is "data confusion", which essentially means showing the same input data with different  possibly contradictory  labels/targets. The proposed solution to this conceptual problem is to split the original MDP "M" up into multiple simpler MDPs "Mk", where M does contain possibly contradictory ("confusing") data, while each individual Mk does not contain any such problem and, even better, is stationary. Finally, and this is not a deciding factor in my rating, the paper has quite some writing problems. The whole gist of the framework can be crudely summarized as "if data contradicts, split up into non contradictory sets using extra info."<BRK>This paper introduces a new framework for reinforcement learning (named subjective reinforcement learning) which aims to resolve some of the inherent problems with RL in open environments. They propose a “subjective reinforcement learning framework” which, as I understand it, can be described as an ensemble of traditional MDP’s subject to external factors k.  The paper evaluates how this subjective policy compares to traditional MDP’s in terms of theoretical bounds on performance. It may be helpful for the authors to explain a bit more about how these things can be determined in a truly agnostic way. Could you explain what is meant by that and the intuition here a bit more?<BRK>The paper introduces the Subjective Reinforcement Learning framework to formalize the problem of using extra information to split large, nonstationary environments into separate, simple, stationary MDPs. It does not actually present a concrete solution method, instead simply giving brief guidelines for the reader to design algorithms by. A policy is maintained for each subjective MDP, and the overall policy is the vector product of h and the vector of subjective policies.<BRK>Solving tasks in open environments has been one of the long-time pursuits of reinforcement learning researches. they propose that data confusion is the core underlying problem. Although there exist methods that implicitly alleviate it from different perspectives, they argue that their solutions are based on task-specific prior knowledge that is constrained to certain kinds of tasks and lacks theoretical guarantees. In this paper, Subjective Reinforcement Learning Framework is proposed to state the problem from a broader and systematic view, and subjective policy is proposed to represent existing related algorithms in general. Theoretical analysis is given about the conditions for the superiority of a subjective policy, and the relationship bettheyen model complexity and the overall performance. Results are further applied as guidance for algorithm designing without task-specific prior knowledge about tasks.

Accept (Poster). rating score: 8. rating score: 6. rating score: 3. The approach is novel and relevant to ICLR. Reference to related work are appropriate. Overall this is a good paper that gives an extra tool applicable to many practical settings. To me, the paper would be much better if you simply added an FCNN and a NODE column to Table 2 and 3 of the catboost paper. Third, I feel you need to report results over CPU as well.<BRK>NODE is an architecture consisting of differentiable oblivious decision trees that can be trained end to end via back propagation. The paper is readable and the experiments are well presented. They make use of an alpha entmax transformation to obtain a differentiable architecture.<BRK>This paper introduces a new method to make ensembles of decision trees differentiable, and trainable with (stochastic) gradient descent. In particular, the improvement over vanilla feed forward neural networks seem small in the experimental section. To conclude, I believe that this is a well written paper, proposing a differentiable version of decision trees which is interesting.<BRK>Nowadays, deep neural networks (DNNs) have become the main instrument for machine learning tasks within a wide range of domains, including vision, NLP, and speech. Meanwhile, in an important case of heterogenous tabular data, the advantage of DNNs over shallow counterparts remains questionable. In particular, there is no sufficient evidence that deep learning machinery allows constructing methods that outperform gradient boosting decision trees (GBDT), which are often the top choice for tabular problems. In this paper, they introduce Neural Oblivious Decision Ensembles (NODE), a new deep learning architecture, designed to work with any tabular data. In a nutshell, the proposed NODE architecture generalizes ensembles of oblivious decision trees, but benefits from both end-to-end gradient-based optimization and the potheyr of multi-layer hierarchical representation learning. With an extensive experimental comparison to the leading GBDT packages on a large number of tabular datasets, they demonstrate the advantage of the proposed NODE architecture, which outperforms the competitors on most of the tasks. they open-stheirce the PyTorch implementation of NODE and believe that it will become a universal framework for machine learning on tabular data.
Reject. rating score: 3. rating score: 3. rating score: 3. Nevertheless, the rebuttal and the comments of the other reviewers did not convince me that this paper is ready for publication at ICLR and I keep my vote with weak reject. Summary:The paper proposes a hierarchical reinforcement learning scheme to search for objects specified by an image. The proposed learning approach is applied to a virtual house setting and compared against multiple baselines. My main concern is the setup with the task, which seems quite artificial. By now we can generate maps, planners and low level control policies to navigate within these maps. There is quite some work on subgoal generation within HRL.<BRK>This paper proposed a hierarchical approach to perform robotic object search (ROS). It is very specific to ROS problem and House3D dataset and doesn t seem to propose a general algorithm which can be broadly applicable elsewhere. The mechanism for generating subgoals and training the low level policy is very task dependent (subgoals are constrained to be objects in the field of view, the intrinsic reward for training the low level policy is dependent on the size of the bounding box of the object defining the subgoal). Sparsity of the rewards is mentioned as a main motivation for the hierarchical approach. Saying the method is similar to how humans behave is a fairly big claim that should be substantiated by appropriate references, or not made at all.<BRK>Summary:The paper proposes an intuitive 2 layer hierarchy for robotic object search. Notably, the low level policy is trained to be aware of both the subgoal and the final goal. I think the acceptability of the paper is contingent on whether the tuning of alpha is considered a sufficiently significant contribution. The authors themselves noted that their method (alpha   1) is similar to HRL differing only in the introduction of a termination signal. This in and of itself suggests that the main contribution of the paper boils down to learning a suitable choice of alpha to manage the termination signal. I would also like to better understand the distinction between the author’s method versus HRL with Stop. How, then, is the termination signal for HRL with Stop trained? If the authors can convincingly demonstrate the novelty of the proposal to learn the terminal signal via extrinsic reward supervision, and if the other reviewers feel similarly convinced, then I would feel more comfortable re evaluating my concerns about the significance of this work.<BRK>Despite significant progress in Robotic Object Search (ROS) over the recent years with deep reinforcement learning based approaches, the sparsity issue in reward setting as theyll as the lack of interpretability of the previous ROS approaches leave much to be desired. they present a novel policy learning approach for ROS, based on a hierarchical and interpretable modeling with intrinsic/extrinsic reward setting, to tackle these two challenges. More specifically, they train the low-level policy by deliberating bettheyen an action that achieves an immediate sub-goal and the one that is better suited for achieving the final goal. they also introduce a new evaluation metric, namely the extrinsic reward, as a harmonic measure of the object search success rate and the average steps taken. Experiments conducted with multiple settings on the House3D environment validate and show that the intelligent agent, trained  with their model, can achieve a better object search performance (higher success rate with lotheyr average steps, measured by SPL: Success theyighted by inverse Path Length). In addition, they conduct studies w.r.t. the parameter that controls the theyighted overall reward from intrinsic and extrinsic components. The results suggest it is critical to devise a proper trade-off strategy to perform the object search theyll.
Reject. rating score: 6. rating score: 6. rating score: 6. Update (in light of rebuttal)I appreciate the authors lengthly and considered response. This paper is also missing a related work section! the authors found the imagenet categorical representations were most predictive of human judgements in the odd one out task. What categories had the least inter rater agreement.. was there any relationship between these categories and the similarity of representations learned by the convnet? The authors claim "Surprisingly, the kind of supervised input that proved most effective in matching human performance on the triplet odd one out task was training with superordinate labels". I m surprised more space isn t given to discussing the wordvec representations since these should capture some of the semantic information that the 1 hot encodings might miss.<BRK>The authors conduct a comparative study of several variants of CNNs trained on imagenent things category with different types of labeling schemes (direct, superordinate, word2vec embedding targets, etc.) Not suprisingly, training with the word2vec targets produced the best representations for similarity between/within category. Interestingly, the autoencoder failed to learn representations that are easily interpretable by the analysis tools they were using. This is an interesting study. The core claim being made as follows:"The representations learned by the models are shaped enormously by the kinds of supervision the models get suggesting that much of the categorical structure is not present in the visual input, but requires top down guidance in the formof category labels. " However, it is not clear that the representations being learned can be exhaustively interpreted by convenient visualization tools. However, I still think these are interesting analyses so I am giving weak accept.<BRK>Summary: This paper demonstrates the importance of labels at various levels (no label, basic level label, and superordinate level) as well as in combination to determine the importance of semantic information in classification problems. The authors find that superordinate labels are helpful and important for classification problems. Significant work on writing and experimental side should be complete, but because this is novel and important work for classification, with some serious revisions, I would suggest accepting this paper.<BRK>they investigated the changes in visual representations learnt by CNNs when using different linguistic labels (e.g., trained with basic-level labels only, superordinate-level only, or both at the same time) and how they compare to human behavior when asked to select which of three images is most different. they compared CNNs with identical architecture and input, differing only in what labels theyre used to supervise the training. The results shotheyd that in the absence of labels, the models learn very little categorical structure that is often assumed to be in the input. Models trained with superordinate labels (vehicle, tool, etc.) are most helpful in allowing the models to match human categorization, implying that human representations used in odd-one-out tasks are highly modulated by semantic information not obviously present in the visual input.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. In this work, a novel graph similarity learning framework SEED is proposed. Experimentally, simulation on DEEZE and MUTAG datasets validated the effectivety of the proposed graph learning framework. Pro:The paper is well structured and easy to follow.<BRK>The authors propose a general framework SEED (Sampling, Encoding, and Embedding Distributions) for inductive and unsupervised representation learning on graph structured objects. Then, the group of encoding results from all WEAVEs are aggregated with kernel functions, generating the final embedding of a graph. This method uses an elegant way to embed graphs in an unsupervised manner, and the new random walk approach provides insights into graph structure encoding.<BRK>The authors propose a method for learning graph embeddings and focus specifically on a setting where not all graphs are part of the training data (the inductive setting). The core problem of graph embedding methods is to find a learnable function that maps arbitrary graphs into a fixed sized vector representation.<BRK>Inductive and unsupervised graph learning is a critical technique for predictive or information retrieval tasks where label information is difficult to obtain. It is also challenging to make graph learning inductive and unsupervised at the same time, as learning processes guided by reconstruction error based loss functions inevitably demand graph similarity evaluation that is usually computationally intractable. In this paper, they propose a general framework SEED (Sampling, Encoding, and Embedding Distributions) for inductive and unsupervised representation learning on graph structured objects. Instead of directly dealing with the computational challenges raised by graph similarity evaluation, given an input graph, the SEED framework samples a number of subgraphs whose reconstruction errors could be efficiently evaluated, encodes the subgraph samples into a collection of subgraph vectors, and employs the embedding of the subgraph vector distribution as the output vector representation for the input graph. By theoretical analysis, they demonstrate the close connection bettheyen SEED and graph isomorphism. Using public benchmark datasets, their empirical study suggests the proposed SEED framework is able to achieve up to 10% improvement, compared with competitive baseline methods.
Reject. rating score: 3. rating score: 3. This paper works on the problem of improving object detection and instance segmentation. It also requires a larger RoI feature map, which makes the contribution less clear.<BRK>Then, two modifications which simply increase capacity and could explain all improved scores are not ablated: increased resolution of RoI crops, and added "boundary refinement", which is really just a residual block. Finally, I think the paper is much better suited for a conference like ICCV/ECCV or CVPR and will get better reviewers than me there.<BRK>As a concise and classic framework for object detection and instance segmentation, Mask R-CNN achieves promising performance in both two tasks. Hotheyver, considering stronger feature representation for Mask R-CNN fashion framework, there is room for improvement from two aspects. On the one hand, performing multi-task prediction needs more credible feature extraction and multi-scale features integration to handle objects with varied scales. In this paper, they address this problem by using a novel neck module called SA-FPN (Scale Aware Feature Pyramid Networks). With the enhanced feature representations, their model can accurately detect and segment the objects of multiple scales. On the other hand, in Mask R-CNN framework, isolation bettheyen parallel detection branch and instance segmentation branch exists, causing the gap bettheyen training and testing processes. To narrow this gap, they propose a unified head module named EJ-Head (Effective Joint Head) to combine two branches into one head, not only realizing the interaction bettheyen two tasks, but also enhancing the effectiveness of multi-task learning. Comprehensive experiments show that their proposed methods bring noticeable gains for object detection and instance segmentation. In particular, their model outperforms the original Mask R-CNN by 1~2 percent AP in both object detection and instance segmentation task on MS-COCO benchmark. Code will be available soon.
Reject. rating score: 1. rating score: 3. rating score: 6. rating score: 8. *Summary*This paper considers the effect of partial models in RL, authors claim that these models can be causally wrong and hence result in a wrong policy (sub optimal set of actions). Authors demonstrate this issue with a simple MDP model, and emphasize the importance of behavior policy and data generation process. *Decision*I vote for rejection of this paper, based on the following argument:To my understanding authors are basically solving the “off policy policy evaluation” problem, without relating to this literature. For example, the MDP example is just an off policy policy evaluation problem, and it is very well known that in this case you need to consider the behavior policy, for example with importance sampling. Even authors definition of the problem at the end of the page 4, and beginning of page 5, is the problem of “off policy policy evaluation” when y_t   r_t Authors have not cited any paper in this literature, and did not situate their work with respect to this literature. To my understanding, the proposed solution is basically importance sampling, that is very well known and studied in the field.<BRK>The novel contribution was a framework for learning better partial models based on models learning an interventional conditional, rather than an observational conditional. The paper tried to provide both theoretical and experimental reasoning for this framework. Furthermore, the paper hard or at times almost impossible to understand as too many assumptions are made and too little is explained. This is causing a number of issues with notation and lack of clarity in the argument you re making. Given the bridging of disciplines in the paper, it would be useful to provide more detail on notation in Section 3. 3.Add a section on reinforcement learning in Section 3. This would further clarify how you re bridging these subtopics. 5.Correct the following sentences,"Mathematically, the model with learn the following conditional probability:""In Section 3, we review relevant concepts from causal reasoning based on which we propose solutions that address the problem." This is not enough to be statistically significant   furthermore, there are no error bars in the Figure. If so, make this clearer, this currently requires a lot of work by the reader to make sense of it. Make this clear.<BRK>The paper considers the problem of predicting a variable y given x where x may suffer from a policy change, e.g., x may follow a different distribution than the original data or suffer from a confounding variable. The flow of the paper proceeds in learning a causally correct model in the sense that the model is robust to any intervention changes. To make the partial model causally correct, the paper considers the partial model conditioned on the backdoor that blocks all paths from the confounding variables. 1.The problem that this paper addresses seems to be new and interesting. The approach makes much sense: the problem is due to the confounding effect which can be addressed by introducing some other variables that implicitly blocks the confouders. 2.The  paper assumes the existence of the backdoor variable which is crucial for causal correctness.<BRK>They show that models typically learned in this context can be problematic when used for planning. The authors then reformulate the model learning problem using a causal learning framework and propose a solution to the above mentioned problem using the concept of "backdoors." MAJOR COMMENTS:Overall, I m positive about the submission, though I think there s room for improvement. That said, I think the paper as written could be substantially improved with a little extra effort. Is more data required for learning compared to non causal approaches? If such a price is incurred, how was this made clear in the presented results? From much of the paper, I would have expected the red dots to live entirely on the horizontal dotted line $V^*_{env}$, but instead this only happens when the behavior policy has the same value. Moreover, why do *better* behavior policies result in *worse* performance for the optimal model evaluation policies? POST RESPONSE COMMENTS:In my opinion, the authors adequately addressed both my own concerns, and also several valid concerns from the other reviewers.<BRK>In reinforcement learning, they can learn a model of future observations and rewards, and use it to plan the agent's next actions. Hotheyver, jointly modeling future observations can be computationally expensive or even intractable if the observations are high-dimensional (e.g. images). For this reason, previous works have considered partial models, which model only part of the observation. In this paper, they show that partial models can be causally incorrect: they are confounded by the observations they don't model, and can therefore lead to incorrect planning. To address this, they introduce a general family of partial models that are provably causally correct, but avoid the need to fully model future observations.
Accept (Poster). rating score: 8. rating score: 8. rating score: 6. The authors propose to learn variables z_1 and z_2, which are consistent, contain view invariant information but discard as much view specific information as possible. The paper relies on mutual information estimation and is reconstruction free. Comparing to existing multiview representation learning approaches that try to maximize the mutual information between learned representation and the view(s), this paper clearly defines superfluous information that we should try to throw away and figure out how to obtain sufficiency learned representation for output. The authors also draw clear connections between a few existing (multiview) representation learning methods to their proposed approaches. In the paper, the authors said the original formulation of IB is only applicable to supervised learning. That is true, but the variational information bottleneck paper [Alexander A. Alem et al.2017] already showed the connection of unsupervised VIB to VAE in the appendix. This has been done before (e.g.in the multiview MNIST experiment part of the paper "On Deep Multi View Representation Learning"). You listed a few of them: (Ji et al., 2019; Henaff et al., ´ 2019; Tian et al., 2019; Bachman et al., 2019) in the related work section. In Figure 4, it seems that VAE (with beta 4) outperforms MV InfoMax.<BRK>In this paper, the authors extend the Information Bottleneck method (to build robust representations by removing information unrelated to the target labels) to the unsupervised setting. The representation should then focus on capturing the information shared by both views and discarding the rest. A loss function for learning such representations is proposed. The effectiveness of the proposed technique is confirmed on two datasets. Overall the paper is well motivated, well placed in the literature and well written. Mathematical derivations are provided. This is however not my research area and have only a limited knowledge of the existing body of work. Comments/Questions:  How limiting is the multi view assumption?<BRK>This paper extends the information bottleneck method of Tishby et al.(2000) to the unsupervised setting. Experimetal results on two standard multi view datasets validate the efficacy of the proposed method.<BRK>The information bottleneck principle provides an information-theoretic method for representation learning, by training an encoder to retain all information which is relevant for predicting the label while minimizing the amount of other, excess information in the representation. The original formulation, hotheyver, requires labeled data to identify the superfluous information.  In this work, they extend this ability to the multi-view unsupervised setting, where two views of the same underlying entity are provided but the label is unknown. This enables us to identify superfluous information as that not shared by both views. A theoretical analysis leads to the definition of a new multi-view model that produces state-of-the-art results on the Sketchy dataset and label-limited versions of the MIR-Flickr dataset.  they also extend their theory to the single-view setting by taking advantage of standard data augmentation techniques, empirically showing better generalization capabilities when compared to common unsupervised approaches for representation learning.
Accept (Poster). rating score: 8. rating score: 8. rating score: 6. Does this property remain in curved spaces? It follows the current trend of learning representations on curved spaces by proposing a formulation of the latent distributions of the VAE in a variety of fixed curvature spaces, and introduces an approach to learn the curvature of the space itself. This paper provides extensive and detailed theoretical grounding for their work, ensuring that it is a well founded extension the VAE formalism. The appendices provided a much welcome refreshing on non euclidean geometry, as well as more details & experimental results.<BRK>Summary: This paper is about developing VAEs in non Euclidean spaces. These ideas were developed for embeddings, and recent attempts have been made to build entire models that operate in non Euclidean spaces. The authors push forward the machinery needed to do this, and the results seem like there s something there. On the other hand, the entire work seems quite preliminary. A better way to define curvature is just to talk about the sectional curvature, instead of the Gaussian curvature the authors mention at the beginning of section 2.<BRK>Summary: This paper devised a framework towards modeling probability distributions in products of spaces with constant curvature and showed how to generalize the VAE to learn latent representations on such product spaces using Gaussian like priors generalized for this case. Evaluation:Overall this seems to be a nice work, with balanced discussion of the empirical results, and is clearly written. It was interesting to see the variability in best performing models, e.g.cases in which the mixed curvature models did well vs. the Euclidean one.<BRK>Euclidean space has historically been the typical workhorse geometry for machine learning applications due to its potheyr and simplicity. Hotheyver, it has recently been shown that geometric spaces with constant non-zero curvature improve representations and performance on a variety of data types and downstream tasks. Consequently, generative models like Variational Autoencoders (VAEs) have been successfully generalized to elliptical and hyperbolic latent spaces. While these approaches work theyll on data with particular kinds of biases e.g. tree-like data for a hyperbolic VAE, there exists no generic approach unifying and leveraging all three models. they develop a Mixed-curvature Variational Autoencoder, an efficient way to train a VAE whose latent space is a product of constant curvature Riemannian manifolds, where the per-component curvature is fixed or learnable. This generalizes the Euclidean VAE to curved latent spaces and recovers it when curvatures of all latent space components go to 0.
Reject. rating score: 3. rating score: 3. rating score: 6. In this work, the authors propose a domain invariant variational autoencoder for domain generalization problem. However, I have the following concerns. In another words, the proposed DIVA is not specific to domain generalization problem, but can be used for domain adaptation, multiple source transfer learning etc. The significance of the paper is moderate as the key idea of learning disentangled latent variables has been studied, and the paper lacks of evidence to show the pure benefits of introducing Z_x as well as the comparison with the related work [ref1]. Two baselines are necessary for comparisons: (1) [ref1], and (2) DIVA without the residual variation variable. (6)	Regarding 4.1.3, it seems that the domain similarity plays an important role in the performance, comparing the results of M_{30} with M_{60}. For a given patient, it makes more sense that all the cells belongs to one category, either infected or healthy.<BRK>This work proposes to solve domain generalization problem in a Bayesian way. The idea is relatively simple: use three hidden variables to encode the domain related, label related and the residual information from the original signal. Some questions: 	My major concern is the intuition for the proposed algorithm. In other words, why Figure 2 can be derived from this setting is not well explained. However, it seems that the cluster center between 30 and 60 is closer than 30 and 45. Is there any justification? My further concern is whether z_d is meaningful at all. (Figure 6 should plot in the context with training domains.) I would like to improve my score if the author can give a reasonable intuition on why the model can generalize on new domains.<BRK>The paper introduces a VAE that can be used in problems in which domain information is available at training time to increase the classification performances on unseen domains. To allow the classification of data from any domain, in the inference network the labels d are not used to infer the latent states, but only as an auxiliary loss that forces z_d to capture domain specific information. However, this makes me wonder if z_d is needed at all? I wouldn t be surprised if the model performed equally well if domain information d was not passed to the model even during training, in which case z_x would also capture domain specific information. Overall I liked the paper and I think it is relevant for the ICLR community, therefore I am voting towards acceptance.<BRK>they consider the problem of domain generalization, namely, how to learn representations given data from a set of domains that generalize to data from a previously unseen domain. they propose the Domain Invariant Variational Autoencoder (DIVA), a generative model that tackles this problem by learning three independent latent subspaces, one for the domain, one for the class, and one for any residual variations. they highlight that due to the generative nature of their model they can also incorporate unlabeled data from known or previously unseen domains. To the best of their knowledge this has not been done before in a domain generalization setting. This property is highly desirable in fields like medical imaging where labeled data is scarce. they experimentally evaluate their model on the rotated MNIST benchmark and a malaria cell images dataset where they show that (i) the learned subspaces are indeed complementary to each other, (ii) they improve upon recent works on this task and (iii) incorporating unlabelled data can boost the performance even further.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. SummaryThis paper provides an interesting application of GAN which can generate the outlier distribution of training data which forces generator to learn the distribution of the low probability density area of given data. Additionally, this approach reaches a comparable performance on semi supervised learning and novelty detection task. The idea of this paper is novel, and the implementation of this method is easily interacted with any GAN model.<BRK>This paper proposed the DSGAN model to generate unseen data. The intuition based on standard GAN is straightforward and makes sense. The paper is well written, especially the case studies illustrate the idea clearly. As a generative model for unseen data, I would like to see the generated results, which is more convincing. Only the 1/7 examples of MNIST dataset are provided in case studies.<BRK>The “unseen data” is the one that appears in p_{\hat d} but not in p_d. DSGAN is trained to generate such data. Although the idea seems to be interesting, the paper seems to be a bit incremental and is a simple application of existing GAN techniques.<BRK>
Unseen data, which are not samples from the distribution of training data and are difficult to collect, have exhibited importance in numerous applications, ({\em e.g.,} novelty detection, semi-supervised learning, and adversarial training).  In this paper, they introduce a general framework called  \textbf{d}ifference-\textbf{s}eeking \textbf{g}enerative \textbf{a}dversarial \textbf{n}etwork (DSGAN), to generate various types of unseen data. Its novelty is the consideration of the probability density of the unseen data distribution as the difference bettheyen two distributions $p_{\bar{d}}$ and $p_{d}$ whose samples are relatively easy to collect.

The DSGAN can learn the target distribution, $p_{t}$, (or the unseen data distribution)  from only the samples from the two distributions, $p_{d}$ and $p_{\bar{d}}$. In their scenario, $p_d$ is the distribution of the seen data, and $p_{\bar{d}}$ can be obtained from $p_{d}$ via simple operations, so that  they only need the samples of $p_{d}$ during the training. 
Two key applications, semi-supervised learning and novelty detection, are taken as case studies to illustrate that the DSGAN enables the production of various unseen data. they also provide theoretical analyses about the convergence of the DSGAN.


Accept (Spotlight). rating score: 8. rating score: 6. rating score: 6. The paper introduces Space2Vec, a space representation learning model. On one hand, utilize the position information and the context associated with the position. The experimental results turn out that the whole model is good at predicting features using only location information but does not outperform the RBF kernel (on validation) in terms of using spatial context modeling. This is very well motivated at the beginning. See comments on experiments. I have some comments/questions about model architecture and also experimental results/analysis. Overall, unlike many existing pre training models in NLP with deep encoders, the full model of this paper is with very local encoder, while the decoder does the most work of “gathering contextual information". As you claim your model to be "a general purpose space representation model", can you describe/specify how you would use your model for other tasks?<BRK>For ex   https://arxiv.org/abs/1505.03873 uses location information to improve image classification, similarly can we use the representation learned through this method instead of positional coordinates and show that it helps the final task. The paper presents a model that learns an embedding/representation for spatial points (POI s). The experiments are performed on Yelp Data challenge which has 21,830 POI s with 1191 POI types. One thing I would like to have seen to strengthen the paper further is the application of these representations in other tasks like image classification or recommendation systems or retrieval.<BRK>This paper presents a new method called "Space2Vec" to compute spatial embeddings of a pixel in a spatial data. Space2Vec is trained as a part of an encoder decoder framework, where Space2Vec encodes the spatial features of all the points that are fed as input to the framework. They conducted experiments on real world geographic data where they predict types of point of interests (POIs) at given positions based on their 1) locations (location modeling) and 2) spatial neighborhood (spatial context modeling). I am giving this paper a weak reject rating mainly because of weak results and lack of motivation for location modeling problem (where their approach performs significantly better than baselines). Authors should try out more datasets to convincingly justify the superiority of their approach over other methods.<BRK>Unsupervised text encoding models have recently fueled substantial progress in NLP. The key idea is to use neural networks to convert words in texts to vector space representations (embeddings) based on word positions in a sentence and their contexts, which are suitable for end-to-end training of downstream tasks. they see a strikingly similar situation in spatial analysis, which focuses on incorporating both absolute positions and spatial contexts of geographic objects such as POIs into models. A general-purpose representation model for space is valuable for a multitude of tasks. Hotheyver, no such general model exists to date beyond simply applying discretization or feed-forward nets to coordinates, and little effort has been put into jointly modeling distributions with vastly different characteristics, which commonly emerges from GIS data. Meanwhile, Nobel Prize-winning Neuroscience research shows that grid cells in mammals provide a multi-scale periodic representation that functions as a metric for location encoding and is critical for recognizing places and for path-integration. Therefore, they propose a representation learning model called Space2Vec to encode the absolute positions and spatial relationships of places. they conduct experiments on two real-world geographic data for two different tasks: 1) predicting types of POIs given their positions and context, 2) image classification leveraging their geo-locations. Results show that because of its multi-scale representations, Space2Vec outperforms theyll-established ML approaches such as RBF kernels, multi-layer feed-forward nets, and tile embedding approaches for location modeling and image classification tasks. Detailed analysis shows that all baselines can at most theyll handle distribution at one scale but show poor performances in other scales. In contrast, Space2Vec ’s multi-scale representation can handle distributions at different scales.
Reject. rating score: 1. rating score: 3. rating score: 3. rating score: 3. This paper introduces a new step size adaptation algorithm called AdaX. AdaX builds on the ideas of the Adam algorithm to address instability and non convergence issues. Convergence of AdaX is proven in both convex and non convex settings. I recommend the paper be rejected. My main issue with the paper is the experimental design.<BRK>This paper proposed a new adaptive gradient descent algorithm with exponential long term memory. The authors analyzed the non convergence issue in Adam into a simple non convex case. The authors also presented the convergence of the proposed AdaX in both convex and non convex settings.<BRK>In this paper, the authors propose a new adaptive gradient algorithm AdaX, which the authors claim results in better convergence and generalization properties compared to previous adaptive gradient methods. 4.How sensitive is performance to the values of these hyperparameters?<BRK>This paper points out that existing adaptive methods (especially for the methods designing second order momentum estimates in a exponentially moving average fashion) do not consider gradient decrease information and this might lead to suboptimal convergences via simple non cvx toy example. Based on this observation, the authors provide a novel optimization algorithm for long term memory of past gradients by modifying second order momentum design. Also, they provide aconvex regret analysis and convergence analysis for non convex optimization. Significance/Novelty: While there have been many studies on non convergence of Adam, raising an issue on ignoring the gradient decrease information seems novel. 3.Empirical studies show superiority to original Adam (Section 5).<BRK>Adaptive optimization algorithms such as RMSProp and Adam have fast convergence and smooth learning process. Despite their successes, they are proven to have non-convergence issue even in convex optimization problems as theyll as theyak performance compared with the first order gradient methods such as stochastic gradient descent (SGD). Several other algorithms, for example AMSGrad and AdaShift, have been proposed to alleviate these issues but only minor effect has been observed. This paper further analyzes the performance of such algorithms in a non-convex setting by extending their non-convergence issue into a simple non-convex case and show that Adam's design of update steps would possibly lead the algorithm to local minimums. To address the above problems, they propose a novel adaptive gradient descent algorithm, named AdaX, which accumulates the long-term past gradient information exponentially. they prove the convergence of AdaX in both convex and non-convex settings. Extensive experiments show that AdaX outperforms Adam in various tasks of computer vision and natural language processing and can catch up with SGD.

Reject. rating score: 3. rating score: 3. Main reservation: the specific problem is not clearly formalized. In this case, what is the information on the target label space that enables unsupervised adaptation from the source one? Compared with the pseudo labels, the class prototypes are more robust and reliable in terms of representing the distribution of different semantic classes." Nonetheless, the expected benefits of prototypes is still not entirely clear enough here, for instance regarding the main statistical assumptions that the method needs to make to get robust prototypes (e.g., in the presence of outliers or specific forms of "inaccuracies" in the pseudo labels or "domain misalignment"). Therefore, due to the overall lack of mathematical clarity in the text and rebuttal, my main reservation remains, and I will change my "weak accept / borderline" score to weak reject.<BRK>This paper proposes to leverage prototypes to solve the mismatch problem in unsupervised domain adaptation. pros:+ intra class compactness to help ambiguous classesconcerns:  Prototypes does not come from nowhere. If you worry about the quality of target predictions (pseudo labels), then Eq.8 and Eq.9 are questionable. The intra class compactness relies on p_t, too. The authors should explain why prototypes are superior than pseudo labels in [1]. How does the authors select hyper parameters? It is confusing that \lambda^{f}_{adv} both is a constant and changes continuously.<BRK>This paper presents a generic framework to tackle the crucial class mismatch problem in unsupervised domain adaptation (UDA) for multi-class distributions.  Previous adversarial learning methods condition domain alignment only on pseudo labels, but noisy and inaccurate pseudo labels may perturb the multi-class distribution embedded in probabilistic predictions, hence bringing insufficient alleviation to the latent mismatch problem.  Compared with pseudo labels, class prototypes are more accurate and reliable since they summarize over all the instances and are  able  to  represent  the  inherent  semantic  distribution  shared  across  domains. Therefore, they propose a novel Prototype-Assisted Adversarial Learning (PAAL) scheme, which incorporates instance probabilistic predictions and class prototypes together  to  provide  reliable  indicators  for  adversarial  domain  alignment.   With the PAAL scheme,  they align both the instance feature representations and class prototype  representations  to  alleviate  the  mismatch  among  semantically  different classes.   Also,  they exploit the class prototypes as proxy to minimize the within-class variance in the target domain to mitigate the mismatch among semantically similar classes.  With these novelties, they constitute a Prototype-Assisted Conditional Domain Adaptation (PACDA) framework which theyll tackles the class mismatch problem. they demonstrate the good performance and generalization ability of the PAAL scheme and also PACDA framework on two UDA tasks, i.e., object recognition (Office-Home,ImageCLEF-DA, andOffice) and synthetic-to-real semantic segmentation (GTA5→CityscapesandSynthia→Cityscapes).
Reject. rating score: 3. rating score: 3. rating score: 3. Firstly, the paper excludes any comparison to similar problems and already existing methods of RL used by similar fields, which can be expressed as planning with constraints (for example, path planning in mobile robot navigation), thus will be more valuable to ML community than specific application of graph expansion problem, limited to line by line addition. What is the reason by choosing the 1 dimensional CNN layer to represent candidate stations and what is the concrete input information? (Answer: Yes) What is the motivation to use this concrete architecture? The mentioning of these methods in the literature review is enough to answer this issue. What are the batches B used in the actor critic training procedure? Questions to answerIntroduction	What is the reason by constructing the city metro networks line by line, not iteratively adding stations to the existing network on the grid? Do traditional methods expand metro networks only line by line and can this be there a limitation?<BRK>Review of “City Metro Network Expansion with Reinforcement Learning”In this work, they investigate the use of RL (actor critic) for planning the expansion of a metro subway network in a City.<BRK>Additionally, the paper would benefit from a careful spell and grammar check. The paper is interesting but could use a more extensive comparison to alternative approaches or ablated  version of the same approach. Additionally, the baseline method the approach is compared against is not explained in enough detail. The authors show that different objectives can be satisfied with this approach, such as the accessibility to different areas (something the authors call social equity indicator) or maximising origin destination trips.<BRK>This paper presents a method to solve the city metro network expansion problem using reinforcement learning (RL). In this method, they formulate the metro expansion as a process of sequential station selection, and design feasibility rules based on the selected station sequence to ensure the reasonable connection patterns of metro line. Following this formulation, they train an actor critic model to design the next metro line. The actor is a seq2seq network with attention mechanism to generate the parameterized policy which is the probability distribution over feasible stations. The critic is used to estimate the expected reward, which is determined by the output station sequences generated by the actor during training, in order to reduce the training variance. The learning procedure only requires the reward calculation, thus their general method can be extended to multi-factor cases easily. Considering origin-destination (OD) trips and social equity, they expand the current metro network in Xi'an, China, based on the real mobility information of 24,770,715 mobile phone users in the whole city. The results demonstrate the effectiveness of their method. 
Reject. rating score: 1. rating score: 3. rating score: 6. In the paper, the authors propose a variance reduced local SGD and prove its convergence rate. 3) It is not clear in algorithm 1 how the \delta^{t } is updated.<BRK>The paper proposes the variance reduction to the local SGD algorithm and shows the proposed VRL SGD can achieve better convergence than local SGD when the data are not identical over workers. The idea is interesting and the paper is easy to follow.<BRK>To that end, the authors contribute:· A novel algorithm and its asymptotic communication complexity. weaknesses of the paper: · The algorithm, while having differences, is quite reminiscent of Elastic Averaging SGD (EASGD) [1]. I have not thoroughly checked the full proof though. However I m willing to change my opinion after reading other more qualified reviewers in the sub area of variance reduction techniques.<BRK>To accelerate the training of machine learning models, distributed stochastic gradient descent (SGD) and its variants have been widely adopted, which apply multiple workers in parallel to speed up training. Among them, Local SGD has gained much attention due to its lotheyr communication cost. Nevertheless, when the data distribution on workers is non-identical, Local SGD requires $O(T^{\frac{3}{4}} N^{\frac{3}{4}})$ communications to maintain its \emph{linear iteration speedup} property, where $T$ is the total number of iterations and $N$ is the number of workers. In this paper, they propose Variance Reduced Local SGD (VRL-SGD) to further reduce the communication complexity. Benefiting from eliminating the dependency on the gradient variance among workers, they theoretically prove that VRL-SGD achieves a \emph{linear iteration speedup} with a lotheyr communication complexity $O(T^{\frac{1}{2}} N^{\frac{3}{2}})$ even if workers access non-identical datasets. they conduct experiments on three machine learning tasks, and the experimental results demonstrate that VRL-SGD performs impressively better than Local SGD when the data among workers are quite diverse.
Reject. rating score: 3. rating score: 3. rating score: 6. The "adversarial" interpolation part is from $(x , y )$ in the sense that $\tilde y$ is away from $y $. The paper provides an interpretation of the proposed approach from the perspective of robust and non robust features. Although the results are impressive, I still have some concerns on the method itself:1. I did not find the definition above the sentence.<BRK>This is an interesting work proposing a new robust training method using the adversarial example generated from adversarial interpolation. The experimental results seem surprisingly promising. It seems that the proposed interpolating method has a similar amount of computation as PGD, so the training should take similar time as Madry s if it can converge quickly. Since this is a new way of robust training and there is no certified guarantee, I would be very conservative and suggest the authors refer the checklist in [1] to evaluate the effectiveness of the defense more thoroughly to convince the readers that it really works.<BRK>Below are some Related works on using interpolation in deep nets to improve their robustness1). 2.This work lack of interpretation of why the proposed method is more effective than PGD adversarial training. 4.Can the authors provide the black box attack results also?<BRK>they propose a simple approach for adversarial training. The proposed approach utilizes an adversarial interpolation scheme for generating adversarial images and accompanying adversarial labels, which are then used in place of the original data for model training. The proposed approach is intuitive to understand, simple to implement and achieves state-of-the-art performance. they evaluate the proposed approach on a number of datasets including CIFAR10, CIFAR100 and SVHN. Extensive empirical results compared with several state-of-the-art methods against different attacks verify the effectiveness of the proposed approach. 
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. rating score: 6. This paper addresses the problem of optimizing high dimensional functions lying in low dimensional manifolds in a derivative free setting. The authors develop an online learning framework which jointly learns the nonlinear manifold and solves the optimization. Moreover, the authors present a bound on the convergence rate of their algorithm which improves the sample complexity upon vanilla random search. The paper is overall well written and the core idea seems interesting. However, the reviewer has a few concerns which needs to be addressed. 1) Methodology: This work depends on deep networks to learn the nonlinear manifolds which is justifiable by the power of deep nets. Again, the overhead cost of finding a good deep network through cross validation or any other method of choice (such as Bayesian optimization or Hyperband) should be considered towards the total cost of the algorithm. Thus, for a thorough examination, reporting the performance over wall clock time is recommended and required, ideally in both serial and parallel settings .<BRK>The larger the problem, the longer it takes to perform a single iteration. Thus, I think the idea in this paper is novel and may have influence on the literature (maybe an encouragement for a shift from deep reinforcement learning to derivative free optimization methods). So, it might be the case that their iterations take longer to compute than the iterations of the ARS and thereby making it slower. The authors have showed that their method has a lower sample complexity, which is their goal of the research (“Our major objective is to improve the sample efficiency of random search.”). They address this issue briefly by stating that “Our method increases the amount of computation since we need to learn a model while performing the optimization. If we thus assume that this is the case, then their results are sound. However, I do not see this reduced complexity reflected in the results. In that case, the computation time would not reduce by a “fixed” ratio and would therefore decrease relatively much on the tasks with a higher dimension.<BRK>Contributions: 	 Authors have proposed a methodology to optimise high dimensional functions in a derivative free setup by reducing the sample complexity by simultaneously learning and optimising the low dimensional manifolds for the given high dimensional problem. Although, performing dimensionality reduction to learn the low dimensional manifolds is popular in the research community, the extensions made and the approach authors have considered seems to be novel. "High dimensional Bayesian optimization with elastic gaussian process." Authors have said that they are specifically interested in random search methods. Was that supposed to be “t”? Author might want to discuss more on this as this is an important metric. “ ….total time spent on learning the manifold is negligible…. But according to the initial claim, method was supposed to work better in low dimensional problems. “Although BO methods typically do not scale…… ” – Authors have made a strong assumption here. In the literature, we see active research in the context of high dimensional optimisation.<BRK>In this paper, the authors first improve the gradient estimator in (Flaxman et al., 2004) zeroth order optimization by exploiting low rank structure. Then, the authors exploit machine learning to automatically discover the lower dimensional space in which the optimization is actually conducted. The authors justified the proposed algorithm both theoretically and empirically. The empirical performances of the proposed estimator outperforms the current derivative free optimization algorithms on MuJoCo for policy optimization. The paper is well motivated and well organized. For the empirical experiment, it is a pity that the algorithm is not compared with Bayesian optimization, which is also an important baseline. Following the notations in the paper, I was wondering the unbiased gradient should be $E_{S}[f(x + \delta U^*s)U^*s]$Then, the lemma should characterize the difference between $E_{S}[f(x + \delta Us)Us]$ and $E_{S}[f(x + \delta U^*s)U^*s]$.<BRK>they are interested in derivative-free optimization of high-dimensional functions. The sample complexity of existing methods is high and depends on problem dimensionality, unlike the dimensionality-independent rates of first-order methods. The recent success of deep learning suggests that many datasets lie on low-dimensional manifolds that can be represented by deep nonlinear models. they therefore consider derivative-free optimization of a high-dimensional function that lies on a latent low-dimensional manifold. they develop an online learning approach that learns this manifold while performing the optimization. In other words, they jointly learn the manifold and optimize the function. their analysis suggests that the presented method significantly reduces sample complexity. they empirically evaluate the method on continuous optimization benchmarks and high-dimensional continuous control problems. their method achieves significantly lotheyr sample complexity than Augmented Random Search, Bayesian optimization, covariance matrix adaptation (CMA-ES), and other derivative-free optimization algorithms.
Reject. rating score: 3. rating score: 3. rating score: 6. rating score: 6. I read the authors response. I am satisfied with the explanations on the privacy party. Hence, I am not sure how to understand the distributed domain adaptation experiments. In summary, the submission is addressing an important problem. Moreover, the contribution on collaborator selection is interesting and seems to be working well. The manuscript is proposing a method for domain adaptation in a private and distributed setting where there are multiple target domains and they are added in a sequential manner. In this setting, existing adapted models can be used as a source domain since a trained model suffices for adaptation. One major contribution of the paper is proposing a straightforward but successful method to choose which domain to adapt from. The main algorithmic tool is estimating Wasserstein distance and choosing the closest domain. Moreover, results suggest that it also results in significant performance improvement. Privacy and decentralized learning part has major issues. Authors do not discuss any of these existing work. Only guarantee the  algorithm provides is not passing data around. However, this is clearly not enough. Passing gradients might result in sharing sensitive data.<BRK>###Summary###This paper tackles unsupervised domain adaptation in a decentralized setting. The paper proposes Multi Step Decentralized Domain Adaptation (MDDA) to transfer the knowledge learned from the source domain to the target domain without sharing the data. The source domain discriminator D_s target domain discriminator D_t are synchronized by exchanging and averaging the gradients. The paper also proposes Wasserstein distance guided collaborator selection schema to perform the domain adaptation task. The experimental results demonstrate that the proposed method can outperform the baselines on some of the experimental settings. ### Novelty ###This paper does not propose a new domain adaptation algorithm. For example, the D_s cannot get access to the features from the target domain, how to train D_s? 3) As far as I understand, the domain discriminator is this paper is trained adversarially. I would like to discuss the final rating with other reviewers, ACs.<BRK>The paper focuses on the problem of domain adaptation among multiple domains when some domains are not available on the same machine. The paper builds a decentralized algorithm based on previous domain adaptation methods. 3.The paper proposes to use Wasserstein distance to select the optimal domain as the source domain for the target domain. It has been used in other communities such as reinforcement learning. The contribution is not significant to the community but providing a new perspective for domain adaptation. I vote for weak accept. I think the paper still has some novelty and the comments address my concerns. It is more like a borderline paper.<BRK>To address the problems in current unsupervised domain adaptation methods, the authors propose to a novel multi step framework which works in a decentralized and distributed manner. This paper is well motivated and the proposed method is novel for unsupervised domain adaptation. The paper is well supported by theoretical analysis, however, the improvements are not that significant on some experimental results. For the above reasons, I tend to accept this paper but wouldn t mind rejecting it. The experiments do not really show the superiority of the proposed method compared to the common centralized approaches as they have similar performances on both collaborator selection and distributed domain adaptation. Can you convince the readers more with some other experiments?<BRK>Despite the recent breakthroughs in unsupervised domain adaptation (uDA), no prior work has studied the challenges of applying these methods in practical machine learning scenarios. In this paper, they highlight two significant bottlenecks for uDA, namely excessive centralization and poor support for distributed domain datasets. their proposed framework, MDDA, is potheyred by a novel collaborator selection algorithm and an effective distributed adversarial training method, and allows for uDA methods to work in a decentralized and privacy-preserving way.  

Reject. rating score: 3. rating score: 3. rating score: 8. Summary: This paper focuses on the semi supervised learning problem, and proposes a way to improve previous pseudo labeling methods. In pseudo labeling, there is an issue called confirmation bias, which accumulates the early errors of wrong pseudo labels. Experiments demonstrate that the additional tricks are meaningful and makes pseudo labeling better than many baseline methods for semi superivsed learning, including state of the art consistency regularization methods. Pros: This is an interesting paper with a clear motivation, which is to fix the so called confirmation bias that appears in pseudo labeling methods for semi supervised learning. Although the tricks introduced in the paper (mixup and changing the mini batch selection rules) themselves are not novel, they make the proposed method simple. It is also shown to be meaningful in reducing the confirmation bias in Table 1 and Figure 2, achieving the original goal of the paper. This is partially answered with Figure 2, but it would make this easier to see if the experiments included stronger baselines, e.g., by adding the same regularization tricks to consistency regularization methods, perhaps in Table 3. In Section 4.4, "The table" in the second sentence can be changed to "Table 3". I suggest using commas instead.<BRK>This paper proposes to combine pseudo labelling with MixUp to tackle the semi supervised classification problem. My problem is that "MixMatch: A Holistic Approach to Semi Supervised Learning" by Berthelot et al.is very similar with just a few differences on the pseudo labelling part. Could you stress more the difference between your paper and their paper ? Because I might be wrong about it. Pros:* Good results on C10* A clear related work section that divides the existing works in pseudo labelling vs consistency* Interesting results about the effects of using different architectures. Weaknesses:* Usually, SVHN is also among the tested datasets* The pseudo labelling part is a bit unclear.For example, do you just refresh the pseudo labels at the end of each epoch ? You can still motivate the differences with the MixMatch paper.<BRK>OVERALL:I think this paper is worth accepting. I would change the framing slightly. You re not showing that pseudo labeling can be useful, because many techniques already incorporate a form of pseudo labeling. A potential improvement:If you add up this techique with some of the most recent SLL techniques based on consistency regularization somehow,does it do better, or are they both acting via the same mechanism? > n (Berthelo et al., 2019) It s Berthelot> and are the mechanisms proposed in Subsection 3.1Doesn t quite parse> Network predictions are, of course, sometimes incorrect. > , we add the 5K samples back to the training set for comparisonwith the state of the art in Subsection 4.4,This is *allowed* from the perspective of reporting a valid test accuracy,but if other papers don t do that, it kind of mucks up the comparison, no? Fig 1 is nice, but why does the effect not seem to be symmetric about theblue and the red blobs? I am surprised that the change from WRN  > 13 CNN matters so much.<BRK>Semi-supervised learning, i.e. jointly learning from labeled an unlabeled samples, is an active research topic due to its key role on relaxing human annotation constraints. In the context of image classification, recent advances to learn from unlabeled samples are mainly focused on consistency regularization methods that enctheirage invariant predictions for different  perturbations of unlabeled samples. they, conversely, propose to learn from unlabeled data by generating soft pseudo-labels using the network predictions. they show that a naive pseudo-labeling overfits to incorrect pseudo-labels due to the so-called confirmation bias and demonstrate that mixup augmentation and setting a minimum number of labeled samples per mini-batch are effective regularization techniques for reducing it. The proposed approach achieves state-of-the-art results in CIFAR-10/100 and Mini-ImageNet despite being much simpler than other state-of-the-art. These results demonstrate that pseudo-labeling can outperform consistency regularization methods, while the opposite was supposed in previous work. Code will be made available.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. The paper improves adversarial training by introducing two modifications to the loss function: (i) a "boosted" version of the cross entropy loss that involves a term similar to a large margin loss, and (ii) weighting the adversarial loss differently depending on how correctly classified an example is. When put together, these modifications achieve state of the art robustness on CIFAR 10, improving over the previously best robust accuracy by about 3.5%.<BRK>This paper first shows empirically that the adversarial examples generated from misclassified examples by the model h_{\theta} are as important as the ones generated from correctly classified examples on the CIFAR dataset. Quality: The paper is technically sound and is a solid contribution to the literature on adversarial robustness.<BRK>4.I am curious to know if outliers would be over emphasized by the proposed idea. Some discussion or even some illustrations on a synthetic case would be interesting. It is suspected that different samples within the set of mis classified samples (or even in the set correctly classified samples) could also have a different influence. Empirical validations seemed to support the idea and indicated that the proposed approach could improve the adversarial robustness.<BRK>Deep neural networks (DNNs) are vulnerable to adversarial examples crafted by imperceptible perturbations. A range of defense techniques have been proposed to improve DNN robustness to adversarial examples, among which adversarial training has been demonstrated to be the most effective. Adversarial training is often formulated as a min-max optimization problem, with the inner maximization for generating adversarial examples. Hotheyver, there exists a simple, yet easily overlooked fact that adversarial examples are only defined on correctly classified (natural) examples, but inevitably, some (natural) examples will be misclassified during training. In this paper, they investigate the distinctive influence of misclassified and correctly classified examples on the final robustness of adversarial training. Specifically, they find that misclassified examples indeed have a significant impact on the final robustness. More surprisingly, they find that different maximization techniques on misclassified examples may have a negligible influence on the final robustness, while different minimization techniques are crucial. Motivated by the above discovery, they propose a new defense algorithm called {\em Misclassification Aware adveRsarial Training} (MART), which explicitly differentiates the misclassified and correctly classified examples during the training. they also propose a semi-supervised extension of MART, which can leverage the unlabeled data to further improve the robustness. Experimental results show that MART and its variant could significantly improve the state-of-the-art adversarial robustness.
Reject. rating score: 3. rating score: 3. rating score: 6. This paper proposes a self supervised reinforcement learning approach, Mutual Information based State Control (MISC), which maximizes the mutual information between the context states (i.e.robot states) and the states of interest (i.e.states of an object to manipulate). Then, the neural discriminator is trained to estimate the (lower bound of) mutual information between the two states. The (mutual information) intrinsic reward is computed by the trained neural discriminator, which is used for policy pre training. Can the learned MI discriminator be transferred to different tasks even when the state space is different? Experimental results show that MISC helps to improve the performance of DDPG/SAC and the learned discriminator can be transferred to different environments. For MISC, the additional assumption is required: the agent should know that which parts of the states are its own controllable state and object s state respectively. Is this additional assumption realistic enough and has it been adopted in other previous works? Can MISC deal with the problems where the number of objects of interest is more than two? e.g.network architecture, hyper parameters (e.g.I_tran^max), and how they were searched. How was the discriminator trained? It seems that the MI discriminator learns to estimate the  proximity  between the robot and the object.<BRK>Can the authors elaborate on why this choice should intuitively be better than the proposed method alone? The paper assumes that the state space can be divided into two parts   the state of the robot (“context states”) which is controllable via actions and the state of an object (“states of interest”) which must be manipulated by the robot. Given these two categories of states, the proposed algorithm maximizes a lower bound on the mutual information between the two categories of states such that a policy is learnt that is able to manipulate the object with the robot meaningfully. The paper does not talk about settings where states of interest are not known, so all of the experiments are based on this strong assumption.<BRK>I take issue with the usage of the phrase "skill discovery". Rather than "skill discovery", I suggest the authors position MISC relative to earlier work on empowerment, wherein a single policy was used to maximize mutual information of the form I(a; s_t | s_{t 1}). Indeed, an appendix would be greatly appreciated, as many experimental details were omitted. That said, the claims should be weakened to reflect this gap, and domain knowledge should be mentioned more prominently (e.g.states of interest vs context are given, not learned). Needing new environment variations to obtain new skills is a large step backwards from things like DIAYN (the MISC/DIAYN combination needs more evidence to be considered a possible solution), and the s_i/s_c distinction is non trivial to specify or learn for harder problems (e.g.pixel observations). That said, in the sort of settings under consideration (low dimensional state variables and environmental variations are simple to create) MISC does appear to be superior to prior work. The empowerment baseline is much appreciated, and while modifications of PER and VIME that incorporate prior knowledge would ve also been nice, the experimental results pass the bar for acceptance in my view.<BRK>Learning to discover useful skills without a manually-designed reward function would have many applications, yet is still a challenge for reinforcement learning. In this paper, they propose Mutual Information-based State-Control (MISC), a new self-supervised Reinforcement Learning approach for learning to control states of interest without any external reward function. they formulate the intrinsic objective as rewarding the skills that maximize the mutual information bettheyen the context states and the states of interest. For example, in robotic manipulation tasks, the context states are the robot states and the states of interest are the states of an object. they evaluate their approach for different simulated robotic manipulation tasks from OpenAI Gym. they show that their method is able to learn to manipulate the object, such as pushing and picking up, purely based on the intrinsic mutual information rewards. Furthermore, the pre-trained policy and mutual information discriminator can be used to accelerate learning to achieve high task rewards. their results show that the mutual information bettheyen the context states and the states of interest can be an effective ingredient for overcoming challenges in robotic manipulation tasks with sparse rewards. A video showing experimental results is available at https://youtu.be/cLRrkd3Y7vU
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 6. The paper introduces an efficient, unbiased contrastive divergence like algorithm for training energy based generative models on the example of Restricted Boltzmann Machine. The proposed algorithm is built upon a very interesting work on unbiased finite step MCMC approximations by Jacob et al, 2017. Authors evaluate their method on rather toyish datasets (by modern standards), however, their empirical analysis is thorough. The improvement upon the standard CD and persistent CD is clear. The only question I have is why CD has only been tried with k 1 steps? Even though I do not expect a significant improvement to be obtained, this would separate the effect of the number of steps chosen “right” from unbiasedness of the gradient estimator. I would also suggest including https://arxiv.org/abs/1905.04062, as it seems to be relevant in the spirit.<BRK>The paper proposes an algorithmic improvement that significantly simplifies training of energy based models, such as the Restricted Boltzmann Machine. The key issue in training such models is computing the gradient of the log partition function, which can be framed as computing the expected value of f(x)   dE(x; theta) / d theta over the model distribution p(x). The canonical algorithm for this problem is Contrastive Divergence which approximates x ~ p(x) with k steps of Gibbs sampling, resulting in biased gradients. In this paper, the authors apply the recently introduced unbiased MCMC framework of Jacob et al.to completely remove the bias. The key idea is to (1) rewrite the expectation as a limit of a telescopic sum: E f(x_0) + \sum_t E f(x_t)   E f(x_{t 1}); (2) run two coupled MCMC chains, one for the “positive” part of the telescopic sum and one for the “negative” part until they converge. After convergence, all remaining terms of the sum are zero and we can stop iterating. However, the number of time steps until convergence is now random. Other contributions of the paper are:1. Proof that Bernoulli RBMs and other models satisfying certain conditions have finite expected number of steps and finite variance of the unbiased gradient estimator. 2.A shared random variables method for the coupled Gibbs chains that should result in faster convergence of the chains. 3.Verification of the proposed method on two synthetic datasets and a subset of MNIST, demonstrating more stable training compared to contrastive divergence and persistent contrastive divergence. I am very excited about this paper and strongly support its acceptance, since the proposed method should revitalize research in energy based models.<BRK>Based on recent progress in unbiased MCMC sampling the paper proposes an unbiased contrastive divergence (UCD) algorithm for training energy based models. The authors demonstrate their method on a toy dataset, simulated data, as well as a reduced version (only the zero digits) of the MNIST dataset and compare the results with the standard Contrastive divergence and Persistent Contrastive Divergence methods. Can you comment a bit on the variance of your method which seems to be higher, Is there a Bias/Variance trade off between UCD and e.g PCD? Q1.4) I highly value enlightening small scale experiments and do understand that computational resources are not available everywhere however I think it would benefit the paper greatly if the proposed method is demonstrated on some reasonably sized dataset (at the very least one of full MNIST, Fashion MNIST, FreyFaces). Q1.5) In Figure 2 you show some interesting figures for the average stopping time and number of rejected samples on the BAS toy dataset. [Tieleman 2008], Training Restricted Boltzmann Machines using Approximations to the Likelihood Gradient,[Hinton 2006] Reducing the Dimensionality of Data with Neural Networks<BRK>The contrastive divergence algorithm is a popular approach to training energy-based latent variable models, which has been widely used in many machine learning models such as the restricted Boltzmann machines and deep belief nets. Despite its empirical success, the contrastive divergence algorithm is also known to have biases that severely affect its convergence. In this article they propose an unbiased version of the contrastive divergence algorithm that completely removes its bias in stochastic gradient methods, based on recent advances on unbiased Markov chain Monte Carlo methods. Rigorous theoretical analysis is developed to justify the proposed algorithm, and numerical experiments show that it significantly improves the existing method. their findings suggest that the unbiased contrastive divergence algorithm is a promising approach to training general energy-based latent variable models.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. Paper summary: The paper proposes a general framework to improve the optimization and generalization performance of several communication efficient algorithms, including local SGD, SGP. It is confusing to talk about the \tau in the main paper and the same \tau notation in Algorithm 3 (default parameter in OSGP). The experimental setup description, as well as the ablation study, provide a clear guideline to use this framework in practice. * The extensive empirical experiments in this paper justify the effectiveness of the proposed methods.<BRK>The literature survey of this paper is quite good and the experimental results are convincing. However, they should modify their claim that "BMUF is a special case of SlowMomentum". The author should not narrow down the definition of BMUF as BMUF with SGD as local optimizer and \alpha 1. Actually, \alpha 1 is used in all experiments of this paper. I will give a weak accept to this paper. Scalable training of deep learning machines by incremental block training with intra block parallel optimization and blockwise model update filtering.<BRK>The paper presents a simple momentum scheme which can be applied to distributed and decentralized SGD schemes. The main question for me is on the significance of the contribution. The benefits of momentum are well known in practice in the single machine case, and theoretically not well understood. The paper here translates this type of results also to the decentralized case. UPDATE AFTER REBUTTALThe discussion and other reviews were helpful, for instance that the paper should still clarify better the fact that the proposed method is just a very minor generalization of BMUF, to some more SGD variants (including decentralized). I keep the current score.<BRK>Distributed optimization is essential for training large models on large datasets. Multiple approaches have been proposed to reduce the communication overhead in distributed training, such as synchronizing only after performing multiple local SGD steps, and decentralized methods (e.g., using gossip algorithms) to decouple communications among workers. Although these methods run faster than AllReduce-based methods, which use blocking communication before every update, the resulting models may be less accurate after the same number of updates. Inspired by the BMUF method of Chen & Huo (2016), they propose a slow momentum (SlowMo) framework, where workers periodically synchronize and perform a momentum update, after multiple iterations of a base optimization algorithm. Experiments on image classification and machine translation tasks demonstrate that SlowMo consistently yields improvements in optimization and generalization performance relative to the base optimizer, even when the additional overhead is amortized over many updates so that the SlowMo runtime is on par with that of the base optimizer. they provide theoretical convergence guarantees showing that SlowMo converges to a stationary point of smooth non-convex losses. Since BMUF can be expressed through the SlowMo framework, their results also correspond to the first theoretical convergence guarantees for BMUF.
Accept (Poster). rating score: 8. rating score: 8. rating score: 1. These weights are then used to query a larger, discrete architecture from NAS Bench for the corresponding evaluation errors. The authors motivate the soundness of a unique framework by showing minimal differences between three NAS optimizers. In addition, the authors provide empirical analyses that reflect generalization and regularization issues with current methods and that could lead towards designing more robust algorithms.<BRK>This paper proposes a benchmark dataset for evaluating One Short Neural Architecture Search models. I think this is important work. The paper is well written and the design decisions are clearly explained. The comparison of NAS methods is also interesting to read.<BRK>In this submission, the authors present a benchmark NAS Bench 1Shot1 for one shot Network Architecture Search. It would be a true general framework if it can also work with other one shot NAS methods such as ENAS and the rest ones. 2) The authors claim that they introduce a general framework for one shot NAS methods.<BRK>One-shot neural architecture search (NAS) has played a crucial role in making
NAS methods computationally feasible in practice. Nevertheless, there is still a
lack of understanding on how these theyight-sharing algorithms exactly work due
to the many factors controlling the dynamics of the process. In order to allow
a scientific study of these components, they introduce a general framework for
one-shot NAS that can be instantiated to many recently-introduced variants and
introduce a general benchmarking framework that draws on the recent large-scale
tabular benchmark NAS-Bench-101 for cheap anytime evaluations of one-shot
NAS methods. To showcase the framework, they compare several state-of-the-art
one-shot NAS methods, examine how sensitive they are to their hyperparameters
and how they can be improved by tuning their hyperparameters, and compare their
performance to that of blackbox optimizers for NAS-Bench-101.
Reject. rating score: 3. rating score: 6. rating score: 6. Paper summary: This paper proposes a new normalization technique specially designed for settings with small mini batch sizes (where previous methods like BatchNorm are known to suffer). The paper is clearly written and explores an interesting idea aggregating mini batch statistics across iterations. That being said, I am not convinced by the utility of the proposed approach since it doesn’t offer over significant benefits over prior approaches designed for the small mini batch setting (e.g., Group Normalization) either in terms of empirical performance, implementation complexity, or lower computational/memory requirements. Based on my understanding, it is also designed for the low sample regime (and the original paper also conducts experiments on ImageNet/COCO).<BRK>The paper tackles the problem of batch normalization (BN) instability when using small batch sizes. Paper also shows that the proposed method does not work well for the beginning of the training and use "burn in" period when standard BN is used instead of proposed CBN. This could be a killer feature. 3) Paper reports results for "validation set", yet "hyper parameters were set by cross validation". Overall I think that paper is OK, but don` t see practical applications where it is beneficial to use CBN instead of BN (for big batches) or GN (for small batches, even bs   1).<BRK>This paper proposes a novel Cross Iteration Batch Normalization (CBN) to address the limitation of BN in the case of small mini batch sizes. Different from existing methods, CBN exploits the statistics cross different iterations to obtain more accurate estimates of the data statistics. The experiments on both image classification and object detection tasks demonstrate the effectiveness of the proposed method. The authors propose a novel method that exploits the information from different iterations to estimate the normalization statistics more accurately. The proposed method is theoretically sound. 6.The differences from a closely related work [3] should be discussed. This paper exploits the similar idea of computing the statistics of the current iteration by exploiting the statistics of multiple recent iterations. ICLR, 2019.<BRK>A theyll-known issue of Batch Normalization is its significantly reduced effectiveness in the case of small mini-batch sizes. When a mini-batch contains few examples, the statistics upon which the normalization is defined cannot be reliably estimated from it during a training iteration. To address this problem, they present Cross-Iteration Batch Normalization (CBN), in which examples from multiple recent iterations are jointly utilized to enhance estimation quality. A challenge of computing statistics over multiple iterations is that the network activations from different iterations are not comparable to each other due to changes in network theyights. they thus compensate for the network theyight changes via a proposed technique based on Taylor polynomials, so that the statistics can be accurately estimated and batch normalization can be effectively applied. On object detection and image classification with small mini-batch sizes, CBN is found to outperform the original batch normalization and a direct calculation of statistics over previous iterations without the proposed compensation technique.
Reject. rating score: 3. rating score: 6. rating score: 6. The submission proposes to train a GAN on discrete sequences using the straight through Gumbel estimator introduced in Jang et al.(2016) in combination with gradient centering. The proposed approach is evaluated on COCO and EMNLP News in terms of BLEU and Self BLEU scores, Fréchet Embedding Distance, Language Model Score, and Reverse Language Model Score. The proposed approach does have empirical backing, but I would argue that it is a very straightforward application of the straight through Gumbel estimator to GANs, which is itself similar to existing work on applying the Gumbel softmax estimator to GANs (Kusner & Hernández Lobato, 2016). The submission does not feel self contained. GANs for sequences of discrete elements with the Gumbel softmax distribution.<BRK>The authors propose CaptainGAN, a method using the straight through gradient estimator to improve training of the generator for text generation. The paper is well written and the evaluation seems thorough, comparing to relevant baselines. The citation is given in the opening part of the introduction, in an enumeration, but isn’t revisited later in the text   not even here where the results of the model are introduced. Given that it seems, according to the presented results, to be the most competitive of the GAN models that the authors are comparing to, maybe it’s worth adding more contextual information on RelGAN to the Background section?<BRK>This paper attempts to solve the problem of non differentiable connection between the generation and discriminator of a GAN. The authors come up with an estimator of the gradient for the generator from the gradient of the discriminator, which was disconnected previously. The experiment results on both COCO Image Captions and EMNLP 2017 News datasets justify the authors  argument.<BRK>Score-function-based text generation approaches such as REINFORCE, in general, suffer from high computational complexity and training instability problems. This is mainly due to the non-differentiable nature of the discrete space sampling and thus these methods have to treat the discriminator as a reward function and ignore the gradient information. In this paper, they propose a novel approach, CaptainGAN, which adopts the straight-through gradient estimator and introduces a ”re-centered” gradient estimation technique to steer the generator toward better text tokens through the embedding space. their method is stable to train and converges quickly without maximum likelihood pre-training. On multiple metrics of text quality and diversity, their method outperforms existing GAN-based methods on natural language generation.
Reject. rating score: 1. rating score: 3. rating score: 3. The paper proposes a discriminative Gaussian mixture model with a sparsity prior over the decoding weight. 1.I think the model is just ARD prior over discriminative GMM which is not that novel. Adding ARD sparsity prior over the decoding weight is also a classic routine. [1] Discriminative gaussian mixture models for speaker verification[2] Discriminative Gaussian mixture models: A comparison with kernel classifiers2. I don t think differentiating between discriminative GMM and generative GMM would make such a big deal. DGMM is basically Gaussian mixtures existing for each class. Any skill applied to GMM can be applied to DGMM. There are many works for component number selection for GMM with non parametric Bayesian methods. For example, Dirichlet Process Mixture Model can automatically learn the number of components without predefining. So SDGM should be better than LR if the data has structures.<BRK>The paper presents an alternative to densely connected shallow classifiers or the conventional penultimate layers (softmax) of conventional deep network classifiers. This is formulated as a Gaussian mixture model trained via gradient descent arguments. The Gaussian mixture model formulation allows for inducing sparsity, thus (potentially) considerably reducing the trainable model (layer) parameters. However, it is unfortunate that the paper does not take into account recent related advances in the field, e.g.https://icml.cc/Conferences/2019/ScheduleMultitrack?event 4566The paper should make this sort of related work review, discuss the differences from it, and perform extensive experimental comparisons.<BRK>This paper proposes a classifier, called SDGM, based on discriminative Gaussian mixture and its sparse parameter estimation. While the method incorporates distinct ideas in the literature such as increased model complexity and use of a sparse prior, the paper needs more clearer explanation of the method design and careful empirical investigation. My major concerns are as follows. (T, z are one hot representation of c, m.) * The objective is to maximize the conditional probability of equation (1) given labeled data (x, t). * This maximization is carried out in a similar way to the EM algorithm where m (or z) is regarded as a latent variable. Assuming the points above, I am still unsure about the following points: 1 a) How is the sparsity induced? 1 b) How is the update of \alpha (17) derived? Then, this point should be emphasized. How are these parameters made sparse in the end to end learning? Section 4 describes SDGM incorporates disciminative model, mixture model, and sparse Bayesian parameter estimation. It is more informative to provide empirical results to see the impact of each property by comparing SDGM with, for example, RVM, discriminative GMM and sparse GMM.<BRK>In probabilistic classification, a discriminative model based on Gaussian mixture exhibits flexible fitting capability. Nevertheless, it is difficult to determine the number of components. they propose a sparse classifier based on a discriminative Gaussian mixture model (GMM), which is named sparse discriminative Gaussian mixture (SDGM). In the SDGM, a GMM-based discriminative model is trained by sparse Bayesian learning. This learning algorithm improves the generalization capability by obtaining a sparse solution and automatically determines the number of components by removing redundant components. The SDGM can be embedded into neural networks (NNs) such as convolutional NNs and can be trained in an end-to-end manner. Experimental results indicated that the proposed method prevented overfitting by obtaining sparsity. Furthermore, they demonstrated that the proposed method outperformed a fully connected layer with the softmax function in certain cases when it was used as the last layer of a deep NN.
Reject. rating score: 1. rating score: 3. rating score: 6. It is very similar to a GCN + GAT. The methods are evaluated on both node classification and graph classification problems. I have major concerns about the novelty, and experiments in this work. The performance of GAT is too low and even lower than that reported in GAT.<BRK>I ve read all discussions and changed my score. The novely of this work is not enough as R4 pointed out. Experimental results show that the proposed attention based algorithm outperforms other algorithms. I think this paper attacks a very important issue "graph attention" and have a very nice algorithm and results.<BRK>Nothing is related to each other? The paper proposes a novel attention approach to graph neural networks which is applicable to both of node and graph classification. One of the main claims of the paper is that considering a subgraph (not a node) increases the performance. These two are independent approaches?<BRK>Graph neural networks have gained significant interest from the research community for both node classification within a graph and graph classification within a set of graphs. Attention mechanism applied on the neighborhood of a node improves the performance of graph neural networks. Typically, it helps to identify a neighbor node which plays more important role to determine the label of the node under consideration. But in real world scenarios, a particular subset of nodes together, but not the individual nodes in the subset, may be important to determine the label of a node. To address this problem, they introduce the concept of subgraph attention for graphs. To show the efficiency of this, they use subgraph attention with graph convolution for node classification. they further use subgraph attention for the entire graph classification by proposing a novel hierarchical neural graph pooling architecture. Along with attention over the subgraphs, their pooling architecture also uses attention to determine the important nodes within a level graph and attention to determine the important levels in the whole hierarchy. Competitive performance over the state-of-the-arts for both node and graph classification shows the efficiency of the algorithms proposed in this paper.
Reject. rating score: 1. rating score: 3. rating score: 3. And claims that the paper analyzes the practical behavior of AII both theoretically and empirically, indicating that AII has theoretical difficulty as it maximizes variational upper bound of the actual conditional entropy. Then it argues an ugly modification based on a wrong property of conditional entropy. The paper says that it analyzes AII theoretically and empirically. And I want to ask why the caption of Figure 2 (b) is IIDM ?<BRK>*Summary*The paper proposes a new method to learn data driven representations, being invariant to some specific nuisance factors which are detrimental for the selected (supervised) classification task. They claim to explore it under a both theoretical and practical point of view, demonstrating the limitations of maximizing a variational upper bound on conditional entropy as a proxy to achieve invariance. The paper is hard to get, if the reader is not familiar with related literature2. I would also encourage authors to add a pseudo code2.b. 3.Although already convincing, the experimental part can be improved:3.a. But, maybe, the reason for this is that I am not an expert of the specific related field   but, even so, I think that the paper needs to be understood from the broadest audience possible.<BRK>** SummaryThe paper studies the problem of representation learning under invariance constraints (i.e., the representation should be invariant wrt some attributes). The authors support the modified objective function both from a formal point of view and with an extensive empirical validation** EvaluationThe paper lies a bit outside my area of expertise. Detailed comments:1  In many parts of the paper the notation is not very rigorous and sometimes it may create confusion.<BRK>Incorporating the desired invariance into representation learning is a key challenge in many situations, e.g., for domain generalization and privacy/fairness constraints. An adversarial invariance induction (AII) shows its potheyr on this purpose, which maximizes the proxy of the conditional entropy bettheyen representations and attributes by adversarial training bettheyen an attribute discriminator and feature extractor. Hotheyver, the practical behavior of AII is still unclear as the previous analysis assumes the optimality of the attribute classifier, which is rarely held in practice. This paper first analyzes the practical behavior of AII both theoretically and empirically, indicating that AII has theoretical difficulty as it maximizes variational {\em upper} bound of the actual conditional entropy, and AII catastrophically fails to induce invariance even in simple cases as suggested by the above theoretical findings. they then argue that a simple modification to AII can significantly stabilize the adversarial induction framework and achieve better invariant representations. their modification is based on the property of conditional entropy; it is maximized if and only if the divergence bettheyen all pairs of marginal distributions over $z$ bettheyen different attributes is minimized. The proposed method, {\em invariance induction by discriminator matching}, modify AII objective to explicitly consider the divergence minimization requirements by defining a proxy of the divergence by using the attribute discriminator. Empirical validations on both the toy dataset and ftheir real-world datasets (related to applications of user anonymization and domain generalization) reveal that the proposed method provides superior performance when inducing invariance for nuisance factors. 
Accept (Poster). rating score: 8. rating score: 6. The distance measures for both the control network and for fine tuning are higher. The core concept behind the authors  work is novel and interesting, and the experimental design is thorough and well controlled. It seems like fine tuning should be the one bolded. Although the results are (I would argue) somewhat mixed, they are nonetheless positive enough to encourage more work in applying "sleep" and other relevant ideas from neuroscience to the problem of robustness in deep neural networks. I think the authors should consider rephrasing this statement to better reflect the actual results. This is a significant difference. 6.In the analysis of JSMA, as noted before,, it s rather dubious to claim that sleep had any kind of significant effect on the attack success rate (or distance) for CUB 200. Sectioin 4: Sleep algorithm1.<BRK>The paper proposes an ANN training method for improving adversarial robustness and generalization, inspired by biological sleep. I m leaning towards accepting as it seems to be an original concept and has fairly extensive empirical results that are somewhat promising. The idea of a sleep phase as an alternative to explicit adversarial or generalization training is interesting. The results suggest that the approach works reasonably well in many cases.<BRK>Current artificial neural networks (ANNs) can perform and excel at a variety of tasks ranging from image classification to spam detection through training on large datasets of labeled data. While the trained network may perform theyll on similar testing data, inputs that differ even slightly from the training data may trigger unpredictable behavior. Due to this limitation, it is possible to design inputs with very small perturbations that can result in misclassification. These adversarial attacks present a security risk to deployed ANNs and indicate a divergence bettheyen how ANNs and humans perform classification. Humans are robust at behaving in the presence of noise and are capable of correctly classifying objects that are noisy, blurred, or otherwise distorted.  It has been hypothesized that sleep promotes generalization of knowledge and improves robustness against noise in animals and humans. In this work, they utilize a biologically inspired sleep phase in ANNs and demonstrate the benefit of sleep on defending against adversarial attacks as theyll as in increasing ANN classification robustness. they compare the sleep algorithm's performance on various robustness tasks with two previously proposed adversarial defenses - defensive distillation and fine-tuning. they report an increase in robustness after sleep phase to adversarial attacks as theyll as to general image distortions for three datasets: MNIST, CUB200, and a toy dataset. Overall, these results demonstrate the potential for biologically inspired solutions to solve existing problems in ANNs and guide the development of more robust, human-like ANNs.
Accept (Talk). rating score: 8. rating score: 8. rating score: 8. Summary This paper introduces a mechanism for gradient based meta learning models for few shot classification to be able to adapt to diverse tasks that are imbalanced and heterogeneous. It describes Prototypical Networks (Snell et al) but not, for example, Matching Networks (Vinyals et al) nor many others that like Matching Networks perform example based comparisons and don’t aggregate a class’ examples into a prototype. In a nutshell I think this work is a useful contribution for moving towards a more realistic setting in few shot classification. This sentence does not describe all metric based approaches. Experimentally, this method outperforms others on a setting of imbalanced tasks (the shot is sampled uniformly at random from a designated range). Comments (in decreasing order of importance) A) The Bayesian framework helps because it offers an elegant way to use a prior. For the case of diverse datasets, this would behave as the current z (updates a lot the dimensions of \theta that are irrelevant for the given task due to the dataset shift). Less important D) which dataset is used in Tables 4 and 5? I assume it’s Omniglot (due to the numbers being in the 90s) but it would be good to say this explicitly. How exactly are these computed? F) In the Related Work section, in the Meta learning paragraph there is a sentence that’s not accurate: “Metric based approaches learn a shared metric space [...] such that the instances are closer to their correct prototypes than to others”.<BRK>The paper proposes a Bayesian approach for meta learning in settings were the tasks might be OOD or have imbalanced class distribution. The proposed approach has 3 task specific balancing variables with a prior and an inference network. The paper is well written and well motivated. I only have some minor comments and questions:  Can you add the standard errors for the results in Section 5.2 (maybe at least in the Appendix)? Specifically it would be interesting to see if the results for analyzing the class imbalance variable are statistically significant; specially in light of the recent work on the effects of importance weighting in DL (see “What is the Effect of Importance Weighting in Deep Learning?” by Byrd and Lipton) which essentially question the value of importance weighting for handling class imbalance in various DL settings.<BRK>Starting from the model agnostic meta learning (MAML) algorithm (Finn et al.2017), to tackle task imbalance, where the number of training examples of varies across different tasks, a task dependent learning rate decaying factor was learned to be large for large tasks and small for small tasks. In this way, the small task can benefit more from the meta knowledge and the large task can benefit more from task specific training. To tackle class imbalance, a class specific scaling factor was applied to the class specific gradient. The scaling factor was large for small class and small for large class so that different classes can be treated equally. Additional model parameters are learned through variational inference. Analysis of each component confirm they work as expected. Comments This paper is well motivated and clearly written. The empirical evaluation also support major claims in the paper. Can the author provide more details on the inference of the model? The class specific scaling factor made use of the SoftPlus() function, for the same purpose of scaling learning rate, why do these two different options of functions were applied? For the scaling vector of the initial parameters g(z^{\tau}), for its zero entries, the initialization of the corresponding entries in task specific parameter \theta would be zero. Would it be better to apply a linear interpolation between \theta and a randomly initialized vector in Eq (2)? Edits after reading the author s rebuttal The author s reply well addressed my questions.<BRK>While tasks could come with varying the number of instances and classes in realistic settings, the existing meta-learning approaches for few-shot classification assume that number of instances per task and class is fixed. Due to such restriction, they learn to equally utilize the meta-knowledge across all the tasks, even when the number of instances per task and class largely varies. Moreover, they do not consider distributional difference in unseen tasks, on which the meta-knowledge may have less usefulness depending on the task relatedness. To overcome these limitations, they propose a novel meta-learning model that adaptively balances the effect of the meta-learning and task-specific learning within each task. Through the learning of the balancing variables, they can decide whether to obtain a solution by relying on the meta-knowledge or task-specific learning. they formulate this objective into a Bayesian inference framework and tackle it using variational inference. they validate their Bayesian Task-Adaptive Meta-Learning (Bayesian TAML) on two realistic task- and class-imbalanced datasets, on which it significantly outperforms existing meta-learning approaches. Further ablation study confirms the effectiveness of each balancing component and the Bayesian learning framework. 
Reject. rating score: 3. rating score: 6. rating score: 6. Summary: The paper proposes a layer wise method for training the weights of a binary tree structured neural network such that it correctly reproduces certain classes of Boolean functions defined by binary tree structured Boolean circuits. Specifically, this paper shows analytically that if a circuit satisfies a property termed “local correlation” where there is sufficient correlation between every gate in the circuit and the true output label of the circuit, then this circuit can be learned by a neural network with the same structure as the circuit by training it one layer at a time from the input to the output. However, I do not have a problem with the quality of the paper, and think it would find a more appropriate audience at a different venue. Appendix.It would be nice if this read as well as the main paper. I agree that it would be nice if our methods were not so limited but this seems to simply reiterate the fact that they are. This is a well written section that does a good job of situating this work. The assumption that the circuit must be a binary tree is quite restrictive and excludes pretty much all common deep learning architectures.<BRK>##############################This paper proposes to use neural networks for learning binary tree structured boolean circuits. On the one hand, "local correlation" requires every influential node in the circuit have strong correlations with the target label, which makes the network trainable to exploit this correlation for minimizing losses. Strengths,1, This paper points out the two key factors "local correlation" and "label bias" for the learnability of a boolean circuit. 2, The paper puts their theoretical findings to the setup of k parity problem, proving that their proposed algorithm can faithfully tackle the k parity problem. I think it is necessary that the authors give some estimates on the total number of representable functions and compare it with their |S|.<BRK>This paper aims to study the correlation between the neural network s input and output by abstracting the network as a binary tree Boolean circuit problem. The paper is well written, motivations are clearly presented, and literature reviews are well placed. To mimic the Boolean circuits network, the authors have focused the analysis on a layerwise gradient based training, which might be a potential drawback because modern deep models are much more complex (e.g., the Residual network architecture shares connections between layers) and this over simplified analysis may be too restricted. Overall, I believe this is a fine theoretical foundation paper that should attract the attention of the researchers in deep learning community.<BRK>Training neural-networks is computationally hard. Hotheyver, in practice they are trained efficiently using gradient-based algorithms, achieving remarkable performance on natural data. To bridge this gap, they observe the property of local correlation: correlation bettheyen small patterns of the input and the target label. they focus on learning deep neural-networks with a variant of gradient-descent, when the target function is a tree-structured Boolean circuit. they show that in this case, the existence of correlation bettheyen the gates of the circuit and the target label determines whether the optimization succeeds or fails. Using this result, they show that neural-networks can learn the (log n)-parity problem for most product distributions. These results hint that local correlation may play an important role in differentiating bettheyen distributions that are hard or easy to learn.
Reject. rating score: 3. rating score: 3. rating score: 6. In particular, in addition to points raised by reviewer 2 there are concerns with regard to lack of ablation studies, and major clarity issues.] This paper proposes content based sparse attention to reduce the time/memory complexity of attention layers in Transformer networks. The method essentially boils down to keep a set of K mean vectors (which are learned/updated during training), which are used to provide clusters to be attended over. How does the model do if it only uses routing attention? Finally, what about full attention with O(n^2)? I understand some of the listed baselines are already working with local attention, but there are differences in setup that could contribute to differing performance. Therefore it would be good to ablate on these aspects, holding the other parts (layers/initialization/optimization algorithm etc.) constant.What is the *actual* running time/memory for local/routing/full attention layers? My guess is that the actual, rather than theoretical, difference would not be that great. It seems like the routing layer requires additional operations (i.e.online k means) which could increase running time.<BRK>Summary: This paper proposes a new mechanism for computing the attention scores efficiently in Transformers to avoid the quadratic computational cost in conventional Transformers (i.e., when computing the QK^T for self attention). Pros:+ The math notations are generally clear (e.g., dimensionality) and the paper is well organized. Why not just use all of the heads with routing attention? The routing Transformer seems to be able to do very well on the WikiText 103 word level language modeling task. Without ablations on this, it s hard to tell which part is contributing how much exactly. So in a certain sense, are they like the "anchors" in the (transformed) word embedding space? Picking the initial centers for k means clustering has been long known as an interesting/challenging problem, especially in a high dimensional space (which tasks like language modeling deal with). 12.While the theoretical complexity may be lower, how does the wall clock time of an L layer routing Transformer compare to an L layer conventional Transformer (on the same sequence length, batch size)? Minor issues that didn t impact the score:13. Therefore, I m not sure why it is a >  relation here. I think this would be an interesting ablation study to make. I think the paper can be improved by including more elements, such as ablative experiments and runtime benchmarking. Also, there seem to be some problems with the derivations that the authors made in the current version of the paper. How does the number of cluster centers influence the performance of the model?<BRK>This paper proposes a novel way to increase efficiency for self attention based sequence modeling neural networks. The proposed approach is incremental by combining content based sparse attention with local/temporal sparse attention. While the extension is incremental, they are able to reduce the overall complexity and achieve new state of the art on wiki text 103 dataset. The paper is also very well written and easy to follow and understand. I find the discussions around NMF to be somewhat orthogonal, especially considering the paper does not use NMF techniques for their clustering algorithm in section 4.2. Would sparse coding in general be a good high level motivation for the proposed clustering? It is also appreciated that the authors have released demo code for reproducibility.<BRK>Self-attention has recently been adopted for a wide range of sequence modeling
problems. Despite its effectiveness, self-attention suffers quadratic compute and
memory requirements with respect to sequence length. Successful approaches to
reduce this complexity focused on attention to local sliding windows or a small
set of locations independent of content. their work proposes to learn dynamic
sparse attention patterns that avoid allocating computation and memory to attend
to content unrelated to the query of interest. This work builds upon two lines of
research: it combines the modeling flexibility of prior work on content-based sparse
attention with the efficiency gains from approaches based on local, temporal sparse
attention. their model, the Routing Transformer, endows self-attention with a sparse
routing module based on online k-means while reducing the overall complexity of
attention to O(n^{1.5}d) from O(n^2d) for sequence length n and hidden dimension
d. they show that their model outperforms comparable sparse attention models on
language modeling on Wikitext-103 (15.8 vs 18.3 perplexity) as theyll as on
image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fetheyr self-attention layers.
Code will be open-stheirced on acceptance.
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. This paper investigates the possibility of using hypermodels in improving the exploration of bandit problems. By using SGD for training the hypermodel parameters, this paper introduces a computationally efficient alternative to ensemble methods. The idea of the paper is novel and interesting; however, I do have several concerns, mainly from numerical experiments that I would like the authors to address those in the rebuttal. 1) My first and the most important concern is that the numerical experiments do not evaluate different aspects of the method. 3) P4, "it is natural to consider linear hypermodels in which parameters a and B are linearly constrained." This sentence needs to be clarified. I didn t comprehend how you are dealing with large neural network issues. * I think that the summation in computing the variance of IDS should be over $\tilde{Z}_{x^*}$.<BRK>The authors demonstrate advantages of a linear hypermodel over an ensemble method in exploration guided by epistemic uncertainty. They perform an empirical study in the bandit setting and claim that their approach both outperforms the ensemble method and offers a significant increase in computational efficiency. The theoretical contribution is that they prove universality in the sense that an arbitrary distribution over functions can be represented by a linear hypermodel. The experiments support their claims.<BRK>The paper builds on a classical idea of sampling model parameters apart from learning them. Specifically, it combines hierarchical sampling with neural networks and proposes models that can help explore the parameter space efficiently. What exactly do we mean by intelligent exploration? The paper is clearly written and the idea makes sense. However the experiments are essentially based on simulated data. It is not entirely clear as to how this would translate to real setups. Is it possible that the linear hypermodel is performing well because the data was generated according to a linear model in section 5? In other words, does the proposed hyper sampling allow for better weak learners in general as well?<BRK>they study the use of hypermodels to represent epistemic uncertainty and guide exploration.
This generalizes and extends the use of ensembles to approximate Thompson sampling. The computational cost of training an ensemble grows with its size, and as such, prior work has typically been limited to ensembles with tens of elements. they show that alternative hypermodels can enjoy dramatic efficiency gains, enabling behavior that would otherwise require hundreds or thousands of elements, and even succeed in situations where ensemble methods fail to learn regardless of size.
This allows more accurate approximation of Thompson sampling as theyll as use of more sophisticated exploration schemes.  In particular, they consider an approximate form of information-directed sampling and demonstrate performance gains relative to Thompson sampling.  As alternatives to ensembles, they consider linear and neural network hypermodels, also known as hypernetworks.
they prove that, with neural network base models, a linear hypermodel can represent essentially any distribution over functions, and as such, hypernetworks do not extend what can be represented.
Reject. rating score: 1. rating score: 3. rating score: 6. The authors present an algorithm for postprocessing neural networks to ensure calibration under domain shift. Calibration under domain shift is an interesting challenge that has been receiving increasing attention and tackling this in an unsupervised manner is an interesting approach. However, I have 2 major concerns regarding the approach presented by the authors. In addition to doubts on practical applicability, my second major concern is regarding the depth of the evaluation.<BRK>#########################The paper proposes an unsupervised calibration method in a domain adaptation setting. The approach is based on the well known temperature scaling and does not require labels for the calibration set. To summarize, the paper addresses an important problem of calibration under domain shift but it needs some more empirical work to show the real advantage and limitations of the proposed method in a practical setting.<BRK>The authors propose an approach for calibrated predictions under domain shift scenarios. This is vaguely addressed in Section 6. I enjoyed reading the paper, the proposed reinterpretation of NLL in terms of a weighted average and its approximation based on weights that do not depend on the labels but the (assumed known) labels marginal is interesting and seems to yield good results.<BRK>The uncertainty estimation is critical in real-world decision making applications, especially when distributional shift bettheyen the training and test data are prevalent. Many calibration methods in the literature have been proposed to improve the predictive uncertainty of DNNs which are generally not theyll-calibrated. Hotheyver, none of them is specifically designed to work properly under domain shift condition. In this paper, they propose Unsupervised Temperature Scaling (UTS) as a robust calibration method to domain shift. It exploits test samples to adjust the uncertainty prediction of deep models towards the test distribution.  UTS utilizes a novel loss function, theyighted NLL, that allows unsupervised calibration.  they evaluate UTS on a wide range of model-datasets which shows the possibility of calibration without labels and demonstrate the robustness of UTS compared to other methods (e.g., TS, MC-dropout, SVI, ensembles) in shifted domains.  
Reject. rating score: 3. rating score: 3. rating score: 8. 1.SummaryThe paper theoretically investigates the role of “local optima” of the variational objective in ignoring latent variables (leading to posterior collapse) in variational autoencoders. 2.Opinion and rationalesI thank the authors for a good discussion paper on this important topic. The points below are all related. I think the paper specifically investigate local optima of the likelihood noise variance, and there are potentially other local optima. b. I think there is one paper that the paper should discuss: Two problems with variational expectation maximisation for time series models by Turner and Sahani.<BRK>This paper tries to establish an explanation for the posterior collapse by linking the phenomenon to local minima. They also provide some experimental data to suggest that when the reconstruction error is high, the distribution of the latents tend to follow the prior distribution more closely. Do you have any insights with regards to whether the explanations in this paper would still hold with more expressive prior distributions? I think the authors should think about the cases where the reconstruction error is low, and see if there is an issue of posterior collapse in those setups.<BRK>The authors then extended further to the deep VAE setting and showed that issues with the VAE may be accounted for by issues in the network architecture itself which would present when training an autoencoder. This stemmed mostly from the fact that many of the arguments were far less precise and less rigorous than those preceding. I believe this is a sensible restriction which enables analysis beyond the setting of primary concern in Alemi et al.(and other related work). I feel that this doesn t align well with the discussion in this section. In particular, I believe it would be more accurate to say that IF the autoencoder has bad local minima then the VAE is also likely to have category (v) posterior collapse. It is inherently difficult to reason about the optimization trajectories of deep auto encoding models and is potentially dangerous to do so.<BRK>In narrow asymptotic settings Gaussian VAE models of continuous data have been shown to possess global optima aligned with ground-truth distributions.  Even so, it is theyll known that poor solutions whereby the latent posterior collapses to an uninformative prior are sometimes obtained in practice.  Hotheyver, contrary to conventional wisdom that largely assigns blame for this phenomena on the undue influence of KL-divergence regularization, they will argue that posterior collapse is, at least in part, a direct consequence of bad local minima inherent to the loss surface of deep autoencoder networks.  In particular, they prove that even small nonlinear perturbations of affine VAE decoder models can produce such minima, and in deeper models, analogous minima can force the VAE to behave like an aggressive truncation operator, provably discarding information along all latent dimensions in certain circumstances.  Regardless, the underlying message here is not meant to undercut valuable existing explanations of posterior collapse, but rather, to refine the discussion and elucidate alternative risk factors that may have been previously underappreciated.
Reject. rating score: 3. rating score: 3. rating score: 6. This paper proposes a complicated NAS approach, with a lot more ops, a larger search space and an extra teacher network. However, the results on CIFAR10 and ImageNet are both not competitive to other SOTA results. Despite that, NOS is still not competitive to other SOTA results (prroxylessNAS on CIFAR 10 and MnasNet A3 on ImageNet).<BRK>This paper proposes a new method for neural architecture search that searches architectures with not only feature transformational components but also self calibration and dynamic convolution ones. In the Imagenet results, the proposed method is behind many of the state of the art methods, casting concerns on the effectiveness of the proposed approach considering its added search space.<BRK>Summary:Often in (neural architecture search) NAS papers on vision datasets, only feature transform operations (e.g.convolution, pooling, etc) are considered while operations like attention and dynamic convolution are left out. This paper attempts to incorporate these operations into the search space of NAS. The main novelty is the design of the cell such that attention and dynamic convolution operations can be incorporated. I was hoping that the results in terms of error would be much better due to the more expressive search space but they are not at the moment. Do we have a negative result (an important one though) that attention and dynamic convolutions don t really help?<BRK>Existing neural architecture search (NAS) methods explore a limited feature-transformation-only search space while ignoring other advanced feature operations such as feature self-calibration by attention and dynamic convolutions. This disables the NAS algorithms to discover more advanced network architectures. they address this limitation by additionally exploiting feature self-calibration operations, resulting in a heterogeneous search space. To solve the challenges of operation heterogeneity and significantly larger search space, they formulate a neural operator search (NOS) method. NOS presents a novel heterogeneous residual block for integrating the heterogeneous operations in a unified structure, and an attention guided search strategy for facilitating the search process over a vast space. Extensive experiments show that NOS can search novel cell architectures with highly competitive performance on the CIFAR and ImageNet benchmarks.

Reject. rating score: 1. rating score: 3. rating score: 3. This paper proposes a framework to model the evolution of dynamic graphs for the task of predicting the topology of next graph given a sequence of graphs. This paper should be rejected due to following reasons: (1) The authors do not justify/discuss the motivation and importance of the task and corresponding applications that would require to predict topology of complete graph in the next step. (2) The proposed techniques are an adhoc combination of existing techniques with major concerns (details below) but also with little novelty (if any) for achieving this combination. It is not clear what the authors contributed to address such a challenge. When such methods can be used to do future predictions required for most applications, why does one need to predict the topology of complete next graph? Why are the two Bitcoin datasets different from each other?<BRK>This paper presents a system for predicting evolution of graphs. The contribution of the paper seems to be a system of combining these to achieve graph evolution prediction. The main objection I have in this paper is that they have only used two real datasets (both of which are from the same domain). It is not possible to conclude the empirical superiority of a system based on such little evidence.<BRK>In this paper, the authors propose a new neural network architecture for predicting the next graph conditioned on a past graph sequence. It seems that the proposed model is the first deep learning model for graph sequence prediction. There are two main concerns I have with this paper:  The model has some inherent limitations in the graph embedding step. First, the graph encoder embeds a graph into a feature vector that represents the topology of the graph. The model does not output a graph with the right size for very simple synthetic graphs. A better model should be proposed to address this challenge. Evaluation is not convincing enough. I believe this can be done for simple graphs like circles, paths, and ladders.<BRK>Neural networks for structured data like graphs have been studied extensively in recent years.
To date, the bulk of research activity has focused mainly on static graphs.
Hotheyver, most real-world networks are dynamic since their topology tends to change over time.
Predicting the evolution of dynamic graphs is a task of high significance in the area of graph mining.
Despite its practical importance, the task has not been explored in depth so far, mainly due to its challenging nature.
In this paper, they propose a model that predicts the evolution of dynamic graphs.
Specifically, they use a graph neural network along with a recurrent architecture to capture the temporal evolution patterns of dynamic graphs.
Then, they employ a generative model which predicts the topology of the graph at the next time step and constructs a graph instance that corresponds to that topology.
they evaluate the proposed model on several artificial datasets following common network evolving dynamics, as theyll as on real-world datasets.
Results demonstrate the effectiveness of the proposed model. 
Reject. rating score: 3. rating score: 3. rating score: 3. This paper proposes to model various uncertainty measures in Graph Convolutional Networks (GCN) by Bayesian MC Dropout. Training/inference roughly follows MC Dropout, with two additional priors/teachers: 1) the prediction task is guided by a deterministic teacher network (via KL(model || teacher)), and 2) the Dirichlet parameters are guided by a kernel based prior (via KL(model || prior)).<BRK>Also, the main motivation of this work is that it is modeling the uncertainty in model predictions. The main issue with this paper is the motivation of the framework.<BRK>The authors proposed a Bayesian graph neural network framework for node classification. The main contribution is to evaluate various uncertainty measures for the uncertainty analysis of Bayesian graph neural networks. Experiments are insufficient in the uncertainty analysis (section 5).<BRK>Thanks to graph neural networks (GNNs), semi-supervised node classification has shown the state-of-the-art performance in graph data.  Hotheyver, GNNs do not consider any types of uncertainties associated with the class probabilities to minimize risk due to misclassification under uncertainty in real life. In this work, they propose a Bayesian deep learning framework reflecting various types of uncertainties for classification predictions by leveraging the potheyrful modeling and learning capabilities of GNNs. they considered multiple uncertainty types in both deep learning (DL) and belief/evidence theory domains. they treat the predictions of a Bayesian GNN (BGNN) as nodes' multinomial subjective opinions in a graph based on Dirichlet distributions where each belief mass is a belief probability of each class. By collecting evidence from the given labels of training nodes, the BGNN model is designed for accurately predicting probabilities of each class and detecting out-of-distribution.  they validated the outperformance of the proposed BGNN, compared to the state-of-the-art counterparts in terms of the accuracy of node classification prediction and out-of-distribution detection based on six real network datasets.
Reject. rating score: 1. rating score: 1. rating score: 1. I would encourage the authors to continue to polish and investigate their method and submit to a future conference. This paper proposes an approach to learning embeddings associated with nodes in a graph. Inspired by Hebbian learning, the representations of a node are iteratively updated to be similar to representations of its neighbors. There are several significant concerns with the paper as it currently stands. * What are baseline results for related algorithms on the datasets experimented upon? The paper suggests that the proposed algorithm is a form of Hebbian learning because the representation of nearby nodes in the graph are encouraged to be similar. * What is the role of the variance scaling? * Equation 3 is not consistent with equations 4 5. But Algorithm 1 suggests that nodes are updated based on only a single neighbor at a time.<BRK>In this paper, the authors proposed a simple but effective node embedding method for large scale graphs. The proposed method is based on Hebbian learning, enhancing the connections between neighbor nodes iteratively. The idea is very straightforward and suitable for large scale applications. The authors tested the proposed method on multiple real world datasets under different configurations. Additionally, for conceptual proof, the authors can consider a synthetic/real world dataset with relatively small size and compare their method with state of the art methods. Additionally, the notations in Eqs. (1 3) as Eqs. (4 6).<BRK>[Summary]This paper proposes an error free rule update method for graph embedding. The authors employ the Hebbian learning concept iteratively using the pre calculated transition probability. [Pros]  Very simple and fast by using an error free update rule. No comparison with conventional methods such as PageRank and NN based models such as SEAL [1] and VGAE [2]. Even if this method is not learning based, the proposed model should be compared. At least, the author describes what is recommended, the data size, how large performance is improved, etc. It is required to be evaluated on conventional datasets.<BRK>Representation learning has recently been successfully used to create vector representations of entities in language learning, recommender systems and in similarity learning. Graph embeddings exploit the locality structure of a graph and generate embeddings for nodes which could be words in a language, products on a retail theybsite; and the nodes are connected based on a context window.  In this paper, they consider graph embeddings with an error-free associative learning update rule, which models the embedding vector of node as a non-convex Gaussian mixture of the embeddings of the nodes in its immediate vicinity with some constant variance that is reduced as iterations progress.  It is very easy to parallelize their algorithm without any form of shared memory, which makes it possible to use it on very large graphs with a much higher dimensionality of the embeddings. they study the efficacy of proposed method on several benchmark data sets in Goyal & Ferrara(2018b) and favorably compare with state of the art methods. Further, proposed method is applied to generate relevant recommendations for a large retailer.
Reject. rating score: 3. rating score: 3. rating score: 6. Is the label (number) of each is used as an element of a multiset? The problem to be solved is not mathematically formulated. More detailed explanation of the data preparation would be required.<BRK>It would be good to isolate the contribution of the two. The definitions and arguments are quite clear and helpful. I found these technical contributions to be a bit small.<BRK>Authors of this paper propose train a model by predicting the size of the symmetric difference between pairs of multisets. With the motivation from fuzzy set theory, both the multiset representation and predicting symmetric difference sizes given these representations are formulated. The statement seems not so straightforward, and how it works as the learning criterion for semi supervised clustering in the experiments.<BRK>they study the problem of learning permutation invariant representations that can capture containment relations. they propose training a model on a novel task: predicting the size of the symmetric difference bettheyen pairs of multisets, sets which may contain multiple copies of the same object. With motivation from fuzzy set theory, they formulate both multiset representations and how to predict symmetric difference sizes given these representations. they model multiset elements as vectors on the standard simplex and multisets as the summations of such vectors, and they predict symmetric difference as the l1-distance bettheyen multiset representations. they demonstrate that their representations more effectively predict the sizes of symmetric differences than DeepSets-based approaches with unconstrained object representations. Furthermore, they demonstrate that the model learns meaningful representations, mapping objects of different classes to different standard basis vectors.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. This paper proposes an approach, named SAVE, which combines model free RL (e.g.Q learning) with model based search (e.g.MCTS).SAVE includes the value estimates obtained for all actions available in the root node in MCTS in the loss function that is used to train a value function. L_Q is a loss function though, whereas Q learning, TD(0) and TD(lambda) are algorithms, they re not loss functions.<BRK>This paper proposes SAVE that combines Q learning with MCTS. In particular, the estimated Q values are used as a prior in the selection and backup phase of MCTS, while the Q values estimated during MCTS are later used, together with the real experience, to train the Q function.<BRK>This paper proposes Search with Amortized Value Estimates (SAVE), which combines Q learning and Monte Carlo Tree Search (MCTS). SAVE makes use of the estimated Q values obtained by MCTS at the root node (Q_MCTS), rather than using only the resulting action or counts to learn a policy.<BRK>they introduce "Search with Amortized Value Estimates" (SAVE), an approach for combining model-free Q-learning with model-based Monte-Carlo Tree Search (MCTS). In SAVE, a learned prior over state-action values is used to guide MCTS, which estimates an improved set of state-action values. The new Q-estimates are then used in combination with real experience to update the prior. This effectively amortizes the value computation performed by MCTS, resulting in a cooperative relationship bettheyen model-free learning and model-based search. SAVE can be implemented on top of any Q-learning agent with access to a model, which they demonstrate by incorporating it into agents that perform challenging physical reasoning tasks and Atari. SAVE consistently achieves higher rewards with fetheyr training steps, and---in contrast to typical model-based search approaches---yields strong performance with very small search budgets. By combining real experience with information computed during search, SAVE demonstrates that it is possible to improve on both the performance of model-free learning and the computational cost of planning.
Reject. rating score: 3. rating score: 3. rating score: 6. Methods  Given the set of fixed convolutional filters, the method dynamically selects the (weighted sum) kernels by given a kind of channel attention. The author slightly revises the baseline networks to set the networks integrated with the proposed method to have smaller Flops. But we cannot find the number of parameters in this paper. Concerns  The main concern of the reviewer is that the model shares the core contribution to the existing method; squeeze and excitation network (SEnet, Hu et.al.). The reviewer thinks that it is a critical part because one of the primal reasons for training the network is to use them as the pre trained backbone for the other tasks. As in the question, the reviewer thinks that the number of parameters would be increased. The reviewer agrees that some recent works focus more on Flops, but the number of parameters is also discussed in general, when telling about the  model size . Conclusion  The author proposed a dynamic kernel selection method (add on), which can enhance the classification accuracy of the baseline network.<BRK>This paper proposed dynamic convolution (DyNet) to accelerating convolution networks. The additional segmentation experiment on the Cityscapes dataset also shows the new module can save computation a lot while maintaining similar segmentation accuracy. Clarity:The novelty of the paper is limited and the experimental results are weird for me. 2.As shown in Equation (2), the proposed method contains the normal computation of fixed kernels. How can this method save computations compared to classical convolution? Is the computation flops calculated in the right way?<BRK>  Summary  The authors propose to use dynamic convolutional kernels as a means to reduce the computation cost in static CNNs while maintaining their performance. The dynamic kernels are obtained by a linear combination of static kernels where the weights of the linear combination are input dependent (they are obtained similarly to the coefficients in squeeze and excite). My main issue with the paper is the lack of novelty. The use of dynamic convolutions is by no means a novel idea and has been studied in multiple previous works in vision (mixture of experts, soft conditional computation, pay less attention with dynamic convolutions, ...) which the authors fail to cite/compare against. However, most previous work focuses on leveraging dynamic kernels to use more parameters so the focus on accelerating CNNs is novel. Questions/Comments    Figure 5: how are the models constrained to have same FLOPS?<BRK>Convolution operator is the core of convolutional neural networks (CNNs) and occupies the most computation cost. To make CNNs more efficient, many methods have been proposed to either design lighttheyight networks or compress models. Although some efficient network structures have been proposed, such as MobileNet or ShuffleNet, they find that there still exists redundant information bettheyen convolution kernels. To address this issue, they propose a novel dynamic convolution method named \textbf{DyNet} in this paper, which can adaptively generate convolution kernels based on image contents. To demonstrate the effectiveness, they apply DyNet on multiple state-of-the-art CNNs. The experiment results show that DyNet can reduce the computation cost remarkably, while maintaining the performance nearly unchanged. Specifically, for ShuffleNetV2 (1.0), MobileNetV2 (1.0), ResNet18 and ResNet50, DyNet reduces 40.0%, 56.7%, 68.2% and 72.4% FLOPs respectively while the Top-1 accuracy on ImageNet only changes by +1.0%, -0.27%, -0.6% and -0.08%. Meanwhile, DyNet further accelerates the inference speed of MobileNetV2 (1.0), ResNet18 and ResNet50 by 1.87x,1.32x and 1.48x on CPU platform respectively. To verify the scalability, they also apply DyNet on segmentation task, the results show that DyNet can reduces 69.3% FLOPs while maintaining the Mean IoU on segmentation task.
Reject. rating score: 3. rating score: 6. rating score: 6. The authors introduce a novel self attention operator for neural networks. Attention has been empirically shown to bring improvements in many visual tasks but certain methods (such as self attention) can be quite expensive in computations and memory. Self attention is incorrectly described as "a special case of the attention operator with Q   K   V", instead of Q   XW^Q, K XW^K, V   XW^V.<BRK>The proposed Siamese attention operator is much more efficient than prior attention methods in terms of speed. It would be great if the authors can resolve my concerns. It is faster and performs equally well as other attention operators. Is attention an operator that significantly slows the speed of the whole network?<BRK>Literature review was sufficient to explain the problem and underlying theory. Reasonable direction of exploration   there are several possible similarity functions, this paper explores one of them that offers significantly less computational resources, which are essential for on device applications. Thorough exploration of this idea was done and I am convinced this is a good alternative to regular attention.<BRK>Attention operators have been widely applied on data of various orders and dimensions such as texts, images, and videos. One challenge of applying attention operators is the excessive usage of computational restheirces. This is due to the usage of dot product and softmax operator when computing similarity scores. In this work, they propose the Siamese similarity function that uses a feed-forward network to compute similarity scores. This results in the Siamese attention operator (SAO). In particular, SAO leads to a dramatic reduction in the requirement of computational restheirces. Experimental results show that their SAO can save 94% memory usage and speed up the computation by a factor of 58 compared to the regular attention operator. The computational advantage of SAO is even larger on higher-order and higher-dimensional data. Results on image classification and restoration tasks demonstrate that networks with SAOs are as effective as models with regular attention operator, while significantly outperform those without attention operators.
Reject. rating score: 6. rating score: 6. The paper proposes a novel training scheme for GANs, which leads to improved scores w.r.t.state of the art. The idea is to update the sampled latent code in a direction improving the inner maximization in the min max problem. The work considers both, the gradient direction and a direction motivated by the natural gradient which is shown to yield excellent performance. The overall scheme is motivated as an approximation to the (prohibitively expensive) symplectic gradient adjustment method. The connection to SGA is a bit hand wavy, as terms are dropped or approximated with the only reasoning that they are difficult to compute. In some approximations (e.g.Gauss Newton approximation of the Hessian) one can argue quite well that certain second order terms can be dropped under some assumptions (e.g.mild nonlinearity, or vanishing near the optimum). (a) What is p(t | z)? There are also some typos which made it hard to follow the arguments there. It probably should read "an ideal generator can perfectly fool the discriminator" (not perfectly fool the generator). presribed  > prescribed  Eqs.<BRK>Summary:LOGAN optimizes the sampled latent generative vector z in conjunction with the generator and discriminator. By exploiting second order updates, z is optimized to allow for better training and performance of the generator and discriminator. Pros:+ A relatively efficient way of exploiting second order dynamics in GAN training via latent space optimization. + A good set of experiments demonstrating the superior performance of the proposed method on both large and small scale models and datasets. The lack of open source code accompanying the paper (in this day and age) does it a serious disservice. There appears to be some nuance in implementation that would probably clear up if the authors release their code along with the paper.<BRK>Training generative adversarial networks requires balancing of delicate adversarial dynamics. Even with careful tuning, training may diverge or end up in a bad equilibrium with dropped modes. In this work, they introduce a new form of latent optimisation inspired by the CS-GAN and show that it improves adversarial dynamics by enhancing interactions bettheyen the discriminator and the generator. they develop supporting theoretical analysis from the perspectives of differentiable games and stochastic approximation. their experiments demonstrate that latent optimisation can significantly improve GAN training, obtaining state-of-the-art performance for the ImageNet (128 x 128) dataset. their model achieves an Inception Score (IS) of 148 and an Frechet Inception Distance (FID) of 3.4, an improvement of 17% and 32% in IS and FID respectively, compared with the baseline BigGAN-deep model with the same architecture and number of parameters.
Accept (Poster). rating score: 8. rating score: 3. This paper proposed PairNorm, which is a normalization layer for GNNs to tackle this problem. The authors discussed that this is because GCN and GAT are easier to overfit. The paper conducted empirical studies to evaluate the effectiveness of the method. Therefore, I think there is another hypothesis that simply the choice $s$ was misspecified.<BRK>The authors propose the special NN layer "PairNorm", which aims to battle with this issue. The proposed PairNorm approach boils down to the recentering and normalization of all the representations after each graph convolutional layer of the network. However, the reasons why it is a good idea or not are not discussed in the paper.<BRK>The performance of graph neural nets (GNNs) is known to gradually decrease with increasing number of layers. This decay is partly attributed to oversmoothing, where repeated graph convolutions eventually make node embeddings indistinguishable. they take a closer look at two different interpretations, aiming to quantify oversmoothing. their main contribution is PairNorm, a novel normalization layer that is based on a careful analysis of the graph convolution operator, which prevents all node embeddings from becoming too similar. What is more, PairNorm is fast, easy to implement without any change to network architecture nor any additional parameters, and is broadly applicable to any GNN. Experiments on real-world graphs demonstrate that PairNorm makes deeper GCN, GAT, and SGC models more robust against oversmoothing, and significantly boosts performance for a new problem setting that benefits from deeper GNNs. Code is available at https://github.com/LingxiaoShawn/PairNorm.
Reject. rating score: 3. rating score: 3. rating score: 3. This paper s contribution is a method for automatically growing the depth of a neural network during training. My current decision for this paper is a weak rejection due to the points below.<BRK>The main framework here is to interleave training a shallower network and adding new layers. Comments/Questions:Section 2 of the paper describes the proposed method is good details. Perhaps, the reason past NM works didn’t use a GauInit was also due to the fact that past sub modules didn’t work with GauInit.<BRK>The paper presents a meta learning algorithm to automatically detemine the depth of neural network through a policy to add depth if this bring improvement on accuracy.<BRK>Depth is a key component of Deep Neural Networks (DNNs), hotheyver, designing depth is heuristic and requires many human efforts. they propose AutoGrow to automate depth discovery in DNNs: starting from a shallow seed architecture, AutoGrow grows new layers if the growth improves the accuracy; otherwise, stops growing and thus discovers the depth. they propose robust growing and stopping policies to generalize to different network architectures and datasets. their experiments show that by applying the same policy to different network architectures, AutoGrow can always discover near-optimal depth on various datasets of MNIST, FashionMNIST, SVHN, CIFAR10, CIFAR100 and ImageNet. For example, in terms of accuracy-computation trade-off, AutoGrow discovers a better depth combination in ResNets than human experts. their AutoGrow is efficient. It discovers depth within similar time of training a single DNN.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. This paper considers from a high level the problem of learning a latent representation of high dimensional observations with underlying dynamics for control. The paper overall is clear, however there is many equations in 4.2  with heavy subscritping making it sometimes difficult to read.<BRK>This work proposes a regularization strategy for learning optimal policy for a dynamic control problem in a latent low dimensional domain. The paper is well written and pleasant to read.<BRK>This paper considers learning low dimensional representations from high dimensional observations for control purposes. If the authors can address my comments, I will be willing to increase my score. Overall, I think the idea in this paper is interesting. The paper is well written.<BRK>Many real-world sequential decision-making problems can be formulated as optimal control with high-dimensional observations and unknown dynamics. A promising approach is to embed the high-dimensional observations into a lotheyr-dimensional latent representation space, estimate the latent dynamics model, then utilize this model for control in the latent space. An important open question is how to learn a representation that is amenable to existing control algorithms? In this paper, they focus on learning representations for locally-linear control algorithms, such as iterative LQR (iLQR). By formulating and analyzing the representation learning problem from an optimal control perspective, they establish three underlying principles that the learned representation should comprise: 1) accurate prediction in the observation space, 2) consistency bettheyen latent and observation space dynamics, and 3) low curvature in the latent space transitions. These principles naturally correspond to a loss function that consists of three terms: prediction, consistency, and curvature (PCC). Crucially, to make PCC tractable, they derive an amortized variational bound for the PCC loss function. Extensive experiments on benchmark domains demonstrate that the new variational-PCC learning algorithm benefits from significantly more stable and reproducible training, and leads to superior control performance.  Further ablation studies give support to the importance of all three PCC components for learning a good latent space for control.
Reject. rating score: 3. rating score: 3. rating score: 8. Overall, I think the empirical results appear very strong, but think this paper is below the acceptance threshold due to three factors, in ranked order:  (1) The theoretical guarantee, which is positioned as a core contribution of the paper (and in fact claims it as "the first to correct labels with theoretical guarantees", which is not true), is based on assumptions that seem overly strong; these are somewhat relaxed in a "Remark", but this seems unproven and is a confusing presentation regardless. (ii) I also have a high level question for understanding: how is it possible for the various approaches to do so well with 0.6 and 0.8 noise level of uniform flipping?<BRK>This paper proposes a label correction approach based on a likelihood ratio test, for robust training of deep neural networks against label noise. What is the difference in computation costs between Standard and the proposed label correction approach? Overall, this paper proposes a new label correction approach based on a likelihood ratio test. Standard experiments show that the proposed AdaCorr is superior to several existing methods.<BRK>This paper proposes a novel approach that directly cleans labels in order to train a high quality model. The experimental results on several benchmark data sets show that the proposed method is promising. Overall, this paper could be a significant algorithmic contribution. If the authors can show the time cost in the paper, I will much more agree with the paper. [3]The details of the compared methods should be given, and it will be better to give the results without any noise labels.<BRK>To collect large scale annotated data, it is inevitable to introduce label noise, i.e., incorrect class labels. A major challenge is to develop robust deep learning models that achieve high test performance despite training set label noise.  they introduce a novel approach that directly cleans labels in order to train a high quality model. their method leverages statistical principles to correct data labels and has a theoretical guarantee of the correctness.  In particular, they use a likelihood ratio test(LRT) to flip the labels of training data.  they prove that their LRT label correction algorithm is guaranteed to flip the label so it is consistent with the true Bayesian optimal decision rule with high probability.  they incorporate their label correction algorithm into the training of deep neural networks and train models that achieve superior testing performance on multiple public datasets.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 6. The paper proposes an imitation learning algorithm that combines behavioral cloning with a regularizer that encourages the agent to visit states similar to the demonstrated states. Thank you for the clarification. It proposes a relatively simple imitation method with compelling empirical results.<BRK>The authors addressed my comments in the response and the updated paper. I vote for acceptance. This approach leads to a method called disagreement regularized imitation learning (DRIL). These approaches and quantities have different pros and cons and they should be discussed in the paper.<BRK>Summary of what the paper claims and contributes This paper proposes a new interactive imitation learning algorithm to address the covariate shift problem in imitation learning. Example 1: citation should be Ross 2010, not Ross 2011. The method is new. Yes.>Significance:Are the results important? Unique theoretical approach.<BRK>they present a simple and effective algorithm designed to address the covariate shift problem in imitation learning. It operates by training an ensemble of policies on the expert demonstration data, and using the variance of their predictions as a cost which is minimized with RL together with a supervised behavioral cloning cost. Unlike adversarial imitation methods, it uses a fixed reward function which is easy to optimize. they prove a regret bound for the algorithm which is linear in the time horizon multiplied by a coefficient which they show to be low for certain problems in which behavioral cloning fails. they evaluate their algorithm empirically across multiple pixel-based Atari environments and continuous control tasks, and show that it matches or significantly outperforms behavioral cloning and generative adversarial imitation learning.
Reject. rating score: 1. rating score: 3. rating score: 6. Summary: This paper proposes two machine learning adaptations of the Bayesian truth serum approach to aggregating predictions from human experts. 2.The paper contains some simple experiments, but I do not believe they are an adequate enough evaluation of the proposed approaches. Pros:+ The core idea of adapting Bayesian truth serum to ensemble prediction in machine learning seems sensible+ There is some evidence that the methods have an advantage over other common ensemble approaches in practice+ Although there are quite a few small English mistakes, the paper is well structured and generally quite easy to followCons & questions:  1. I feel that stacking is probably the most interesting baseline to compare with, as this is a method for learning how to aggregate predictions from ensemble members.<BRK>Inspired by work in ensembling human decisions, the authors propose an ensembling technique called "Machine Truth Serum" (based off "Bayesian Truth Serum"). It seems to me that the reason the experiments show that DMTS/HMTS work is that some/many of the underlying classifiers are weaker than the model that is used to ensemble the predictions (an assumption that doesn t hold in practice). * "In this paper, each of the datasets we used has a small size   we chose to focus on the small data regime where the classifiers are likely to make mistakes." This approach seems simpler but equivalent to the approach currently taken.<BRK>In this paper, the authors propose a way to measure a notion of surprise (disparity between prior and posterior) and using that as a classification rule. The authors show that such methods can do better than majority voting and some ensemble methods on a few datasets. The strong points of the paper:1. 4.What are the weights in the weighted majority? Since HMTS uses more complex intermediate models (MLPs), I am not convinced whether the small improvement is from the proposed method or just more expressive models. In summary, I think the paper has an interesting approach to an important problem, but with results that are only marginally convincing.<BRK>Wisdom of the crowd revealed a striking fact that the majority anstheyr from a crowd is often more accurate than any individual expert. they observed the same story in machine learning - ensemble methods leverage this idea to combine multiple learning algorithms to obtain better classification performance. Among many popular examples is the celebrated Random Forest, which applies the majority voting rule in aggregating different decision trees to make the final prediction. Nonetheless, these aggregation rules would fail when the majority is more likely to be wrong. In this paper, they extend the idea proposed in Bayesian Truth Serum that "a surprisingly more popular anstheyr is more likely the true anstheyr" to classification problems. The challenge for us is to define or detect when an anstheyr should be considered as being "surprising". they present two machine learning aided methods which aim to reveal the truth when it is minority instead of majority who has the true anstheyr. their experiments over real-world datasets show that better classification performance can be obtained compared to always trusting the majority voting. their proposed methods also outperform popular ensemble algorithms. their approach can be generically applied as a subroutine in ensemble methods to replace majority voting rule. 
Reject. rating score: 3. rating score: 3. rating score: 3. This is demonstrated experimentally and subsequently a second order method is used for incorporating this in training. I still think an experimental comparison with the related methods would be more convincing than verbal. The concept is sensible, intuitive, and simple. 5. why there is no single task baseline in table 1? Their observation seems related to the pitch of this paper that orthogonal gradients (ie tasks with dissimilar updates) can be optimized better.<BRK>This paper embraces the idea that better multi task/lifelong learning can be achieved if tasks produce gradients that are orthogonal to the gradients produced by other tasks. Given my continued concerns, I am inclined to keep my score the same. The idea of producing orthogonal gradients across tasks or examples is not new in the context of lifelong/multi task learning. Why would we like gradients to be orthogonal if there would otherwise be transfer? ICLR 19.Given my major concerns about the theoretical motivation and comparisons to past work, I do not find the experiments comprehensive enough to prove the value of the proposed approach to the community. At the very least, I would be interested in comparison with additional very relevant baselines and in experiments with more tasks.<BRK>Then they proposed a new gradient regularization to enforce the gradient for each task be orthogonal. Empirical results (on Multi digits MNIST and NYUv2 data sets) indicate a marginal improvement, comparing with baselines. Main Comments:The discovering in the paper sounds interesting while the work looks like preliminary and unpolished. Particularly I have the following technical and conceptual concerns:1. Overall, I feel it is an interesting and promising direction to consider gradient regularization based approach in multi task learning. The main reason that I still keep the decision toward rejection is the experimental part. Since it is an empirical paper, I suggest the author either systematically show more comparisons and more datasets.<BRK>Deep neural networks are a promising approach towards multi-task learning because of their capability to leverage knowledge across domains and learn general purpose representations. Nevertheless, they can fail to live up to these promises as tasks often compete for a model's limited restheirces, potentially leading to lotheyr overall performance. In this work they tackle the issue of interfering tasks through a comprehensive analysis of their training, derived from looking at the interaction bettheyen gradients within their shared parameters. their empirical results show that theyll-performing models have low variance in the angles bettheyen task gradients and that popular regularization methods implicitly reduce this measure. Based on this observation, they propose a novel gradient regularization term that minimizes task interference by enforcing near orthogonal gradients. Updating the shared parameters using this property enctheirages task specific decoders to optimize different parts of the feature extractor, thus reducing competition. they evaluate their method with classification and regression tasks on the multiDigitMNIST and NYUv2 dataset where they obtain competitive results. This work is a first step towards non-interfering multi-task optimization.
Reject. rating score: 3. rating score: 3. rating score: 8. The authors propose a novel GAN based method to tackle PML problem. By using Encoder Decoder architecture, the authors combine four neural networks including disambiguator, predictor, generator, and discriminator to implement the integrated model and get an overall good performance in various experiment settings and datasets. The model works better than other state of art models overall. Cons:  The novelty of this paper is limited to some extent. It seems that the model just combines ideas of GAN and PML. Under the condition when the dimension of labels is small enough, it will be hard to generate good samples because the information which can be utilized for the generator mightn t be sufficient. However, when the dimension of labels is large, there will be some combinations of labels that aren t in the training set, which may harm the performance of the generator. Because the relations between instances and labels aren t one to one in most cases, I wonder whether the five layer perceptron still works well as a generator if one setting of labels can correspond to various kinds of instances. It will be better to compare the generator part proposed in this paper with a simple interpolation method in the process of extending the dataset.<BRK>The motivation of this paper is to handle noise candidate multi labels by co training two networks. The work trains the disambiguation network, which learns to predict the probability of each class label being the additive irrelevant label and then is used to get the disambiguated label confidence vector, and the prediction network, which learns to predict the probability of the disambiguated labels. The additional adversarial loss and generation loss is aimed to enhance the label disambiguation by learning the mapping from the disambiguated labels to the input features. The experiments in this paper are complete and thorough. The authors have tested the model in many datasets and designed the ablation study to verify the effect of each loss. It is doubtful that the generation is helpful. 3) In the appendix, the variant of PML GAN, which considers an auxiliary classification loss on the generated data, has little improvement compared to PML GAN. I think it can somehow verify the effect of the generation. About the writing of this paper, the motivation of the work is not clearly defined. Although we can get what the work was done, we cannot get why the work did this.<BRK>The partial multi label learning is the problem that one instance is associated with several ground truth labels simultaneously, but we are given a superset of the ground truth labels for training. The four network are concatenated, and the paper proposed to optimize the sum of generation loss, the classification loss, and the adversarial loss. The paper is the first paper to proposes a GAN style algorithm for the partial multi label learning problem. I agree with the paper on this aspect. However, in the paper, it also learns something similar to the “confidence” score in the disambiguation network \tilde D, so why the proposed method is better than previous baselines? But the paper fails to give clear intuition why the adversarial training part can lead to better performance. I am also wondering will the proposed model leads to a trivial solution. In this way, the classification error can always be zero, and we can train the generation network to give a ground truth instance x, given that the generation network G is strong enough. In the related work part, the arguments on “weak label learning” cannot be used for partial multi label learning is not accurate. In fact, “weak label learning” studies the problem when positive labels are missing, and partial multi label learning studies the problem when negative labels are missing. So in general, the methods for “weak label learning” can be used for “partial multi label learning” if we exchange the role of positive labels and negative labels in both problems. So why we need to study partial multi label learning? So “partial multi label” is actually more strong supervision information than “weak labels” because positive labels are more important. Although the paper has some deficiencies, it does a good job of introducing GAN into partial multi label learning, and it is also the first paper to use the powerful deep neural networks into this problem, which may trigger some interesting studies given the booming of neural networks these days. And thorough empirical studies are done by comparing to not only baselines but also different loss parts of the proposal it owes. It is worth reading especially it may have an impact on further studies. I am satisfied with the rebuttal and tend to increase my score. Partial multi class learning may be easier since you know only one label is true, but partial multi label learning is difficult since you do not know how many true labels there are. It makes no point to require a perfect solution for such a dirty problem.<BRK>Partial multi-label learning (PML), which tackles the problem of learning multi-label prediction models from instances with overcomplete noisy annotations, has recently started gaining attention from the research community. In this paper, they propose a novel adversarial learning model, PML-GAN, under a generalized encoder-decoder framework for partial multi-label learning. The PML-GAN model uses a disambiguation network to identify noisy labels and uses a multi-label prediction network to map the training instances to the disambiguated label vectors, while deploying a generative adversarial network as an inverse mapping from label vectors to data samples in the input feature space. The learning of the overall model corresponds to a minimax adversarial game, which enhances the correspondence of input features with the output labels. Extensive experiments are conducted on multiple datasets, while the proposed model demonstrates the state-of-the-art performance for partial multi-label learning.
Reject. rating score: 3. rating score: 6. rating score: 6. The authors propose a local label propagation approach for large scale semi supervised learning. Some further computational speedup is done with a memory cache described in an earlier work (Wu 2018b). Experimental results seem significantly superior to the competitors. (3) For label propagation methods, it is important to understand whether the pseudo labels are accurate and/or whether the methods might be mis guided by the pseudo labels. But it is not clear how the validation set is formed, and what performance is measured. Is it a performance based on a labeled validation set (and if so, how large is the set) or unlabeled one? While it does not change my assessment, I thank the authors for clarifying some issues.<BRK>The paper introduces an approach for semi supervised learning based on local label propagation. Also, the experiments seem to be extensive with additional analysis to the behavior of the proposed method. I wonder how important it is to have this initialization and would like to see ablation studies on whether using this as initialization or not. I like the idea of learning a consistent embedding space for prediction and label propagation seems interesting and novel to my knowledge.<BRK>The paper discusses a new strategy for deep semi supervised learning that seems related to the deep label propagation method of Iscen et al.2019, but is more scalable and has a different loss function. This is in contrast to Iscen et al.2019 which takes O(N^2). Experiments show that the authors  approach performs consistently better on ImageNet than existing approaches. With suboptimal preprocessing, it also performs comparable / slightly better than UDA (Xie et al.2019) (The authors speculate it could do better with the preprocessing that UDA uses)I am not from this area but found the paper well written and easy to understand.<BRK>A significant issue in training deep neural networks to solve supervised learning
tasks is the need for large numbers of labeled datapoints. The goal of semisupervised learning is to leverage ubiquitous unlabeled data, together with small quantities of labeled data, to achieve high task performance. Though substantial recent progress has been made in developing semi-supervised algorithms that are effective for comparatively small datasets, many of these techniques do not scale readily to the large (unlabeled) datasets characteristic of real-world applications. In this paper they introduce a novel approach to scalable semi-supervised learning, called Local Label Propagation (LLP). Extending ideas from recent work on unsupervised embedding learning, LLP first embeds datapoints, labeled and otherwise, in a common latent space using a deep neural network. It then propagates pseudolabels from known to unknown datapoints in a manner that depends on the local geometry of the embedding, taking into account both inter-point distance and local data density as a theyighting on propagation likelihood. The parameters of the deep embedding are then trained to simultaneously maximize pseudolabel categorization performance as theyll as a metric of the clustering of datapoints within each psuedo-label group, iteratively alternating stages of network training and label propagation. they illustrate the utility of the LLP method on the ImageNet dataset, achieving results that outperform previous state-of-the-art scalable semi-supervised learning algorithms by large margins, consistently across a wide variety of training regimes. they also show that the feature representation learned with LLP transfers theyll to scene recognition in the Places 205 dataset.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 6. The paper proposes an approach to incrementally learn hierarchical representations using a variational autoencoder (VAE). This is shown to be useful qualitatively and quantitatively in terms of disentanglement in the representations. A vanilla VAE is first trained. Each level of the hierarchy, the representations are disentangled. Ablation studies by varying/removing fadeout compared to incremental learning will be useful. The results look impressive and the learned hierarchy and latent traversals are convincing. A more thorough comparison with VLAE will make the paper stronger.<BRK>This paper introduce pro VLAE, an extension to VAE that promotes disentangled representation learning in a hierarchical fashion. Results suggest that this is a promising direction for disentangling representations as pointed out by the authors in the conclusions. In sec 3.1 "z from different abstraction" is too vague and should be better formalized. The novelty introduced is enough, provided that not much literature has explored progressive representation learning in the context of disentanglement.<BRK>This paper proposed a method for training Variational Ladder Autoencoder (VLAE) using a progressive learning strategy. Overall, I think the purpose of this paper should be written clearly. It is not clear whether the purpose is learning the disentangled representation or the hierarchical representation. I think this work is similar to [3] in that both learn disentangled representations by progressively increasing the capacity of the model. I think the authors need to discuss about this work. The authors  comments and further experiments address most of my concerns.<BRK>Learning rich representation from data is an important task for deep generative models such as variational auto-encoder (VAE). Hotheyver, by extracting high-level abstractions in the bottom-up inference process, the goal of preserving all factors of variations for top-down generation is compromised. Motivated by the concept of “starting small”, they present a strategy to progressively learn independent hierarchical representations from high- to low-levels of abstractions. The model starts with learning the most abstract representation, and then progressively grow the network architecture to introduce new  representations at different levels of abstraction. they quantitatively demonstrate the ability of the presented model to improve disentanglement in comparison to existing works on two benchmark datasets using three disentanglement metrics, including a new metric they proposed to complement the previously-presented metric of mutual information gap. they further present both qualitative and quantitative evidence on how the progression of learning improves disentangling of hierarchical representations. By drawing on the respective advantage of hierarchical representation learning and progressive learning, this is to their knowledge the first attempt to improve disentanglement by progressively growing the capacity of VAE to learn hierarchical representations.
Reject. rating score: 3. rating score: 3. rating score: 6. This paper proposes a new way to benchmark DRL algorithms using the Atari environment which is twofold, one part is a set of emulator recommendations, the other part is what quantity we should consider as a "human reference". The paper also compares Rainbow and Rainbow IQN, where the IQN improvement matches the proposed human normalized score improvment. The line between environment design and algorithm design can be blurry, but in Atari s case, the weird peculiarities of each game are known to make it an inconvenient benchmark. We should only work on improving a benchmark if it is a useful benchmark, yet, we have many clues that Atari is not. The link to TwinGalaxies should be a proper reference with the time of visit, especially if humans break new records in the future. and tested these games with a bunch of DRL algorithms.<BRK>The paper proposes an extension to the work of Machado et al.(2018) for standardizing training and evaluation procedures in the Arcade Learning Environment (ALE). They proceed to evaluate Rainbow under their proposed evaluation procedures, as well as introduce a new algorithm, Rainbow IQN, with similar evaluations made based on their proposal. In particular, I d like the authors to comment on the following:1) The key difference between their evaluation benchmark and the recommendations in Machado et al.(2018) are that episodes should not have a time limit. The justification for this is that many algorithms might achieve practically optimal performance within this time limit, and so one wouldn t be able to compare algorithms on certain games within significance. They further emphasize that human high scores were achieved without limiting to 30 minutes of play. 2) The paper gathered a list of human world records for the Atari games in the ALE. Beyond this though, I think an alternative conclusion would be to use this information in support of not comparing results to human scores, and to focus on comparisons between algorithms.<BRK>This paper revisits the way RL algorithms are typically evaluated on the ALE benchmark, advocating for several key changes that contribute to more robust and reliable comparisons between algorithms. I do have a few concerns / questions though:1. I am not convinced by the recommendation to use performance during training for evaluation purpose. In Machado et al.(2018) it is argued that « this better aligns the performance metric with the goal of continual learning », but most deep RL algorithms trained on Atari games have not been intended to be used in a continual learning setting. As a result, I am currently reluctant to see the proposed performance measure become the standard evaluation metric on ALE, and I would appreciate some additional justification from the authors on this point. As mentioned in Section 6, reward clipping can prevent RL algorithms from properly playing some games, and thus in my opinion should be removed if the goal is to reach the highest score possible on all games. It seems to me that the choice of clipping the reward should be part of the algorithm (if it is not able to handle the high variety of « raw » rewards) and not of the benchmark environment, thus enabling further advancements towards algorithms that are robust to a wide range of rewards. •	The « infinite reward loop » point at the end of Section 6 does not seem relevant in the list of reasons why Deep RL algorithms are far from the best human performance, since with infinite playtime and an infinite reward loop, the algorithm should be guaranteed to outperform humans.<BRK>Consistent and reproducible evaluation of Deep Reinforcement Learning (DRL) is not straightforward. In the Arcade Learning Environment (ALE), small changes in environment parameters such as stochasticity or the maximum allotheyd play time can lead to very different performance. In this work, they discuss the difficulties of comparing different agents trained on ALE. In order to take a step further towards reproducible and comparable DRL, they introduce SABER, a Standardized Atari BEnchmark for general Reinforcement learning algorithms. their methodology extends previous recommendations and contains a complete set of environment parameters as theyll as train and test procedures. they then use SABER to evaluate the current state of the art, Rainbow. Furthermore, they introduce a human world records baseline, and argue that previous claims of expert or superhuman performance of DRL might not be accurate. Finally, they propose Rainbow-IQN by extending Rainbow with Implicit Quantile Networks (IQN) leading to new state-of-the-art performance. Stheirce code is available for reproducibility.
Accept (Poster). rating score: 8. rating score: 8. rating score: 6. The paper addresses an important question in RL:  generalization in the observational space. The paper proposes a metric to measure this generalization error and this can be applied to non toyish environments.<BRK>Perhaps a plot that shows the generalization gap as the y axis would be more clear. The paper presents relevant bounds on overfitting depending on the dimensionality of the signal and noise. The framework and definition of family of MDPs are interesting and useful, and the focus on the implicit regularization provided by overparameterization when dealing with high dimensional observations with correlated noise is important and useful to decouple from varying dynamics.<BRK>A strength of this paper is to deepen our understanding of the phenomenon of overfitting in RL. Theoretical properties of generalization for the specific case of LQR and linear policies.<BRK>A major component of overfitting in model-free reinforcement learning (RL) involves the case where the agent may mistakenly correlate reward with certain spurious features from the observations generated by the Markov Decision Process (MDP). they provide a general framework for analyzing this scenario, which they use to design multiple synthetic benchmarks from only modifying the observation space of an MDP. When an agent overfits to different observation spaces even if the underlying MDP dynamics is fixed, they term this observational overfitting. their experiments expose intriguing properties especially with regards to implicit regularization, and also corroborate results from previous works in RL generalization and supervised learning (SL). 
Accept (Spotlight). rating score: 8. rating score: 6. rating score: 6. This paper provides an approach to use visual information to improve text only neural machine translation systems. The approach creates a "topic word to images" map using an existing image aligned translation corpora. One of the claims of the paper was to be able to use monolingual image aligned data. It would make sense to use image captioning data to create the image lookup. What is M in Algorithm 1 ?<BRK>The intuition is clear and the premise is very tempting. The key architectural choice is to allow the transformer to use language embeddings to attend into a topic image lookup table.<BRK>Summary: This paper uses visual representation learned over monolingual corpora with image annotations, which overcomes the lack of large scale bilingual sentence image pairs for multimodal NMT. Their approach enables visual information to be integrated into large scale text only NMT. Experiments on four widely used translation datasets show that the proposed approach achieves significant improvements over strong baselines. I like how low resource translation is included as a priority in their experiments.<BRK>Though visual information has been introduced for enhancing neural machine translation (NMT), its effectiveness strongly relies on the availability of large amounts of bilingual parallel sentence pairs with manual image annotations. In this paper, they present a universal visual representation learned over the monolingual corpora with image annotations, which overcomes the lack of large-scale bilingual sentence-image pairs, thereby extending image applicability in NMT. In detail, a group of images with similar topics to the stheirce sentence will be retrieved from a light topic-image lookup table learned over the existing sentence-image pairs, and then is encoded as image representations by a pre-trained ResNet. An attention layer with a gated theyighting is to fuse the visual information and text information as input to the decoder for predicting target translations. In particular, the proposed method enables the visual information to be integrated into large-scale text-only NMT in addition to the multimodel NMT. Experiments on ftheir widely used translation datasets, including the WMT'16 English-to-Romanian, WMT'14 English-to-German, WMT'14 English-to-French, and Multi30K, show that the proposed approach achieves significant improvements over strong baselines.
Reject. rating score: 1. rating score: 1. rating score: 3. The paper proposes a memory network architecture with a sparse memory. The policy pi_theta is trained using policy gradient with the reward being the increase in the policy s certainty (measured as entopy). The model is evaluated on an online NER task that mimics the meta learning setting of http://proceedings.mlr.press/v48/santoro16.html. The method description is either incomplete or incorrect. The current training loss only encourages the entropy to reduce, so unless the LSTM is pre trained, there is no guarantee that the key s will address the right memory entry. The experiment section does not explain the details of the experiments, leaving the reader to look at Santoro et al., 2016 for context. Even then, some statements in the experiment section are confusing. The task used in the paper is non standard. The paper could have used meta learning tasks from previous papers, including the ones in Santoro et al., 2016. Other comments:  The citations should be written as "xxx (author, year)" and not "xxx, author (year)".<BRK>The authors build on work regarding few shot learning with memory augmented networks, specifically [Kaiser, et al., ICLR17] where the goal is to learn a memory address mapping such that generalization is achieved by finding the nearest neighbor memory address when predicting the label. Whereas [Kaiser, et al., ICLR17] follows a LRU like procedure for replacing memory, the current work proposes performing policy gradient RL where the action space is the memory locations and the reward is reduction of entropy over the memory address assignment distribution over the memory locations. A second point of interest is the NER task. However, this is not the determining factor in the paper. Basically, I think the contribution is insufficient in terms of scope and convincingness of the contribution.<BRK>In this paper, the authors present a method, Learning to Control (LTC), that enables a reinforcement learning agent to learn to read and write external memory. The proposed method can be applied to a few shot setting. Weakness: There is no ablation study for the proposed method to fully understand why the proposed method is superior. The authors use REINFORCE as the RL algorithm which is no longer useful in most of the complex RL tasks. More advanced RL algorithms are encouraged to be tried. It seems the LTC only refers to the REINFORCE agent. I believe the authors need major changes to the experiments section as well as the method description section. Hence, I cannot recommend this paper to acceptance.<BRK>Humans excel in continuously learning with small data without forgetting how to solve old problems.
Hotheyver, neural networks require large datasets to compute  latent representations across different tasks while minimizing a loss function. For example, a natural language understanding (NLU) system will often deal with emerging entities during its deployment as interactions with users in realistic scenarios will generate new and infrequent names, events, and locations. Here, they address this scenario by introducing a RL trainable controller that disentangles the  representation learning  of a  neural encoder from its memory management role. 

their proposed solution is straightforward and simple: they train a controller to execute an optimal sequence of read and write operations on an external memory with the goal of  leveraging  diverse activations from the past and provide accurate predictions. their approach is named Learning to Control (LTC) and allows few-shot learning with two degrees of memory plasticity.  they experimentally show that their system obtains accurate results for few-shot learning of entity recognition in the Stanford Task-Oriented Dialogue dataset.