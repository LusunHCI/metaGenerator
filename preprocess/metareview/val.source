Invite to Workshop Track. rating score: 7. rating score: 5. rating score: 5. <BRK>The entire architecture is trained end to end with backpropagation, Monte Carlo rollouts and a baseline for variance reduction. The results show that the architecture is able to classify accurately on all syntactic levels, faster than a baseline that reads the entire text. The approach is simple and seems to work well and could be applied to other tasks where inference time is important.<BRK>The authors propose a unified framework enabling the recurrent network to reread or skip some parts of a document. This choice is probably made to fit with classical sequential decision algorithms, assuming that the confidence level can be extracted from the latent representation... The interest of rereading a word/sentence is not clear for me: we simply choose to overweight the recent past wrt the further. how many time does the algorithm choose to reread? Given the chosen tasks, this work should be compared to the beermind system:http://deepx.ucsd.edu/#/home/beermindand the associated publicationhttp://arxiv.org/pdf/1511.03683.pdfBut the authors should also refer to previous work on their topic:https://arxiv.org/pdf/1107.1322The above mentioned reference is really close to their work. This article describes an interesting approach but its main weakness resides in the lack of positioning wrt the literature and the lack of comparison with state of the art models on the considered tasks.<BRK>Apart from the Yu et al.(2017) cited, there is older work trying to save computational time in NLP, e.g.: Dynamic Feature Selection for Dependency Parsing. But this is just an assumption, which is not guaranteed in any way. It could be that the earlier parts of the text are hard for the model. But wouldn t the relative savings depend on the architecture used for the RNN and the RL agent?
Reject. rating score: 3. rating score: 4. rating score: 6. <BRK>It would be even better to have a pseudo code. What is the error measure used in the paper? (3) The experiments part is not complete. Therefore, it is hard to justify whether the proposed algorithm is really useful based on Fig 3. Also, the idea of "local" disentangled LV is not well justified to be useful.<BRK>This paper proposed a method called Locally Disentangled Factors for hierarchical latent variable generative model, which can be seen as a hierarchical variant of Adversarially Learned Inference (Dumoulin el atl.2017).The idea seems to be a valid variant, however, the quality of the paper is not good. More specifically, the content in section 3 and experiment section is messy. Also the experiments have not been conducted thoroughly, and the results and the interpretation of the results are not complete. In conclusion, the reviewer thinks that this work is incomplete and does not worth publishing with its current quality. Not just leave it there. The paper only shows several examples, and the reviewer cannot draw any conclusion about it.<BRK>The paper investigates the potential of hierarchical latent variable models for generating images and image sequences. The paper relies on the ALI model from [Dumoulin et al, ICLR 16] as the main building block. The proposed hierarchical model is trained in stages. The first level is similar to PatchGAN from [1] but is trained as an ALI model. I think the paper would be stronger if it directly reproduced the experiments from [Dumoulin et al.]
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>The interesting paper provides theoretical support for the low dimensional vector embeddings computed using LSTMs or simple techniques, using tools from compressed sensing. The paper also provides numerical results to support their theoretical findings. The paper is well presented and organized.<BRK>The main insight in this paper is that LSTMs can be viewed as producing a sort of sketch of tensor representations of n grams. I didn t check all the proof details, but based on my knowledge of compressed sensing theory, the results seem plausible. I think the paper is a nice contribution to the theoretical analysis of LSTM word embeddings.<BRK>In general, I find many of the observations in this paper interesting. However, this paper is not strong enough as a theory paper; rather, the value lies perhaps in its fresh perspective.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. <BRK>Inspired by recent works on large batch studies, the paper suggests to adapt the learning rate as a function of the batch size. 1) The total budget / number of training samples is fixed. 4) Drops of the learning rates are scheduled to happen at certain times represented in terms of the number of training samples passed so far (not parameter updates).<BRK>## Review SummaryOverall, the paper s paper core claim, that increasing batch sizes at a linearrate during training is as effective as decaying learning rates, isinteresting but doesn t seem to be too surprising given other recent work inthis space. ), and included some comparisons withother recent recommended ways to increase batch size over time. Why not report actual wallclock times? If wescale up to batch sizes of ~ N/10, we can only get 10x speedups inparallelization (in terms of number of parameter updates). The central thesis is thatinstead of the "conventional wisdom" to fix the batch size during training anddecay the learning rate, it is equally effective (in terms of training/testerror reached) to gradually increase batch size during training while fixingthe learning rate.<BRK>The paper analyzes the the effect of increasing the batch size in stochastic gradient descent as an alternative to reducing the learning rate, while keeping the number of training epochs constant. In the experimental part, it is shown that the loss function and test accuracy depend only on the schedule of the decaying noise scale over training time, and are independent of whether this decaying noise schedule is achieved by a decaying learning rate or an increasing batch size. The conclusion in the paper is that training at high momentum requires additional training epochs before $A$ reaches its equilibrium value. It would be great to see the equivalent of Figure 7 with correctly rescaled $A$. It is therefore not surprising that reducing the number of updates does not deteriorate the test set accuracy.
Reject. rating score: 3. rating score: 5. rating score: 5. <BRK>The paper introduced recurrent relational network (RRNs), an enhanced version of theexisting relational network, that can be added to any neural networks to addrelational reasoning capacity. Overall the paper is well written and structured. Unfortunately, it is current form, the paper has two major downsides. First of all,  the sudoku example does not illustrate “complex relational reasoning” as claimed in the title. Indeed, this allows to realise end to end learning but does not illustrate complex reasoning. Consequently, the claim of the conclusions, namely that “we haveproposed a general relational reasoning model” is not validated, unfortunately. However, for that one should show capabilities of relational logic. Unfortunately, the paper falls short on discussion related work. EMNLP 2016: 1389 1399and even neural symbolic approaches with a long publication history. ICML 2017: 136 145that have also considered Sudoku.<BRK>This paper introduces recurrent relational networks: a deep neural network for structured prediction (or relational reasoning). Overall I think that by itself the algorithm suggested in the paper is not enough to be presented in ICLR, and on the other hand the authors didn t show it has a big impact (could do so by adding more tasks   as they suggest in the discussion). The motivation to do better relational reasoning is clear and the network suggested in this paper succeeds to achieve it in the challenging tasks. Cons  The recurrent relational networks is basically a complex learned message passing algorithm. It would been interesting to compare results to these algorithms. For the Sudoku the proposed architecture of the network seems a bit to complex, for example why do a 16 embedding is needed for representing a digit between 0 9?<BRK>This paper describes a method called relational network to add relational reasoning capacity to deep neural networks. The previous approach can only perform a single step of relational reasoning, and was evaluated on problems that require at most three steps. The current method address the scalability issue and can solve tasks with orders of magnitude more steps of reasoning. The proposed method should be better explained. It is hard to appreciate without a precise definition of interface. The proposed recurrent relational networks are only defined informally. A definition of the model as well as related algorithms should be defined more formally.
Reject. rating score: 5. rating score: 5. rating score: 6. <BRK>This paper addresses this problem by posing the Net trim objective function as  a Difference of convex(DC) function. The contribution in posing the objective as a DC function looks limited as it is very straightforward. The benefits seem to be a faster algorithm for pruning. Experimental investigations are reasonable and the results are convincing.<BRK>Quality: of good quality, but incomplete. Clarity: clear with some typosOriginality: a new approach to the NetTrim algorithm, which is somewhat original, and a new generalization bound for the algorithm. Significance: somewhat significant. CONS  Non trivial loss of accuracy on the pruned network, which cannot be estimated for larger scale pruning as the experiments only prune one layer. Where do you use the 0 1 loss in Thm. The amount of white space should be reduced (e.g.around Eq.(1)).<BRK>2.Although the complexity of the proposed method is much lower than the compared approaches (Net Trim and LOBS), there seems to be a large sacrifice on accuracy. Besides, there is no discussion for the results in Table 1. The complexity of the proposed algorithm is much lower than Net Trim and its fast version LOBS (Dong et al., 2017). Although the main idea is clearly presented, there are many syntax errors and I suggest the authors carefully checking the manuscript.
Reject. rating score: 4. rating score: 4. rating score: 4. <BRK>1.The idea is interesting, but the study is not comprehensive yet2. need to visualize the input data space, with the training data, test data, the  gaps  in training data [see a recent related paper   Stoecklein et al.Deep Learning for Flow Sculpting: Insights into Efficient Learning using Scientific Simulation Data. Maybe the network architecture is too deep for the simple data characteristics and size of training set.<BRK>Since it s a toy problem anyway, the insights seem somewhat trivial. if we include the bias and L2 regularise only the encoder weights, it works better in terms of interpolation for a limited data sample.<BRK>This paper proposes a simple task (learning the manifold of all the images of disks) to study some properties of Autoencoders. The task proposed in the paper is interesting but the study made is somewhat limited:  They only studied one choice of Autoencoder architecture, and the results shown depends heavily on the choice of the activation, in particular sigmoid should not suffer from the same problem. A more detailed comparison with all previous regularization scheme would be much needed.
Reject. rating score: 4. rating score: 5. rating score: 6. <BRK>2.The approach:(1) Based on the discussion in the related work section and the approach section, it seems the proposed approach proposes to augment each instance in the visual feature space by adding more features, as shown by [x_i; x_i^A] in 2.3.<BRK>One candidate for this is the image feature space learned by a deep network. Comparison with existing work: There has been a lot of work recently on one shot and few shot learning that would be interesting to compare against.<BRK>The idea is nice and simple, however the current framework has several weaknesses:1. This three networks need to be clearly described; ideally combined into one end to end training pipeline.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>In this paper, an number of very strong (even extraordinary) claims are made:* The abstract promises "a framework to understand the unprecedented performance and robustness of deep neural networks using field theory."<BRK>The paper makes a mathematical analogy between deep neural networks and quantum field theory, and claims that this explains a large number of empirically observed phenomena.<BRK>The paper promises quite a few intriguing connections between information bottleneck, phase transitions and deep learning. Specifically, starting from Section 2.3, especially around the transition to continuous layers, very little information is provided how one is dealing with the cost function and the results are derived.
Reject. rating score: 2. rating score: 3. rating score: 6. <BRK>The proof states that  Y  > T  > X forms a Markov chain, but this implies that T is a function ofY, not X. The paper is missing numerous prepositions and articles, and contains  multiple spelling mistakes & typos. Explicit minimums help the reader to follow the logic, and implicit onesshould only be used when it is obvious what the minimum is over.<BRK>I found the paper extremely hard to follow and seemingly incorrect in places.<BRK>It is a theoretical papers and there is no experimental section. This is the only drawback for the paper as the claims is not supported by any experimental section. The author could add some experiments to support the idea presented in the paper.
Accept (Poster). rating score: 8. rating score: 6. rating score: 5. <BRK>This is a nice paper which I would like to see accepted. Could you elaborate on this? Also, the presentation is quite clear and the paper well written.<BRK>Another comment is related to the overall content of this paper. I still have the following comments.<BRK>Thus I think that although this paper is written well, the theory is mostly recycled and the empirical results in Section 4 are known; thus it is below acceptance threshold due to lack of novelty.
Reject. rating score: 4. rating score: 4. rating score: 5. <BRK>There are a number of papers focusing on this for both planning and learning though these are not cited in the current draft.<BRK>I think this paper would improve by demonstrating how time aware policies can help in domains of interest (which are usually not time limited). The time aware agent shows improved performance in a time limited gridworld and several control domains. They show that by bootstrapping from the final state of the time limited domain, they are able to learn better policies for the time unlimited case.<BRK>The first idea of the paper is to include time remaining in the state. It is not surprising that algorithms that take this into account do better, as your examples and experiments illustrate.
Accept (Poster). rating score: 9. rating score: 8. rating score: 4. <BRK>Overall: Authors defined a new learning task that requires a DNN to predict mixing ratio between sounds from two different classes. Detailed Evaluation: The approach presented is simple, clearly presented, and looks effective on benchmarks. In terms of originality, it is different from warping training example for the same task and it is a good extension of previously suggested example mixing procedures with a targeted benefit for improved discriminative power.<BRK>The propose data augmentation and BC learning is relevant, much robust than frequency jitter or simple data augmentation. In equation 2, please check the measure of the mixture. The comments about applying a CNN to local features or novel approach to increase sound recognition could be completed with some ICLR 2017 work towards injected priors using Chirplet Transform. Discussion on mixing more than two sounds leads could be completed by associative properties, we think... ?<BRK>The authors show how the so called BC learning helps training different deep architectures for the sound recognition task. The authors argue that it is not a data augmentation technique, but rather a learning method. Naturally, the literature review deals with data augmentation technique, which supports my point of view. However, this can be solved by adding one dimension, 4 classes and 3 dimensions seems something feasible. One can easily understand that if there is one more class than the number of dimensions, the assumption should be feasible, but beyond it starts to get problematic.
Reject. rating score: 3. rating score: 4. rating score: 4. <BRK>This paper addresses the problem of one class classification. The suggested techniques are nice and show promising results. The writing of the paper is also very unclear, with several repetitions and many typos e.g.: we first introduce you a  architexture  future work remain to  it self I believe there is a lot of potential in the approach(es) presented in the paper. In my view a much stronger experimental section together with a clearer presentation and discussion could overcome the lack of theoretical discussion.<BRK>The idea of using GANs for outlier detection is interesting and the problem is relevant. * The clarity of this paper is not high as the proposed method is not well explained. The authors say that this is used to generate outliers, and since it will generate inliers when convergence, the authors propose the technique of early stopping in Section 4.1 to avoid convergence. Since this approach is not straightforward, more theoretical analysis of the proposed method is desirable.<BRK>This allows for robust performance comparison. The goal is to build up a series of generators that sample examples close the data distribution boundary but are regarded as outliers. AUC is a standard choice for evaluation in outlier detection. The clarity of the paper has improved but the experimental evaluation is lacking realistic datasets and further simple baselines (as also stated by the other reviewers)
Reject. rating score: 3. rating score: 4. rating score: 5. <BRK>Summary:The contribution of this paper is an alternative activation function which is faster to compute than the Exponential Linear Unit, yet has similar characteristics. The paper does not present clear benchmarks showing a) what is the fraction of CPU cycles spent in evaluating the activation function in any reasonably practical neural network, b) and what is the percentage of cycles saved by employing the ISRLU. The ISRLU is then reported to be 2.6x faster compared to ELU using AVX2 instructions.<BRK>This paper introduces a new nonlinear activation function for  neural networks, i.e., Inverse Square Root Linear Units (ISRLU). Experiments show that ISRLU is promising compared to competitors like ReLU and ELU. Moreover, authors also propose ISRU which is similar to tanh for RNN, but do not provide any experimental results. Pros:(1) The paper is clearly written.<BRK>Summary:  The paper proposes a new activation function that looks similar to ELU but much cheaper by using the inverse square root function. The authors also refer us to faster ways to compute square root functions numerically, which can be of general interests to the community for efficient network designs in the future. Cons:  Clearly, the proposed function is not faster than ReLU.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>This could be insightful information that this paper could provide if possible. In experiments, they show the advantage of BNNs by conducting experiments based on black box and white box adversarial attacks without the need to artificially mask gradients. It would be useful to have such numbers to make a better evaluation of the BNN performance in the black box attack setting.<BRK>Training BNNs with adversarial examples is hard. What do you think would happen? The CIFAR 10 results are barely discussed. Overall I think this paper is interesting and relevant to ICLR. It could have stronger results both in terms of the datasets used and the variety of attacks tested, as well as some more details concerning how to perform adversarial training with BNNs (or why that s not a good idea).<BRK>his work presents an empirical study demonstrating that binarized networks are more robust to adversarial examples. The paper is well written overall and the main idea is simple and elegant. The experimental results validate the main claims of the paper on some datasets. Therefore, larger models may be needed to make this method work for higher dimensional inputs.
Accept (Poster). rating score: 9. rating score: 7. rating score: 5. <BRK>This paper proposes an improvement in the speed of training/inference with structured prediction energy networks (SPENs) by replacing the inner optimization loop with a network trained to predict its outputs. The improved understanding of SPENs and potential for further work justify accepting this paper. The key idea of this paper is to replace the minimization of the energy function min_y E(f(x), y) with a neural network which is trained to predict the resulting output of this minimization.<BRK>My recommendation is that you either rebrand the paper to be more about general methods for amortizing structured prediction inference using model compression or do more fine grained experiments with SPENs that demonstrate empirical gains that leverage their flexible deep network based energy functions. Most of your empirical success is obtained by taking a pretrained CRF energy function and using this as a teacher model to train a feed forward inference network. The idea of amortizing inference is perhaps more general. In particular, the principal contribution of the sequence tagging experiments seems top be different than what is advertised earlier on in the paper.<BRK>The setup is analogous to generative adversarial networks, where the role of the discriminator is played by a structured prediction energy network (SPEN) and the generator is played by an inference network. The setup focuses on using SPENs as an inference network, but this seems inessential.
Reject. rating score: 3. rating score: 4. rating score: 5. <BRK>  The paper proposes to use RBF kernel based neurons with each training data point as a center of  one of the RBF kernel neuron. (i) Kernel based neural networks have been explored before [A] and  (ii) ideas similar to the nearest neighbour based efficient but approximate learning for mixture  of Gaussians like settings have also been around, e.g.in traning GMMs [B]. Hence I would consider  the novelty to be very low   The paper says that the method can be applied to embedding learning and classification, which were  previously separate problems. This is largely incorrect as many methods for classification,  especially in zero  and few shots settings (on some of the datasets used in the paper) are using  embedding learning [C], one of the cited and compared with paper (Sohn 2016) also does both  (mostly these methods use k NN classifier with Euclidean distance between learned embeddings)  It seems that the method thus is adding a kernel neuron layer, with the number equal to the number  of training samples, centers initialized with the training samples, followed by a normalized  voting based on the distance of the test example with training examples of different classes  (approximately a weighted k NN classifier)  The number of neurons in the last layer thus scales with the number of training examples, which  can be prohibitively large   It is difficult to understand what exactly is the embedding; if the number of neurons in the  RBF layer is equal to the number of training examples then it seems the embedding is the activation  of the layer before that (Fig1 also seems to suggest this). In that case the empirical validation is not fair as  the network was made deeper.<BRK>The authors propose a loss that is based on a RBF loss for metric learning and incorporates additional per exemplar weights in the index for classification. In the metric learning comparison with softmax (end of page 9) the authors mentions that a Gaussian standard deviation for softmax is learned. It appears as if the authors use the softmax logits as embedding whereas the more common approach is to use the bottleneck layer. This is also indicated by the discussion at the end of page 10 where the authors mention that softmax is restricted to axis aligned embeddings. Some positive points:  The authors mention in Sec 3.3 that updating the RBF centres is not required. Additional experiments that can investigate this point would greatly contribute to a well rounded paper. The numbers reported in Tab 1 show very significant improvementsIf the paper was re framed and builds on top of the already existing NCA loss, there could be valuable contributions in this paper. I encourage the authors to extend the paper and flesh out some of the experiments and then submit it again.<BRK>(Summary)This paper proposes weighted RBF distance based loss function where embeddings for cluster centroids and data are learned and used for class probabilities (eqn 3). (Cons)The proposed method is unlikely to scale with respect to the number of classes. "..our approach is also free to create multiple clusters for each class.." This makes it unfair to deep metric learning baselines in figures 2 and 3 because DMP baselines has memory footprint constant in the number of classes. In contrast, the proposed method have linear memory footprint in the number of classes. The method is unlikely to scale and the important details on how many centroids the authors used in each experiments is omitted.
Accept (Poster). rating score: 8. rating score: 7. rating score: 5. <BRK>This is an interesting paper, exploring GAN dynamics using ideas from online learning, in particular the pioneering "sparring" follow the regularized leader analysis of Freund and Schapire (using what is listed here as Lemma 4). By restricting the discriminator to be a single layer, the maximum player plays over a concave (parameter) space which stabilizes the full sequence of losses so that Lemma 3 can be proved, allowing proof of the dynamics  convergence to a Nash equilibrium. A very simple queue for the latter is shown to do quite competitively in practice.<BRK>The present paper proposes to obtain mixed strategy through an online learning approach. Online learning (no regret) algorithms have been used in finding an equilibrium for zero sum game. Based on the theory developed, the paper presents a practical algorithm. Compared to the standard GAN training, the new algorithm returns mixed strategy and examine several previous models (instead of the latest) in each iteration. Overall, I think it is a borderline paper.<BRK>In the case of a shallow discriminator, the authors proved some results on the convergence of their proposed algorithm (an adaptation of FTRL) in GAN games, by leveraging the fact that when D update is small, the problem setup meets the ideal conditions for no regret algorithms. Overall the paper is very well written. The theory is significant to the GAN literature, probably less so to the online learning community. But in this case, it is not.
Reject. rating score: 5. rating score: 6. rating score: 6. <BRK>The paper presents an elegant and simple idea in a dense and complex way making the paper difficult to follow. Table 1 and 3 presents the results only for LogDenseNet V1, would it be possible to add results for V2 that have different MBD. Other issues:  Table 3, has an accuracy of nan. What does it mean? Could the authors comment on that? Abstract: “… Log DenseNets are easier than DenseNet to implement and to scale.” It is not clear why would LogDenseNets be easier to implement.<BRK>This paper investigates how to impose layer wise connections in DenseNets most efficiently. The authors also propose maximum backpropgation distance (MBD) for measuring the fluency of gradient flow in the network, and justify the Log DenseNet s advantage in this framework. Empirically, the author demonstrates the effectiveness of Log DenseNet by comparing it with two other intuitive connection patterns on CIFAR datasets. Generally, DenseNet is memory hungry if the connection is dense, and it is worth studying how to sparsify a DenseNet. 2.The ablation experiments are well designed and the visualizations of connectivity pattern are clear. Adding a comparison with Log DenseNet and vanilla DenseNet in the Table 2 experiment would make the paper stronger. It should be interesting to study whether it is possible to further sparsify DenseNet BC, as it has much higher efficiency. 3.The improvement of efficiency on classifications task is not that significant.<BRK>Then, results are reported for both image classification and semantic segmentation tasks. The main contribution of the paper is a network design that places skip connections to minimize the distances between layers, increasing the distance from 1 to 1 + log L when compared to traditional DenseNets. Experiments seem well executed; the authors consider several sparse connectivity patterns for DenseNets and provide empirical evidence highlighting the advantages of having a short maximum backpropagation distance (MBD). Moreover, they provide an analysis on the trade off between the performance of a network and its computational cost. [a] https://arxiv.org/pdf/1412.6550.pdfIt is not clear why Log DenseNets would be easier to implement than DenseNets, as mentioned in the abstract. In Tables 1 2 3, it would be good to add the results for Log DenseNet V2. In Table 3, what does “nan” accuracy mean?
Accept (Poster). rating score: 7. rating score: 5. rating score: 4. <BRK>This paper presents an image to image cross domain translation framework based on generative adversarial networks. The paper shows promising results on applying a supervised method on top of AN GAN’s matches. The results show that the proposed method is superior for the task of exact correspondence identification and that AN GAN rivals the performance of pix2pix with strong supervision.<BRK>The paper presents a method for finding related images (analogies) from different domains based on matching by synthesis. The general idea is interesting and the results show improvements over previous approaches, such as CycleGAN (with different initializations, pre learned or not). Instead of the longer intro and related work discussion, I would prefer to see a Figure with the architecture and more illustrative examples to show that the insights are reflected in the experiments.<BRK>This paper adds an interesting twist on top of recent unpaired image translation work. A domain level translation function is jointly optimized with an instance level matching objective. My main criticism is with the experiments and results. The experiments focus almost entirely on the setting where there actually exist exact matches between the two image sets. I was also not convinced by the supervised second step in Section 4.3.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. <BRK>While the differentiable quantization optimise the quantization function in a unified back propagation framework. What will be the prediction speed for each of models? It is interesting to see the performance improvements by using the one step optimisation method. The starting point is quite interesting and reasonable.<BRK>Also, the first sentence of the paper reads as "... have showed tremendous performance", which is not proper English. The paper proposes to combine two approaches to compress deep neural networks   distillation and quantization. Somewhat surprisingly, nobody has combined the two approaches before, which makes this paper interesting. This needs more clarification.<BRK>Two strategies are proposed and the ideas are reasonable and clearly introduced. Experiments on various datasets are conducted to show the effectiveness of the proposed method. Cons:(1) The differentiable quantization strategy seems not to be consistently better than the straightforward quantized distillation which may need more research. (2) The actual speedup is not clearly calculated.
Reject. rating score: 4. rating score: 6. rating score: 6. <BRK>This paper aims to push the LSTM gates to be binary. The novelty of this paper is limited. Just directly apply the Gumbel Softmax trick. The motivation is not explained clearly and convincingly. According to the paper, it may give better generalization performance. The results of the new G2 LSTM are not significantly better than baselines in the experiments.<BRK>This paper propose a new "gate" function for LSTM to enable the values of the gates towards 0 or 1. The motivation behind is a  flat region of the loss surface is likely to generalize well. Also, can we apply this idea to the binarynet? Is there any experimental evidence to show it s not working?<BRK>The paper argues for pushing the input and forget gate’s output toward 0 or 1, i.e., the LSTM tends to reside in flat region of surface loss, which is likely to generalize well. To achieve that, the sigmoid function in the original LSTM is replaced by a function G that is continuous and differentiable with respect to the parameters (by applying the Gumbel Softmax trick). In short, this work is worth a read. Although the experimental results are not quite persuasive, the method is nice and promising.
Reject. rating score: 4. rating score: 4. rating score: 4. <BRK>The paper proposes another training objective for training neural sequence to sequence models. In Paragraph “Maximum Likelihood”, page 2, the formalization of the studied problem is unclear. The proposed method is evaluated on just one dataset. Crucially, there is no comparison to a trivial linear combination of ML and RL, which in one way or another was used in almost all prior work, including GNMT, Bahdanau et al, Ranzato et al.The paper does not argue why alpha divergence is better that the aforementioned combination method and also does not include it in the comparison. To sum up, I can not recommend the paper to acceptance, because (a) an important baseline is missing (b) there are serious writing issues.<BRK>This paper considers a dichitomy between ML and RL based methods for sequence generation. Unfortunately, I do not understand main points made in this paper and am thus not able to give an accurate evaluation of the technical content of this paper. For example, 1.1 The q(.|.) 1.2 I understand that the ML objective is different from what the users really care about (e.g., blue score), but this does not seem a "discrepancy" to me. In summary, I m not convinced that the fact that ML optimizes a different objective than the blue score is a problem with the ML estimator. 1.3 In point (ii) under the maximum likelihood section, I don t understand it at all and I think both sentences are wrong.<BRK>My comments / feedback: The paper is well written and the problem addressed by the paper is an important one. My main concerns about this work are have two aspects: (a)	Novelty1. The idea is a good one and is great incremental research building on the top of previous ideas. I do not agree with statements like “We demonstrate that the proposed objective function generalizes ML and RL objective functions …” that authors have made in the abstract. The performance of the proposed method is not significantly better than other models in MT task.
Accept (Oral). rating score: 8. rating score: 7. rating score: 7. <BRK>This paper proposes "spectral normalization"   constraining the spectral norm of the weights of each layer   as a way to stabilize GAN training by in effect bounding the Lipschitz constant of the discriminator function. Overall, this is a well written paper that tackles an important open problem in training GANs using a well motivated and relatively simple approach. The experimental results seem solid and seem to support the authors  claims. Overall this seems to represent a strong step forward in improving the training of GANs, and I strongly recommend this paper for publication.<BRK>The authors observe that the product of spectral norm of gradients per each layer serves as a good approximation of the overall Lipschitz continuity of the entire discriminating network, and propose gradient based methods to optimize a "spectrally normalized" objective. I think the methodology presented in this paper is neat and the experimental results are encouraging. However, I do have some comments on the presentation of the paper:1.<BRK>This paper borrows the classic idea of spectral regularization, recently applied to deep learning by Yoshida and Miyato (2017) and use it to normalize GAN objectives. However,  the authors here argue that spectral normalization is more powerful; it allows for models of higher rank (more non zero singular values) which implies a more powerful discriminator and eventually more accurate generator. The experimental results are very good and give strong support for the proposed normalization. The paper is overall well written (though check Comment 3 below), it covers the related work well and it includes an insightful discussion about the importance of high rank models. More details in the comments below. I found the discussion about rank to be very intuitive, however this intuition is not fully tested.
Reject. rating score: 4. rating score: 5. rating score: 5. <BRK>To begin with, the authors seem to be missing some recent developments in the field of deep learning which are closely related to the proposed approach; e.g.:Sotirios P. Chatzis, “Recurrent Latent Variable Conditional Heteroscedasticity,” Proc. The paper does not satisfy this requirement. In addition, the authors claim that Gaussian process based models are not appropriate for handling asynchronous data, since the assumed Gaussianity is inappropriate for financial datasets, which often follow fat tailed distributions. 42nd IEEE International Conference on Acoustics, Speech and Signal Processing (IEEE ICASSP), pp.<BRK>The  Significance  network was described as critical to the performance, but there is no experimental result to show the sensitivity of the model s performance with respect to the architecture of the  Significance  network. It s entirely unclear how the train and test data was split. Quality: The quality of the paper was okay. More details of the experiments should be included in the main text to help interpret the significance of the experimental results. It s unclear how useful the architecture really is.<BRK>The method is applied to a proprietary dataset as well as a couple UCI problems and a synthetic dataset, showing improved performance over baselines in the asynchronous setting. This paper is mostly an applications paper. The method itself seems like a fairly simple extension for a particular application, although perhaps the authors have not clearly highlighted details of methodological innovation. Given the fairly empirical nature of the paper in general, it feels like a strong argument should be made, which includes experiments, that this work will be generally significant and impactful.
Reject. rating score: 2. rating score: 3. rating score: 5. <BRK>This paper attempts to improve the beta VAE (Higgins et al, 2017) by removing the trade off between the quality of disentanglement in the latent representation and the quality of the reconstruction. Given the points outlined above and the fact that the paper is hard to read and is excessively long, I do not believe it should be accepted. The authors assume that VAEs typically model the data using a Guassian distribution with a fixed noise.<BRK>This paper proposes to modify how noise factors are treated when developing VAE models. Of course, the paper also invents some new evaluation metrics and then applies them on benchmark datasets, but this content only appears much later in the paper (well after the soft 8 page limit) and I admittedly did not read it all carefully. The appropriateness of using additional pages over the recommended length will be judged by reviewers."<BRK>The paper is too long (30 pages) and dense, so it is very hard to read and understand the whole stuff. Remember that the ‘recommended’ page limit is 8 pages. The proposed algorithm was not compared to the generative models other than the basic VAE or beta VAE. This paper studies the importance of the noise modelling in Gaussian VAE.
Accept (Poster). rating score: 6. rating score: 6. rating score: 5. <BRK>The details on how significance in each experiment has been determined are not sufficient. Genomic control is applied in the real world experiment but not on the simulations. Genomic control is a heuristic that adjusts for being too anti conservative, but also for being too conservative, making it hard to judge the performance of each method on its own. Consequently, the paper should provide additional detail on the results and should contrast the performance of the method without the use of genomic control. While neural networks may approximate highly non linear functions, it still  seems as if the confounders are modeled largely as linear. This is indicated by the fact that the authors report performance gains from adding the confounders as input to the final layer. The two step approach to confounder correction is compared to PCA and LMMs, which are stated to first estimate confounders and then use them for testing.<BRK>This paper tackles two problems common in genome wide association studies: confounding (i.e.structured noise) due to population structure and the potential presence of non linear interactions between different parts of the genome. The main concerns with this paper is that 1) the claim that it can detect epistatic interactions is not really supported. The authors did a great job on the simulation framework, but table 1 falls short in terms of evaluation metric: to properly assess the performance of the method on simulated data, it would be good to have evidence that the type 1 error is calibrated (e.g.by means of qq plots vs null distribution) for all methods. The technical parts of this paper are definitely high quality, the experimental side could be improved. Clarity: if the target audience of this paper is the probabilistic ML community, it’s very clear.<BRK>Overal the paper is interesting and relatively well written but some important details are missing and way more experiments need to be done to show the effectiveness of the approach. * In this paper, the authors use deep neural networks to model the general functional causal models. The sentence “These models typically focus on the task of causal discovery, and they assume fixed nonlinearities or smoothness which we relax using neural networks.” in the related work section is not appropriate. Is there a theoretical guarantee for the proposed method?
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>Overall, this is a strong paper, and I would advocate for accepting it. It focuses on an important problem (speeding up program synthesis), it’s generally very well written, and it features thorough evaluation. The results are impressive: the proposed system synthesizes programs from a single example that generalize better than prior state of the art, and it does so ~50% faster on average. Still, it would be nice to know what is going on here.<BRK>The LSTM learns frominputs of program spec + candidate branch (given by a grammarproduction rule) and ouputs of quality scores for programms. The issueof how greedy to be in this search is addressed. In the authors  set up we simply assume we are given a  rankingfunction  h as an input (which we treat as black box). However, the  deep learning  aspect of the paper is notprominent: an LSTM is used as a plug in and that is about it. Also,although the search method chosen was reasonable, the only realinnovation here is to use the LSTM to learn a search heuristic.<BRK>This paper extends and speeds up PROSE, a programming by example system, by posing the selection of the next production rule in the grammar as a supervised learning problem. Moreover the work mentions a neurally guided search, but little time is spent on that portion of their contribution. The experimental results do show the programs can be faster but only if the user is willing to suffer a loss in accuracy. It is difficult to conclude overall if the technique helps in synthesis.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>This article introduces a new method to improve neural network performances on tasks ranging from continual learning (non stationary target distribution, appearance of new classes, adaptation to new tasks, etc) to better handling of class imbalance, via a hybrid architecture between nearest neighbours and neural net. Minor typos:* Figure 4: the title of the key says “New/Old” but then the lines read, in order, “Old” then “New”   it would be nicer to have them in the same order. It is a very solid paper, which this reviewer believes to be of real interest to the ICLR community.<BRK>More specially, it locally adapts the parameters of a network using the episodic memory structure. That would be great if the author can list "explicitly" the contribution of the paper with comparing with those. The proposed model does adaption during the test time, but other papers such as Li & Hoiem, 2016 handles the shift across domain in the train time. Can authors say sth about the motivation behind adaptation during test time vs. training time? There are some inconsistencies in the text about the parameters and formulations:        what is second subscript in {v_i}_i? Paper is well beyond the 8 page limit and should be fitted to be 8 pages. I have some intuition but not sure.<BRK>Overall, the idea of this paper is simple but interesting. It seems that, you are implicitly using test data to fit model. The paper is written well, and the proposed method is evaluated on a number of relevant applications (e.g., continuing learning, incremental learning, unbalanced data, and domain shifts.) How large should it be?
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>This paper introduces a new architecture for end to end neural machine translation. Moreover, there is no link between notations used for the swan part and the ones used in the reordering part.<BRK>Also, the concept of local reordering mechanism looks well to improve the basic SWAN model to reconfigure it to the situation of machine translation tasks.<BRK>The paper introduces a neural translation model that automatically discovers phrases. The local reordering layer has the ability to swap inputs, however, how do you ensure that it actually does swap inputs rather than ignoring some inputs and duplicating others? In Figure 1 both a BRNN and SWAN layer are shown, is there another RNN in the SWAN layer, or does the BRNN emit the final outputs after the segments have been determined?
Invite to Workshop Track. rating score: 9. rating score: 6. rating score: 4. <BRK>In this paper, the authors have proposed a GAN based method to conduct data augmentation. The paper is technically sound and the novelty is significant. However, this paper still suffers from some drawbacks as below:(1)	The illustration of the framework is not clear enough.<BRK>This paper is good at using the GAN for data augmentation for the one shot learning, and have demonstrated good performance for a variety of datasets. However, it seems that the main technique contribution is not so clear.<BRK>The paper is well written and consistent. To be more specific, the paper uses the previously proposed conditional GAN as the main component of their model. In summary, the idea of the paper is very interesting to learn data augmentation but yet I am not convinced the current paper has enough novelty and contribution and see the contribution of paper as on more the application side rather than on model and problem side.
Reject. rating score: 3. rating score: 4. rating score: 6. <BRK>The paper presents an application of deep learning to predict optimal treatment of sepsis, using data routinely collected in a hospital. The paper is very clear and well written, with a thorough review of related work. However, the approach is mainly an application of existing methods and the technical novelty is low. As written, the paper may be more appropriate for an application focused venue.<BRK>While the work represents a combination of leading methods in the machine learning literature, key details are missing: most importantly, that the reinforcement learning is based on observational data and in a setting where the unconfoundedness assumption is very unlikely to hold. How could the model be updated for a clinician who acknowledges the action suggestion but dismisses it as incorrect? The data is almost certainly MNAR (missing not at random). The authors might consider expanding on this.<BRK>This paper presents an important application of modern deep reinforcement learning (RL) methods to learning optimal treatments for sepsis from past patient encounters. The experiments are thorough and the results promising. Pros (+) and cons ( ) are listed below:+ discussion of the sepsis application is very strong. + thorough comparison of competing baselines and clear variants   though it would be cool to apply offline policy evaluation (OPE) to some of the standard clinical approaches, e.g., EGDT, discussed in the introduction. CLARITYPaper is well written, for the most part. I have some nitpicks about the writing, but in general, it s not a burden to read. Try to focus on one concept at a time (and the solution offered by a proposed approach). ORIGINALITYThis work scores relatively low in originality. One could read those two papers and immediately conclude this paper s findings (the GP helps; RL helps; GP + RL is the best).
Reject. rating score: 4. rating score: 4. rating score: 6. <BRK>In the paper, the authors proposed using GAN for anomaly detection. In my first reading of the paper, I felt that the baselines in the experiments are too primitive.<BRK>Authors propose an anomaly detection scheme using GANs. It relies on a realistic assumption: points that are badly represented in the latent space of the generator are likely to be anomalous. OC SVM is a well known technique that gives similar performances: authors fail at convincing that there are advantages of using the proposed framework, which do not strongly differs from previously published AnoGAN.<BRK>The latent space of the generator no longer is the same that was achieved by training on the original data. The paper is original, well written, easy to follow and presented ideas are interesting. The authors use a GAN based approach where it is trained in a standard way. Weaknesses:   It is not clear why updating the generator during the anomaly detection helps.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>In this paper, the authors study the relationship between training GANs and primal dual subgradient methods for convex optimization. Their technique can be applied on top of existing GANs and can address issues such as mode collapse. The authors also derive a GAN variant similar to WGAN which is called the Approximate WGAN. Experiments on synthetic datasets demonstrate that the proposed formulation can avoid mode collapse.<BRK>This paper formulates GAN as a Lagrangian of a primal convex constrained optimization problem. They then suggest to modify the updates used in the standard GAN training to be similar to the primal dual updates typically used by primal dual subgradient methods. Technically, the paper is sound. I think this is a nice contribution that does yield to some interesting insights. For the synthetic data, the empirical evidence seem to indicate that the technique proposed by the authors does work but I’m not sure the empirical evidence provided for the MNIST and CIFAR 10 datasets is sufficient to judge whether or not the method does help with mode collapse.<BRK>This paper proposed a framework to connect the solving of GAN with finding the saddle point of a minimax problem. As a result, the primal dual subgradient methods can be directly introduced to calculate the saddle point. But this saddle point model reformulation  in section 2 is quite standard, with limited theoretical analysis in Theorem 1. As follows, the resulting algorithm 1 is also standard primal dual method for a saddle point problem. Most important I think, the advantage of considering GAN type model as a saddle point model is that first order methods can be designed to solve it. But the numerical experiments part seems to be a bit weak, because the MINST or CIFAR 10 dataset is not large enough to test the extensibility for large scale cases.
Reject. rating score: 3. rating score: 4. rating score: 5. <BRK>The paper proposes a biologically inspired model of mammalian navigation which includes head direction cells, boundary vector cells, place cells, and grid cells. Also , because MSE is not scale free error measure it is hard to tell how significant the errors are.<BRK>The evaluation of the loop closure is limited to a qualitative measure and is therefore not convincing.<BRK>Is it possible to test your method with a regression task? The vision based agent localization approach is novel compared to the methods of the literature.
Reject. rating score: 4. rating score: 4. rating score: 4. <BRK>The top 50 entries have METEOR scores >  0.25, while the maximum METEOR score reported by the authors is 0.22. The authors should evaluate on these datasets to make their findings stronger and more valuable. No training details are provided.<BRK>Tables 2 and 3 are missing the original baselines. Getting better performance by switching from VGG19 to ResNet152 is expected, however they obtain worse results than Vinyals et al.2016 with inception_v3.<BRK>Overall, the idea seems relevant and there are some good findings but I am sure that image captioning community is already aware of these findings. I recomend this paper for a workshop presentation.
Reject. rating score: 4. rating score: 5. rating score: 5. <BRK>2.There is no difference between the architecture of the single task learning network and multi task learning network.<BRK>This paper presents a multi task neural network for classification on MNIST like datasets. So the novelty of this paper is very limited.<BRK>The paper applies multi task learning to MNIST (M), FashionNIST (F), and NotMNIST (N) datasets.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. <BRK>At which point would it break? The idea is simple and well explained. ** DETAILED REVIEW **Overall, this is a good paper. I have a few suggestions along the text but nothing major.<BRK>Experiments indicate that the  approach is most successful when the regularized RNNs are conditional generators, which emit sequences of low entropy, such as decoders of a seq2seq speech recognition network. Quality:The proposed approach is well motivated and the experiments show the limits of applicability range of the technique. Clarity:The paper is clearly written. Originality:The presented idea seems novel. Also, as the authors note the method seems to be limited to conditional sequence generators.<BRK>+ The idea is clearly explained and well motivated. Given, that the authors were able to improve the results in the sequential MNIST and improve the average baselines, my rating improves one point. However, I still have concerns about this method not being shown to improve the best methods presented in Table 3 which would give a more solid result.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. <BRK>The method builds on and is directly inspired by REBAR. Similarly to REBAR, RELAX is an unbiased estimator, and the idea is to introduce a control variate that leverages the reparameterization gradient. In contrast to REBAR, RELAX learns a free from control variate, which allows for low variance gradient estimates for both discrete and continuous random variables. Overall, I enjoyed reading the paper. I think it is a neat idea that can be of interest for researchers in the field. + Why is the method called LAX? + In Section 3.3, it is unclear to me why rho! + Consider adding a brief review of the REBAR estimator in the Background section for those readers who are less familiar with this approach. This would probably be more clear than "based on gradients of a learned function." + In Section 3.3, it reads "differentiable function of discrete random variables," which does not make sense.<BRK>This paper suggests a new approach to performing gradient descent for blackbox optimization or training discrete latent variable models. You just say it is not ReLU... To me, the main strengths of the paper is the very clear account of existing gradient estimators (among other things it helped me understand obscurities of the Q prop paper) and a nice conceptual idea. It took me a while to get it. Taking this perspective, I would be glad to see how the regression part performs with respect to standard least square regression,i.e.just using $||f(b) c_\phi(b)||^2$ as loss function. The caption mentions "variance (log scale)", but saying "log variance" would be more adequate. p9: the optimal control variate: what is this exactly? In Section 4, can you explain why, in the RL case, you must introduce stochasticity to the inputs? and E.1.2 for the two layer model(s?)<BRK>This setting has attracted a huge interest in ML communities (that is related to learning policy in RL as well as variational inference with hidden variables). The paper provides a framework for such optimization, by interestingly combining three standard ways. Given Tucker et al, its contribution is somehow incremental, but I think it is an interesting idea to use neural networks for control variate to handle the case where f is unknown. Moreover, it would be good to actually show if the variation of g_hat is much smaller than other standard methods.
Accept (Oral). rating score: 8. rating score: 7. rating score: 6. <BRK>In this paper, the authors investigate variance reduction techniques for agents with multi dimensional policy outputs, in particular when they are conditionally independent ( factored ). Or as a  fair  training procedure?)<BRK>This paper presents methods to reduce the variance of policy gradient using an action dependent baseline. The fonts are too small for the numbers and the legends.<BRK>The paper proposes a variance reduction technique for policy gradient methods.
Reject. rating score: 4. rating score: 6. rating score: 8. <BRK>I wonder how a simple matrix factorization approach would work for session based recommendation (which is an important baseline that is missing): regarding the claim that MF is not suited for session based because of the absence of the concept of a user, each session can simply be considered as a pseudo user and approaches like asymmetric matrix factorization (Paterek 2007, Improving regularized singular value decomposition for collaborative filtering) can even eliminate the need for learning user factors. This paper presents a few modifications on top of some earlier work (GRU4Rec, Hidasi et al.2016) for session based recommendation using RNN. I wonder how the propose loss would perform comparing with more competitive baselines. The writing could have been more clear, especially in terms of notations and definitions.<BRK>Good performance improvements have been reported for the several datasets to show the effectiveness of the proposed methods. The good point of this work is to show that the loss function is important to train a better classifier for the session based recommendation. (2017) into table 2 as well. As these work has already been published and should be compared with and reported in the formal table.<BRK>This is an interesting paper that analyzes existing loss functions for session based recommendations. The empirical results on two large scale datasets are pretty impressive. It also provides a nice introduction to some of the recent literature on RNNs for session based recommendations. In terms of impact, while it studies a fairly applied (and narrow) question, it seems like it would be of interest to researchers and practitioners in recommender systems. In ranking max losses, it seems like "outliers" could have a bigger impact.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. <BRK>This paper extends a previously proposed monotonic alignment based attention mechanism by considering local soft alignment across features in a chunk (certain window). Pros.the paper is clearly written. the proposed method is applied to several sequence to sequence benchmarks, and the paper show the effectiveness of the proposed method (comparable to full attention and better than previous hard monotonic assignments). Cons.in terms of the originality, the methodology of this method is rather incremental from the prior study (Raffel et al), but it shows significant gains from it.<BRK>This paper proposes a small modification to the monotonic attention in [1] by adding a soft attention to the segment predicted by the monotonic attention. The paper is very well written and easy to follow. The first set of questions is about the monotonic attention. The second question is about the window size $w$. For the experiments, it is intriguing to see that $w 2$ works best for speech recognition.<BRK>The paper proposes an extension to a previous monotonic attention model (Raffel et al 2017) to attend to a fixed sized window up to the alignment position. For document summarization the paper presents a strong result for an online model, but the fact that it is still less accurate than the soft attention baseline makes it hard to see the real significance of this. Results show that for online speech recognition the model matches the performance of an offline soft attention baseline, doing significantly better than the monotonic attention model.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. <BRK>The paper uses a variational bound for entropy regularized RL to learn a versatile latent space which represents the skill to execute. It represents one of the most principled approches for hierarchical RL. This is a good paper. More comments:  There are several papers that focus on learning versatile skills in the context of movement primitive libraries, see [1],[2],[3]. These papers should be discussed.<BRK>The approach relies on using an embedding space defined by latent variables and entropy regularized policy gradient / variational inference formulation that encourages diversity and identifiability in latent space. The exposition is clear and the method is well motivated. I see no issues with the mathematical correctness of the claims made in the paper. Overall, I believe this is in interesting piece of work at a fruitful intersection of reinforcement learning and variational inference, and I believe would be of interest to ICLR community.<BRK>In this paper, (previous states, action) pairs and task ids are embedded into the same latent space with the goal of generalizing and sharing across skill variations. Once the embedding space is learned, policies can be modified by passing in sampled or learned embeddings. Do you use Bernoulli in the experiments? I find the method to be theoretically interesting and valuable to the learning community.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. <BRK>This paper applies a predictive coding version of the Sigma Delta encoding scheme to reduce a computational load on a deep learning network.<BRK>The shorthand notation in the paper is hard to follow in the first place btw, perhaps this could be elaborated/remedied in an appendix, there is also some rather colloquial writing in places: "obscene wast of energy" (abstract), "There s" "aren t" (2.6, p5). This approach is original and significant, though the presented results are a bit on the thin side.<BRK>Overall, the paper is interesting and promising; only a few works tackle the problem of learning with spikes showing the potential advantages of such form of computing.
Reject. rating score: 4. rating score: 5. rating score: 5. <BRK>Here the authors propose a variant of an analog LSTM and then further propose a mechanism by which to convert it to a spiking network, in what a computational neuroscientist would call a  mean field  approach. In general I think that the problem of training or even creating spiking networks from analog networks is interesting and worthy of attention from the ML community. However, this manuscript feels very early and I believe needs further focus and work before it will have impact in the community. Neurophysiological realism    It appears the authors are not interested in this direction given the focus of the manuscript ( other than mentioning the brain as motivation). A focus in this direction could find an applied audience. As a minor comment, the paper could stand to be improved in terms of exposition. In particular, the paper relies on ideas from other papers and the assumption is largely made that the reader is familiar with them, although the paper is self contained.<BRK>and thecaption does not match. In general, the paper presents an interesting idea. Also, I believe that the tasks are rather simple and therefore it is not demonstrated that the approach performs well on practically relevant tasks. In particular, that the CEC is modeled with an infinitely long integration time constant of the input current. However, I think there is a chance that minor changes of the model could still work while being more realistic. For example, I would find it more convincing to put the CEC into the adaptation time constants by using a large tau_gamma or tau_eta. If the model is meant to provide efficient spiking neural networks, I find the tasks too simple and too artificial. The authors say in the introduction that they target to model recurrent neural networks. Is there a reason for this? The sigma delta neuron model seems quite ad hoc and incompatiblewith most simulators and dedicated hardware.<BRK>The authors propose a first implementation of spiking LSTMs. However, the present work somewhat incomplete, and requires further experiments and clarifications. The authors tackle an interesting and challenging problem. 2.Figure 1 is not very easy to read. 4.A major point in mapping LSTMs to spiking networks is its biological plausibility. However, the authors do not seem to explore this. 6.Current LSTMs are applied in much more challenging problems than the original ones. It would be important to test one of this, perhaps the relatively simple pixel by pixel MNIST task.
Reject. rating score: 4. rating score: 5. rating score: 6. <BRK>Unfortunately this is not the case here. In the absence of any theoretical analysis of the proposed approach, I would have expected an in depth empirical validation.<BRK>Prioritized replay, for example, could have a very similar effect to proposed approach in the tabular case. The results presented by this paper shows improvement over the baseline. In the non tabular case, the authors have actually moved away from Q learning and defined an objective that is both on and off policy.<BRK>The paper is written in a clear way. I think this should have been one of the baselines to compare to for that reason.
Reject. rating score: 3. rating score: 3. rating score: 4. <BRK>Although this paper aims at an interesting and important task, the reviewer does not feel it is ready to be published.<BRK>Since the proposed method does not build on the SBM or pWSBM the detailed equations on page 2 are not necessary. Also, Figure 1, 2, and 3 are not necessary.<BRK>I don t think there is much novelty in the "generic" approach. More essential abstraction and comprehensive analysis is needed for a strong ICLR paper.
Reject. rating score: 3. rating score: 4. rating score: 5. <BRK>Also scheduling regularization parameter like reducing the size of lambda exponentially would be interesting as well. Best response dynamics is not used in the conventional GAN training, because it s very hard to find the global optimal of inner minimization and outer maximization. In the WGAN paper, the gradient is clipped to a number less than 1, because it is a sufficient condition to being 1 Lipshitz, but this paper provides no justification on this number. I also found that the hypothesis on the model collapsing has very limited connection to the convex concave case.<BRK>This paper addresses the well known stability problem encountered when training GANs. Relevance: Although I think some of the empirical results provided in the paper are interesting, I doubt the scientific contribution of this paper is significant. Comparison to existing work: This is not the first paper that suggests adding a regularization. In this paper, the author apply the same penalty to the GAN objective with the alternative update rule which is also a lower bound for the Wasserstein distance.<BRK>Clarity Overall, the paper is clear and well written. Originality The idea is novel and interesting. Moreover, the numerical experiments are in favor of the proposed method. Comments   Why should the norm of the gradient should to be equal to 1 and not another value?
Reject. rating score: 4. rating score: 4. rating score: 4. <BRK>It lacks both in sound theoretical justification and intuitive motivation of the approach.<BRK>The attack model is too rough. As s_attack increases, the anomaly data points are moved farther away from the normal data cloud, altering the position of the separating hyperplane. Experiments   why/how would you have distorted test data?<BRK>Although the problem addressed in the paper seems interesting, but there lacks of evidence to support some of the arguments that the authors make. The idea proposed by the authors seems too quite simple.
Reject. rating score: 5. rating score: 5. rating score: 5. <BRK>This paper presents an empirical study of whether data augmentation can be a substitute for explicit regularization of weight decay and dropout. However, overall I do not find the authors’ premises and conclusions to be well supported by the results and would suggest further investigations.<BRK>REVISION: I applaud the effort the authors have put in to address many of my and the other reviewers  comments. The paper proposes data augmentation as an alternative to commonly used regularisation techniques like weight decay and dropout, and shows for a few reference models / tasks that the same generalization performance can be achieved using only data augmentation. It would be interesting to include this setting in the experiments as well.<BRK>The data augmentation techniques are also shown to be insensitive to hyper parameters, so easier to use than explicit regularizers when changing architectures. It is good to have a systematic study of data augmentations, however, the materials in this paper in the current state might not be a strong ICLR publication.
Invite to Workshop Track. rating score: 6. rating score: 5. rating score: 4. <BRK>The authors propose a method for learning node representations which, like previous work (e.g.node2vec, DeepWalk), is based on the skip gram model. The paper is well written and it is quite easy to follow along with the discussion.<BRK>2.The experiment setup is not fair to the competitors.<BRK>In general, the paper lacks an in depth analysis of when the approach works and when it does not. The faster runtime is interesting but not surprising given the ego centric nature of the approach.
Reject. rating score: 5. rating score: 5. rating score: 7. <BRK>The authors propose autoencoding text using a byte level encoding and a convolutional network with shared filters such that the encoder and decoder should exhibit recursive structure. They show that the model can handle various languages and run various experiments testing the ability of the autoencoder to reconstruct the text with varying lengths, perturbations, depths, etc.<BRK>The paper aims to illustrated the representation learning ability of the convolutional autoencoder with residual connections is  proposed by to encode text at the byte level. It appears that the byte error shoot up for sequences of length 512+ (fig.6 and fig. Are these numbers on the same test set? While the datasets are large and would take a lot of time to process for each case study, a final result on the complete data set, to illustrate if the model does learn well with lots of data would have been useful.<BRK>This paper presents a convolutional auto encoder architecture for text encoding and generation. It works on the character level and contains a recursive structure which scales with the length of the input text. The authors have decided to encode the text into a length of 1024   Why? Would different lengths result in a better performance? Minor issues:Please correct minor linguistic mistakes as well as spelling mistakes. In Fig.3, for example, the t of Different is missing.
Reject. rating score: 3. rating score: 4. rating score: 5. <BRK>The key argument authors present against ReLU+BN is the fact that using ReLU after BN skews the values resulting in non normalized activations. Although the BN paper suggests using BN before non linearity many articles have been using BN after non linearity which then gives normalized activations (https://github.com/ducha aiki/caffenet benchmark/blob/master/batchnorm.md) and also better overall performance. I encourage the authors to validate their claims against simple approach of using BN after non linearity.<BRK>This paper proposes an activation function, called displaced ReLU (DReLU), to improve the performance of CNNs that use batch normalization. The arguments for skipping this experiments are respectful, but not convincing enough.<BRK>Overall, I don’t think this paper meet ICLR’s novelty standard, although the authors present some good numbers, but they are not convincing. The author runs a few CIFAR 10/100 experiments with DReLU. Using expectation to explain why DReLU works well is not sufficient and convincing. 3.In all experiments, ELU/LReLU are worse than ReLU, which is suspicious.
Invite to Workshop Track. rating score: 6. rating score: 6. rating score: 5. <BRK>Since existing GNNs are not computational efficient when dealing with large graphs, the key engineering contributions of the proposed method, GPNN, are a partitioning and the associated scheduling components. The paper is well written and easy to follow. However, vanilla GCN could be trivially partitioned and propagating just as shown in this paper. The primary one is that the method is incremental and rather heuristic.<BRK>The proposed solution is to partition the graph into sub graphs, and use a schedule alternating between performing intra and inter graph partitions operations.<BRK>Perhaps it is my misunderstanding of the way in which GNNs work, but isn t the objective actually to reach a set of fixed point equations. Experimentally, the proposed approach seems to perform comparably to existing methods (or slightly worse on average in some settings). The paper is well written and easy to read. Specific comments:1)  "When information from any one node has reached all other nodes in the graph for the first time, this problem is considered as solved."
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>The paper shows that several recently proposed interpretation techniques for neural network are performing similar processing and yield similar results. The authors propose a metric called sensitivity n for that purpose, and make some observations about the optimality of some interpretation techniques with respect to this metric in the linear case. Results further outline the resemblance between the compared methods. In the appendix, the last step of the proof below Eq.7 is unclear.<BRK>This paper discusses several gradient based attribution methods, which have been popular for the fast computation of saliency maps for interpreting deep neural networks. This gives a more unified way of understanding, and implementing the methods. The paper points out situations when the methods are equivalent  The paper analyses the methods  sensitivity to identifying single and joint regions of sensitivity  The paper proposes a new objective function to measure joint sensitivityOverall, I believe this paper to be a useful contribution to the literature.<BRK>The paper summarizes and compares some of the current explanation techniques for deep neural networks that rely on the redistribution of relevance / contribution values from the output to the input space. The main contributions are the introduction of a unified framework that expresses 4 common attribution techniques (Gradient * Input, Integrated Gradient, eps LRP and DeepLIFT) in a similar way as modified gradient functions and the definition of a new evaluation measure ( sensitivity n ) that generalizes the earlier defined properties of  completeness  and  summation to delta . The unified framework is very helpful since it points out equivalences between the methods and makes the implementation of eps LRP and DeepLIFT substantially more easy on modern frameworks.
Reject. rating score: 3. rating score: 4. rating score: 4. <BRK>While the idea makes sense, the paper is not well executed, and I cannot understanding how gradient descend is performed based on the description of Section 4. The update rule seems to be clearly wrong.<BRK>The idea is introduced clearly and rather straightforward. cons:The provided experiments are weak to demonstrate the effectiveness of the proposed method. (2) the results on MNIST and CIFAR 10 are not good enough for practical deployment.<BRK>The paper proposes a technique for training quantized neural networks, where the precision (number of bits) varies per layer and is learned in an end to end fashion. More generally, the direction of the gradient will be highly dependent on the specific setting of the current weights. This is worked around by updating the parameter using the sign of its gradient.
Reject. rating score: 5. rating score: 7. rating score: 7. <BRK>This paper studies the problem of learning to generate graphs using deep learning methods. The main challenges of generating graphs as opposed to text or images are said to be the following:(a) Graphs are discrete structures, and incrementally constructing them would lead to non differentiability (I don t agree with this; see below)(b) It s not clear how to linearize the construction of graphs due to their symmetries. A downside to the algorithm is that it has complexity O(k^4) for graphs with k nodes, but the authors argue that this is not a problem when generating small graphs. Experimentally, generative models of chemical graphs are trained on two datasets. The exposition is clear (although a bit more detail on MPM matching would be appreciated)However, there are some significant weaknesses. Second, the experiments are quite weak. No baselines are presented to back up the claims motivating the formulation. This is very clearly not true. Overall, the paper is about an interesting subject, but in my opinion the execution isn t strong enough to warrant publication at this point.<BRK>This work proposed an interesting graph generator using a variational autoencoder. The work should be interesting to researchers in the various areas. However, the work can only work on small graphs. The search space of small graph generation is usually very small, is there any other traditional methods can work on this problem? Moreover, the notations are a little confusing.<BRK>The authors propose a variational auto encoder architecture to generate graphs. Pros:  the formulation of the problem as the modeling of a probabilistic graph is of interest   some of the main issues with graph generation are acknowledged (e.g.the problem of invariance to node permutation) and a solution is proposed (the binary assignment  matrix)  notions for measuring the quality of the output graphs are of interest: here the authors propose some ways to use domain knowledge to check simple properties of molecular graphs Cons:   the work is quite preliminary  many crucial elements  in graph generation are not dealt with:  a) the adjacency matrix and the label tensors are not independent of each other, the notion of a graph is in itself a way to represent the  relational links  between the various components b) the boundaries between a feasible and an infeasible graph are sharp: one edge or one label can be sufficient for acting the transition independently of the graph size, this makes it a difficult task for a continuous model. The authors acknowledge this but do not offer ways to tackle the issue c) conditioning on the label histogram should make the problem easy: one is giving away the number of nodes and the label identities after all; however even in this setup the approach fails more often than not  d) the graph matching procedure proposed is a rough patch for a much deeper problem  the evaluation should include a measure of the capacity of the architecture to : a) reconstruct perfectly the input b) denoise perturbations over node labels and additional/missing  edges
Reject. rating score: 6. rating score: 6. rating score: 6. <BRK>This seems indeed an important point, but it does not appear as clear how to proceed (e.g., uncertainty on w_phi(lambda) which later needs to propagated to L_val); could the authors perhaps further elaborate? While the first two may be used in practice, the third scheme is not used in practice to the best of my knowledge. Together with the new experiments and comparisons, I have therefore updated my rating from 5 to 6. The paper should reformulate its statements in the light of this literature.<BRK>The lack of representative models such as the ones used in practical applications. However, it is not clear to what extent this is true. The paper is clearly written with only a few typing errors.<BRK>This is similar to the Bayesian optimization setting but with some advantages such as the ability to evaluate the function stochastically. I find the approach to be interesting and the paper to be well written.
Accept (Poster). rating score: 7. rating score: 6. rating score: 5. <BRK>The authors present a scalable model for questioning answering that is able to train on long documents.<BRK>I am wondering if the authors can also add results on other RC datasets (e.g., SQuAD) and see if the model can generalize or not.<BRK>Did the authors try truncating after more words (e.g., 10k)? The reported results were state of the art(*) on the TriviaQA dataset at the time of the submission deadline. Overall, I found the paper interesting to read, and scaling QA up to larger documents is definitely an important research direction. * The TriviaQA leaderboard shows a submission from 9/24/17 (by "chrisc") that has significantly higher EM/F1 scores than the proposed model.
Reject. rating score: 4. rating score: 4. rating score: 4. <BRK>The claimed results of  "combining transformations" in the context of RC was done in the works of Herbert Jaeger on conceptors [1], which also should be cited here. Also the authors overstate the claim of biological plausibility (just because we don t train the recurrent weights does not make a method biologically plausible). [1] H. Jaeger (2014): Controlling Recurrent Neural Networks by Conceptors. In total this paper does not have enough novelty for acceptance and the experiments are not well chosen for this kind of work.<BRK>The technical part of the paper is a nice study for classification with Echo State Networks. The main novelty here is the task itself, classifying different distortions of MNIST data. The task is interesting but by itself I don t find it convincing enough. If biological plausibility was the goal, a different approach should have been used altogether (e.g., what about local training of connections, unsupervised training, ...). A small number of training examples would have been a more specific and better motivation, given that the number of "training" examples for humans is only discussed qualitatively and without a reference. The analysis using the PCs is nice; the works by Jaeger on Conceptors (2014) make also use of the principal components of the reservoir states during presentation of patterns (introduction in https://arxiv.org/abs/1406.2671), so seem like relevant references to me.<BRK>If so, what were the results? The paper is well written and easy to follow, but I have concerns about the claims it makes relative to how convincing the results are. Also, does the order in which the transforms are applied affect their relative representative strength in the reservoir? Further, treating MNIST data as a time series is artificial and clunky. I don t believe that there is sufficient support for this statement in the conclusion, "[ML/deep networks] do not work as well for generalization of learning.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>Nevertheless, it appears to be effective against FGSM attacks, and competitive with adversarial training specifically to defend against FGSM. The authors provide less convincing evidence that the defense is effective against white box attacks. Utilizing a trained GAN, the authors propose the following defense at inference time. This is done by SGD.<BRK>This paper presents a method to cope with adversarial examples in classification tasks, leveraging a generative model of the inputs. Is that what is meant by the "defense network" (in experiments bullet 2)? The authors test their method on various adversarially constructed inputs (with varying degrees of noise). In the black box vs. white box scenarios, can the attacker know the GAN parameters?<BRK>This paper presents Defense GAN: a GAN that used at test time to map the input generate an image (G(z)) close (in MSE(G(z), x)) to the input image (x), by applying several steps of gradient descent of this MSE. + The paper is easy to follow. + It seems (but I am not an expert in adversarial examples) to cite the relevant litterature (that I know of) and compare to reasonably established attacks and defenses.
Reject. rating score: 5. rating score: 5. rating score: 6. <BRK>Another direction I think would be interesting, is how few examples are needed in the canonical distribution? The single net baseline is good, but I d like to get a clearer picture of its results. However, I think the experimental conditions are very limited:  Only one collection of transformations is studied, and on MNIST digits only.<BRK>2) The authors only run experiments on the MNIST data, where 1) the mechanisms aresimulated and relatively simple, and 2) samples from the canonical distributionare also available. Did the authors run experiments on other datasets? Experiments on MNIST data shows that in the end of training, each expert winsalmost all samples from one transformation and no other, which confirms thateach expert model a single inverse transformation.<BRK>This paper presents a framework to recover a set of independent mechanisms. p7 authors have also noticed that several experts fail to specialize and I bet that is the reason why. Why will D answer negatively (or positively) on this example ? My main concern with this work is that I don t see any mechanism in the framework that prevents an expert  (or few of them) to win all examples except its own learning capacities.
Accept (Oral). rating score: 8. rating score: 7. rating score: 10. <BRK>This work proposes a variation of the DenseNet architecture that can cope with computational resource limits at test time. My only major concern is the degree of technical novelty with respect to the original DenseNet paper of Huang et al.(2017).The authors add a hierarchical, multi scale structure and show that DenseNet can better cope with it than ResNet (e.g., Fig.3).They investigate pros and cons in detail adding more valuable analysis in the appendix. However, this work is basically an extension of the DenseNet approach with a new problem statement and additional, in depth analysis.<BRK>This paper presents a method for image classification given test time computational budgeting constraints. A convolutional neural network structure with a diagonal propagation layout over depth and scale is used, so that each activation map is constructed using dense connections from both same and finer scale features. Also on p.6 I m not entirely clear on how the "network reduction" is performed   it looks like finer scales are progressively dropped in successive blocks, but I don t think they exactly correspond to those that would be needed to evaluate the full model (this is "lazy evaluation").<BRK>This paper introduces a new model to perform image classification with limited computational resources at test time. The model is based on a multi scale convolutional neural network similar to the neural fabric (Saxena and Verbeek 2016), but with dense connections (Huang et al., 2017) and with a classifier at each layer.
Reject. rating score: 4. rating score: 5. rating score: 6. <BRK>This paper proposes replacing fully connected layers with block diagonal fully connected layers and proposes two methods for doing so. It also make some connections to random matrix theory. The parameter pruning angle in this paper is fairly weak. Moreover, they are both missing their other half where the technique or insight they propose is exploited to achieve something.<BRK>As a second contribution, the authors show experimentally that the random matrix theory can provide a good model of the spectral behavior of the weight matrix when it is large. Originality: this paper introduces block diagonal matrices to structure the weights of a neural network. The idea of structured matrices in this context is not new, but the diagonal block structure appears to be.<BRK>I was also wondering about when 2 or more layers are block sparse, do these blocks overlap? (2) that therefore, block diagonal layers lead to more efficient networks. The authors seem to restrict themselves to convolutional networks in the first paragraph (and experiments) but don t discuss the implications or reasons of this assumption.
Accept (Poster). rating score: 9. rating score: 8. rating score: 3. <BRK>I think either a stronger supervised baseline should be included (trained on comparable data as the unsupervised approach), or the language/claims in the paper should be softened. I therefore recommend that the paper be accepted. I think the paper s comparisons are valid, but the abstract and introduction make very strong claims about outperforming "state of the art supervised approaches". Would it be possible to add weights to the terms in eq.(6), or is this done implicitly?<BRK>The paper is very well written and makes for a rather pleasant read, save for some need for down toning the claims to novelty as voiced in the comment re: Ravi & Knight (2011) or simply in general: it s a very nice paper, I enjoy reading it *in spite*, and not *because* of the text sales pitching itself at times. There are some gaps in the awareness of the related work in the sub field of bilingual lexicon induction, e.g.the work by Vulic & Moens (2016).<BRK>The paper, however, misses comparison against important work from the literature that is very relevant to their task — decipherment (Ravi, 2013; Nuhn et al., 2012; Ravi & Knight, 2011) and other approaches like CCA. The former set of works, while focused on machine translation also learns a translation table in the process. Besides, the authors also claim that their approach is particularly suited for low resource MT and list this as one of their contributions.
Reject. rating score: 5. rating score: 5. rating score: 6. <BRK>This paper introduces a simple extension to parallelize Hyperband. Likewise for the LSTM acoustic model. Likewise, for the large scale experiment, a single run of Vizier actually yields as good performance as the best of the 5 SHA variants, and it is unknown beforehand which SHA variant works best   in this example, actually Bracket 0 (which is often the best) stagnates. The justification to not compare against Fabolas in the parallel regime is clearly valid. Therefore, all empirical results for parallel Hyperband reported in the paper appear to be negative. However, this is not what I see in the results. If not, based on these negative results the paper does not seem to quite clear the bar for ICLR.<BRK>Briefly, Hyperband builds on a "successive halving" algorithm. I think the non model based approach to hyperparameter tuning is compelling and is of interest to the AutoML community, as it raises an obvious question of how approaches that exploit the fact that training can be stopped any time (like Hyperband) can be combined with model based optimization that attempt to avoid evaluating configurations that are likely to be bad. However, I do have a few comments and concerns for the for the authors to address that I detail below. The authors further extend Hyperband by allowing the successive halving algorithm to be run in parallel.<BRK>This paper adapts the sequential halving algorithm that underpins Hyperband to run across multiple workers in a compute cluster. The paper is crisply written, the extension is a natural one, the experiment protocols and choice of baselines are appropriate.
Reject. rating score: 2. rating score: 3. rating score: 3. <BRK>I am not sure how to interpret this paper. The paper seems to be very thin technically, unless I missed some important details. Two proposals in the paper are:(1) Using a learning rate decay scheme that is fixed relative to the number of epochs used in training, and (2) Extract the penultimate layer output as features to train a conventional classifier such as SVM. And for (2), it is a relatively standard approach in utilizing CNN features. I fail to see how this would lead to superior performance compared to conventional CNNs.<BRK>This paper deals with early stopping but the contributions are limited. The training time of the feature extractor has never been a problem for transfer learning tasks for example: once it is trained, you can reuse the architecture in a wide range of tasks. Furthermore, this supervised technique is only compared to unsupervised or predefined methods, which is is not fair and the training time of the Scattering Transform is not reported, for example. Finally, extracting features is mainly useful on ImageNet (for realistic images) and this is not reported here. I believe re thinking new learning rate schedules is interesting, however I recommend the rejection of this paper.<BRK>This paper proposes a fast way to learn convolutional features that later can be used with any classifier. The acceleration of the training comes from a reduced number of training epocs and a specific schedule decay of the learning rate. Cons:  Considering an adaptive schedule of the learning decay is common practice in modern machine learning. It is not clear why, once a CNN has been trained, one should want to change the last layer and use a SVN or other classifiers.
Accept (Poster). rating score: 7. rating score: 7. rating score: 4. <BRK>the paper proposes an evaluation method for training GANs using four standard distribution distances in literature namely:  JSD  Pearson chi square  MMD  Wasserstein 1For each distance, a critic is initialized with parameters p. The critic is a neural network with the same architecture as the discriminator. If one can clearly identify the good generators from bad generators using a weighted sum of these 4 distances on Imagenet or LSUN, this is a metric that is going to stand well. If the paper has to convince me that this metric is good and should be used, I need to see experiments on one large scale datset, such as Imagenet or LSUN.<BRK>* nice to see an evaluation on how models scale with the increase in training data. Using an Independent critic for evaluation has been proposed and used in practice before, see “Comparison of Maximum Likelihood and GAN based training of Real NVPs”, Danihelka et all, as well as Variational Approaches for Auto Encoding Generative Adversarial Networks, Rosca at all.<BRK>This paper proposes using divergence and distance functions typically used for generative model training to evaluate the performance of various types of GANs. In Section 3, the evaluation metrics are existing metrics and some of them have already been used in comparing GAN models. Maximum mean discrepancy has been used before in work by Yujia Li et al.(2016, 2017)2. Some empirical results are still preliminary.
Reject. rating score: 4. rating score: 5. rating score: 6. <BRK>This paper proposed a GAN to unify classification and novelty detection. The 3rd issue is that the novelty for the novelty detection part in the proposed GAN seems quite incremental. Therefore, it is conceptually and theoretically strange to apply GAN to novelty detection, which is the major contribution of this paper. Moreover, there are many papers known as "learning with abstention" and/or "learning with rejection" from NIPS, ICML, COLT, etc.<BRK>The paper presents a method for novelty detection based on a multi class GAN which is trained to output images generated from a mixture of the nominal and novel distributions. can t the approach presented in the paper be naively extended to a regular GAN as long as it is trained to output a mixture distribution? Comparing ROC curves for the output of a discriminator from such a regular GAN to that of the ND GAN would help to asses the importance of the discussion of the mixture distribution. i.e.a GAN which was not trained to output a mixture distribution.<BRK>pros:  good application of GAN models  good writing and clarity  solid experiments and explanationscons:   results weak relative to naive baseline (entropy)   weak comparisons   lack of comparison to density models [1] Louizos, Christos, and Max Welling. The paper is well written, derives cleanly from previous work, and has solid experiments. The experiments are weak 1) in the sense that they are not compared against simple baselines like p(x) (from, say, just thresholding a vae, or using a more powerful p(x) model   there are lots out there), 2) other than KNNs, only compared with class prediction based novelty detection (entropy, thresholds), and 3) in my view perform consistently, but not significantly better, than simply using the entropy of the class predictions. The authors may be interested in [1], a principled approach for learning a well calibrated uncertainty estimate on predictions.
Reject. rating score: 4. rating score: 5. rating score: 7. <BRK>Summary: The paper considers the prediction problem where labels are given as multisets. The results show that the proposed methods optimizing the loss function perform better than other alternatives. Comments: The problem of predicting multisets looks challenging and interesting. On the other hand, I have several concerns about writing and technical discussions. :    An exact definition of the term multiset is not given. The definitions appear in Appendix, which might not be well defined. Then, by definition, Prec(y,Y) 3/3  1. 3 since it is not known a priori.<BRK>This paper proposes a type of loss functions for the problem of multiset prediction. The proposed loss function assumes that T (i.e.the size of the output set) is known, which is unrealistic. I think this solution is rather hacky and I’d like to see a more elegant solution, e.g.incorporate the loss on T into the proposed loss function.<BRK>In more detail, the authors construct an oracle policy, shown to be optimal (in terms of precision and recall). A parametrized policy instead of the oracle policy is utilized in the proposed multiset loss function, while furthermore, a termination policy facilitates the application on variable sized multiset targets. The paper describes an interesting approach to the problem.
Reject. rating score: 5. rating score: 5. rating score: 5. <BRK>This paper presents a procedure to efficiently do K shot learning in a classification setting by creating informative priors from information learned from a large, fully labeled dataset. Image features are learned using a standard convolutional neural network the last layer form image features, while the last set of weights are taken to be image "concepts". Probabilistic modeling section: treating the trained weights like "data" is a good way to convey intuition about your method. If not, would this influence the choice of covariance structure for \Sigma_MAP? How sensitive are inferences to the choice of Normal inverse Wishart hyper parameters? Clarity: The paper is pretty clearly written, however some specific details of the method are difficult to understand. Novel: I am not familiar with K shot learning tasks to assess the novelty of this approach. Impact: While the reported results seem impressive and encouraging, I believe this a relatively incremental approach.<BRK>1.The assumption about independence of w vectors across classes is a very strong one and as far as I can see, it does not have a sound justification. Therefore, it is very likely that vectors of different classes are highly correlated. The proposed model estimates $\theta^{MAP}$ using only one W matrix, the one that is estimated by training the original network in the most usual way. As far as I can see, there is no good recipe presented in the article for setting this prior. If the prior is set to 0, how different is this vector from 0? Authors should focus on this in my opinion to explain why methods work differently in 1 shot learning. In the other problems, the results suggest they are pretty much the same. If the method was substantially different than baseline, I believe this would have been no problem. Given the proximity of the proposed method to the baseline with regularised logistic regression, lack of empirical advantage is an issue. If the proposed model works better in the 1 shot scenario, then authors should delve into it to explain the advantage. It needs to be rewritten in my opinion.<BRK>The authors introduce a probabilistic k shot learning model based on previous training of a CNN on a large dataset. The paper introduces a very simple idea that allows transfer knowledge from a network trained with a large dataset to a network trained with a smaller amount of data, the data with k shot examples per class. This is not a general way to do k shot learning, because it heavily depends on the availability of the large dataset where the weights of the soft max function can be extracted. But it seems to work for natural image data. The experiments on this dataset use models that are variations of the same proposed model.
Accept (Poster). rating score: 5. rating score: 4. rating score: 4. <BRK>Moreover, authors also look into privacy analysis to guarantee some level ofdifferential privacy is preserved. Reading the paper,many questions arise in mind:  The paper implicitly assumes that the statistics from all the users must  be collected to improve "general English". Why is this necessary? Why not  just using better enough basic English and the text of the target user? There are huge number of previous work on context dependent language models,  let alone a mixture of general English and specific models. Minor:  t of $G_t$ in page 2 is not defined so far.<BRK>The goal is to customize the predictions for each user to account for member specific information while adhering to the strict compute constraints and privacy requirements. The authors propose a simple method of mixing the global model with user specific data. Collecting the user specific models and averaging them to form the next global model. However, I am not convinced that this is novel enough for publication at ICLR. However, it is not necessary that the population of users will adhere to general English and hence the averaged model at the next time step t+1 might be significantly different from general English.<BRK>my main concern is the relevance of this paper to ICLR. This paper is much related not to representation learning but to user interface. The paper is NOT well organized and so the technical novelty of the method is unclear. For example, the existing method and proposed method seems to be mixed in Section 2. You should clearly divide the existing study and your work. The experimental setting is also unclear.
Invite to Workshop Track. rating score: 6. rating score: 4. rating score: 4. <BRK>The model is evaluatedon two sets of datasets and the tree to tree model outperforms seq2tree andseq2seq models significantly for the program translation problem. The tree encoder, however, is based on the standard Tree LSTM and the applicationin this case is synthetic as the datasets are generated using a manual rule based translation. As the authors also mention it might be challenging to obtain the aligned examplesfor training the model in practice. What type of attention mechanisms are used? It would be interesting to observe the generalization capabilities of all the different models.<BRK>In addition, authors had generated the source CoffeeScript codes, which seems that this task is only one of "synthetic" task and no longer capture any real world s programs. Second, it is also unclear what the method to generate train/dev/test data is.<BRK>Experimentally, the model is applied to two synthetic datasets, where programs in the source domain are sampled from a PCFG and then translated to the target domain with ahand coded translator. Results show that thenproposed approach outperforms sequence representations or serialized tree representationsof inputs and outputs. I would be interested to know more details about how the hand coded translator works. Statements like "We are the first to consider employing neural network approaches towards tackling the problem [of translating between programming languages]" areobviously not true (surely many people have *considered* it), and they re particularlygrating when the treatment of related  work is poor, as it is in this paper. Association for Computational Linguistics, 2011.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. <BRK>Specifically, the authors proposed a novel architecture to decompose the identity latent code and non identity latent code. Overall, I think this is a good paper. The authors should also evaluate this kind of identity. 3.When making the comparison with related work, the authors mentioned that Info GAN was not able to determine which factors are assigned to each dimension.<BRK>Also, there is no comparison with CoGAN, which I believe is the most relevant work for coupled image generation. Assuming images can be disentangled by identity related factors and style factors, this paper proposed an algorithm that produces a pair of images with the same identity. The discriminator was used to distinguish paired images from database or paired images sampled by the algorithm. First, during training, the proposed SD GAN needs to access the identity information and there is only limited identity in the dataset. Overall, I rate this paper slightly above borderline.<BRK>The idea for disentangling identity from other factors of variation using identity matched image pairs is quite simple, but the experimental results on faces and shoes are impressive. SignificanceDisentangled generative models are an important line of work in my opinion. Not much technically new or surprising compared to past work on disentangling generative models.
Accept (Poster). rating score: 9. rating score: 6. rating score: 6. <BRK>Pros:  Clear presentation, easy to follow. Joint training of compression + other tasks. As far as I know this is the first paper to talk about this particular scenario. Cons:  I would have liked to have a discussion on the effect of the encoder network. Only one architecture/variant was used. For PSNR, SSIM and MS SSIM I would like a bit more clarity whether these were done channel wise, or on the grayscale channel. While runtime is given as pro, it would be nice for those not familiar with the methods to provide some runtime numbers (i.e., breakdown how much time does it take to encode and how much time does it take to classify or segment, but in seconds, not flops).<BRK>PROSP.1 Joint training for both compression and classification. First time to the authors knowledge. The authors already point at another work where they explore the efficient compression with GPUs, but this point is the weakest one for the adoption of the proposed scheme. An effort of compression would be advisable, moving some of the non core results to the appendixes. Q2.Did you try a joint training for the segmentation task as well ? Q4.Actually results in Figure 10 do not seem good... or maybe I am not understanding them properly.<BRK>3.Experimental setup part is long but not well explained and is not self contained particularly for the evaluation metrics. 4.3 contains extra information and could be summarized in a more consistent way. In the last part of the paper, both compression and classification parts are jointly trained, and it is empirically presented that both results improved by jointly training them. However, to me, it is not clear if the trained compression model on this specific dataset and for the task of classification can work well for other datasets or other tasks. The experimental setup and the figures are not well explained and well written.
Invite to Workshop Track. rating score: 5. rating score: 4. rating score: 4. <BRK>The paper proposes a set of methods for using temporal information in event sequence prediction. Two methods for time dependent event representation are proposed. The motivation of the paper is interesting and I like the approach. More advance models may be needed.<BRK>The authors present a model base on an RNN to predict marks and duration of events in a temporal point process. The main innovation of the paper is a new representation of a point process with duration (which could also be understood as marks), which allows them to use a "time mask", following the idea of word mask introduced by Choi et al, 2016. They compare their method to several variations of their own method, two trivial baselines, and one state of the art method (RMTPP) using several real world datasets and report small gains with respect to that state of the art method. More specifically, my concerns, which prevent me from recommending acceptance, are as follows:  The authors assume the point process contains duration and intervals, however, point processes generally do not have duration per event but they are discrete events localized in particular time points.<BRK>It is a nice attempt that in this work, duration is used for event representation. However, the choices are not “principled” as claimed in the paper. 2.Event time joint embedding sounds sensible as it essentially remaps from the original value to some segments. Experiments on various datasets were conducted and most details are provided.
Reject. rating score: 5. rating score: 6. rating score: 8. <BRK>The paper proposes a method for inferring dynamical models from partial observations, that can later be used in model based RL algorithms such as I2A. While this is an interesting approach, many of the architectural choices involved seem arbitrary and unjustified. This wouldn t be so bad if they were justified by empirical success rather than principled design, but I m also a bit skeptical of the strength of the results. This makes it hard to evaluate whether the significant wins are indicative of the strength of the proposed method.<BRK>There are several variations of the hidden state space [ds]SSM model: using det/stochastic latent variables + using det/stochastic decoders. Main results seem to be:1. Writing is clear, although a bit dense in places. The paper lacks any visualization of the latent codes. Are the latent codes relevant in the stochastic model?<BRK>The authors provide a deeper exploration of Imagination Agents, by looking more closely at a variety of state space models. Of course, while we can always demand more and more experiments, I felt that this paper did a good enough job to merit publication. Overall, I think the strongest part of this paper is in its conceptual contributions   I find the work thought provoking and inspiring.
Accept (Poster). rating score: 7. rating score: 6. rating score: 4. <BRK>Brief Summary:The paper introduces a deep learning based approach that approximates spectral clustering. The method is more scalable than the vanilla spectral clustering algorithm and it can also be used to cluster a new incoming data point without redoing the whole spectral clustering procedure. It is not clear why the learned matrix should outperform Gaussian kernel, but the experiments show that it does. Incorporating the ortho normalization constraint in the final layer of the neural network is interesting.<BRK>The authors study deep neural networks for spectral clustering in combination with stochastic optimization for large datasets. Overall it is an interesting study, though the connections with the existing literature could be strengthened:  The out of sample extension aspects and scalability is stressed in the abstract and introduction to motivate the work. On the other hand in Table 1 there is only compared with methods that do not possess these properties.<BRK>PAPER SUMMARYThis paper aims to address two limitations of spectral clustering: its scalability to large datasets and its generalizability to new samples. But this issue can be addressed using approximate nearest neighbors obtained, e.g., via hashing. Overall, the paper compares only to vanilla spectral clustering, which is not representative of the state of the art. 3)  Continuing with the point above, an experimental comparison with prior work on large scale spectral clustering (see, e.g.[a] and the references therein) is missing. In particular, the result of spectral clustering on the Reuters database is not reported, but one could use other scalable versions of spectral clustering as a baseline. 4)  Another benefit of the proposed method is that it can handle out of sample data. The methods in [c] are also tested on larger datasets. [a] Choromanska, et.
Invite to Workshop Track. rating score: 6. rating score: 5. rating score: 4. <BRK>I have several concerns:  The author should discuss the intuition why the rotation has to be from the generated memory target τ to the embeded input ε but not the other way around or other direction in this 2D subspace. The description of parameter meter τ is not clear.<BRK>They run experiments on several toy tasks and on language modelling with PTB character level language modeling (which I would still consider to be toyish.) Why this particular parameterization of the rotation matrices is used and where does actually that come from? This does not tell us much. Can you point out to some citation?<BRK>The motivation and the details of the rotational memory are explained clearly. However, the experimental results reported in the paper seem to be a bit weak to support the claims made by the authors. The performance improvements are not so clear to me.
Reject. rating score: 3. rating score: 3. rating score: 5. <BRK>The paper intends to interpret a well trained multi class classification deep neural network by discovering the core units of one or multiple hidden layers for prediction making. However, these discovered core units are specific to a particular class, which are retained to maintain the deep neural network’s ability to separate that particular class from the other ones.<BRK>The paper develops a technique to understand what nodes in a neural network are importantfor prediction. One question from the use of the Indian Buffet Process: how do the asymptotics of the feature allocation determine the number of hidden units selected? The results are neat, but I couldn t tell why this approach was better than others.<BRK>The paper should include more analysis of how this method helps interpret the  actions of the neural net, once the core units have been identified. However, it only identifies hidden units that are important fora class, not what are important for any particular input. ClarityThe problem formulation and objective function (Section 3.1) was hard to follow. SignificanceThe paper addresses an important problem of trying to have more interpretableneural networks.
Accept (Poster). rating score: 9. rating score: 7. rating score: 5. <BRK>They find that the performance of networks that memorize more are also more affected by ablations. * Originality: This work is one of many recent papers trying to understand generalization in deep networks. The insights of this paper will have a large impact on regularization, early stopping, generalization, and methods used to explain neural networks. * Observations are very clearly contextualized with respect to several active areas of deep learning research. Does reliance on single directions not also imply a local encoding scheme?<BRK>Summary:  nets that rely on single directions are probably overfitting  batch norm helps not having large single directions  high class selectivity of single units is a bad measure to find "important" neurons that help a NN generalize. The experiments that this paper does are quite interesting, somewhat confirming intuitions that the community had, and bringing new insights into generalization. The presentation is good overall, but many minor improvements could help with readability. Figure 2 s y values drop rapidly as a function of x, maybe make x have a log scale or something that zooms in near 0 would help readability.<BRK>This is an "analyze why" style of paper:  the authors attempt to explain the relationship between some network property (in this case, "reliance on single directions"), and a desired performance metric (in this case, generalization ability). Clarity:  The paper is fairly clearly written. Originality: This work is one in a series of papers about the topic of trying to understand what leads to good generalization in deep neural networks. I think the demonstration that the existence of strongly class selective neurons is not a good correlate for generalization is interesting. This seems to me the key question to understanding the significance of their results. ImageNet or some other challenging dataset?
Invite to Workshop Track. rating score: 7. rating score: 5. rating score: 4. <BRK>The author uses reinforcement learning to find new potential activation functions from a rich set of possible candidates. One candidate stood out and is thoroughly analyze in the reste of the paper. The analysis is conducted across images datasets and one translation dataset on different architectures and numerous baselines, including recent ones such as SELU. Overall the paper is well written and the lack of theoretical grounding is compensated by a reliable and thorough benchmark. While a new activation function is not exiting, improving basic building blocks is still important for the community.<BRK>I recommend to submit this paper to ICLR workshop track. The authors also run a number of ImageNet experiments, and one NTM experiment. 2.There is no theoretical depth in the searched activation about why it is better.<BRK>Authors propose a reinforcement learning based approach for finding a non linearity by searching through combinations from a set of unary and binary operators. The best one found is termed Swish unit; x * sigmoid(b*x). As pointed out by the authors themselves for b 1 Swish is equivalent to SiL proposed in Elfwing et. Again, the authors do state that "our results may not be directly comparable to the results in the corresponding works due to differences in our training steps." More explanation is required on why is it important and how does it help optimization.
Reject. rating score: 6. rating score: 6. rating score: 7. <BRK>SUMMARY.The paper presents an extension of graph convolutional networks. The authors propose two extensions of GCNs, they first remove intermediate non linearities from the GCN computation, and then they add an attention mechanism in the aggregation layer, in order to weight the contribution of neighboring nodes in the creation of the new node representation. Interestingly, the proposed linear model obtains results that are on par with the state of the art model, and the linear model with attention outperforms the state of the art models on several standard benchmarks. OVERALL JUDGMENTThe paper is, for the most part, clear, although some improvement on the presentation would be good (see below). I like the idea of using a very minimal attention mechanism. It would be interesting if the authors could elaborate a bit more on the choice of the similarity function. DETAILED COMMENTSPage 2. I do not understand the point of so many details on Graph Laplacian Regularization. Page 2.The use of the term  skip grams  is somewhat odd, it is not clear what the authors mean with that.<BRK>The paper proposes graph based neural network in which weights from neighboring nodes are adaptively determined. Further the proposed method also provides class relation based on the edge wise relevance. The paper is easy to follow and the idea would be reasonable. Importance of the propagation layer than the non linear layer is interesting, and I think it is worth showing. Variance of results of AGNN is comparable or even smaller than GLN. This is a bit surprising because AGNN would be more complicated computation than GLN. Interpretation of Figure 2 is not clear.<BRK>The Algorithm is inspired from Graph Neural Networks and more precisely graph convolutional NNs recently proposed by ref (Kipf et al  2016)) in the paper. The authors propose a model with simplified projection layers and more sophisticated diffusion ones, incorporating a simple attention mechanism. Comparisons with published results on the same datasets are presented. One finding is that simple models perform as well as more complex ones in this setting where labeled data is scarce. The experiments show that the proposed model is state of the art for graph node classification. The performance is on par with some other recent models according to table 2. Overall this is an interesting paper with nice findings.
Accept (Poster). rating score: 9. rating score: 8. rating score: 8. <BRK>In this paper, the authors propose deep architecture that preserves mutual information between the input and the hidden representation and show that the loss of information can only occur at the final layer. They illustrate empirically that the loss of information can be avoided on large scale classification such as ImageNet and propose to build an invertible deep network that is capable of retaining the information of the input signal through all the layers of the network until the last layer where the input could be reconstructed. The obtained result is competitive with the original Resnet and the RevNet models. This is called “information bottleneck principle”. The proposed approach is potentially of great benefit. It is also simple and easy to understand. Indeed, the authors have succeed in showing that this is not necessarily the case.<BRK>The main contribution is to propose a variant of the RevNet architecture that has a built in pseudo inverse, allowing for easy inversion. The main contribution is to use linear and invertible operators (pixel shuffle) for performing downsampling, instead of non invertible variants like spatial pooling. The authors evaluate in Section 4.2 the show samples obtained by the pseudo inverse and study the properties of the representations learned by the model. I find this section really interesting. Further analysis will make the paper stronger. I assume that the network evaluated with the Basel Faces dataset, is the same one trained on Imagenet, is that the case? It would be very good to empirically evaluate this claim.<BRK>ICLR I RevnetThis paper build on top of ReVNets (Gomez et al., 2017)  and introduce a variant that is fully invertible. It is indeed very interesting an thought provoking to see that contrary to popular belief in the community no information loss is necessary to learn good generalizable features. What is missing, is more motivation for why such a property is desirable. And the study of the property of the learned futures might probably limited to this i RevNet only.
Accept (Poster). rating score: 8. rating score: 6. rating score: 5. <BRK>Previous approaches have focused on transferring trained deep networks to a sparse graph for fast or efficient utilization; using this approach, sparse networks can be trained efficiently online, allowing for fast and flexible learning. Perhaps the heuristics for initializing the connectivity matrices will be insufficient, but could these be improved in further work?<BRK>This is a big advantage when training is performed on hardware with computational limitations, in comparison to "post hoc" sparsification methods, that compress the network after training. Readibility could be slightly increased by putting the figures on the respective pages. References to previous work in this area are missing, e.g.[1] de Freitas et al., Sequential Monte Carlo Methods to Train Neural NetworkModels, Neural Computation 2000[2] Welling et al., Bayesian Learning via Stochastic Gradient Langevin Dynamics, ICML 2011Especially the stochastic gradient method in [2] is strongly related to the existing approach.<BRK>Second, the relationship to existing work needs to be explained better. Pro:The algorithm is clearly explained, well motivated, and empirically supported. [1] Welling, M. and Teh, Y.W., 2011. Bayesian learning via stochastic gradient Langevin dynamics. Stochastic gradient Riemannian Langevin dynamics on the probability simplex.
Invite to Workshop Track. rating score: 8. rating score: 4. rating score: 3. <BRK>The authors propose a scheme to generate questions based on some answer sentences, topics and question types. Topics are extracted from questions using similar words in question answer pairs. Experimental results on question generation are convincing and clearly indicate that the approach is effective to generate relevant and well structured short questions. The main weakness of the paper is the selected set of question types that seems to be a fuzzy combination of answer types and question types (for ex.yes/no).Some questions type can be highly ambiguous; for instance “What” might lead to a definition, a quantity, some named entities... I would also suggest, for your next experiments, that you try to generate questions leading to answers with list of values.<BRK>The authors performed the experiment on AQAD dataset, and show their performance achieve state of the art result when using automatically generated topic, and perform better when using the ground truth topic. [Strenghts]This paper introduced a topic based question generation model, which generate question conditioned on the topic and question type. The proposed model can generate question with respect to different topic and pre decode seems a useful trick. [Weaknesses]This paper proposed an interesting and intuitive question generation model. But it also leads to different answers. If we only consider the automatically generated topic, the performance of the proposed model is similar to the previous method (Du et al). It is true that the proposed method may have better compositionality, but I didn t see any **theoretical** explantation about this. 4: The automatically extracted topic can be very noisy, but the paper didn t mention any of the extracted topics on AQAD dataset. However, as I pointed out above, there are several weaknesses in the paper.<BRK>Experiments and evaluation have been conducted on the AQAD corpus to show the effectiveness of the approach. Although the main contributions are clear, the paper contains numerous typos, grammatical errors, incomplete sentences, and a lot of discrepancies between text, notations, and figures making it ambiguous and difficult to follow. Overall, a misconception about topic vs. keywords might have led the authors to claim that their work is the first to generate topic specific questions whereas this has been studied before by Chali & Hasan (2015) in a non neural setting. "Topic" in general has a broader meaning, I would suggest authors to see this to get an idea about what topic entails to in a conversational setting: https://developer.amazon.com/alexaprize/contest rules . Figure 3 and the associated descriptions are very hard to follow. My major concern is with the experiments and evaluation. The dataset essentially contains questions about product reviews and does not match authors motivation/observation about real world conversations. Moreover, evaluation has been conducted on a very small test set (just about 1% of the selected corpus), making the results unconvincing.
Reject. rating score: 4. rating score: 7. rating score: 7. <BRK>Dialogue acts (DAs; or some other semantic relations between utterances) are informative to increase the diversity of response generation. It is interesting to see how DAs are used for conversational modeling, however this paper is difficult for me to follow. Not sure whether it is necessary to formulate the problem with a RL framework, since the data have everything that the model needs as for a supervised learning. 2) the formulation in equation 4 seems to be problematic3) "simplify pr(ri|si,ai) as pr(ri|ai,ui−1,ui−2) since decoding natural language responses from long conversation history is challenging" to my understanding, the only difference between the original and simplified model is the encoder part not the decoder part. Did I miss something? At least, it should show the generation texts were affected about DAs in a systemic way.<BRK>In their evaluations, this approach, and one that additionally uses policy gradient RL with discourse level objectives to fine tune the dialogue act predictions, outperform past models for human scored response quality and conversation engagement. While this is a fairly straightforward idea with a long history, the authors claim to be the first to use dialogue act prediction for open domain (rather than task driven) dialogue. A more comprehensive search strategy (e.g.selecting the top K dialogue acts, then evaluating several responses for each DA) might lead to higher response diversity. If these choices (like the dialogue termination conditions) are motivated by previous work, they should be cited. 8.A consistent focus on human evaluation, as found in this paper, is probably the right approach for contemporary dialogue research.<BRK>The paper describes a technique to incorporate dialog acts into neural conversational agents. I do not have any suggestion for improvement. This is good work that should be published. The paper also describes how to increase the relevance of responses and the length of conversations by self reinforcement learning.
Reject. rating score: 4. rating score: 4. rating score: 4. <BRK>This paper considers hyperparameter searches in which all of thecandidate points are selected in advance. The most common approachesare uniform random search and grid search, but more recentlylow discrepancy sequences have sometimes been used to try to achievebetter coverage of the space. This paper proposes using a variant ofthe determinantal point process, the k DPP to select these points. The idea is that the DPP provides an alternative form of diversity tolow discrepancy sequences. Some issues I have with this paper:1. Why not use any of the other(potentially cheaper) repulsive point processes that also achievediversity? Statisticians have thought about this for along time. Given that the discrete settingwas the motivation for the DPP over LDS, it seems strange not to evenlook at that case. 5.Why no low discrepancy sequence in the experimental evaluation ofsection 5?<BRK>The authors propose k DPP as an open loop (oblivious to the evaluation of configurations) method for hyperparameter optimization and provide its empirical study and comparison with other methods such as grid search, uniform random search, low discrepancy Sobol sequences, BO TPE (Bayesian optimization using tree structured Parzen estimator) by Bergstra et al.(2011).The k DPP sampling algorithm and the concept of k DPP RBF over hyperparameters are not new, so the main contribution here is the empirical study. Second, their study only applies to a small number like 3 6 hyperparameters with a small k 20. The real challenge lies in scaling up to many hyperparameters or even k DPP sampling for larger k. Third, the authors do not compare against some relevant, recent work, e.g., Springenberg et al.(http://aad.informatik.uni freiburg.de/papers/16 NIPS BOHamiANN.pdf) and Snoek et al.(https://arxiv.org/pdf/1502.05700.pdf) that is essential for this kind of empirical study.<BRK>In this paper, the authors consider non sequential (in the sense that many hyperparameter evaluations are done simultaneously) and uninformed (in the sense that the hyperparameter evaluations are chosen independent of the validation errors observed) hyperparameter search using determinantal point processes (DPPs). Under the RBF kernel, the more diverse a set of vectors is, the closer the kernel matrix becomes to the identity matrix, and thus the larger the determinant (and therefore probability under the DPP) grows. The authors propose to do hyperparameter tuning by sampling a set of hyperparameter evaluations from a DPP with the RBF kernel. Overall, I have a couple of concerns about novelty as well as the experimental evaluation for the authors to address. Ultimately, my concern is that considering these tools are open source and relatively stable software at this point if DPP only based hyperparameter optimization is truly better than the parallelization approach of SMAC, it should be straightforward enough to download SMAC and demonstrate this.
Invite to Workshop Track. rating score: 6. rating score: 4. rating score: 4. <BRK>Summary:The paper studies the problem of learning distributions with disconnected support. The paper is very well written, and the analysis is mostly correct, with some important exceptions. I would like to point a lot of tunneling issues can be seen and studied in toy datasets. The authors did a great job at addressing some of the concerns. There are a number of claims in the paper that are not supported by experiments, citations, or a theorem.<BRK>This paper concerns a potentially serious issue with current GAN based approaches. However standard GANs use continuous noise and generators and must therefore output a connected distribution over inputs. The only experimental results presented are qualitative analysis of samples by the authors. The proposed CelebRoom dataset (a 50/50 mixture of celebA and LSUN bedrooms) is a good dataset to validate the problem on but it is disappointing that the authors do not actually scale their method to their motivating example. However the paper lacks quantitative evidence for both the importance of the problem and demonstrating the proposed solution.<BRK>The authors propose to train multiple generators (with same set of parameters), each of which with a different linear mapping in the first layer. The idea is that the final output of the generator should be a distribution whose support are disconnected. The idea does look interesting. But a lot of details is missing and needs clarification. 2) The experiments are not convincing. But there is no evidence that this is actually due to the proposed idea.
Invite to Workshop Track. rating score: 6. rating score: 6. rating score: 6. rating score: 6. <BRK>This work presents a bound to learn from multiple source domains for domain adaptation using adversarial learning. This is a simple extension to the previous work based on a single source domain. The bound used in the paper accounts for the worst case scenario, which may not be a tight bound when some of the source domains are very different from the target domain. However, given that the best single results are close to the target only results, SVHN should be similar to one or more of the source domains.<BRK>The generalization bounds proposed in this paper is an extension of Blitzer et al.2007.The previous bounds was proposed for single domain single target setting, and this paper extends it to multiple source domain setting. The proposed bound is presented in Theorem 3.4, showing some interesting observations, such as the performance on the target domain depends on the worst empirical error among multiple source domains. Pros + The proposed bound is of some interest. + The bound leads to an efficient learning strategy using adversarial neural networks. Cons:  My major concern is that the baselines evaluated in the experiments are quite limited. There are other publications working on the multi source domain setting, which were not mentioned/compared in the submission.<BRK>Whereas Ganin et al.(2016) were building directly on the (single source) domain adaptation theorem of Ben David et al., the authors prove a similar result for the multiple sources case. This succeeds in convincing me that the proposed approach is of interest. This appears to me to be the weakest claim of the paper, since it is not backed by an empirical or a theoretical study. Based on a theoretical study of the multi source domain adaptation problem. Cons:  The soft max version of the algorithm   obtaining the best empirical study   is not backed by the theory. It is not obvious that the theoretical study and the proposed algorithm is actually the right thing to do.<BRK>Quality:The paper appears to be correct. Clarity:The paper is very clearOriginality:The theoretical contribution extends the seminal work of Ben David et al., the idea of using adversarial learning is not new, the novelty is mediaumSignificance:The theoretical analysis is interested but for me limited, the idea of the algorithm is not new but as far as I know the first explicitly presented for multi source. Pros: new theoretical analysis for multisource problem paper clear smoothed version is interestingCons Learning bounds with worst case standpoint is probably not the best analysis for multisource learning experimental evaluation limited in the sense that similar algorithms in the literature are not compared Extension a bit direct from the seminal work of Ben David et al.Summary:This paper presents a multiple source domain adaptation approach based on adversarial learning. The setting considered contains multiple source domains with labeled instances and one target domain with unlabeled instances. After rebuttal The new results and experimental evaluation have improved the paper. Maybe, the authors could propose a learning bound that correspond to the smoothed version proposed in the paper and that works best.
Reject. rating score: 4. rating score: 4. rating score: 6. <BRK>The major drawback is that none of these ideas are fully explored. The second major drawback is that the experimental setting seems very unrealistic: 5 base classes and 2 novel classes. To conclude: the ideas in this paper are very interesting, but difficult to gather insights given the focus of the experiments. Minor remarks  Sect 4.1 "The randomly ... 5 novel classes" is not a correct sentence.<BRK>The paper proposes a method for adapting a pre trained network, trained on a fixed number ofclasses, to incorporate novel classes for doing classification, especially when the novel classesonly have a few training examples available. Having only O(10) classes is not convincing, especially when the datasets used do have large  number of classes. This ensures that, in the new expanded andfine tuned network, the class confusions will only be between the old and new classes and notbetween the old classes, thus avoiding catastrophic forgetting. They show experiments on public benchmarks with three different scenarios, i.e.base and novelclasses from different domains, base and novel classes from the same domain and novel classes havesimilarities among themselves, and base and novel classes from the same domain and each novel classhas similarities with at least one of the base class.<BRK>On few shot learning problem, this paper presents a simple yet powerful distillation method where the base network is augmented with additional weights to classify the novel classes, while keeping the weights of the base network unchanged. This is slightly undesired. The good points are as follows,1. There still is drop in accuracy on the base classes after adding new classes, and the accuracy may still drop as adding more classes due to the fixed parameters corresponding to the base classes.
Invite to Workshop Track. rating score: 8. rating score: 4. rating score: 4. <BRK>The paper studied the generalization ability of learning algorithms from the robustness viewpoint in a deep learning context. Pros: 1, The problem studied in this paper is interesting. 2, The paper is well shaped and is easy to follow.<BRK>This paper proposes a study of the generalization ability of deep learning algorithms using an extension of notion of stability called ensemble robustness. The paper then gives bounds on the generalization error of a randomized algorithm in terms of stability parameter and provides empirical study attempting to connect theory with practice. 2.In intro, "Thus statistical learning theory ... struggle to explain generalization ...".<BRK>This is a clear weak aspect of the experimental analysisIn the experimental setup, as far as I understand the setup, I find the term "generalization error" a bit abusive since it is actually the error on the test set. The paper presents learning bounds and an experimental showing correlation between empirical ensemble robustness and generalization error.
Reject. rating score: 5. rating score: 6. rating score: 8. <BRK>The contribution seems to mix two objectives: on one hand to prove that it is possible to do data augmentation for fMRI brain decoding, on the other hand to design (or better to extend) a new model (to be more precise two models). Concerning the first objective the empirical results do not provide meaningful support that the generative model is really effective. This analysis is missing a straw man. From the perspective of neuroscience a reader,  would expect to look at the brain maps for the same collection with different methods. The pairwise brain maps would support the interpretation of the generated data.<BRK>This paper proposes to use 3D conditional GAN models to generatefMRI scans. One claim of the paper is that a generative model of fMRIdata can help to caracterize and understand variability of scansacross subjects. Generating high resolution images with GANs even on faces for whichthere is almost infinite data is still a challenge. Here a few thousanddata points are used. So it raises too concerns: First is it enough?<BRK>QualityThis is a very clear contribution which elegantly demonstrates the use of extensions of GAN variants in the context of neuroimaging. Methods and results are clearly described. The authors state significant improvements in classification using generated data. These claims should be substantiated with significance tests comparing classification on standard versus augmented datasets. OriginalityThis is one of the first uses of GANs in the context of neuroimaging. Significance The approach outlined in this paper may spawn a new research direction.
Accept (Poster). rating score: 7. rating score: 7. rating score: 3. <BRK>This paper builds on Zhang et al.(2016) (Understanding deep learning requires rethinking generalization). Firstly, it shows experimentally that the same effects appear even for simple models such as linear regression. It also shows that the phenomenon that sharp minima lead to worse result can be explained by Bayesian evidence. With both theoretical and experimental analysis, it suggests the optimal batch size given learning rate and training data size. The paper is well written and provides excellent insights. Pros:1.Very well written paper with good theoretical and experimental analysis. 2.It provides useful insights of model behaviors which are attractive to a large group of people in the community. One important contribution of the paper is about optimal batch sizes, but related work in this direction is not discussed. There are many related works concerning adaptive batch sizes, such as [1] (a summary in section 3.2 of [2]). "Advances in Variational Inference." ————— Update: I lowered my rating considering other ppl s review and comments.<BRK>Summary:This paper presents a very interesting perspective on why deep neural networks may generalize well, in spite of their high capacity (Zhang et al, 2017). It would be good if it were made clearer which part of the mathematical analysis in sections 2 and 5 are original. The paper goes on to show that if the evidence is approximated using a second order Taylor expansion of the cost function around a minimia $w_0$, then the evidence is controlled by the cost at the minimum, and by the logarithm of the ratio of the curvature at the minimum compared to the regularization constant (eg, standard deviation of gaussian prior). Thus, Bayesian evidence prefers minima that are both deep and broad. This could help parallelize neural network training considerably. Review:Quality: The quality of the work is high. Experiments and analysis are both presented clearly. It seems to me that the model trained on meaningful data should have a larger margin. 3) The connection between the work on Bayesian evidence, and the work on SGD, felt very informal. There is a footnote on page 7 regarding Bayesian posterior sampling   I think this should be brought into the body of the paper, and explained in more detail. Are there any theoretical results which can be leveraged from the stochastic processes literature? Does the generalization gap not appear when no momentum is used? Also, why is the same learning rate used for both small and large batch training for Figures 3 and 4?<BRK>The paper takes a recent paper of Zhang et al 2016 as the starting point to investigate the generalization capabilities of models trained by stochastic gradient descent. The main contribution are scaling rules that relate the batch size k used in SGD with the learning rate \epsilon, most notably \epsilon/k   const for optimal scaling. Maybe there is something deeper going on here, but it is not obvious to me. Chapter 2 provides a sort of a mini tutorial to (Bayesian) model selection based on standard Bayes factors. This, in my opinion, is a biased and limited starting point, which ignores much of the literature in learning theory. I find this of limited usefulness. (vi) The argument why B ~ N is not clear to me. (iii) In which way is a Gaussian prior uncorrelated, if there is just a scalar random variable? I feel the discussion to be too much obsessed by the claims made in Zhang et al 2016 and in no way suprising. Chapter 4 takes the work of Mandt et al as a starting point to understand how SGD with constant step size effectively can be thought of as gradient descent with noise, the amplitude of which is controlled by the step size and the mini batch size. Here, the main goal is to use evidence based arguments to distinguish good from poor local minima. Chapter 5 takes a stochastic differential equation as a starting point. So maybe one should not feature the term SDE so prominently. (ii) While it is commonly done, it would be nice to get some insights on why a Gaussian approx.
Reject. rating score: 4. rating score: 4. rating score: 4. <BRK>Our main concern is regarding thorough comparison to similar work and provision of comparative work assessment to support novelty claims. The paper presents a well laid research methodology that shows a good decomposition of the problem at hand and the approach foreseen to solve it. The results obtained (as compared to FHEW) seem to indicate well thought off decisions taken to optimize the different gates  operations as clearly explained in the paper.<BRK>3.I appreciate that optimizations such as low precision and point wise convolution are discussed in this paper. I recommend the authors to tone down their claims. For example, the authors mentioned that "there has been no complete implementation of established deep learning approaches" in the abstract, however, the authors did not define what is "complete".<BRK>There is also a work in this area that the authors do not cite or contrast to, bringing the novelty into question; please see the following papers and references therein:GILAD BACHRACH, R., DOWLIN, N., LAINE, K., LAUTER, K., NAEHRIG, M., AND WERNSING, J. Cryptonets: Applying neural networks to encrypted data with high throughput and accuracy. It is unclear what operations are referred to here.
Reject. rating score: 3. rating score: 5. rating score: 6. <BRK>Summary: The paper proposes a multi task feature learning framework with a focus on avoiding negative transfer. Strength: The method has some contribution in dealing with negative transfer. The experimental results are positive. Overall conceptual issues. The difference between MT NN and AMTFL is not significant. The performance boost is more likely due to using NNs rather than the proposed MTL module. 1.2  Very unclear intuition of the algorithm.<BRK>The paper is not clearly written, so I outline my interpretation on what is the main idea of the manuscript. The interpretation is that negative transfer is avoided because only subset of relevant tasks is considered for transfer. Subspace learning could help to scale up to many tasks. The empirical evaluations are not convincing. Conceptual picture is a bit lacking.<BRK>This paper presents a deep asymmetric multi task feature learning method (Deep AMTFL). One concern is that the high similarity between the proposed Deep AMTFL and an existing AMTL method.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>This paper describes an approach to decode non autoregressively for neural machine translation (and other tasks that can be solved via seq2seq models). The disadvantage is that it is more complicated than a standard beam search as auto regressive teacher models are needed for training and the results do not reach (yet) the same BLEU scores as standard beam search. Overall, this is an interesting paper.<BRK>This work proposes non autoregressive decoder for the encoder decoder framework in which the decision of generating a word does not depends on the prior decision of generated words. What is the impact of the external word aligner quality? The positional attention is rather unclear and it would be better to revise it. Experiments are carried out very carefully.<BRK>The goal here is to make the target sentence generation non auto regressive. The idea is interesting and trendy. The training process looks highly elaborate with a lot of hyper parameters. Maybe you could comment on this. How you use IBM model for supervision.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. <BRK>Paper proposes a shallow model for approximating stacks of Resnet layers, based on mathematical approximations to the Resnet equations and experimental insights, and uses this technique to train Resnet like models in half the time on CIFAR 10 and CIFAR 100. While the experiments are not particularly impressive, I liked the originality of this paper.<BRK>This expression is used to develop a new layer called a “warp layer” which essentially tries to compute several layers of the residual network using the Taylor expansion expression — however in this expression, things can be done in parallel, and interestingly, the authors show that the gradients also decouple when the (ResNet) model is close to a local minimum in a certain sense, which may motivate the decoupling of layers to begin with. Finally the authors stack these warp layers to create a “warped resnet” which they show does about as well as an ordinary ResNet but has better parallelization properties. To me the analytical parts of the paper are the most interesting, particularly in showing how the gradients approximately decouple. Throughout, the authors write out equations as if the weights in all layers are equal, but this is confusing even if the authors say that this is what they are doing, since their explanation is not very clear. The confusion is particularly acute in places where derivatives are taken, because the derivatives continue to be taken as if the weights were untied, but then written as if they happened to be the same. * The relative speed up of WarpNet compared to ResNet needs to be better explained — the authors break the computation of the WarpNet onto two GPUs, but it’s not clear if they do this for the (vanilla) ResNet as well. * In the experimental results, K is set to be 2, and the authors only mention in passing that they have tried larger K in the conclusion. I wonder what are the differences between the two Taylor expansions, and whether the insights of this paper could be used to help the other paper and vice versa?<BRK>Motivated via Talor approximation of the Residual network on a local minima, this paper proposed a warp operator that can replace a block of a consecutive number of residual layers. As demonstrated in the paper, this improves the training time with multi GPU parallelization, while maintaining similar performance on CIFAR 10 and CIFAR 100. One thing that is currently not very clear to me is about the rotational symmetry.
Invite to Workshop Track. rating score: 7. rating score: 6. rating score: 5. rating score: 4. <BRK>This paper analyzes the expressiveness and loss surface of deep CNN. I think the paper is clearly written, and has some interesting insights.<BRK>At the end of the day, if a friend asked me to summarize the paper, I would tell them :"Features are basically full rank. It simplifies the statement of some of their results. This sentence is strange to read, but I can understand what the authors mean. The authors are using it correctly in the rest of the paper, but in this sentence I think they simply mean that something holds if "all" the bottom layers have random features.<BRK>Under this and some additional technique conditions, the paper shows that this layer can extract linearly independent features and all critical points are local minimums. I like the presentation and writing of this paper. The mathematical intuition is that the severe overfitting induced by the wide layer essentially lifts the loss surface to be extremely flat so training to zero/small error becomes easy.<BRK>If an optimization algorithm falls onto these solutions, it will be hard to escape. (2) If the wide layer is at the top of all hidden layers, then the neural network can perfectly fit the training data. I would consider result (1) as the main result of this paper, because (2) is a direct consequence of (1). Intuitively, (1) is an easy result.
Accept (Poster). rating score: 7. rating score: 7. rating score: 5. <BRK>original review  Thank you for an interesting read. Approximate inference with implicit distribution has been a recent focus of the research since late 2016. This paper provides a good start in this direction. There are other ways to do implicit posterior inference such as amortising deterministic/stochastic dynamics, and approximating the gradient updates of VI. Please check the literature.<BRK>The paper presents experiments on a variety of scenarios to show the performance of KIVI. The closed form solution involves a matrix inversion, but this shouldn t be an issue, as the matrix size is controlled by the number of samples, which is a parameter that the practitioner can choose. The experiments seem convincing too, although I believe the paper could probably be improved by comparing with other implicit VI methods, such as [Liu & Feng], [Tran et al.], or others. Section 3: The first paragraph of the KIVI section is also unclear to me. "are nor applicable". Additionally, Figure 3(a) (and the explanation in the text) was unclear to me.<BRK>Update: I read the other reviews and the authors  rebuttal. Thanks to the authors for clarifying some details. I m still against the paper being accepted. The work has interesting ideas. Unfortunately, I m not convinced that the method overcomes these difficulties as they argue in Sec 3.2. This is especially the case given a limited number of samples from both p and q (which is the same problem as previous methods) as well as the RBF kernel. I also suggest the authors perform the experiment suggested above with HMC as ground truth on a non toy problem such as a fairly large Bayesian neural net.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. <BRK>Clarity: The paper has a good flow, starting out with the theoretical foundation, description of how to construct the network, followed by the probabilistic formulation. Additionally, it would have been nice to see a figure of the entire network architecture, at least for one of the applications considered in the paper. Adams and Zemel also seems to propose Sinkhorn operator for neural network. Although they focus only on the document ranking problem, it would be good to hear the authors  view on what differentiates their work from Adams and Zemel. The methodology appears to be straight forward  to implement using the existing software libraries, which should help increase its usability.<BRK>The third empirical study on the C. elegans neural inference problem shows significant improvement over Linderman et al.(arxiv 2017). The authors propose a new method that approximates the discrete max weight matching by a continuous Sinkhorn operator, which looks like an analog of softmax operator on matrices. I am not an expert in this line of research, so I hope other reviewers can more thoroughly examine the heuristics discussed by the authors in Section 5.4 and Appendix C.3 to get around the intractable sub problems in their approach. This is still not sufficient as Gumbel matching and Gumbel Sinkhorn distributions have intractable densities.<BRK>The idea on which the paper is based   that the limit of the entropic regularisation over Birkhoff polytope is on the vertices   permutation matrices  , and the link with optimal transport, is very interesting. The core of the paper, Section 3, is interesting and represents a valuable contribution. I think I got it but it is better to be written down)There should also be better metrics for bigger jigsaws   for example, I would accept bigger errors if pieces that are close in the solution tend also to be put close in the err ed solution.
Accept (Oral). rating score: 9. rating score: 8. rating score: 7. <BRK>First off, this paper was a delight to read. The authors develop an (actually) novel scheme for representing spherical data from the ground up, and test it on three wildly different empirical tasks: Spherical MNIST, 3D object recognition, and atomization energies from molecular geometries. The paper was also exceptionally clear and well written. This can sometimes be an apples to oranges comparison, but it s nice to fully contextualize the comparative advantage of this new scheme over others. That is, does it perform as well and train just as fast?<BRK>The goal is to detect patterns in spherical signals irrespective of how they are rotated on the sphere. On the other hand, was there an attempt to add additional layers to the proposed approach for the shape recognition experiment in Sec.5.3 to improve performance? This naming of the proposed approach should be made clear earlier in the manuscript. As an aside, this appears a little confusing since convolution is performed first on S^2 and then SO(3).<BRK>To get invariance on the sphere (S^2), the idea is to consider the group of rotations on S^2 [SO(3)] and spherical convolution [Eq.(4)].To be able to compute this convolution efficiently, a generalized Fourier theorem is useful. The validity of the idea is illustrated on 3D shape recognition and atomization energy prediction. The paper is nicely organized and clearly written; it fits to the focus of ICLR and can be applicable on many other domains as well.
Accept (Poster). rating score: 7. rating score: 6. rating score: 4. <BRK>The paper proposes to learn a custom translation or rotation invariant kernel in the Fourier representation to maximize the margin of SVM. Instead of using Monte Carlo approximation as in the traditional random features literature, the main point of the paper is to learn these Fourier features in a min max sense. As for the related work, it seems the authors have missed some very relevant pieces of work in learning these Fourier features through gradient descent [1, 2]. It would be interesting to compare these algorithms as well. A la Carte — Learning Fast Kernels.<BRK>However, there are several issues need to be addressed. 3, Although the proof of the algorithm relies on the online learning regret bound, the algorithm itself requires visit all the data in each update, and thus, it is not suitable for online learning. Please clarify this in the paper explicitly. 4, The experiment is weak. Meanwhile, Since the proposed algorithm requires extra optimization w.r.t.random feature, it is more convincing to include the empirical runtime comparison. Suggestion: it will be better if the author discusses some other model besides l1 SVM with such kernel learning.<BRK>They choose the alignment of the kernel to data as the objective function to optimize. My problem with that paper is that even though at first glance learning adaptive feature maps seems to be an attractive approach, authors  contribution is actually very little. But in fact later they admit that the problem of optimizing the alignment is a non convex problem and the authors end up with a couple of heuristics to deal with it. Another problem is that it is not clear at all to me how authors  approach can be extended to non shift invariant kernels that do not benefit from Bochner s Theorem.
Reject. rating score: 3. rating score: 4. rating score: 5. <BRK>Although the idea of applying deep learning for temporal clustering is novel and interesting, the optimization problem is not clearly stated and experiments section is not comprehensive enough. Is that a difference? Even for the longer ones, DTC does dimensionality reduction to make the time series shorter. In summary, the method need to be described clearer, state of the arts need to be compared and the usability of the method needs to be discussed. Therefore, at the current stage the paper cannot be accepted in my opinion.<BRK>Though several datasets are evaluated in experiments, they are relatively small. A temporal clustering model and a DCNN decoder are applied on the encoded representations and jointly trained. Minor suggestion: In Figure 3, instead of showing the decoded output (reconstruction), it may be more helpful to visualize the encoded time series since the clustering method is applied directly on those encoded representations.<BRK>This paper proposes an algorithm for jointly performing dimensionality reduction and temporal clustering in a deep learning context. minor note: the dynamic time warping is formally not a metric Also, do the cluster centroids appear to be roughly stable over many runs of the algorithm?
Accept (Poster). rating score: 6. rating score: 6. rating score: 5. <BRK>The main strength of this paper, I think, is the theoretical result in Theorem 1. It would be really nice to complete the analysis of Thm1 with a formal analysis of convergence speed for ||what w*|| as lambda and k are scheduled in this fashion. I wish the authors actually concluded with the following minor improvement to the proof that actually strengthens the result further.<BRK>This paper introduces MiniMax Curriculum learning, as an approach for adaptively train models by providing it different subsets of data. The authors formulate the problem as an iterative technique which involves solving a minimax objective at every iteration. Pros:  The analysis of the minimax objective is novel and the proof technique introduces several interesting ideas. Cons:  The main algorithm MCL is only a hueristic. Though the MiniMax subproblem can converge, the authors use this in somewhat of a hueristic manner. It seems to me the authors have experimented with smaller datasets (CIFAR, MNIST, 20NewsGroups). Overall, I would like to see if the paper could have been stronger empirically. Nevertheless, I do think there are some interesting ideas theoretically and algorithmically.<BRK>The proposed method is based on a submodular set function over the examples, which is intended to capture diversity of the included examples and is added to the training objective (eq.2).The set is optimized to be as hard as possible (maximize loss), which results in a min max problem. This is in turn optimized (approximately) by alternating between gradient based loss minimization and submodular maximization. The paper is mostly clear and the idea seems nice. On the downside, there are some limitations to the theoretical analysis and optimization scheme (see comments below).
Reject. rating score: 3. rating score: 5. rating score: 5. <BRK>Again, there are many ways that this could be done. Most of the experiments in the paper seem a bit contrived. I find this misleading.<BRK>The term "structural equivalence" is used incorrectly in the paper. Experiments were not conducted to see how the competing approaches such as RolX compare with GraphWave on transfer learning tasks. I strongly recommend running experiments that test the predictive power of the roles found by GraphWave.<BRK>The paper derived a way to compare nodes in graph based on wavelet analysis of graph laplacian.
Reject. rating score: 6. rating score: 6. rating score: 7. <BRK>The approach solves an important problem as getting labelled data is hard. The focus is on the key aspect, which is generalisation across heteregeneous data. The novel idea is the dataset embedding so that their RL policy can be trained to work across diverse datasets. It is unclear if the strategy is general enough to work on harder problems / larger datasets, or with higher dimensional data like images. More evidence in the paper for why it would work on harder problems would be great. Significance/Conclusion: The idea of meta learning or learning to learn is fairly common now. While they do show good performance, it’s unclear if the specific embedding strategy suggested in this paper will generalise to harder tasks. I agree the computational budget makes sense for cross data transfer, however the embedding strategy and lack of larger experiments makes it unclear if it ll generalise to harder tasks. I update my review to 6.<BRK>They use meta learning on feature histograms to embed heterogeneous datasets into a fixed dimensional representation. The experiments show the proposed approach is effective on 14 UCI datasets. strength* The paper is mostly clear and easy to follow. * The overall idea is interesting and has many potentials. * The experimental results are promising on multiple datasets. * The motivation of using feature histograms as embedding is not clear. The term "posterior value" sounds ambiguous. * The experiment sets a fixed budget of only 20 instances, which seems to be rather few in some active learning scenarios, especially for non linear learners. * The term A(Z) in the objective function can be more clearly described. There is also no evidence on whether adaptive learning on the fly is needed or not.<BRK>This reviewer has found the proposed approach quite compelling, but the empirical validation requires significant improvements:1) you should include in your comparison Query by  Bagging & Boosting, which are two of the best out of the box active learning strategies2) in your empirical validation you have (arbitrarily) split the 14 datasets in 7 training and testing ones, but many questions are still unanswered:    would any 7 7 split work just as well (ie, cross validate over the 14 domains)   do you what happens if you train on 1, 2, 3, 8, 10, or 13 domains? are the results significantly different?
Accept (Poster). rating score: 7. rating score: 6. rating score: 2. <BRK>This paper proposes a novel regularization scheme for Wasserstein GAN based on a relaxation of the constraints on the Lipschitz constant of 1. The paper is interesting and well written, the proposed regularization makes sens since it is basically a relaxation of the constraints and the numerical experiments also suggest it s a good idea. The reviewer believes that this space could be better spend discussing the theoretical implication of the proposed regularization (see next). A discussion of the links is necessary and will clearly bring more theoretical ground to the method. It would be nice to see the behavior for different values of lambda. I change the rating to Accept.<BRK>The article deals with regularization/penalization in the fitting of GANs, when based on a L_1 Wasserstein metric. The approach is illustrated by numerical experiments. More importantly, The heuristic proposed in the paper is interesting and promising in some respects but there is a real lack of theoretical guarantees motivating the penalty form chosen, such a theoretical development could allow to understand what may rule the choice of an ideal value for lambda in particular.<BRK>Overall the paper is too vague on the mathematical part, and the experiments provided are not particularly convincing in assessing the benefit of the new penalty. d is used as a discriminator and then as a distance. This should be improved. please can you describe the distributions in a more precise way? page 5:  the examples are hard to understand. It could be also interesting to (geometrically) interpret the coupling proposed.
Accept (Poster). rating score: 7. rating score: 6. rating score: 5. <BRK>The paper presents a new algorithm for inference based reinforcement learning for deep RL. The algorithm is similar to other inference based RL algorithm, but is the first application of inference based RL to deep reinforcement learning.<BRK>This is an interesting policy as inference approach, presented in a reasonably clear and well motivated way. Is this guaranteed to be Gaussian?<BRK>This paper studies new off policy policy optimization algorithm using relative entropy objective and use EM algorithm to solve it. The general idea is not new, aka, formulating the MDP problem as a probabilistic inference problem. Overall, although the motivation of this paper is interesting, I think there is still a lot of details to improve.
Reject. rating score: 3. rating score: 4. rating score: 4. <BRK>Major commentsFirst, as mentioned above, this paper leans heavily on existing work. The references are inconsistently and incorrectly formatted (e.g., “Bayesian” should be capitalized). e.g., which layer is removed?<BRK>Finally the writing of the paper could be highly improved. The experiment compares with a weak baseline and a baseline that is unknown to me.<BRK>I cannot see why there should be a guarantee of any sort. Section 4.2: I am surprised that there is only a comparison to TPOT, not one to Auto sklearn. Does TPOT overfit on these?
Reject. rating score: 5. rating score: 6. rating score: 7. <BRK>This paper propose a variant of generative replay buffer/memory to overcome catastrophic forgetting. They use multiple copy of their model DGMN as short term memories and then consolidate their knowledge in a larger DGMN as a long term memory. The main novelty of this work are 1 balancing mechanism for the replay memory. Using the STTM during testing means essentially (number of STTM) + 1 models are used which is not same as preventing one network from catastrophic forgetting. As it is the closet method to this paper it is essential to be compared against.<BRK>This paper reports on a system for sequential learning of several supervised classification tasks in a challenging online regime. Unfortunately, the paper does not go beyond the relatively simplistic setup of sequential MNIST, in contrast to some of the methods used as baselines. ICLR 2016. This setting is not ideal, though.<BRK>This paper introduces a neural network architecture for continual learning. Overall, although the result are not very surprising, the approach is well justified and extensively tested. Comments:	1 	The results are somewhat unsurprising: as we are able to learn generative models of each tasks, we can use them to train on all tasks at the same time, a beat algorithms that do not use this replay approach.
Reject. rating score: 3. rating score: 4. rating score: 5. <BRK>The other two attacks require that the foe is inserted in the middle of the training of the VAE. Once it is train it will give away the decoder and keep the encoder for sending information. In which one person encodes an image, send the latent variable to a friend, but a foe intercepts it on the way and tampers with it so the receiver recovers the wrong image without knowing. Now if the sender believes the sample can be tampered with, if the sender codes z with his private key would not make the attack useless?<BRK>This paper is concerned with both security and machine learning. Assuming that data is encoded, transmited, and decoded using a VAE,the paper proposes a man in middle attack that alters the VAE encoding of the input data so that the decoded output will be misclassified. The objectives are to: 1) fool the autoencoder; the classification output of the autoencoder is different from the actual class of the input; 2) make minimal change in the middle so that the attack is not detectable. This is an unrealistic scenario. Without considering the state of the art security defending mechanism, it is difficult to judge the contribution of the paper to the security community.<BRK>The idea is clearly stated (but lacks some details) and I enjoyed reading the paper. In Section 3.1, the attack methods #2 and #3 should be detailed more. Also, the detailed specification of the VAE should be detailed. From figures in Figure 4 and Figure 5, I could see that the proposed scheme performs successfully in a qualitative manner, however, it is difficult to evaluate the proposed scheme qualitatively without comparisons with baselines. Also, can you experimentally show that attacks on latent variables are more powerful than attacks on inputs?
Reject. rating score: 4. rating score: 6. rating score: 7. <BRK>I thank the authors for the thoughtful response and rebuttal. The authors need to provide more justification for this motivation. The authors present an interesting new method for generating adversarial examples. The authors demonstrate that the network works well in the semi white box and black box settings. The authors wrote a clear paper with great references and clear descriptions. I am not aware of any use cases, but if there are some, the authors should describe the rationales at length in their paper. Previous methods, e.g.FGSM, may work on arbitrarily sized images. The best GAN models for generating high resolution images are  difficult to train and it is not clear if they would work in this setting.<BRK>This paper describes AdvGAN, a conditional GAN plus adversarial loss. The authors evaluate AdvGAN on semi white box and black box setting. AdvGAN is a simple and neat solution to for generating adversary samples. The author also reports state of art results. I am wondering this method is trying to mixture several samples into one to generate adversary samples. For real color samples, it is harder to figure out the mixture.<BRK>The paper proposes a way of generating adversarial examples that fool classification systems. They formulate it for a blackbox and a semi blackbox setting (semi being, needed for training their own network, but not to generate new samples). The paper is generally easy to understand and clear in their results. I am not awfully familiar with the literature on adversarial examples to know if other GAN variants exist. So this paper is innovative in two parts:  it applies GANs to adversarial example generation  the method is a simple feed forward network, so it is very fast to computeThe experiments are pretty robust, and they show that their method is better than the proposed baselines.
Invite to Workshop Track. rating score: 6. rating score: 5. rating score: 4. <BRK>The authors  claims could be validated if the optimal choices always use the same choice for the training and test time similarity functions. This makes the paper an empirical paper based on a reasonable and sensible intuition, rather than a theoretical result. If dot product (or cosine similarity) is going to be used as the similarity function for the representation, then it makes more sense, the paper argues, to use the decoder hidden state(s) as the representation of the input sentence. I think this is an interesting result. Cons: I have several concerns. It seems like the paper may want to start with formal definitions of an encoder and a decoder, then define what is meant by a "decoder that is log linear with respect to the encoder", and define what it means for a distance to be optimal with respect to a training objective.<BRK>They also provide a simple method for improving the performance of RNN based models. However, the experiments suggest that RNN mean always does better and in some cases significantly better (referring to Table 1). How does this empirical observation reconcile with the theory ? However, it is not clear how this causes it to be "closer" to the optimal space. 4) Can you elaborate a bit more on how the model is used at test time? How will you use RNN mean and RNN concat to find the similarity between these two sentences.<BRK>This paper proposes the concept of optimal representation space and suggests that a model should be evaluated in its optimal representation space to get good performance. It could be a good idea if this paper could suggest some ways to find the optimal representation space in general, instead of just showing two cases. It is disappointing, because this paper is named as "finding optimal representation spaces ...".
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. <BRK>The paper proposes a simple modification to conditional GANs, obtaining impressive results on both the quality and diversity of samples on ImageNet dataset. Instead of concatenating the condition vector y to the input image x or hidden layers of the discriminator D as in the literature, the authors propose to project the condition y onto a penultimate feature space V of D (by simply taking an inner product between y and V) . Appropriate comparison with existing conditional models: AC GANs and PPGNs.<BRK>This manuscript makes the case for a particular parameterization of conditional GANs, specifically how to add conditioning information into the network. It motivates the method by examining the form of the log density ratio in the continuous and discrete cases. This paper s empirical work is quite strong, bringing to bare nearly all of the established tools we currently have for evaluating implicit image models (MS SSIM, FID, Inception scores).<BRK>This model employs a  projection discriminator  in order to incorporate image labels and demonstrate that the resulting model outperforms state of the art GAN models. The AC GAN work performed an analysis to assess how information is being introduced at each spatial resolution by assessing the gains in the Inception score versus naively resizing the image. I am not clear how much of the gains arose from employing conditional batch normalization versus the proposed method for incorporating the projection based discriminator.
Invite to Workshop Track. rating score: 6. rating score: 6. rating score: 5. <BRK>This paper presents an adversarial combinatorial game: Erdos Selfridge Spencer attacker defender game, with the goal to use it as a benchmark for reinforcement learning. It first compares PPO, A2C, DQN on the task of defending vs. an epsilon sub optimal attacker, with varying levels of difficulty. Secondly it compared RL and supervised learning (as they know the optimal actiona at all times). The defender network has to do this to the features of A and of B, and compare the values; the attacker (with the action space following theorem 3) has to do this for (at most) K progressive partitions. All of this leads me to think that a linear baseline is a must have in most of the plots, not just Figure 15 in the appendix on one task, moreso as the environment (game) is new. A linear baseline also allows for easy interpretation of what is learned (is it the exact formula of phi(S)? ), and can be parametrized to work with varying values of K.   In the experimental section, it seems (due transparent coloring in plots, that I understand to be the minimum and maximum values as said in the text in section 4.1, or is that a confidence interval or standard deviation(s)? sections 6.2, 6.3 and the appendix are more promising but there is only one experiment with potential 1.0 (which is the most interesting operating point for multiagent training) in Figure 8, and potential 0.999 in the appendix.<BRK>This paper presents a study of reinforcement learning methods applied to Erdos Selfridge Spencer games, a particular type of two agent, zero sum game. The authors describe the game and some of its properties, notably that there exists a tractable potential function that indicates optimal play for each player for every state of the board. The comparison of supervised learning vs RL performance is interesting. Why do you think the defender trained as part of a multiagent setting generalizes better than the single agent defender? Quality: The method appears to be technically correct, clearly written, and easy to read. I am also not aware of trying to use games with known potential functions/optimal moves as a way to study the performance of RL algorithms. Impact: I think this is an interesting and creative contribution to studying RL, particularly the use of an easy to analyze game in an RL setting.<BRK>The paper presents Erdos Selfridge Spencer games as environments for investigatingdeep reinforcement learning algorithms. The proposed games are interesting and clearly challenging, but I am not sure what they tell us about the algorithms chosen to test them. Also, in the text the authors say:  >> The results, shown in Figure 4 are surprising. Is "sup rewards" a supervised learning method trained on rewards, or evaluated on rewards, or both? • As the authors state, this paper is an empirical evaluation, and the theorems presented are derived from earlier work. In particular, this relates to:    ◦ The architecture of each of the tested Deep RL methods. Is it true that both these inequalities are strict? The authors could be clearer and more targetted with respect to this question.
Reject. rating score: 3. rating score: 5. rating score: 6. <BRK>The idea is to learn a feature extraction network so that the image classification network performs well and the image reconstruction network performs poorly. Then 2.5 pages are spent on channel pruning. It s not clear to my why PSNR is a useful way to measure privacy loss.<BRK>Average case notions of privacy are usually not appreciated in the privacy community because of their vulnerability to a suite of attacks. The paper may have a valid point that differential privacy is hard to work with, in the case of Deep NN.<BRK>From the initial part of the paper, it seems that the proposed PrivyNet is supposed to be a meta learning framework to split a DNN in order to improve privacy while maintaining a certain accuracy level3.
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. <BRK>All in all, it is difficult not to highly recommend an architecture that achieves state of the art results on such a popular dataset. Importantly, the model achieves state of the art performance of the SQuAD dataset. The paper is very well written and easy to follow.<BRK>I think this is nice since it shows how their mechanism helps on two different tasks (question answering and natural language inference). It basically takes into account a richer context of the word at different levels in the neural net to compute various attention scores.<BRK>In turn, the paper proposes a model that performs attention at all levels of abstraction, which achieves the state of the art in SQuAD. Strengths:  The paper is well written and clear.
Accept (Poster). rating score: 9. rating score: 5. rating score: 4. <BRK>Summary:The manuscript introduces a principled way of network to network compression, which uses policy gradients for optimizing two policies which compress a strong teacher into a strong but smaller student model. Using the teacher network as starting point for the architecture search is a good choice, as initialization strategies are a critical component in knowledge distillation. I am looking forward to seeing work on the research goals outlined in the Future Directions section. The experimental analysis of the approach is very convincing and confirms the author’s claims.<BRK>This paper proposes to use reinforcement learning instead of pre defined heuristics to determine the structure of the compressed model in the knowledge distillation process. The draft is well written, and the method is clearly explained. However, I have the following concerns for this draft:1. Second, the proposed method seems not significantly different from the architecture search method in [1][2] – their major difference seems to be the use of “remove” instead of “add” when manipulating the parameters.<BRK>On the positive side the paper is well written and the problem is interesting. On the negative side there is very limited innovation in the techniques proposed, that are indeed small variations of existing methods.
Accept (Poster). rating score: 7. rating score: 7. rating score: 3. <BRK>The analysis answers:1) When empirical gradients are close to true gradients2) When empirical isolated saddle points are close to true isolated saddle points3) When the empirical risk is close to the true risk.<BRK>Should the risk bound only depends on the dimensions of the matrix W? The main results analyzed: 1) Correspondence of non degenerate stationary points between empirical risk and the population counterparts. 2) Uniform convergence of the empirical risk to population risk.<BRK>This paper studies empirical risk in deep neural networks. Also this is never explicitly mentioned in the paper, I guess the authors make an assumption that the samples (x_i,y_i) are drawn i.i.d.from a given distribution D. In such a case, I am sure results on the population risk minimization can be found for linear regression and should be compare to results in Section 4.
Invite to Workshop Track. rating score: 5. rating score: 5. rating score: 3. <BRK>The authors make a very interesting observation in their description of the proposed approach. In addition, both qualitative and quantitative results to not provide significant evidence of the value of this technique over and above the establish methods in the literature. The central motivation of the method proposed in the paper, is a conjecture that the lack of global consistency in GAN generated samples is due to the binary classification formulation of the discriminator.<BRK>The authors also propose a version that combines this autoregressive discriminator with a patchGAN discriminator. the samples are good (though not better than existing approaches as far as I can tell). The paper is also well written and easy to read. Cons: As is commonly the case with GAN models, it is difficult to assess the advantage of this approach over exiting techniques. the idea of having some form of  recurrent (either over channels of spatially)  processing in the discriminator seems more general that the specific proposal given here.<BRK>It s not clear what can be said with respect to the convergence properties of this class of models, and this is not discussed. The method is quite similar in spirit to Denoising Feature Matching of Warde Farley & Bengio (2017), as both estimate a density model in feature space   this method via a constrained autoregressive model and DFM via an estimator of the score function, although DFM was used in conjunction with the standard criterion whereas this method replaces it. On the plus side, the authors report an average over Inception scores for multiple runs.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. <BRK>In this paper, the authors propose a method (Dynamically Expandable Network) that addresses issues of training efficiency, how to dynamically grow the network, and how to prevent catastrophic forgetting. The paper is well written with a clear problem statement and description of the method for preventing each of the described issues. A critique would be that the base networks (a two layer FF net and LeNet) are not very compelling.<BRK>A strength of the paper is that the proposed algorithm is interesting and intuitive, even if relatively complex, as it requires chaining a sequence of sub algorithms. It was good to see the impact of each sub algorithm studied separately (to some degree) in the experimental section. It may well be that it has the highest task specificity, though this is not trivial to me. The results are overall strong.<BRK>The relative performance of the different models examined, plotted in the top row of Figure 3, is quite different, and though the authors do devote a paragraph to interpreting the results, I found it slightly hard to follow, and was not sure what the bottom line was. "DNN: dase (sic) DNN": how is this trained? Reference for CIFAR 100?
Reject. rating score: 4. rating score: 4. rating score: 4. <BRK>Overall I think the paper does not have a significant enough contribution or impressive enough results to be published. The context is indexing images with descriptor vectors obtained from a DNN. "d taken between 256 and 1024": which one?<BRK>The paper ignores all such works. 2) Lacks FocusThe paper lacks a good organization in my opinion. Things that are perhaps technically important are moved to the Appendix.<BRK>I believe that the manuscript needs some re writing so that the problem(s) are better motivated and is easier to follow. It would be good to discuss the proposed quantity in reference to these existing quantities.
Invite to Workshop Track. rating score: 6. rating score: 6. rating score: 6. <BRK>Strengths: 	There is an interesting analysis on how CNN’s perform better Spatial Relation problems in contrast to Same Different problems, and how Spatial Relation problems are less sensitive to hyper parameters. Weaknesses: 	While the proposed PSVRT dataset addresses the 2 noted problems in SVRT, using only 2 relations in the study is very limited. The paper describes two sets of relationships, but it soon suggests that current approaches actually struggle in Same Different relationships. However, they only explore this relationship under identical objects. Comments: 	In page 2, authors suggest that from that Gülçehre, Bengio (2013) that for visual relations “failure of feed forward networks […] reflects a poor choice of hyper parameters. Authors reason about biological inspired approaches, using Attention and Memory, based on existing literature.<BRK>..the effectiveness of an architecture to learn visual relation problems should be measured in terms of generalization over multiple variants of the same problem, not over multiple splits of the same dataset." They then evaluate several existing network architectures on these tasks, and show that results are not as impressive as others might have assumed they would be. However, this suspicion on my part is actually a reason I think it might be *good* for this paper to be published at ICLR. This will give the people working on (e.g.)<BRK>QualityThis paper demonstrates that convolutional and relational neural networks fail to solve visual relation problems by training networks on artificially generated visual relation data. ClarityThe rationale in the paper is straightforward. OriginalityWhile others have pointed out limitations before, this paper considers relational networks for the first time. Significance This work demonstrates failures of relational networks on relational tasks, which is an important message. ProsImportant message about network limitations.
Reject. rating score: 3. rating score: 4. rating score: 5. <BRK>The paper was fairly easy to follow, but I would not say it was well written. These are minor annoyances; there were some typos and a strange citation format. There is nothing wrong with the fundamental idea itself, but given the experimental results it just is not clear that it is working. What was the performance of the "regression policy", that was learned during the supervised pretraining phase?<BRK>It provides two main contributions: pre training the policy network of a DDPG agent in a supervised manner so that it begins in reasonable state action distribution and regalurizing the Q updates of the q network to be biased towards existing actions. using a known contoller as your behavior policy (behavior policy is a good term for your data generating policy.) al s paper on imitation learning, and although you cite it, you should contrast your approach more clearly with the one in that paper.<BRK>This paper proposes to combine reinforcement learning with supervised learning to speed up learning. Unlike their claim in the paper, the idea of combining supervised and RL is not new. I think even alphaGo uses some form of supervision. Doesn’t this prevent exploration?
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. <BRK>Empirical evaluations conducted on the SQuAD dataset indicates that this architecture achieves an improvement of at least 3%, both on F1 and exact match accuracy, over other comparable systems. It seems to me that this paper is a significant contribution to the field of question answering systems.<BRK>Summary:This paper proposed an extension of the dynamic coattention network (DCN) with deeper residual layers and self attention. The resulting DCN+ model achieved significant improvement over DCN. It is interesting that the mixed objective (which targets F1) also brings improvement on EM.<BRK>This paper proposed an improved version of dynamic coattention networks, which is used for question answering tasks. The proposed model achieved STOA performance on Stanford Question Asnwering Dataset and several ablation experiments show the effectiveness of these two improvements. Although DCN+ is an improvement of DCN, I think the improvement is not incremental. One question is that since the model is compicated, will the authors release the source code to repeat all the experimental results?
Reject. rating score: 4. rating score: 6. rating score: 7. <BRK>Review Summary:The primary claim that there is "a strong correlation between small generalization errors and high learnability" is correct and supported by evidence, but it doesn t provide much insight for the questions posed at the beginning of the paper or for a general better understanding of theoretical deep learning. In this case, the small network can be thought of as applying higher label noise relative to the larger network.<BRK>The proposed approach to figure out what do deep network learn is interesting   the approach of learning a learned network. Some aspects needs more work to improve the work. The presentation of the results can be improved further. A simple figure with increasing layers vs. learnability values would do a better job at conveying the trends. Comments after response from authors   The authors have clarified and shown results for several of the issues I was concerned about.<BRK>The paper suggests that the learnability of a model is a good measure of how simple the function learned by that model is   furthermore, it shows that this notion of learnability correlates well (across extensive experiments) with the test accuracy of the model. It seems to me that this particular use of "learnability" is original, even though PAC learnability was defined a while ago. Significance:  I find the results in this paper to be quite significant, and to provide a new way of understanding why deep neural networks generalize. Additionally, a discussion of why learnabiblity might imply low generalization error would have been interesting (the more formal, the better), though it is unclear how difficult this would be.
Reject. rating score: 5. rating score: 5. rating score: 6. <BRK>3) For an experimental paper as this one, it would be good to have many more problems analysed and a deeper analysis than the one given for the language problem. Whilst the proposed method is interesting and relevant, I find the analysis quite superficial and limited. 1) The distribution for the segment length is fully specified a priori. Rather than chopping the sequence into subsequences of equal length as in truncated BPTT, the authors suggest to segment the sequence into subsequences of differing lengths according to an a priori specified distribution for the segment length.<BRK>It is well known that TBPTT is biased because of a fixed truncation length. The authors propose to make it  unbiased by sampling different truncation lengths and hence changing  the optimization procedure which corresponds to adding noise in the gradient estimates which leads to  unbiased gradients. Its interesting to see in there PTB results that they get better validation score as compared to truncated BPTT. [1] Scheduled Sampling For Sequence Prediction with RNN s https://arxiv.org/abs/1506.03099[2] Professor Forcing  https://arxiv.org/abs/1610.09038Overall, Its an interesting paper which requires some more analysis to be published in this conference.<BRK>In particular, the investigation on the choice of the sampling probability is very helpful to consider how to enhance benefits of the proposed truncated BPTT methods. The proposed methods stochastically determine truncation points with importance sampling. This paper shows two experimental results, in which one is a simple synthetic task and the other is a real data task.
Reject. rating score: 4. rating score: 6. rating score: 6. <BRK>This paper presents AMPNet, that addresses parallel training for dynamic networks. The paper also alludes to “simulator" of a 1 TFLOPs FPGA in the conclusion. However, your entire evaluation is over CPU. 2) To continue on the hardware front and the evaluation, I feel for this paper to be accepted or appreciated, a simulated hardware is not necessary. Personally, I found the evaluation with simulated sleep functions more confusing than helpful.<BRK>This paper proposes new direction for asynchronous training. This paper discusses an implementation of this approach and compares the results on dynamic neural networks as compared to existing parallel approaches. The new approach seems to show positive results on certain dynamic neural network problems. This does seem to make their compiler more complex. Comparisons with Dynet (somewhat hidden away) that offers auto batching in a dynamic mode aren t very positive.<BRK>The paper describes a model parallel training framework/algorithm that is specialized for new devises including FPGA. Most current other frameworks are for model parallelism, so in this sense, the framework proposed by the authors is different and original. Some concerns/questions are 1) The framework is targeted at devices like FPGA, but the implementation is a multicore CPU SMP. It makes the computational result less convincing. The reported speedup results confirm this conjecture. Can the limitation of pipeline model parallelism be improved? The paper is clearly written and easy to follow.
Reject. rating score: 4. rating score: 4. rating score: 5. <BRK>CONCLUSION Overall this work does not introduce any new importance measure for neurons, it merely formalizes the use of standard backpropagation gradients as influence measure. The only comparison to a related work method is done qualitatively on one image visualization, where the proposed method is compared to Integrated Gradients [Sundararajan et al.2017].WEAKNESSES The similarity and differences between the proposed method and related work is not made clear.<BRK>Their extension proposes to measure the influence over a set of images by adding up influences over individual images. Overall it s not clear what this paper adds to existing body of work:1. axiomatic treatment takes a bulk of the paper, but does not motivate any significantly new method2. from experimental evaluation it s not clear the results are better than existing work, ie Yosinsky http://yosinski.com/deepvis<BRK>In this paper, the authors propose a way to measure influence that satisfies certain axioms. My main criticism of this paper is the definition of influence.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 6. <BRK>This paper, to me, is a clear rejection from these basic observations:A *model* is not a computation graph and should never be presented that way. The authors claim this: "So, no encoder or inference models are used since the generators also serve as their inverse transformations."<BRK>2. dynamics of interaction in sequential data in the form of a linear dynamical system w.r.t.latent representation, controlled by secondary latent variables. The model is evaluated in two scenarios:1. 2.The paper proposes novel strategies to update the latent distributions  parameters. Moreover, the paper only experiments with S of length 2.<BRK>The paper proposes a hierarchical probabilistic model that learns both static representations and the dynamics of the data. Figs.4 5 are hard to understand.<BRK>This is useful for for example frame prediction in video and detection of changes in video as a consequence to changes in the dynamics of objects in the scene. There is however not much in terms of discussion nor analysis of the two experiments. I find the contribution fairly significant but I lack some clarity in the presentation as well as in the experiments section. The paper seem to be technically correct. It is mentioned that those that are not detected by the approach are more similar.
Accept (Poster). rating score: 9. rating score: 8. rating score: 6. <BRK>SUMMARY.The paper presents a novel approach to procedural language understanding. The authors tested their model on the proposed dataset and compared it with several baselines. I enjoyed reading this paper, I found the proposed architecture very well thought for the proposed task.<BRK>The method is interesting and presents a good amount of evidence that it works, compared to relevant baseline solutions. The proposed tasks of tracking entities and generating sentences are also interesting given the procedural context, and the authors introduce a new dataset with dense annotations for evaluating this task.<BRK>WeaknessesWhile I find the Neural Process Networks architecture interesting and I acknowledge that it outperforms Recurrent Entity Networks for the presented tasks, after reading the paper it is not clear to me how generally applicable the architecture is. If they the architecture cannot be applied in these tasks, the authors should explain why. I have a similar concern regarding the generation task. What s eI? As mentioned in the weaknesses, this might however come at the price of a less general model, which should be discussed.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>The paper is thorough and on the whole clearly presented. The methods proposed are heuristic in nature, and it s not clear what the guiding principle is. What exactly is the problem without this correction? Can the issue be described more precisely? This step is conventionally executed after gradient aggregation from all nodes. It reads like a story of what the authors did, but it s not really clear why they did it. What is the expected performance of the 1 bit SGD method proposed by Seide et al.?<BRK>I think it should be accepted and my comments are mostly suggestions for improvement or requests for additional information that would be interesting to have. Generally, my feeling is that this work is a little bit too dense, and would like to encourage the authors in this case to make use of the non strict ICLR page limit, or move some details to appendix and focus more on more thorough explanations. Pointing to them from appropriate places would improve clarity I think. This should surely be explained somewhere. A pointer here would be good, at that point it is not clear if it is in your work later, or in another paper. I feel the presentation of experimental results is somewhat disorganized.<BRK>This paper proposes additional improvement over gradient dropping(Aji & Heafield) to improve communication efficiency. First of all, the experimental results are thorough and seem to suggest the advantage of the proposed techniques. The result for gradient dropping(Aji & Heafield) should be included in the ImageNet experiment. A discussion about the relation to asynchronous update is helpful. It seems that local update of momentum is likely going to diverge,  and the momentum masking somehow reset that. In general, this is a paper shows good empirical results. But requires more work to justify the proposed correction techniques. I have read the authors updates and changed my score accordingly(see series of discussions)
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>The exposition in the paper is also not well suited for people without a systems background, although I ll admit I m mostly using myself as a proxy for the average machine learning researcher here. For instance, the authors could do more to explain Lamport Timestamps than a 1974 citation. The work is clearly novel, and the contributions are clear and well justified using experiments and ablations.<BRK>The evaluation in the main paper is largely on synthetic workloads (i.e.large layers with artificial sparsity). I would encourage the authors to put the NMT evaluation in the main paper (and perhaps add other workloads).<BRK>Unfortunately, due to the low level and GPU specific nature of the work, I would think that this work will be better critiqued in a more GPU centric conference. Another concern is that while experiments are provided to demonstrate the speedups achieved by exploiting sparsity, these are not contrasted by presenting the loss in accuracy caused by introducing sparsity (in the main portion of the paper).
Reject. rating score: 4. rating score: 4. rating score: 5. <BRK>The author present a language for expressing hyperparameters (HP) of a network. This language allows to define a tree structure search space to cover the case where some HP variable exists only if some previous HP variable took some specific value. Using this tool, they explore the depth of the network, when to apply batch normalization, when to apply dropout and some optimization variables. The novelty in this paper is below what is expected for a publication at ICLR.<BRK>Monte Carlo Tree Search is a reasonable and promising approach to hyperparameter optimization or algorithm configuration in search spaces that involve conditional structure. This paper must acknowledge more explicitly that it is not the first to take a graph search approach. The cited work related to SMAC and Hyperopt / TPE addresses this problem similarly. The [mis cited] paper titled “Making a science of model search …” is about using TPE to configure 1, 2, and 3 layer convnets for several datasets, including CIFAR 10. There have been near annual workshops on AutoML and Bayesian optimization at NIPS and ICML (see e.g.automl.org). There is a benchmark suite of hyperparameter optimization problems that would be a better way to evaluate MCTS as a hyperparameter optimization algorithm: http://www.ml4aad.org/automl/hpolib/<BRK>This paper introduces a DeepArchitect framework to build and train deep models automatically. Specifically, the authors proposes three components, i.e., model search specification language, model search algorithm, model evaluation algorithm. The paper is written well, and the proposed framework provides us with a systematical way to design deep models. However, my concern is mainly about its importance in practice. The experiments and computational modules are basic and small scale, i.e., it may be restricted for large scale computer vision problems.
Invite to Workshop Track. rating score: 6. rating score: 6. rating score: 4. <BRK>This paper shows a simple method for predicting the performance that neural networks will achieve with a given architecture, hyperparameters, and based on an initial part of the learning curve. I would also request the paper not to casually mention the 7x speedup that can be found in the appendix, without quantifying this. Typos / Details:   The range of the coefficient of determination is from 0 to 1. Overall, this paper appears very interesting.<BRK>This paper explores the use of simple models for predicting the finalvalidation performance of a neural network, from intermediate valuesduring training. The linear models do very well, which means it should be  possible to look at the magnitude of the weights. Theresulting simple prediction framework is then used for early stopping,in particular within the Hyperband hyperparameter search algorithm. Why not include Bayesian linear regression and  Gaussian process regression as baselines?<BRK>This paper proposes the use of an ensemble of regression SVM models to predict the performance curve of deep neural networks. Pros: The paper is proposing a simple yet effective method to predict accuracy. Joining SRM with MetaQNN is interesting as the method is a computation hog that can benefit from such refinement. The overall structure of the paper is appropriate. By the way still, SRM is interesting method if it can be trained once and then be used for different datasets without retraining.
Invite to Workshop Track. rating score: 8. rating score: 5. rating score: 3. <BRK>The paper makes some bold claims. Actually understanding it at depth and validating the proofs and validity of the experiments will require some digestion. I view this as an issue in and of itself. In addition to being quite bold in claims, it is also somewhat confrontational in style. So, while I cannot vouch for the correctness, I think it can and should go through a serious revision to make it succinct and that will likely considerably help in making it accessible to a wider readership and aligned to the expectations from a conference paper in the field.<BRK>The motivation for this measure is that it is invariant to simple rescaling of layers that preserves the function. They focus on linear MLPs in the paper for computational simplicity. They empirically test this out for networks 50 layers deep and 100 layers wide, where they find that some architectures have exploding gradients after random initialization, and others do not, but those that do not have other drawbacks. They then overview the notion of effective depth for a residual network: a linear residual network can be written as a product of terms of the form (I + r_i). In summary, I think this paper needs more work on the presentation to make clear what they are proving and under what conditions, and with experiments that are closer to those used in practice to support their claims.<BRK>The authors do mention experiments on page 8, but confess that some of the results are somewhat underwhelming. This paper has a lot of content, but not all of it appears to be relevant to the authors’ central points. * The paper relies heavily on the supplement to make their central points. * In the text, figure 4 (which is in the supplement) is referenced before figure 3 (which is in the text). It should be noted that all three reviewers pointed out the length of the paper as a weakness of the paper, and that in the most recent draft, the authors made the main text of the paper longer.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>The authors present experiments on unidirectional and bidirectional LSTM models which demonstrate the effectiveness of this method. 7.Section 4.2: Please explain what the exact match (EM) and F1 metrics used to measure performance of the BIDAF model are, in the text. It is interesting that the model structure where the number of parameters is reduced to the number of ISSs chosen from the proposed procedure does not attain the same performance as when training with a larger number of nodes, with the group lasso regularizer.<BRK>Significance:Leaning small models is important and previous sparse RNN work (Narang, 2017) did not do it in a structured way, which may lead to slower inference step time. So this is an investigation of interest for the community. The authors found that their method beats "direct design".<BRK>The paper spends lots of (repeated)  texts on motivating and explaining ISS. But the algorithm is simple, using group lasso to find components that are can retained to preserve the performance. The experiments results are good.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. <BRK>This paper studies active learning for convolutional neural networks. Experiments are performed on three datasets to validate the effectiveness of the proposed method comparing with some baselines.<BRK>It is happy to see such paper in the field. To validate such theoretical result, a non deep learning model should be adopted. The authors should provide more competing algorithms in batch mode active learning.<BRK>On the whole this is interesting work and the results are very nice. Also, important references in active learning literature are missing. The loss function is a function of the CNN function and the true label. This cover tells us which points are to be queried.
Reject. rating score: 4. rating score: 5. rating score: 5. <BRK>On the negative side, from what I can tell, the authors don t seem to have introduced any fundamentally new architectural choices in their neural network, so the contribution seems fairly specific to mastering StarCraft, but at the same time, the authors don t evaluate how much their defogger actually contributes to being able to win StarCraft games. All of their evaluation is based on the accuracy of defogging. All that said, as a paper on an application of deep learning, the paper appears to be solid, and if the area chairs are looking for that sort of contribution, then the work seems acceptable. Is this a substantial fraction of the time of the games studied? If it is not, then perhaps the defogger would not help so much at winning. (The authors state they are favoring the baseline in this comparison, but it would be nice to have those numbers.)<BRK>The paper considers a problem of predicting hidden information in a poMDP with an application to Starcraft. * A related issue, is that the definition of metrics is very informal and, again, does not use the already defined notation. I find the problem of defogging quite interesting, even though it is a bit too Starcraft specific some findings could perhaps be translated to other partially observed environments. In the game of Starcraft, only screen contains information about unit types, but it’s field of view is limited.<BRK>The result shows that the proposed method performs better than several hand designed baselines on two downstream prediction tasks in Starcraft. The evaluation is a bit limited to two specific downstream prediction tasks. Overall, this paper is too specific to Starcraft domain + particular downstream prediction tasks. Alternatively, it could be also interesting to consider an RL problem where the agent should reveal the hidden state of the opponent as much/quickly as possible. # Quality  The experimental result is not much comprehensive. # Clarity  I did not fully understand the learning objective.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>Taking all these into account, I suggest accepting this paper if the authors could provide more ablation study on the proposed methods. [Weaknesses]This paper proposed an interesting and intuitive counting model for VQA.<BRK>Strengths:   I generally agree with the authors that approaching counting as a region set selection problem provides an interpretable and human intuitive methodology that seems more appropriate than attentional or monolithic approaches.<BRK>The idea of sequential counting is novel and interesting.
Reject. rating score: 3. rating score: 4. rating score: 7. <BRK>The winning system has a WER of only 5.8%(vs. 33.4% for the baseline system), while more than half of the submissions to the challenge were able to cut the WER of the baseline system in half or better! The idea is tested on several speech recognition datasets, including TIDIGITS and CHiME3, where the attention is over audio channels, and GRID, where the attention is over video channels. Review:The paper reads well, but as a standard application of attention lacks novelty. Their results on CHiME3 are terrible: the baseline CHiME3 system is very weak, and their system is only slightly better!<BRK>Speech recognition experiments on synthetic noise on audio and video, as well as real data are shown. Attention models are well known and methods to merge information from multiple sensors also (very easily, Multiple Kernel Learning, but many others). Third, the experiments with synthetic noise are significant to a reduced extend. But this is not the kind of noise found in many applications, and this is clearly shown in the performances on real data (not always improving w.r.t state of the art). It is difficult to understand the benefit of this technique if no other baseline is benchmarked.<BRK>This paper uses the same CHiME 3 database, and also showing a similar analysis of channel selection. This paper proposes sensor transformation attention network (STAN), which dynamically select appropriate sequential sensor inputs based on an attention mechanism. Also, the paper does not clearly mention the attention mechanism part, and needs some improvement. Section 4 Models “The parameters of the attention modules are either shared across sensors (STAN shared) or not shared across sensors (STAN  default).”: It’s better to explain this part in more details, possibly with some equations.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>What is the significance of bold in the tables? Originality: original.<BRK>The paper is reasonably well written with clear background and diagrams for the overall architecture. Intuitively, I would expect that different treatments should have different outcomes and the distribution of the factual and counterfactual `y` should differ.<BRK>Comments1) This paper is well written. The background and related works are wellorganized. 2) To the best of my knowledge, this is the first work that applies GAN to ITE estimation.
Reject. rating score: 4. rating score: 5. rating score: 7. <BRK>I see that the first inequality is potentially an application of the conjugate function inequality, but more details are needed (f^* is not even defined). Numerical comparisons between the proposed approach, and the approach from Swaminathan s paper are required to demonstrate the superiority of the proposed approach. The paper is in an okay status.<BRK>The paper proposes an interesting alternative to recent approaches to learning from logged bandit feedback, and validates their contribution in a reasonable experimental comparison. There are several lingering concerns about the approach (detailed below) that detract from the quality of their contributions. [Major] In Lemma 1, L(z) is used before defining it.<BRK>This paper studies off policy learning in the bandit setting. quality and clarity:++ code made available+ well written and clear  The proof of theorem 2 is not in the paper nor appendix (the authors say it is similar to another work). I like that the proposed method is both supported by theory and empirical results.
Invite to Workshop Track. rating score: 8. rating score: 5. rating score: 4. <BRK>In this paper the author propose a CNN based solution for somatic mutation calling at ultra low allele frequencies. The tackled problem is a hard task in computational biology, and the proposed solution Kittyhawk, although designed with very standard ingredients (several layers of CNN inspired to the VGG structure), seems to be very effective on both the shown datasets. Maybe the manuscript part with the definition of the accuracy measures may be skipped. Moreover, the authors themselves suggest how to proceed along this line of research with further improvements. I would only suggest to expand the experimental section with further (real) examples to strengthen the claim. Overall, I rate this manuscript in the top 50% of the accepted papers.<BRK>his paper proposes a deep learning framework to predict somatic mutations at extremely low frequencies which occurs in detecting tumor from cell free DNA. The method is validated on simulations as well as in cfDNA and is shown to provide increased precision over competing methods. They also should compare to Strelka which interestingly they included only to make final calls of mutations but not in the comparison. Further, I  would also have liked to see the use of standard benchmark datasets for mutation calling ( https://www.nature.com/articles/ncomms10001). The authors should explore this behavior in greater detail.<BRK>Summary:In this paper the authors offer a new algorithm to detect cancer mutations from sequencing cell free DNA (cfDNA). The idea is that in the sample being sequenced there would also be circulating tumor DNA (ctDNA) so such mutations could be captured in the sequencing reads. The issue is that the ctDNA are expected to be found with low abundance in such samples, and therefore are likely to be hit by few or even single reads. This makes the task of differentiating between sequencing errors and true variants due to ctDNA hard. The data is based on mutations in 4 patients with lung cancer for which they have a sample both directly from the tumor and from a healthy region. Pros:The paper tackles what seems to be both an important and challenging problem. We also liked the thoughtful construction of the network and way the reference, the read, the CIGAR and the base quality were all combined as multi channels to make the network learn the discriminative features of from the context. The entire dataset is based on 4 patients. It is not clear what is the source of the other cancer control case.
Accept (Poster). rating score: 8. rating score: 8. rating score: 5. <BRK>The paper aims tackles the problem of generate vectorized sketch drawings by using a RNN variational autoencoder. A bi directional LSTM is used to encode latent space in the training stage. Auto regressive VAE is used for decoding. Similar to standard VAEs, log likelihood has bee used as the data term and the KL divergence between latent space and Gaussian prior is the regularisation term. Very interesting dataset to be released. Did each points are sampled from same travelling distance or according to the same time interval? How do you deal with this case?<BRK>This paper introduces a neural network architecture for generating sketch drawings. The authors propose that this is particularly interesting over generating pixel data as it emphasises more human concepts. I agree.The contribution of this paper of this paper is two fold. Firstly, the paper introduces a large sketch dataset that future papers can rely on. The model is inspired by the variational autoencoder. It seems to have a negative impact for the range of values that are discussed. The decoder model has randomness injected in it at every stage of the RNN.<BRK>The paper presents both a novel large dataset of sketches and a new rnn architecture to generate new sketches. This is exciting as it could significantly push the state of the art in sketch understanding and generation. Table 1 is good, but not sufficient. How diverse are the sketches? Additionally, the authors should present a few standard evaluation metrics they would like to compare algorithms on?
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. <BRK>Summary:This paper applies active learning to a deep neural model (CNN CNN LSTM) for  named entity recognition, which allows the model to match state of the art performance with about 25% of the full training data. The difference in performance among the sampling strategies (as shown in Figure 4) seems very tiny. How is the variance of the model performance?<BRK>Pros:* Active learning may be used for improving the performance of deep models for NER in practice* All the proposed approaches are sound and the experimental results showed that active learning is beneficial for the deep models for NERCons:* The novelty of this paper is marginal. The proposed approaches turn out to be a combination of existing active learning strategies for selecting data to query with the existing deep model for NER.<BRK>The paper is well written. The ideas are simple, but they seem to work well in the experiments. * After rebuttalThank you for your response and revision of the paper.
Reject. rating score: 3. rating score: 4. rating score: 6. <BRK>Properly capturing named entities for goal oriented dialog is essential, for instance location, time and cuisine for restaurant reservation. The proposed methods and experiments are not understandable in the current way the paper is written: there is not a single equation, pseudo code algorithm or pointer to real code to enable the reader to get a detailed understanding of the process. Then we have to trust the authors that on their modified dataset, the accuracies of the proposed method is around 100% while not using this method yields 0% accuracies? Is it the same as the generated representation?<BRK>The paper addresses the task of dealing with named entities in goal oriented dialog systems. The proposed solution is to extend neural dialog models by introducing a named entity table, instantiated on the fly, where the keys are distributed representations of the dialog context and the values are the named entities themselves. In its current form, it s not clear how the proposed approach tackles the shortcomings mentioned in the introduction. This raises the question whether the named entity table can only work in this context. Is that correct?<BRK>The paper proposes to generate embedding of named entities on the fly during dialogue sessions. To my understanding, because of the presence of the NER in the With NE Table model, you could directly do update to the NE embeddings and query from the DB using a combination of embedding and the NE words (as the paper does), whereas the W/O NE Table model cannot because of lack of the NER. This seems to prove that an NER is useful for tasks where DB queries are needed, rather than that the dynamic NE Table construction is useful.
Reject. rating score: 2. rating score: 3. rating score: 3. <BRK>Summary of paper and review:The paper presents the instability issue of training GANs for semi supervised learning. Furthermore, Theorem 2.1 is not sufficient to demonstrate existence of this issues. The novelty of the paper is minor, since similar approaches have been done before. Analogous experiments are clearly missing.<BRK>The paper presents a novel architecture for training adversarial networks in a semi supervised settings (Algorithm 1). I need to say the paper is poorly written and not properly polished.<BRK>The co training framework is not so novel for me, which combined the Wasserstein loss and general GAN loss. Meanwhile, the experimental results are not solid. In the paper, the author tried to address the training issue of SSL GANs.
Reject. rating score: 5. rating score: 5. rating score: 6. <BRK>Overall, I feel that the paper is hard to understand and that it would benefit from more clarity, e.g., section 3.3 states that decoding from the softmax q distribution is similar to the Bayes decision rule.<BRK>I have a few questions on the motivation and the results. In Table 1, the differences between RAML and SQDML do not seem to support the claim that SQDML is better than RAML. Are the differences actually significant? In addition, how should \tau be chosen in these experiments?<BRK>The numerical results are relatively weak. Experiment in section 4.1.2 do not validate significant improvement, either. There are several typos in the proof of Theorem 2. 12 and Eqn.
Accept (Poster). rating score: 9. rating score: 8. rating score: 6. <BRK>(a) SignificanceThis is an interesting theoretical deep learning paper, where the authors try to provide the theoretical insights why SGD can learn the neural network well. I think the quality of this work is above the acceptance bar of ICLR and it should be published in ICLR 2018.<BRK>It doesn t assume the input is Gaussian as in most previous work and shows that starting from random initialization, the (stochastic) gradient descent can learn the underlying convolutional filter in polynomial time.<BRK>Overall this is a first step in an interesting direction, so even though it is currently a bit weak I think it is OK to be accepted.
Reject. rating score: 3. rating score: 3. rating score: 5. <BRK>Furthermore, the manuscript seems to suggest, that the simulation results are somehow related to human vision as it is stated:“The model provides apparently realistic saccades, for they cover the full range of the image and tend to point over regions that contain class characteristic pixels.”but no actual comparisons or evaluations are provided.<BRK>The paper is also rife with spelling mistakes and grammatical errors and needs a thorough revision. Overall, I think the paper considers an important problem but the contribution to the state of the art is minimal, and editing highly lacking. Active sensing as Bayes optimal sequential decision making.<BRK>– Figure 3 is low resolution and difficult to read. Post rebuttal comments:I have revised my score after considering comments from other reviewers and the revised paper.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>The paper presents a new approach to generate adversarial attacks to a neural network, and subsequently present a method to defend a neural network from those attacks. My comments are the following:1  I would like to know if benign examples are just regular examples or some short of simple way of computing adversarial attacks. 4  As a side note, it would be interesting to have an explanation about why region classification is providing a worse accuracy than point classification for CIFAR 10 benign samples. As a summary, the authors presented a method that successfully attacks other existing defense methods, and present a method that can successfully defend this attack. I would like to see more formal definitions of the methods presented.<BRK>Compared to previous studies, this paper mainly claims that the information from larger neighborhoods (more directions or larger distances) will better characterize the relationship between adversarial examples and the DNN model. The idea of employing ensemble of classifiers is smart and effective. Results are well discussed with reasonable observations. Authors are suggested to discuss in more detail.<BRK>The analysis of section 4.1 is interesting, it was insightful and to the best of my knowledge novel. A more detailed review follows. Since FSGM is known to be robust to small random perturbations, I would be surprised that for a majority of random directions, the adversarial examples are brought back to the original class. This and / or the defense of training to adversarial examples is an important experiment to assessing the limitations of the attack.
Accept (Poster). rating score: 8. rating score: 5. rating score: 4. <BRK>The authors have undertaken a large scale empirical evaluation on sensitivity and generalization for DNNs within the scope of image classification. They are investigating the suitability of the F norm of the input output Jacobian in large scale DNNs and they evaluate sensitivity and generalization metrics across the input space, both on and of the data manifold. The problem is clearly presented and motivated. The paper seem to be technically correct. I believe that this is important work and applaud the authors for undertaking it. I hope that the interesting leads will be further investigated and that similar studies will be conducted beyond the scope of image classification. The research and further investigations would be strengthened if they would include a survey on the networks presented in the literature in a similar manner as the authors did with the generated networks within the presented study. Figure 4:I find Figure 4:Center a bit confusion. If it is not, then is it maybe possible to make the different sub figure in Figure 4 more distinctive, as to not visually float into each other? Figure 5:The figure makes me curious about what the regions look like close to the training points, which is currently hidden by the content of the inset squares.<BRK>The validation is based on the Jacobian of the network, and in the detection of the “transitions” associated to the data space. The paper proposes an interesting analysis aimed at the empirical exploration of neural network properties, the proposed metrics provide relevant insights to understand the behaviour of a network under varying data points. When considering the linear interpolation of training data, the authors are actually creating data instances not compatible with the original data source: for example, the pixel wise intensity average of digits is not a digit anymore. Meaningful data variation can be way more complex and high dimensional, for example by considering spatial warps of digits, or occlusions and superpositions of natural images. I would expect that highly parameterised models would lead to worse performance when applied to genuinely independent cohorts, and I believe that this work should extend the investigation to this experimental setting. The 14 figures(!)<BRK>The reviewer believes that it is an over simplistic assumption. The problem is not stated in a clear manner, and paper’s contribution is not outlined. The proposed architectures should be explained in detail. The authors should explain the approach of traversing the data manifold with ellipses (although the reviewer believes that such approach needs to be changed with something more principled). The work is definitely too long considered its quality. Unfortunately, despite the authors  effort, the reviewer deems that the conceptual issues that have been highlighted are still present in the paper which, therefore, is not ready for acceptance yet.
Reject. rating score: 5. rating score: 7. rating score: 7. <BRK>The paper considers distribution to distribution regression with MLPs. They test on a few problems, showing similar performance to other distribution to distribution alternatives, but requiring fewer parameters. This seems to be a nice treatment of distribution to distribution regression with neural networks. Its practical utility is questionable. In the introduction, it would also improve the paper to outline clear points of methodological novelty.<BRK>Summary:This paper presents a new network architecture for learning a regression of probability distributions. Notes to authors:I m not familiar with 3BE but the fact that it is used outside of its intended use case for the stock data is worrying. The approach is evaluated on three tasks, two synthetic and one real world.<BRK>This is an intriguing paper on running regressions on probability distributions: i.e.a target distribution is expressed as a function of input distributions. The paper uses three problems to illustrate the idea   a synthetic dataset, a mean reverting stochastic process and a prediction problem on stock indices.
Reject. rating score: 5. rating score: 5. rating score: 6. <BRK>The authors should also improve the intro and related work section. Pros:  The model is fairly easy to understand and it achieves state of the art performance on CLEVR. The model fuses text and image features in a single model. So it’s not very surprising that the proposed method can outperform FiLM (by a little bit). I don’t fully agree with the title   the stack operations are not differentiable.<BRK>Experiments on CLEVR show that the proposed model DDRprog outperforms existing models, but it requires explicit program supervision. Strengths:— For CLEVR VQA task, the proposed model outperforms the state of the art with significantly less number of parameters. Weaknesses:— The paper doesn’t describe the model clearly. — Why is DDRstack not compared to StackRNN? — Can the authors provide training time comparison of their model and other/baseline models? These names have not being defined formally in the paper.<BRK>Summary:The paper proposes a novel model architecture for the visual question answering task in the CLEVR dataset. The proposed model is novel and interesting. The paper writing about the model architecture can be improved. 5.The paper does not have any qualitative examples for either of the two tasks. Overall: The experimental results look good, however, the proposed model needs to be better motivated.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. <BRK>This paper proposes MaskGAN, a GAN based generative model of text based onthe idea of recovery from masked text. *Based on the rebuttals and thorough experimental results, I modified the global rating. This issue is also pointed out by authors in AppendixA.2.<BRK>Quality: The work focuses on a novel problem of generating text sample using GAN and a novel in filling mechanism of words. Clarity: The mechanism of generating the text samples using the proposed methodology has been described clearly.<BRK>Generating high quality sentences/paragraphs is an open research problem that is receiving a lot of attention. This text generation task is traditionally done using recurrent neural networks. This paper proposes to generate text using GANs. (2) how was the masking done? how did you decide on the words to mask?
Invite to Workshop Track. rating score: 7. rating score: 5. rating score: 5. <BRK>As with the earlier papers in this recent program, the paper is notation heavy but generally written well, though there is some overreliance on the readers  knowledge of previous work, for instance in presenting the evidence as above. I was not able to review all the proofs, but what I checked was sound.<BRK>With that said, the main thrust of the paper is very interesting. I m open to reevaluating the review if the issues of clarity and missing literature review are fixed. As someone asked to conduct an  emergency  review of this paper, I would have greatly appreciated the authors making more of an effort to present their results clearly. This seems quite speculative since it is not clear to me what effect either of these things will have on training.<BRK>To the best of my knowledge we do not have a good understanding of mean field theory for neural networks and this paper  and some references therein are starting to address some of it. However, my concern about the paper is in readability. I am very familiar with the literature on mean field theory but less so on deep nets. I found it difficult to follow many parts because the authors assume that the reader will have the knowledge of all the terminology in the paper, which there is a lot of.
Reject. rating score: 4. rating score: 4. rating score: 5. <BRK>Summary:This paper studies learning forward models on latent representations of the environment, and use these for model based planning (e.g.via MCTS) in partial information real time strategy games. The experimental setting is very non trivial and novel. 1.The different forward models are not explained well (what is MatchPi, MatchA, PredN?). Which forward model is trained from which model free agent? Is this because of the MCTS approach? Because the latent h is not informative enough?<BRK>Cons:  The model based approach is disappointing compared to the model free approach. The idea of learning a model based on the features from a model free agent seems novel but lacks significance in that the results are not very compelling (see below). Significance:I feel the paper overstates the results in saying that the learned forward model is usable in MCTS.<BRK>The paper proposes to use a pretrained model free RL agent to extract the developed state representation and further re use it for learning forward model of the environment and planning. Experimentally, the results are rather weak compared to pure model free agents. To me, the paper in it’s current form is not written well and does not contain strong enough empirical results, so that I can’t recommend acceptance. As I understand, MCTS was not used in this experiment.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. <BRK>The map is part of the observation. This resubmission has been massively improved and definitely deserves to be published at ICLR. This paper formulates the problem localisation on a known map using a belief network as an RL problem.<BRK>This is an interesting paper that builds a parameterized network to select actions for a robot in a simulated environment, with the objective of quickly reaching an internal belief state that is predictive of the true state. Early work by Cassandra, Kurien, et al used POMDP models and solvers for active localization with known transition and observation models.<BRK>The paper describes a neural network based approach to active localization based upon RGB images. The framework employs Bayesian filtering to maintain an estimate of the agent s pose using a convolutional network model for the measurement (perception) function. * The comment that the PoseNet and VidLoc methods "lack a strainghtforward method to utilize past map data to do localization in a new environment" is unclear.
Invite to Workshop Track. rating score: 8. rating score: 7. rating score: 5. <BRK>Authors propose a neural network based algorithm for learning from data that arises from dynamical systems with governing equations that can be written as partial differential equations. The network architecture is constrained such that regardless of the parameters, it always implements discretization of an arbitrary PDE. How does the model behave without it? However, it is not clear how this relates to “time stability”, which is also not defined in the article. I understand with larger n, training would be easier since more data would be used to estimate parameters. In using networks, the method differs from previously proposed approaches for learning PDEs. However, it would have been a stronger article if authors have applied to a real life model with real initial and boundary conditions, and real observations. I have three main criticism about the article:  1. 2.Authors emphasize the importance of interpretability, however, the constraint on the moment matrices might cripple this aspect. Can one really interpret the final c_{ij} for filters whose M(q) have many non zeros? In addition to the main criticisms, I have some other questions and concerns:   1.<BRK>The paper advocates the following approach:One assumes a dynamic PDE system involving differential operators up to a given order. An explicit Euler scheme is adopted for time discretization. The parameters of the system are learned by minimizing the approximation error at each timestep. In the experiments reported in the paper the reference signal is provided by numerical simulation of a ground truth system and the authors compare the prediction quality of different versions of their system (eg, for different kernel size). Overall I find the paper good, well written and motivated. The advocated approach should be appealing for scientific applications of deep learning where not only the quality of approximation but also the interpretability of the identified model is important. I would like to see a couple more experiments comparing the proposed approach with those extremes. (2) On the other side, what happens if no sum (vanishing order) constraints are enforced during model training? This abandons the interpretability of the model as approximating a PDE of given order but I am curious to see what is the generalization error of this less constrained system.<BRK>This paper addresses complex dynamical systems modelling through nonparametric Partial Differential Equations using neural architectures. The most important idea of the papier (PDE net) is to learn both differential operators and the function that governs the PDE. To achieve this goal, the approach relies on the approximation of differential operators by convolution of filters of appropriate order. In particular, the interest of learning the filters involved in the approximation of the differential operators is tested against a frozen variant of the PDE net. Parts of the puzzle have to be found in the core of the paper as well as in simulations. If one want to achieve interpretability of the resulting PDE, this is very important. I also found difficult to measure the degree of novelty of the approach considering the recent works and  the related work section should have been much more precise in terms of comparison. However the approach is not compared to the closest works (Sonoda et al., for instance).
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. <BRK>While it is not very surprising that in a potential game it is easy to find Nash equilibria (compare to normal form static games, in which local maxima of the potential are pure Nash equilibria), the idea of approaching these stochastic games from this direction is novel and potentially (no pun intended) fruitful. The paper is well written, the motivation is clear, and some of the ideas are non trivial.<BRK>This manuscript considers a subclass of stochastic games named Markov potential games. As someone with no knowledge in the topic, I find the paper interesting to read, but I have not followed any proofs.<BRK>It also shows how one might find the potential function J, which is used in the single objective optimization problem. The paper does not do a good job comparing and positioning with respect to them. So the paper provides some properties that lead to finding J easier. If I understand correctly, the policies are considered to be functions from the state of the system to a continuous action.
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. <BRK>Summary: The paper presents a modification of the Winograd convolution algorithm that enables a reduction of multiplications in a forward pass of 10.8x almost without loss of accuracy. Review:The paper shows good results using the proposed method and the description is easy to follow.<BRK>This paper proposes a method to build a CNN in the Winograd domain, where weight pruning and ReLU can be applied in this domain to improve sparsity and reduce the number of multiplication. The resultant CNN can achieve ~10x theoretical speedup with little performance loss. The paper does not report the actual speedup in the wall clock time. The results on ImageNet using ResNet 18 architecture are also promising.<BRK>This paper proposes to combine Winograd transformation with sparsity to reduce the computation for deep convolutional neural network. The resulting Winograd ReLUCNN shows strong performance in three scenarios (CIFAR10 with VGG, CIFAR100 with ConvPool CNN C, and ImageNEt with ResNet 18). Overall, the paper is well written and the experiments seems to be quite thorough and clear. Note that I am not an expert in this field and I might miss important references along this direction. Putting ReLU in the Winograd domain (or any transformed domain, e.g., Fourier) seems to be an interesting idea, and deserves some further exploration.
Reject. rating score: 3. rating score: 5. rating score: 6. <BRK>There is nothing new technically in the paper and I find the results uninteresting given the spate of results of this kind. already shows that the depth  3 approximations are uniform approximators. The fact that sigmoidal neural networks with bounded weights can be expressed as "low" degree polynomials is not new.<BRK>This paper proves a new separation results from 3 layer neural networks to 2 layer neural networks. The core of the analysis is a proof that any 2 layer neural networks can be well approximated by a polynomial function with reasonably low degrees. The paper proves the separation by constructing a very specific function that cannot be approximated by 2 layer networks.<BRK>Specifically, the paper shows that there are functions on R^d that can be approximated well by a depth 3 sigmoidal network with poly(d) weights, that cannot be approximated by a depth 2 sigmoidal network with poly(d) weights, and with respect to any input distributions with sufficiently large density in some part of the domain.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>2.I do not really see how the sequence of minimization Eq(5) helps in practice. The Wasserstein term is difficult to hand.<BRK>I reviewed this paper for NIPS with a favorable decision toward weakacceptance; and the authors also addressed some of my questions inthis newer version (namely, some comparisons to related work; clearerwriting). The experiments are only "encouraging"; they do not illustrate clearimprovements over previous methods. However, I think the workdemonstrates useful ideas furthering the idea of continuous timetransformations that warrants acceptance.<BRK>Overall I think the theory is properly described and has a couple of interesting formulations, in spite of being not particularly novel. The authors also propose the use of CTF in density estimation, as a generator of samples from the  true  distribution, and show competitive performance w.r.t.inception score for some common datasets. I think CTFs like the one described here will see increased usage in the VAE setting, and thus the paper will be of interest to the community.
Reject. rating score: 5. rating score: 5. rating score: 5. <BRK>The authors extend the ResNeXt architecture. Last, the impact of ResNeXt/ResNet lies in their ability to generalize to other tasks. They show results on CIFAR 100 and ImageNet (as well as mini ImageNet).<BRK>The paper is clear and well written. It is an incremental modification of prior work (ResNeXt) that performs better on several experiments selected by the author; comparisons are only included relative to ResNeXt.<BRK>The paper proposes replacing each layer in a standard (residual) convnet with a set of convolutional modules which are run in parallel. The paper shows marginal improvements on image classification datasets (2% on CIFAR, .2% on ImageNet) over the ResNeXt architecture that they build on. The paper is well written and conceptually simple. Furthermore, this learning extends to large scale image datasets.
Reject. rating score: 4. rating score: 6. rating score: 6. <BRK>In your experiments, you do not compare with any state of the art RL or hierarchical RL algorithm on your domain, and use a new domain which has no previous point of reference. is there a replay memory?<BRK>None of the comparisons in the paper feature any learning. A method for learning this is presented, and fine tuned with an actor critic method.<BRK>The method is evaluated in one experiment with many different settings. It is well written, the idea is well articulated and presented. The idea to represent task graphs are quite interesting.
Reject. rating score: 2. rating score: 3. rating score: 4. <BRK>Many claims are made without justification (e.g.2.2.“Cavallanti 2012 is not suitable for lifelong learning”… why? Many mistakes in the presentation and experiments. The experiments are not reliable.<BRK>The kernel recursive least squares algorithm. However, the whole paper does not define what “life long learning” is. It is not clear what the new proposal in the paper.<BRK>The paper proposes a budgeted online kernel algorithm for multi task learning. The main contribution of the paper is an online update of the output kernel, which measures similarity between pairs of tasks.
Reject. rating score: 3. rating score: 3. rating score: 7. <BRK>I m finding this paper really difficult to understand. The introduction is very abstract, and it is hard for me to understand the model as it is explained at the moment.<BRK>The paper is confusingly written, fails to mention a lot of related work, has a weak evaluation where it doesn t compare to related systems, and I feel that it would benefit from "toning down". Hence, I do not recommend it for acceptance.<BRK>I think that the author should prepared a revised version of section 2. The proposed approach is novel and not standard and the paper reports significant improvement of entailment results  compared to previous state of the art The method part of the paper (sections 2.1 and 2.2 ) which is the main contribution is not clearly written.
Reject. rating score: 4. rating score: 7. rating score: 7. <BRK>But the paper cannot establish such a result. As the author admitted, the results don t provide any formal guarantees for the convergence to a global minimum. additional review after seeing the author s response: The author s response pointed out some of the limitation of Soudry and Carmon, and Xie et al s which I agree. To some extent this is not consistent with the empirical observation that relu is very important for deep learning. I think the writing of the paper is also misleading in several places.<BRK>Detailed comments:1) My main concern is that the abstract and intro claims things that are never proven (or even stated) in the rest of the paperExample 1 from abstract: “We show that for a wide class of differentiable activation functions (this class involved “almost” all functions which are not piecewise linear), we have that first order optimal solutions satisfy global optimality provided the hidden layer is non singular.”This is certainly not proven and in fact not formally stated anywhere in the paper.<BRK>The results are interesting, but more explanation is needed for the main message to be conveyed more clearly. The paper tackles an important theoretical question; and it offers results that are complementary to existing results (e.g., Soudry et al). I agree with most of the criticism raised by other reviewers.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>The paper describes a neural end to end architecture to solve multiple tasks at once. If this is the case, how to prevent it from happening? The authors should analyze if this is the case, and if not, what are we actually benefiting from multitask learning? If that is the case, what information of the input is being retained when training with multiple tasks and what information of the input is being discarded when training with a single task? Furthermore, based on the results in Table 2, the model clearly fails on the speech recognition task.<BRK>The paper presents a multi task, multi domain model based on deep neural networks. I thus suggest to either drop the speech experiment, or the modify the network to do continuous speech recognition. The experiments clearly show the viability of the approach and give interesting insights. Comments:* In the introduction where the 8 databases are presented, the tasks should also be explained clearly, as several domains are involved and the reader might not be familiar with the task linked to each database. * The training procedure of the model is not explained in the paper. What is the cost function and what is the strategy to train on multiple tasks ? The paper should at least outline the strategy. However, I tend to agree with Reviewer2, as a more comprehensive analysis should be presented to show that the network is not simply multiplexing tasks.<BRK>The paper presents a multi task architecture that can perform multiple tasks across multiple different domains. The authors design an architecture that works on image captioning, image classification, machine translation and parsing. The proposed model can maintain performance of single task models and in some cases show slight improvements. This is the main take away from this paper. Section 2 of the same paper also notes it (depthwise convolutions can be traced back to at least 2012).
Reject. rating score: 4. rating score: 4. rating score: 5. <BRK>The model produces competitive Likelihood results on MNIST and was further tested on CIFAR 10. Best results are obtained when using a ResNet decoder. I wondered how much a standard VAE is improved by using such a powerful decoder. From the motivation the advantages of the model did not become very clear to me.<BRK>Similar models have been trained before; it’s not clear that the proposed pretraining procedure is a practical step forwards. And quite some decisions seem ad hoc and not principled.<BRK>The only results that seem promising are those on binarized MNIST, for the non convolutional architecture, and this setting isn t particularly exciting. So while this work seem to have been seriously and thoughtfully executed, I think it falls short of the ICLR acceptance bar.
Reject. rating score: 3. rating score: 4. rating score: 4. <BRK>I guess the derivation of the learning rule (eq.3) is not really clear to me. Maybe some more discussion on that would be good. This looks rather than a numerical error.<BRK>This information is vital to compare the model to findings from neuroscience and judge the biologic realism. Finally, ideas for the biological implementation of the rule are suggested.<BRK>The model builds on the FEVER model (Druckmann and Chklowskii, 2012) and stays fairly close to the framework and goals laid out in this paper. First, it seems that ICLR is not the right venue for this work. The paper goes on to show a few properties of the new memory model including it s ability to remember multiple stimuli and sensitivity to various degrees of connectivity and plasticity.
Reject. rating score: 3. rating score: 4. rating score: 6. <BRK>It is unclear why discriminative training, such as MMI, sMBR, and lattice free MMI, is mentioned in section 2.3. It is fair to say that this paper contains almost no novelty. Arguments in section 2.3 are weak because, again, all other grapheme based end to end systems have the same benefit as CTC and ASG. Otherwise, the objective won t be a proper probability distribution.<BRK>The paper is interesting, but needs more work, and should provide clear and fair comparisons. the form of equation (3) looks like an MMI criterion to me? Also the units that generate the alignment and the units that are trained on an alignment can be different (I can use a system with 10000 states to write alignments for a system with 3000 states)   this needs to be corrected.<BRK>The paper describes some interesting work but for a combination of reasons I think it s more like a workshop track paper. There are also quite a few misspellings. Since the system is presented without any comparisons to alternatives for any of the individual components, it doesn t really shed any light on the significance of the various modeling decisions that were made. If rejected from here, it could perhaps be submitted as an ICASSP or Interspeech paper.
Accept (Poster). rating score: 9. rating score: 6. rating score: 4. <BRK>The paper presents a method for navigating in an unknown and partially observed environment is presented. The reviewer suggests providing both more details in the main section of the paper and providing the precise architecture including hyperparameters in the supplementary materials section. The proposed method is tested against three problems: a gridworld, a graph search, and a robot environment. While the author’s compare to DRL methods with limited horizon (length 4), there is no comparison to memory based RL techniques.<BRK>The paper addresses the important problem of planning in partially observable environments with sparse rewards, and the empirical verification over several domains is convincing. Maybe the authors meant to use f() for that transition? From Section 2.4, it appears that these map estimates are essential in computing the low level policies from which the final, high level policy is computed.<BRK>Summary:A method is proposed for robot navigation in partially observable scenarios. E.g.2D navigation in a grid world from start to goal but the robot can only sense obstacles in a certain radius around it. The controller takes as input both the convolutional features, the VIN module and has access to a differential memory module. A linear layer takes inputs from both the controller and memory and predicts the next step of the robot. This architecture is termed as MACN. In comparison to the size of problems that can be handled by such planners the experiments shown here are much smaller and crucially the network can output actions which collide with obstacles while the search based planners by definition will always produce feasible paths and require no training data.
Reject. rating score: 3. rating score: 5. rating score: 6. <BRK>This scheme is shown to perform well compared to other methods, specifically combinations of pretraining vs not pretraining embeddings on the training data, updating vs not updating embeddings during training, and others.<BRK>The method itself is to combine d1 dimensional word embeddings that were pretrained on a large unannotated corpus (vocabulary S) with distinct d2 dimensional word embeddings that are trained on the task specific training data (vocabulary T).<BRK>The proposed approach uses generic embeddings and combines them with the embeddings trained on the training dataset in a straightforward string matching algorithm. In addition, the paper also makes a couple of improvements to Chen et. The results are shown on the standard Ubuntu dialogue dataset as well as a new Douban conversation dataset.
Accept (Poster). rating score: 7. rating score: 6. rating score: 4. <BRK>The work is interesting and novel. The fitness of the proposed architecture and methodological choices to the task at hand is sufficiently argued. Table 2 shoes that M NSRF is best for BLEU 1/2, but not for BLEU 3/4. What are these? The paper should be proofread and corrected.<BRK>It is not very clear how to extend from match tensor model to a multi task match tensor model. This makes me feel like this paper is not self contained. The setting for this model is not introduced either in Section 4.2. Section 3 is written mostly about what has been done but not why doing this. It would be nice to know how sensitive this model is to the definition / segmentation of sessions.<BRK>For fair comparison, the regularization trick should also be applied to the baselines. For the model architecture, it is a standard multi task learning framework. Therefore, I think the technical novelty of the work is limited. Clarify: The paper is in general well written.
Reject. rating score: 3. rating score: 4. rating score: 5. <BRK>However, we may find policy pi_1 that makes A true and B false (in general, there is no single optimal policy) and find pi_2 that makes A false and B false, and it will not be possible to satisfy the phi_1 and phi_2 by switching between the policies. The first part of the paper offers a strategy for constructing a product MDP out of an original MDP and the automaton associated with an LTL formula, and reminds us that we can learn within that restricted MDP. This just doesn t make sense to me. There are many small errors in syntax;  it would be best to have this paper carefully proofread.<BRK>The combination of temporal logic formulas with reinforcement learning was developed previously in the literature, and the main contribution of this paper is for fast skill composition. This means that skill composition that is proposed in this paper might be quite far from the best policy that could be learned directly from a single conjunctive TLTL formula. The main contributions of the paper are to provide a method that converts a TLTL formula that specifies a task into a reward function for a new augmented MDP (that can be used by a conventional RL algorithm to yield a policy), and a method for quickly combining two such formulas (and their policies) into a new policy. The main problem with this paper is that the connections between the TLTL formulas and the conventional RL objectives are not made sufficiently clear.<BRK>The idea of using task decomposition to create intrinsic rewards seems really interesting, but does not appear to be explored in any depth. Are there theorems to be had? this makes"  > " making". "and is hardly reusable"  > "and are hardly reusable". "Skill composition is the idea of constructing new skills with existing skills ("  > "Skill composition is the idea of constructing new skills out of existing skills ("."to synthesis"  > "to synthesize". "automatons"  > "automata". "with low level controllers can"  > "with low level controllers that can". "learn a policy that satisfy"  > "learn a policy that satisfies".
Invite to Workshop Track. rating score: 7. rating score: 6. rating score: 5. <BRK>Originality/Significance: Kronecker factorization was introduced for Convolutional networks (citation is in the paper). Soft unitary constraints also have been introduced in earlier work (citations are also in the paper). Nevertheless, showing that these two ideas work also for RNNs in combination (and seeing, e.g.the nice relationship between Kronecker factors and unitary) is a relevant contribution.<BRK>The paper presents a method to parametrize unitary matrices in an RNN as a Kronecker product of smaller matrices. The idea is original to the best of my knowledge and is presented clearly. I think this is the main contribution of this work. Indeed in the experimental section only those members are tested. This needs to be explored. The authors claim that the soft unitary constraint was key for the success of the network, yet no details are provided as to how this constraint was applied, and no analysis was made for its significance.<BRK>Significance: The idea of using factorization for RNNs is not particularly novel. Also, the combination of Kronecker product and soft unitary constraint is really interesting. Do you have an hyper parameter that sets the amplitude of the constraint? Some experimental setups are unfair, and some other could be clearer2. Do you also not train the recurrent matrix in the other models (RNN, LSTM,...)? Remarks The main claim of the paper is that RNN are over parametrized and take a long time to train (which I both agree with), but you didn t convinced me that your parametrization solve any of those problems. Compare more clearly setups where you fix the hidden size.
Accept (Poster). rating score: 9. rating score: 6. rating score: 5. <BRK>Distributed prioritized experience replay. "the full critic architecture is completed by attaching a critic head as defined in Section A"I could find no further documenation in the paper with regard to the "head" or a separate critic for the "head". Overall, I believe that the community will find this to be interesting work.<BRK>"Distributional DDPG" or "Distributional Actor Critic" or variant perhaps could be more fair title choices? The additions investigated are distributional Bellman updates, N step returns, and prioritized experience replay. I also believe "Distributional Policy Gradients" is an overly broad title for this submission as this work still relies on off policy updates and does not tackle the problem of marrying distributional updates with on policy methods. However, I have a concern about the soundness of using N step returns in DDPG setting.<BRK>The method is evaluated on a wide variety of many control and robotic talks. In general, the paper is well written and organised. Two modifications are also simple and well known techniques. It would be nicer if the description in Section 3 is less straightforward by giving more justifications and analysis why and how distributional updates are necessary in the context of policy search methods like DDPG.
Reject. rating score: 2. rating score: 3. rating score: 3. <BRK>The paper states that "it remains unknown what actual features CNNs learn from waveforms.". Moreover, optimization pathways don t have to be linear in parameter space, and oscillations can occur. The claim that "ReLU and non linear activations can improve the network performance, but they are not the main factors in the inner workings of CNNs" is also unfounded.<BRK>Summary: The authors aim to analyze what deep CNNs learn, and end up proposing “SimpleNet”, which essentially reduces the early feature extraction stage of the network to a single convolutional layer, which is initialized using pre defined filters. Furthermore, the adaptation of “front end” signal processing modules in and end to end manner has been considered extensively before (e.g.Sainath et al., 2015), and recent work on very deep networks for signal processing that shows gains on more substantial tasks have not been cited (e.g.Dai, Wei, et al.below).Finally, the experimental results, considering the extensive previous work in this area, are insufficient to establish novel performance in lieu of novel ideas.<BRK>An analysis of convolution and pooling layers applied on waveforms is first presented. Minor comments:* More references for raw waveforms based approach for speech recognition should be added [3,4,6,7,8,9] in the introduction. The SimpleNet approach is interesting but not sufficiently backed with experimental results. The network analysis is minimal and provides almost no insights. The authors should discuss these previous works in the paper. The authors also mention that some utterances have overlapping speech. Overall, in the current form, the results are not convincing.
Reject. rating score: 4. rating score: 5. rating score: 6. <BRK>However, the experiments are too weak to demonstrate the effectiveness of using discrete representations. The design of the experiments on language model is problematic. There are a few interesting points about discretizing the represenations by saturating sigmoid and gumbel softmax, but the lack of comparisons to benchmarks is a critical defect of this paper. However, I didn t see the intuitions behind the model that would result in its superiority to the continuous counterpart. The qualitative evaluation on  Deciperhing the Latent Code  is not enough either. Overall, this paper is more suitable for the workshop track.<BRK>The topic is interesting however the description in the paper is lacking clarity. The paper is written in a procedural fashion   I first did that, then I did that and after that I did third. Another big issue is the lack of proper validation in Section 3.4. Even if you do not know what metric to use to objectively compare your approach versus baseline there are plenty of fields suffering from a similar problem yet  doing subjective evaluations, such as listening tests in speech synthesis.<BRK>The authors describe a method for encoding text into a discrete representation / latent space. The proposed method seems effective, and the proposed DSAE metric is nice, though it’s surprising if previous papers have not used metrics similar to normalized reduction in log ppl. However, overall, the paper is difficult to read and parse, especially since low level details are weaved together with higher level points throughout, and are often not motivated. The major critique would be the qualitative nature of results in the sections on “Decipering the latent code” and (to a lesser extent) “Mixed sample beam decoding.” These two sections are simply too anecdotal, although it is nice being stepped through the reasoning for the single example considered in Section 3.3.
Invite to Workshop Track. rating score: 6. rating score: 5. rating score: 5. <BRK>The paper introduces a formalism to perform graph classification and regression, so called "covariant compositional networks", which can be seen as a generalization of the recently proposed neural message passing algorithms. They argue that relying on permutation  invariance will led to some loss of structural information. In order to address this issue they introduce covariant comp nets, which are a hierarchical decompositon of the set of vertices, and propose corresponding aggregation rules based on tensor arithmetic. * Section 3 is rather lengthy. I wonder if its contents are really needed in the following. This step should be clarified.<BRK>The paper presents a generalized architecture for representing generic compositional objects, such as graphs, which is called covariant compositional networks (CCN). Although, the paper is well structured and quite well written, its dense information    and its long size made it hard to follow in depth. Moreover,  in Table 1, training performances shouldn t be shown, while in Table 3, RMSE it would be nice to be shown in order to gain a complete image of the actual performance.<BRK>The paper covers a very interesting topic and presents some though provoking ideas. The paper introduces "covariant compositional networks" with the purpose of learning graph representations. Unfortunately, the presentation of the approach is extremely verbose and introduces old concepts (e.g., partially ordered set) under new names. The basic idea (which is not new) of this work is that we need to impose some sort of hierarchical order on the nodes of the graph so as to learn hierarchical feature representations. Please address the following points or clarify in your rebuttal if I misunderstood something:  what precisely is the novel contribution of your work (it cannot be "compositional networks" and the propositions concerning those because these are just old concepts under new names)?
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. <BRK>(2016) as a non stochastic baseline, but no comparisons to, e.g., Vondrick et al.(2016) are given.<BRK>Quality: above thresholdClarity: above threshold, but experiment details are missing. Significance: above thresholdPros:This paper proposes a stochastic variational video prediction model. It can be used for prediction in optionally available external action cases.<BRK>I would like to thank the authors for a very nice paper that will definitely help the community towards developing better video prediction algorithms that can now predict multiple futures.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>The paper describes the problem of continual learning, the non iid nature of most real life data and point out to the catastrophic forgetting phenomena in deep learning.<BRK>The finding of the effectiveness of idea (1) seems to be significant. Their finding of the effectiveness of online variational inference for continual learning should be reflected in the writing of the paper as well.<BRK>Via performing variational inference in a kind of online manner, one can address continual learning for deep discriminative or generative networks with considerations of model uncertainty. The paper is written well, and literature review is sufficient. The neural networks in the experiments are shallow.
Accept (Poster). rating score: 7. rating score: 6. rating score: 4. <BRK>This paper proposes a novel hierarchical reinforcement learning method for a fairly particular setting. The core idea is that the master policy is given a relatively easy learning task of selecting between useful motor primitives and this can be efficiently learned from scratch on each new task, whereas learning the motor primitives occurs slowly over many different tasks.<BRK>Please see my detailed comments in the "official comment"The extensive revisions addressed most of my concernsQuality The idea is interesting, the theory is hand wavy at best (ADDRESSED but still a bit vague), the experiments show that it works but don t evaluate many interesting/relevant aspects (ADDRESSED). It is also unclear how much tuning is involved (ADDRESSED). Clarity The paper reads OK. The general idea is clear but the algorithm is only provided in vague text form (and actually changing from sequential to asynchronous without any justification why this should work) (ADDRESSED) leaving many details up the the reader s best guess (ADDRESSED).<BRK>The authors consider the problem of learning a useful set of ‘sub policies’ that can be shared between tasks so as to jump start learning on new tasks drawn from the task distribution. The problem setup is of general interest to the community.
Reject. rating score: 4. rating score: 5. rating score: 5. <BRK>The authors study the effect of label noise on classification tasks. They perform experiments of label noise in a uniform setting, structured setting as well provide some heuristics to mitigate the effect of label noise such as changing learning rate or batch size. Overall, I feel the paper is not up to mark and suggest the authors devote using these insights in a more actionable setting.<BRK>The paper makes a bold claim, that deep neural networks are robust to arbitrary level of noise. It also implies that this would be true for any type of noise, and support this later claim using experiments on CIFAR and MNIST with three noise types: (1) uniform label noise (2) non uniform but image independent label noise, which is named "structured noise", and (3) Samples from out of dataset classes. The experiments show robustness to these types of noise.<BRK>The paper is well written with considerable number of experiments. Batch size and learning rate analysis was very informative but should be done on ResNets and larger datasets to make the paper strong and provide value to the research community. 1.Annotation noise is one of the biggest bottleneck while collecting fully supervised datasets.
Invite to Workshop Track. rating score: 8. rating score: 5. rating score: 3. <BRK>Very intriguing paper and results to say the least. I like the way it is written, and the neat interpretations that the authors give of what is going on (instead of assuming that readers will see the same). Interesting insight into defensive distillation and the effects of uncertainty in neural networks. Quality/Clarity: well written and was easy for me to readOriginality: Brings both new ideas and unexpected experimental results. The caption of Figure 5 doesn t match the labels in the figure s legend, and also has a weird wording, making it unclear what (a) and (b) refer to. In Figure 4, I m not sure I understand the right tail of the distributions. Does it mean that when Delta_ij is very large, epsilon can be very small and still cause an adversarial pertubation?<BRK>This paper insists that adversarial error for small adversarial perturbation follows power low as a function of the perturbation size, and explains the cause by the logit difference distributions using mean field theory. Then, the authors propose two methods for improving adversarial robustness (entropy regularization and NAS with reinforcement learning). * Discovery of the fact that adversarial error follows a power low as a function of the perturbation size epsilon for small epsilon. * Their neural architecture search (NAS) with reinforcement learning found robust deep networks. [weak points]* Unclear derivation of Eq.(9).(What expansion is used in Eq.(21)?)* Non strict argument using mean field theory.<BRK>This work presents an empirical study aiming at improving the understanding of the vulnerability of neural networks to adversarial examples. Further, the authors note that "the universality is not a result of the specific content of these datasets nor the ability of the model to generalize." It can be theoretically proven at least using two routes. They are also in contradiction with other empirical observations consistent across several previous studies. It is easy to show that the difference between the scores (logits) of the two classes is linear in the operator norm of the hidden weight matrix and linear in the L2 norm of the last weight vector. While this is convenient for optimization reasons, it indeed hurts the calibration. The authors should try to train a neural network with a large margin criteria and see if the same phenomenon still holds when they measure the geometric margin. I believe the conclusions of this study are misleading, hence I recommend to reject the paper.
Reject. rating score: 3. rating score: 4. rating score: 5. <BRK>Summary of evaluationThere is not much novelty in this idea (of optimizing carefully only the last layer as a post training stage or treating the last layer as kernel machine in a post processing step), which dates back at least a decade, so the only real contribution would be in the experiments.<BRK>However, the results do not convince this reviewer to switch to using  post training . However, freezing the layers and continue to train the last layer is of a minor novelty.<BRK>Originality:It would be great to have some more theory (if any) for the post training step, or investigate more cases, rather than optimizing only the last layer. One reason is convexity in W of the problem (2).
Reject. rating score: 4. rating score: 4. rating score: 5. <BRK>The paper proposes a new form of regularization that is an extension of "Shake Shake" regularization (Gastaldi, 2017). For one, the method is evaluated only on small toy datasets: CIFAR 10 and CIFAR 100.<BRK>The paper proposes ShakeDrop regularization, which is essentially a combination of the PyramidDrop and Shake Shake regularization. Apparently the goal is to "disturb" the training, and the procedure yields state of the art results on CIFAR 10/100.<BRK>This paper proposes a regularization technique for deep residual networks. + Experimental results on CIFAR 10 and CIFAR 100 well exceed exceed the existing "vanilla" techniques + regularizers.
Reject. rating score: 3. rating score: 4. rating score: 6. <BRK>There are no clear insight, no theorems, and an empirical evaluation on an ill defined problem in time series forecasting. How do the authorsimplement the other graph related approaches in this problem featuringtime series?)<BRK>The authors develop a novel scheme for backpropagating on the adjacency matrix of a neural network graph. If this is true that learning better graph representations really doesn t help very much, that would be good to know, and publishable, but actually *establishing* that requires considerably more experiments. Cons:  The grammar in the paper is pretty bad.<BRK>Learning adjacency matrix of a graph with sparsely connected undirected graph with nonnegative edge weights is the goal of this paper. The UPS optimizer by itself is not new. They mentioned that linear fully connected layer or a more complex neural network can be build on top of the GPS features. However, no detailed information is given in the paper.
Reject. rating score: 4. rating score: 4. rating score: 4. <BRK>This paper introduces siamese neural networks to the competing risks framework of Fine and Gray. You claim to adjust for event imbalance and time interval imbalance but this is not mathematically shown nor documented in the experiments. The results show only one form of comparison, and the results have confidence intervals that overlap with at least one competing method in all tasks.<BRK>The authors tackle the problem of estimating risk in a survival analysis setting with competing risks. The application/setting may be novel, but not the architecture of choice. From Eq.4 to Eq.5, the authors argue that the denominator does not depend on the model parameters and can be ignored.<BRK>The paper entitled  Siamese Survival Analysis  reports an application of a deep learning to three cases of competing risk survival analysis. The author follow the reasoning that  ... these ideas were not explored in the context of survival analysis , thereby disregarding the significant published literature based on the Concordance Index (CI). Besides this deficit, the paper does not present a proper statistical setup (e.g.Is censoring assumed to be at random?
Accept (Poster). rating score: 9. rating score: 6. rating score: 2. <BRK>This paper shows that RNNs are very suitable for convolutional codes and achieves state of the art performance for the first time. The second contribution is on adaptivity outside the AWGN noise model. The authors show that their decoder performs well for different noise statistics outside what it was trained on. This is very interesting and encouraging. The last part goes further in designing new error correcting schemes using RNN encoders and decoders for noisy feedback communication. The paper is very well written with good historical context and great empirical results. It would be interesting to discuss this briefly.<BRK>This paper makes two contributions. First, recurrent neural networks (RNN) are proposed to replace the Viterbi and BCJR algorithms for decoding of convolutional and Turbo decoders. The second contribution of the paper constitutes the design and decoding of codes based on RNNs for a Gaussian channel with noisy feedback. For this channel the optimal codes are unknown. This is a nice step. While at the decoding end it does not bring in anything new (modern coding theory already relies on iterative decoders, that are super fast), at the designing end the Gaussian feedback channel part can be a new direction. I am mostly satisfied with the experiments, barring Fig 8, which does not show the results that the authors claim.<BRK>In this paper the authors propose to use RNNs and LSTMs for channel coding. I believe that machine learning, in general, and deep learning, in particular, might be of useful for physical layer communications. In Page 6, we can read: “Unlike the convolutional codes, the state of the art (message passing) decoders for turbo codes are not the corresponding MAP decoders, so there is no contradiction in that our neural decoder would beat the message passing ones”. This is so true, so I expected the DNN structure to be significantly better than turbodecoding. Also in this case the training sequence is measured in the megabits for extremely simple components. Finally, the last result would be the more interesting one, because it would show that we can learn a better channel coding and decoding mechanism that the ones humans have been able to come up with.
Reject. rating score: 5. rating score: 6. rating score: 7. <BRK>Most of the results are qualitative and I reckon the paper was written in haste. The rest of the comments are below:  3.1: I got a bit confused over what X actually is:   "We would like to learn a generative model for **sets X** of the form". If not, could you please elaborate on what is different (in the case of 3.2 only, I mean)? Figure 1 is helpful to clarify the main idea of a VHE. "However, sharing latent variables across an entire class reduces the encoding cost per element is significantly": typo.<BRK>The paper presents some conceptually incremental improvements over the models in “Neural Statistician” and “Generative matching networks”. Nevertheless, it is well written and I think it is solid work with reasonable convincing experiments and good results. Although, the authors use powerful PixelCNN priors and decoders and they do not really disentangle to what degree their good results rely on the capabilities of these autoregressive components.<BRK>Specifically, the authors propose using approximate posteriors shared across groups of examples, rather than posteriors which treat examples independently. The presentation of the core idea is solid, though it did take two read throughs before the equations really clicked for me. I think the paper could be improved by spending more time on a detailed description of the model for the Omniglot experiments (as illustrated in Figure 3). E.g., explicitly describing how group wise and per example posteriors are composed in this model, using Equations and pseudo code for the main training loop, would have saved me some time. I appreciate that the authors developed extensions of the core method to more complex group structures, though I didn t find the related experiments particularly convincing.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. <BRK>If that is true, then is \calL(\theta, \phi, \phi_x, \phi_y) are right cost function since one does not maximize all three ELBO terms when optimizing \theta?<BRK>Comparing to BiVCCA, apart from the differences that the author discussed, a big difference is the choice of q.  BiVCCA uses two inference networks q(z|x) and q(z|y), while the proposed method uses three. The JMVAE seems to generate more diverse (better coverage) results which are not consistent with the claims in the related work.<BRK>nan
Accept (Poster). rating score: 7. rating score: 6. rating score: 4. <BRK>The introduction of the kernel inception metric is well motivated and novel, to my knowledge. Pros   best description of MMD GANs that I have encountered   good contextualization of related work and descriptions of relationships, at least among the works surveyed   reasonable proposed metric (KID) and comparison with other scores   proof of unbiased gradient estimates is a solid contributionCons   although the review of related work is very good, it does focus on ~3 recent papers.<BRK>The main contribution of the paper is that authors extend some work of Bellemare: they show that MMD GANs [which includes the Cramer GAN as a subset] do possess unbiased gradients. This is a useful result; however, given the lack of other outstanding theoretical or empirical results, it almost seems like this paper would be better shaped as a theory paper for a journal.<BRK>It was noted by the authors that biased gradient estimate can cause problem when performing stochastic gradient descent, as also noted previously by Bellemare et al.The authors also proposed a kernel inception distance (KID) as a quantitative evaluation metric for GAN. The KID is defined to be the squared MMD between inception representation of the distributions. The empirical results show the benefits of using the MMD on top of deep convolutional features. The major flaw of this paper is that its contribution is not really clear.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 5. <BRK>This paper tries to analyze the interpretability of a trained neural network, by representing the concepts, as their hidden features (vectors) learned on training data. However, guided back propagation or grad cam method also does not need any retraining or model tweaking. quantification: Provide quantitative and testable information.<BRK>Summary This paper proposes the use of Concept Activation Vectors (CAVs) for interpreting deep models. I m not sure exactly what this is supposed to mean. Please state it more clearly.<BRK>Summary: This paper proposes a novel framework for explaining the functionality of neural networks by using a simple idea. I would first encourage the authors to improve the overall presentation and organization of this paper. 6.I think the experimental sections suffers from the following shortcomings: i. it does not substantiate all the claims made in the introduction ii.<BRK>* The structure and exposition of the paper needs to be significantly improved. How should this be addressed? I m not sure what the authors do for the latter.
Reject. rating score: 5. rating score: 6. rating score: 6. <BRK>They show that their idea is effective in reducing private information leakage, but this idea alone might not be signifcant enough as a contribution. The author also mentioned using high capacity networks as adversaries does not work well in practice in the introduction, and this could also be due to the high model variance of such high capacity networks.<BRK>Without tuning lambda for each method, the empirical experiments seem unfair. The fact that the paper does not come with substantial theoretical contributions/justification still stands out. The way the log likelihoods of the multiple adversarial models are aggregated does not yield a probability distribution as stated in Eq.2.While there is no requirement to have a distribution here   a simple loss term is sufficient   the scale of this term differs compared to calibrated log likelihoods coming from a single adversary.<BRK>Experiments show that it is suited to create censoring representations for increased anonymisation of data in the context of wearables. Experiments a are satisfying and show good performance when compared to other methods. The paper is well written, but can use some proof reading.
Reject. rating score: 4. rating score: 5. rating score: 7. <BRK>The main contribution of the paper is to propose to learn a Latent Attention Network (LAN) that can help to visualize the inner structure of a deep neural network. It is very interesting. However, one question is what is the potential usage of the model?<BRK>The proposed method iteratively optimize learnable masks for each training example to find the most relevant content in the input that was "attended" by the neural network. Weakness:             Most of the experiments in the paper are performed on small neural networks and simple datesets. It is unclear how the hyperparameter is chosen for the proposed method. It would be great to show a range of samples from high to low beta values. Does it require tuning for different visualization samples?<BRK>The framework takes a pre trained network F as target of the analysis, and trains another network A that generates masks for inputs. It would be interesting to report and discuss convergence properties of the proposed framework.
Accept (Poster). rating score: 9. rating score: 7. rating score: 5. <BRK>The authors lay down a general method for addressing various transfer learning problems: transferring across domains and tasks and in a unsupervised fashion. The paper is clearly written and easy to understand. Even though the method combines the previous general learning frameworks, the proposed algorithm for  LEARNABLE CLUSTERING OBJECTIVE (LCO) is novel, and fits very well in this framework. cons/suggestions:   the authors should discuss in more detail the limitations of their approach: it is clear that when there is a high discrepancy between source and target domains, that the similarity prediction network can fail. How to deal with these cases, or better, how to detect these before deploying this method?<BRK>The authors propose a method for performing transfer learning and domain adaptation via a clustering approach. Explaining how f is used to construct the CCN could help in clarifying exactly what role f plays in the final model. It would be nice to see some examples of the types of predictions and mistakes the model makes to further develop an intuition for how the model works. I m also curious how well the model works if, you do not make use of the labeled source data in the cross domain setting, thereby mimicking the cross task setup. However, the presentation of the method is lacking, and makes it unnecessarily difficult to understand how the model is composed of its parts and how it is trained. I am satisfied with the improvements to the paper and have changed my review to  accept .<BRK>(Summary)This paper tackles the cross task and cross domain transfer and adaptation problems. What s similar vs dissimilar is trained with a binary classifier. (Cons)1.The authors overclaim to be state of the art. Perhaps the authors can clear this up in the text after sec 4.3. Refer to the Cons section above.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. <BRK>LHS of the second line of (6) should be theta.<BRK>None of the theoretical arguments presented earlier in the paper seem to even hint at this.<BRK>(The constant “d” is also dropped but clearly has no effect on the optimization.) Cons/SuggestionsThe paper could use a good deal of proofreading/revision for clarity and correctness.
Reject. rating score: 4. rating score: 5. rating score: 7. <BRK>Experiments: The authors show that on small size networks Tandem Block outperforms Residual Blocks, since He at. al.(2016) in Tab 1 showed a contrary effect, does it mean that the observations do not scale to higher capacity networks?<BRK>The authors hypothesize that the success of shortcut connections comes from the combination of linear and non linear features at each layer and propose to substitute the identity shortcuts with a convolutional one (without non linearity). The paper is well structured and easy to follow. My main concerns are related to the contribution of the paper and experimental pipeline followed to perform the comparison. It would be beneficial to add the mentioned best performing models in Table 2 to back this statement. I wonder how the conclusions drawn scale to much deeper networks (of 100 layers for example) and on larger datasets such as ImageNet. Finally, it might be interesting to initialize the convolutions in the shortcut connections with the identity, and check what they have leant at the end of the training.<BRK>The paper contains a good amount of experiments, but in my opinion not quite enough to conclude that identity skip connections are inherently worse. The question is then: how non trivial is it that tandem networks work? For someone who understands and has worked with ResNet and similar architectures, this is not a surprise. I would suggest removing reason (ii). However, using tandom blocks instead of identity skip connections does not change the number of nonlinearity layers. I do not see them in table 2.
Accept (Poster). rating score: 7. rating score: 7. rating score: 4. <BRK>SUMMARY The paper is fairly broad in what it is trying to achieve, but the approach is well thought out. The purpose of the paper is to investigate the effectiveness of prior machine learning methods with predicting logical entailment and then provide a new model designed for the task. ", and "Which architectures are best at inferring, encoding, and relating features in a purely structural sequence based problem?". The baseline benchmark networks are covered in depth and the reader is provided with a deep understanding on the limitations of some networks with regard to exploiting structure in data. The use of a singular dataset for learning logical entailment.<BRK>Overall, the paper is well written and the proposed model is quite intuitive. The functions themselves are designed as tree neural networks to take advantage of logical structure. Also, should some form of cross validation be applied to smooth out variance in the evaluation results. This problem, I think, is quite related to model counting. a discussion on how this relates to those lines of work would be interesting.<BRK>This is a wonderful and a self contained paper. In fact, it introduces a very important problem and it solves it. The major point of the paper is demonstrating that it is possible to model logical entailment in neural networks.
Invite to Workshop Track. rating score: 6. rating score: 5. rating score: 5. <BRK>This paper presents a tensor decomposition method called tensor ring (TR) decomposition. The proposed method is compared against the TT method on some synthetic high order tensors and on an image completion task, and shown to yield better results. This is an interesting work. TT decompositions have gained popularity in the tensor factorization literature recently and the paper tries to address some of their key limitations. This seems to be a good direction. The experimental results are somewhat limited but the overall framework looks appealing.<BRK>This paper proposes a tensor train decomposition with a ring structure for function approximation and data compression. Most of the techniques used are well known in the tensor community (outside of machine learning). The paper is rather preliminary in its examination.<BRK>The paper proposes Tensor Ring (TR) decomposition which improves over and bases on the Tensor Train (TT) decomposition method. I think such generalization is interesting but the innovation seems to be very limited. All of these are well known methods. Finally, the paper provides experimental results on synthetic data (3 oscillated functions) and image data (few sampled images). I think the paper could be significantly improved by providing more applications of this property in both theory and experiments. 6.SGD also needs to update at least d times for all d latent tensors. The experimental part could be improved by providing more results and description about a guidance on how to choose from different solvers.
Invite to Workshop Track. rating score: 7. rating score: 5. rating score: 4. <BRK>This paper studies the control of symmetric linear dynamical systems with unknown dynamics.<BRK>The paper presents a provable algorithm for controlling an unknown linear dynamical system (LDS). However, I am not sure if the paper is a good fit for ICLR since it is purely theoretical in nature and has no experiments. (C) Do the authors expect that it will be straightforward to remove the assumption that A is symmetric, or is this an inherent limitation of the approach?<BRK>This paper proposes a new algorithm to generate the optimal control inputs for unknown linear dynamical systems (LDS) with known system dimensions. I found it s hard to keep track of which one is inside the expectation. I agree with authors that this is an attempt to combine system identification with generating control inputs together, but I am not sure how to remove the restriction on A. The paper is not clearly written and there are several areas need to be improved.
Accept (Poster). rating score: 7. rating score: 6. rating score: 5. <BRK>Revision: I appreciate the effort by the authors to update the paper. I update my review to 7: Good paper, accept.<BRK>Maybe this is a matter of the dataset FB15k itself but then having experiments on another dataset with hundreds of relation types could be important. And as noted in the paper, fact prediction is much easier. Because the model needs to discover and learn that R < > R   R  R  .<BRK>However, important parts of the paper seem currently unfinished and would benefit from a more detailed discussion and analysis. For instance, the authors mention multi hop methods such as (Neelakantan, 2015; Guu, 2015) in the introduction.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>The paper is clearly written. DAGGER algorithm (2011):  http://www.jmlr.org/proceedings/papers/v15/ross11a/ross11a.pdf  "A Reduction of Imitation Learning and Structured Prediction to No Regret Online Learning"  Professor Forcing (NIPS 2016)  http://papers.nips.cc/paper/6099 professor forcing a new algorithm for training recurrent networks.pdf  Learning Human Motion Models for Long term Predictions (2017)  https://arxiv.org/abs/1704.02827  https://www.youtube.com/watch?v PgJ2kZR9V5w  While the motions do not freeze, do the synthesized motion distributions match the actual data distributions? This is not clear, and would be relatively simple to evaluate. An interesting open issue (in motion, but also of course NLP domains) is that of how to best evaulatesequence prediction models. However, it is included for completeness. It is worthwhile acknowledging that the synthesized motions are still low quality, particular when rendered with more human like looking models, and readily distinguishable from the original motions. In this sense, they are not comparable to the quality of results demonstrated in recent works by Holden et al.or some other recent works. However, the authors should be given credit for including some results with fully rendered characters, which much more readily exposes motion flaws.<BRK>This paper proposes acLSTM to synthesize long sequences of human motion. The key idea is to combine prediction and ground truth in training. The exposition is mostly clear. It also gives a practical guidance to readers how to tune the condition length. In summary, I like the method proposed in the paper. I have not seen an LSTM based architecture predicting a complex motion sequence for that long. However, more detailed analysis about condition length is needed to make this paper complete and more valuable.<BRK>Paper presents an approach for conditional human (skeleton) motion generation using a form of the LSTM, called auto conditioned LSTM (acLSTM). The key difference of acLSTM is that in it parts of the generated sequences, at regular intervals, are conditioned on generated data (as opposed to just ground truth data). It is shown that trained models are more accurate at long term prediction (while being a bit less accurate in short term prediction). Generally the idea is very sensible. For example, “Professor Forcing: A New Algorithm  for Training Recurrent Nets” by Goyal et al.is a more recent variant that does away with the bias that the scheduled sampling of Bengio et al., 2015 would introduce. The lack of comparison to these different methods of training RNNs/LSTMs with generated or mixture of ground truth and generated data is the biggest shortcoming of the paper. That said, the results appear to be quite good in practice, as compared to other state of the art methods that do not use such methods to train. This is incorrect; u v 4 in the figure.
Reject. rating score: 4. rating score: 4. rating score: 5. <BRK>I would argue that if the input has a certain structure, then it should be allowed for the saliency method to make use of this structure. The question that the authors address is how different saliency methods react to transformations of the input data. The mean shift explored by the authors assumes that there is no special point in the input space (especially that zero is not a special point).<BRK>Assuming inputs are transformed in this way, the input invariance property (for mean shift) is always trivially satisfied. Then, they examine whether a number of existing saliency methods satisfy this property. The paper does not discuss why this shift matters. It is not at all clear to me that the quality of the interpretation is adversely affected by these shifts.<BRK>The authors state that saliency methods that do not satisfy an input invariance property can be misleading. From section 3.1 it becomes rather unclear which parts of the paper relate to the literature and which parts relate to section 2.1. It is not clear why CNN haven t been used here (especially because the examples are on MNIST images), while in section 3.1 this is mentioned.
Reject. rating score: 4. rating score: 4. rating score: 6. <BRK>Yet these variations cannot really be captured by the best out of n performance indicator unless modelled as random variables (which would lead to different sorts of problems). Pros:  The widespread reporting of just the best model is clearly leading to very biased results and does not help with reproducibility. Why is it better than just reporting a specific quantile, for example?<BRK>Firstly, the proposed metric requires calculation of multiple test set experiments for every evaluation. In the paper up to 100 experiments were used. And the randomization is also not extended to parameters controlling the model architecture (I suspect that a number of experiments went into picking the 32 layers in the ResNet used by this paper). The metric may be very useful at development time in helping researchers build a reasonable expectation of test time performance in cases where the dev and test sets are strongly correlated. Ultimately, the decision about this paper is a subjective one.<BRK>The method of choosing the best model under  internal  cross validation to take through to  external  cross validation against a second hold out set should be regarded as one possible stochastic solution to the optimisation problem of hyper parameter selection. I think that the fundamental issue of stochasticity of concern for repeatability and generalisability of these performance evaluation exercises is not in the stochastic optimisation search but in the use of a single hold out sample.
Invite to Workshop Track. rating score: 7. rating score: 6. rating score: 5. rating score: 3. <BRK>This work is interesting and fairly thorough. The ablation studies at the end of the paper are the most compelling part of the argument, more so than achieving SoTa. I m also glad that the authors establish statistical significance! I think this would be a compelling addition. Cons:   Authors make the broad claim of world knowledge being helpful for textual entailment, and show usefulness in a limited datasize setting, but don t test their method on other datasets RTE (which has a lot less data). If this helps performance on RTE then this could be a technique for low resource settings.<BRK>This paper does not make that much progress on the problem in general—the methods here are quite specific to words and to NLI—and the proposed methods yields only yield large empirical gains in a reduced data setting, but the paper serves as a well executed proof of concept. as a way to provide evidence for the paper s core claim (that the added knowledge in the proposed model helps). Can the authors comment on this? Minor points:For TransE, what does this mean:"However, these kind of approaches usually need to train a knowledge graph embedding beforehand." Equation (15) is confusing. I suspect that this might be an opportunity for a gratuitous self citation.<BRK>This is a very interesting paper! We are finally back to what has been already proven valid for NLI also know as RTE. In fact, alignment is one of the basis for building RTE systems. It appears that external knowledge is useful only in the case of restricted data (see Figure 3). One of the important question here is then if the knowledge of all the data is in fact replicating the knowledge of wordnet. If this is the case, this may be a major result.<BRK>All of these three additions help, especially in the low data learning scenario. I think that the integration of structured knowledge representations into neural models is a great avenue of investigation. But very little was done to investigate different ways in which these data can be integrated. The authors mention work on knowledge base embeddings and there has been plenty of work on learning WordNet embeddings. An obvious avenue of exploration would compare the use of these to the use of the indicator features in this paper. Another avenue of exploration is the integration of more resources such as VerbNet, propbank, WikiData etc. If so please state this and if not, what is the motivation of your hypernymy and hyponymy features?
Reject. rating score: 4. rating score: 6. rating score: 8. <BRK>[Summary]The paper is overall well written and the literature review fairly up to date. The main issue is the lack of novelty. The proposed method is just a straightforward dimensionality reduction based onconvolutional and max pooling layers. Using CNNs to handle variable length time series is hardly novel. In addition, as always with metric learning, why learning the metric if you can just learn the classifier? If the metric is not used in some compelling application, I am not convinced. [Detailed comments and suggestions]* Since "assumptions" is the only subsection in Section 2, I would use \texbf{Assumptions.}<BRK>There is something incoherent about training a convolutional network to classify time series, then discarding the classification layer and using the internal representation as input to a 1NN classifier. On the whole, I like the line of inquiry and the elegant simplicity of the proposed approach, but the paper has some flaws (and there are some gaps in both motivation and the experiments) that led me to assign a lower score. Apart from the custom pooling layer, the architecture is common and well understood by the community   thus, the figure can probably be removed. If, however, the learned representations are "overfit" to the classification task (I suspect they are), and if the learned classifier outperforms embeddings + 1NN, then what would I use these representations for?<BRK>The algorithm is simple and experiments show that it is effective on a limited benchmark. It would be interesting to enlarge the dataset to be able to compare statistically the results with state of the art algorithms. In addition, Authors compare themselves with time series metric learning and generalization of DTW algorithms. It would also be interesting to compare with other types of time series classification algorithms (Bagnall 2016) .
Accept (Oral). rating score: 8. rating score: 7. rating score: 7. <BRK>First, the description is mapped to a "sketch" (Y) containing high level program structure but no concrete details about, e.g., variable names. The paper presents an abstraction method for converting a program into a sketch, a stochastic encoder decoder model for converting descriptions to trees, and rejection sampling like approach for converting sketches to programs. Experimentally, it is shown that using sketches as an intermediate abstraction outperforms directly mapping to the program AST. This is one of the strongest points of the paper. Would the approach work as well using a more standard encoder decoder model with determinstic Z? Some discussion of Grammar Variational Autoencoder (Kusner et al) would probably be appropriate. While the general pattern of mapping a specification to abstraction with a neural net and then mapping the abstraction to a full program with a combinatorial technique is not necessarily novel, I think this paper adds an interesting new take on the pattern (it has a very different abstraction than say, DeepCoder), and this paper is one of the more interesting recent papers on program synthesis using machine learning techniques, in my opinion.<BRK>This is a novel contribution to existing machine learning approaches to automated programming that is achieved by training on a large corpus of Android apps. The combination of the proposed technique and leveraging of real data are a substantial strength of the work compared to many approaches that have come previously. This paper has many strengths:1) The writing is clear, and the paper is well motivated2) The proposed algorithm is described in excellent detail, which is essential to reproducibility3) As stated previously, the approach is validated with a large number of real Android projects4) The fact that the language generated is non trivial (Java like) is a substantial plus5) Good discussion of limitationsOverall, this paper is a valuable addition to the empirical software engineering community, and a nice break from more traditional approaches of learning abstract syntax trees.<BRK>The authors introduce an algorithm in the subfield of conditional program generation that is able to create programs in a rich java like programming language. In this setting, they propose an algorithm based on sketches  abstractions of programs that capture the structure but discard program specific information that is not generalizable such as variable names. •	Excellently structured and presented paper 	•	Motivation given in form of relevant applications and mention that it is relatively unstudied	•	The hypothesis/ the papers goal is clearly stated. •	Explanations are exceptionally well done: terms that might not be familiar to the reader are explained. This is true for mathematical aspects as well as program generating specific terms. The task itself is interesting and novel. The large evaluation section that discusses many different properties is a further indication that this approach is not only novel but also very promising. Even though no conclusive section is provided, the paper is not missing any information.
Reject. rating score: 2. rating score: 7. rating score: 7. <BRK>The authors are advised to use the insights from their proposed SDR analysis to demonstrate quantifiable improvements the VAEs/GANs. Furthermore, they are missing a reference to beta VAE (Higgins et al, 2017) when discussing VAE based approaches to disentangled factor learningIn summary, the paper is not ready for publication in its current form. This analysis is performed in the context of the Independence of Cause and Mechanism (ICM) framework.<BRK>This work exploits the causality principle to quantify how the weights of successive layers adapt to each other. Generally, the result is interesting and the presentation is easy to follow. For example,  it is hard to obtain the conclusion "more independence lead to better performance" from the experimental results.<BRK>The authors apply their newly defined measure to DCGANs and plain VAEs with ReLUs, and show that dependency between successive layers may lead to bad performance. It would be also good to add some more related work. I’m not an expert, but I assume there must be some similar idea in CNNs. From my limited point of view, this seems like a sound, novel and potentially useful application of a interesting idea. If the writing was improved, I think the paper may have even more impact.
Invite to Workshop Track. rating score: 7. rating score: 6. rating score: 6. <BRK>The authors analyzed the the generalization for the following scenarios  the generalization ability of RNNs on random subset of SCAN commands  the generalization ability of RNNs on longer SCAN commands  The generalization ability of composition over primitive commands. The experiments supported the hypothesis that the RNNs are able to   generalize zero shot to new commands. difficulty generalizing to longer sequence (compared to training sequences) of commands. the ability of the model generalizing to composition of primitive commands seem to depend heavily on the whether the action is seen during training. The model does not seem to generalize to completely new action and commands (like Jump), however, seems to generalize much better for Turn Left, since it has seen the action during training (even though not the particular commands)Overall, the paper is well written and easy to follow. The results and analysis are informative. As for future work, I think an interesting direction would also be to investigate the composition abilities for RNNs with latent (stochastic) variables.<BRK>This paper argues about limitations of RNNs to learn models than exhibit a human like compositional operation that facilitates generalization to unseen data, ex. To do this, they introduce a new dataset that facilitates the analysis of a Seq2Seq learning case. The main idea in the paper is that RNNs applied to Seq2Seq case are learning a representation based only on "memorizing" a mixture of constructions that have been observed during training, therefore, they can not show the compositional learning abilities exhibit by humans (that authors refer as systematic compositionality). While the experiments are compelling, as I explain below, I believe there is an underlying assumption that is not considered. Accordingly, my main point is the following: the model is indeed learning the task, as measured by performance on training set, so authors are only showing that the solution selected by the RNN does not follow the one that seems to be used by humans. In this sense, the paper would really produce a more significant contribution is the authors can include some ideas about the ingredients of a RNN model, a variant of it, or a different type of model, must have to learn the compositional representation suggested by the authors, that I agree present convenient generalization capabilities.<BRK>Description of the model architecture is largely done in the appendix, this puts the focus of the paper on the experimental section. This choice seems to be appropriate, since standard methods are used. Figure 2 is sufficient to illustrate the model to readers familiar with the literature. This search enables the authors to come to convincing conclusions regarding the shortcomings of current models. Model generalization to unknown data similar to the training set. These experiments show that modern sequence to sequence models do not solve the systematicity problem, while making clear by application to machine translation, why such a solution would be desirable. The SCAN data set has the potential to become an interesting test case for future research in this direction. However, where the paper falls a bit short is in the discussion / outlook in terms of suggestions of how one can go about tackling these shortcomings.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>The main idea is to exploit a schema of semisupervised learning based on ACOL and GAR for an unsupervised learning task. Since it is not known in advance what might be a good set of transformations, it is not clear what is the behaviour of the model when the large portion of transformations are not encoding the latent representation of clusters.<BRK>So my main problem with this paper, lack of novelty, is addressed and my score has changed. This paper utilizes ACOL algorithm for unsupervised learning. The authors also modified affinity and balance term utilized in GAR (as part of ACOL algorithm) to improve it. I updated my scores based on the reviewers responses.<BRK>(*) Cons minor: in the title, I find the expression "unsupervised clustering" uselessly redundant since clustering is by definition unsupervised. The author also addressed this point, and I changed my scores accordingly. The novelty seems to be in the adaptation to GAR from the semi supervised to the unsupervised setting with labels indicating if data have been transformed or not.
Reject. rating score: 4. rating score: 7. rating score: 8. <BRK>* The explanation of eqs 1 and 2 is quite poor. "L_target is a target objective which can be a negative class probability .." this assumes that the example is a positive class. * In typical ICLR style the authors use a deep network to learn the encoder and decoder networks. Does this method also work on say 2s and 3s? * Overall: the paper contains an interesting idea, but given the deficiencies raised above I judge that it falls below the ICLR threshold.<BRK>The authors address two important issues: semi supervised learning from relatively few labelled training examples in the presence of many unlabelled examples, and visual rationale generation: explaining the outputs of the classifiier by overlaing a visual rationale on the original image. The main idea is to train a GAN on the unlabeled examples to create a mapping from a lower dimensional space in which the input features are approximately Gaussian, to the space of images, and then to train an encoder to map the original images into this space minimizing reconstruction error with the GAN weights fixed. The idea in the paper is, to my knowledge, novel, and represents a good step toward the important task of generating interpretable visual rationales. There are a few limitations, e.g.the difficulty of evaluating the rationales, and the fact that the resolution is fixed to 128x128 (which means discarding many pixels collected via ionizing radiation), but these are readily acknowledged by the authors in the conclusion. 2) It would be good to include PSNR/MSE figures for the reconstruction task (fig 2) to have an objective measure of error.<BRK>The main contribution of the paper is a method that provides visual explanations of classification decisions. It would be great if this analysis could be performed for MNIST as well. The paper proposes a (I believe) novel method to obtain visual explanations. This looks very noisy and non interesting.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>This paper proposes Graph2Gauss (G2G), a node embedding method that embeds nodes in attributed graphs (can work w/o attributes as well) into Gaussian distributions rather than conventionally latent vectors. Overall, the paper is well written and the contributions are remarkable.<BRK>This paper is well written and easy follow.<BRK>The paper proposes to learn Gaussian embeddings for directed attributed graph nodes. Detailed comments:The title of the paper is “Deep …”. The paper reads well.
Reject. rating score: 3. rating score: 5. rating score: 6. <BRK>The main idea is to fit a classifier on the training data and also learn a GAN model using the training data. The proposed approaches are experimental but does not require human inspection. 2.Since mode collapse is a well known phenomenon, the novelty of this paper is not sufficient. The distribution of predicted labels  and the labels of the true data can be easily compared.<BRK>VAEs can obtain very good samples on celebA, a dataset with relative low diversity, but not so good samples on cifar. This failure mode can then be captured by this metric. Looks like subtle labels on faces are not being captured by GAN models. That makes the paper comparable with previous work and is a test against bugs in model implementations or other parts of the code. This would also allow to test for claims such as the fact that the Improved GAN has more mode collapse than DCGAN, while the Improved GAN paper says the opposite.<BRK>The paper proposes a new evaluation measure for evaluating GANs. I m not super convinced that this is an useful evaluation metric as the absolute number is somewhat to interpret and dependent on the details of the classifier used. It seems like a generator which generates samples close to the classification boundary (but drops examples far away from the boundary) could still achieve a high score under this metric.
Reject. rating score: 5. rating score: 5. rating score: 7. <BRK>This paper proposes an iterative approach to train deep neural networks based on morphism of the network structure into more complex ones. The ideas are rather simple, but could be potentially important for improving the performance of the networks. Because I am not an active researcher in the topic, I cannot judge if the benefits that are shown in the experiments are enough for publication (the theoretical part is not the strongest of the paper).<BRK>This paper proposed an iterative learning scheme to train a very deep convolutional neural network. However, many parts of the ideas discussed in the paper (Section 3.3) are already investigated in Wei et al., 2016, which limits the novel contribution of the paper.<BRK>This submission develops a learning scheme for training deep neural networks with adoption of network morphism (Wei et al., 2016), which optimizes a dynamic objective function in an iterative fashion capable of adapting its function form when being optimized, instead of directly optimizing a static objective function. The shown experimental results should be able to validate the effectiveness of the learning scheme to some extent. It would be more convincing to include the performance evaluation of the learning scheme in some representative applications, since the optimality of the training objective function is not necessarily the same as that of the trained network in the application of interest.
Accept (Oral). rating score: 9. rating score: 7. rating score: 5. <BRK>This paper presents a set of studies on emergent communication protocols in referential games that use either symbolic object representations or pixel level representations of generated images as input. The work is extremely creative and packed with interesting experiments. \mathbf{v} is a set, but it s denoted as a vector. in which space are the reported pairwise similarities computed?<BRK>The authors provide novel analysis of the learned languages and perceptual system across a number of environmental settings, coming to the (perhaps uncontroversial) finding that varying the environment and restrictions on language result in variations in the learned communication protocols. In the context of existing literature, the novelty of this work is somewhat limited   consisting primarily of the extension of multi agent reference games to raw pixel inputs.<BRK>The results show that the agents generate effective communication systems, and some analysis is given of the extent to which these communications systems develop compositional properties – a question that is currently being explored in the literature on language creation. This is an interesting question, and it is nice to see worker playing modern neural network models to his question and exploring the properties of the solutions of the phone. However, there are also a number of issues with the work.
Reject. rating score: 5. rating score: 5. rating score: 6. <BRK>The paper proposes a novel graph convolutional network in which a variety of random walk steps are involved with multiple GCNs. Robustness for the feature remove is also interesting. How the good balance of training and validation can be determined? In introducing many degrees (GCNs) for small labeled nodes semi supervised setting seems to cause over fitting.<BRK>In this work a new network of GCNs is proposed. Different GCNs utilize different powers of the transition matrix to capture varying neighborhoods in a graph. It also was tested on graph classification datasets, but the results were not as good for some of the datasets. I think that comparison to DCNN is important to justify the importance of using multilayer GCN modules. Could you please report mean and standard deviation of all the runs? This way the effect of different degrees of neighborhoods in a graph could be understood better.<BRK>The paper presents a Network of Graph Convolutional Networks (NGCNs) that usesrandom walk statistics to extract information from near and distant neighborsin the graph. They build on this notion to  introduce the idea to make the GCN directly operateon random walk statistics to better model information across distant nodes. I find that the comparison can be considered slightly unfair as NGCN has k timesthe number of GCN models in it. Overall I like the interpretation, even if a bit forced, of GCN as using one steprandom walk statistics. The main issue I have with the approach is that it does not bring a very novelway to perform deep learning on graphs, but rather improves marginally upona well established one.
Reject. rating score: 4. rating score: 5. rating score: 6. <BRK>The paper proposes a new data augmentation technique based on picking random image pairs and producing a new average image which is associated with the label of one of the two original samples. the method is presented as a heuristic technique.<BRK>The paper investigates a method of data augmentation for image classification, where two images from the training set are averaged together as input, but the label from only one image is used as a target. This is a simple technique, and the paper is concise and to the point.<BRK>The paper reports that averaging pairs of training images improves image classification generalization in many datasets. The paper is also straightforward to read and clear, which is positive.
Reject. rating score: 4. rating score: 4. rating score: 5. <BRK>I ve described many of the points I was confused by in more detailed comments below. I m also unsure why on p.6 MNIST uses 2d windows, while CIFAR uses 3d   The paper mentions the extra dimension is for features, but MNIST would have a features dimension as well at this stage, I think? But in the upper layers, the spatial extent can be very small compared to the image size, sometimes even 1x1 depending on the network downsampling structure. So are do the "windows" correspond to spatial windows, and if so, how?<BRK>The paper seems to claims that1) certain ConvNet architectures, particularly AlexNet and VGG, have too many parameters,2) the sensible solution is leave the trunk of the ConvNet unchanged, and to randomly sparsify the top most weight matrices. If not, your fully connected baseline may be unnecessarily overfitting the training data.<BRK>The authors propose reducing the number of parameters learned by a deep network by setting up sparse connection weights in classification layers. Numerical experiments show that such sparse networks can have similar performance to fully connected ones. It was not clear to me why scatter (the way it is defined in the paper) would be a useful performance proxy anywhere but the first classification layer.
Invite to Workshop Track. rating score: 6. rating score: 5. rating score: 4. <BRK>Pros:1.A new DNA structure GAN is utilized to manipulate/disentangle attributes. 3.Based on the experiment results, this proposed method outperformed previous methods (TD GAN, IcGAN). Cons:1.It assumes that each individual piece represents an independent factor of variation, which can not hold all the time. The authors also admit that when two factors are dependent, this method might fail. How about A and A2 here? 3.Only one attribute can be "manipulated" each time? Is it possible to change more than one attribute each time in this method?<BRK>Novelty and Significance  Multi attribute image generation is an interesting task but has been explored to some extent. The integration of generative adversarial networks with auto encoding loss is not really a novel contribution. Based on my understanding, additional loss was added to encourage effective disentangling (prevent trivial solution). I would appreciate the authors to elaborate this a bit. Third, it seems that the proposed multi attribute generation pipeline works for binary attribute only. However, such assumption limits the generality of the work. Additionally, considering the generation quality, the CelebA samples in the paper are not the state of the art. I suspect the proposed method only works in a more constrained setting (such as Multi PIE where the images are all well aligned).<BRK>This paper proposes to disentangle attributes by forcing a representation where individual components of this representation account for individual attributes. Pros: + The idea of forcing different parts of the latent representation to be responsible for different attributes appears novel. + A theoretical guarantee of the efficiency of an aspect of the proposed method is given. The results from the proposed method do not seem much better than the baselines. What is the objective for the images in Fig.4?For example I m looking at the bottom right, and that image looks more like a merger of images, than a modification of the image in the top left but adding the attributes of choice. Quantitative results are missing.
Reject. rating score: 4. rating score: 4. rating score: 5. <BRK>For these reasons I feel that the paper would clearly be more interesting for the practitioners (and maybe to some extent for the audience of ICLR) if numerical applications of the presented theory were discussed or sketched in classical reinforcement learning settings. This allows to draw insights on the convergence of empirical practices in the field.<BRK>The main object of the paper is the (entropy regularized) policy updates. I fail to see the significance nor the novelty in this work (esp.in light of  Jordan et al.(1998) and Peyre (2015)). That said, I believe that exposing such connections will prove to be useful, and I encourage the authors to push the area forward.<BRK>My major critic to this paper is its practical value. To the best of my knowledge, this line of research was dated back to the paper by Jordan et al in 1998, where they showed that the continuous control policy transport follows the Fokker Planck equation. I also think this paper is well written and mathematically sound.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>Significance: The problem that the paper is solving is significant. In particular, section 3 describes the problem statement but in terms of learning a POMDP policy.<BRK>Overall the paper is well written with clear logic and accurate narratives.<BRK>I think their method and their evaluation has some major weaknesses, but I think that it still provides a good baseline to force work in this space towards tasks which can not be solved by simpler models like this. This paper extends an existing thread of neural computation research focused on learning resuable subprocedures (or options in RL speak). So, it s possible that all the prior work is doing is learning to approixmate a much simpler architecture of this form.
Invite to Workshop Track. rating score: 6. rating score: 5. rating score: 5. <BRK>This paper proposes a method to learn a control policy from both interactions with an environment and demonstrations. Especially the work builds upon the recent work by Haarnoja et al (2017) and Schulman et al (2017) (both unpublished Arxiv papers). Also, the fact that the importance sampling ration is always cut at 1 (or not used at all) is inherited from these papers too. I m also uncomfortable with the way most of the expert data are generated for experiments. Indeed, a single error in some given state will often generate totally different trajectories and not affect a single transition.<BRK>Entropy regularized formulation leads to nice relationships between the value function and the policy, and has been explored recently by many, including [Ziebart, 2010], [Schulman, 2017], [Nachum, 2017], and [Haarnoja, 2017]. As a result, Algorithm 1 does not use importance sampling. This basically suggests that by ignoring the fact that the data is collected off policy, and treating it as an on policy data, the agent might perform better. I also find the empirical results encouraging. But I have some concerns about this paper:  The derivations of the paper are unclear. But if it is the case, shouldn’t we have a gradient of Q in (15) too? Currently the novelty is not obvious.<BRK>Thanks for all the explanations on my review and the other comments. The application to learning from (partially adversarial) demonstrations is a cool idea but effectively is a very straightforward application based on the insight that the approach can handle truly off policy samples. Clarity The paper reads well, but it is not really clear what the claimed contribution is. Originality The application seems original. Pros and Cons + good results+ interesting idea of using the algorithm for RLfD  weak experiments for an application paper  not clear what s new
Invite to Workshop Track. rating score: 7. rating score: 5. rating score: 5. <BRK>Deep learning is a technique that has attracted a lot of attention. A drawback is that the execution performance can be limited, e.g., due to run time interpretation of the computation graph. This paper takes a different approach and presents a compiler framework that allows definition of domain specific languages (DSLs) for deep learning system, defines a number of compilation stages that can take advantage of standard compiler optimizations as well as specialized optimizations for neural networks using an intermediate representation, and also a back end. The intention is to provide the framework as open source in the future. The main drawback of the paper is the lack of evaluation.<BRK>The success of Deep Learning is, in no small part, due the development of libraries and frameworks which have made building novel models much easier, faster and less error prone and also make taking advantage of modern hardware (such as GPUs) more accessible. This is still a vital area of work, as new types of models and hardware are developed. This work argues that prior solutions do not take advantage of the fact that a tensor compiler is, essentially, just a compiler. This paper is not well adapted for an ICLR audience, many of which are not experts in compilers or LLVM. The primary weakness of this work is the lack of careful comparison with existing framework. Because of this work seems likely to be of limited interest to the ICLR audience, most of which are potentially interested users rather than compiler experts. There is also no benchmarking, which is at odds with the claims the compiler approaches allows easier optimization.<BRK>Unfortunately, the manner in which they go about doing this is ad hoc and does not adopt best practices developed in the compilers and programming languages communities. Unfortunately, the paper falls short in two significant respects: It does not adequately cite related work and it does not present any experiments to quantify the benefits they claim will be achieved by their new compiler. Pros:  The paper proposes a very relevant and timely proposal to design a modern day deep learning compiler framework. Cons:  Related work is not adequately referenced. They provide no micro benchmarks or end to end deep learning use cases to quantify the benefits of their compiler vs. some of the currently available ones.
Invite to Workshop Track. rating score: 8. rating score: 4. rating score: 4. <BRK>Authors propose sampling stochastic gradients from a monotonic function proportional to gradient magnitudes by using LSH. I found the paper relatively creative and generally well founded and well argued. Why not use the more standard 1/t decay? Fig 1: Suspicious CIFAR100 that test objective is so much better than train objective? Why were so many of the chosen datasets have so few training examples?<BRK>In the paper, the authors concerned "more accurate gradients" and "faster convergence"; their causality is very clear (the first leads to the second), and there is no causality dilemma. Even from a computational perspective, "SDG schemes aim for computational efficiency" and "stochastic makes the convergence slow down" are not a causality dilemma. The reason behind is that the latter is the cost of the first one, just the old saying that "there is no such thing as a free lunch". 5.Occasionally, there are typos, and it is not good to use words in formulas.<BRK>  The main idea in the paper is fairly simple: The paper considers SGD over an objective of the form of a sum over examples of a quadratic loss. The paper is not well written. The presentation is much more complex that need be. This excludes NN models and even the addition of regularization to the quadratic loss can affect the effectiveness.
Reject. rating score: 3. rating score: 4. rating score: 7. <BRK>The main issues with the paper is that its contributions are not new. Restricting the type of the decoded tokens based on the programming language has already been done by the Neural Symbolic Machines of Liang et al.2017.Then Krishnamurthy et al.expanded that in EMNLP 2017 and used typing in a grammar at decoding time. But again, this is something that has been done already in the context of pointer networks and other work like See  et al.2017 for summarization and Jia et al., 2016 for semantic parsing. In general I tend to agree that using RL for this task is probably unnecessary when you have the full program as supervision.<BRK>The paper claims to develop a novel method to map natural language queries to SQL. Using a grammar to guide decoding 2. Using a new loss function for pointer / copy mechanism. For each output token, they aggregate scores for all positions that the output token can be copied from. I was also not sure why there is a need to copy items from the input question, since all SQL query nouns will be present in the SQL table in some form. The references need work. There are repeated entries for the same reference (one form arxiv and one from conference).<BRK>Some discussion of these issues would be helpful. How does the additional preprocessing done by the authors affect the performance of the original baseline system of Zhong et al.? Overall, even though the architecture is not very novel, the paper is well written and the results are strong; as such, I d recommend the paper for acceptance. Some questions:  How can the proposed approach scale to more complex queries (i.e., those not found in WikiSQL)?
Reject. rating score: 2. rating score: 3. rating score: 3. <BRK>This paper proposes a new method for solving the analogy task, which can potentially provide some insight as to why word2vec recovers word analogies. ASSUMPTIONSThe author assumes that there the community does not understand why word embedding methods such as word2vec recover analogies. I believe that, in fact, we do have a good understanding of this phenomena.<BRK>This paper presents, and analyzes, a method for learning word relationships based on co occurrence. Also Assumption 1 does not actually claim what the text says it claims (the text says words outside the window are *not* semantically related, but the assumption does not actually say this) and furthermore is soon discarded and only the frequency of noun occurrences around co mentions is used.<BRK>I would expect this question to be explored, even if the answer is negative. This sentence seems dangerous, and the claim about humans is not really treated in the article itself. It would be much easier for the reader to simply put into plain terms what the algorithm does.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. <BRK>The manuscript proposes a generative approach to detect which samples are within vs. out of the sample space of the training distribution. The manuscript is reasonably well written overall, though some of the writing could be improved e.g.a clearer description of the cost function in section 2. The manuscript also does a good job giving pointers to related prior work. In particular, if a proportional number of samples is generated for the 50x50 case, I would expect the plots to be similar. Could the authors comment on how the modifications affect prediction score calibration?<BRK>This paper proposes a new method of detecting in vs. out of distribution samples. One thing missing is a discussion of how this approach is related to semi supervised learning approaches using GANS where a generative model produces extra data points for the classifier/discriminator. This paper proposes a different approach (with could be combined with these methods) based on a new training procedure.<BRK>I have read authors  reply. I upgrade my score to 6. This paper presents a novel approach to calibrate classifiers for out of distribution samples. The problem setting is new and objective (1) is interesting and reasonable. Suppose that theta is set appropriately so that p_theta (y|x) gives a uniform distribution over labels for out of distribution samples. Because of the construction of U(y), which uniformly assign labels to generated out of distribution samples, the conditional probability p_g (y|x) should always be uniform so p_g (y|x) divided by p_theta (y|x) is almost always 1.
Reject. rating score: 1. rating score: 2. rating score: 3. <BRK>The main concern is that this looks like a class project rather than a scientific paper.<BRK>If this is still enough for ICLR, the paper could be okay. This makes it even likely that the same subject with the same disguise appears in the training and test set.<BRK>This paper is an application paper on detecting when a face is disguised, however it is poorly written and do not contribute much in terms of novelty of the approach. To make the paper better, more empirical results are needed.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>A thorough investigation on Info Bottleneck and deep learning, nice to read with interesting experiments and references. Even though not all of the approach is uncontroversial (as the discussion shows), the paper contributes to much needed theory of deep learning rather than just another architecture. It would have been good to see a discussion whether another measurement that would be useful for single sided saturating nonlinearities that do show a compression (eg information from a combination of layers), from learnt representations that are different to representations learnt using double sided nonlinearities. Regarding the finite representation of units (as in the discussion) it might be helpful to also consider an implementation of a network with arbitrary precision arithmetic as an additional experiment. Overall I think it would be nice to see the paper accepted at the very least to continue the discussion.<BRK>The authors address the issue of whether the information bottleneck (IB) theory can provide insight into the working of deep networks. They show, using some counter examples, that the previous understanding of IB theory and its application to deep networks is limited. PROS: The paper is very well written and makes its points very clearly. Since it clearly elucidates the limitations of IB theory in its ability to analyse deep networks, I think it is a significant contribution worthy of acceptance. Some detailed comments:In section 2, the influence of binning on how the mutual information is calculated should be made clear. A justification for the choice made for binning the relu case would be helpful. If networks with small weights are able to learn most datasets, the arguments given in this section wouldn t be applicable in its entirety. Additionally, figures that show the phase plane dynamics for other non linearities e.g.relu+ or sigmoid, should be added, at least in the supplementary section. It is mentioned in the first line of page 7 that batch gradient descent is used, but it is not clear why SGD couldn t have been used to keep things consistent. This applies to figure 4 too.<BRK>This paper presents a study on the Information Bottleneck (IB) theory of deep learning, providing results in contrasts to the main theory claims. Instead, the results provided by this paper show that: the generalization can happen even without compression; that SDG is not the primary factor in compression; and that the compression does not necessarily occur after the ‘fitting phase’. The main concern is that the paper is built to argue against another theoretical work, raising a substantial discussion with the authors of the IB theory. There are, moreover, some open questions that are not fully clear in this contribution:1)	To evaluate the mutual information in the ReLu networks (sec.2) the authors discretize the output activity in their range. 3)	What are the main conclusions or impact of the present study in the theory of neural networks?
Reject. rating score: 4. rating score: 6. rating score: 6. <BRK>This paper studies the generalization properties of 2 layer neural networks based on Fourier analysis. Studying the generalization property of neural network is an important problem and Fourier based analysis is a promising direction, as shown in (Lee et al., 2017). However, I am not satisfied with the results in the current version. 2) Even if for sin activation functions, the analysis is NOT complete. The authors claimed in the abstract that gradient based methods will converge to generalizable local minima.<BRK>This paper presents a theoretical analysis for 2 layer neural networks (NNs) through a spectral approach. Specifically, the authors develop a Fourier based generalization bound. (1) The scope is a bit limited. Is there an essential difficulty in extending the result here to NNs with more layers?<BRK>This leads to generalization for 2 layer networks with appropriate bounded size. The idea of applying the Fourier based method to generalization is interesting.
Reject. rating score: 4. rating score: 4. rating score: 6. <BRK>Summary: The paper demonstrates the use of a quantum annealing machine to solve a free energy based reinforcement learning problem. Relevance: RL, probabilistic models, and function approximators are all relevant topics. However, the focus of the paper seems to be on parts (like hardware aspects) that are not particularly relevant to the ML community.<BRK>However, it is an exciting research area and this paper is an interesting demonstration of the feasibility of using quantum annealers for reinforcement learning. It may be better suited to a workshop specific to quantum machine learning methods.<BRK>* The fact that the simulation on a classical computer agrees with the one on a quantum computer is promising, but I would say that this shows that, so far, there is not yet a clear advantage in using a quantum computer. Other than this, the paper is interesting, certainly correct, and provides a nice perspective on the future of learning with quantum computers. I feel, however, but it might be a bit far from the main interest of the conference. * While i liked the introduction of the quantum Boltzmann machine, I would be happy to learn what they can do?
Reject. rating score: 2. rating score: 4. rating score: 4. <BRK>But the same efficient search is possible in many of the classic "discriminatively trained" KB completion models also. The authors seem unaware of a large literature on "knowledge base completion."<BRK>My main concern about the paper is that it seems that the authors have tuned the hyperparameters and tested on the same validation set.<BRK>The authors suggest using a variational autoencoder to infer binary relationships between medical entities. 2) The word embeddings used seem to be sufficient to capture the "knowledge" included in the corpus.
Reject. rating score: 6. rating score: 6. rating score: 6. <BRK>So my rating based on the message of the paper would be 6/10. Most of the training takes place without the skip connection. In (B), the `without skip connections  modifies `deep feed forward networks  and suggests that the network trained has no skip connections. Again, please reword or include evidence. "where the proposed method is shown to outperform many architectures without skip connections" Again, this sentence makes no sense to me. +++ ResNet scaling +++There is a crucial difference between VANs and ResNets. Please make this more clear.<BRK>multipliers to 1 at the end of training? If not, I’d be curious as to why. Unfortunately, this seems to be a simple idea that doesn t work as well as the simpler idea (ResNets) that inspired it.<BRK>The paper does use skip connections, but the difference is that they are phased out over training. Original Review Summary:The contribution of this paper is a method for training deep networks such that skip connections are present at initialization, but gradually removed during training, resulting in a final network without any skip connections. It is proposed to optimize the formulation using the method of Lagrange multipliers.
Reject. rating score: 4. rating score: 5. rating score: 6. <BRK>The paper is well motivated and written. However, there are several issues. 1b.Why should we keep increasing the regularization constant beyond a limit? Is this for compressing the networks (for which there are alternate procedures), or anything else. 2.The proposed experiments are not very conclusive. Secondly, more datasets including imagenet needs to be tested. Thirdly, it is not clear what Figure 5 means in terms of goodness of learning.<BRK>The work was prompted by  an interesting observation: a phase transition can be observed in deep learning with stochastic gradient descent and Tikhonov regularization. When the regularization parameter exceeds a (data dependent) threshold, the parameters of the model are driven to zero, thereby preventing any learning. The authors then propose to moderate this problem by letting the regularization parameter to be zero for 5 to 10 epochs, and then applying the "strong" penalty parameter. In their experimental results, the phase transition is not observed anymore with their protocol. The proposed protocol is reported to work well, but since it is really ad hoc, it fails to convince the reader that it provides the right solution to the problem.<BRK>The results seem to show that a delayed application of the regularization parameter leads to improved classification performance. The proposed scheme, which delays the application of regularization parameter, seems to be in contrast of the continuation approach used in sparse learning. In the latter case, a stronger parameter is applied, followed by reduced regularization parameter. It would be interesting to see whether deep networks can benefit from the continuation approach, and the strong regularization parameter may not be an issue because the regularization parameter decreases as the optimization progress goes on.
Invite to Workshop Track. rating score: 6. rating score: 5. rating score: 5. <BRK>QualityThe method description, particularly about reference ambiguity, I found difficult to follow. But that seems to be one of the main goals of the paper. What does “different common factor” mean? Overall the argument seems plausible   pairs of images in which a single factor of variation changes have a reference ambiguity   but the details are unclear. OriginalityThe model is very similar to Mathieu et al, although using image pairs rather than category labels directly. SignificanceDisentangling factors of variation with weak supervision is an important problem, and this paper makes a modest advance in terms of the model and potentially in terms of the theory. The method description and the description of reference ambiguity are unclear.<BRK>This paper studies the challenges of disentangling independent factors of variation under weakly labeled data. A term "reference ambiguity" is introduced, which refers to the fact that there is no guarantee that two data points with same factor of variation will be mapped to the same point if there is only weakly labeled data to that extend. I am having a hard time understanding the message of the paper. The proof in section 3.1, although elementary, is nice. A rewrite with strong connection between the theory and the experiments is required.<BRK>The paper considers the challenges of disentangling factors of variation in images: for example disentangling viewpoint from vehicle type in an image of a car. They then go on to suggest an interesting AE+GAN architecture where the main novelty is the idea of taking triplets such that the first two instances vary in only one factor of variation, while the third instance varies in both from the pair. It s only defined later in the paper. Pros:1.Interesting use of constructed triplets. What is the goal of the training? 14.The weak labels are never properly defined and are discussed in a vague manner. Instead of "Generative Adversarial Nets Goodfellow et al.(2014)", you should have "Generative Adversarial Nets (Goodfellow et al., 2014)" In fact, I suspect that this is what allows their method to succeed.
Invite to Workshop Track. rating score: 7. rating score: 6. rating score: 5. rating score: 5. <BRK>One thing that needs to be clarified is that, if the network is not targeted at solving certain ODEs, then why is the intuition from ODE matters? The paper does not motivate readers in this perspective. Therefore, the potential impact of the this paper to vision community is probably limited. The proposed new architecture can be considered in future architecture designs.<BRK>The authors proposed to bridge deep neural network design with numerical differential equations. This paper is interesting in general and it will be useful to design new and potentially more effective deep networks. Is it possible to show stability of the architecture of deep networks based on their associated ODEs? Are there any limitations of this interpretation or discrepancy due to the weak approximation?<BRK>(a) It helps if we faithfully discretize the ODE. Introducing multi step discretization is novel. Thus the evidence is only partial, i.e., we still don t know why the connection between ODE and ResNet is helpful at all. Weakness: Agreed that LM methods are better approximations of the ODEs.<BRK>The authors cast some of the most recent CNN designs as approximate solutions to discretized ODEs. On that basis, they propose a new type of block architecture which they evaluate on CIFAR and ImageNet. In addition, it is unclear from the paper how the proposed approach (LM architecture) compares to the recent works, what are the benefits and gains from casting is as a direct relative to the multi step scheme in numerical ODEs. How do the different approximations relate in terms of convergence rates, error bounds etc.?
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>(I wouldn t be surprised if you disagree with what the main takeaways are; I found the flow of the paper somewhat disjointed, and had something of a hard time identifying what the "point" was.)<BRK>It is unclear to me what the contribution of this paper is.<BRK>To me, the paper seems a survey paper instead of a research one. However, the author did not claim what the main contributions are.
Reject. rating score: 4. rating score: 5. rating score: 7. <BRK>2011.However, there is no clear indication that there is an improved exploration policy. The paper says the problem is fully observable, but fails to make explicit if this is *individually* fully observable, or jointly. This notion of a correlation device also highlights to potential relation to methods to learn/compute correlated equilibria. (2004)."Compared to the single agent RL setting, multi agent RL poses unique difficulties. "Coordinated reinforcement learning."<BRK>References:[1] Lauer, M., Riedmiller, M.: An algorithm for distributed reinforcement learning in cooperativemulti agent systems. Detailed comments: "we restrict to fully cooperative MDPs that are fully observable, deterministic and episodic." However, in the current form of the paper this is a questionable claim.<BRK>Other remarks: Empirical result show a clear advantage for this method over the baselines. It also seems that by assuming a shared model, shared global state and a fully cooperative problem, the authors remove many of the complexities of a multi agent system. The method presented in the paper seems quite novel. The comparison to existing approaches using variational inference is quite brief.
Invite to Workshop Track. rating score: 7. rating score: 5. rating score: 2. <BRK>This paper proposes a multi view semi supervised method. My understanding is that the key factor is not the so called k views (as in the first sight, this method resembles conventional ensemble learning very much), but the smoothing distribution around some input x (consistency related loss).<BRK>The paper proposes a ’Cross View training’ approach to semi supervised learning. In the teacher student framework for semi supervised learning, it introduces a new cross view consistency loss that includes auxiliary softmax layers (linear layers followed by softmax) on lower levels of the student model. There are no discussions of insights and why the proposed strategy work, for what cases it will work, and for what cases it will not work? It is unfair to compare to the baseline models with much fewer parameters. The current results show that the improvements are mostly from VAT, instead of CVT.<BRK>This paper presents a so called cross view training for semi supervised deep models. Experiments were conducted on various data sets and experimental results were reported. Cons:* The novelty of this paper is marginal. Leveraging the sub regions of the image to improve performance is not new and has been widely studied in image classification and retrieval. * The proposed approach suffers from a technical weakness or flaw.
Reject. rating score: 5. rating score: 5. rating score: 5. <BRK>I still keep my original rating of the paper because the method presented is heavily based on previous works, which limits the novelty of the paper. _______________ORIGINAL REVIEW:This paper proposes to use a clipping activation function as a replacement of ReLu to train a neural network with quantized weights and activations. The experiments are thorough, and report results on many datasets, showing that PACT can reduce down to 4 bits of quantization of weights and activation with a slight loss in accuracy compared to the full precision model.<BRK>The parameterized clipping activation (PACT) idea is very clear: extend clipping activation by learning the clipping parameter. Then,  PACT is combined with quantizing the activations. The proposed technique sounds. The performance improvement is expected and validated by experiments.<BRK>The idea is interesting and novel that PACT has not been applied to compressing networks in the past. The results from this paper is also promising that it showed convincing compression results. The experiments in this paper is also solid and has done extensive experiments on state of the art datasets and networks. Overall the paper is a descent one, but with limited novelty.
Reject. rating score: 5. rating score: 5. rating score: 5. <BRK>The paper proposes to use LSH to approximate softmax, which greatly speeds up classification with large output space. A few questions about the implementation,(1) As stated in the manuscript, the proposed method contains three steps, hashing, lookup and distance. Is it a typo? (1) More baselines be evaluated and compared. It is one of the best datasets to test the scalability of language models.<BRK>In this paper, the authors propose a new approximation of the softmax, based on approximate nearest neighbors search and sampling. Weak evaluations  I believe that the empirical evaluation of section 6 are a bit weak. In fact, the main contribution of this paper is to show how to apply the technique of Mussmann et al.in the setup of neural network. Second, the authors do not report any runtime numbers for their method and the baselines on GPUs.<BRK>Authors present LSH Softmax   a fast, approximate nearest neighbor search based, approach for computing softmax that utilizes the Gumbel distribution and it relies on an LSH implementation of the maximum inner product search. What about a performance comparison on an extrinsic task? This in turn gives the overall impression that their work is a simple addition to it.
Reject. rating score: 6. rating score: 6. rating score: 6. <BRK>MNIST, CIFAR 10 are too simple tasks perhaps suitable for debugging but not for a comprehensive validation of quantization/compression techniques. A number of general statements is made based on MNIST data, such as on page 3 when comparing GMM and k means priors, on page 7 and 8 when claiming that parameter tying and sparsity do not act strongly to improve generalization. In addition, by making a list of all hyper parameters you tuned I am not confident that your claim that this approach requires less tuning. Additional comments:(a) you did not mention student teacher training(b) reference to previously not introduced K means prior at the end of section 1(c) what is that special version of 1 D K means? (d) Beginning of section 4.1 is hard to follow as you are referring to some experiments not shown in the paper. (f) Any comparison to a classic compression technique would be beneficial. (g) You are referring to a sparsity at the end of page 8 without formally defining it.<BRK>This is yet another paper on parameter tying and compression of DNNs/CNNs. The key idea here is a soft parameter tying under the K means regularization on top of which an L1 regularization is further imposed for promoting sparsity. In Eq.3, it appears that the L1 regularization is always used in optimization. 2.A follow up question on K means and L1. 4.It would be helpful to show if the proposed technique work well on sequential models like LSTMs.<BRK>As the authors mentioned, weight sharing and pruning are not new to neural network compression. It is much less convincing with no comparison result of compression on large neural networks and large datasets. As discussed in the paper, it is a trade off between accuracy and compression. The network could be compressed to very small size but with significant accuracy loss. This paper also considers quantization for compression which is related to this work.
Invite to Workshop Track. rating score: 7. rating score: 5. rating score: 4. <BRK>Overall, this research direction seems fruitful, both in terms of different applications and in terms of extra machine learning that could be done to improve performance, such as ensuring that the optimization doesn t leave the manifold of reasonable designs. * You can afford to spend lots of time interacting with the deep network in order to optimize it really well with respect to the inputs. Why not do lots of random initializations for the optimization? See, for example, uses of Langevin dynamics as a non convex optimization method. This is of course confounded by the necessary cost to construct the training set, which is necessary for the gradient descent approach.<BRK>It uses back propagation (gradient descent) to improve the design. This paper targets at a potentially very useful application of neural networks that can have real world impacts. It mixes the method, the heat sink example and the airfoil example throughout the entire paper. They might be true for a narrow field of application. But in general, I think they are not quite correct. 3) The key of this paper is to approximate the dynamics using neural network (which is a continuous mapping) and take advantage of its gradient computation.<BRK>1.This is a good application paper, can be quite interesting in a workshop related to Deep Learning applications to physical sciences and engineering2. Lacks in sufficient machine learning related novelty required to be relevant in the main conference3. 4.However, this paper introduces two different types of networks for "parametrization" and "physical behavior" mapping, which is interesting, can be very useful as surrogate models for CFD simulations 5. what are the effects of regulariazations in this regard?
Reject. rating score: 3. rating score: 5. rating score: 5. <BRK>The precise mathematical definition of this term should be clarified somewhere, since there are several cost functions described in the paper, and it s unclear which terms are actually being plotted here.<BRK>In its present state, the paper is very hard to parse, and the evaluation appears too rushed for me to be able to deduce how well the method works. Is that true? Or are there other modifications?<BRK>The paper is generally well written, but has some bugs and typos. There are parts of the paper that should be moved to an appendix to accommodate the page limit.
Reject. rating score: 5. rating score: 5. rating score: 5. <BRK>Frankly this is not very surprising; matrix factorization is very powerful, and these simple word similarity tasks are well suited for matrix factorization. The method is shown to be superior in tasks of 3 way outlier detection, supervised analogy recovery, and sentiment analysis. The multiplicative relation analysis is interesting, but at this point it is not clear to me why multiplicative is better than additive in either performance or in giving meaningful interpretations of the model. In conclusion, because the novelty is also not that big (CP decomposition for word embeddings is a very natural idea) I believe the evaluation and analysis must be significantly strengthened for acceptance.<BRK>The paper proposes to extend the usual PPMI matrix factorization (Levy and Goldberg, 2014) to a (3rd order) PPMI tensor factorization. The paper s most clear contribution is the observation that the objective results in multiplicative compositionality of vectors, which indeed does not seem to hold in CBOW. While the paper reports superior performance, the empirical claims are not well substantiated. The proposed approach is simple and has an appealing compositional feature, but the work is not adequately validated and the novelty is somewhat limited. The observation on multiplicative compositionality is the main strength of the paper.<BRK>The approach is evaluated experimentally on several tasks such as outlier detection, supervised analogy recovery, and sentiment analysis tasks. CLARITY: The paper is very well written and is easy to follow. However, some implementation details are missing, which makes it difficult to assess the quality of the experimental results. ORIGINALITY: The idea of using a pointwise mutual information tensor for word embeddings is not new, but the authors fairly cite all the relevant literature. My understanding is that the main novelty is the proposed tensor factorization algorithm and extensive experimental evaluation. The experimental evaluation presents indeed interesting results.
Reject. rating score: 5. rating score: 5. rating score: 8. <BRK>Pros:  The model achieves SOTA on SQuAD among published papers. Given the above two cons, the main contribution of the paper is 1.1% improvement over previous state of the art. I think this is a valuable engineering contribution, but I feel that it is not well suited / sufficient for ICLR audience. The paper is overall well written and clear.<BRK>(1) One concern I have for this paper is about the evaluation. The paper only evaluates the proposed method on the SQuAD data with systems submitted in July 2017, and the improvement is not very large. As a result, the results are not suggesting significance or generalizability of the proposed method. Because this paper is not comparing to the state of the art, no specification of the leaderboard version may confuse the other reviewers and readers.<BRK>This paper introduces a fairly elaborate model for reading comprehension evaluated on the SQuAD dataset. The model is shown to improve on the published results but not as of submission leaderboard numbers. The main weakness of the paper in my opinion is that the innovations seem to be incremental and not based on any overarching insight or general principle.
Reject. rating score: 4. rating score: 4. rating score: 4. <BRK>This paper addresses the question of unsupervised clustering with high classification performance. They propose a deep variational autoencoder architecture with categorical latent variables at the deepest layer and propose to train it with modifications of the standard variational approach with reparameterization gradients. The proposed model is a simple variant on the standard VAE models (see for example the Ladder VAE https://arxiv.org/abs/1602.02282 for deep models with multiple stochastic layers). This would be OK if a thorough evaluation on at least two other datasets showed similar improvements as the lymphocytes dataset. Minor questions / concerns:  The authors claim in the first paragraph of 3.2 that deterministic mappings lack expressiveness.<BRK>The authors propose a deep hierarchical model for unsupervised classification by using a combination of latent continuous and discrete distributions. Although, the detailed description of flow cytometry and chronic lymphocytic leukemia are appreciated, they are probably out of the scope of the paper or not relevant for the presented approach. From Figures 3 and 4 for example, it is difficult to grasp the performance gap between the proposed approach and \beta VAE. The results in Tables 1 and 2 are not very convincing without clarity on the selection of the thresholds for each of the models.<BRK>SummaryThe authors propose a hierarchical generative model with both continuous and discrete latent variables. The results seem very promising as the model clearly separates the two types of cells. But more baseline experiments are needed to assess the robustness of the results. NoveltyThe model introduced is a variant of a deep latent Gaussian model, where the top most layer is a discrete random variable. Given the extensive literature on combining discrete and continuous latent variables in VAEs, the novelty factor of the proposed model is quite weak.
Reject. rating score: 3. rating score: 5. rating score: 7. <BRK>The main idea is essentially learning the manifold of the data distribution and using Gaussian mixture models (GMMs) and dictionary learning to train a "reformer" (without seeing adversarial examples) to detect and correct adversarial examples. Although the motivation is well grounded, there are two major issues of this work: (i) limited  novelty   the idea of unsupervised manifold projection method has been proposed in the previous work; and (ii) insufficient attack evaluations   the defender performance is evaluated against weak attacks or attacks with improper parameters. The details are as follows. 2.Insufficient attack evaluations   the attacks used in this paper to evaluate the performance of PCL are either weak (no longer state of the art) or incorrectly implemented. For FGSM, the iterative version proposed by (Kurakin, ICLR 2017) should be used. Carlini Wagner attack is still strong, but the authors only use 40 iterations (should be at least 500) and setting the confidence 0, which is known to be producing non transferable adversarial examples.<BRK>This paper present a method for detecting adversarial examples in a deep learning classification setting. The idea is to characterize the latent feature space (a function of inputs) as observed vs unobserved, and use a module to fit a  cluster aware  loss that aims to cluster similar classes tighter in the latent space. Which parameters are fit using the fine tuning loss described on page 3? Are the checkpointing modules designed to only detect adversarial examples? Technical: It is hard to tell how some of the components of this approach are technically justified. Novel: I am not familiar enough with adversarial deep learning to assess novelty or impact.<BRK>Both are based on the claim that adversarial examples lie outside a certain sub space occupied by the natural image examples, and modeling this sub space hence enables their detection. The input defender is based on sparse coding, and the latent defender on modeling the latent activity as a mixture of Gaussians. However, an ablation study of the latent defender is missing: Specifcially, it is not clear a) how much does stage 2 (model refinement with clusters)  contribute to the accuracy (how does the model do without it?And 3) how important is the HDDA and the specific variant used (which is not clear) important: is it important to model the Gaussians using a sub space? I went on to read the relevant HDDA paper, but it is also not clear which of the model variants presented there is used in this paper. However, this form is not appropriate for the case presented at this paper, since the method presented only models one of these PDFs (Specifically p(x | W1)    there is not generative model of p(x|W2)).
Reject. rating score: 4. rating score: 5. rating score: 7. <BRK>Strengths:  Unlike most previous approaches that suffer from significant accuracy drops for good feature map compression, the proposed method achieves reductions in feature map sizes of 1 order of magnitude at effectively no loss in accuracy. Weaknesses:  The primary downside is that the approach requires a specialized architecture to work well (all experiments are done with SqueezeNets). Thus, the approach is less general than prior work, which can be applied to arbitrary architectures. From the experiments it is not fully clear what is the performance loss due to having to use the SqueezeNet architecture rather than state of the art models.<BRK>The paper reads well and the methods and experiments are generally described in sufficient detail. My main concern with this paper and approach is the performance achieved. According to Table 1 and Table 2 there is a small accuracy benefit from using the proposed approach over the "quantized" SqueezeNet baseline.<BRK>The method of this paper minimizes the memory usage of the activation maps of a CNN. : impact on runtime is not reported. + : well related to the state of the art and good comparison with other works. My understanding is that P^l has but R^l not.
Invite to Workshop Track. rating score: 7. rating score: 6. rating score: 4. <BRK>The authors propose a new gradient compression method for efficient distributed training of neural networks. The authors propose a novel way of measuring ambiguity based on the variance of the gradients. The paper is well written: clear and easy to understand. Particularly, I found it interesting to re evaluate the variance with (virtually) increasing larger batch size. The performance shown in the experiments is also impressive.<BRK>This paper proposes a variance based gradient compression method to reduce the communication overhead of distributed deep learning. Experiments on real datasets are used for evaluation. The idea to adopt approximated variances of gradients to reduce communication cost seems to be interesting. Secondly, the experimental results are unconvincing. Obviously, the learning procedure is not convergent.<BRK>The paper proposes a novel way of compressing gradient updates for distributed SGD, in order to speed up overall execution. 4.2: This section is not fully comprehensible to me. Possible because the problems above. Therefore I highly recommend including proper time comparison with a baseline in the future.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. <BRK>This paper proposes approximating the Wasserstein distance between normalized greyscale images based on a learnable approximately isometric embedding of images into Euclidean space. The paper is well written with clear and generally thorough prose. It presents a novel, straightforward and practical solution to efficiently computing Wasserstein distances and performing related image manipulations. Mention that OT stands for optimal transport in section 4.3.<BRK>The paper presents a simple idea to reduce the computational cost of computing Wasserstein distance between a pair of histograms. Specifically, the paper proposes learning an embedding on the original histograms into a new space where Euclidean distance in the latter relates to the Wasserstein distance in the original space. Despite simplicity of the idea, I think it can potentially be useful practical tool, as it allows for very fast approximation of Wasserstein distance. The empirical results show that embeddings learned by the proposed model indeed provide a good approximation to the actual Wasserstein distances. The paper is well written and is easy to follow and understand. My biggest concern however is the applicability of this approach to high dimensional data.<BRK>The paper proposes to use a deep neural network to embed probability distributions in a vector space, where the Euclidean distance in that space matches the Wasserstein distance in the original space of probability distributions. The method is straightforward, and clearly explained. Two analyses based on Wasserstein distances (computing barycenters, and performing geodesic analysis) are then performed directly in the embedded space.
Accept (Oral). rating score: 8. rating score: 7. rating score: 7. <BRK>I only got access to the paper after the review deadline; and did not have a chance to read it until now. The paper is reasonably well written, and tackles an important problem. Besides the missing literature mentioned by other reviewers (all directly relevant to the current paper), the authors should also comment on the availability of accelerated methods inn the finite sum / ERM setting. There, the questions this paper is asking are resolved, and properly modified stochastic methods exist which offer acceleration over SGD (and not through minibatching).<BRK>This question is also valid when it comes to other optimization schemes that are designed for deep learning problems. For instance, Entropy SGD and Path SGD should be mentioned and compared with. As a consequence, the literature analysis is insufficient.<BRK>Significance: I think that this paper is important because it shows that the classical HB method cannot achieve acceleration in a stochastic regime. Clarity: I was easy to read the paper and understand it. I think you are missing 1/2 in the definition of the function. in "SGD We validate"....4.
Reject. rating score: 4. rating score: 5. rating score: 5. <BRK>The paper proposes an unsupervised structure learning method for deep neural networks. However, the reviewer indeed finds a major technical flaw in the paper. Besides that, the reviewer also finds unfair comparisons in the experiments. Did the authors compare with fully connected network with similar number of neurons? But it is not a fair comparison with vanilla network. 3.In Fig.6, again, comparing the learned structure with fully connected network by keeping parameters to be similar and resulting in large difference of the number of neurons is unfair from my point of view. No other structure learning methods are compared with.<BRK>The presentation of the paper could be improved. My main comments are related to the experimental section:  Section 5 highlights that experiments were repeated 5 times; however, the standard deviation of the results is only reported for some cases. Would it be possible to provide the number of parameters of the vanilla model, the pre trained feature extractor and the learned structure separately? In section 5.2., there is only one sentence mentioning comparisons to alternative approaches. It seems that the main focus of the experiments is to highlight the parameter reduction achieved by the proposed algorithm.<BRK>The width of each layer (determined by number of neurons in each group) is still tuned as a hyper parameter. If it is the gain in accuracy compare with other learn to learn methods and show that you achieve same or higher accuracy. Specially without reporting the standard deviation. The rest of the paper is also about number of parameters. Therefore, the experiments in this section should be in terms of number of parameters as well.
Reject. rating score: 5. rating score: 5. rating score: 6. <BRK>This paper proposed a probabilistic framework for domain adaptation that properly explains why maximizing both the marginal and the conditional log likelihoods can achieve desirable performances. It is not clear why mSDA cannot handle time series data but DAuto can. 3.If my understanding is not wrong, the proposed DAuto is just a simple combination of three losses (i.e.prediction loss, reconstruction loss, domain difference loss).<BRK>The authors propose a probabilistic framework for semi supervised learning and domain adaptation. The proposed DAuto is essentially DANN+autoencoder. It would be interesting to see if the additional auto encoder part help address the issue. DAuto does seem to offer more boost in domain pairs that are less similar.<BRK>The only criticism that I have towards this analysis is that the concept of shared parameter between the discriminative and predictive model (denoted by zeta in the paper) disappear when it comes to designing the learning model. They successfully show that using autoencoder can help to learn a good representation for discriminative domain adaptation tasks. Also, a comparison with the Generative Adversarial Networks of Goodfellow et al.(2014) would be a plus.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. rating score: 6. <BRK>The paper deals with the increasingly popular GAN approach to constructing generative models. The authors prove a theorem related to the properties of the Sobolev norm, and express it in terms of the component wise conditional distributions.<BRK>This paper designs a new IPM(Integral Probability Metric) that uses the gradient properties of the test function.<BRK>The proposed penalty is forcing the expected squared norm of the gradient to be equal to 1. Pros:  The paper provides a nice overview of WGAN GP, Fisher GAN and Sobolev GAN.<BRK>They relate this MMD to the Cramer and Fisher distance and then produce a recipe for training GANs with this sort of function class. However, I finished reading the paper wondering why one would want to trust this GAN over any of the other GANs.
Invite to Workshop Track. rating score: 7. rating score: 5. rating score: 4. <BRK>The results establish the standard issues noticed in training  GANs. Is the noise added in each step? Comments:1) This is an interesting paper studying the dynamics of GANs on a simpler model (but rich enough to display mode collapse).<BRK>Since this is the main meat of the paper (i.e.no methodological innovations), I feel that this is too little an innovation for deserving publication in ICLR2018. Although I have not checked all the mathematical fine details, the approach/proof looks sound (although it is not at all clear too me why the choice of gradient step sizes does not play a more important roles the the stated results).<BRK>This paper tried to make a new insight of GAN from theories and I think their approach is a good first step to build theories for GAN. However, I believe this paper is not enough to be accepted. The main reason is that the main theorem (Theorem 4.1) is too restrictive. 3.The authors could make more interesting results using the current ingredients.
Reject. rating score: 2. rating score: 3. rating score: 4. <BRK>The paper claims that it is not well known that TD with function approximation ignores part of the gradient of the MSVE.<BRK>This paper is reasonably readable. If so, this further means that this is not a true gradient.<BRK>I suspect this is the actual cause for it not converging to zero for Baird s, although please correct me if I m wrong on that.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>In this paper, the authors proposed the Stein gradient estimator, which directly estimates the score function of the implicit distribution. The experiments in Section 4.3 seems to be a bit out of context. The novelty of this work consists of an approach based on score matching and Stein’s identity to estimate the gradient directly and the empirical results of the proposed method on meta learning for approximate inference and entropy regularized GANs.<BRK>), I  think the paper should be accepted. I am giving an indifferent score mostly because I did not follow most of the details. In addition, the derivation of the estimator in Section 3 was also sloppy. The idea is based off the Stein s identity, for which the authors propose a kernelized solution. The authors present applications in Bayesian NNs and GANs. Summary The authors present a method for estimating the gradient of some training objectivefor generative models used to sample data, such as GANs.<BRK>Some boundary condition should be assumed to assure that integration by parts works properly. Some methods were introduced and a new method using Stein identity was proposed. The setup of the trasnductive learning was introduced to add the prediction power to the proposed method. This is an interesting approach to estimate the score function for location models in a non parametric way.
Reject. rating score: 4. rating score: 5. rating score: 7. <BRK>The paper describes an end to end differentiable model to answer questions based on a knowledge base. I will be convinced if evaluation can be done on a real data set. Minor complaints: The paper does not compare to NMN, or a standard semantic parser. That is true only if the groundings are not considered part of the parse. They actually enumerate all parses based on types, and then find the right groundings for the best parse. Also, some work on CLEVR had some questions collected from humans, maybe you can try to evaluate on that.<BRK>This paper presents a model for visual question answering that can learn bothparameters and structure predictors for a modular neural network, withoutsupervised structures or assistance from a syntactic parser. Results on VQA, CLEVR, andNLVR (even if they re not all state of the art!) I think the core technical idea here is really exciting! But the experimentalvalidation of the approach is a bit thin, and I m not ready to accept the paperin its current form.<BRK>This paper proposes for training a question answering model from answers only and a KB by learning latent trees that capture the syntax and learn the semantic of words, including referential terms like "red" and also compositional operators like "not". There  is now the Cornell NLVR dataset that is more challenging from a language perspective and it would be great to have an evaluation there as well. The authors indeed state that it is state of the art on clevr. This could have been really nice if one could do that. The authors have a figure of a purported tree, but where does this tree come from?
Reject. rating score: 3. rating score: 4. rating score: 7. <BRK>Paper claims it is a deep learning method, however it is not an end to end network. The main issue of the paper is lack of technical contributions. Why is that? See   paper from CVPR2012    Arandjelović, Relja, and Andrew Zisserman. If features are L2 normalized, why you need to normalize the features again in equation 5? Technically, there is nothing new here.<BRK>It is known that in such a situation, cosine similarity is equivalent to Euclidean distance. The motivation should be further explained. (3) The whole pipeline is not trained in an end to end manner. It requires some other features as the input (RMAC used in this work), and three stage training. It is interesting to see some more experiments where image pixels are the input. (4) The algorithm is not comparable to the state of the art. Some representative papers have reported much better performances on the datasets used in this paper.<BRK>P2.The authors make a very nice effort in motivation the paper, relating it with the state of the art and funding their proposal on studies regarding human visual perception. C2.The main drawback of this approach is in terms of computation. The authors already point at this in Section 4.3. C3.I am somehow surprised that the authors did not explore also training the network that would extract the high level representations, that is, a complete end to end approach. C4.There are a couple of recent papers that include results of the state of the art which are closer and sometimes better than the ones presented in this work.
Reject. rating score: 2. rating score: 4. rating score: 5. <BRK>The proposed architecture outperforms recursive autoencoder on a self to self predicting trees, and outperforms an lstm seq2seq on En Cn translation. Comment:  The idea of tree2tree has been around recently but it is difficult to make it work. I thus appreciate the authors’ effort.<BRK>This paper presents a model to encode and decode trees in distributed representations. This is not the first attempt of doing these encoders and decoders. In fact, it has been demonstrated that it is possible to encode and decode trees in distributed structures without learning parameters, see "Decoding Distributed Tree Structures" and "Distributed tree kernels".<BRK>This paper proposes a tree to tree model aiming to encode an input tree into embedding and then decode that back to a tree. The contributions of the work are very limited. In general, it is not ready for publication. Evaluation is not very convincing. The baseline performance in MT is too low. It is unclear if the proposed model is still helpful when other components are considered (e.g., attention). Earlier work on tree kernels (in terms of defining tree distances) may be related to this work.
Reject. rating score: 3. rating score: 3. rating score: 4. <BRK>The proposed assumptions are not well motivated and seem arbitrary. The paper is very hard to read.<BRK>4.There is no theoretical novelty and the empirical one seems to be very limited, with less convincing results.<BRK>The paper proposes and evaluates a method to make neural networks for image recognition color invariant. The paper is incomplete without the appendices.
Invite to Workshop Track. rating score: 6. rating score: 5. rating score: 4. <BRK>This paper proposes a neural architecture search method that achieves close to state of the art accuracy on CIFAR10 and takes much less computational resources. In summary, getting net2net to work for architecture search is interesting. And I love the results.<BRK>For example, the RNNCell discovered can be fixed and used in other tasks, and the RNN controller for CNN architecture search could potentially be applied to other tasks too (though not reported). This paper presents a method to search neural network architectures at the same time of training. The paper can be understood with no problem.<BRK>This paper proposes a variant of neural architecture search. It uses established work on network morphisms as a basis for defining a search space. (4) Performance is worse than the best hand designed baselines. Negatives:(1) The state of the art CNN architectures are not mysterious or difficult to find, despite the paper s characterization of them being so.
Accept (Poster). rating score: 7. rating score: 6. rating score: 4. <BRK>The paper presents a clever trick for updating the actor in an actor critic setting: computing a guide actor that diverges from the actor to improve critic value, then updating the actor parameters towards the guide actor. The paper is mostly clear and well presented, except for two issues: 1) there is virtually nothing novel presented in the first half of the paper (before Section 3.3); and 2) the actual learning step is only presented on page 6, making it hard to understand the motivation behind the guide actor until very late through the paper.<BRK>The authors devise and explore use of the hessian of the(approximate/learned) value function (the critic) to update the policy(actor) in the actor critic  approach to RL. The paper has a technical focus.<BRK>Using second order methods is not an end in itself. Given the high variability of deep RL, they have not convincingly shown it performs better. The paper does not discuss the computational cost of the method. My worry is that the method is more complicated and slower than existing methods, without significantly improved performance. I recommend the authors take the time to make a much stronger conceptual and empirical case for their algorithm.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>() DiscussionOverall, I think that the proposed method is sound and well justified. Thus, the model also have a vector representation for pair of word and position in the region. I think that it would make the paper stronger.<BRK>The authors present a model for text classification. the choice of baselines is convincing. The introduction was fine. A simple explanation in the introduction would improve the writing. The authors should consider adding equation numbers. A more concise explanation of the context word region embeddings and the word context region embeddings would be to instead give the equation for r_{i,c}. Since it works so well, maybe it could be promoted into the method section?<BRK>The authors propose two specific variants of this approach, which combine the K matrix and constituent word embeddings (in a given region) in different ways. Small comments There is a typo in Figure 4   "Howerver" should be "However"*** Update after author response ***Thanks to the authors for their responses.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>This paper generalizes the sparse distributed memory model of Kanerva to the Kanerva Machine by formulating a variational generative model of episodes with memory as the prior. I found the relation to Kanerva’s original model interesting and well explained.<BRK>The paper presents the Kanerva Machine, extending an interesting older conceptual memory model to modern usage. The review of Kanerva’s sparse distributed memory in the appendix was appreciated. The experiments and results on Omniglot and CIFAR provide an interesting insight to the model s behaviour with the comparisons to VAE and DNC also seem well constructed. Overall I found the paper well written and reintroduced + reframed a relatively underutilized but well theoretically founded model for modern use.<BRK>The model is evaluated on the following tasks:* Qualitative results on denoising and one shot generation using the Omniglot dataset. What is the task being solved in Section 4.4 by the DNC and the Kanerva machine? Please state this in the main paper. Training and Evaluation: There is a mismatch in the training and evaluation procedure the implications of which I don tfully understand yet. If so, doesn t that correspond to evaluating the model under a different generative assumption? This seems at odds with models, such as DRAW, evaluate the likelihood   once at the end of the generative drawing process.
Reject. rating score: 3. rating score: 3. rating score: 5. <BRK>Main reasons:* Section 3 does not bring much value. * In 4.2, writing that both RESCAL and KBTD explain a RDF triple through a similar latent form is not an observation that could explain intrinsic similarities between the methods but the direct consequence of the deliberate choice made for f(.) at the line before. * The experiments are hard to use to validate the model because they are based on really outdated baselines. Most methods in Table 4 and 5 are performing well under their best known performance.<BRK>In addition, the paper discusses the connections of the margin and cross entropy loss and evaluates the proposed method on WN18 and FB15k. However, I m concerned about several aspects in the current form of the paper. This sampling method isn t used consistently across models and also brings its own problems, e.g., see the LCWA discussion in [4]In addition, the semantics that are introduced by the weighting scheme are not clear to me either. With regard to the significance of the contributions: Using a least squares loss in combination with tensor methods is attractive because it enables ALS algorithms with closed form updates that can be computed very fast. In this context, it is not clear to me why a tensor framework/least squares loss would be preferable. However, this doesn t have to be the case. For instance, see [5] for a detailed discussion. The margin based ranking loss has been proposed earlier than in (Collobert et al, 2011).<BRK>Contribution (1) related to the connection between margin based and negative sampling based loss functions is sort of obvious in hindsight and I am not sure if it has been not recognized in prior work (I m not very well versed in this area). I would like the authors to comment on this aspect. However, similar connections have been studied for word embedding methods. For example, prior work has shown that word embedding methods that optimize loss functions such as negative sampling can be seen as doing implicit matrix factorization of a transformed version of the word counts. Overall, the paper does have some interesting insights but it is unclear if these insights are non trivial/surprising, and are of that much practical utility.
Reject. rating score: 5. rating score: 5. rating score: 7. <BRK>To me, the most interesting bit of information in this paper was the realization that you can weight the reconstruction and KL terms of a VAE and interpret it as variational inference in a generative model with multiple copies of pixels (below Equation 7). As the paper is written now, it is not clear what the goal of the authors is. Is it representation learning? Then the paper is missing experiments to support the idea that the learned representations are in any way an improvement. However, samples can be very misleading. If the former, what was sigma and how was it chosen? The samples in this case would be dominated by the VAE, which depends on the latent state. The log likelihood would be dominated by the first term and would be minimally effected (see Theis et al., 2016). What happens if the KL term is simply downweighted but the factorial decoder is not included? The paper is well written and clear.<BRK>This is done by controlling the capacity of the autorregressive component within an auxiliary loss function. The proposed approach is a straightforward combination of VAE and PixelCNN that although empirically better than PixelCNN, and presumably VAE, does not outperform PixelCNN++. Provided that the authors use PixelCNN++ in their approach, quantitively speaking, it is difficult to defend the value of adding a VAE component to the model.<BRK>The proposed approach is straight forward, experimental results are good, but don’t really push the state of the art. But the empirical analysis (e.g.decomposition of different cost terms) is detailed and very interesting.
Reject. rating score: 3. rating score: 3. rating score: 4. <BRK>Cons:  The proposed approach follows largely the existing work and thus its technical novelty is weak. Paper presentation quality is clearly below the standard. Empirical results do not clearly show the advantage of the proposed method over state of the arts. Pros:   The network compression problem is of general interest to ICLR audience.<BRK>1.This paper proposes a deep neural network compression method by maintaining the accuracy of deep models using a hyper parameter. However, all compression methods such as pruning and quantization also have this concern. Therefore, the novelty of the proposed method is somewhat weak.<BRK>1.SummaryThis paper introduced a method to learn a compressed version of a neural network such that the loss of the compressed network doesn t dramatically change. 3.High level technical  I m confused at the first and second lines of equation (19). In the first line, shouldn t the first term not contain \Delta W ? In the second line, shouldn t the first term be \tilde{\mathcal{L}}(W_0 + \Delta W) ?
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. <BRK>Aggregating neural network weights to identify feature interactions is very interesting. 6.At least, RuleFit (Random Forest regression for getting rules + l1 regularized regression) should be used as a baseline in the experiments. In summary, the idea of using neural networks for screening pairwise and high order feature interactions is novel, significant, and interesting.<BRK>Pros:  detecting (any order / any form of) statistical interactions by neural networks is provided. The automatic cutoff determination is also proposed by using a GAM fitting based on these two networks. Is my guessing that it is a 1 10 10 10 1 network (in the experiments) correct...?<BRK>This paper presents a method to identify high order interactions from the weights of feedforward neural networks. Do the same for the synthetic functions already in the paper. By the way, what is the sample size of the current set of synthetic experiments? 4)	Why aren’t the baselines evaluated on the real datasets and heatmaps similar to figure 5 are produced?
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>Additionally, it is demonstrated thatflipout allows evolution strategies utilizing GPUs. The paper also proves that this approachreduces the variance of the gradient estimates (and in practice, flipout shouldobtain the ideal variance reduction). In a set of experiments it is demonstratedthat a significant reduction in gradient variance is achieved, resultingin speedups for training time.<BRK>In this article, the authors offer a way to decrease the variance of the gradient estimation in the training of neural networks. The numerical results in Table 1 and Table 2 also do not show a clear improvement: Flipout does not provide the best accuracy. While it is a rather simple idea which could be summarised much earlier in the  single equation (3), I really like the thoroughness and the clarity of the exposure of the idea. Any intuition on that?<BRK>The paper is well written. It is clear that the perturbations E1 and E2 are to be uniform +/ 1. Although the technical contribution of this work is relevant for network learning, several key aspects are yet to be addressed thoroughly, particularly the experiments.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>This paper proposes to extend the Prototypical Network (NIPS17) to the semi supervised setting with three possible strategies. + the paper is well written, well organized and overall easy to read+/   this work builds largely on previous work.<BRK>In this paper, the authors studied the problem of semi supervised few shot classification, by extending the prototypical networks into the setting of semi supervised learning with examples from distractor classes. The studied problem is interesting, and the paper is well written. Extensive experiments are performed to demonstrate the effectiveness of the proposed methods.<BRK>The paper solves a new semi supervised situation, which is more close to the setting of the real world, with an extension of the prototype network. Sufficient implementation detail and analysis on results. There are plenty of works on this topic [R1, R2, R3]. Another concern is that the novelty.
Reject. rating score: 5. rating score: 5. rating score: 6. <BRK>Except from a few typos here and there, the paper is overall well written. The authors also briefly discuss the problem of the little overlap between the teacher s covered state space and the learner s. A state screening function (SSF) method is proposed to drive the learner to remain in areas of the state space that have been covered by the teacher.<BRK>The goal of this paper is to do the same thing but with deterministic policies as a way of decreasing the sample complexity. The examples above come from just the abstract. The text of the paper seems even less well edited. ": The justification for filtering is pretty weak. As such, it is very difficult to assess the results. As best I can tell, the empirical results seem impressive and interesting.<BRK>The method is combined with a type of density estimation of the expert to avoid noisy policy updates. I found the paper a bit hard to read.
Reject. rating score: 3. rating score: 4. rating score: 6. <BRK>This paper proposed to use deep AE to do rating prediction tasks in recommender systems. And it would be better to show the performance of the model on implicit rating data, since it is more desirable in practice, since many industry applications have only implicit rating (e.g.whether the user watches the movie or not.<BRK>This paper presents a deep autoencoder model for rating prediction. 1.Lack of novelty. The dense output re feeding is not something particularly novel, it is more or less a data imputation procedure with expectation maximization — in fact if the authors intend to seek explanation for this output re feeding technique, EM might be one of the interpretations.<BRK>In this paper the authors present a model for more accurate Netflix recommendations (rating predictions, RMSE). In particular, the authors demonstrate that a deep autoencoder, carefully tuned, can out perform  more complex RNN based models that have temporal information. The dense re feeding technique seems to be novel with incremental (but meaningful) benefits. Cons:  Experimental results on only one dataset.
Reject. rating score: 2. rating score: 3. rating score: 3. <BRK>This paper compares 2 CNN architectures (Alexnet and a VGG variant) for the task of classifying images of lung cancer from CT scans. The comparison is trivial and does not go in depth to explain why one architecture works better than the other. No example of input data is given (what does an actual input look like). But there is no explanation of that module anywhere. Instead the authors spend most of the paper listing in wordy details the architecture of their VGG variant. Overall, the paper does not provide any insight beyond: i tried this, i tried that and this works better than that; a strong reject.<BRK>The authors compare a standard DL machine (AlexNet) with a custom CNN based solution in the well known tasks of classifying lung tumours into benign or cancerous in the Luna CT scan dataset, concluding that the proposed novel solution performs better. The paper is interesting, but it has a number of issues that prevents it from being accepted for the ICLR conference. First, the scope of the paper, in its present form, is very limited: the idea of comparing the novel solution just with AlexNet is not adding much to the present landscape of methods to tackle this problem. The whole sections 2.3 and 2.4 include only standard material unnecessary to mention given the target venue, and the references are limited and incomplete.<BRK>Major comments  I did not fully understand the motivation of the custom CNN over AlexNet. Some more description of the dataset will be helpful. Are the authors predicting the location of the malignant nodule, or are they classifying if the image has a malignant nodule? How do the authors compute a true positive? Minor comments  The paper is difficult to read, and contains a lot of spelling and grammatical errors.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. <BRK>The paper provides proof that gradient based meta learners (e.g.MAML) are "universal leaning algorithm approximators". Pro:  Generally well written with a clear (theoretical) goal  If the K shot proof is correct*, the paper constitutes a significant contribution to the theoretical understanding of meta learning. The theoretical results are not applied nor demonstrated in the empirical section and only functions as an underlying premise. I wonder if a purely theoretical contribution would be preferable (or with even fewer empirical results).<BRK>The paper tries to address an interesting question: does deep representation combined with standard gradient descent have sufficient capacity to approximate any learning algorithm. The authors provide answers, both theoretically and empirically. The presentation could be further improved. I couldn t understand the sentence "can approximate any continuous function of (x,y,x^*) on compact subsets of R^{dim(y)}" in Lemma 4.1". "The first goal of this paper is to show that f_{MAML} is a universal function approximation of (D_{\mathcal{T}},x^*)"?<BRK>Expressivity of the model or algorithm is far from the main or most significant consideration in a machine learning problem, even in the standard supervised learning scenario. Questions pertaining to issues such as optimization and model selection are just as, if not more, important. These sorts of ideas are explored in the empirical part of the paper, but I did not find the actual experiments in this section to be very compelling. Still, I think the universal learning algorithm approximator result is sufficient on its own for the paper to be accepted. The universal learning algorithm approximator result is a nice result, although I do not agree with the other reviewer that it is a  "significant contribution to the theoretical understanding of meta learning," which the authors have reinforced (although it can probably be considered a significant contribution to the theoretical understanding of MAML in particular).
Accept (Poster). rating score: 7. rating score: 6. rating score: 3. <BRK>The authors provide an insight into the discriminative and generalizable aspect of the discriminator in GANs. They analyze the weak convergence of probability measure under neural distance and generalize it to the other distances by bounding the neural distance.<BRK>The authors may argue that the referenced result deals only with MMDs, that is IPMs specified to the function classes belonging to the Reproducing Kernel Hilbert Spaces. In my point of view, the main *novel* contributions are: (a) Conditions on function classes guaranteeing that the induced IPMs are metrics and not pseudo metrics (Theorem 2.2).<BRK>When going to the next equation, the authors accidentally made the two suprema have the same variable and invoke a fortuitous but incorrect cancellation. The key arguments of the paper are as follows. I think this needs a much more serious justification.
Accept (Poster). rating score: 7. rating score: 7. rating score: 4. <BRK>The approach relies on dividing the task space into subareas (defined by task context vectors) over which individual policies are trained, but are still required to operate well on tasks outside their context. The exposition is clear and the method is well motivated. I see no issues with the mathematical correctness of the claims made in the paper. The experimental results show a convincing benefit over TRPO and Distral on a number of manipulation and locomotion tasks. It would be very useful for readers to see performance of the algorithm under other task decompositions to alleviate the worries that the algorithm is not sensitive to the decomposition choice. Would such progressive tasks decompositions work better in your framework?<BRK>The most closely related works to this one are Guided Policy Search (GPS) and "Distral", and the authors compare and contrast their work with the prior work suitably. The paper and included experiments are a valuable contribution to the community interested in solving harder and harder tasks using reinforcement learning. If the local policies are trained to convergence, (and the context omega is provided by an oracle), how well does this mixture of local policies perform? This result would be instructive to see for each of the tasks.<BRK>This paper presents a method for learning a global policy over multiple different MDPs (referred to as different "contexts", each MDP having the same dynamics and reward, but different initial state). The basic idea is to learn a separate policy for each context, but regularized in a manner that keeps all of them relatively close to each other, and then learn a single centralized policy that merges the multiple policies via supervised learning. The method is evaluated on several continuous state and action control tasks, and shows improvement over existing and similar approaches, notably the Distral algorithm. The authors define a contextual MDP setting where in addition to the initial state there is an observed context to the MDP that can affect the initial state distribution (but not the transitions or reward). Minor comments:• There are several missing words/grammatical errors throughout the manuscript, e.g.on page 2 "gradient information can better estimated".
Reject. rating score: 2. rating score: 3. rating score: 3. <BRK>This article propose to combine a form of contextual Thompson sampling policy with memory networks to handle dialog engagement in mobility interfaces. The article is however poorly written and it reflects a severe lack of  scientific methodology : the problem statement is too vague and the experimental protocol is dubious.<BRK>Ok  authors give p(\Theta|D) as a product between prior and likelihood. My main concern about this paper is that the proposal is not enough well described. But nothing is said about how it is implemented in the case of the proposed model.<BRK>This paper attempts to use contextual bandits for a dialog system. Why is it necessary? Below is a detailed review and questions for the authors:1.
Reject. rating score: 4. rating score: 5. rating score: 7. <BRK>Experiments didn t compare to any similar smoothness regularization (and my preferred would have been a comparison to graph Laplacian or total variation on graphs formed by the same clustered examples). It s also not clear either how important it is that they hand define the groups over which to minimize variance or if just generally adding smoothness regularization would have achieved the same results. Would it have fixed the Google gorilla problem?<BRK>The proposed method, which makes use of grouping information, seems reasonable and useful. It is nice that the authors use "counterfactual regularization". But I failed to see a clear, new contribution of using this causal regularization, compared to some of the previous methods to achieve invariance (e.g., relative to translation or rotation). Why do the features human cognition uses give an optimal predictive accuracy? Could the authors give more detail on this? A reference would be appreciated.<BRK>The main idea is that in several datasets there are many instances of images for the same object/person, and that taking this into account by learning a classifier that is invariant to the superficial changes (or “style” features, e.g.hair color, lighting, rotation etc.) Although I really wanted to like the paper, I have several concerns. First and most importantly, the paper is not citing several important related work. Since there is some related work, it may be also worth to compare with it, or use the same datasets.
Reject. rating score: 5. rating score: 5. rating score: 7. <BRK>Various VAEs using either PixelCNN++ or a simpler model for the encoder, decoder, or marginal distribution of a VAE are trained on MNIST (with some additional results on OMNIGLOT) and analyzed in terms of samples, reconstructions, and their rate distortion trade off. Review:I find it difficult to point my finger to novel conceptual or theoretical insights in this paper. Deeper connections between variational inference and rate distortion have been made before (e.g., Balle et al., 2017; Theis et al., 2017), while this paper merely seems to rename the reconstruction and KL terms of the ELBO. The main contributions therefore seem to be the proposed analysis of models in the R D plane, and the empirical contribution of analyzing beta VAEs. The toy example in Figure 2 is interesting. The authors write: “we are able to learn many models that can achieve similar generative performance but make vastly different trade offs in terms of the usage of the latent variable”.<BRK>  I think that VAEs are rather forced to be interpreted from an information theoretic point of view for the sake of it, rather than for the sake of a clear and unequivocal contribution from the perspective of VAEs and latent variable models themselves. How is that useful for a VAE? As mentioned earlier in the paper, there are well known problems with taking this information theory perspective, e.g.difficulties in estimating MI values, etc. "(2) an upper bound that measures the rate, or how costly it is to transmit information about the latent variable. page 8: "as show"<BRK>EvaluationPros:  While no one facet of the paper is particularly novel (as similar observations and discussion has been made by [1 4]), the paper, as far as I’m aware, is the first to formally decompose the ELBO into the R vs D tradeoff, which is natural. Moreover, it’s nice to have a clear reference for the unutilized latent space behavior mentioned in various other VAE papers. Placing such varied models (CNN vs autoregressive vs VampPrior etc) onto the same plot from comparison (Figure 3) is a valuable contribution. ConclusionsI found this paper to present valuable analysis of the ELBO objective and how it relates to representation learning in VAEs. I recommend the paper be accepted, although it could be substantially improved by including more discussion at the end.
Reject. rating score: 4. rating score: 4. rating score: 6. <BRK>The empirical evaluations do not show evidence/disprove regarding this matter. Specifically, the authors argue that adding samples in random order is as beneficial as adding them with some curriculum strategy, i.e.from easiest to hardest, or reverse. The empirical evaluations are not clear.<BRK>The experiments are conducted on the UCI dataset with mixed results. Review: My overall assessment of the paper is that it is extremely weak, both in terms of the novelty of method proposed, its impact, and the results of the experiments. Also, do we tune the hyper parameters of the model along the way? Furthermore, even under the limited set of experiments the authors conducted, the results are highly inconclusive. For example, I think Assumption (a) is quite strong and may not necessary hold in many cases.<BRK>The results on 36 data sets show that to some extent the ordering of the training instances in the Curriculum and Self paced learning is not important.
Reject. rating score: 3. rating score: 3. rating score: 4. <BRK>This paper proposes to train a classifier neural network not just to classifier, but also to reconstruct a representation of its input, in order to factorize the class information from the appearance (or "style" as used in this paper). If that s not the motivation, then what is it? Experiments on MNIST are provided to analyse what this approach learns. But most importantly, none of these results are measured in a quantitative way: they are all qualitative, and thus subjective.<BRK>The paper proposes training an autoencoder such that the middle layer representation consists of the class label of the input and a hidden vector representation called "style memory", which would presumably capture non class information. (This paper should be cited.) The results in the paper are mostly qualitative and only on MNIST.<BRK>The paper proposes combining classification specific neural networks with auto encoders. This is done in a straightforward manner by designating a few nodes in the output layer for classification and few for reconstruction. The training objective is then changed to minimize the sum of the classification loss (as measured by cross entropy for instance) and the reconstruction error (as measured by ell 2 error as is done in training auto encoders).
Reject. rating score: 3. rating score: 4. rating score: 4. <BRK>This paper proposes to improve time complexity of factorization machine.<BRK>The authors introduce a novel novel for collaborative filtering. Overall the paper feels a little bit incomplete . This is particularly apparent in the empirical study.<BRK>This paper presents a method for matrix factorization using DNNs. Is this easy to do, and can the authors demonstrate this in their experiments and show interpretable examples?
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. <BRK>This work proposes an approach to meta learning in which temporal convolutions and attention are used to synthesize labeled examples (for few shot classification) or action reward pairs (for reinforcement learning) in order to take the appropriate action. The resulting model is general purpose and experiments demonstrate efficacy on few shot image classification and a range of reinforcement learning tasks. A wide range of experiments are conducted to demonstrate performance of the proposed method. I am wondering if the authors can clarify this point. Overall, the proposed approach is novel and achieves good results on a range of tasks.<BRK>The paper proposes a general neural network structure that includes TC (temporal convolution) blocks and Attention blocks for meta learning, specifically, for episodic task learning. For results in Table 1 and Table 2, how are the confidence intervals computed? Through intensive experiments on various settings including few shot image classification on Omniglot and Mini ImageNet, and four reinforcement learning applications, the authors show that the proposed structure can achieve highly comparable performance wrt the corresponding specially designed state of the art methods.<BRK>I think comparison to such a similar model would strengthen the novelty of this paper (e.g.convolution is a superior method of incorporating positional information). My second concern is that the authors do not provide analysis and/or intuitions on why the proposed models outperform prior art in few shot learning. In retrospect, I wish the authors would have spent more time doing ablation studies than tackling more task domains. The model interweaves attention layers, akin to those used by Vaswani et al, with temporal convolution.
Reject. rating score: 3. rating score: 3. rating score: 5. <BRK>The authors ask when the hidden layer units of a multi layer feed forward neural network will display selectivity to object categories. Overall, I find the work to hint at an interesting phenomenon. For example, what are "figures 3, ?? Given that the 0.05 number is somewhat arbitrary, this seems worth checking.<BRK>Quality and ClarityThe neural networks and neural codes are studied  in a concise way, most of the paper is clear. The section on data design, p3, could use some additional clarification wrt to how the data input is encoded (right now, it is hard to understand exactly what happens). Similarly, for a larger network, you will find fewer localist codes (though this is hard to judge, as an exact definition of selectivity is missing).<BRK>The idea is interesting and the findings are intriguing. Local codesincrease understandability and could be important for betterunderstanding natural neural networks. Understanding how local codesform and the factors that increase their likelihood is criticallyimportant. This is a good start in that direction, but still leavesopen many questions. The issues raised in the Conclusions section arealso very interesting   do the local codes increase with networksthat generalize better, or with overtrained networks? Could this explain the much higher selectivity for this case in thehidden units? I would like to see the selectivity of the input unitsfor each of the plots/curves. Minor Note   The Neural Network Design section looks like it stillhas draft notes in it.