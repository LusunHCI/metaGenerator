Accept (Poster). rating score: 8. rating score: 6. rating score: 6. Thus, I m generally very supportive of the paper.<BRK>On the downside, many sections of the paper appear unnecessarily cryptic and lack important details.<BRK>Overall, I believe this paper can be a valuable contribution to the conference, and I recommend acceptance.<BRK>Overall, their study suggests that the orientation-tilt illusion is a byproduct of neural circuits that help biological visual systems achieve robust and efficient contour detection, and that incorporating these circuits in artificial neural networks can improve computer vision.
Reject. rating score: 3. rating score: 3. rating score: 3. I am curious on the choice of CEM. The problem is important and impactful. In summary, the paper is very impactful. The proposed model is novel from a modelling perspective since it makes CEM part of end to end learnable models. First of all, Proposition 1 is an existing result, hence authors should give a proper citation in its definition. This might be true but not really experimented.<BRK>This allows end to end learning of energy functions that can be used, for example, in continuous control. Advancements in this field are of broad interest to the ICLR community. This paper will be of interest to many readers, since it works at the interface between these. It appears to me that the experiments on cheetah and walker do not compare a particularly broad set of methods. Also, why is DCEM not also sensitive to the number of steps?<BRK>This paper proposes a differentiable variant of the Cross Entropy method and shows its use for a continuous control task. It introduces 4 hyper parameters and it is not clear how robust the method is to these. Also, there needs to be an ablation/robustness study for the DCEM method. Detailed review below:  The abstract should mention clearly that the proposed method allows you to differentiate through argmin operation and can be used for end to end learning. Similarly, "t" is for both for the iterations of CEM and the time stamp in the control problem. I don t understand how Proposition 1 adds to the paper.<BRK>They study the Cross-Entropy Method (CEM) for the non-convex optimization of a continuous and parameterized objective function and introduce a differentiable variant (DCEM) that enables us to differentiate the output of CEM with respect to the objective function's parameters.
Accept (Spotlight). rating score: 8. rating score: 6. rating score: 3. Authors in introduce a new competitive/cooperative physics based environment in which different teams of agents compete in a visual concealment and search task with visibility based team based rewards (although There are no explicit incentives for agents to interact with objects in the environment). Agents trained using self play In my opinion, this is an excellent paper which main contribution is to provide experimental evidence that relevant and complex skills and strategies can emerge from multi agent RL competing scenarios. I wouldn’t say that this is a “ human relevant strategies and skills “ as the authors claim.<BRK>This review is for the originally uploaded version of this article. Comments from other reviewers and revisions have deliberately not been taken into account. After publishing this review, this reviewer will participate in the forum discussion and help the authors improve the paper. If so, why and how (concat, sum, multiply, conditional batch norm, etc.)? ## Overall**Summary**The article introduces a new multi agent physics environment called "hide and seek". That being said, there are glaring issues with some of the writing that need to be addressed before I think this work conforms to the standards of ICLR. How exactly does the "surfing" work? And do all agents learn new skills at the same time or is there a delay? All in all an interesting work.<BRK>The main point of this paper is that RL agents learning at scale (large number of samples, batch size 64000). Hence this is self play: hiders and seekers use the same agent model. Note that a large body of multi agent RL work in fact uses agents that do not share weights, etc. Reject.The main point of the paper is empirical RL at scale. The paper also does not give new insights in how to make large scale RL `` work .<BRK>They find clear evidence of six emergent phases in agent strategy in their environment, each of which creates a new pressure for the opposing team to adapt; for instance, agents learn to build multi-object shelters using moveable boxes which in turn leads to agents discovering that they can overcome obstacles using ramps. They further provide evidence that multi-agent competition may scale better with increasing environment complexity and leads to behavior that centers around far more human-relevant skills than other self-supervised reinforcement learning methods such as intrinsic motivation.
Reject. rating score: 1. rating score: 1. rating score: 3. The rebuttal did not address my concerns convincingly. There were also simple fixes that the authors could have implemented but they decided not to update the paper. The premise of the work is very interesting: RNNs that are permutation invariant. Unfortunately, the paper seems rushed and needs a better justification for not having a RNN memory that is associative. The paper says "In this section and the remainder of the paper, we focus on the latter [commutative RNN memory operator], namely introducing a constraint (or equivalently, regularizer) that is commutative", but it never talks about the impact of a RNN memory using a non associative operator. Actually, my guess to why the RNNs experiments work well, even without an associative memory, is because the training examples come in multiple permuted forms, which is the data augmentation version of the pi SGD optimization described in Janossy pooling. Also missing related work for graphs:Bloem Reddy, Benjamin, and Yee Whye Teh. "Probabilistic symmetry and invariant neural networks." The paper has an interesting question but needs to build on prior work. As of now, I am unconvinced that not having an associative operator for the RNN memory will lead to a good nearly permutation invariance function (unless there is data augmentation, per Janossy pooling).<BRK>Summary: this paper proposes a new principled methodology for deriving and training RNN neural networks for prediction of permutation invariant functions. Authors show on simple tasks their method may outperform DeepSets, the state of the art. Although the idea is interesting and the paper reflects thorough work, I believe in its current form results are too weak to deserve publication. 1)Mathematical results and statements are mostly trivial and may well be omitted or included as an appendix. Some of these results are also mostly anectodal 2)The regularization idea seems interesting, but I am concerned it is showing that the final learned networks have a deepset like architecture: more specifically, theorem 3.6 shows RNN can implement permutation invariant functions by making identifying the parameters with the ones of deepperm. Also, as the authors mentioned, when learning a permutation invariant function then for any degree of regularization the regularization loss can be made zero. So for me, results seem to indicate that the network might have learned a deepperm kind of representation, which equivalently can be expressed as a RNN.<BRK>Not surprisingly, if these applications were associative and commutative the RNN would be permutation invariant. Although it is not shown if an RNN regularized that way is permutation invariant, since the associativity is not demonstrated, empirically it is shown that it may be already of use. A regularizer for RNNs that enforces commutativity   2. A fully learnable permutation invariant "deep" network, per an empirical demonstration   The main contribution is the empirical demonstration of the learnable nature of the obtained function unlike the prior art (e.g.DeepSets) where a choice of the aggregation function severily affects the results. The theoretical component of the paper is unclear:      1. 3.In essence, the result of the paper is a way to encourage commutativity in an RNN and a demonstration that it works in practice for encouraging permutation invariance. RNNs usually are only able to operate on very small sequences because of the vanishing gradient problem, yet the proposed approach will not directly work on the more robust LSTM. Significance or lack of the difference between the proposed method and DeepSets is unclear as the plots are missing the error bars.<BRK>Many machine learning tasks involve analysis of set valued inputs, and thus the learned functions are expected to be permutation invariant. Recent works (e.g., Deep Sets) have sought to characterize the neural architectures which result in permutation invariance. Here they take a different approach to such architectures and focus on recursive architectures such as RNNs, which are not permutation invariant in general, but can implement permutation invariant functions in a very compact manner. They first show that commutativity and associativity of the state transition function result in permutation invariance. Finally, they demonstrate that the resulting method outperforms other methods for learning permutation invariant models, due to its use of recursive computation.
Reject. rating score: 3. rating score: 3. rating score: 3. This paper proposes a framework for training time filter pruning for convolutional neural networks. Pros:+ The proposed method seems relatively easy to implement. + The quantitative results in the paper indicate that MaskConvNet achieves performance competitive with previously proposed pruning methods. The authors propose a workaround which they call “mask decay update”. This approach looks quite hacky and I’m not sure how easy it is to make it work in practice. The same goes for the mask. * Section 4.2, CIFAR 10: The authors mention that (Lemaire et al., 19) achieve better FLOP sparsity due to usage of Knowledge Distillation. I’d appreciate if the authors could elaborate on this. I must admit that I’m not an expert in the field of NN pruning but I’m surprised that training time masking of filters has not been tried before.<BRK>In this work, the authors propose a network pruning method to learn a pruned network during training. Specifically, they add a pruning mask for each layer and induce a sparisity loss on the mask variables during training. The pruned network is obtained by applying the learned mask to the networks. However, my assessment of this paper is weak reject. My questions are summarized as follows:Q1: In the methods part, the authors said that “Previous pruning approaches often prune Conv filters with their successive Batch Normalization layer unchanged.” Can the authors give some reference here as to which pruning approaches? Q2: Did the authors compare the proposed approach to training the pruned networks from scratch as done in [1]? Also can the authors analyze the sparsity patterns of the pruned networks as done in section G in the appendix of [1]? Q3: What is the difference of your approach to [2]? Is there any experimental results for comparison with [2]? [1] Rethinking the Value of Network Pruning.<BRK>For imagenet, if using Resnet 50, would be easier to compare to other numbers. In the imagenet comparison, L1 pruning is the version A of the paper. Paper claims multiple times that related works need a train prune retrain process which is only valid for post processing works. I missed sparsity promoting works related to training from scratch where similar masks are implicitly used during training (even not explicitly stated) to maintain the zeros in the network. How are the actual groups made? Why the initialization of the mask is to the mean? I do not see  much higher  parameter sparsity. The claim that this method is  much simpler  is a bit subjective. It would be great to have more and clearer details on how is this done. This is repeated in section 3.3 but details are missing. (page 6 before 3.4). The part with the sparsity budget is interesting. Could it be possible that the budget suggested is not the right for the task at hand and, therefore, the additional parameters are not really relevant / needed? Experiments:  There is loss with little sparsity when it comes to Imagenet (15 and 17% pruning) does not seem very promising even the training time is similar.<BRK>In this paper, they propose a framework, called MaskConvNet, for ConvNets filter pruning. (2) Simple, the mask module is implemented by a hard Sigmoid function with a small number of trainable mask variables, adding negligible memory and computational overheads to the networks during training. (3) Effective, it is able to achieve competitive pruning rate while maintaining comparable accuracy with the baseline ConvNets without pruning, regardless of the datasets and ConvNet architectures used. (5) Budget-aware, with a sparsity budget on target metric (e.g.model size and FLOP), MaskConvNet is able to train in a way that the optimizer can adaptively sparsify the network and automatically maintain sparsity level, till the pruned network produces good accuracy and fulfill the budget constraint simultaneously. Results on CIFAR-10 and ImageNet with several ConvNet architectures show that MaskConvNet works competitively well compared to previous pruning methods, with budget-constraint well respected.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. This paper introduces "projected error function regularization loss" or PER, an alternative to batch normalization. PER is based on the Wasserstein metric. The experimental results show that PER outperforms batch normalization on CIFAR 10/100 with most activation functions.<BRK>This submission belongs to the general field of neural networks and sub field of activation regularisation. I think the idea described in this submission is interesting. The PER is presented as an objective function that minimises an upperbound on 1 Wasserstein.<BRK>This paper proposes a new method to normalize activations in neural networks, based on an upper bound of the sliced Wasserstein distance between the empirical activation distribution and a standard Gaussian distribution. Pros:  The idea is clearly presented.<BRK>PER is similar to the Pseudo-Huber loss in the projected space, thus taking advantage of both $L^1 $and $L^2 $regularization losses.
Accept (Poster). rating score: 8. rating score: 3. rating score: 1. [score raised from weak accept to accept due to rebuttal/improvements]SummaryThe paper investigates the practice of using pixel level saliency maps in deep RL to “explain” agent behavior in terms of semantics of the scene. This mismatch and its potential interference with the method should be discussed as a current shortcoming. ContributionsThe paper nicely summarizes the main contributions, namely: (i) a literature survey on pixel saliency methods in deep RL and their use to “explain” agent behavior, (ii) a detailed description of the problem with the latter and a proposal to mitigate the main issues, and (iii) three experimental case studies to illustrate the problem further and show how the proposed method can help. This is a somewhat disenchanting message, but I personally think it is important to ensure that this message is heard in the field of interpretable ML in particular, and in the wider deep learning community in general. It is tempting to give simple answers to complex problems, and while I think saliency maps will play a large role in interpreting deep network decisions, I am also convinced that we need causal explanations, which salience maps (currently) cannot provide on a semantic level. The paper is well written and clear, the literature survey is quite extensive and valuable. In particular: visually estimating densities / correlations from scatter plots is often impossible and misleading   while the plots are nice to have, the claims regarding Figure 5, 8, 9 (b) and (c) must be backed up by reporting actual correlations / statistical tests. For instance, it is impossible to judge visually whether there’s any trend in 5 (c). It’s fine to include tables reporting the quantitative results in the appendix, they don’t necessarily have to be in the main paper. It would be nice to see an example where the method is used but the original hypothesis is not rejected (i.e.there’s now stronger evidence for the original hypothesis due to the counterfactual analysis). While I think that many aspects carry over from feedforward architectures to recurrent ones, I personally think that some issues with counterfactual analysis could become more intricate with recurrent agents. If you agree, please make this distinction clear in the paper (where appropriate) or state that the paper only applies to feedforward agents.<BRK>Abstract:The author suggests that saliency maps should be viewed as exploratory tools rather than explanation. The explore this idea in the context of a game. Here is my main issue:Although I believe there is a value in studies like this. I am not sure ICLR is the right venue for it. Yes, I agree that the method of interpreting the black box has a lot of issues and the counterfactual approach/causal approach is probably the right way to go but this is hardly news to the community. In short: what the generalizable contribution of the paper? I am open to change my mind if the discussion is convincing.<BRK>First, it is a survey on saliency maps used in explaining deep reinforcement learning models. Second, it is a proposal of a method that should overcome limitations of the current approaches described in the survey. This double aim makes the paper hard to understand as the survey is not complete and the model is not well explained. The main limitations the novel model aims to solve seems to be the production of "falsifiable" hypothesis in the explaination with saliency maps. However, experiments are really hard to follow and it is not clear why this is the case.<BRK>Saliency maps are frequently used to support explanations of the behavior of deep reinforcement learning (RL) agents. However, a review of how saliency maps are used in practice indicates that the derived explanations are often unfalsifiable and can be highly subjective. They use Atari games, a common benchmark for deep RL, to evaluate three types of saliency maps. Their results show the extent to which existing claims about Atari games can be evaluated and suggest that saliency maps are best viewed as an exploratory tool rather than an explanatory tool.
Reject. rating score: 3. rating score: 3. rating score: 6. The paper is well written and flows very well. Here are my feedbacks: Method Methodology wise, the novelty is somewhat limited. The main technical contributions are: 1) formulate linear programming to reduce energy costs, and the formulation of linear programming is straightforward. I don t think this makes a compelling case here. I had some experiences in implementing the LAPACK and BLAS on GPUs, and it should not be that slow.<BRK>3.The novelty is limited by the prior work. 1.The mathematical justification of the optimization on energy cost is not very sound, and the definition of optimal splitting set seems arbitrary. The paper is well written and easy to follow. In brief, this paper is an improvement to a splitting algorithm in a previous work, achieving good efficiency and enabling application on large datasets.<BRK>Summary:This paper builds on a recently proposed algorithm ("splitting steepest descent", Wu et al 2019) for guiding the growth of a smaller network into a larger one in architecture search. In practice the non energy aware "vanilla" networks do tend towards models that are small in size (fewer parameters) but are not necessarily low in energy consumption. There is new material here, although I find the novelty a bit limited (e.g.only an additional constraint compared to the original approach of Wu et al and addressing a clear scalability issue with the original work, i.e.eigendecomposition of a matrix, with what seem straightforward approximations, ).<BRK>Recently, Wu et al. (2019) framed the search of efficient neural architectures into a continuous splitting process: it iteratively splits existing neurons into multiple off-springs to achieve progressive loss minimization, thus finding novel architectures by gradually growing the neural network. However, this method was not specifically tailored for designing energy-efficient networks, and is computationally expensive on large-scale benchmarks.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. Minor pointsIn Eqn (1), \lambda and \gamma are not defined. Thus, it is an important step towards a more realistic attack on the visual perception pipeline in autonomous driving. The paper also presents a thorough background of MOT.<BRK>Compared to existing work on adversarial examples against object detection, to attack MOT techniques, the adversary needs to successfully fool multiple frames, and the authors show that by naively using existing attack approaches, the adversary needs to achieve 98% single frame attack success rate to fool the tracking system, which is too hard for existing attack algorithms. Therefore, this paper proposes a smart way of attacking MOT techniques by leveraging the properties of the tracking algorithm. 2.How large does the adversarial patch need to be in order to successfully launch the attacks?<BRK>Both subprocesses in the visual perception pipeline, object detection and multiple object tracking (MOT), are considered. The authors state that a previous attack is only effective when AE can reliably fool at least R consecutive frames. The paper also provides a nice overview on the processing pipeline.<BRK>In this paper, they are the first to study adversarial machine learning attacks against the complete visual perception pipeline in autonomous driving, and discover a novel attack technique, tracker hijacking, that can effectively fool MOT using AEs on object detection.
Reject. rating score: 1. rating score: 3. rating score: 6. FastSpeech is one that was noted in public comment. I don t believe this undermines the fundamentals of the work, but it will color claims of being "first" by each submission. However it performs works than the IAF distilled ClariNet parallel vocoder with the same amount of parameters. It is not clear what advantages WaveVAE has over ClariNet. However, this claim is somewhat misleading. The maintained quality is only achieved when using an autoregressive wavenet vocoder. It is worth comparing attention distillation to, say, training a traditional alignment model based on forced alignment and feeding the target durations as a conditioning feature along with the text.<BRK>In particular, it looks at the problem of efficient training and inference. The main idea behind this paper is to remove autoregressive components. I believe there is a great deal of interest for efficient and fast text to speech. I believe that the approach proposed does indeed accomplish the task of removing autoregressive components from the text to speech model. I find that the presentation of this work to be lacking a balance. This work makes 2 contributions 1) non autoregressive text to spectrogram and 2) non autoregressive spectrogram to speech model. Due to complexity involved I believe each of these contributions needs to be written (and assessed) separately. For instance, on page 6 you have a) encoder, b) decoder, c) VAE objective, d) STFT loss all discussed in very short details. 2) Block diagrams are helpful but not having a mathematical description makes them more ambiguous than they should be.<BRK>For the most part (as far as I am aware), neural architectures for TTS are encoder decoder based. Synthesizing audio from the feature representation (mel spectrogram) by a neural vocoder is usually an autoregressive model from the wavenet family. The authors mention that there are similarities to the approach used in Clarinet (this has a closed form KLD between the distilled distributions, which makes things easy). My thoughts: The paper is generally a good addition to the TTS literature. It is suggested that we can use a non autoregressive model with speedups. Likewise, they also use a wavenet VAE, which they claim can be trained without distillation (could the authors please clarify this point?) 1) The presentation is not at all clear. Just as a comparison, I would like to draw attention to Tacotron (1, 2) in which I think the details can actually be worked out with some effort. Are we also using the  IAF setup as described in Kingma s work? In summary, while I see that the work will definitely be useful to the Speech Synthesis practitioner, the clarity of the paper could be improved and we need a few more diagrams (maybe even code) to make it implementable.<BRK>In this work, they first propose ParaNet, a non-autoregressive seq2seq model that converts text to spectrogram. It is fully convolutional and obtains 46.7 times speed-up over Deep Voice 3 at synthesis while maintaining comparable speech quality using a WaveNet vocoder. ParaNet also produces stable alignment between text and speech on the challenging test sentences by iteratively improving the attention in a layer-by-layer manner. Based on ParaNet, they build the first fully parallel neural text-to-speech system using parallel neural vocoders, which can synthesize speech from text through a single feed-forward pass. They investigate several parallel vocoders within the TTS system, including variants of IAF vocoders and bipartite flow vocoder.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. Comments:  I think this paper is a rather nice reminder not to forget our linear algebra while engaged in deep learning. The paper derives a bound for few shot performance that depends on the shot. Would this be equivalent to the proposed method?<BRK>How is the formulation derived between Eq.3 and Eq.4?More details should be given here. The paper focuses on an important problem: number of shots in few shot learning, and chooses prototypical network which is a very famous and widely used method for detailed analysis. If so, it seems to be very slow. I was wondering if it is due to the prototypical network itself (the intrinsic property of the prototypical network limits its improvement upperbound) or something else?<BRK>This seems to be a valuable and novel analysis of the problem, and the problem is important since the number of shots might not be known a priori. (2.3) I found the proof of Lemma 1 difficult to follow. It is shown that this makes the procedure relatively robust to the number of shots during training. Nevertheless, to make the paper accessible to a wider audience, I believe it s necessary to improve the clarity (see points below). (3.4) In the statement of Theorem 4 in the appendix, there should be something after "Var"?<BRK>The number of labeled examples per category is called the number of shots (or shot number). Generally, the shot number used in meta-training should match the one used in meta-testing to obtain the best performance. From their analysis, they propose a simple method that is robust to the choice of shot number used during meta-training, which is a crucial hyperparameter.
Reject. rating score: 3. rating score: 3. rating score: 6. The conclusions (discriminator more sensitive than generator to quantization, quantizing both generator and discriminator helps) are sensible and interesting. Weaknesses of the paper:  The related work section could be greatly improved, thereby showing the limited novelty of the proposed method (QGAN).<BRK>The paper introduces a fairly simple yet seemly effective method for quantizing GAN. These are related works, yet neither included nor discussed in this paper. In short, the comparison to previous works seems insufficient in my point of view. [Advantage]The paper is clearly written and easy to follow.<BRK>This paper propose to study the quantization of GANs parameters. I feel some details are missing or at least lack some precision. There is some typos in the text<BRK>Motivated by these observations, they develop a novel quantization method for GANs based on EM algorithms, named as QGAN. Experiments on CIFAR-10 and CelebA show that QGAN can quantize weights in GANs to even 1-bit or 2-bit representations with results of quality comparable to original models.
Reject. rating score: 1. rating score: 1. rating score: 1. There is not enough novelty with the proposed method.<BRK>The proposed method is simple extension of that of (Kidzinski & Hastie, 2018), by adding a term that accounts for treatment and its timing.<BRK>This paper proposed a framework for modeling disease progression taking into account the treatment events.<BRK>nan
Accept (Poster). rating score: 6. rating score: 3. rating score: 3. This paper analyzes the peculiar case that deep generative models often assign a higher likelihood to other datasets than they were trained on. The running hypothesis here is, that input complexity plays a central role. I also think that the conclusion of this work, that this behavior may be natural and a consequence of likelihood itself, seems very sensible. I think it is already mentioned that empirically there is no reason to believe a better model would solve this, it would just be nice to have a theoretical statement here as well. Another concern is that even though the empirical results look quite promising, it would be good to stress test the proposed score on more common OOD detection benchmarks against state of the art methods to see if likelihood generative models with the proposed criterion are competitive there. However, I will not change my score as the response did not change my opinion on the need for a more theoretical treatment of the statement, which is a serious (and potentially hard to resolve) shortcoming of the paper in my opinion.<BRK>The first natural idea is to use the likelihood (density) given by p, which, by now, is known to be problematic since the likelihood can be high when evaluated on data points from a completely different domain. Show that "simple" images (e.g., constant color) tend to give high likelihood, whereas complex images (e.g., noise) tend to give low likelihood. * The paragraph before Section 3: "... could replace the computed likelihood values for the negative of our complexity estimate ..." I think it is too soon to make this conclusion. There is no hypothesis testing there. Empirical results on more than 10 image datasets show that the proposed measure works better than the negative log likelihood in most cases. #   Review  The paper is easy to follow. How much does the conclusion that "simple images tend to give high likelihood" depend on the complexity of the model? As far as I can see, only a few models are studied here: Glow and PixelCNN++. 2.Given a model p and a test input image y, how exactly do you tell if y is out of distribution? Is there a threshold? If so, what is the threshold? But this does not explain how to perform OOD detection given one input image.<BRK>The paper discusses the inductive biases in generative models that tend to assign higher likelihoods to "less complex" images. In particular, a likelihood based generative model (like Glow or PixelCNN++) trained on a particular dataset has significantly higher likelihood for data that have lower compression ratios (e.g  with PNG). The authors propose a simple approach based on the likelihood ratio between a trained model and a "prior" model (based on existing compression methods) and demonstrate that this improves unsupervised OOD detection on certain dataset pairs. The idea is quite simple (and surprising it seems to work!), but it seems that better understanding of the proposed method could be achieved. Questions:If our goal is to perform OOD detection, then higher likelihood for test samples might not be an issue, as we can simply declare samples with higher and lower likelihoods as OOD? What are the AUC of OOD detection if we had simply used these? From Figure 4, the x axis is p(x|M) and is between zero and one, while L(x) should be  log p(x|M_0). If AUROC is very small, we can still obtain good classifiers with flipped predictions.<BRK>Likelihood-based generative models are a promising resource to detect out-of-distribution (OOD) inputs which could compromise the robustness or reliability of a machine learning system. However, likelihoods derived from such models have been shown to be problematic for detecting certain types of inputs that significantly differ from training data. In this paper, they pose that this problem is due to the excessive influence that input complexity has in generative models' likelihoods. They find such score to perform comparably to, or even better than, existing OOD detection approaches under a wide range of data sets, models, model sizes, and complexity estimates.
Accept (Poster). rating score: 6. rating score: 3. This paper is technically sound, well written and propose an interesting modification of a previous algorithm. The algorithm is favorably compared with the state of the art on three image test sets (MNIST, CIFAR 10n ImageNet).<BRK>I have decided to weak reject the paper for the following key reasons:1.<BRK>They find that Sign-OPT attack consistently requires 5X to 10X fewer queries when compared to the current state-of-the-art approaches, and usually converges to an adversarial example with smaller perturbation.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. The paper presents a new attack, called the shadow attack, that can maintain the imperceptibility of adversarial samples when out of the certified radius.<BRK>The generated adversarial images are imperceptible and have a large norm to escape the certification regions. Could the authors elaborate more of the results? Quantitative studies on CIFAR 10 and ImageNet shows that the new attack method can generate adversarial images that have larger certified radii than natural images.<BRK>The paper presents a new attack: Shadow Attack, which can generate imperceptible adversarial samples. This method is easy to follow and a lot of examples of different experiments are shown. Therefore, the authors claim that the method can attack certified systems.<BRK>They present an attack that maintains the imperceptibility property of adversarial examples while being outside of the certified radius. Furthermore, the proposed "Shadow Attack" can fool certifiably robust networks by producing an imperceptible adversarial example that gets misclassified and produces a strong "spoofed" certificate.
Reject. rating score: 3. rating score: 6. rating score: 6. Since the main claim of this paper is a new off policy method, outperforming the previous off policy methods is a fair game. The current results are not convincing enough. and what is "action_std" in the code? This paper proposes an off policy reinforcement learning method in which the model parameters have been updated using the regression style loss function.<BRK>This is known to be an issue, as most algorithms avoid approximating it and prefer estimating the Q function. I basically subscribe to John Schulman s comment below, both about empirical results and about citing Self Imitation Learning, but I have a stronger point against the paper, which is insufficient positionning with respect to the relevant literature. So to me it is exactly the same idea, and it is mandatory that the authors clearly establish what is the novelty of their work with respect to this previous paper.<BRK>It seems emphasizing more that the aim is to show that simple methods are competitive rather than focusing on novelty could be a good idea. Provided that the authors incorporate the feedback of the other reviewers and update the paper accordingly, it will make a good contribution.<BRK>In this paper, they aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods.
Reject. rating score: 1. rating score: 3. First, the multi objective optimization used in this paper is quite confusing. There are many numbers for a single clustering problem.<BRK>[Contribution summary]Authors propose MODiR, a dimensionality reduction approach that utilizes both semantic and structural features. In the current manuscript, the quantitative analysis does not immediately show the benefit or the characteristics of the proposed approaches. In the main section of the manuscript, the analysis is limited to citation networks.<BRK>Furthermore, social networks can be extracted from email corpora, tweets, or social media. In this paper, they propose to incorporate both, text and graph, to not only visualise the semantic information encoded in the documents' content but also the relationships expressed by the inherent network structure.
Reject. rating score: 3. rating score: 3. rating score: 3. In this work authors consider a problem of  model compatibility  of GANs, i.e.usefullness of the generated samples for classification tasks. proposed method seems to improve the accuracy of classifiers trained on generated data. Additionally, as pointed out by reviewer #4, the results seem somewhat incremental.<BRK>In this paper the authors propose a method for improving "model compatibility" in GANs. For this reason they add to the loss of the generation procedure a term that depends on the maximum mean discrepancy between the following datasets: (1) the output of a classifier with input the real dataset, (2) the output of the same classifier with input GAN generated samples. The increase in the model compatibility is very mild.<BRK>This paper aims at training a GAN that can generate data matches the real data distribution well especially at the boundaries of the classifiers. There are several typo and mistakes. The experiments only show that the proposed method got a good performance, but the analysis of the reason is not shown. The reason to name the loss as Boundary Calibration loss (BC loss) should be explained and the experiments should show some effect on the boundary areas. Some concerns are listed below,1. 4.The author said that image quality of MNIST and CIFAR10 are not improved, then why the classification results are improved? 5.What kind of  generator do you use for the UCI data? How do you settle the output problem?<BRK>Generative Adversarial Networks (GANs) is a powerful family of models that learn an underlying distribution to generate synthetic data. Many existing studies of GANs focus on improving the realness of the generated image data for visual applications, and few of them concern about improving the quality of the generated data for training other classifiers---a task known as the model compatibility problem.
Reject. rating score: 1. rating score: 6. rating score: 6. (Prop.4.1, Sec.4)C2.Gating cannot provide explanations in the way that attention is alleged to do. C4.Outputs are sensitive to alteration of attention weights for two sequence but not one sequence models. (I also have reasons to question whether one  vs two sequence inputs is the right distinction that needs to be accounted for.) The same is true of translation. But the two sequence tasks are NLI and QA, where (ideally) BOW models should not do nearly so well: paying attention to the right tokens should be important. But what I see in the paper does not convince me that (*) is true.<BRK>Motivated by an existing paper, the paper analyzes the interpretability of attention mechanism over three NLP tasks. 2. in figure 5, for the single sequence (original), most of attentions leading to correct predictions are labeled meaningful. The paper is well written and all claims are supported. This means that attention still can contribute to the final prediction but not significant enough (some like yelp are significant).<BRK>This paper investigates the degree to which we might view attention weights as explanatory across NLP tasks and architectures. Notably, the authors distinguish between single and "pair" sequence tasks, the latter including NLI, and generation tasks (e.g., translation). The argument here is that attention weights do not provide explanatory power for single sequence tasks like classification, but do for NLI and generation. Another notable distinction from most (although not all; see the references below) prior work on the explainability of attention mechanisms in NLP is the inclusion of transformer/self attentive architectures.<BRK>The attention layer in a neural network model provides insights into the model ’ s reasoning behind its prediction, which are usually criticized for being opaque. In this work, they attempt to fill this gap by giving a comprehensive explanation which justifies both kinds of observations (i.e., when is attention interpretable and when it is not). Through a series of experiments on diverse NLP tasks, they validate their observations and reinforce their claim of interpretability of attention through manual evaluation.
Accept (Poster). rating score: 8. rating score: 3. The domain is loop invariant detection, in the static program analysis space. On the whole, I like the presentation and the thinking here, and think it will be interesting to folks in the field, possibly spurring on further thinking in compilers, program synthesis, constrained optimization, etc, so recommend accepting.<BRK>Summary:This paper introduces a novel way to find loop invariants for aprogram. Thisallows an invariant to be learned. Lots of details are missing in this paper. How long did it take? The paper talks about neural architecture, but all I see iseffectively a curve fitting task for some template. This feelsdifferent from the code2inv paper where a program can be fedinto the system and the pretrained model emits the invariant.<BRK>Program verification offers a framework for ensuring program correctness and therefore systematically eliminating different classes of bugs. In this paper, they present the Continuous Logic Network (CLN), a novel neural architecture for automatically learning loop invariants directly from program execution traces. CLN2INV is the first tool to solve all 124 theoretically solvable problems in the Code2Inv dataset.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. This paper proposes Episode Reinforcement Learning with Associative Memory (ERLAM), which maintains a graph based on the state transitions (i.e.nodes correspond to states, and edges correspond to transitions) and propagates the values through the edges in the graph in the reverse order of each trajectory. Experimental results show that ERLAM significantly improves the sample efficiency in Atari benchmarks. The experimental results demonstrate that the proposed method is promising. It seems that ERLAM would more likely to over estimate the values for the state than the existing episodic RL algorithms. How was this determined?<BRK>The paper proposes to combine DQN with a nonparametric estimate of the optimal Q function based on the graph of all observed transitions in the buffer. They show that this regularizer facilitates learning, and compare to other nonparametric approaches. Thanks for your response and the additional experiments. I found the paper easy to read.<BRK>The paper proposes a new method for organizing episodic memory in with deep Q networks. It organizes the memory as a graph in which nodes are internal representations of observed states and edges link state transitions. First, it lacks theoretical rigor to explain why the proposed propagation mechanism works, for instance, a proof of the optimal substructure in Eq.(3) would be helpful. While I see the authors mentioned previous work which do not have optimal substructures and mention this for the case of navigation like tasks, the matter is not discussed and unclear in other scenarios.<BRK>Non-parametric episodic control has been proposed to speed up parametric reinforcement learning by rapidly latching on previously successful policies. They build a graph on top of states in memory based on state transitions and develop a reverse-trajectory propagation strategy to allow rapid value propagation through the graph. Results on navigation domain and Atari games show their framework achieves significantly higher sample efficiency than state-of-the-art episodic reinforcement learning models.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. The paper proposes a framework, called LORD, for better disentanglement of the class and content information in the latent space of image representations. The main issue that the authors tackle is the information leakage between the representations for the class and content. Firstly, the paper makes a distinction between content and style. The reverse, leakage from class codes to content codes is achieved by adding any asymmetric noise regularization term. This also seems to be aimed at reducing the total variability in the content codes.<BRK>This paper proposes LORD, a novel non adversarial method of class supervised representation disentanglement. 2. eliminate the class information from content code by asymmetric noise regularization. The experimental results indicate that LORD succeeds to disentangle class information on content codes, while it outperforms style content disentangled representations on style switching tasks (Figure 2 & 3).<BRK>They demonstrate that the method achieves impressive empirical results both in terms of disentanglement and a limited experiment on unsupervised domain translation. I find the experimental results in this paper very appealing. Here are some main comments:1. 2.Cost of trainingOne thing I feel should be made more clear in the paper is the training cost of GLO v. amortized models. Second, the authors observed that the amortized models leak class information into the content representation. I recommend that the authors try at least one other dataset. If the authors are able to address the above questions and requests, then I am more than happy to raise my score.<BRK>They present a unified formulation for class and content disentanglement and use it to illustrate the limitations of current methods. They therefore introduce LORD, a novel method based on Latent Optimization for Representation Disentanglement. They find that latent optimization, along with an asymmetric noise regularization, is superior to amortized inference for achieving disentangled representations. They further introduce a clustering-based approach for extending their method for settings that exhibit in-class variation with promising results on the task of domain translation.
Reject. rating score: 1. rating score: 3. rating score: 3. This thing aside, the paper is an interesting contribution. The concept of spread divergence can be valuable in many context.<BRK>The paper introduced a way to modify densities such that their support agrees and that the Kullback Leibler divergence can be computed without diverging. Also mention Jensen Shannon divergence, a KL symmetrization, which is always finite and used in GAN analysis. Comments:In Sec 1, mention that f should be strictly convex at 1. What about spread KL?<BRK>The approach is motivated from the concern that traditional divergence such as f divergence or KL divergence may not always exist, in which the spread divergence may be a substitute. I believe the motivation of this paper is interesting.<BRK>For distributions $p $and $q $with different supports, the divergence $\div {p} {q} $may not exist. They define a spread divergence $\sdiv {p} {q} $on modified $p $and $q $and describe sufficient conditions for the existence of such a divergence.
Reject. rating score: 3. rating score: 6. rating score: 6. Overall, this paper is an extension of the prior work Kurth Nelson & Redish (2009). It seems to me that the difference between this paper and Kurth Nelson & Redish (2009) is that in this paper, the approximated Q value with hyperbolic discounting function is a weighted sum over each Q values using exponential discounting factor gamma, while in Kurth Nelson & Redish (2009), the Q value is estimated by sampling one Q value based on the distribution of the gamma.<BRK>Overall, I very slightly tend towards accepting this paper for publication. Also there is no measure of standard error or statistical significance on the graphs. Introducing a hyperparameter that increases computation and memory does not really seem like it s solving the problem, and certainly not "efficiently" as claimed in the abstract. How was the value of the new hyperparameter set for the experiments?<BRK>Therefore, I am not sure about the technical contribution of this paper to the area.<BRK>Reinforcement learning (RL) typically defines a discount factor as part of the Markov Decision Process. Here they extend earlier work of Kurth-Nelson and Redish and propose an efficient deep reinforcement learning agent that acts via hyperbolic discounting and other non-exponential discount mechanisms.
Reject. rating score: 1. rating score: 1. rating score: 1. If it is commonly used, please add a reference. The paper would be much stronger if the it could demonstrate that with the more accurate hybrid model, model based learning performs better or transfer learning is more straightforward. First, the conclusion of the paper is not clear to me.<BRK>c) Section 3.2 states a parallel ensemble of NN and non NN models has the advantage of finding ‘global optimum’ … this is a strong statement and needs to be demonstrated. b) The main claim of the paper is that combining physics models with NNs is better than NNs alone, esp.<BRK>It is not mentioned how to find such a value or what the value for the experiments for. There should be a table to accompany this figure, so that it s easier to compare the models.<BRK>In this paper, they demonstrate that by composing domain models with machine learning models, by using extrapolative testing sets, and invoking decorrelation objective functions, they create models which can predict more complex systems. Although this work is preliminary, they show that the ability to combine models is a very promising direction for neural modeling.
Reject. rating score: 3. rating score: 3. rating score: 3. Weakness:     * The paper lacks novelty. As pointed above, I did not see that the contribution from the paper is sufficiently original. in the abstract: "character based" model. Because BERT is a word piece based model. IIUC, You probably want to say "Chinese character" instead of character.<BRK>This paper tries to improve the performance of Chinese NER by developing a novel attention mechanism that leverages BERT pre trained model which considers bi directional context. Experiments on a number of tasks show that the proposed approach is effective. So, the contribution of this paper is limited[3] The proposed algorithm is simple and effective, but the novelty is a bit low<BRK>: maybe it should be moved to the introduction as a novelty proposed by the paper. The proposed model outperforms sometimes the other models, often by a small margin as it is usually the case in NER experiments. But more insight on the strengths of the models should be given by conducting an ablation study. In conclusion ,this paper present an incremental improvement over BERT based NER for Chinese.<BRK>The character-based model, such as BERT, has achieved remarkable success in Chinese named entity recognition (NER). However, such model would likely miss the overall information of the entity words. In this paper, they propose to combine priori entity information with BERT. Experiments show that their model has achieved state-of-the-art results on 3 Chinese NER datasets.
Reject. rating score: 1. rating score: 3. rating score: 6. Through viewing the Critic as a Lyapunov function, optimizing the policy with a Lyapunov based constraint is meant to ensure the stability of the policy through a ‘cost stability’ metric.. Q5: The review believes that model free control and stability guarantees are fundamentally orthogonal ideas, rather than just under studied work as the authors have been suggesting in the script and rebuttal. Q9: This remark was aimed at earlier in the paper, either the introduction or main section, rather than the experimental section. I would image the cost function needs to be a measure on the entire dynamic state. The results don’t demonstrate this.<BRK>In this work the authors studied the model free RL approach for learning a policy with stability guarantees. Leveraging the Lyapunov stochastic stability criterion, instead if minimizing the cumulative cost (plus a soft entropy), they propose optimizing an objective function with a specific Lyapunov critic, which is a specific critic function that satisfies the Lyapunov criterion to guarantee stability. They also show in several Cartpole, Mujoco, and Repressilator experiments that this approach is more robust to perturbations (such as sinusoids), where the agent are more robust to dynamic uncertainties and disturbances. Through the specific parameterization of quadratic Lyapunov function (in the latent space), the authors proposed learning a new critic function that is a value function but at the same time (almost) satisfies the Lyapunov constraints. While this is an interesting idea, and the experimental results look promising, I do have several questions. Can the authors provide numerical comparisons with the method proposed by Chow 19 as well?<BRK>In this paper, the authors introduce an algorithm to learn a stable controller using deep NN actor critic method. The problem is important to control with deep RL. The paper is written clearly. See, for example, the Lyapunov stability of stochastic systems (survey in [1])? Then the target network is trained to minimize the difference between the target and the critic. The question is, how is the stability of the target ensured by minimizing the difference with a Lyapunov critic?<BRK>Its integration with deep learning techniques has promoted the field of deep RL with an impressive performance in complicated continuous control tasks. However, from a control-theoretic perspective, the first and most important property of a system to be guaranteed is stability. In this paper, they propose a stability guaranteed RL framework which simultaneously learns a Lyapunov function along with the controller or policy, both of which are parameterized by deep neural networks, by borrowing the concept of Lyapunov function from control theory. Compared with RL algorithms without stability guarantee, their approach can enable the system to recover to the operating point when interfered by uncertainties such as unseen disturbances and system parametric variations to a certain extent.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. This paper proposes a generative model for scalable sequential object oriented representation. So has the author compared the method SCALOR with previous methods in few objects setting? Does the technical improvements of the method benefit in the few objects setting?<BRK>I thank the authors for the detailed rebuttal, as well as for the updates to the text and several new experiments in the revised version of the paper. Most of my comments are addressed well. Why is it not? The main contribution is in improving the efficiency of object detection/tracking by parallelizing the computation, without much conceptual innovation. Both can be measured as functions of the number of objects in the scene.<BRK>Therefore, I would recommend the paper be accepted. The key components of their approach is the parallel discovery and propagation of object latents as well as the explicit modeling of the background. Would the authors provide an experiment analyzing this case? Research Problem: This paper tackles the problem of scaling object oriented generative modeling of scenes to scenes with a large number of objects.<BRK>In this paper, they propose SCALOR, a probabilistic generative world model for learning SCALable Object-oriented Representation of a video. With the proposed spatially parallel attention and proposal-rejection mechanisms, SCALOR can deal with orders of magnitude larger numbers of objects compared to the previous state-of-the-art models.
Reject. rating score: 3. rating score: 6. rating score: 6. This paper introduces a optimisation for BERT models based on using block matrices for the attention layers. This allows to reduce the memory footprint and the processing  time during training while reaching state of the art results on 5 datasets. An interesting study on memory consumption in BERT is conducted. No results are given at test time : is there also a memory and processing time reduction ? Even if the proposition is interesting, the impact of the paper is limited to the (flourishing) scope optimising Bert models ("Bertology"). The authors do not mention if their code is available.<BRK>The paper propose to sparsify the attention matrix to decrease memory usage and to speed up training. The model gains ~20% efficiency with ~20% decrease in memory use while maintaining comparable performance to the state of the art model. The paper is clear and well organized with good experiment results.<BRK>The authors propose BlockBERT, a model that makes the attention matrix of Transformer models sparse by introducing block structure. This has the dual benefit of reducing memory and reducing training time. However, can the authors also include metrics on training time and memory in Table 2 for Sparse BERT as well as other sparse attention transformer architectures proposed (for example the Correia paper or the Sukhbaatar paper)? It is not clear the savings from this architecture compared to sparse Transformers in general. 5.The authors mention that attention heads can be sparsified due to the memory usage and quadratic time complexity.<BRK>They present BlockBERT, a lightweight and efficient BERT model that is designed to better modeling long-distance dependencies. Their model extends BERT by introducing sparse block structures into the attention matrix to reduce both memory consumption and training time, which also enables attention heads to capture either short- or long-range contextual information. Results show that BlockBERT uses 18.7-36.1% less memory and reduces the training time by 12.0-25.1%, while having comparable and sometimes better prediction accuracy, compared to an advanced BERT-based model, RoBERTa.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 8. This paper studies whether and how position information is encoded in CNNs. [Pros]1.I enjoy reading this paper: probing CNNs is not easy, but it designs experiments in an intuitive way and rigorously performs ablation studies and analysis. 2.The observations and findings are interesting and helpful to the community.<BRK>The paper investigates to what degree Convolutional Neural Networks (CNNs) learn to encode positional information. Rather interesting finding is the not only they do encode this information, but that it is to a large degree function of the padding commonly used in the CNN architectures. Previous and related work seems to be well referenced. How were the numbers effected?<BRK>This paper studied the problem of the encoded position information in convolution neural networks. The hypothesis is that CNN can implicitly learn to encode the position information. Clarity:This paper is interesting for me. The paper mainly discussed the zero padding and found it is the source of position information. Can you help investigate where the position information comes from for this case?<BRK>In contrast to fully connected networks, Convolutional Neural Networks (CNNs) achieve efficiency by learning weights associated with local filters with a finite spatial extent. An implication of this is that a filter may know what it is looking at, but not where it is positioned in the image. A comprehensive set of experiments show the validity of this hypothesis and shed light on how and where this information is represented while offering clues to where positional information is derived from in deep CNNs.
Reject. rating score: 3. rating score: 6. rating score: 6. This paper considers the problem of embedding graphs into continuous spaces. The emphasis is on determining the correct dimension and curvature to minimize distortion or a threshold loss of the embedding.<BRK>The origin of this idea is that various researchers have studied embedding graphs into non Euclidean spaces. The authors study this setting, consider a variety of existing notions of curvature for graphs, introduce a notion of global curvature for the entire graph, and now to efficiently compute it.<BRK>The paper presents a novel notion named glocal graph curvature, which offers a solution to determine the optimal curvature for embedding. In particular, the global graph curvature depends on both dimension and loss function used for the network embedding. Besides, the authors studied the existing local curvatures and show that the existing graph curvatures may not be able to properly capture the global graph structure curvature. I could not find anything wrong with this paper, but also do not have any intelligent questions to ask.<BRK>Recently, non-Euclidean spaces became popular for embedding structured data. In this paper, they define a notion of global graph curvature, specifically catered to the problem of embedding graphs, and analyze the problem of estimating this curvature using only graph-based characteristics (without actual graph embedding). They show that optimal curvature essentially depends on dimensionality of the embedding space and loss function one aims to minimize via embedding. They review the existing notions of local curvature (e.g., Ollivier-Ricci curvature) and analyze their properties theoretically and empirically.
Reject. rating score: 1. rating score: 3. rating score: 6. cons:  WaveGAN was a preliminary and encouraging trial for raw audio synthesis with GAN. The posted failure cases and some samples tend to have overlapped sounds from different digits. Note that, its audio fidelity is far away from the state of the art results and it was only tested on simple dataset (sounds of ten digit commands). However, the proposed PUGAN was still tested on very simple dataset (sounds of ten digit commands), and the quality of generated samples are only comparable to WaveGAN.<BRK>Overall, I liked the story of the paper, but the paper lacks clarity and details. The performance obtained by PUGAN 1 is nonetheless noticeable   +1 quality score over WaveGAN, and much better pairwise win ratios. Also, the fact that evaluations are carried out with PUGAN 1 suggests that the progressive training does not really works well past a single block.<BRK>The authors detail PUGAN, architectural changes to models for raw waveform generation with GANs. The paper is well motivated and experiments are correct, but the quality improvements overall are a little underwhelming.<BRK>This paper proposes a novel generative model called PUGAN, which progressively synthesizes high-quality audio in a raw waveform. Their experiments show that the audio signals can be generated in real-time with the comparable quality to that of WaveGAN with respect to the inception scores and the human evaluation.
Accept (Poster). rating score: 8. rating score: 8. rating score: 3. The authors present a new first order optimization method that adds a corrective term to Nesterov SGD. They demonstrate that this adjustment is necessary and sufficient to benefit from the faster convergence of Nesterov gradient descent in the stochastic case. Their approach is justified by a well conducted theoretical analysis and some empirical work on toy datasets. Rate of convergence is also not reported for Adam in fig 5.<BRK>This paper shows the non acceleration of Nesterov SGD theoretically with a component decoupled model. Moreover, the authors introduce an additional compensation term and derive a novel optimization method, MaSS. Pros1.It s amazing to see the great improvement introduced by the compensation term into the theoretical result of MaSS. "(stochastic) gradients are always perpendicular to W^*" seems not that obvious. 2.The empirical result merely involves two settings of learning rate: 0.01, 0.3.<BRK>However, numerical experiments show for some non convex functions, specifically for deep learning problems. It is unclear what kind of loss function the author(s) are using for training classification problems on MNIST and CIFAR 10. This could be softmax cross entropy but not quadratic. 3) The theoretical results in this paper are not strong.<BRK>Nesterov SGD is widely used for training modern neural networks and other machine learning models. Indeed, as they show in this paper, both theoretically and empirically, Nesterov SGD with any parameter selection does not in general provide acceleration over ordinary SGD. This is in contrast to the classical results in the deterministic setting, where the same step size ensures accelerated convergence of the Nesterov's method over optimal gradient descent.
Reject. rating score: 1. rating score: 1. rating score: 1. The paper is very poorly written, with many incomprehensible passages.<BRK>Badly and unprofessionally writtenI had and still have a hard time to report what this paper is about. The paper is badly written with grammatical issues almost in all the sentences.<BRK>This is a clear case of less than half baked paper. The paper does not cite any previous research (no references) and is poorly written.<BRK>Reinforcement learning methods that continuously learn neural networks by episode generation with game tree search have been successful in two-person complete information deterministic games such as chess, shogi, and Go. The experiments with a small problem showed that it had robust performance compared to the existing method, Alpha Zero.
Reject. rating score: 3. rating score: 3. rating score: 6. This paper presents an unsupervised learning approach to solve forward and inverse problems represented by partial differential equations. A framework is proposed based on the minimisation of loss functions that enforce the boundary conditions of the PDE solution and promote its smoothness. While the paper presents an elegant solution folding forward and inverse problems into a single framework, the presentation is missing a few important details which difficult the assessment of the contribution and favour a rejection of the paper. The main issues are insufficient experimental comparisons and a lack of theoretical support for the method. An important detail in this loss is the third term, which enforces boundary conditions for the coefficients at the boundary of the domain. 3.The paper proposes a general framework, but experimental results are presented for only one specific problem, the electrical impedance tomography. "The neural network approach to inverse problems in differential equations."<BRK>Concerns: there have been plenty of works that use neural networks to model the function $u$ for forward problems and another bunch of works that use neural networks to model parameters to do inverse problems. The experiments in the paper is limited. It compares with only one work in the forward task, but no comparison in the inverse problem. I would suggest submit this work for a workshop. Decision: This work need further experiments and theoretical analyze. I suggest weakly reject this paper.<BRK>2) Convergence testsConvergence tests are an important part that is missing in the current paper. However, a more thorough discussion on how the current approach complements existing literature on neural network based PDE solvers would be in order. vii) Perhaps it would be clearer to use ‘row’ instead of ‘line’ when referring to the results in the table. The neural network based approach proposed in this paper seems general and simple with encouraging experimental results.<BRK>This setting is flexible in the sense that regularizers can be tailored to specific problems. The solver is grid free, mesh free and shape free, and the solution is approximated by a neural network. The network is trained to minimize deviations of the learned function from the PDE solution and satisfy the boundary conditions.
Accept (Poster). rating score: 8. rating score: 8. rating score: 6. Summary:  key problem: neural architecture search (NAS) to improve both accuracy and runtime efficiency of deep nets for semantic segmentation;  contributions: 1) a novel NAS search space leveraging multi resolution branches, efficient operators ("zoomed convolutions"), and parametrized expansion ratios, 2) a decomposition and normalization of the latency objective to avoid a bias towards very fast but weak architectures, 3) a natural extension of the optimization problem to simultaneously search for teacher and student architectures in one pass, 4) a novel state of the art efficient architecture (FasterSeg) found by the aforementioned NAS algorithm, 5) a detailed experimental evaluation on 3 datasets and an ablative analysis quantifying the benefits of the aforementioned contributions. When compared to related efficient architectures, the proposed method results in competitive accuracy at significantly higher frame rates. This is validated on Cityscapes, CamVid, and BDD with the architecture found on Cityscapes. The ablative analysis shows that the numerous individual contributions are significant, esp. Key reason 2: well motivated method with a collection of multiple novel contributions that are interesting and practical.<BRK>This paper presents an automatically designed semantic segmentation network utilising neural architecture search. The proposed method is discovered from a search space integrating multi resolution branches, that has been recently found to be vital in manually designed segmentation models. To calibrate the balance between the goals of high accuracy and low latency, the authors propose a decoupled and fine grained latency regularization, that effectively overcomes the observed phenomenons that the searched networks are prone to “collapsing” to low latency yet poor accuracy models. Moreover, the authors extend the proposed method to a new collaborative search (co searching) framework, simultaneously searching for a teacher and a student network in the same single run.<BRK>This paper proposes a neural architecture search (NAS) algorithm which automatically finds a efficient network architecture, FasterSeg, for real time semantic segmentation. For instances a) it explores and integrates multi resolution branches from BiSeNet during NAS b) simultaneously optimizes the loss for accuracy and latency (as done in CAS algorithm) and c) knowledge distillation for semantic segmentation. The difference between teacher and student will then only be in loss function. 2) There is not a single concrete contribution. Seachable  > SearchableUpdates:I read through the reviews of other reviewers as well as the rebuttal posted by authors. Overall, I am satisfied with the authors response and hence Improving my scores to Weak Accept.<BRK>They present FasterSeg, an automatically designed semantic segmentation network with not only state-of-the-art performance but also faster speed than current methods. Utilizing neural architecture search (NAS), FasterSeg is discovered from a novel and broader search space integrating multi-resolution branches, that has been recently found to be vital in manually designed segmentation models. To better calibrate the balance between the goals of high accuracy and low latency, they propose a decoupled and fine-grained latency regularization, that effectively overcomes their observed phenomenons that the searched networks are prone to "collapsing" to low-latency yet poor-accuracy models. Moreover, they seamlessly extend FasterSeg to a new collaborative search (co-searching) framework, simultaneously searching for a teacher and a student network in the same single run.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. It compares supervised, unsupervised, and self supervised representation learning methods and studies the impact of locality and compositionality on ZSL performance for these methods. ### Other minor remarks   "Formally, f(x) \elem R is compositional if it can be expressed as a combination of the elements of ..."The definition would be more clear if the authors could mention some reasonable combination operators here (such as weighted average etc). Why are CNN representations restricted to  only  global information? ### Supporting arguments for the reasons for the decision. This paper acts as a good reminder that ZSL research should be done keeping in mind the goal of ZSL. Why does LC loss hurt performance for CMDIM? As a result, it s not a truly unsupervised learning method.<BRK>The paper proposes an evaluation framework for Zero Shot Learning (ZSL) methods called zero shot learning from scratch (ZFS) where the model is not allowed to be pretrained on other datasets such as ImageNet. Two main criteria are studied to study neural networks:  compositionality (ability to be expressed as a combination of simpler parts)  locality (ability to encode only information specific to locations of interest)To provide a better understanding of their claims, the authors use MTurk annotations to construct boolean map for each local part labelled in the CUB dataset.<BRK>Summary: This paper aims to investigate the role of locality and compositionality in zero shot learning. The authors propose a novel evaluation setup that differs from the original zero shot learning framework in that the model is not allowed to be pretrained on another dataset. The empirical evaluation is extensive.<BRK>In this work they study locality and compositionality in the context of learning representations for Zero Shot Learning (ZSL). The results of their experiment show how locality, in terms of small parts of the input, and compositionality, i.e.how well can the learned representations be expressed as a function of a smaller vocabulary, are both deeply related to generalization and motivate the focus on more local-aware models in future research directions for representation learning.
Accept (Poster). rating score: 8. rating score: 6. The paper introduces two neural networks, one for average regrets and one for average strategy, to (approximately) run CFR algorithm in (large) IIG. Issues:1) One downside of the paper is that it is very close to the "Deep Counterfactual Regret Minimization". I believe it should be accepted. For the large game, I like that the authors evaluated against an ACPC agent.<BRK>The authors proposed a double neural counterfactual regret minimization algorithm (DNCFR) that uses a RegretSumNetwork to approximate cumulative regret and an AvgStrategyNetwork to approximate the average strategy. The calculation of average strategy seems wrong. It should be noted that the sampling probability ofeach information set is not equal. If they areshowed, it would be helpful to understand the bias in the bootstrap learning.<BRK>Counterfactual regret minimization (CFR) is a fundamental and effective technique for solving Imperfect Information Games (IIG). It's a successful application of neural CFR in large games. In this paper, they propose a double neural representation for the IIGs, where one neural network represents the cumulative regret, and the other represents the average strategy. Empirically, on games tractable to tabular approaches, neural strategies trained with their algorithm converge comparably to their tabular counterparts, and significantly outperform those based on deep reinforcement learning.
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. just before 4.1, and "showed significance performance" in the last sentence of the paper. I strongly encourage the authors to carefully edit the paper before publication so that their nice work will be properly presented. The authors of this paper make several important contribution to this problem.<BRK>The approach is interesting, and the problem (modeling of diversity constraints) seems important. They have experimental results (on metric learning and image learning tasks) to show that optimization with the DPP + wasserstein  gan constraint (to ensure features lie in a bounded space) result in better quality.<BRK>Furthermore, the authors do not report standard deviations for their experiments. A key consideration when using DPPs is their compulational cost: most operations involving them require SVD (which seems to be used in this work), matrix inversion, and often both. Decision: I recommend that this paper be rejected. Could you report number of trials and standard deviations for your experiments?<BRK>In this paper, they devise a simple but effective algorithm to address this issue to optimize DPP term directly expressed with L-ensemble in spectral domain over gram matrix, which is more flexible than learning on parametric kernels. By further taking into account some geometric constraints, their algorithm seeks to generate valid sub-gradients of DPP term in case when the DPP gram matrix is not invertible (no gradients exist in this case).
Reject. rating score: 1. rating score: 3. rating score: 3. rating score: 3. The submission argues for the modeling the relationships between different tasks and incorporating such relationships when training multi task frameworks. Elaborated comments: A) Authors seem to be unaware of critically related prior work that specially modeled task relationships and did much of what s proposed in this submission, especially "Taskonomy: Disentangling task transfer learning". The "relationship among tasks" that this submission frequently talks about is the main concept in taskonomy 2018 paper (see their abstract). This submission should be majorly revised in light of prior work and the critically relevant ones should be discussed and experimentally compared to. B) The presentation suffers from missing critical specifics. While that seem to be one of the most important components of the method and its definition and extraction method should be discussed. Overall, unfortunately the submission suffers from serious issues in its current shape. Comments after rebuttal stage: Thanks to the authors for the rebuttal. A clear discussion on how the proposal is different and why it is better than the recent works that were not cited would be needed, and likely authors needed strong experimental comparison with some of them, eg to prove both general and data specific task dependency is needed. The task dependency at the basic level (word and node) may be different from the general task dependency".<BRK>The authors propose a multi task learning method that uses attention mechanism to identify relations between the tasks. Method:  The authors motivate the use of attention mechanism for identifying a sample dependent measure  of task relatedness by that "task dependency can be different for different data samples.." At the introduction stage this argument was not clear to me. I would suggest to expand this part of the paper by providing a stronger motivation for the proposed approach. Additional comments:  in its current form the manuscript is rather hard to follow, it requires a thorough proof reading  it is unclear what Figure 1 on page 2 is for  on page 2 phrase "... the label ratio is imbalanced." I believe the authors meant that the data (not label) proportions between the tasks are uneven  on page 3 the authors say that minimisation of the empirical risk (eq.(1)) is "the goal of multi task learning". I thank the authors for their comments. The quality fo the manuscript has indeed improved and the differences with the existing methods are clearer. However, in light of the reviewers  comments, I agree that at least the experimental section needs to be extended by adding relevant baselines. In particular, comparison to "End to end multi task learning with attention" is needed to demonstrate importance of the task level dependence measure.<BRK>This paper proposes a ‘Learning to Transfer via Modeling Multi level Task Dependency’ for multi task learning’, which uses the attention mechanism to learn task dependency. In the introduction, authors claim ‘most of the current multitask learning framework rely on the assumption that all the tasks are highly correlated’. I don’t think this claim is correct. In fact, most state of the art multi task learning models can learn task dependency via different forms. In the proposed network, different tasks have their own encoder, which leads to a large number of model parameters especially when there are a large number of tasks. The attention has been used in multi task learning. Of course authors need to compare with those related works. A typo: “is theposition wise mutual attention between”<BRK>The paper is on an improvement of multi task learning by considering the input tasks at two levels: (1) at task level, i.e.the relationship between the tasks and (2) by the data associated with each task. Their major argument is that most current methods hold the assumption that the tasks are correlated with each other but they conjecture that in the real world this is not necessarily true and try to model the relationship between the input tasks at these two levels and incorporate that in the learning framework. To show effectiveness of their approach they test their method on differently oriented public datasets representing graphs, nodes and text and compare performance with some of the recent approaches to multi task learning. 2.The distinction between the "general task dependency" and the "data dependency" does not seem significant enough. The data dependent task dependency actually depends on the "general task dependency" as stated in the paper. This is probably manifested in the relatively slight improvement of the method compared with the SOTA. Perhaps more clarity on the difference and contribution of each "level" would make the significance stand out  clearer.<BRK>Multi-task learning has been successful in modeling multiple related tasks with large, carefully curated labeled datasets. Besides, the understanding of relationships among tasks has been ignored by most of the current methods. Along this line, they propose a novel multi-task learning framework - Learning To Transfer Via Modelling Multi-level Task Dependency, which constructed attention based dependency relationships among different tasks. At the same time, the dependency relationship can be used to guide what knowledge should be transferred, thus the performance of their model also be improved. To show the effectiveness of their model and the importance of considering multi-level dependency relationship, they conduct experiments on several public datasets, on which they obtain significant improvements over current methods.
Reject. rating score: 3. rating score: 6. rating score: 6. The authors argue that the Fourier analysis is essential to show the bias of the estimator. However, the only conclusion they draw from Fourier analysis is eq.5. The paper can be greatly simplified if they remove all boolean analysis parts and start from eq.5 (which has a straightforward proof), using the conventional notation instead of Fourier coefficients. Overall, I argue rejecting the paper in its current form. This is not my first time reviewing this paper. It is only for the boolean space, that f has an multi linear form with Fourier expansion.<BRK>***Straight Through is a popular, yet not theoretically well understood, biased gradient estimator for Bernoulli random variables. The low variance of this estimator makes it a highly useful tool for training large scale models with binary latents. However, the bias of this estimator may cause divergence in training, which is a significant practical issue. The paper develops a Fourier analysis of the Straight Through estimator and provides an expression for the bias of the estimator in terms of the Fourier coefficients of the considered function.<BRK>Summary:The authors analyze the bias in the straight through gradient estimator using the framework of harmonic analysis of boolean functions. Based on this analysis, they propose three methods to reduce the bias of the straight through estimator, resulting in a less biased estimator that is the same computational complexity as the original. I enjoyed this paper   the exposition is clear, the ideas are (to my knowledge) novel and make sense, and the experimental evaluation is thorough and convincing. I recommend an accept. I skimmed through the proofs in the appendix so cannot with absolute confidence vouch for their correctness.<BRK>Stochastic neural networks with discrete random variables are an important class of models for their expressivity and interpretability. Since direct differentiation and backpropagation is not possible, Monte Carlo gradient estimation techniques have been widely employed for training such models. They use it to derive an analytic formulation for the source of bias in the biased Straight-Through estimator. Based on the analysis they propose \emph {FouST}, a simple gradient estimation algorithm that relies on three simple bias reduction steps.
Accept (Talk). rating score: 8. rating score: 8. rating score: 6. While such a pipeline for scaling using a graph coarsening and refinement based approach is not new, the authors have carefully designed the pipeline to be effective and be scalable such as without any costly learning components (as in mile). The effectiveness of the proposed approach is evaluated with the node classification task on 6 datasets. The paper proposes a well designed pipeline to scale existing embedding models. Weaknesses:  While the experimental results are convincing on the computation front, I have few concerns on the performance front. It can been seen from Figure 3 that the incorporation of the attribute graph provides a significant performance benefit. b) Improvements are inconclusive without additional results on other standard non attributed graph datasets. It is important to evaluate the embeddings additionally for the link prediction task at the least.<BRK>Summary: This paper proposes GraphZoom, a framework for augmenting unsupervised graph embedding methods by (a) fusing feature information into the graph topology, (b) learning embeddings on a coarsened graph, and (c) refining the coarsened embeddings to obtain embeddings for the original graph nodes. In particular, a nearest neighbor graph over node features is computed and this adjacency matrix is linearly combined with the original adjacency matrix to obtain a graph with feature information "fused in". The graph is then coarsened using a spectral approach, embeddings are learned on the coarsened graph (via any strategy), and the embeddings are then refined back to the original nodes (again using a spectral approach). The authors take care to heed the advice of Maehara et al.and remove high frequency information from the features.<BRK>Summary: The authors propose a way to fuse information on nodes of a graph with the topology of the graph in the large scale setting. Experimentally the authors see improvements in the performance using their approach compared to the baselines considered. Hence,  node covariance/fusion matrix being dense will be a blessing for spectral approaches since they make spectral methods work. However, is this what we want in *all* the cases? This means that the choice of \beta in their fusion step is *very* important, and I don t see any plots on the sensitivity of their procedure with respect to \beta. I kindly request the authors to include a plot or results showing the sensitivity of the final results with respect to the choice of \beta.<BRK>GraphZoom first performs graph fusion to generate a new graph that effectively encodes the topology of the original graph and the node attribute information. This fused graph is then repeatedly coarsened into much smaller graphs by merging nodes with high spectral similarities. GraphZoom allows any existing embedding methods to be applied to the coarsened graph, before it progressively refine the embeddings obtained at the coarsest level to increasingly finer graphs. They have evaluated their approach on a number of popular graph datasets for both transductive and inductive tasks.
Reject. rating score: 1. rating score: 1. rating score: 3. This paper argues that text generated by existing neural language models are not as good as real text and proposes two metric functions to measure the distributional difference between real text and generated text. Major issues:This manuscript is poorly organized and the introduction is not well written. The description in the first paragraph about neural language models is not accurate. To test the effectiveness of a good metric, extensive experiments on toy datasets such as MNIST, CIFAR10, and synthetic datasets should be conducted. The claimed failure experiments make the proposed metrics even more questionable. The proposed metrics are questionable and should be thoroughly tested on synthetic and toy datasets before deploying it for text generation.<BRK>This paper proposes two metrics to measure the discrepancy between generated text and real text, based on the discriminator score in GANs. Empirically, it shows that text generated by current text generation methods is still far from human generated text, as measured by the proposed metric. It s also unclear how the proposed metrics compare to simply using the discriminator for evaluation. What s the advantage of the proposed metric, compared to existing ones, e.g.KL divergence, total variation etc.? The discrepancy could be due to both data difference and classification error.<BRK>The methodology is however not particularly well motivated and the experiments do not convince me that this proposed measure is superior to other reasonable choices. Overall, the writing also contains many grammatical errors and confusing at places. "Density Ratio Estimation in Machine Learning"Given all these existing methods (I am sure there are many more), it is unclear to me why the estimator proposed in this paper should be better. As such, the authors should demonstrate a power analysis of their test to detect differences between real vs generated text and show this new test is better than tests based on existing discrepancy measures. The authors claim training a generator to minimize their proposed divergence is superior to a standard language GAN.<BRK>The text generated by neural language models is not as good as the real text. Generative Adversarial Nets (GAN) are used to alleviate it. However, some researchers argue that GAN variants do not work at all. In this paper, they theoretically propose two metric functions to measure the distributional difference between real text and generated text. Experimenting on two existing language GANs, the distributional discrepancy between real text and generated text increases with more adversarial learning rounds.
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. The authors prove global optimality and rates of convergence of neural natural/vanilla policy gradient. Their results rely on the key factor for "compatibility" between the actor and critic. This is ensured by sharing neural architectures and random initializations across the actor and critic. The paper is well written with clear derivations. I suggest the publication of this paper.<BRK>[Summary]This paper studies the convergence of actor critic algorithms with two layer neural networks under iid assumption. Theoretical results show that, in the aforementioned setting, policy gradient and natural policy gradient converge to a stationary point at a sublinear rate and natural policy gradient s solution is globally optimal. While these results may not have immediate practical interest, the analysis is an important step in understanding the behavior of actor critic algorithms with neural networks. [Comments]The first important assumption is the architecture of the neural network. The description of results in the abstract and introduction should clarify this setting.<BRK>This paper studies policy gradient where the policy is parameterized by an extremely wide neural network. The authors should provide more details about the function $u_{\hat \theta}$ defined in eq (4.4), which seems to approximate the critic function. There is a prior work by Agarwal et al.(2019) that proves the global convergence of both vanilla policy gradient and natural policy gradient methods.<BRK>Policy gradient methods with actor-critic schemes demonstrate tremendous empirical successes, especially when the actors and critics are parameterized by neural networks. Particularly, they show that a key to the global optimality and convergence is the "compatibility" between the actor and critic, which is ensured by sharing neural architectures and random initializations across the actor and critic. To the best of their knowledge, their analysis establishes the first global optimality and convergence guarantees for neural policy gradient methods.
Reject. rating score: 3. rating score: 3. rating score: 8. ######## Updated Review ############The author(s) have presented a sincere rebuttal, which I really appreciate. #################################This paper proposed a generative modeling framework called potential flow generator. Instead of deriving new matching criteria between distributions, the authors considered redefining the generative process via simulating a continuous flow that is constrained by the optimality conditions on the flow potential field derived based on L2 optimal transport. This is certainly an interesting direction to explore, however, while the points made are valid, they are not well justified. My major criticism is that too much compromise needs to be made in order to construct such a flow generator. My overall evaluation for this work is a straightforward/brute force application of well known (but less practical) results, without proposing any remedies to the real challenges that underlie. This point is partly evidenced by the experiment section where none of the input distributions is far from the target. It s questionable whether this framework can efficiently perform "generative modeling", in which a simple noise distribution is pushed to a more sophisticated target distribution. 4.The experiments are weak and not convincing. First, ss mentioned in earlier comments, 2D toy transport and image translation are fairly easy tasks. Second, only qualitative results are reported, and there is no baseline model to compare with. Third, without ablation study, We can hardly verify the fact the gains are actually coming from the flow part, as vanilla GANs can also perform a similar task.<BRK>The paper proposes a ‘potential flow generator’ that can be seen as a regularizer for traditional GAN losses. It is based on the idea that samples flowing from one distribution to another should follow a minimum travel cost path. This regularization is expressed as an optimal transport problem with a squared Euclidean cost. Experiments on a simple 1D case (where the optimal transport map is known), and on images with an MNIST / CelebA qualitative example. The use of this dynamic formulation is well known in the OT community. The novelty arises from the use of neural networks to represent the potentials. However, the claim that the obtained map is the optimal transport map seems wrong to me, because:The class of potential functions over which the optimization is performed is not the whole class of functions, leading to approximations;The optimality conditions (a.k.a continuity or preservation of mass equations) are only enforced on sampled trajectories, not on the entire space. While this claim should definitely be lowered, it is nonetheless still acceptable provided that the proposed model is performing good. On this part, the paper strength could be improved provided that comparisons with existing methods computing a Monge map could be given. Notably, a comparison with the approach from Seguy et al.2018 is missing. The final, total, optimization problem is never clearly expressed. I believe a general algorithm presentation could help in understanding the general picture of the method. Can you comment on this point ?<BRK>This is a great paper using optimal transport theory for generative and implicit models. Instead of using general vector fields, the authors apply the potential vector fields in optimal transport theory to design neural networks. The mathematics is correct with convincing examples. This brings an important mathematical connection between fluid dynamics and GANs or implicit models. I suggest the acceptance of this paper after addressing the following minor questions. 1.Would the authors provide slightly more details about the design of networks? "W. Li, G. Montufar, Natural gradient via optimal transport, 2018"The Wasserstein natural gradient method there may improve the computational speed of the proposed models. In all, this is an exciting paper with many potentials in future neural network designs.<BRK>They propose a potential flow generator with $L_2 $optimal transport regularity, which can be easily integrated into a wide range of generative models including different versions of GANs and flow-based models. With up to a slight augmentation of the original generator loss functions, their generator is not only a transport map from the input distribution to the target one, but also the one with minimum $L_2 $transport cost. Subsequently, they demonstrate the effectiveness of the potential flow generator in image translation tasks with unpaired training data from the MNIST dataset and the CelebA dataset.
Reject. rating score: 3. rating score: 6. rating score: 6. The experimental validation is largely lacking, as the authors only perform experiments on ImageNet and do not compare against recent state of the art Bayesian sparsification methods (SBP, VIB, L0 regularization). Pros  The proposed model, RigL, is memory  and computation  efficient, and thus allows to train a large network in an efficient manner. [Yoon et al.18] Lifelong learning with dynamically expandable networks, ICLR 2018<BRK>Due to the dynamic network topology, the paper s methods exactly achieve the memory and computation efficient. The author claims that the ticket in the paper does not rely on a "lucky" initialization. i) Required memory is only proportional to the size of the sparse model.<BRK>As a result, networks can stay sparse throughout training and testing, leading to a large reduction in computational cost. I also appreciate the experiments on MobileNet, a setting where one expects investigations into sparse network architectures to have significant application. As the authors state, the novelty of their method is that they use the gradients with the highest magnitudes to grow connections.<BRK>Sparse neural networks have been shown to yield computationally efficient networks with improved inference times. There is a large body of work on training dense networks to yield sparse networks for inference (Molchanov et al., 2017; Zhu & Gupta, 2018; Louizos et al., 2017; Li et al., 2016; Guo et al., 2016). This limits the size of the largest trainable sparse model to that of the largest trainable dense model.
Reject. rating score: 1. rating score: 3. rating score: 3. This paper proposes a training objective that combines three terms:* A Stein discrepancy for learning a energy model with intractable normalizing constant* A Wasserstein GAN objective for learning an implicit neural sampler. * A Stein discrepancy for minimizing the distance between distributions defined by the energy model and the GAN. The third term is called "Stein bridging" by the authors.<BRK>The energy based model (E) is trained using Stein Divergence with a fixed kernel k or a learned critic who s parameters are denoted pi in the paper. Summary of the paper: The paper proposes to train implicit  model such as gan and an explicit model (Energy Based ) jointly . 3  There a lot of gaps in the proofs of Theorems 1 and 2.<BRK>This paper proposes to train a GAN and an EBM jointly, and bridge them using a Stein discrepancy. Both the idea and the experiment results are interesting. There are also typos and issues elsewhere.<BRK>Deep generative models are generally categorized into explicit models and implicit models. To mitigate these issues, they propose Stein Bridging, a novel joint training framework that connects an explicit density estimator and an implicit sample generator with Stein discrepancy.
Reject. rating score: 1. rating score: 3. rating score: 3. rating score: 6. The paper proposes a method for lossy image compression. Based on the encoder decoder framework, it replaces the discrete codes by continuous ones, so that the learning can be performed in an end to end way. It only compares with one method, which is not enough. From the main content, the proposed method is improved upon (Havasi 2018). But it is not compared. 5.Ablation study is also needed.<BRK>Variational image compression with a scale hyperprior. The compression performance is evaluated by training the proposed model and the competing neural network based method [1] on the Clic dataset, and evaluating it on a subset of the images of the Kodak dataset. JPEG is also used as a baseline. Moreover, the encoding time is significantly longer than this same baseline. Supporting arguments for decision:Although the motivation for circumventing the quantization step seems plausible, the authors show no evidence that the competing method [1], which does perform a post training quantization step, actually suffers from it. More importantly, the “theoretically” achievable results of the proposed method in seem only competitive in the low bit rate regime and worse in the higher bit rate regimes. The authors do provide a reason for why the “theoretical” and “actual” results are so far apart, but are unfortunately not able to overcome this issue. The quality of the empirical study can be improved.<BRK>The authors propose a new image compression method that does not require quantizing the encoded bits in an auto encoding style image compression model. The results also beg the question: How much does quantization in existing methods impact performance, and how much will fixing this benefit the overall system. Finally, in my opinions, in some sense the sampling of z_{c^\star} is also a form of quantization. Experiments were conducted on the Kodak dataset based on the model of Balle et al.The proposed method works only slightly worse than Balle et al.at low bit rate region, but the gap becomes larger in higher bit rate regions.<BRK>This paper studied the image compression problem. Specially, the authors proposed to use neural networks to act as encoder / decoder. In summary, this paper gets rid of commonly used quantization techniques in image compression by using an approximate importance sampler which produces the encoding of images in a non deterministic manner. With the construction of parameterized encoder / decoder, end to end training is conducted by popular gradient descent. In other words, how to ensure that the performance is not obtained from the power of the backbone but from the proposed method itself. Are there any possible experiments which can be conducted to show the effectiveness by using different architectures?<BRK>This process, due to the quantization step, is inherently non-differentiable so these algorithms must rely on approximate methods to train the encoder and decoder end-to-end. In this paper, they present an innovative framework for lossy image compression which is able to circumvent the quantization step by relying on a non-deterministic compression codec. The decoder maps the input image to a distribution in continuous space from which a sample can be encoded with expected code length being the relative entropy to the encoding distribution, i.e.it is bits-back efficient. To showcase the efficiency of their method, they apply it to lossy image compression by training Probabilistic Ladder Networks (PLNs) on the CLIC 2018 dataset and show that their rate-distortion curves on the Kodak dataset are competitive with the state-of-the-art on low bitrates.
Reject. rating score: 1. rating score: 3. The paper considers synthetic data generation using deep GANs and autoencoders that can be shared for model training. In particular, there is a significant number of misprints and inconsistencies here and there (see more on this below). The algorithm $\mathcal{M}$ is not defined. Articles in many places can be improved (finding good autoencoder  > finding a good autoencoder)7. No reference to Figure 3 in the text of the paper.<BRK>This paper proposed a new algorithm for synthetic data generation under differential privacy. The presentation of the paper needs to be improved. The authors claimed that the proposed new evaluation metrics are novel contributions of the paper but there is no discussion on why they are good metrics for evaluating the quality of synthetic datasets nor which metric should be used in what scenarios. The authors mentioned comparison with DP GAN but it is not marked in the figures the performance of DP GAN and how its results compared with DP auto GAN.<BRK>This framework can be used to take in raw sensitive data, and privately train a model for generating synthetic data that should satisfy the same statistical properties as the original data. They implement this framework on both unlabeled binary data (MIMIC-III) and unlabeled mixed-type data (ADULT). They also introduce new metrics for evaluating the quality of synthetic mixed-type data, particularly in unsupervised settings.
Reject. rating score: 3. rating score: 3. rating score: 8. Do the overall results mean that the "maximum informedness" based metrics are superior to the others for assessing selectivity of a unit? In overall, the manuscript is well written and easy to follow. One of my key concerns, however, is that I am still not fully convinced whether the key finding in this paper   the lack of highly selective units in CNNs   is an indeed important problem for ICLR community: Personally, I feel the "existence" of selective units in RNN could be interesting, but the "non existence" in the case of CNN is not that surprising for some readers, as it seems much likely (at least to me): The final layer of CNN would be surely selective across classes, but it may be not the case for the hidden layers   Nevertheless, some of the units may act selectively, not across classes but in some other concepts: e.g.stripes, orientations, etc.<BRK>The paper empirically studies the category selectivity of individual cells in hidden units of CNNs. The claimed finding is that there are no cells that are "sufficiently" selective to be called object detectors. In the words of the paper, the "selective units are sensitive to some feature that is frequently, but not exclusively associated with the class"   I thought this is the standard majority view, not a surprising finding. I agree with the authors that there is by now a zoo of selectivity metrics that are not always highly correlated. That might be unrepresentative, e.g., a neuron might, for that particular class, always have high activation due to some very common background context, and still be not selective at all.<BRK>This work investigates the collection of methods that have been proposed to find units in neural networks that are selective for certain object classes. Previous works have used different measures of selectivity (with sometimes contradictory results), and the authors investigate the degree to which these units qualify as “object detectors”. The authors find that (1) different proposed measures of selectivity are not consistent and (2) units identified as selective cannot be considered object detectors due to the high false alarm / low hit rates, analyzing a large number of selectivity measures.<BRK>Various methods of measuring unit selectivity have been developed with the aim of better understanding how neural networks work. Indeed, the most selective units had a poor hit-rate or a high false-alarm rate (or both) in object classification, making them poor object detectors. They fail to find any units that are even remotely as selective as the 'grandmother cell' units reported in recurrent neural networks. In order to generalize these results, they compared selectivity measures on a few units in VGG-16 and GoogLeNet trained on the ImageNet or Places-365 datasets that have been described as 'object detectors'.
Reject. rating score: 1. rating score: 6. rating score: 6. The motivation there is to use the recovered reward on a new system with different transition dynamics. In the context of this paper though, I would like to understand the angle of reward ambiguity. Diversity analysis and ablations are also performed to dissect the performance of the proposed approach. This problem is not explained or motivated well in the paper, but instead the readers are referred to the AIRL paper.<BRK>Overall this is a good paper. 1.2 The motivation for adding a constant to shift nash equailibrium could be stated clearer. 1.3 what is the state s in the context of image captioning? 1.4 since the reward is computed for each pair of (a, s), how to get the reward for the whole sentence? 1.5 in Eq.(5) and Eq.(11), there are expectations. 2. experiments:  1.1 the experiment in terms of "compactness" may not  reflect well the concept of compactness.<BRK>Furthermore, a conditional term is introduced in the loss function to avoid mode collapse and to increase the diversity of the generated captions. Throughout experiments on MS COCO show that the proposed method achieves state of the art performance with several evaluation scores. The idea to disentangle the sentence level reward into word level ones with Adversarial Inverse Reinforcement Learning (AIRL) is highly motivated. As reported, this refinement surprisingly improves the performance and achieves state of the art performance, while the original AIRL degraded the performance. I would like to recommend that the source code to reproduce the result should be released.<BRK>However, the learned reward of existing adversarial methods is vague and ill-defined due to the reward ambiguity problem. In addition, they introduce a conditional term in the loss function to mitigate mode collapse and to increase the diversity of the generated descriptions. Their experiments on MS COCO show that their method can learn compact reward for image captioning.
Reject. rating score: 1. rating score: 1. rating score: 3. The paper proposes a new sentence embedding method. The idea to use linguistic knowledge in the design of sentence embeddings is attractive. Moreover, the proposed method do not scale well and empirical results on classical downstream tasks are not convincing. Last, in my opinion, the redaction of the paper should be improved and the bibliography should be updated. Please explain why you choose dependency trees and explain why their use can improve sentence embeddings. * Related work. This is not surprising because dependency trees are used for learning.<BRK>Overview: This work proposes to learn sentence embeddings using both contrastive learning and multiple "views" of sentences. Overall, I recommend rejecting this work. Overall, I don t think the claims in the paper are well supported by the model proposed or the experiments. I have a number of concerns about the experiments. The results seem to indicate that this method underperforming recent work significantly.<BRK>This paper describes a self supervised sentence embedding approach that incorporates a different view from plain text where some extent of linguistic knowledge is incorporated through the application of tree LSTM. Although the experiments are thorough, I am in favor of rejecting this paper with the following reasons:First, the proposed model is trained with 4.6M sentences among 78M available for 33 hours. I am happy to raise my score if authors can show the results of a well trained proposed model. According to cosine similarity, wouldn’t this be 0 and also show up in the baseline model regardless of the embeddings?<BRK>In this work, they propose a self-supervised method to learn sentence representations with an injection of linguistic knowledge. Formally, multiple views of the same sentence are mapped to close representations.
Reject. rating score: 1. rating score: 3. rating score: 3. The problem addressed by this paper is the estimation of trajectories of moving objects thrown / launched by a user, in particular in computer games like angry birds or basketball simulation games. I have several objections, which can be summarized by the simplicity of the task (parabolic trajectories without any object/object or object/environment collisions / interactions), the interest of the task for the community (how does this generalize to other problems?), and the writing and structuring of the paper. The work should be properly described and related to the proposed work. The simplicity of the task is also further corroborated by the small number of samples used to estimate these parameters (in the order of 300). A further indication is the fact, that the decoder in the model is fully hardcoded. The paper is not well enough structured and written, many things are left unsaid.<BRK>This paper presents a method for predicting the trajectories of objects in video (or phone) games by learning the physical parameters underlying the movements. Yet, I think the authors did spend some time and this work might be suited for a workshop. Remarks and questions:  the writing of the paper is not enough to make it clear, and a lot of sentences are not readable. Estimating the parameters of an equation used in a game is not really interesting.. as we have to know the equation, it has to be simple, we have to extract the trajectory from the game... but there might be other related applications that could motivate this work. Figure 6: How come in the two last images, we see different starting points? and compared to the distance of the trajectory?<BRK>This paper proposes an architecture that encodes a known physics motion equation of a trajectory of a moving object. Moreover, after reading the paper the use case of the proposed method is not clear to me and the writing is unclear (see examples above and below). It is not clear to me why the authors don’t use these for the MSE loss used to train InferNet (rather than using the projectile motion equation). — Minor —  The term ‘in game variables’ is used in a few places and is explained later in the text (Pg.5).<BRK>In this work they present an approach that combines deep learning together withlaws of Newton ’ s physics for accurate trajectory predictions in physical games. Their model learns to estimate physical properties and forces that generated givenobservations, learns the relationships between available player ’ s actions and estimatedphysical properties and uses these extracted forces for predictions. They evaluate their model abilities to extractphysical properties and to generalize to unseen trajectories in two games with ashooting mechanism. They also evaluate their model capabilities to transfer learnedknowledge from a 2D game for predictions in a 3D game with a similar physics.
Reject. rating score: 3. rating score: 3. This paper introduced a linear interpolation method that could be applied to the latent space of a generative model. With their method, interpolating instances generated by those generative models all maintain high quality in terms of the realism index they proposed. There are two types of realism index which has analytic form introduced in this paper. The one based on normal density. If the density of latent feature is normal, normal density based index is used and it could be approximated with a analytic form; while if the density is not accessible, then f is the gaussian density of certain transformation of features in the latent space. Typo in Equation (3): w >s?<BRK>Overall, this score is empirical and aims at circumventing the subjective analysis and it should be better reflected in the paper. While it is obvious that this score could be used, is it possible to make the empirical assessment? E.g.compare between two scores on an extensive amount of data. This might be used for the assessment of the realism index on real images as stated before. .., k} and then consider the linear interpolation between xi and xj given with... . Could the authors elaborate on why does this acceleration happen?<BRK>In order to perform plausible interpolations in the latent space of a generative model, they need a measure that credibly reflects if a point in an interpolation is close to the data manifold being modelled, i.e.if it is convincing. In this paper, they introduce a realism index of a point, which can be constructed from an arbitrary prior density, or based on FID score approach in case a prior is not available.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. The main contributions are 1) a spatial discount factor that can stablize the learning process, 2) a differentiable communication protocol NeurComm. I voted for "Weak Accept" because this paper can have important real world applications such as traffic control, autonomous driving and power grid. I really like its evaluations on the realistic environments. In addition, the paper is clearly written, the algorithm seems reasonable and the evaluations are comprehensive. It is not a very strong result. And I would also suggest the paper adding more test cases to show that the proposed algorithm indeed can dominate in most of the cases.<BRK>Each agent might control a traffic light (exp 1) or a car in traffic (exp 2). The authors compare their method with CommNet (averages messages before broadcast), DIAL (small scale direct communication), etc. 3.Supporting argumentsThe experiments and analysis of the more general communication scheme are nice and the assumptions used make sense for the environments considered (spatial interactions and dynamics). 4.Additional feedback with the aim to improve the paper.<BRK>This paper is concerned with network multi agent RL (N MARL), where agents need to update their policy based on messages obtained only from neighboring nodes. This is done under sensible restrictions on the state transition distribution, which can be claimed to hold true in realistic networked settings. The authors argue that introducing a spatial discount factor (along a temporal one), where neighboring nodes have a small distance, stabilizes learning. Also, they provide a way of learning a networked communication protocol.<BRK>This paper considers multi-agent reinforcement learning (MARL) in networked system control. They formulate such a networked MARL (NMARL) problem as a spatiotemporal Markov decision process and introduce a spatial discount factor to stabilize the training of each local agent. Further, they propose a new differentiable communication protocol, called NeurComm, to reduce information loss and non-stationarity in NMARL. Based on experiments in realistic NMARL scenarios of adaptive traffic signal control and cooperative adaptive cruise control, an appropriate spatial discount factor effectively enhances the learning curves of non-communicative MARL algorithms, while NeurComm outperforms existing communication protocols in both learning efficiency and control performance.
Reject. rating score: 3. rating score: 3. rating score: 6. This paper proposes a functional characterization to understand the empirical success of deep neural networks. In particular, this paper focuses on the case of deep fully connected univariate ReLU networks, and show that the parameters will result in a Continuous Piecewise Linear (CPWL) approximation to the target function. Moreover, the authors derive the induced distributions of the function space parameters and show that increasing width can reduce the roughness of the initial function. However, this paper is not well written and organized, and there are some “?? The authors should elaborate more on this. Thanks for your response. I still think the contribution of this paper is not enough as the theoretical analysis may not be able to be generalized to deep networks.<BRK>The paper studies a number of interesting phenomena in deep learning by characterizing the linear regions of fully connected ReLU networks. The paper has interesting analyses, but I think the main drawback is that the clarity and presentation could be improved. In particular, while reading the paper, I found myself wanting:  More discussion of connections to relevant work. There have been a few papers (e.g.https://arxiv.org/abs/1611.01491, https://papers.nips.cc/paper/5422 on the number of linear regions of deep neural networks.pdf, http://proceedings.mlr.press/v80/serra18b/serra18b.pdf) that use linear regions to understand deep networks. More expository text for particular concepts. A number of results are presented, and it would be helpful to have brief high level summaries of the main findings of each section after diving through technical details.<BRK>This paper wants to answer the question what is the value of the neural network’s depth? (2) The conclusions of the paper are inspiring, e.g., depth makes it easier for GD to the optimizer. Suggestions: I think that a figure that shows breakpoint and input data distribution together will be very interesting. They found that the value of depth in deep nets seems less about expressivity, but enable GD to find better solutions.<BRK>Despite their popularity and successes, deep neural networks are poorly understood theoretically and treated as 'black box' systems. This allows us us to theoretically or experimentally probe properties of these networks, including the effect of standard initializations, the value of depth, the underlying loss surface, and the origins of generalization. One key result is that generalization results from smoothness of the functional approximation, combined with a flat initial approximation.
Reject. rating score: 3. rating score: 3. rating score: 6. However, I vote for reject, since the novelty is somehow limited, the claims made in the paper is not well supported and experiments are not very convincing. Though it is claimed that this paper extends previous results on classification/regression to conditional density estimation which is a more general case. This claim is not well supported.<BRK>It further proves the consistency of the method. Pros:(i) The paper is well written. Cons:(i)  The method itself is not novel. Adding noise to for regularization is a quite common technique used in many different applications. (ii) The experiments performed in the paper are all very small scaled. Not very convincing.<BRK>This is the same idea behind image augmentation which forms a crucial part of training supervised models in vision. It would surprise that this very natural idea has not been tried before. I have gone through the theoretical derivations in the paper and they look sound to me. I am not an expert however in this setting so it is hard for me to judge the quality and significance of the benchmarks. The experiment methodology nevertheless looks sound.<BRK>Due to the inherent structure of such models, classical regularization approaches in the parameter space are rendered ineffective. To address this issue, they develop a model-agnostic noise regularization method for CDE that adds random perturbations to the data during training. In their experiments, noise regularization significantly and consistently outperforms other regularization methods across seven data sets and three CDE models.
Reject. rating score: 3. rating score: 3. rating score: 3. The approach in this paper and its formalization of the task are interesting, but the significance of the techniques is somewhat unclear and the experiments could be more thorough in terms of the baselines and data sets considered. Making crisper, less ambiguous distinctions between this work and previous work would help. Finally, experiments that consider larger hierarchies (here, the number of target classes tends to be small, CIFAR 10 and MNIST each have ten classes, meaning the hierarchies are not very rich) would help illustrate the potential power of the techniques. MinorI didn’t understand the following statement, and given that it’s a fairly bold claim I would rephrase it or explain it better in the paper body rather than referring the reader to the appendix:“A standard DNN unknowingly uses low quality data also to train higher layers, even if there is no high level information in the data.”I don’t understand what the right arrow operator on the top of page 5 means. I think it would be helpful if before Equation 1, you mentioned this holds for strictly nested Y_i’s (since earlier in the paper, Y_i referred to more general things). I assume the ECE is computed over held out validation data (i.e., not training data)?<BRK>The paper proposes a network architecture with multiple bottleneck layers, one for each label level, and skip connections. The experiments show that coarse labels help learning and can improve label efficiency, i.e.don’t need all fine labels to get good classification performance. 2.Opinion and rationalesWhilst I think the execution of ideas is good and the motivation is very practical, I’m leaning towards “reject” for this paper due to the reasons below. This ensures the relationship of the entropies between the label layers. ii.The novelty of the proposed architecture and training approach is low. The network is a nested structure of successive classifiers. 3.Minor detailsSome citations should be enclosed in brackets<BRK>Nor is the architecture. I would’ve liked to have seen more of what the role of the training regime is on the outcomes and how the network’s gradient’s behave in different regimes. But in general, the paper is well organized and argued, although there is a little belaboring of ideas of entropy and mutual information only to use it to buttress a point that is made and left hanging. Generalization to any number of nested labels is not demonstrated  The empirical demonstration of contribution of skip connections is not too    Corollary to above, what do you think would be the role of attention in the hierarchical representation learning? Why not use, for example, regularization instead? Does this not refute the claim that the nested model “gradually breaks”?<BRK>This specificity often leads to overconfident models that generalize poorly to samples that are not from the original training distribution. Since the network can be naturally trained with mixed data labeled at different levels of nested details, they also study what is the most efficient way of annotating data, when a fixed training budget is given and the cost of labels increases with the levels in the nested hierarchy. Finally, standard DNNs do not produce results with simultaneous different levels of confidence for different levels of detail, they are most commonly an all or nothing approach. They explicitly enforce this behaviour by creating a sequence of nested information bottlenecks.
Accept (Poster). rating score: 6. rating score: 3. rating score: 3. However, these asynchronous methods suffer from the stale gradients where by the time a worker sends the gradients to the master server, the model parameters have changed based on the gradients received from other workers. In particular, when the master receives a gradient from a worker, it computes the norm of the difference between the current model parameter and the past model parameter associated with the gradient. The master then computes *gap* value based on this norm and the norm of the average gradient. The paper establishes the convergence rate for the gap aware asynchronous method which is similar to the convergence rate of SGD. The empirical results demonstrate the advantage of the proposed gap aware method over other baselines. Pros  The extensive empirical evaluation shows that the proposed method is effective in preventing performance degradation in an asynchronous setup across tasks and models. This is much higher than the staleness aware method where the master stores a single scalar for each worker.<BRK>The paper introduces a new variant of asynchronous SGD, GA ASGD, for distributed training. The main contribution of this paper is to introduce a new way of measuring weight staleness and to explore the idea of penalizing the gradient itself to mitigate the staleness issue. Strengths:+ Introduced a novel approach to measure the parameter staleness, which helps penalize the gradients instead of the learning step. Overall, I think this is good work. The comparison to prior ASGD based approaches are extensive, and the improvements seem decent. For example,  the gap between the accuracy of GA vs. SGD can be as large as 3.46% when there are 128 workers. That gap might be closed with additional hyperparameter tuning, but it is unclear from the current draft and results. It would have been better to show the trade off between accuracy and performance in comparison with SGD. The convergence analysis indicates that by increasing the batch size, the convergence speed of GA ASGD will decrease.<BRK>This paper proposes a simple idea to mitigate gradient staleness in asynchronized distributed systems. Instead of scaling the staleness as in SA method, the authors propose to scale with the GAP, which is defined as the distance between current parameter and the staled parameter. The paper is fairly well written, and the overall idea is interesting and simple in implementation. I have the following comments:1. I think the theory, at the current stage, is not sufficient. The authors only provide a convergence bound for the proposed method, which does not tell anything about the advantages over other methods such as the ASGD and SA methods. From the current presentation, it seems there is no theoretical advantages of the proposed method over existing methods, which seems to suggest that the empirical advantage might not come from the algorithm itself.<BRK>Synchronous stochastic gradient descent (SSGD) suffers from substantial slowdowns due to stragglers if the environment is non-dedicated, as is common in cloud computing. Asynchronous SGD (ASGD) methods are immune to these slowdowns but are scarcely used due to gradient staleness, which encumbers the convergence process. In this paper they define the Gap as a measure of gradient staleness and propose Gap-Aware (GA), a novel asynchronous-distributed method that penalizes stale gradients linearly to the Gap and performs well even when scaling to large numbers of workers. They also provide convergence rate proof for GA. Despite prior beliefs, they show that if GA is applied, momentum becomes beneficial in asynchronous environments, even when the number of workers scales up.
Reject. rating score: 3. rating score: 3. rating score: 8. This paper suggests using a Unet type architecture to perform end to end source separation. They also report a very marginal performance improvement over an STFT based model (open unmix) The improment is 0.02 dB (table 1), and I am not sure if it is statistically significant.<BRK>The main contribution of this work is the improvement of an end to end waveform to waveform separation models through a number of architectural changes that allow such waveform to waveform models to perform comparably with other current state of the art methods that instead operate in the spectrogram domain. Overall, the paper is generally well written and the method is easy to follow. The specific architecture proposed in this task does appear to improve performance for this particular task, but it is not clear to me that the conclusions drawn based on the study in this work will be generally applicable to other related tasks. 2.The section describing the evaluation metric SDR wasn’t very clear to me. Or alternatively, more details could be added to explain the computation more clearly.<BRK>The authors present modifications to the state of the art (waveform based) source separation model (Wave U Net) and improve the state of the art to be comparable to spectral masking based methods. They clearly outline all the architectural changes they make (GLU nonlinearities, strided upsampling, bidirectional RNN), perform thorough evaluations against strong baselines, and a complete ablation study to demonstrate the value of each component.<BRK>Source separation for music is the task of isolating contributions, or stems, from different instruments recorded individually and arranged together to form a song.Such components include voice, bass, drums and any other accompaniments. While end-to-end models that directly generate the waveform are state-of-the-art in many audio synthesis problems, the best multi-instrument source separation models generate masks on the magnitude spectrum and achieve performances far above current end-to-end, waveform-to-waveform models. This makes their model match the state-of-the-art performances on this dataset, bridging the performance gap between models that operate on the spectrogram and end-to-end approaches.
Reject. rating score: 3. rating score: 3. rating score: 3. The paper presents a method to derive shaping rewards from a representation learnt with CPC. They propose learning a CPC representation from some random data and fix it. They assume exploration is not too difficult so that rewards are achievable without additional mechanisms. distance or via a clustering step. I find the analysis of the results well executed though I think that they should be improved for publication. In that paper they don t need the CPC future state predictors but instead contrast the goal and the final state of the trajectory. They use the resulting embedding to learn a reward function and ignore the extrinsic reward.<BRK>  *Synopsis*:  This paper proposes using the features learned through Contrastive Predictive Coding as a means for reward shaping. Specifically, they propose to cluster the embedding using the clusters to provide feedback to the agent by applying a positive reward when the agent enters the goal cluster. Unfortunately, I have several concerns over the method as currently implemented and the empirical comparisons (specifically with the chosen competitors) which I detail below. 3.The current competitors are unsatisfactory as they don t include other reward shaping techniques from the literature. In light of these other methods, I m not sure your discussion on only using predictive features for reward shaping is accurate, and instead these claims should be softened for only features learned through CPC.<BRK>The paper proposes a reward shaping method which aim to tackle sparse reward tasks. The main difference from the previous work (i.e.CPC) is that the paper uses the learned representation for reward shaping, not for learning on top of these representation. However, the proposed approach seems only able to work in environments where exploration with random policy can generate trajectories that contain sufficient environment dynamics (e.g.dynamics near the goal states). Why did you use the default the parameters? The paper has some imprecise parts:1.<BRK>By learning predictive representations offline and using these representations for reward shaping, they gain access to reward signals that understand the structure and dynamics of the environment. In particular, their method achieves better learning by providing reward signals that 1) understand environment dynamics 2) emphasize on features most useful for learning 3) resist noise in learned representations through reward accumulation.
Reject. rating score: 3. rating score: 3. rating score: 6. The paper presents graph neural network approach for learning multi view feature similarity. ****After rebuttal update. 2) Paper states that the method is unsupervised. I don`t understand, why not use them. images, which are related by homography. Since, it is not training, it could be done quite fast. Overall, our paper is a novel approach to learning feature representations, a topic of great interest to the ICLR community, rather than a new structure from motion system that has to prove its superior performance over the current state of the art.<BRK>This paper proposes a multi image matching method using a GNN with cyclic and geometric losses. Given two vertices, it has the form of Eq.(4), which is called cycle consistency constraints in this paper. This needs to be justified. And, the effect of geometric consistency term is not clear at all in the experiments.<BRK>Hence applying cycle consistency to figure out image matchings. Experiments show effectiveness of adding the epipolar constraints. Their experiments show that this is a promising approach, but probably requires further research to achieve state of the art results. I believe this work is a valuable and novel method for pruning the sift feature matches. would be a promising first step. Also it has been shown that GNNs performance deteriorates with increased depth.<BRK>They use cycle consistency to train their network in an unsupervised fashion, since ground truth correspondence can be difficult or expensive to acquire. Geometric consistency losses are added to aid training, though unlike optimization based methods no geometric information is necessary at inference time. To the best of their knowledge, no other works have used graph neural networks for multi-image feature matching. Their experiments show that their method is competitive with other optimization based approaches.
Reject. rating score: 3. rating score: 3. rating score: 8. This paper studies an important question: how to reduce memory bandwidth requirement in neural network computation and hence reduce the energy footprint. It proposes to use lossy transform coding before sending network output to memory. My concern with the paper is two fold:1) The major technique of transform domain coding is borrowed from previous work (e.g., Goyal 2001), hence the novelty of the proposed method is in doubt. What is the novelty?<BRK>A lossy transform coding approach was proposed to reduce the memory bandwidth of edge devices deploying CNNs. The proposed method and initial results are promising.<BRK>The submission proposes to reduce the memory bandwidth (and energy consumption) in CNNs by applying PCA transforms on feature vectors at all spatial locations followed by uniform quantization and variable length coding. It’s harder for me to precisely assess the significance of the proposed approach, but at a high level it looks reasonable and is backed by convincing empirical evidence.<BRK>In this paper, they introduce a lossy transform coding approach, inspired by image and video compression, designed to reduce the memory bandwidth due to the storage of intermediate activation calculation results.
Reject. rating score: 3. rating score: 3. rating score: 6. 1.SummaryThe authors employ a multi agent learning approach for learning how to set payoffs optimally for crowdsourcing contestsand auctions. Reject.Although the high level approach is interesting (use learning to design auctions for cases where no theoretical solution is known), the actual experimental results and methodological improvement over e.g.Dutting 2017 are weak. The authors only consider 3, 4 agent auctions. 4.Additional feedback with the aim to improve the paper. Make it clear that these points are here to help, and not necessarily part of your decision assessment. in Algo 1, 2?<BRK>This paper considers a scenario of bidding contest where the goal os to find optimal allocation w   (w_1, w_2, ..., w_n) of the total prize that maximizes the principal’s expected utility function. The paper is sound an clear, but it s not clear to me which part is novel and which part is from existing work, hence I doubt the contribution level of this paper. Furthermore, I m not quite sure whether the topic fits ICLR as it s more related to game theoretic society and not related to representation learning. Thanks for the response from the authors. I have read it carefully, especially regarding the novelty part.<BRK>After reading the rebuttal, I increased my score to weak accept, since it addressed my concern. SummaryThis paper presents a general machine learning method for contest / auction problems. The underlying idea is to collect data pairs (i.e., [design, utility]), fit a model to the data, and then optimize over all the designs to figure out the best one. The authors mainly applied their method on an auction design problem, and finished a few experiments. Weaknesses  My major concern of this paper is the lack of novelty. However, as the authors demonstrated in Figure 1, the main idea of this approach is: collect the data, fit a model, and finally optimize the objective, which is a pretty common approach.<BRK>They propose a multi-agent learning approach for designing crowdsourcing contests and all-pay auctions. Prizes in contests incentivise contestants to expend effort on their entries, with different prize allocations resulting in different incentives and bidding behaviors. In contrast to auctions designed manually by economists, their method searches the possible design space using a simulation of the multi-agent learning process, and can thus handle settings where a game-theoretic equilibrium analysis is not tractable. Their method simulates agent learning in contests and evaluates the utility of the resulting outcome for the auctioneer. Given a large contest design space, they assess through simulation many possible contest designs within the space, and fit a neural network to predict outcomes for previously untested contest designs.
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. Although the results for sequence task do not outperform the gated counterparts, the authors present an interesting exploration of initializing non normal RNNs that outperform the orthogonal counterparts. Issues to be addressed in the paper:1. The paper explores non normal RNNs and demonstrates on  3 synthetic tasks   copy, addition and pMNIST   how with careful initialization the proposed approach outperforms their orthogonal initialization counterpart. 2.The authors do a great job in motivating the paper, and the explanation is clear and easily understandable.<BRK>Contributions: This paper proposes to explore nonnormal matrix initialization in RNNs. Comments:The paper is well written and pleasant to read. In addition, did you try saturating non linearities for the RNN experiments? Overall, I think the method is promising, but comparison with prior work is missing. In particular, the introduction states that the stochasticity of SGD is a source of noise which is true.<BRK>The paper is easy to follow. The novelty of the work is limited though. Chen et.al.(2018) already pointed out the limitation of orthogonal initialization alone for nonlinear RNNs, and proposed closed form initialization for RNNs with different activation functions. It would be worthwhile to include a comparison to that method. Results in section 2.3.2 Table 1 are not exactly align with the story.<BRK>Motivated by this finding, here they investigate the potential of non-normal RNNs, i.e.RNNs with a non-normal recurrent connectivity matrix, in sequential processing tasks. Their experimental results show that non-normal RNNs outperform their orthogonal counterparts in a diverse range of benchmarks. Short of designing new RNN architectures, previous methods for dealing with this problem usually boil down to orthogonalization of the recurrent dynamics, either at initialization or during the entire training period. Previous work has shown that in the linear case, recurrent networks that maximize the SNR display strongly non-normal, sequential dynamics and orthogonal networks are highly suboptimal by this measure.
Reject. rating score: 3. rating score: 6. rating score: 8. AE and TopoAE behave in a similar way and it is not clear whether such a difference is statistically significant. The main idea proposed in this work is to use the topological signature directly as a loss for autoencoders. The extended experiments don t provide an empirical evidence of the added value of a topological AE.<BRK>Minor comments Sec.6: We presented a topological autoencoders  > We presented a topological autoencoderOverall, I think this is a nicely done paper, but with quite some question marks at many places. The theoretical part of the work deals with the issue of using mini batches for PH computation and whether this computation is close to the computation on the full point cloud. There are questions here and there (see below), but I do think they can be answered. This is realized via an additional (i.e., in addition to reconstruction) loss term (optimized over mini batches) which requires differentiating through the PH computation.<BRK>This paper shows how to train an autoencoder that preserves topologicallyrelevant distance across varying length scales. It presents a newdifferentiable loss term for topological distance between input and latentspace. Embedding quality is evaluated with a wide variety of metrics on realand synthetic datasets highlighting the preservation of global and localtopology.<BRK>They propose a novel approach for preserving topological structures of the input space in latent representations of autoencoders. Using persistent homology, a technique from topological data analysis, they calculate topological signatures of both the input and latent space to derive a topological loss term.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. This paper presents an algorithm to generate curiosity modules for reinforcement learning. The paper is very novel   the idea of developing a domain specific language full of building blocks to represent various curiosity modules is unique and interesting.<BRK>Instead of hand designing exploration bonuses and intrinsic reward, the paper proposes to view the curiosity algorithms as programs described with domain specific language (DSL), then search programs which allows RL agents to optimize the environment reward combined with the curiosity reward generated by the program. This is a very interesting idea, and it s partially inspired by the architecture search line of research.<BRK>This paper proposes to meta learn a curiosity module via neural architecture search. There are also some interesting results in the appendix which show the efficacy of their predictive approach to program performance. The method is evaluated by learning a curiosity module on the MiniGrid environment (with the true reward being linked to discovering new states in the environment) and evaluating it on Lunar Lander and Acrobot. A reward combination module (which combines intrinsic and extrinsic rewards) is further evaluated on continuous control tasks (Ant, Hopper) after having been meta trained on Lunar Lander.<BRK>They hypothesize that curiosity is a mechanism found by evolution that encourages meaningful exploration early in an agent's life in order to expose it to experiences that enable it to obtain high rewards over the course of its lifetime. They formulate the problem of generating curious behavior as one of meta-learning: an outer loop will search over a space of curiosity mechanisms that dynamically adapt the agent's reward signal, and an inner loop will perform standard reinforcement learning using the adapted reward signal.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 6. The paper proposes to directly model the (conditional) inter event intervals in a temporal point process, and demonstrates two different ways of parametrizing this distribution, one via normalizing flows and another via a log normal mixture model.<BRK>The authors propose a new paradigm for learning models for point processes which circumvents the need to explicitly model the conditional intensity. A key discussion missing in the paper is that of the complexity of training the model. Some ways of improving the paper:   Theorem 1 can be made more rigorous by making the role of parameters like  K  more explicit.<BRK>This paper describes a simple yet effective technique for learning temporal point processes using a mixture of log normal densities whose parameters are estimated with neural networks that also adds conditional information. They can both  capture sequences and should be able to model inter event times, instead of an RNN. How was this done in the experiments and how sensitive the performance is to this parameter?<BRK>Temporal point processes are the dominant paradigm for modeling sequences of events happening at irregular intervals. They show how to overcome the limitations of intensity-based approaches by directly modeling the conditional distribution of inter-event times. They additionally propose a simple mixture model that matches the flexibility of flow-based models, but also permits sampling and computing moments in closed form.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. The idea is to have a learnable threshold Delta that determines if an output activation should be computed in high or low precision, determined by the most significant bits of the value. Assuming that high activations are more important, these are computed at higher precision. From a hardware point of view, the paper focuses on GPU implementations. Fig.2 shows by example how the method works for an input $I$.<BRK>This paper outlines a new method that allows using a variety of precision in the numerical representation of the network to increase performance (both in terms of accuracy and speed). This enables substantial performance gains. As such, I think it should be accepted. However, the major question I had as I read the paper was the efficacy on GPU, which the paper discusses, but does not implement, nor show any empirical results for, which weakens the paper. The contribution is strong, however, and should be published in some form, either now, or at a future date. How were they chosen?<BRK>This paper presents an interesting quantization technique that is, unusually, end to end trainable and not just an inference technique. According to the experiments, the method achieves better performance and computational savings as compared to other quantization method baselines. I feel this work is being released prematurely and could use some more polish to help sell the method better. For example, I am not sure I understand the effects of the threshold on this method. If it uses B_avg, why not calculate the bitwidth per layer and sum things up? * When the authors address runtime changes except in table 6, they changed their baseline to a vanilla ResNet 18 with dense weights.<BRK>PG computes most features in a low precision and only a small proportion of important features in a higher precision to preserve accuracy. The proposed approach is applicable to a variety of DNN architectures and significantly reduces the computational cost of DNN execution with almost no accuracy loss. Their experiments indicate that PG achieves excellent results on CNNs, including statically compressed mobile-friendly networks such as ShuffleNet.
Reject. rating score: 1. rating score: 3. rating score: 6. The authors argue that the proposed method can support dynamic and out scope target classes, which are particularly applicable to backdoor attacks in the transfer learning setting.<BRK>This paper proposes an adaption of existing backdoor attacks, with the main goal of enabling backdoor attacks in the transfer learning setting. Could the authors provide some explanation on it? However, I am not convinced that the proposed approach is necessary a good way to do so, and have the following questions:1.<BRK>This paper proposes a general framework for constructing Trojan/Backdoor attacks on deep neural networks, specifically in cases where the end user plans to perform transfer learning on the backdoored classifier. The proposed method is general, and is shown to work across a variety of datasets.<BRK>Existing studies have explored NN trojaning attacks in some small datasets for specific domains, with limited numbers of fixed target classes. In this paper, they propose a more powerful trojaning attack method for large models, which outperforms existing studies in capability, generality, and stealthiness. First, the attack is programmable that the malicious misclassification target is not fixed and can be generated on demand even after the victim's deployment. Third, their trojan shows no biased behavior for different target classes, which makes it more difficult to defend.
Reject. rating score: 1. rating score: 3. rating score: 8. This work proposes to learn a latent space for the PixelCNN by first computing the Fisher score of the PixelCNN model and then projecting it onto a lower dimensional space using a sparse random matrix. My first concern about this work is its novelty. The paper proposes to get around this problem by projecting the Fisher score onto a lower dimensional space using random matrices.<BRK>Motivated by the observation that powerful deep autoregressive models such as PixelCNNs lack the ability to produce semantically meaningful latent embeddings and generate visually appealing interpolated images by latent representation manipulations, this paper proposes using Fisher scores projected to a reasonably low dimensional space as latent embeddings for image manipulations. 2) The comparisons to baselines are unfair.<BRK>This paper focuses on the problem of interpolating between data points using neural autoregressive models. The core idea is that it is possible to use (a smaller dimensional projection of) the Fisher score of the density function defined by the autoregressive model to represent data points in embedding space, and a neural decoder for mapping them back to input space.<BRK>In this paper, they propose using Fisher scores as a method to extract embeddings from an autoregressive model to use for interpolation and show that their method provides more meaningful sample manipulation compared to alternate embeddings such as network activations.
Reject. rating score: 3. rating score: 6. rating score: 6. Thus, the theoretical contribution of this paper is limited.<BRK>WeaknessThe main theorem of this paper is an extension of existing methods, so the novelty of theoretical analysis is somewhat limited. For this reason, I think it is okay but not good enough at this time.<BRK>First this paper proposes a new theory for this domain that extends generalized discrepancy theory to multi source setting. The evaluation of the proposed method is not complete.<BRK>The algorithm they develop, Domain AggRegation Network (DARN), is able to effectively adjust the weight of each source domain during training to ensure relevant domains are given more importance for adaptation.
Accept (Spotlight). rating score: 6. rating score: 6. rating score: 6. Summary: Authors extend on work that attempts to learn fair data representation (features) and propose an algorithm (which is a modification of a loss essentially) and show that it allows to achieve accuracy and equalized odds parity, and show that while achieving equalized odds they don t hurt demographic parity. The experiments demonstrate utility of the algorithm for balanced datasets (without sacrificing performance to fairness)Disclaimer: I am completely out of this areaBut it is an easy read and an interesting angle.<BRK>There are quite a lot of papers in the fairness literature that experiment with the Adult dataset. Zhang et al., 2018. This paper proposes an adversarial representation learning approach. Authors argue that proposed approach can simultaneously achieve accuracy parity and equalized odds. The notion of accuracy parity does not seem to be very meaningful.<BRK>This paper focuses on learning representations which can simultaneously achieve equalized odds and accuracy parity without impacting demographic parity. The authors show both theoretically and empirically that the proposed algorithm show better utility fairness tradeoff on balanced datasets. This is indeed a useful result. There should be an additional condition on distribution D, which is not clear at that moment in the paper. 6.How did the authors construct the optimal classifiers in the experiments for real data?<BRK>They propose a novel algorithm for learning fair representations that can simultaneously mitigate two notions of disparity among different demographic subgroups in the classification setting. They show how these two components contribute to ensuring accuracy parity and equalized false-positive and false-negative rates across groups without impacting demographic parity.
Reject. rating score: 1. rating score: 6. rating score: 6. This reformulation allows to train a recurrent neural network of spiking neural network with good performances. I think this contribution could not be accepted for a methodological problem: the main idea is to use neuromorphic chips that use spiking neural networks with an approximation that is basically similar to an artificial neural network. This point should be shown experimentally. Unfortunately, it is not possible to assess if the proposed neuron model is working on energy efficient architecture.<BRK>The paper promises efficient training of spiking neuron models using back propagation. The authors say that this is important because spiking networks offer significant energy savings, yet they typically perform poorly compared to prevalent artificial neural networks (ANNs). No comparisons are to baselines are presented in the text. * In sections 4.4 and 4.5, experiments are discussed, but results are not presented. * A figure showing the operation of the novel SigmaDelta neuron, compared to the standard aI&F neuron, would be welcome.<BRK>This paper deals with neuromorphic computing architectures for solving deep learning problems. Based on my understanding, the algorithmic approach seems logical and the empirical results are convincing. So, I am giving the benefit of the doubt to the authors, given that this is a critical topic for the ML community.<BRK>However, most of these architectures rely on spiking neural networks, which typically perform poorly compared to their non-spiking counterparts in terms of accuracy. In this paper, they propose a new adaptive spiking neuron model that can also be abstracted as a low-pass filter. Application of these results will lead to the development of powerful spiking models for neuromorphic hardware that solve relevant edge-computing and Internet-of-Things applications with high accuracy and ultra-low power consumption.
Reject. rating score: 3. rating score: 3. rating score: 6. rating score: 8. Although the performance does not convincingly exceed its competitors, the contribution seems to be getting the spatio temporal adversarial loss to work at all. However, I am concerned about generalizability of the method. My current decision is weak reject, since the generations look quite good but I have concerns about generalization of the approach to other video generation tasks as well as the justification of the ping pong loss. Questions  Is RecycleGAN unable to be applied to video super resolution? Why is it not compared to in Table 2? The loss is motivated by the issue of "artifacts", which I assume are poor generations due to lacking a good model of the world. The paper says this issue could be alleviated by training with longer video sequences, but that would prevent the generator from working with sequences of arbitrary length. I do not believe the ping pong loss allows the generator to work with arbitrary sequences, as training only on short sequences and their reverse should not allow the model to generalize to longer sequences.<BRK>Summary:This paper proposes a training objective for higher quality video generation for the tasks of Video Super Resolution (VSR) and Unpaired Video Translation (UVT) and also two evaluation metrics tOF and tLP. They provide a comprehensive ablative study of the proposed method and also show comparisons against baselines for the tasks of VSR and UVT. Pros:+ Novel video generation method for VSR and UVT. It would be good if the authors can clarify this in the rebuttal. Conclusion:In conclusion, the paper seems to present a novel method and evaluation metrics but has many issues as stated above. Did the authors see any behavior like this? Or was there a very small weight applied to this loss? TecoGAN vs baselines generator parameters (rather than TecoGAN^{ }). TecoGAN has more parameters in the generator compared to TecoGAN^{ }. Did the authors make sure that the generator had the same number of parameters as the other methods? These are mentioned as contributions of the paper so their description should be in the main text. The UVT task is mentioned in the abstract and as a target task in this work, however, the evaluations for this are in the Appendix. UVT task only evaluated with the proposed evaluations? I understand that, since it’s an unpaired video translation task, there is no ground truth to compare against.<BRK>The paper presents video generation method with spacio temporally consistent features. This is done through: a) temporal adversarial learning, b) Ping Pong loss, and c) metrics that quantify the quality. The methods are evaluated on two datasets and user studies. The idea is interesting and the paper is well written. The results are convincing. The originality of the concatenation of several frames is somewhat limited, since it is a standard procedure in other domains such as robotics. Seems like the metrics definitions were not included in the main body of the paper   the authors should either include them to remove from the contributions.<BRK>The paper presents a novel method for training video to video translation (vid2vid) models. The authors introduce a spatio temporal adversarial discriminator for GAN training, that shows significant benefits over prior methods, in particular, parallel (as opposed to joint) spatial and temporal discriminators. In addition the authors introduce a self supervised objective based on cycle dependency that is crucial for producing temporally consistent videos. I really like this paper. Although the method is a rather complex mix of multiple losses, these are justified in detail, both intuitively and empirically. The appendix is filled with much more detail about implementation, architecture and more results. Finally the results show that the proposed method is superior across the board compared to previous approaches from the literature.<BRK>This is crucial for sequential generation tasks, e.g.video super-resolution and unpaired video translation. For the former, state-of-the-art methods often favor simpler norm losses such as L2 over adversarial training. However, their averaging nature easily leads to temporally smooth results with an undesirable lack of spatial detail. For both tasks, they show that temporal adversarial learning is key to achieving temporally coherent solutions without sacrificing spatial detail. They also propose a novel Ping-Pong loss to improve the long-term temporal consistency.
Reject. rating score: 3. rating score: 6. rating score: 6. The paper is well structured with background literatures, formula, as well as experiments to show the advantage of the proposed method. I find it generally interesting, with the following major concerns.<BRK>### WeaknessesLack of systematic description of the authors’ major contribution. How long is it for training the proposed model and getting the experiment results? ### Suggestions to improve the paper1. The proposed model looks more like a combination of previous conclusions, which makes readers feel the core parts of this paper build heavily on previous work.<BRK>The authors discuss the a novel technique, called Random Distance Prediction, to learn rich features from domains where massive data are hard to produce; in particular, they focus on the two tasks of anomaly detection and clustering. The paper is well written and understandable by a non specialistic audience; introduction and references are adequate, and the theoretical analysis is reasonably explained, although the two optional losses should have been discussed more deeply.<BRK>To enable downstream unsupervised learning on those domains, in this work they propose to learn features without using any labelled data by training neural networks to predict data distances in a randomly projected space. To well predict these random distances, the representation learner is optimised to learn class structures that are implicitly embedded in the randomly projected space.
Reject. rating score: 3. rating score: 3. rating score: 6. would be important to show the effectiveness of ML3. Right now, the method is only compared to ML3 with task loss, which seems not very conclusive. While the idea is natural, there is a prior work [1] that has investigated the effectiveness of learned loss in gradient based meta learning, which seems pretty similar to this paper. Besides, I wonder how important the extra information added during the meta training time is and the authors should present comparison to ML3 without the extra information.<BRK>Overall, the presentation in this paper is hard for me to understand technical details and see the difference with existing methods. I am glad to discuss problems with the authors  reply during the rebuttal.<BRK>Potential improvements:(1) paper layout, to be honest, I m not sure if Figure 1 is really needed(2) related work section seems very long, would be good if it can be shorten and use the extra space for results display<BRK>They develop a pipeline for training such loss functions, targeted at maximizing the performance of model learn- ing with them. They observe that the loss landscape produced by their learned losses significantly improves upon the original task-specific losses in both supervised and reinforcement learning tasks. Furthermore, they show that their meta-learning framework is flexible enough to incorporate additional information at meta-train time. This information shapes the learned loss function such that the environment does not need to provide this information during meta-test time.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. So when the mini batch is small, the performance can drop significantly. The paper addresses this issue by analyzing extra statistics in the batch normalization and introducing moving average statistics, weights centralization and a slightly modified normalization. The theoretical analysis and guarantees are provided as well. Experiments on typical datasets demonstrate the effectiveness of the proposed trick. Overall, the idea is interesting to me.<BRK>Authors propose to replace EMA in backward pass by simple moving average (SMA) and show that under some assumptions such replacement reduces variance. Authors do not reflect this problem in the work. Authors do not study the influence of this on the performance of the method. The paper proposes the improvement of batch normalization techniques for the case of small batch size. Overall, almost all of my concerns were justified.<BRK>This paper provides a new method to deal with the small batch size problem of BN, called MABN. This would affect the performance of the proposed method. Although there are several issues not addressed by the authors (the assumption of this paper  "W mean(W) and X_{input} are irrelevant"; why not using ImageNet pre trained model on COCO), I keep my initial rating of weak accept.<BRK>They prove the benefits of MABN by both theoretical analysis and experiments. Based on their analysis, they propose a novel normalization method, named Moving Average Batch Normalization (MABN). MABN can completely restore the performance of vanilla BN in small batch cases, without introducing any additional nonlinear operations in inference procedure.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. rating score: 1. The paper focuses on training neural networks using 8 bit floating point numbers (FP8). The goal is highly motivated: training neural networks faster, with smaller memory footprint and energy consumption. * Equation (3) uses "i prime" in the argument for "max", but "i prime" is not used.<BRK>There has been a great deal of interest and research into reduced numerical precision of weights, activations and gradients of neural networks. In this work the authors propose an 8 bit floating point format (denoted S2FP8) for tensors. The key idea here is that for each tensor of 8 bit numbers, two 32 bit floating point statistics are recorded as well. For a large tensor, this additional reduction to compute statistics may be expensive (in memory bandwidth and computation), particularly since this is done with FP32. [1] https://en.wikipedia.org/wiki/Bfloat16_floating point_format<BRK>Could be very useful for many embedded applications. I’m not a hardware expert but can see why this would be Useful.<BRK>The paper suggest the method to train neural networks using 8 bit floating point precision values. While the method is interesting, I do not think it is practical due to the required hardware modifications. I am by no means not a hardware design expert, but I am not convinced that the gain of using 8 vs 16 bit floating point numbers outweights any extra complexity of hardware implementation.<BRK>This necessitates increased memory footprint andcomputational requirements for training. Here they introduce a novel methodologyfor training deep neural networks using 8-bit floating point (FP8) numbers. They name this method Shifted and Squeezed FP8 (S2FP8). They introduce twolearnable statistics of the DNN tensors - shifted and squeezed factors that are usedto optimally adjust the range of the tensors in 8-bits, thus minimizing the loss ininformation due to quantization.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. The paper studies the role of depth on incremental learning in several toy models for neural networks. In particular, they show that in these models, deep models require polynomially small initializations to exhibit incremental learning than shallow models. The authors contribute analysis for non asymptotically small initializations, and study an interesting role of depth in how small this initialization must be. Some very important ones. This makes it seem that results apply to realistic settings, which is really far from true.<BRK>This paper studies the phenomenon of incremental learning in several deep models. It starts with analyzing the optimization dynamics of a toy model, and showing that it follows incremental learning, a notion defined clearly in the paper. The nice contributions include a clear definition of incremental learning, results showing the depth’s effect on incremental learning as well as extensions to several other models. My main question is regarding the relevance of the toy model to more realistic models, as I will discuss below, and I’d love to hear more about the authors’ thoughts on this.<BRK>More precisely, this paper define a notion of incremental learning for a particular learning dynamics and study how the depth of the network influence it. But, I really like the main takeaway which is that there is a huge discrepancy between 2 and 3 layers in terms of dynamics. *Decision*Weak accept: The key results in this work is Theorem 2 and it’s extension to discrete case Theorem 3. they seems really interesting: when $N > 2$ in order to observe sequential learning, the dependence in the eigengap for the initialization goes from polynomial ($N 2$) to polynomial. However, these results are very hard to read. Actually for a fixed $c$, if we do $\sigma_j^* \to \sigma_1^*   \sigma_i^*$ then we got $A   1/B $ thus one of them is smaller than 1. Isn’t it an issue in practice ?<BRK>A leading hypothesis for the surprising generalization of neural networks is that the dynamics of gradient descent bias the model towards simple solutions, by searching through the solution space in an incremental order of complexity. They formally define the notion of incremental learning dynamics and derive the conditions on depth and initialization for which this phenomenon arises in deep linear models. Their main theoretical contribution is a dynamical depth separation result, proving that while shallow models can exhibit incremental learning dynamics, they require the initialization to be exponentially small for these dynamics to present themselves.
Reject. rating score: 1. rating score: 1. rating score: 3. 1.Contributions: A) Extension of robustness bound based on margin and Lipschitz constant of the network to arbitrary l_p norms. As such It has questionable novelty and limited potential impact. I vote for rejecting this submission. As such it is not surprising that. The proposal of OVA networks and the bound on the adversarial risk are two parallel ideas that could be explored and hopefully find a significant contribution. Clarity: The paper could see some improvements in notation and some erroneous claims:1. constrained maximization of convex functions is a hard problem. Not to beconfused with certain instances of constrained minimization of convex functions that can be solved efficiently.<BRK>The paper is missing references to several key pieces of related work tackling similar problems. I identified (fixable) issues with the theoretical results in the paper and felt that overall the paper was rushed and difficult to read in places as a result. 3) Ignoring issues discussed below, this paper presents an interesting theoretical result on providing stochastic robustness guarantees for Lipschitz constrained neural networks. Further, Proposition 2 presents the wrong terms in the sum (so that E[L] !adversarial risk). I believe the stated result can be made rigorous and correct, however in its current form there are mistakes. 6) In section 4.1 you introduce the methods use to constrain the Lipschitz constant of the network. If so, this would give a loose upper bound (see [3]). I would consider raising my score if the issues present in the theoretical results are addressed by the authors.<BRK>Summary:The author show that lipschitz constants of the neural network can be used to bound adversarial robustness. The method of Gouk et al.is a very loose overapproximation of the Lipschitz constant of the neural network (multiplication of lipschitz constant of each layer). Note: I did not assess the correctness of Proposition 2. This would improve the paper if there was some signs that the justification proposed by the authors have some experimental validation. Experiments in 5.1 are made on linear SVMs, which means that the lipschitz constant can be computed exactly but this doesn t really reflect how useful the bound will be on network with hidden layers, for which the lipschitz constant will have to be an approximation. Opinion:At the moment, the paper is quite confusing and hard to read, and it s not entirely clear what the crux of the paper, the new architecture is doing.<BRK>They introduce the first bound that makes use of Lipschitz continuity to provide a more general guarantee for threat models based on any p-norm. Additionally, a new strategy is proposed for designing network architectures that exhibit superior provable adversarial robustness over conventional convolutional neural networks. Experiments are conducted to validate their theoretical contributions, show that the assumptions made during the design of their novel architecture hold in practice, and quantify the empirical robustness of several Lipschitz-based adversarial defence methods.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. The authors propose a promising automated curricula generation scheme, which considers goal validity, goal feasibility, and goal coverage to construct useful curricula for the underlying agents. Empirical studies demonstrate the capability of the proposed model in generating task curricula across several complex goals. However, I am not familiar with the curricula generation in the reinforcement learning setting. A minor concern is about the experiments.<BRK>This paper proposes an autocurricula scheme to train a goal conditional agent in a dynamic and sparse rewarding environment. Update after rebuttal:Thanks for the detailed reply from the authors! They answer most of my questions. Experimental results show that different combinations of the three types of losses can bring improvements in some scenarios. Given a desired distribution of goals, the learning becomes more efficient. The paper compares this method with Goal GAN as a baseline and outperforms it on the three tasks.<BRK>I wanted to point out that the authors could consider adding other (simpler) baselines such as Sukhbaatar et al.(2017) to make their empirical results more complete for the navigation tasks (even though these methods make certain assumptions, it would be interesting to see how much of an effect they have compared to the proposed method). The authors propose a combination of different losses to help the setter balance its goal predictions — validity, feasibility and coverage. Empirical results on two setups demonstrate the effectiveness of this approach in learning a good curriculum.<BRK>But in dynamic or sparsely rewarding environments these correlations are often too small, or rewarding events are too infrequent to make learning feasible. While curricula are also useful for artificial agents, hand-crafting them is time consuming. Here they explore automatic curriculum generation in rich, dynamic environments. They demonstrate the success of their approach in rich but sparsely rewarding 2D and 3D environments, where an agent is tasked to achieve a single goal selected from a set of possible goals that varies between episodes, and identify challenges for future work.
Reject. rating score: 3. rating score: 3. rating score: 8. The agent is trained with Q learning and GNN as the function approximator: the input is presented as a bipartite graph with variables and clauses corresponding to a SAT instance in CNF form and the GNN predicts Q values for assigning each unassigned variable to True or False. Then authors compare their method to the popular VSIDS branching heuristic (which counts the number of conflicts a literal or variable has been involved in) and report the median relative iteration reduction over all problems (~2 3x for satisfiable instances, a bit less for unsatisfiable instances). GQSAT generalizes well to larger satisfiable instances than those seen during training and somewhat to unsatisfiable instances (unSAT). Recommendation  This paper is well motivated (improving speed of complete SAT solvers) and presents a good overview of related work. The paper is well written overall, although I found section 3 hard to read and I think the writing can be a bit improved by writing explicitly the algorithm or at least the MDP (see misc comments)While the reduction in number of branching decisions is satisfactory, this does not necessarily translates to actual speed ups (since the VSIDS branching heuristic is faster in practice) and there are some concerns about applying GQSAT to larger instances. Misc    Can the authors hypothesize on why GQSAT does not benefit from seeing more instances of SAT during training? What is the motivation for using Median Relative Iteration Reduction as a metric? The state of such an MDP consists of unassigned variables and unsatisfied clauses.". NeuroSAT cannot generalize from SAT to unSAT : the NeuroSAT paper showed that they could learn to predict satisfiability as well.<BRK>The paper proposes learning a branching heuristic to be used inside the SAT solver MiniSat using reinforcement learning. At each step of an episode the policy selects a variable to branch on and assigns a value to it. The episode terminates once the solver finds a satisfying assignment or proves unsatisfiability. The policy is trained using DQN. Results on randomly generated SAT instances show that the learned policy is able to solve problems with fewer steps than VSIDS, the branching heuristic commonly used by state of the art solvers. The general idea of using RL to learn distribution specific branching heuristics is a very interesting research problem, and SAT is a difficult test case for it. Cons:  Showing improvements in the number of steps compared to VSIDS is not interesting because VSIDS as implemented in MiniSat and state of the art solvers like Glucose has been tuned to minimize running time rather than number of steps. A better discussion of how to scale up the proposed approach to instances with millions of variables is needed. Although most ML papers on SAT deal with at most hundreds of variables, such small instances are trivial for the state of the art solvers. (While the paper references that work, it doesn’t compare to it.) There is no attempt to address the scalability issue in this work. Without a better understanding of these challenges, it is not clear that learning can help much to improve the state of the art SAT solvers.<BRK>This paper investigates the problem of learning new branching heuristics in SAT solvers. The idea is very simple: take MiniSat, remove the usual VSIDS heuristic, and replace it with a variable selection policy that has been trained from a deep reinforcement learning algorithm. The resulting GQSAT heuristic is endowed with attractive properties: on random SAT instances, it outperforms VSIDS and generalizes relatively well to other SAT distributions. Overall, this is a very interesting paper. It is well written, well motivated, and well positioned with respect to related work. To sum up, I have no major reasons for not accepting this paper. I am wondering whether such “solver features” in state representations could improve the GQSAT heuristic, and could help in generalizing from a class of SAT problems to another one. Since the number of actions per episode is capped, a terminal state can be a leaf of the MiniSat search tree (where a satisfying assignment was found, or a dead end was reached), or an internal node of the tree (when the maximum number of actions per episode was reached).<BRK>Solvers using GQSAT are complete SAT solvers that either provide a satisfying assignment or a proof of unsatisfiability, which is required for many SAT applications. The branching heuristic commonly used in SAT solvers today suffers from bad decisions during their warm-up period, whereas GQSAT has been trained to examine the structure of the particular problem instance to make better decisions at the beginning of the search. Training GQSAT is data efficient and does not require elaborate dataset preparation or feature engineering to train. They train GQSAT on small SAT problems using RL interfacing with an existing SAT solver. They show that GQSAT is able to reduce the number of iterations required to solve SAT problems by 2-3X, and it generalizes to unsatisfiable SAT instances, as well as to problems with 5X more variables than it was trained on.
Reject. rating score: 3. rating score: 3. rating score: 6. The paper proposes an adversarial transfer learning network that can handle the adaptation of both the input space and the output space. Therefore, the reviewer is concerned about the modeling contribution made in this paper is somewhat incremental given existing literature.<BRK>This paper proposes a novel Transfer Learning Method, namely Adversarial Inductive Transfer Learning (AITL), which adapts not only the input space but also the output space. The idea is interesting. However, the proposed method is not convincing from either theorical analysis or experimental results. It is not convincing that the method can be used in another application.<BRK>the paper studies transfer learning, which addresses the inconsistencies of the source and target domains in both input and output spaces. the paper proposes adversarial inductive transfer learning which uses adversarial domain adaptation for the input space and multi task learning for the output space. the main contribution of the paper is in identifying a new type of problem that may be worth studying.<BRK>To the best of their knowledge, AITL is the first adversarial inductive transfer learning method to address both input and output discrepancies.
Reject. rating score: 3. rating score: 3. rating score: 3. This paper introduces the compression risk in domain invariant representations. * The organization of this manuscript is poor and difficult to follow. The paper presents an in depth analysis of compression and invariance, which provides some insight. However, I have several concerns:* In Section 4, the authors propose a regularization to ensure h belongs to H_0.<BRK>So I suggest that the analysis could be revisited by directly incorporating (representation) norms in the theoretical framework and in particular for defining more properly H_0. Summary This paper presents a revisit of existing theoretical frameworks in unsupervised domain adaptation in the context of learning invariant representation. The weighting aspect of the contribution is not supported by any experiment. This is maybe subjective, but in the context of learning representation, I would have interpreted it as a way to sparsify the representation, and thus compression could then be measured with respect to a given norm (L2?)<BRK>This submission provides a new theoretical framework for domain adaptation. I think this submission discusses about an important problem and provides new insight, but it is not a thorough theoretical work because of above reasons.<BRK>They support these statements with a theoretical analysis illustrated on a standard domain adaptation benchmark. In this paper, they show that the search for invariance favors the compression of representations. This supports the claim that representation invariance is too strict a constraint. First, they introduce a new bound on the target risk that reveals a trade-off between compression and invariance of learned representations.
Reject. rating score: 3. rating score: 3. rating score: 6. In this paper, the authors developed GNN by integrating an interpolation based regularization. Moreover, the predicted samples of GCN are used to augment the data of MLP for training in a self supervised manner.<BRK>The authors claim that the proposed method for semi supervised graph learning is based on data augmentation which is efficient and can be applied to different graph neural architectures, that is an architecture agnostic regularization technique is considered in the paper.<BRK>Summary:This paper presents a data augmentation procedure for semi supervised learning on graph structured data. The relative simplicity and generality of GraphMix is appealing. Instead of trying to incorporate an augmented dataset into the graph, this paper uses a seperate fully connected network (FCN) that shares weights with a graph neural net (GNN).<BRK>They present GraphMix, a regularization technique for Graph Neural Network based semi-supervised object classification, leveraging the recent advances in the regularization of classical deep neural networks. Specifically, they propose a unified approach in which they train a fully-connected network jointly with the graph neural network via parameter sharing, interpolation-based regularization and self-predicted-targets.
Reject. rating score: 3. rating score: 6. rating score: 6. More precisely, they propose a graph neural network that apply several layers of graph convolution in parallel to the node attributes and to the embedding, then takes an average of the result, which is fed to another series of graph convolution. This is combined with some form of sampling which strongly resembles median based quantization but is solved with some basic heuristics and without acknowledging the resemblance (I might be missing something). The authors are mixing references to graph convolution for fixed graph structure with references about general graph neural networks.<BRK>Specifically, if two nodes, that may be far apart in the graph, may be represented as (almost) the same vector. This is simply because when no features/labels are associated with nodes, and the local structure around those two nodes is very similar then the local aggregation of information will result in a similar representation. [Edit: The authors have replied to my comments, and the other reviewers  comments in great detail. Overall, the empirical results are supportive of the fact that the proposed method can help improve the performance of GNNs.<BRK>Original review  In this paper, the authors identify a shortcoming of existing GNN architectures for graph classification tasks   specifically, the fact that, in the featureless regime, the graph convolutional layers rely on propagating very rudimentary structural information, making it hard (or impossible) to distinguish graphs with similar local structure. I believe that the paper clearly exposes and proposes a nice idea which could hold great potential, and which can be useful to graph representation learning practitioners. Comments:* The observation that existing GNN layers may struggle with distinguishing featureless graphs is not particularly novel. It s hard to say that many of the improvements depicted here are statistically significant otherwise. * While I find the proposed pooling method interesting (and more grounded in the graph s structural features than other proposed works), I find that there are many potential limitations to be discussed.<BRK>The GNN is not aware of the locations of the nodes in the global structure of the graph and when the local structures corresponding to different nodes are similar to each other, the convolution layer maps all those nodes to similar or same feature vectors in the continuous feature space. The proposed approach leverages a spatial representation of the graph which makes the neural network aware of the differences between the nodes and also their locations in the graph. A new graph pooling method is proposed and it is shown that the proposed pooling method achieves competitive or better results in comparison with the state-of-the-art methods. The spatial representation which is equivalent to a point-cloud representation of the graph is obtained by a graph embedding method.
Reject. rating score: 1. rating score: 3. rating score: 3. This paper proposed a generative network based on fluid flow solutions of mass transport problems. 4.Very limited experimental validation with no comparison to other algorithms. b) Section 2 is a combination of related work and proposed work. For example, it starts more like a section introducing the dynamic transport formulation of mass transport problems (Equations 2.3a 2.3c), but in fact it contains the authors  proposed approach (2.4a 2.4b), which makes it difficult to tell the authors  contributions.<BRK>As a tougher case, a digit generator with MNIST data is shown, which however, does not reach a level of quality we could expect from other existing methods for generative models. This paper targets training generative adversarial networks with a formulation that is motivated by mass transport of fluid flows. The smooth matching of point clouds does not retain too much of the initial fluid flow model. This is a central challenge for GANs, and based on the submission I don t have the impression that the proposed formulation is competitive with existing GAN methods.<BRK>The paper proposes an alternative view of obtaining generative models by viewing the generation process as a transport problem (specifically, fluid flow mass transport) between two point clouds living in high dimensional space. The experimental evaluation demonstrates on only two simple examples the results of the work. However, with the introduction of a transport based formulation, its respective issues may arise, that are not described in the paper. Will the optimization always converge, and if yes, to which kind of optimum?<BRK>Generative Adversarial Networks have been shown to be powerful tools for generating content resulting in them being intensively studied in recent years. Motivated by techniques in the registration of point clouds and the fluid flow formulation of mass transport, they investigate a new formulation that is based on strict minimization, without the need for the maximization.
Accept (Talk). rating score: 8. rating score: 8. rating score: 6. The paper uses energy based model interpretation for the logits of standard discriminative neural network models to define a generative model inside a classifier that proves useful in many downstream tasks such as uncertainty quantification, out of distribution detection, etc. Overall, the paper provides a substantial contribution and paves the way for further work improving this joint discriminative   generative setting. 1.It would benefit the paper showing that samples with higher unnormalized likelihood are visually more compelling than those with lower likelihood.<BRK>This work is an attempt to bridge the gap between discriminative models, which currently obtain the state of the art on most classification problems, and generative models, which (through a model of the marginal p(x)) have the potential to shine on many tasks beyond generalization to a hold out set with minimal shift in distributions: out of distribution detection, better generalization out of distribution, unsupervised learning etc. While much of the current work is related to normalizing flows / invertible neural networks, the authors here propose a quite simple but appealing method: A standard neural classifier is taken and the softmax is layer chopped off and replaced by an energy based model, which models the joint probability p(x,y) instead of the posterior p(y|x). A couple of details on the training procedure are missing in the experimental part. The paper is well written and easy to understand.<BRK>This paper introduces the idea of energy based model to the traditional classifier, and proposes a new framework to improve the performances of the model in multiple aspects. The idea of reinterpreting the traditional classifier is very interesting, and the experiments show some good results of the proposed method. I think author should emphasize this instead of energy based model.<BRK>They propose to reinterpret a standard discriminative classifier of p (y|x) as an energy based model for the joint distribution p (x, y). They demonstrate that energy based training of the joint distribution improves calibration, robustness, and out-of-distribution detection while also enabling their models to generate samples rivaling the quality of recent GAN approaches. Their approach is the first to achieve performance rivaling the state-of-the-art in both generative and discriminative learning within one hybrid model.
Reject. rating score: 3. rating score: 3. rating score: 6. rating score: 6. This paper proposes to quantize the weights of neural networks that can minimize the L_2 loss between the quantized values and the full precision ones. Many of the solutions in the paper have already been discovered in literature. Moreover, some of the recent quantization methods are not compared. What kind of assumption are used in this approximation, and can the optimality still be guaranteed?<BRK>this paper looks at different quantization schemes (replace values of vector in \R to, basically, plus or minus v, for well chosen v). The main criticisms is therefore the lack of concrete evidences that those schemes are actually helpful and, on the other hand, the relative simplicity (so the theoretical part of the paper are not sufficient by itself)<BRK>This paper proposes a new family of quantized neural networks, where the weights are quantized based on fixed precisions. The reviewer appreciates that this quantized DNN paper has a theory. This theory is based on minimizing the L_2 loss between the original data and quantized data. However, the approximation is loose without any guarantee. Ideally, in section 2, the authors can have some theoretical statements to state the optimality of the rank k quantization.<BRK>I am not an expert on neural network compression so I am not quite sure how the proposed method compares with the state of the art algorithms. On the other hand, I checked several proofs provided by the authors for the 1 bit and 2 bit quantization cases. The proofs look good to me. For the definition (9) can the authors make it clear that it is for all v_j s.t. 2.Can authors provide some explanation why in (8) we want to have v1> v2> vk in the constraint?<BRK>Quantizing weights and activations of deep neural networks results in significant improvement in inference efficiency at the cost of lower accuracy. A source of the accuracy gap between full precision and quantized models is the quantization error. Their quantization algorithms can be implemented efficiently on the hardware using bitwise operations. They present proofs to show that their proposed methods are optimal, and also provide empirical error analysis.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. The paper proposes a model for stochasticity for conditional image generation, building upon the previously available (DCFNet) results on composition of  convolutional filters out of the elements of the filter basis. The idea of introducing stochasticity by convolutional filters into the conditional generative models seems to be novel and the reviewer thinks it could be of interest for the community. It might be that stating Theorem 1 and then defining the method for generation of the basis (how exactly could we get to the basis? )<BRK>In particular, in order to allow for further diversity in conditional signal generation, the BasiGAN proposes to model the convolutional layers as a combination of basis which is stochastically sampled. The idea of the paper is interesting and some interesting experiments are presented. If I get a convincing answer from the authors, I would definitely accept the paper (which otherwise is well written and quite interesting to read).<BRK>In this paper, the authors introduce BasisGAN, a novel method for introducing stochasticity in conditional GANs, i.e., a way of conducting one to many mappings. This is the main contribution and difference of this paper in comparison to DCFNet (Qiu et al., ICML 2018), where the bases are not learned.<BRK>While generative adversarial networks (GANs) have revolutionized machine learning, a number of open questions remain to fully understand them and exploit their power. To illustrate this proposed plug-and-play framework, they construct variants of BasisGAN based on state-of-the-art conditional image generation networks, and train the networks by simply plugging in a basis generator, without additional auxiliary components, hyperparameters, or training objectives. The experimental success is complemented with theoretical results indicating how the perturbations introduced by the proposed sampling of basis elements can propagate to the appearance of generated images.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 6. The paper provides a doubly robust method for reducing bias, since it requires separately estimating a value function. Hence, I would recommend for acceptance of this paper.<BRK>  *Synopsis*:  This paper provides a new doubly robust estimator for off policy policy evaluation, based on the new infinite horizon technique (i.e.using an estimate of the state density ratio as opposed to long products of action importance weights). C2: The bias of the estimator decreases as our estimate of the density ratio and value function improves.<BRK>This paper proposes a new algorithm for the off policy evaluation problem in reinforcement learning. It combines the value function learning method and the stationary distribution ratio estimators. I think this paper has some nice contribution to the area, by introducing a doubly robust estimator based on the density ratio, and also a new idea to achieve double robustness.<BRK>In this paper, they develop a bias-reduced augmentation of their method, which can take advantage of a learned value function to obtain higher accuracy. Their method is doubly robust in that the bias vanishes when either the density ratio or value function estimation is perfect.
Reject. rating score: 3. rating score: 3. rating score: 6. This paper proposes to combine beta VAE and t SNE for anomaly detection. However, the authors mention that beta VAE is trained on normal data, which means that it is not unsupervised. The originality and the technical quality of the proposed method is not high as it is a straightforward combination of two existing method. However, there is no theoretical analysis of the proposed method, hence the significance of the contribution is hot high in its current state. Experimental results are not convincing. This means that representation learning is not meaningful and the raw representation (feature vectors) may be already effective for anomaly detection.<BRK>A paper is purely experimental, based on existing techniques to dimension reduction (beta VAE and t SNE). Given trained beta VAE, latent vectors, obtained for training set, are feed into t SNE algorithm. The overall anomality score for a test point is combined from 1 NN distances on t SNE plot and reconstruction error of beta VNE. This would imply that t SNE step is not needed at all. So, I am curious what actual optimal alpha was in experiments? Or, how would results change if t SNE mapping would be set to Identity tranformation (into space whose dimension is the same as latent space), but formula for anomality remains the same?<BRK>This paper presents a novel deep anomaly detection model. It combines two existing models: B VAE and t SNE. The B VAE is trained unsupervised and learns an encoder and decoder which provide both an embedding and a reconstruction. Another experiment shows that t_SNE dramatically improves the performance over B VAE alone. PROS:* The proposed approach improves over the SOT of competitive recent methods for anomaly detection on four image datasets. * The authors make an effort to abstract the approach into a framework where other deep learning models and dimensionality reduction techniques can be used. * The paper is relatively clearly written (at least sufficiently to easily understand the technical details). CONS:* The novelty of the paper is limited as it is mostly a combination of 2 existing methods. * The timeseries dataset is not compared to SOT methods (although the authors claim SOT in the conclusion).<BRK>Identifying anomalous samples from highly complex and unstructured data is a crucial but challenging task in a variety of intelligent systems. In this paper, they present a novel deep anomaly detection framework named AnoDM (standing for Anomaly detection based on unsupervised Disentangled representation learning and Manifold learning). AnoDM was evaluated on both image and time-series data and achieved better results than models that use just one of the two measures and other deep learning methods.
Reject. rating score: 1. rating score: 3. rating score: 3. Summary of the paper s contributions:This paper introduces two new notions of robustness for randomized classifiers, which are based on the notions of differential privacy (DP) of randomized mechanisms. For experiments, it would help to compare D_\infty and D_{MR} robustness alongside the standard l_2 robustness. Detailed comments:  All the claims made in the paper regarding the optimality of different noise models  are specific to D_\infty and D_{MR} robustness. I don t think it is correct to claim that Gaussian noise is "near" optimal from this analysis.<BRK>I would encourage the authors’ to clarify (and tone down) their statement about the “optimality” of Gaussian noise for L2 robustness. In its current state, I would vote to weakly reject this paper for one key reason. Theorem 12 provides a lower bound on the L_inf norm of the noise added, and they show that Gaussian noise is close to “optimal” in terms of expected L_inf norm. Does it also have the expected lowest L2 norm?<BRK>al.Therefore, I have no issues of using $D_{MR}$ to analyzing the robustness for Gaussian smoothing since it was always analyzed in the DP community with the $\epsilon,\delta$ DP and not the stronger $\epsilon$ DP. Let me clarify. How is it possible that one can guarantee $D_{MR}$ robustness without any dimensionality dependence. I find the paper very interesting and the approach is novel and generic. While $\sqrt{\log{d}}$ may seem small; improving this to a constant in dimension is still a very big gap from $\sqrt{\log{d}}$.<BRK>They further show that the largest $\ell_\infty $radius certified by randomized smoothing is upper bounded by $O (1/\sqrt {d}) $, where $d $is the dimensionality of the data. This theoretical finding suggests that certifying $\ell_\infty $-normed robustness by randomized smoothing may not be scalable to high-dimensional data.
Reject. rating score: 1. rating score: 1. 1.Summary of the paperThis paper introduces *simplicial complex networks*, a new class ofneural networks based on the idea of the subdivision of a simplicialcomplex. 2.Summary of the reviewThis paper brings an interesting perspective to the table, viz. the useof concepts from algebraic topology to develop new neural networkarchitectures with improved approximation capabilities. What about the computational complexity? I found the discussion of storage to be somewhat surprising; is this  a property that the paper wants to stress for SCNs?<BRK>This paper presents a new neural network architecture called simplicial complex networks. (2) The authors do not properly compare their approach to the most common architectures in use: Fully connected nets and CNNs. (3) Usefulness of the approach. Although I believe the authors present an intriguing idea, I ultimately tend for a rejection of this paper.<BRK>However, this property is not the only characteristic that makes neural networks unique as there is a wide range of other approaches with similar property. Despite their abundant use in practice, neural networks are still not well understood and a broad range of ongoing research is to study the interpretability of neural networks. On the other hand, topological data analysis (TDA) relies on strong theoretical framework of (algebraic) topology along with other mathematical tools for analyzing possibly complex datasets. In this work, they leverage a universal approximation theorem originating from algebraic topology to build a connection between TDA and common neural network training framework.
Accept (Poster). rating score: 8. rating score: 8. rating score: 6. This paper presents a new neural network architecture based on transformers called poly encoders. Bi encoders > Poly encoders > Cross encoders in terms of speed and Cross encoders > Poly encoders > Bi encoders in terms of accuracy. The experiments are also comprehensive. Therefore I recommend acceptance.<BRK>While the paper presents strong results on several dialog utterance retrieval tasks, the methods presented have limited novelty and impact. Summary: This work proposes a new transformer architecture for tasks that involve a query sequence and multiple candidate sequences. Paper is well written and easy to follow. So this work can be considered as an application of existing ideas to dialog tasks.<BRK>The paper describes two main existing approaches for this task, namely bi encoders and cross encoders and then proposes a new formulation called poly encoders which aims to sit between the existing approaches offering high accuracy   similarly to cross encoders   and high efficiency   similarly to bi encoders. The paper is well written and although this is not related to my research I enjoying reading it. The approach proposed seems reasonable to me, and of sufficient novelty while the results presented are impressive.<BRK>The former often performs better, but is too slow for practical use. In this work, they develop a new transformer architecture, the Poly-encoder, that learns global rather than token level self-attention features. They show their models achieve state-of-the-art results on four tasks; that Poly-encoders are faster than Cross-encoders and more accurate than Bi-encoders; and that the best results are obtained by pre-training on large datasets similar to the downstream tasks.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. In this paper, the authors propose a new method of self supervised feature learning from videos based on learning future frame prediction. Experiments on several datasets are conducted to show the effectiveness of the proposed method. The key is how to learn good features that can generalize very well. The authors show that the learned features in this paper can be used on other tasks, such as object detection. And state of the art results on KITTI dataset could be achieved based on the learned features with fixed backbone parameters. Is the generalization ability of the proposed method better that existing methods such as CycleGAN, PredNet and ContextVP?<BRK>This paper introduces Conditionally Reversible Network (CrevNet) that consists of the invertible autoencoder and a reversible predictive module (RPM). The experiments on Moving MNIST, Traffic4cast, KITTI, and 2D object detection on KITTI show the improvement compare to other state of the art models. The paper is well written and the contributions are clear. The experiments on diverse tasks and datasets are provided. <After rebuttal>Authors have mostly addressed my concerns, and the contributions are clearer.<BRK>This paper proposed CrevNet, a conditionally reversible network, that performs video prediction efficiently and effectively. The proposed CrevNet used the bijective two way autoencoder to make sure the frames are inherently invertible from the latent space. While the idea of the paper is good, I am not convinced of some points raised in the paper.<BRK>Applying resolution-preserving blocks is a common practice to maximize information preservation in video prediction, yet their high memory consumption greatly limits their application scenarios. They propose CrevNet, a Conditionally Reversible Network that uses reversible architectures to build a bijective two-way autoencoder and its complementary recurrent predictor. Their proposed approach achieves state-of-the-art results on Moving MNIST, Traffic4cast and KITTI datasets. They further demonstrate the transferability of their self-supervised learning method by exploiting its learnt features for object detection on KITTI. Their competitive results indicate the potential of using CrevNet as a generative pre-training strategy to guide downstream tasks.
Reject. rating score: 3. rating score: 3. rating score: 6. Specifically, they train a second "little" neural network to approximate a pre trained "big" network and use simple rules to switch between the little and the big network. While this manuscript proposes a reasonable contribution, it lacks real comparisons to many of the common competing methods that hinder the interpretation. The result is also simpler to implement. I would argue that this is less work than the proposed approach, which requires switching rules and a second trained network. The authors should give better discussion and motivation on the random projections. This is an area with very deep theory, yet the rules are provided without a rationale. The authors should motivate and discuss this section more. As there is a lot of similarity in the motivations, you should discuss that line of research in your related work.<BRK>In this paper, the authors design a big little dual module inference to dynamically skip unnecessary memory access and computation to speedup RNN inference. Strength:(1)	Well written in general. For instance, there is no information in the paper that explain why the authors design functions in such certain way (such as equation (2) and (3))d.	the authors did not provide enough detailed information about how to select the quantization methods since there are lots of approaches such as static (uniform) or dynamic quantization, where different methods may have different impacts on the final performance e.	the authors mentioned that they have tried both sigmoid and tanh activation function to find the sensitive region. However, they do not provide enough reason to do so, how about using other non linear activation functions(3)	The organization in Section 4 can to be improved. (7)	While the paper has good coverage of the prior work, I do suggest the authors can also cite or discuss some newly designed models (in 2019).<BRK>This paper attempts to compress the networks so as to accelerate the running procedure as well as save the storage. Through a statistical investigation, the authors provide a method to choose the little or big module dynamically. By applying this method on LSTM and GRU, the authors make them more efficient. Overall, I think this paper is well written and easy to follow. The idea of dual module is interesting and wise. The experiments is valid. However, I would like the authors to answer me two questions. Intuitively, the distribution should be similar to that of tanh since their functions are similar.<BRK>They propose a big-little dual-module inference to dynamically skip unnecessary memory access and computation to speedup RNN inference. Leveraging the error-resilient feature of nonlinear activation functions used in RNNs, they propose to use a lightweight little module that approximates the original RNN layer, which is referred to as the big module, to compute activations of the insensitive region that are more error-resilient. The expensive memory access and computation of the big module can be reduced as the results are only used in the sensitive region. Their method can reduce the overall memory access by 40% on average and achieve 1.54x to 1.75x speedup on CPU-based server platform with negligible impact on model quality.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 6. This is a sophisticated paper on predicting molecular properties at atom as well as molecular levels using physics inspired, extended GNN architectures. First, the encoding of atom distances for use in neural messages is no longer done in terms of Gaussian radial basis function representations but in terms of spherical Bessel functions. The second and the primary contribution of the paper, beyond the architecture itself, is the use of directional embeddings of messages where angles are transformed into cosine basis for neural mappings. Since local directions are equivariant with an overall molecular rotation, the message passing architecture in this new representation remains invariant to rotations/translations. the authors suggest that the radial information can be transformed simply by element wise multiplication while angular information requires more complex transformations in message calculations. it seems models for QM9 data were trained separately for each different physical target.<BRK>This paper beneficially incorporates directional information into graphneural networks for molecular modeling while preserving equivariance. This paper is a tour de force of architecture engineering. I found the exposition extremely intelligible despite my lack offamiliarity with molecular modeling. The contribution is clear, although applicability beyond the specificdomain of molecular modeling is possibly limited. I recommend acceptance, because the contribution is strong, the writingis excellent, the ideas are well motivated, and the experiments supportthe empirical claims.<BRK>Strength:  The paper is well written and easy to follow  The authors proposed a new approach called directional message passing to model the angles between atoms, which is missing in existing graph neural networks for molecule representation learning   The proposed approach are effective on some targets. This paper studied learning the graph representation of molecules by considering the angles between atoms. Overall, the paper studies a very important problem, which aims to learn the representation of molecules. From the view of graph neural networks, the proposed technique is not that new since edge embedding has already been studied in existing literature. But for the technique could be particular useful for molecule representation learning, especially with the BESSEL FUNCTIONS. In terms of the experiments, the proposed approach does not seem that strong, only achieving the best performance on 5 targets out of 12. Overall, I feel this paper is on the borderline.<BRK>Graph neural networks have recently achieved great successes in predicting quantum mechanical properties of molecules. They do not, however, consider the spatial direction from one atom to another, despite directional information playing a central role in empirical potentials for molecules, e.g.in angular potentials. Each message is associated with a direction in coordinate space. They leverage these innovations to construct the directional message passing neural network (DimeNet). DimeNet outperforms previous GNNs on average by 76% on MD17 and by 31% on QM9.
Reject. rating score: 3. rating score: 6. rating score: 6. The authors propose the adaptive thermostat Monte Carlo sampler for feedforward neural networks. Further, the number of filters is doubled relative to ResNet 56 without explanation. Further, the authors claim that the need for hyperparameter setup is reduced provided that early stopping, stochastic regularization and carefully tuned learning rate schedules are not required. In summary, the proposed approach needs to be described in more detail and the experiments are not very satisfying given the claims made by the authors in the Introduction.<BRK>The design idea is well explained by the authors. What are the advantages of the approach proposed in this paper over [1]? However, in Stochastic gradient Markov chain Monte Carlo (SG MCMC) [1], the authors used a meta learning algorithm to learn Hamiltonian dynamics with state dependent drift and diffusion which makes the system scalable to large datasets, very similar to this work.<BRK>This paper proposes a novel MCMC algorithm (ATMC) that estimates and samples from the posterior distribution of neural network weights. The motivation for this approach is that applying Bayesian inference to deep learning should lead to less overfitting and better uncertainty calibrated models.<BRK>Bayesian inference promises to ground and improve the performance of deep neural networks. Markov Chain Monte Carlo (MCMC) methods enable Bayesian inference by generating samples from the posterior distribution over model parameters.
Reject. rating score: 1. rating score: 3. rating score: 8. The authors propose a method based on GAN  to classify data while automatically removing confounding effects during training, in order to obtain a classifier whose features are not biased by any confounding effect. In practice, the parameters of the feature extraction component are updated to solve both bias prediction and the desired classification problem in adversarial fashion. The paper is interesting and addresses an important problem for the application of machine learning methods in several real context. The feeling is however that the paper should have better explored the implication of the proposed model of bias, and better investigated the relationship with simpler approaches relying on similar hypothesis. The authors should have investigated the relationship between the proposed method and bias removal through canonical correlation analysis (CCA), and perhaps its non linear variants. Using the CCA projections in the latent space for classification would be the closest approach to the state of the art for bias removal in statistical analysis (residual analysis). It is not clear why authors quantify the correlation in the latent space with tSNE projections. tSNE is highly sensitive to the choice of parameters and it would be important to ensure that it was a “fair competition” between all the methods, when showing the results of the dimensionality reduction. The authors also proposed to assess the decorrelation of the estimated features throughout the different methods by measuring the squared distance correlation. Experiment 4.2 has some controversial aspects, as the bias correction is performed on the control population only, while the model is trained on the entire population. We also observe that the results of the baseline CNN are very close to the BR Net. However, what would happen if we corrected for age before applying the CNN ?<BRK>This paper proposes an adversarial approach toward debiasing neural network representations w.r.t protected attributes. The core idea is balance task loss with an adversarial loss from which protected attributes have low correlation with the feature representation used for the end task. The paper provides some synthetic experiments, and evaluates on HIV data (the bias variable being age) and gender classification in images, stratified by skin shade (the bias variable being skin shade). Overall the direction is interesting and the methodology is intuitive and sound. (ICCV 2019) https://arxiv.org/abs/1811.084894. While some some of these papers can potentially be considered contemporary, authors must at least address these issues. Furthermore, modification of the objective seems to have mixed results (Table 2 CatGAN vs. BR NET, although I m not sure whats going on with vgg vs resnet), where the baseline would correspond more closely to the setup in https://arxiv.org/abs/1811.08489 (3. above) . Overall I am positive about the direction, but I am unsure this paper represents a significant contribution over existing work.<BRK>This paper presents Bias Resilient neural network (BR Net) that is designed to learn representations that can accurately predict the desired target while being invariant to the confounding covariates in the data. The proposed method is based on domain adversarial training strategies, especially that of (Ganin et al., 2016), where the adversarial component is modified from “loss of distinguishing between the source and target domains” to “the squared Pearson correlation between the ground truth bias covariate and its estimation from the learned representation”. This design is based on the argument that the ultimate goal of bias control here is removing statistical association with respect to the bias variables, as opposed to maximizing the prediction error of them. Things to improve the paper that did not impact the score:	  Introduction, line 4: Wrong citation format: use of \citet instead of \citep. Correct for all citations throughout the paper. The Journal of Machine Learning Research, 17(1), 2096 2030.<BRK>Presence of bias and confounding effects is inarguably one of the most critical challenges in machine learning applications that has alluded to pivotal debates in the recent years. The alternative is to make use of available data and build models considering these biases. Traditional statistical methods apply straightforward techniques such as residualization or stratification to precomputed features to account for confounding variables. In this paper, they propose a method based on the adversarial training strategy to learn discriminative features unbiased and invariant to the confounder (s). Their results show that the learned features by their method not only result in superior prediction performance but also are uncorrelated with the bias or confounder variables.
Reject. rating score: 3. rating score: 3. rating score: 6. This paper suggests a PAC Bayesian bound for negative log likelihood loss function. Many PAC Bayesian bounds are provided for bounded loss functions but as authors point out, Alquier et al.(2016) and Germain et al.(2016) extend them to  unbounded loss functions. I have two major concerns regarding this paper:1  Technical contribution: Since Alquier et al.(2016) has already introduced PAC Bayesian bounds for the hinge loss, I think the technical contributions of this paper is not significant enough for the publication. Moreover, the particular format of the bound in Theorem 2 is problematic since the right hand side depends on the data distribution. The problem is that in all those networks deeper ones generalize better so it is not clear that the correlation is due to a direct relationship to generalization or a direct relationship to depth.<BRK>Summary:This paper proposes a PAC Bayesian generalization bound for Bayesian neural networks. The author discuss that earlier generalization bounds hold for bounded loss function, whereas the proposed generalization bound considers negative log likelihood loss which can be unbounded. Significance: My biggest concern with this work is its significance. Also as authors mention, there have been two earlier results covering other unbounded loss functions. How can one refute the effect of a specific initial value? * According to the paper, the architectures in table 2, fig 1 are made by keeping the number of parameters roughly the same. As depth is not the only parameter that is changed between the architectures. In the first read one might think this paper provides PAC Bayesian bounds for usual NNs (which has been considered and written about many times in the literature). The authors should mention that the considered networks are Bayesian NNs.<BRK>The paper offers PAC generalization bounds for Bayesian Neural Networks based on a previous result by Alquier et al.(Theorem 1) which connects the generalization gap to the log partition function of the same gap for the prior distribution on the learned parameters (which is identical to the ELBO bound used in Bayesian neural networks for NLL loss). The authors note that the log partition function can in general be easily unbounded for loss functions based on NLL (as in the BNN case); their result shows that if the norm of the gradient is bounded, that is enough to bound the overall generalization gap. While this appears to be a technically impressive feat, the the assumptions involved in Theorem 2 seem significant (probably unavoidable for a theoretically tractable statement). In experiments, the authors explore and analyze the tightness of the proposed bounds for various hyperparameters like the variance of the weights prior. They also perform an exhaustive comparison of the BNN models against non bayesian alternatives, but it is not clear how the new contributions from the generalization bounds are relevant to the results in, say Section 6.2<BRK>Bayesian neural networks, which both use the negative log-likelihood loss function and average their predictions using a learned posterior over the parameters, have been used successfully across many scientific fields, partly due to their ability to ` effortlessly' extract desired representations from many large-scale datasets. However, generalization bounds for this setting is still missing. In this paper, they present a new PAC-Bayesian generalization bound for the negative log-likelihood loss which utilizes the \emph {Herbst Argument} for the log-Sobolev inequality to bound the moment generating function of the learners risk.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 8. To address this, the paper proposed a gain gate to increase the magnitude of a layer in the LISTA algorithm. The second proposed gate, the overshoot gate, lacks theoretical justification and is not as well studied as the gain gate. Thorough synthetic experiments are conducted to empirically validate the proposals as well as the theorems. The effectiveness of the gates is also proved in a real world computer vision task, photometric stereo.<BRK>It discusses the weakness of the ``no false positive  assumption inprevious works and the weakness results in underestimated code components. Both contributions are supported with theoretical andempirical results. The paper is well written and easy to follow, and theempirical results are impressive. When the gain and overshoot mechanisms are combined, however, willthe theoretical convergence still hold? Update: the authors addressed my questions well and I will keep my positive decision on this paper.<BRK>1.SummaryThe authors propose extensions to LISTA with the goal of addressing underestimation (by introducing “gain gates”) and including momentum (by introducing “overshoot gates”). The authors provide theoretical analysis for each step of their LISTA augmentations, showing that it improves convergence rate. What is the input that gives this output? A couple of the plots validating theory are difficult to understand.<BRK>In this paper, they study the learned iterative shrinkage thresholding algorithm (LISTA) for solving sparse coding problems. In addition to the gain gates, they further introduce overshoot gates for compensating insufficient step size in LISTA. Extensive empirical results confirm their theoretical findings and verify the effectiveness of their method.
Reject. rating score: 3. rating score: 3. rating score: 3. This paper proposes a new approach to learning contextualised cross lingual representations based on a three step procedure called: transform extract reorder. The authors claim that 250k pairs of parallel sentences is enough to learn a near optimal cross lingual model and that the model saturates with more parallel sentences. The actual downstream experiments are conducted on only one language pair and for only one (zero shot) transfer task. This definitely requires further and more thorough investigation.<BRK>This paper presents a new method to obtain cross lingual contextual embeddings by aligning monolingual ones through 3 steps: transform each token individually, merge them as needed to obtain a uniform granularity across languages, and reorder them. The only extrinsic evaluation is in XNLI, where the authors evaluate the zero shot cross lingual transfer performance from English into Chinese. This can have some justification (it might be computationally prohibitive for the authors to pre train their own models, which might be the reason why they use public models trained on different data) but they should be more upfront about this. However, the proposed method does not bring any improvement over the current state of the art in this setup.<BRK>This paper proposes a method to learn language independent cross lingual contextual representations by mapping the representations of a monolingual model in one language to the representations of a monolingual model in another language. The proposed approach consists of three steps: 1. A transformation is learned that minimizes the distance between the contextual word representations of the two models of a sentence and its translation in a parallel corpus. The authors perform experiments on intrinsic tasks and on XNLI, mapping between an English and a Chinese BERT model. The high level steps of the approach (transform, align, reorder) make sense. The extrinsic evaluation of the paper could be improved. The accuracy on each intrinsic step is evaluated. Finally, the data requirement of 250k parallel sentences is prohibitive for many language pairs where this approach would be valuable.<BRK>They propose a cross-lingual contextual representation model that generates language-independent contextual representations. This helps to enable zero-shot cross-lingual transfer of a wide range of NLP models, on top of contextual representation models like BERT. They provide a formulation of language-independent cross-lingual contextual representation based on mono-lingual representations. Their formulation takes three steps to align sequences of vectors: transform, extract, and reorder. They present a detailed discussion about the process of learning cross-lingual contextual representations, also about the performance in cross-lingual transfer learning and its implications.
Reject. rating score: 3. rating score: 3. rating score: 8. In this paper, the authors propose a new class of programs they call programming puzzles. The authors argue that this class of programs is ideal for helping learn AI systems to reason. The second contribution of the paper is an adaptive method of puzzle generation inspired by GAN like generation that can generate a diverse and difficult set of programs. The methodology to generate programs is convincing. That would be a strong baseline to compare against. Overall, I feel that the paper puts forth an interesting class of programs. But there are some gaps in the evaluation and the baselines. I am also not sure how this class of programs can help advance artificial reasoning. The paper ends abruptly.<BRK>The authors also seem to miss the entire curricular learning literature. (2) The empirical evaluations are lacking. In particular, more thorough analysis of how the generator is behaving, the type of curriculum it learns, and the resulting impact this has on a trainable solver all are missing. The paper is fairly clear overall. I m not an expert in this area so it is possible I misjudged the significance of this work. It seems the most interesting solver to assess is a trainable solver. Furthermore, it would be helpful to have more discussion of the baseline methods of generating puzzles. When the trainable puzzle solver was originally proposed, how was it trained? I am not very familiar with this literature, and I imagine this paper would be of interest to folks outside the program synthesis space, so it would be very helpful to better explain this (also we note about related work). Is there a way of quantifying the  hardness  of a puzzle? Perhaps a proxy like size? Again, I m not super familiar with this work, but I think there is a lot of curriculum learning stuff within RL that seems super relevant.<BRK>This is an interesting and important problem which sits at the intersection of symbolic and deep learning based AI. The problem is well motivated, and the approach is sensible. As this is a novel problem, the paper also defines their own metric, namely the average time taken to solve the puzzle by given solvers, and the diversity of generated puzzles. It is nice to see that the generator indeed learns to generate puzzles that are significantly harder than random counterparts, while maintaining reasonable diversity. Although I think these are convincing results, my question to the authors is: have you tried or considered other ways of evaluating the generated puzzles? I think this would be interesting to see, which can serve as an alternative evaluation metric. My other comments are regarding the experiment section:1.<BRK>What type of problemswould be best for computers to learn to program, and how should such problemsbe generated? To answer the first question, they suggest programming puzzles as adomain for teaching computers programming. Puzzles are objective in that one can easilytest the correctness of a given solution x by seeing whether it satisfies f, unlike themost common representations for program synthesis: given input-output pairs or anEnglish problem description, the correctness of a given solution is not determinedand is debatable. To address the second question of automatic puzzle generation, they suggest a GAN-like generation algorithm called “ Troublemaker ” which cangenerate puzzles targeted at any given puzzle-solver.
Reject. rating score: 1. rating score: 3. rating score: 3. 1.The paper aims to train a model to move objects in an image using language. The task itself is interesting as it aims to modify system behavior through language. This suggests that the GAN term is not adding as much value as the authors claim. Reason 2: Other baselines need to be considered, AE, VAE or other variations. Reason 3: No ablations on the impact of the parameters to eq 1.<BRK>This paper proposes a model that takes an image and a sentence as input, where the sentence is an instruction to manipulate objects in the scene, and outputs another image which shows the scene after manipulation. The model is an integration of CNN, RNN, Relation Nets, and GAN. The results are mostly on synthetic data, though the authors also included some results on real images toward the end. This paper, despite studying an interesting problem, is limited in terms of its technical innovations and experimental results. To me, it s unclear how the system can inspire future research. There are no comparisons with published, SOTA methods. Considering all this, I believe this paper cannot be accepted to a top conference such as ICLR.<BRK>This paper proposes an architecture for generating images with objects manipulated according to user specified or conditional instructions. This paper is also very clear and easy to follow and understand what the authors have done. It seems that those components are all taken straight out of the shelf and combined. It would be interesting to see what subtle changes were important in a combined system to further increase performance. What are some of the most frequent failure cases? The qualitative results look reasonable, and I’m quite surprised that only 10K images were used for training.<BRK>In this paper, they combine these four techniques to acquire a shared feature representation of the relation between objects in an input image and an object manipulation action description in the form of human language encodings to generate an image that shows the resulting end-effect the action would have on a computer-generated scene. The system is trained and evaluated on a simulated dataset and experimentally used on real-world photos.
Reject. rating score: 3. rating score: 6. The paper reads well, the topic is interesting, and the connection between early stopping and fitting true label distributions before noisy ones, if generally true, is of potential impact. ⁃	Learning a ground truth from a population of turkers (MACE and subsequent work). Can you replicate your results with simple L2 regularisation? ⁃	The observation that “clean examples are fitted faster than noise” is obviously related to baby steps training regimes (training on easy examples first), including active learning. ⁃	It seems to be that it would have been relatively straight forward to construct synthetic datasets that would more directly evaluate the hypothesis that the true labels are learned first. ⁃	That “the noise generated through the label collection process is not nearly as adversarial for learning as the noise generated by randomly flipping labels” is no surprise, and I would not present this as a finding.<BRK>The paper shows that early stopping the training on datasets with noisy labels can achieve classification performance higher than when training on clean labels (on condition that the total number of clean labels in the noisy training set is sufficiently large). The latter is often done in the literature. The “real” noise is more structured and therefore it is easier to fit and less harmful to the classification performance. StrengthsThe paper is well written. The results are very interesting even though they are very intuitive and simple. WeaknessesThe idea seems to be known. As shown by the above paper, it is also a function of the learning rate. The paper does not make it clear under what conditions early stopping prevents the model from memorizing bad labels. The paper focuses on classification. Will the claims hold for other task types such as object detection, segmentation, etc?<BRK>Classification problems today are typically solved by first collecting examples along with candidate labels, second obtaining clean labels from workers, and third training a large, overparameterized deep neural network on the clean examples. In this paper they skip the labeling step entirely and propose to directly train the deep neural network on the noisy raw labels and early stop the training to avoid overfitting. Their results show that early stopping the training of standard deep networks such as ResNet-18 on part of the Tiny Images dataset, which does not involve any human labeled data, and of which only about half of the labels are correct, gives a significantly higher test performance than when trained on the clean CIFAR-10 training dataset, which is a labeled version of the Tiny Images dataset, for the same classification problem. In addition, their results show that the noise generated through the label collection process is not nearly as adversarial for learning as the noise generated by randomly flipping labels, which is the noise most prevalent in works demonstrating noise robustness of neural networks.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. In this paper, the authors apply the Nesterov Accelerated Gradient method to the adversarial attack task and achieve better transferability of the adversarial examples. Furthermore, the authors introduce a scale transformation method to provide the augmentation on the model, which also boosts the transferability of the attack method. Experiments are carried out to verify the scale invariant property and the Nesterov Accelerated Gradient method on both single and ensemble of models. Also, in the setting of the Scale Invariant Transformation, the authors forget to address that what if the attacked network has an input normalization.<BRK>This paper studies how to generate transferable adversarial examples for black box attacks. Two methods have been proposed,  namely Nesterov Iterative Fast Gradient Sign Method (NI FGSM) and Scale Invariant attack Method (SIM). And the second is a model augmentation method to avoid "overfitting" of the adversarial examples. Experiments on ImageNet can prove the effectiveness of the proposed methods. Overall, this paper is well written.<BRK>In this paper, the authors proposed two methods of Nesterov Iterative Fast Gradient Sign Method (NI FGSM) and Scale Invariant attack Method (SIM) to improve the transferability of adversarial examples. Empirical results on ImageNet dataset demonstrate its effectiveness. The authors are expected to make more theoretical analysis.<BRK>In this work, from the perspective of regarding the adversarial example generation as an optimization process, they propose two new methods to improve the transferability of adversarial examples, namely Nesterov Iterative Fast Gradient Sign Method (NI-FGSM) and Scale-Invariant attack Method (SIM). NI-FGSM aims to adapt Nesterov accelerated gradient into the iterative attacks so as to effectively look ahead and improve the transferability of adversarial examples. While SIM is based on their discovery on the scale-invariant property of deep learning models, for which they leverage to optimize the adversarial perturbations over the scale copies of the input images so as to avoid "overfitting ” on the white-box model being attacked and generate more transferable adversarial examples.
Accept (Poster). rating score: 8. rating score: 8. rating score: 3. The empirical study of generalization behavior is inherently tricky, and the authors make a non trivial contribution just by designing an experiment that can convincingly address this question. The results of the authors  experiments are highly valuable to the research community.<BRK>I was impressed with the coverage and variations done. This paper is a natural successor to a long line of papers, most recently Zhang at el (best paper at ICLR 2017, though some like Tom Dietterich weren t impressed), Neyshabur et al (NIPS 2017) and Jiang et al (ICLR 2019).<BRK>3) In Section 4, the authors say « drawing conclusion from changing one or two hyper parameters » can be a pitfall as « the hyper parameter could be the true cause of both change in the measure and change in the generalization ». The writing of the paper is clear and easily understandable. 4) It is still not clear to me how the authors explain why some measures are more correlated with generalization gaps than others.<BRK>Generalization of deep networks has been intensely researched in recent years, resulting in a number of theoretical bounds and empirically motivated measures. They present the first large scale study of generalization bounds and measures in deep networks.
Reject. rating score: 3. rating score: 6. rating score: 6. The paper proposes a novel RL algorithm to minimize ‘surprise’, and the empirical results show the efficiency. However, there are several problems in other parts:1. So I guess it’s a finite horizon MDP? 3.What’s the final situations of the policy and density function? 5.The agent trying to reach stable states sounds like exploration is discouraged.<BRK>SummaryThis paper proposes a novel form of surprise minimizing intrinsic reward signal that leads to interesting behavior in the absence of an external reward signal. This itself is not uncommon and nothing controversial from a practical perspective. However, I do not like one aspect in the way the authors motivate their approach.<BRK>This paper proposes Surprise Minimizing RL (SMiRL), a conceptual framework for training a reinforcement learning agent to seek out states with high likelihood under a density model trained on visited states. To formulate this notion of surprise as a meaningful RL problem it is necessary to include some representation of the state likelihood in the agent s state representation; so that the agent can learn how it s actions affect not only the environment state but it s own potential for future surprise. The agent s policy is then conditioned on the parameters of the density model and the current size of the dataset.<BRK>They propose that such a search for order amidstchaos might offer a unifying principle for the emergence of useful behaviors inartificial agents. The resulting agents acquire several proactive behaviorsto seek and maintain stable states such as balancing and damage avoidance, thatare closely tied to the affordances of the environment and its prevailing sourcesof entropy, such as winds, earthquakes, and other agents.
Reject. rating score: 1. rating score: 3. rating score: 3. The authors also showed experiments to backup their theoretical results. I recommend the authors to add some explanation about why softAdam outperforms other existing algorithms. This paper lacks some references in this area.<BRK>The paper tends to explain how the tradeoffs between convergence speed and convergence performance are made by different optimization methods. Cons1.The writing of this paper is not well organised. 2.The notation in the paper is little confusing. 3.The motivation of the proposed method is weak. 4.There are also several minor problems on the numerical results.<BRK>DecisionI vote for weak rejection because the core modification proposed to Adam is minor and is mostly supported by intuition and preliminary experiments. The code in Appendix corroborates this correction. These modifications could as well be incorporated in Adam without the core modification that is the smoothing presented in section 3.<BRK>This makes it possible to control the way models are trained in much greater detail.
Reject. rating score: 3. rating score: 3. rating score: 8. The authors provide a new universal approximation theorem on real valued functions that doesn t require the assumption of a fixed cardinality of the input set. They further provide examples of functions that can t be mutually approximated by PointNets and DeepSets. The presentation in this paper does remove the assumption of a fixed cardinality, but since this seems to be a mild assumption, it is not clear what is gained by this (beyond mathematical elegance). The paper doesn t give any hints here. Summary: UATs are important and interesting, however, they do exist for the architectures that are targeted in this paper. Disclaimer: While I believe to have a reasonable mathematical background, I m not an expert in this field and my assessment is primarily based on the bottom line of these proofs. Post rebuttal update  I d like to thank the authors for their efforts and additional insights.<BRK>The paper aims to establish novel theoretical properties of known point based architectures for deep learning, PointNet and DeepSets. To this end, the authors prove a series of theoretical results and establish limitations of these architectures for learning from point clouds. I do not find significant practical value in the obtained results. While the theorems proved in the paper are original and novel, they are a refinement of the already known results regarding approximation theorems for PointNet and DeepSets, respectively, hence only a marginal improvement in understanding these function classes.<BRK>However, existing results on their approximation abilities are limited to fixed cardinalities. This paper removes the cardinality limitation and gives two kinds of results:1. This paper brings a valuable theoretical contribution to the existing state of the art of their approximation abilities. With some improvements, I am willing to increase the score. 2.Notations in the section 2.4 make the reading particularly unclear. Notations should showcase the result that theoretically, only two hidden layers (with appropriate definitions) are needed. 3.The paper lacks an experimental section. It would be interesting to investigate empirically the limitations of these architectures, for instance by playing with the diameter and center of mass functions as suggested in 3.3. Post rebuttal: the authors have addressed 3., therefore I am leaning towards accept.<BRK>In this paper they prove new universal approximation theorems for deep learning on point clouds that do not assume fixed cardinality. They do this by first generalizing the classical universal approximation theorem to general compact Hausdorff spaces and then applying this to the permutation-invariant architectures presented in 'PointNet' (Qi et al) and 'Deep Sets' (Zaheer et al). In particular, DeepSets architectures cannot uniformly approximate the diameter function but can uniformly approximate the center of mass function but it is the other way around for PointNet.
Reject. rating score: 3. rating score: 6. rating score: 6. This paper aims to propose a speeding up strategy to reduce the training time for existing GNN models by reducing the redundant neighbor pairs. The idea is simple and clear. Even for an unweighted undirected graph, the symmetric information may also be redundant for further elimination. Then can HAG reduce this symmetric redundancy? Update Thanks very much for the authors  feedback. The revised version has clarified some of my concerns.<BRK>This paper proposes a new graph Hierarchy representation named HAG. The HAG aiming at eliminating the redundancy during the aggregation stage In Graph Convolution networks. The idea is clear and easy to follow. Generally speaking, I think this paper has good theory analysis, the speed up effect is also good from the experimental result.<BRK>In this paper, authors propose a way to speed up the computation of GNN. These theoretical results are nice. I put 6 (weak accept), since we cannot put 5. In particular, for the sequential one, prediction accuracy can be changed due to the aggregation algorithm. It would be nice to have the trade off between the sampling rate and the speedup.<BRK>However, because common neighbors are shared between different nodes, this leads to repeated and inefficient computations.We propose Hierarchically Aggregated computation Graphs (HAGs), a new GNN graph representation that explicitly avoids redundancy by managing intermediate aggregation results hierarchically, and eliminating repeated computations and unnecessary data transfers in GNN training and inference. They introduce an accurate cost function to quantitatively evaluate the runtime performance of different HAGsand use a novel search algorithm to find optimized HAGs. Meanwhile, HAGs improve runtime performance by preserving GNNcomputation, and maintain the original model accuracy for arbitrary GNNs.
Reject. rating score: 3. rating score: 3. rating score: 3. Further, it is difficult for readers to place the results in context. I certainly agree with the authors about the inability of GANs to learn structural constraints with insufficient training data, as this has been demonstrated in many examples of prior work. There is a lack of discussion in the paper on the results of each experiment. In this paper the authors present a Generative Adversarial Neural Networks with Xu et al.’s semantic loss applied to the generator.<BRK>This paper proposed Constrained Adversarial Networks (CAN), which incorporates structural constraints by augmenting a penalty term in the training object. I like the idea of encouraging constraints for generative models, which is useful and interesting. The novelty of this paper is to apply these techniques to generative models, which seem to be a bit straightforward. Overall the contribution of this paper does not seem to be strong enough. [2] Hu, Zhiting, et al."Deep generative models with learnable knowledge constraints."<BRK>What is theta in this equation? The proposed approach relays on the knowledge compilation method, but they re very few details of it in the document. I am concern about the lack of reproducibility of the paper. Significance:It is hard to quantify the significance of the contribution. Seff, Ari, et al."Discrete Object Generation with Reversible InductiveConstruction." Summary:I find that the problem addressed by the authors is highly relevant and the proposed approach has the potential to be useful in practice.<BRK>Despite their success, generative adversarial networks (GANs) cannot easily generate structured objects like molecules or game maps. The issue is that such objects must satisfy structural requirements (e.g., molecules must be chemically valid, game maps must guarantee reachability of the end goal) that are difficult to capture with examples alone. This setup is further extended to hybrid logical-neural constraints for capturing complex requirements like graph reachability.
Reject. rating score: 3. rating score: 6. rating score: 6. This work describes a method for finding mixed strategy Nash equilibria in (normal form) games with continuous action spaces. This update rule has some desirable theoretical properties, which I believe are mostly proven by Raghunathan  19. The central contribution of this work is to parameterize a mixed strategy via a learned NN mapping from a simple distribution U[0,1]^d to the mixed strategy of interest.<BRK>This paper proposes a novel algorithm for finding mixed strategy Nash equilibria in games with continuous action spaces. This experimental finding is not surprising, as its competitors were designed to compute pure strategy Nash equilibria. However, I feel that there are some issues with this paper. The writing in the paper would benefit from revision. While the paper generally gets the point across, much of it feels sloppy.<BRK>The paper proposes a method to learn mixed strategies Nash equilibrium in multi player games. It also provides an application with a strategy approximation made with a deep neural network. I liked the paper very much but I have some concerns. First, I feel that the framework is based on a variational approach which would be well suited for a 0 order optimisation (like a black box or an evolutionary method). Second, I felt the theoretical proofs are not using much more than standard algebra and the convex assumption was a bit unrealistic in most of multi agent problems. Could the authors comment on this ?<BRK>Despite the fact that several deep learning based approaches are designed to obtain pure strategy Nash equilibrium, it is rather luxurious to assume the existence of such an equilibrium. In this paper, they present a new method to approximate mixed strategy Nash equilibria in multi-player continuous games, which always exist and include the pure ones as a special case.
Reject. rating score: 3. rating score: 3. rating score: 6. In this paper, the authors proposed to combine both NN based NAS and Aging EVO to get the benefit of both world: good global and local sample efficiency. Overall, while the novelty of the paper is not exceptional, since it is a rather straightforward combination of two existing approaches, the end results is promising. Moreover, do we need a NN for the EVO NAS? Or something simpler would be sufficient to guide the search.<BRK>This paper is well organized. But it lacks some more detailed analysis. 1.The performance differences between Evolutionary agent and EVO NAS agent seem not significant. It would be better to compare one of these approaches in the experiments.<BRK>It is a nice paper that combines the deep reinforcement learning and evolutionary learning techniques to neural architecture search problem. 1.In Fig 1,2 &3, it seems that the performances of Neural (PQT) keeps increasing.<BRK>Neural Architecture Search has shown potential to automate the design of neural networks. They show that the Evo-NAS agent outperforms both neural and evolutionary agents when applied to architecture search for a suite of text and image classification benchmarks.
Reject. rating score: 3. rating score: 6. rating score: 6. The paper proposes an iterative method that jointly trains the model and a scorer network that places a non uniform distribution over data sets. The paper proposes a gradient method to learn the scorer network based on reinforcement learning, which is novel as to what the reviewer knows. I think the clarity of both methodology and experiment does not reach the acceptance level and would maintain my current rating.<BRK>This paper presents a reinforcement learning approach towards using data that present best correlation with a validation set’s gradient signal. The broader point of this paper is that there is inevitably some distribution shift going from train to test set   and the validation set can be a small curated set whose distribution is closer to the testing distribution than what the training dataset s distribution is. One assumption that I see which needs to be understood more is equation (6)   wherein, somehow, there is a Markov assumption used to zero out the contribution of the scoring network on parameters unto previous time step. I think the paper is well written, handles an important question.<BRK>Summary: This paper introduces a simple idea to optimize the weights of a weighted empirical training distributions. The distribution over the training set is parameterized by a neural network taking as arguments theStrengths:  The method is quite simple. The imagenet results seem quite strong to me. At the very least, this should be clearly reported, and I recommend a more thorough investigation of this choice. Isn t that computed on line 5 already?<BRK>Similarly, a machine learning model could potentially be trained better with a scorer that “ adapts ” to its current learning state and estimates the importance of each training data instance. In DDS, they formulate a scorer network as a learnable function of the training data, which can be efficiently updated along with the main model being trained.
Reject. rating score: 1. rating score: 1. rating score: 3. The aim of connecting the representation quality with adversarial robustness is interesting! 2.The paper claims to study classifiers  "representation quality". However, the claimed correlation is very weakly supported by the evidence in the paper.<BRK>4.The writing of this paper is not clear and has gramma issues. 1.The paper only evaluate robust accuracy on models without robust training. The measurement is also fragile under adversarial attacks, that one can feed in adversarial attacks to fool the score metrics, which is not convincing. However, the authors only include evaluation for non robust trained models, without considering the robust trained model, such as Madry et al.[1].The conclusion is not convincing that the authors studied the robustness using only non robust models, because it is well known that the accuracy for attacking non robust model can be 100% (for CIFAR 10).<BRK>The first is the training dataset. 3.This paper only conducts experiments on CIFAR 10 dataset, which is not convincing enough. Please explain it in detail or give a reference paper. Fig.4 is also confused, for AM, which histogram is with unknown classes and which one is without? 6.The title says "representation quality explain adversarial attacks". After reading this paper, I haven’t found the mechanism leading to the adversarial attacks of DNNs.<BRK>Interestingly, the results suggests that dynamic routing networks such as CapsNet have better representation while current deeper DNNs are trading off representation quality for accuracy. Neural networks have been shown vulnerable to adversarial samples. One is based on clustering validation techniques (Davies-Bouldin Index) and the other is based on soft-label distance of a given correct soft-label.
Reject. rating score: 1. rating score: 3. rating score: 6. This paper aims at solving geometric bin packing (2D or 3D) problems using a deep reinforcement learning framework. Namely, the framework is based on the actor critic paradigm, and uses a conditional query learning model for performing composite actions (selections, rotations) in geometric bin packing. (c) According to the problem formulation and the experiments, it seems that the authors are studying a restricted subclass of 2D/3D bin packing problems: there is only “one” bin, so (it seems that) the authors are dealing with geometric knapsack problems (with rotations). (d) The problem formulation is very unclear. In the algorithm what is n_{gae}? which are left unspecified.<BRK>This paper proposes an end to end deep reinforcement learning based algorithm for the 2D and 3D bin packing problems. Its main contribution is conditional query learning (CQL) which allows effective decision over mutually conditioned action spaces through policy expressed as a sequence of conditional distributions. Efficient neural architectures for modeling of such a policy is proposed. Experiments validate the effectiveness of the algorithm through comparisons with genetic algorithm and vanilla RL baselines.<BRK>Summary: The paper proposes heuristics to solve the bin packing problems based on reinforcement learning with deep neural networks. With a new heuristics of conditional queries, the proposed method works favorably with the previous RL based approach and other baselines. The authors, on the other hand, propose new heuristics, called conditional queries, which divides a unit of actions (rotation, box, and etc.), which turns out to be effective compared to the previous reinforcement learning based method.<BRK>All source code of the model and the test environment is released. However, it is less efficient for more complicated problems such as packing, one type of optimization problem that faces mutual conditioned action space. By embedding previous actions as a conditional query to the attention model, they design a fully end-to-end model and train it for 2D and 3D packing via reinforcement learning respectively. Their model improves 7.2% space utilization ratio compared with genetic algorithm for 3D packing (30 boxes case), and reduces more than 10% bin gap ratio in almost every case compared with extant learning approaches.
Reject. rating score: 1. rating score: 3. rating score: 6. A mixed picture is presented in the experiments which roughly agrees with some of the authors  claims. 3) The related work is not well cited. Better references are [1, 2]. Look at [4] for the sampling variance of SGD procedures; also [5, 6]. Look at [7].<BRK>The introduction is also well written. If so, it may not be surprising that the expected second moment of each parameter would evolve independently from each other, as noted at the end of Section 2. I also found the authors  setting of analyzing noisy least squares problems to be interesting because of its potential usefulness for both analytically and empirically understanding certain forms of DL phenomena.<BRK>Can the authors elaborate on a comparison between SGD with decaying step size vs SVRG with constant step size? Since many of the experiments seem to be on relatively small datasets and easier models, this should not be too burdensome. A weakness is that the regression model and linear separation is a bit oversimplified, and may not really capture the subtleties in deeper models. That being said this is not a huge negative for this paper because both scenarios are considered.<BRK>Their primary focus is to compare the exact loss of SVRG to that of SGD at each iteration t. They show that the learning dynamics of their regression model closely matches with that of neural networks on MNIST and CIFAR-10 for both the underparameterized and the overparameterized models.
Reject. rating score: 1. rating score: 6. rating score: 6. This paper suggests generating a large news summarization dataset by taking advantage of the fact that in news articles it is often the case that first few sentences contain the most important information. I have the following criticisms of this paper:  the idea is not novel. As implemented, it actually picks the second sentence of the original article, and unsurprisingly works worse than the lead X for the other two datasets.<BRK>This paper proposed an interesting idea on how we can leverage the lead bias in summarization datasets to pretrain abstractive news summarization models on large scale unlabelled corpus in simple and effective way. After fine tuning the respective datasets, the gains seem significant. Therefore I suggest to reject this paper. The real world summarizations resemble more like the ones in the DUC dataset and it would be interesting to see if the transfer results of the pretrained model on the DUC datasets. For example, how does the proposed model do compared with GPT 2 in the fine tuning setting, and how do these two models perform on the DUC datasets. How did you make this decision ?<BRK>This paper proposes to use this knowledge as self supervision for training summarization models. For this the author download and clean 3 years of news articles and use this to (pre )train a Tranformer model. If that helps as much as the summarization pre training then it would be disappointing but a nice result in favor of language modeling. I do not understand the last two sentences of Sect 4 ("A candidate word leading...). Could you explain?<BRK>While many algorithms exploit this fact in summary generation, it has a detrimental effect on teaching the model to discriminate and extract important information. They propose that the lead bias can be leveraged in a simple and effective way in their favor to pretrain abstractive news summarization models on large-scale unlabelled corpus: predicting the leading sentences using the rest of an article.
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. The main results from the theory show that the distance between the final solution and its optimal are less or equal to  relative to the distance of the initial source solution to its optimum. So a near optimal solution for the source task will lead to near optimal solution for the target task. ClarityOverall, the paper is well written, well motivated and well structured. NoveltyThe novelty in this work seems to be the application of homotopy methods to the transfer learning settings. The mathematical guarantees are also new and may even offer new ways to interpret fine tuning methods that have been so successful in recent literature. However, given the  non convexity of DNNs, it seems like the analysis in the non convex settings and its implications  should be part of the main text. Experiments:Overall, the experiments are very insightful but limited since you only show the training loss and the validation performance is not evaluated at all.<BRK>Based on homotopy,, the paper describes a more rigorous approach to transfer learning than the so called ‘fine tuning’ heuristic. The citations of the VGG paper is currently referenced by first names of the authors, not their last names, I am not sure if this was intended. Overall, this reviewer found the concept simple and elegant, and well motivated, and also well introduced. Progress in the direction of more principled approaches for transfer learning would be tremendously impactful, since one of the core promises of deep learning is the learning of features, which can be used in different downstream tasks.<BRK>Authors propose a very general framework of Homotopy to the deep learning set up and explores a few relevant theoretical issues. Though the proposed idea is interesting, the depth and breadth of authors  presentation are simply lacking. The entire paper lacks focus and I suggest authors consider focusing on 1 2 well thought out ideas. There are many 3 4 line long sentences that are hard to decipher. Please also consider making the presentation more accessible. Overall, this paper does not meet the bar for ICLR.<BRK>Homotopy methods, also known as continuation methods, are a powerful mathematical tool to efficiently solve various problems in numerical analysis, including complex non-convex optimization problems where no or only little prior knowledge regarding the localization of the solutions is available. In this work, they propose a novel homotopy-based numerical method that can be used to transfer knowledge regarding the localization of an optimum across different task distributions in deep learning applications. They validate the proposed methodology with some empirical evaluations in the regression and classification scenarios, where it shows that superior numerical performance can be achieved in popular deep learning benchmarks, i.e.FashionMNIST, CIFAR-10, and draw connections with the widely used fine-tuning heuristic. In addition, they give more insights on the properties of a general homotopy method when used in combination with Stochastic Gradient Descent by conducting a general local theoretical analysis in a simplified setting.
Reject. rating score: 1. rating score: 1. rating score: 1. This work proposes a corpus based approach of mining lexicon for a low resource language focusing on Amharic sentiment. I have strong concerns to this work. The condition of the proposed method is not clear. Otherwise, it is very hard to replicate the experiments. I think this work should have been submitted to other conferences, workshops or journals, focusing more on low resource languages.<BRK>This paper introduces a corpus based approach to build sentiment lexicon for Amharic. While this paper presents an economical and practical method to generate a sentiment lexicon for resource limited language it is not acceptable in it s current state to ICRL. (1) The generalizability to any language needs to be shown. I would expect a stronger focus on representation learning. (3)	The conclusion is not well proved. Especially about the claim that their method is “with almost minimal costs and time”. The part “4 RESULTS AND DISCUSSION” presents neither results nor discussion. Part 4.1 shows seed words which should belongs to part 3. Therefore, it is better to make a restructure. Are the 2500 comments stemmed?<BRK>This paper proposes a domain specific corpus based approach for generating semantic lexicons for the low resource Amharic language. Instead, this work proposes to automatically generate a semantic lexicon using distributional semantics from a corpus. The proposed approach starts from a seed list of sentiment words in 3 pre determined POS classes. This paper proposes a efficient, unsupervised way of gathering semantic lexicons that perform reasonably well on a downstream task. Few example of ones are 1. 2.Novelty of work: The novelty of the work is rather limited and the paper should try more low dimensional embedding based approaches which has been proven to be very effective for a wide variety of tasks. In section 3, the paper mentions there are primarily two kinds of approaches (count based vs embedding based) — The paper should motivate why it chose one over another. 4.Organization of the paper: The writing and the organization of the paper needs to be better. The results and discussions did not have either and over all there are many grammatical errors. Overall, the paper is a nice effort but in its current form it is not ready for ICLR and I hope the comments will help make the paper better for upcoming NLP workshops and conferences.<BRK>However, manual construction of sentiment lexicon is time consuming and costly for resource-limited languages. The intention of this approach is to handle sentiment terms specific to Amharic language from Amharic Corpus. They developed algorithms for constructing Amharic sentiment lexicons automatically from Amharic news corpus. Based on the threshold value, the top closest words to the mean vector of seed list are added to the lexicon. Finally, the lexicon generated in corpus based approach is evaluated.
Reject. rating score: 3. rating score: 3. rating score: 3. However, my main concern is that the experimental results (Page 8 Table 2) does not support the merits of the proposed approach. This paper introduces a context aware neural network (conCNN) that integrates context semantics into account for object detection. NIPS 2011. The  paper reads well.<BRK>The paper proposes a contextual reasoning module following the approach proposed by the NIPS 2011 paper for object detection.<BRK>This paper proposes a CRF based context module for CNN based object detectors. Every box proposed by the RPN is a node in the CRF, and its label is the classification label. Experiments are performed on the MS COCO object detection task. The presentation of the paper is fine in general. The experimental results are also a bit thin.<BRK>Although the state-of-the-art object detection methods are successful in detecting and classifying objects by leveraging deep convolutional neural networks (CNNs), these methods overlook the semantic context which implies the probabilities that different classes of objects occur jointly. In particular, conCNN features a context-aware module that naturally models the mean-field inference method for CRF using a stack of common CNN operations.
Reject. rating score: 3. rating score: 3. rating score: 6. In this paper, the authors study the problem of adding residual connection to GNN for node classification. In the theoretical analysis, the assumption that the FC layer is identical mapping is too simplistic. The analysis differs from the actual model especially when the residual links are considered in equation (8), where we have a sum of FC layer output and residual connection.<BRK>Summary:This paper studies the “suspended animation limit” of various GNNs – an important one for how to train a good Graph network. Under the assumption, the authors propose several new forms of ResNets for GCNs, which can successfully overcome the limitation. This paper is generally well written and easy to understand.<BRK>The paper studies the causes of the empirically poor performance in deep structures that plagues existing GNNs, and identify the suspended animation problem as the main issue. In analogy to the Residual CNN network, a residual graph network is proposed to address such issue.<BRK>In this paper, they further identify the suspended animation problem with the existing GNNs. To resolve the problem, they introduce the GRESNET (Graph Residual Network) framework in this paper, which creates extensively connected highways to involve nodes ’ raw features or intermediate representations throughout the graph for all the model layers.
Reject. rating score: 3. rating score: 6. rating score: 6. The paper idea of this paper is straightforward and clear: treat the irregular time series as a bag of events, augment them with time information using positional encoding, and process the events in parallel. The proposed idea in this paper can be considered a simplified version of the Transformers. Realizing the relationship with the Transformers not only decreases the novelty degree for this paper but also requires the authors to include the Transformers in the baselines. Finally, the results reported in the experiments are nice, especially for the baseline GRU D!<BRK>Summary:The work is focused on classification of irregularly sampled and unaligned multi modal time series. This paper approaches the problem as a set function mapping between the time series tuples to the class label. is it measurement of specific modality at a specific time instance? The performance mostly appears comparable across baselines but the proposed method has much better run times. The paper is for the most part well written, and related work well characterized. The formulation is interesting and clinically relevant as well so the choice of data sets makes some sense. This is reflected in the fact that experimental results are not drastically better than other baselines. I have a few concerns about the architecture formulation and lack of clarification and intuition in what appears to be the main contribution of the paper (Sec 3.2 and 3.3) which I will detail below:a. I do encourage the authors to try non clinical datasets for a comparison3. d. It would be useful to provide how exactly a label is inferred for a *new* test instance. Or even generally? That is parenthesis are missing?<BRK>This paper considers the problem of supervised classification of time series data that are irregularly sampled and asynchronous, with a special focus on the healthcare applications in the experiments. Together with a positional embedding of the timestamps and an attention based aggregation, the paper reports improved performance of the proposed approach on a few healthcare time series with asynchronous and irregularly sampled data. The idea of SEFT is novel and the results are also showing its promise. In addition, the interpretability shown in section 4.3 is also attractive. But this loses the information of the order of the time series, which can be extremely important in those datasets with long history dependence. Otherwise, it would be not clear whether this set modeling is generally applicable for general time series data. 1) The discussion about complexity (order m and m\log m) at the bottom of page 1 is weird   what does this complexity refer to?<BRK>This paper proposes a novel framework for classifying irregularly sampled time series with unaligned measurements, focusing on high scalability and data efficiency. Their method SeFT (Set Functions for Time Series) is based on recent advances in differentiable set function learning, extremely parallelizable, and scales well to very large datasets and online monitoring scenarios. They extensively compare their method to competitors on multiple healthcare time series datasets and show that it performs competitively whilst significantly reducing runtime.
Reject. rating score: 3. rating score: 6. rating score: 6. [Summary]This paper studies the convergence of Q Learning when a wide multi layer network in the Neural Tangent Kernel (NTK) regime is used as the function approximator. Concretely, it shows that Q learning converges with rate O(1/T) with data sampled from a single trajectory (non i.i.d.) [Cons]The result in this paper seems more or less like a direct combination of existing techniques, and thus may be limited in bringing in new techniques / messages. Key technical bottlenecks that are assumed out in prior work are still assumed out in this paper with potentially different forms but essentially the same thing. But still I tend to think the above adaptations are rather straightforward and technically not quite novel. [Potential improvements]I would like to hear more from the authors about the technical novelty in this paper, specifically how Lemma 6.1   6.3 compare with prior work.<BRK>When the neural function is sufficiently over parameterized, the O(1/T) convergence rate is attained. This is an important but difficult task. Cons: In spite of its theoretical contributions, this paper has a few major issues. It would be of practical interests to seek other proof techniques to avoid such projection step. 2.Assumption 5.3 is problematic for the considered neural Q learning setting. Moreover, it is unclear how to verify this condition in practice. A typically practical observation is that a larger $L$ is better.<BRK>[Cons]+ The novelty is a bit unclear other than the non iid assumption. We note that modern Q learning tends to use batching so doesn t require much of an iid assumption anyways, but this allows for more robust proofs in TD settings with non iid training. + The paper was a bit dense and hard to follow, we suggest reducing p.8 to have more discussion with references to proofs in the Appendix as in Chen2019. + As the authors admit in open commentary, there is a mistake to be fixed which needs to be reviewed before acceptance. I think there is value to this work, however, would require seeing the change to assess a revision.<BRK>Despite its empirical success, the non-asymptotic convergence rate of neural Q-learning remains virtually unknown. In this paper, they present a finite-time analysis of a neural Q-learning algorithm, where the data are generated from a Markov decision process and the action-value function is approximated by a deep ReLU neural network.
Reject. rating score: 3. rating score: 3. rating score: 6. rating score: 8. Summary:This paper proposes the use of a controller that selects whether to act according to a policy trained to maximize an intrinsic reward or a different policy trained to maximize the extrinsic reward of a task. The two policies are trained jointly and off policy. In your case, the control policy is one of the subpolicies and the other subpolicy is only used for exploration. It would be interesting to see how the results change as N varies. Is there any guarantee that the arbitrary feature embeddings (which are not learned in your experiments if I understood correctly) and thus the successor features (SFs) will contain meaningful information about the kinds of states a policy will visit in the future? It is only on  carefully designed tasks (e.g.FlytrapEscape or AppleDistractions) that are not regularly used as benchmarks that the method seems to perform better. The experiments section could be improved by including other (more powerful) baselines such as count/pseudocount exploration methods which have been shown to be more effective than ICM / RND for certain benchmarks, the paper using an intrinsic reward based on successor representations [3] or even Go Explore [4] that is specifically designed to deal with distractor objects for the AppleDistractions task. Additionally, evaluating SID on harder exploration tasks that are generally considered to be good benchmarks by the community would be helpful (e.g.Montezuma Revenge, Pitfall, sparser versions of DoomMyWayHome etc.) would also strengthen the experimental section. Is that the total number of updates used for the control policy, the exploration policy, and the successor features or is it only the number of updates used for the control policy?<BRK>## SummaryThis paper proposes a novel intrinsic reward for exploration called SFC (successor feature control), to deal with sparse reward and hard exploration task. The main idea of SFC is to provide an agent with intrinsic reward defined to be the L2 distance between the successor features of two consecutive states (Equation 4). Empirically, the SFC+SID algorithm is evaluated on custom sparse reward navigation type environments such as VizDoom and DeepMind Lab (as well as a simple pixel based continuous control), and outperforms other intrinsically motivated RL algorithms including RND and ICM. The resulting method presents an improvement over existing intrinsic reward exploration algorithms. To demonstrate that SFC+SID is "generally useful" as claimed in the paper, presenting benchmark results on standard discrete action Atari environments, or more diverse RL environments would have greatly strengthened the paper to be more convincing. However, there are some parts that can be better clarified and improved more. The ablation study (appendix 1) is interesting and very important. It is not very clear to me why the SFC reward agrees with bottleneck states. In SID, a policy for extrinsic rewards and another policy for intrinsic one are learned.<BRK>Summary: A very nice study on the benefits of successor feature control as an intrinsic drive for hard exploration problems. The work builds nicely on previous work on using SF for exploration and proposes using derived (reachability under $\psi^{\pi}$) distances  as intrinsic motivation for an (purely) exploratory policy. This exploratory strategy will be used in conjunction with a policy trained on the extrinsic reward to gather (off policy) data for both learning processes. I would suggest moving that into the main paper, as it nice separate the influence of scheduling component and the  quality  of the proposed intrinsic reward. It maybe worth exploring learning those to capture more interesting features of the task at hand, especially in situation were there is more signal in the extrinsic reward. 3) On the SID setup.<BRK>It has two main technical contributions. This uses the same training setup as prior work, Scheduled Auxiliary Control (SAC), except here the extra policy is trained on intrinsic reward rather than an auxiliary task. Since successor features encompass a notion of which kinds of states the agent will encounter in the future after starting from the current state, this type of intrinsic reward is more far sighted than most state of the art approaches. Empirical analysis shows that SFC leads agents to explore bottleneck states, which is especially helpful for solving navigation tasks. This paper is well motivated and clearly written. The experimental evaluation of this paper is thorough, comparing SID to adding extrinsic and intrinsic reward together, and comparing SFC to two recent approaches for generating intrinsic rewards, ICM and RND. My only concerns with the paper have to do with evaluation. It would be nice to see a wider variety of evaluation domains, for instance Montezuma s Revenge, which is frequently used to evaluate algorithms for computing intrinsic rewards, as well as other methods for improving exploration of RL agents. It would be neat if agents trained using SFC are better able to navigate through the doors in this game, since that seems to be a clear example of bottlenecks.<BRK>Commonly these signals are added as bonus rewards, which results in a mixture policy that neither conducts exploration nor task fulfillment resolutely. In this paper, they instead learn separate intrinsic and extrinsic task policies and schedule between these different drives to accelerate exploration and stabilize learning. Moreover, they introduce a new type of intrinsic reward denoted as successor feature control (SFC), which is general and not task-specific. They evaluate their proposed scheduled intrinsic drive (SID) agent using three different environments with pure visual inputs: VizDoom, DeepMind Lab and DeepMind Control Suite. A video of their experimental results can be found at https: //gofile.io/? c=HpEwTd.
Reject. rating score: 1. rating score: 1. rating score: 1. This paper proposes to use neural networks to evaluate the mathematical expressions by designing 8 small building blocks for 8 fundamental operations, e.g., addition, subtraction, etc. The motivation of this paper is not very clear to me, i.e., why do you want to mimic the arithmetic operations using the logic networks, what is the real use case here. And the writing needs to be significantly improved.<BRK>The paper proposes a method to design a NN based mathematical expression evaluation engine. I am afraid the ML contribution of this work is in my opinion almost non existent. If this would be true in general we would have not seen such a resurgence and widespread use of ANN in the past years.<BRK>The introduction is poorly written that I cannot get a full picture of what goals this paper tries/has achieved after reading it. A new idea the paper proposes is to train a few networks to first learn/fit basic operations, and then use these trained NNs to assemble large NNs which are supposed to form more complex arithmetic operations.<BRK>They propose a solution for evaluation of mathematical expression. In this work they first identify 8 fundamental operations that are commonly used to solve arithmetic operations (such as 1 digit multiplication, addition, subtraction, sign calculator etc). These fundamental operations are then learned using simple feed forward neural networks. They then shows that different operations can be designed simply by reusing these smaller networks.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. Summary: This paper proposes a novel metric "discrepancy ratio" for evaluating the performance of a model where the ground truth for each data point comes from many expert yet imperfect annotators. The authors suggested that this metric can be used for many applications. Other comments:On the limitation:I am satisfied that the authors clearly discuss the limitations of the proposed method and I agree with the discussion. Majority vote cannot evaluate that the model is better than human average performance, which we want to know. On the other hand, I think there are several parts that can be shortened in the main body. 7.I think we should not resize the figure/table caption. Update: I would like to thank the authors for the clarification. I have read the rebuttal.<BRK>The authors presented an interesting paper which tries to solve a practically important question. Therefore, it is important to have a metric that can distinguish the performance of models with noisy labels. In this paper, the authors proposed to measure the model performance based on the ratio of the discrepancy between the model prediction and the labeler, with the discrepancy between the labelers. The authors demonstrated the performance of their proposal in synthetic data as well in two real world medical image datasets. The numerical experiments are well conducted, but I am not totally convinced by their results. Numerical Experiments: This is my biggest complain. Is there any reason that the discrepancy ratio is superior to the majority vote?<BRK>This paper proposed to evaluate model performance when the ground truth labels were not available and noisy labels provided by multiple uncertain experts were provided instead. The proposed evaluation metric, called discrepancy ratio, is defined as the ratio between the average model annotator discrepancy and the average annotator annotator discrepancy. It can be applied to compare 1) the relative performance of different models; and 2) the relative performance of average annotators and the model. Therefore, the discrepancy ratio didn t overcome those "drawbacks" associated with these two baselines, correct? This is an undesirable behavior since it over estimate the performance gap in the low noise regime. In section 4.1.4, the claim that "It is in this regime that the model performance can be said to be better than the average human performance" seems questionable. Third, different experts should have similar expertise. It s more likely to observe multiple groups of experts, where experts in the same group are similar. Could the author provide some ideas on how to relax some of these assumptions?<BRK>However, this luxury is often not afforded to many high-stakes, real-world scenarios such as medical image interpretation, where even expert human annotators typically exhibit very high levels of disagreement with one another. While prior works have focused on overcoming noisy labels during training, the question of how to evaluate models when annotators disagree about ground truth has remained largely unexplored. Conceptually, their approach evaluates a model by comparing its predictions to those of human annotators, taking into account the degree to which annotators disagree with one another. Finally, they demonstrate how this framework can be used effectively to validate machine learning models using two real-world tasks from medical imaging. The discrepancy ratio metric reveals what conventional metrics do not: that their models not only vastly exceed the average human performance, but even exceed the performance of the best human experts in their datasets.
Reject. rating score: 3. rating score: 3. rating score: 3. My primary concern is that the paper is entirely empirical with little if any justification of the results. In addition, it is based on a single architecture and a single dataset. This would have been fine if the results were supported with explanation or theoretical justification. These are not necessarily equivalent. In fact, I think the results support this conclusion (see for example Figure 10). The goal is to help improve the speed of neural architecture search. So, please mention clearly what you recommend in the conclusion section. #post rebuttal remarksThanks for the response.<BRK>I recommend a weak rejection for this paper. In this paper, the authors take a small NAS search space (64 possible networks) and train each network separately to obtain their individual rankings. The topic is interesting, but I haven t been convinced through the limited scope of the experiments, or the arguments made what the real point is. I have two major issues with this paper however. Firstly, the scope is limited; everything is based on looking at 64 convnets trained on CIFAR 10, this makes it tricky to make any broad statements about weight sharing in NAS.<BRK>This paper studies weight sharing in neural architecture search (NAS). It constructs a mini search space with 64 possible choices, and performs various comparisons and studies in an exhaustive way. Some of the observations are quite interesting, exploring the limitations of weight sharing. My biggest concern is the limited search space. Unlike other NAS works that usually have search space size > 10^10, this paper focuses on a very small search space (64 options in total).<BRK>In this paper, they conduct comprehensive experiments to reveal the impact of weight-sharing: (1) The best-performing models from different runs or even from consecutive epochs within the same run have significant variance; (2) Even with high variance, they can extract valuable information from training the super-net with shared weights; (3) The interference between child models is a main factor that induces high variance; (4) Properly reducing the degree of weight sharing could effectively reduce variance and improve performance.
Reject. rating score: 3. rating score: 3. rating score: 3. The authors focus on quantizing the MobileNets architecture to ternary values, resulting in less space and compute. I think this research is quite incremental over MobileNets and is unlikely to spur further research strains. I think a better venue for this research may be a more systems focused conference or journal. There is a significant amount of compute and training complexity required to reduce the model size, e.g.versus model pruning or tensor decomposition.<BRK>Specifically, the paper proposes a layer wise hybrid filter banks which only quantizes a fraction of convolutional filters to ternary values while remaining the rest as full precision filters. This paper is generally well written with good clarity. But it is still an interesting contribution. For this kind of paper, I would like to see a more complete set of empirical results. However, The experiments only perform comparison on ImageNet dataset. 2.The proposed method is only designed for MobileNets.<BRK>The paper presents a quantization method that generates per layer hybrid filter banks consisting of full precision and ternary weight filters for MobileNets.<BRK>Under the key observation that convolutional filters at each layer of a deep neural network may respond differently to ternary quantization, they propose a novel quantization method that generates per-layer hybrid filter banks consisting of full-precision and ternary weight filters for MobileNets.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. What is the paper about ? *The paper proposes a simple method to speed up active learning/core set selection in deep learning framework. What I like about this paper ? While the authors may be the first ones to apply this idea in the context of deep learning, they themselves note that the idea of using a smaller proxy like naive bayes for larger models like decision trees is not new. Same can be said for CIFAR10 and CIFAR100.<BRK>This paper presents a method to speed up the data selection in active learning and core set learning. This paper shows that simple, almost trivial techniques can lead to significant runtime benefits for active learning and core set learning. The correlation values in Figure 3 are quite diverse. 3.Table 1 is very hard to interpret. What would be the effect of using the proxy network as the main network as well?<BRK>The main idea is to train a proxy model, a smaller version of the full neural network, to choose important data points for active learning or core set selection. This paper is well written and was easy to follow, with a clear motivation. I could not find this number in the paper. The paper does good job of demonstrating that the proposed algorithm is effective through a comprehensive set of experiments.<BRK>However, they can be prohibitively expensive to apply in deep learning because they depend on feature representations that need to be learned. In this work, they show that they can greatly improve the computational efficiency by using a small proxy model to perform data selection (e.g., selecting data points to label for active learning). For core-set selection on CIFAR10, proxies that are over 10× faster to train than their larger, more accurate targets can remove up to 50% of the data without harming the final accuracy of the target, leading to a 1.6× end-to-end training time improvement.
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. This paper addresses the following: how do batch normalization and dropout affect the number of linear regions present in a deep network? The linear region is defined as the region of input space that activates the same units/nodes in the network. Comments  This paper enumerates a number of interesting findings, all of which seem to raise intriguing questions about the properties of trained networks.<BRK>Because the paper relies on geometrical reasoning, I wished there would be more visualisations that guide the reader. In their presentation, the authors focus on comparing these properties between models that were either trained without regularisation, with batch normalisation, or with dropout and with different learning rates. This paper aims to give new insights into deep neural networks by presenting a number of approaches to analyse the linear regions in such networks. The authors claim that inspheres of linear regions are highly relate to the expressivity of DNN.<BRK>This work presents an array of analytical tools to characterize linear regions of deep neural networks (DNNs). The paper indeed presents a number properties for analyzing the nature of linear regions in DNNs; however it falls short of connecting them with an improvement in the optimization or interpretability of DNNs. Moreover, how would the figures look if we were using a different objective, dataset or architecture? The paper is clearly written and is easy to follow for the most part.<BRK>A deep neural networks (DNN) with piecewise linear activations can partition the input space into numerous small linear regions, where different linear functions are fitted. It is believed that the number of these regions represents the expressivity of a DNN.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. Summary:The authors propose a method to perform continual learning with neural networks by incorporating variational Gaussian Processes as a top layer (also called Deep Kernel Learning) and constructing an objective utilizing the inducing inputs and outputs to memorize across tasks. Some discussion on the objective might be warranted to demonstrate that it actually lower bounds the true LLK. The mathematical formulation of the basic model is very elegant. 2.The paper is well written overall.<BRK>The authors propose a function space based approach to continual learning problems (CL), wherein a learned embedding    $\hat{\mathbf{x}}   \text{NN}(\mathbf{x}; \theta)$is shared between task specific GPs s.t. Additionally, a novel approach for automatically detect task switching is introduced that exploits the Bayesian aspects of the proposed framework. The provided experiments seem reasonable and do a good job highlighting different facets of the paper.<BRK>The paper develops a continual learning method based on Gaussian Processes (GPs) applied in the way introduced by prior work as Deep Kernel Learning (DKL). Employing inducing point training for task memorization is a novel and interesting idea, which could be useful for the continual learning community. Is it a neural net loss or an ELBO? The fact is that the conceptual novelty of the paper is too slim compared to VCL.<BRK>They introduce a framework for Continual Learning (CL) based on Bayesian inference over the function space rather than the parameters of a deep neural network. Then, the training algorithm sequentially encounters tasks and constructs posterior beliefs over the task-specific functions by using inducing point sparse Gaussian process methods.
Reject. rating score: 3. rating score: 3. rating score: 6. Summary:The paper proposes a method for improving the scalability of communication based cooperative multi agent reinforcement learning. Questions to the Authors:1. The motivation and the impact of the contributions are not very clear. What does this mean?<BRK>This paper proposes a method of learning a hierarchical communication graph for improving collaborative multi agent reinforcement learning, particularly with large numbers of agents. The initial results presented seem promising, but further work is needed to ensure the results are reproducible and repeatable. I note the following issues as simple changes that can be made to improve the readability of the paper:  In the abstract, the sentence "but also communication high qualitative" does not parse.<BRK>Their framework uses a Structured Communication Network Module and Communication based Policy Module. These use a hierarchical decomposition of the multi agent system and a graph neural network that operates over the resulting abstract agent (groups). The authors evaluate on two environments, where this approach outperforms other ways to communication protocols. 4.Additional feedback with the aim to improve the paper. Make it clear that these points are here to help, and not necessarily part of your decision assessment.<BRK>In particular, most of the existing algorithms suffer from issues such as scalability and high communication complexity, in the sense that when the agent population is large, it can be difﬁcult to extract effective information for high-performance MARL. In contrast, the proposed algorithmic framework, termed Learning Structured Communication (LSC), is not only scalable but also communication high-qualitative (learning efﬁcient).
Reject. rating score: 1. rating score: 3. rating score: 6. The paper at hand argues that shallow feature extraction networks should be favored for the computer vision task of stereo matching, rather than the commonly used deep ResNet backbones. To that end, a model is proposed consisting of three convolutional feature extraction layers only. I found this paper lacking in terms of contributions. While the motivation for retaining detailed feature information makes sense for the task in question, it is not clear why it requires replacing the ResNet feature extraction with a very shallow network; an alternative would be to add skip connections originating from lower levels. Overall, I think that this paper should be rejected on the basis of insufficient contributions.<BRK>The paper shows that this leads to a major reduction in the number of model parameters (42% in one case) with comparable performance on the KITTI2015 data set. The paper could be improved by providing more this kind of analysis and by adding more motivate for why low level features are more important for stereo matching. The related work section appears to be just a laundry list of methods. The paper would be stronger if the authors provided more interpretation of the strengths and weaknesses of these methods, some insight into why they work, and why the proposed method is better. Explain what it different. In most cases, the authors  method was not the best.<BRK>This is a paper about a rather specialized area of computer vision (stereo matching),and it s not really a theoretical paper; it s about an improved network topology fora very specific task. My feeling is that this belongs in a computer vision conference, where people would be betterable to appreciate it.<BRK>Stereo matching is one of the important basic tasks in the computer vision field. Existing algorithms generally use deep convolutional neural networks (DCNNs) to extract more abstract semantic information, but they believe that the detailed information of the spatial structure is more important for stereo matching tasks. Based on this point of view, this paper proposes a shallow feature extraction network with a large receptive field. The primary feature extraction network contains only three convolution layers. In addition, a feature fusion module is designed, which integrates the feature maps with multiscale receptive fields and mutually complements the feature information of different scales.
Reject. rating score: 1. rating score: 3. rating score: 3. The authors present an empirical study to evaluate the performance of CNN based object classifiers for situations in which the object of interest is very small relative to the size of the image. Two artificial datasets, based on MNIST and histopathological images are introduced to conduct the experiments. vi) There is no methodological contribution.<BRK>The submission proposes an analysis of the impact of object size in images when performing classification tasks using neural networks of the BagNet family. The paper attack interesting questions and links the size of the object in the image (O2I) to the training dataset size required. Also, showing that max pooling is the only pooling operation that converges for very low O2I (but is the slowest to converge at higher O2I) is interesting and encourages discussion about the training (optimization) process. In my opinion, the main issue about the submission is the limited depth in the contributions, analyzing a single family of network architectures (BagNets) over two datasets, one of which is relatively small. The family of R CNN and its derivatives were especially designed to counter the impact of object size, it would have been interesting to include them in the analysis. Furthermore, limited insights can be carried out for tasks related to classification such as localization and segmentation. I am not sure about this remark, I would have expected the network to learn to focus on the right regions, provided the receptive field is big enough to see the whole object of interest. Could the decrease in performance be attributed to the increased amount of learnable parameters that ended up too large for the “relatively small nCAMELYON dataset used for training”?<BRK>This paper presents a testbed framework to investigate the limitations of CNN at the classification of tiny objects and the effects of signal to noise ratio has in the task. Over all the experiments run, the behaviors observed on the two datasets are not the same. The explanation provided by the authors is that since nCAMELYON is very small, results are different from the ones of nMNIST. While I consider this a valid explanation, this still limits the overall conclusions of this paper. Therefore, my main recommendation to the authors would be to identify other datasets for their experiment. Using the example of the paper, I would argue that it is possible to obtain images of outdoors with people where there are no balls.<BRK>In some important computer vision domains, such as medical or hyperspectral imaging, they care about the classification of tiny objects in large images. However, most Convolutional Neural Networks (CNNs) for image classification were developed using biased datasets that contain large objects, in mostly central image positions. To assess whether classical CNN architectures work well for tiny object classification they build a comprehensive testbed containing two datasets: one derived from MNIST digits and one from histopathology images. This testbed allows controlled experiments to stress-test CNN architectures with a broad spectrum of signal-to-noise ratios. Their observations indicate that: (1) There exists a limit to signal-to-noise below which CNNs fail to generalize and that this limit is affected by dataset size - more data leading to better performances; however, the amount of training data required for the model to generalize scales rapidly with the inverse of the object-to-image ratio (2) in general, higher capacity models exhibit better generalization; (3) when knowing the approximate object sizes, adapting receptive field is beneficial; and (4) for very small signal-to-noise ratio the choice of global pooling operation affects optimization, whereas for relatively large signal-to-noise values, all tested global pooling operations exhibit similar performance.
Reject. rating score: 3. rating score: 3. rating score: 3. The paper didn t motivate properly the use of a hardware agnostic metric in the context of the quantization and pruning. They first show empirically that this Evaluation metric is correlated with the validation accuracy. Which equation for the ESN was used to produce figure 5? Cons:   The paper is not very clear, and the structure is somehow confusing.<BRK>This paper proposes an effective signal norm metric that measures the cost of the neural networks under both compute(ESN_a) or memory(ESN_d) in an ideal hardware setting. The authors then show that the slimmer models with fewer parameters are better than fatter models. Finally given that there are already quite a few methods that prunes model based on real hardware evaluations, it would be great to compare to these methods as well. WeaknessThe main drawback of the paper is the lack of novelty in proposed method.<BRK>The authors propose a hardware agnostic metric called effective signal norm (ESN) to measure the computational cost of convolutional neural networks. What’s more, based on the metric, the authors demonstrate that models with fewer parameters achieve far better accuracy after quantization. Therefore, ESN is not suitable for existing hardware. What’s more, the assumptions are hard to be proved.<BRK>This is because the benefits of pruning and quantization strongly depend on the hardware architecture where the model is executed. These findings are available not only to improve the Pareto frontier for accuracy vs. computational cost, but also give us some new insights on deep neural network. Therefore, in this paper, they first propose hardware-agnostic metric to measure the computational cost.
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. This paper presents a new simpler routing mechanism for capsule networks and achieves good performance on real world data sets making use of this new capsule structure along with a restnet backbone. The paper could be improved by clearing up a few ambiguities:  is the learning rate schedule the same for all three models?<BRK>In this paper, the authors propose a simple and effective routing algorithm for capsule networks. The paper is well written. A nice analysis of the proposed routing algorithm is provided. Here are some issues:1. Would the authors release the code for reproducing the results in the paper?<BRK>Authors improve upon dynamic routing between capsules by removing the squash function (norm normalization) and apply a layerNorm normalization instead. They report results on Cifar10 and Cifar100 and achieve similar to CNN (resnet) performance. In dynamic routing the dot product with the next layer capsule is calculated and then normalized over all next layer capsules. There is no "reconstructing the layer bellow" in Dynamic routing as authors suggest in intro. Did authors tune learning rate schedule individually? rebuttalThank you for your response. I acknowledged the novel contributions of this work. My comment was that some claims in the paper are not right. i.e."inverted dot product attention" is not new and "reconstructing the layer bellow" does not happen in Sabour et al .Parallel execution + layer norm definitely is novel and significant.<BRK>They introduce a new routing algorithm for capsule networks, in which a child capsule is routed to a parent based only on agreement between the parent's state and the child's vote. The new mechanism 1) designs routing via inverted dot-product attention; 2) imposes Layer Normalization as normalization; and 3) replaces sequential iterative routing with concurrent iterative routing. On a different task of recognizing digits from overlayed digit images, the proposed capsule model performs favorably against CNNs given the same number of layers and neurons per layer.
Accept (Poster). rating score: 8. rating score: 8. rating score: 3. Algorithms for Streaming data using a machine learning oracle is analyzed theoretically and empirically. The idea is to build on some recent work (Hsu 19) which used RNNs to predict heavy hitters in streaming data. The purpose of this paper is to analyze whether such an oracle can help streaming algorithms to obtain improved bounds. I am not very familiar with this line of research so my comments will be more general in this case. Experiments are performed on real as well as synthetic datasets using Hsu et al.’s method as an oracle. I am not sure if this is the first work in this direction of proving bounds assuming an oracle or if there is some background work that the authors could provide to put this into context.<BRK>A lot of work has been done in this general area and on the problems that are discussed in the paper. The new idea in the paper is better streaming algorithms under the assumption that there is a “heavy hitters” oracle that returns data items that have a lot of representation in the stream. This also shows the power of such an oracle. At a high level the work seems good and interesting for a large audience interested in streaming data analysis. I have not gone over the proofs in detail (much of which is in the appendix).<BRK>The paper presents algorithms for solving computational problems in a datastream model augmented with an oracle learned from data. It reads very much like a STOC theory paper, and a lot of the key ML details that would be relevant to audience at this conference seem to have been shoved under the rug in a way. Therefore my score for now is a weak reject, but I am very happy to increase the score if the authors address my presentations concerns. What are the particular assumptions under which it exists? What are the requirements on the training data, optimization ability, generalization error, etc. What algorithms should we ideally use in practice? However, you are in a different computational model in which you now have access to an oracle. This needs to be made more explicitly, and language could be a bit toned down (e.g.in this model, we can obtain runtime that match or improve over lower bounds...)<BRK>The data stream model is a fundamental model for processing massive data sets with limited memory and fast processing time. Such techniques were encapsulated by training an oracle to predict item frequencies in the streaming model. In this paper they explore the full power of such an oracle, showing that it can be applied to a wide array of problems in data streams, sometimes resulting in the first optimal bounds for such problems.
Accept (Spotlight). rating score: 8. rating score: 6. rating score: 6. The paper uses a new approach for inductive matrix completion (IMC). The approach (of using subgraph connectivity patterns as features) is clever, novel (at least in my knowledge) and neatly sidesteps the need for extra metadata/features.<BRK>This paper presents an inductive matrix completion model using graph neural networks. It claims to be an inductive model and don t need any side information as it only uses the surrounding sub graph structure to give predictions. 5.I have a big concern for the scalability.<BRK>This paper presents a method for inductive matrix completion that does not rely on side information to make predictions. Increased rating to Weak Accept following the rebuttal below.<BRK>They propose an inductive matrix completion model without using side information. Under the extreme setting where not any side information is available other than the matrix to complete, can they still learn an inductive matrix completion model? In this paper, they propose an Inductive Graph-based Matrix Completion (IGMC) model to address this problem.
Reject. rating score: 1. rating score: 6. rating score: 6. Original review  This paper proposes to generate semantic preserving adversarial examples by first learning a manifold and then perturbing data along the manifold. I have many concerns for this paper:  The approach is not well motivated. Many choices in the algorithm seem to be arbitrary, and there are many approximations in the method whose accuracies have no guarantees. In Equation (6) the authors hard constrain the generated adversarial example such that they cannot differ from the original data by some pre specified l_2 norm. The authors compare their approach to other attacking methods on the success rates of attacking Madry s model and Kolter & Wong s certified model.<BRK>I hope there are other reviewers that are more qualified than I am to check the specifics of the methods. This paper presents a new technique for generating adversarial examples, by first learning the data manifold in an embedding space, then finding an adversarial example that lies on the manifold. For (2), most of the images do indeed look like they should retain their human labels, which is good (but also not hard for adversarial images). Almost all of the textual examples, however, have correct predictions from the model after the adversarial change to the input. This is why semantics preserving attacks are so hard in NLP, and I don t think that this method has accomplished its goal here at all, at least for text. EDIT 11/14: The authors  revisions have satisfied my concerns about how the NLP attacks are described.<BRK>The paper presents an approach to generating adversarial examples that preserve the semantics of the input examples. I have very few comments on the paper which does not present any important lack in my opinion. The presented approach has been tested on toy examples regarding images (both numbers from MNIST or SVHN and images from CelebA datasets) and texts (SNLI dataset). The results show that the manifold shape is preserved while creating perturbed elements.<BRK>While progress has been made in crafting visually imperceptible adversarial examples, constructing semantically meaningful ones remains a challenge. In this paper, they propose a framework to generate semantics preserving adversarial examples. First, they present a manifold learning method to capture the semantics of the inputs. Then, they perturb the elements of the learned manifold using the Gram-Schmidt process to induce the perturbed elements to remain in the manifold. They apply their approach on toy data, images and text, and show its effectiveness in producing semantics preserving adversarial examples which evade existing defenses against adversarial attacks.
Reject. rating score: 1. rating score: 1. rating score: 3. The paper proposes a neural architecture for summarizing trees inspired by capsule networks from computer vision. The paper applies the proposed architecture to three different program classification datasets, which are in three different languages. The paper reports empirical gains compared to two architectures proposed by previous work. I think that it s interesting to apply the capsule network architecture to tree classification, but unfortunately it doesn t appear that some of the motivation for capsule networks on images didn t seem to transfer neatly to this setting; for example, there is no equivalent of inverse graphics as there is no reconstruction loss (as pointed out by the authors in Section 6.4). My biggest concern has to do with the empirical results. For future submissions, it would be good to see a more comprehensive empirical comparison of the proposed method compared to others, and also to have more explanations about the design of the network.<BRK>The paper proposes a capsule network based architecture for predicting program properties and is evaluated on three tasks for predicting an algorithm from a code snippet. To do this, two dimensions describing the position of a node in a tree position are used   the depth of a node in a tree and its index in the list of children of its parent. This choice, however, is similar to image convolutions only at a very artificial level and drops significant amount of semantically interesting information for programs from the index of the node at the parents, while keeping the total depth (which rarely matters in programs, as code is usually semantically similar no matter how nested in other code it is).<BRK>This paper proposes a tree structured capsule network for program source code processing (essentially a program classification task with three datasets). The authors follow the cliché of the importance of tree structures, but show little insight into the use of capsule networks in program analysis. The experiments are very thin. The authors only compare their results to TreeCNN and Gated  Graph NN (GGNN). I am not excited either.<BRK>Existing studies on tree-based convolutional neural networks (TBCNN) and gated graph neural networks (GGNN) are not able to capture essential semantic dependencies among code elements accurately. In this paper, they propose novel tree-based capsule networks (TreeCaps) and relevant techniques for processing program code in an automated way that encodes code syntactical structures and captures code dependencies more accurately. Based on evaluation on programs written in different programming languages, they show that their TreeCaps-based approach can outperform other approaches in classifying the functionalities of many programs.
Reject. rating score: 3. rating score: 3. rating score: 6. rating score: 6. [3] Li and Talwalker, Random Search and Reproducibility for Neural Architecture Search, UAI 19. Could the author also comment on that? While interesting, the experiments show this approach is just as good as some simple human tricks like early stopping. About experiments  Could the author provide an additional experiment, to show in reality, even for a toy example, this gradient trap make DARTS converge to an architecture full of skip connection, i.e., showing the gradient estimate (i.e., g1+g2) w.r.t.\alpha, obtained by original DARTS, proposed approach, if possible, ground truth estimate, after a certain epoch number (e.g.50 or any number that DARTS start turning to skip connection)? This is a subjective opinion, and my subjective opinion is it is wrong. As stated in guideline, we should apply a higher standard to a 10 page paper, and unfortunately, in my opinion, this paper does not meet such standard with the current version, and heavily revision will be necessary. I encourage the authors to shorten their paper by putting some of the derivations to the appendix and to better show their contribution. If the new results are still statistically significant, it will be strong evidence that the proposed algorithm indeed improves original DARTS. However, I have some doubts regarding both theoretical and empirical aspects of this paper. The evidences in the current version is not sufficient to support all the claims. Could you comment on the theoretical bound between the proposed amendment and the original DARTS estimation? Adding this will further strengthen the paper.<BRK>The paper proposed to amend the 2nd order formulation of DARTS for improved stability. This is an interesting method, though the technique itself is not new outside the sub field of architecture search. Some major concerns:* The authors correctly pointed out that the original 2nd order approximation in DARTS is insufficient to make an accurate gradient direction. * According to the authors, an advantage of the proposed approach is that one does not have to rely on early stopping rules which require human expertise and “violates the fundamental ideology of AutoML” (Section 3.2). * I m also a bit concerned about the similar empirical performance but longer search time when comparing with other DARTS variants in Table 1 (using search space S1). However, the proposed approximation technique is rather a heuristic and only addresses the issue at a superficial level.<BRK>The inclusion of experiments that Reviewer1 suggested definitely make the contributions stronger. I believe the paper will be substantially stronger with a careful study of where the empirical improvement is coming from. The theory (that the approximate gradient has an acute angle with the desired gradient) is potentially vulnerable   this property is certainly desirable when stochastic optimizing convex functions (with appropriate step sizes) but it s not trivial that it gives good behavior for optimizing non convex functions like NAS. The paper studies differentiable approaches to neural architecture search and convincingly points out that existing approximations to the gradient w.r.t.architecture parameters are problematic. A new approximation is proposed, and evaluated to show that degenerate architectures are not getting selected once search has converged (empirically) on standard image classification datasets. The problem with existing approximations (e.g.first order or second order DARTS) is explained clearly. is the empirical improvement indeed arising because of better gradients? Studying these two questions carefully via experiment will make the paper s contributions stronger. "has not been studied carefully in previous works"Related work: "exhausted search"  > "exhaustive search"<BRK>This paper offers through analysis on instability of NAS training that the system degenerates and reaches to trivial solutions, e.g., more skip connect operators, as training goes longer. Motivated by this issue, this paper proposes an approach to stabilize NAS training. Strength:[1] Theoretical analysis[2] The paper is well written[3] This paper is trying to solve a very interesting and important problemWeakness:[1] Lack of ablation study. [2]The proposed approach offers comparable results with SOTA (early stopping). I agree that it opens a direction of stable NAS training, but the contribution so far is limited. I expect to see quality gains due to improved training technology<BRK>Differentiable neural architecture search has been a popular methodology of exploring architectures for deep learning. This paper investigates DARTS, the currently most popular differentiable search algorithm, and points out an important factor of instability, which lies in its approximation on the gradients of architectural parameters. Based on this analysis, they propose an amending term for computing architectural gradients by making use of a direct property of the optimality of network parameter optimization. Their approach mathematically guarantees that gradient estimation follows a roughly correct direction, which leads the search stage to converge on reasonable architectures.
Reject. rating score: 3. rating score: 6. rating score: 6. Overall, I think that this is interesting work that can help to broaden the study of adversarial examples and make them more applicable even in non adversarial settings (e.g., by making models more robust to the changes in semantic attributes that the authors consider). There has been quite a bit of interest in the community in adversarial examples that are not just $L_p$ perturbations, and I believe that the authors  approach will encourage a good deal of follow up research. However, my main concern with the paper is that in my opinion, it does not sufficiently address why it is important to generate adversarial examples in the way they do. If that is the case, what is the attack model under which these attacks are realistic? For example, the original $L_\infty$ attacks are motivated in the sense that the adversarial examples are visually imperceptible, so they might not be noticed by the end user. What is the equivalent argument for these semantic attacks? For example, what about the following straw man baseline: use a controllable semantic attribute based generator to generate semantically different images without any notion of an adversarial attack, and then do standard $L_p$ attacks on that generated image? How would that be better or worse than the proposed method? 3) Or is the argument that it is just good to be able to generate examples that models get wrong? If so, why, and why is this method better than other methods? For this reason, my current assessment is a weak reject, though I d be open to changing this assessment. Less critical comments, no need to respond or fix right away  While the overall concept and approach was clear, I generally found the notation and mathematical exposition difficult to follow. These seem to be used somewhat interchangeably? c) I didn t understand equation 4.<BRK>Summary:This paper proposes to generate "unrestricted adversarial examples" via attribute conditional image editing. They conduct extensive experiments for several tasks compared with CW attack, showing broad applicability of the proposed method. The paper is well written and technically sound with concrete experimental results. I m glad to suggest accepting the paper. With the help of attribute conditional StarGAN, SemanticAdv generates adversarial examples by interpolating feature maps conditioned on attributes. They provide experiments showing the effectiveness of SemanticAdv; analysis on attributes, attack transferability, black box attack, and robustness against defenses; as well as user study with subjective. The qualitative results also look nice and the code base is open sourced. A question out of curiosity, the last conv layer in the generator is used as the feature map. How is the attack effectiveness of using other layers?<BRK>This paper proposes adversarial attacks by modifying semantic properties of the image. The most related work is Joshi 2019 and the authors show that the method used in that work (modification in attribute space) is inferior to modification in feature space still via attributes, as the authors proposed. However, I have a few comments and concerns:1) The authors mention on page 3 they assume M is an oracle  what is the impact of this? 2) The results in Table C don t look good  the proposed method can *at best* (in a generous setup) equal the results of CW  maybe I missed something but more discussion would be helpful. 3) Is there a way to evaluate the merits of semantic modification (beyond attack success) in addition to "does it look reasonable"? If attribute based attacks are better, is there a cost to this?<BRK>However, recent studies have shown that DNNs are vulnerable to adversarial examples which are manipulated instances targeting to mislead DNNs to make incorrect predictions. In this paper, they aim to explore the impact of semantic manipulation on DNNs predictions by manipulating the semantic attributes of images and generate “ unrestricted adversarial examples". Such semantic based perturbation is more practical compared with the Lp bounded perturbation. They conduct extensive experiments to show that the semantic based adversarial examples cannot only fool different learning tasks such as face verification and landmark detection, but also achieve high targeted attack success rate against real-world black-box services such as Azure face verification service based on transferability. Such adversarial examples with controlled semantic manipulation can shed light on further understanding about vulnerabilities of DNNs as well as potential defensive approaches.
Reject. rating score: 1. rating score: 1. rating score: 1. in this paper, the authors use stochastic shortest path MDP to model the attacker s planning problem.<BRK>This paper proposes to use reinforcement learning to model an agent that is reaching goal that it is intended to reach. The authors propose to then train an agent to modify the action space to make it difficult for an agent to fool the observer.<BRK>This paper aims to provide a goal recognition framework.<BRK>Goal recognition based on the observations of the behaviors collected online has been used to model some potential applications.
Reject. rating score: 3. rating score: 3. rating score: 8. This paper considers a task of symbolic regression (SR) when additional information called  asymptotic constraints  on the target expression is given. In the paper s setting, for SR with univariate groundtruth functions f(x),  asymptotic constraints  for x > infinity and x  > 0 are given. For this situation, the paper proposes a method called NG MCTS with an RNN based generator and MCTS guided by it to consider asymptotic constraints. Also, quantitative evaluations about extrapolative performance and detailed evaluation of the RNN generator are also reported. (2) the most important claim of this paper would be the proposal to use  asymptotic constraints , but the availability of this information sounds too strong and again artificial in practical situations.<BRK>If I do not misunderstand, it could be the cross entropy loss between the output of GRU and the next target production rule, RMSE and the error on leading power. The proposed NG MCTS is elegant to solve the problem of symbolic regression. 3.The authors of this paper introduce more information – leading power of desired function for symbolic regression but they incorporate the additional information by introducing a simple loss function term. How about the performance of baseline approaches with those kinds of information?<BRK>Summary:This paper introduces the use of asymptotic constraints to findprune the search space of mathematical expressions for symbolicregression. This neural network is then itself also used to guide aMCTS to generate mathematical expressions. The insight about using asymptoticconstraints makes the result a bit limited to only generatingmathematical expressions, and it would have been a bit nicer ifthere was something more generically applicable to programsynthesis in general. It s not really clear to me how theexisting work extends to programs.<BRK>Symbolic regression is a type of discrete optimization problem that involves searching expressions that fit given data points. In many cases, other mathematical constraints about the unknown expression not only provide more information beyond just values at some inputs, but also effectively constrain the search space. They identify the asymptotic constraints of leading polynomial powers as the function approaches 0 and infinity as useful constraints and create a system to use them for symbolic regression. The second part, which they call Neural-Guided Monte Carlo Tree Search, uses the network during a search to find an expression that conforms to a set of data points and desired leading powers.
Reject. rating score: 1. rating score: 1. rating score: 3. rating score: 3.  Summary The authors propose to perform out of distribution detection for regression models by fitting a generative model in the feature space of the regression model. You should fix these and resubmit to a future conference. The paper focuses on the difference between regression and classification tasks and claims that the paper s method addresses an unmet need for OOD for regression.<BRK>works very well in several cases. (The difference between classification and regression is a trivial change to the loss function, and does not change the fundamental idea.) In addition to lack of novelty, the experimental methodology is very weak. More importantly, the results on the two image datasets are suspect.<BRK>Questions to the authors:  Why focus only on regression? The main contribution of the paper is a way of calculating the "OOD scores" that are reported in the different figures. They then use this observation to propose an algorithm for detecting out of distribution data by fitting a simple GMM to the features produced during training.<BRK>This work is tackling the problem of doing out of distribution detection in regression. All in all, the presented method relies too much on heuristics that seem specific to the analyzed architectures and datasets and there is no theoretical reason for this to hold in general as far as I can see. The originality of the paper is also somewhat limited, as generative models for OOD detection are well studied.<BRK>Most existing OOD methods only apply to classification tasks, as they assume a discrete set of possible predictions. In this paper, they propose a method for neural network OOD detection that can be applied to regression problems. They demonstrate that the hidden features for in-distribution data can be described by a highly concentrated, low dimensional distribution. They demonstrate on several real-world benchmark data sets that GMM-based feature detection achieves state-of-the-art OOD detection results on several regression tasks.
Accept (Poster). rating score: 8. rating score: 6. The paper also introduces a new Paired Associative Inference (PAI) task inspired by neuroscience and shows that most of the existing models including transformers struggle to solve this task while the proposed architecture (called MEMO) solves it better. 1.Section 2.1 requires more clarity. Not in MEMO. 4.Are the authors willing to release the code and data to reproduce their results? Page 2, second para: ENM should be EMN.<BRK>Experimental results show that standard memory architectures fail on these tasks. To redress this, the paper proposes a new memory architecture with several new features that allow for much better performance on the paired associate task. Major comments:Overall this is an interesting and useful work which uses a task from cognitive psychology to illuminate reasoning limitations in prior memory augmented neural networks.<BRK>Here they employed a classic associative inference task from the human neuroscience literature in order to more carefully probe the reasoning capacity of existing memory-augmented architectures. Similar results were obtained on a more complex task involving finding the shortest path between nodes in a path. They therefore developed a novel architecture, MEMO, endowed with the capacity to reason over longer distances. First, it introduces a separation between memories/facts stored in external memory and the items that comprise these facts in external memory.
Reject. rating score: 3. rating score: 6. rating score: 6. The primary contribution of this work is the formulation of inverse reinforcement learning (for restricted spaces of context dependent reward functions) as a convex optimization problem. The experimental results are not particularly useful in evaluating the proposed algorithms, as the tasks involved are relatively simple, discrete state MDPs (with continuous state features), and more importantly, no comparisons with existing IRL approaches are provided. With the latter formulation, many existing approaches to inverse reinforcement learning or imitation learning would be applicable.<BRK>This paper introduces a formulation for the contextual inverse reinforcement learning (COIRL) problem and proposed three algorithms for solving the proposed problem. The authors presented their algorithms with thorough theoretical analysis. However, the authors did not analyze or show in either of their experiments how existing IRL algorithms compare with the proposed algorithms. Lastly, it would be more insightful to the readers if the authors can provide some discussion on how they would extend their algorithms to the more interesting/practical case where the context is not directly observed and analyze how latent contexts would affect the performance/complexity of their proposed algorithms. Overall, this is a well written paper on an exciting research topic but lacks sufficient analysis and experimental results to support the significance of the intended contributions.<BRK>The authors consider the problem of inverse reinforcement learning for CMDPs in which the reward function is a function of the context. I think this is a good paper, studying an interesting problem and proposing useful solutions, so it should be accepted. Maybe these guarantees could be moved into a separate section. * One of the main limitations of this work seems to be that the CMDP\M has to be known. Please comment on how one could expand the analysis/applications/experiments to extend to the case where the CMDP is not known.<BRK>In this setting, the reward, which is unknown to the agent, is afunction of a static parameter referred to as the context. The goal of the agent is to learn the expert ’ s mapping by observing demonstrations. They define an optimization problem for finding this mapping and show that whenit is linear, the problem is convex. They present and analyze the sample complexityof three algorithms for solving this problem: the mirrored descent algorithm, evolution strategies, and the ellipsoid method.
Reject. rating score: 3. rating score: 3. rating score: 8. The LSTM output is defined in appendix A.2 but appears not to be used for anything. The intermediate representation (output of the encoder) is a fixed dimensional vector. Please clarify in the paper. The experimental results suggest that the tensor product representation is helpful for both the encoder and the decoder. Both the encoder and decoder are based on the tensor product representation described in section 2. Throughout the main body and appendix, there are lots of instances of poor spacing. Is this indeed assumed? Did the authors consider using a bidirectional LSTM for the encoder?<BRK>The authors propose a binding unbinding mechanism for translating natural language to formal language. Given that there are no other connections between encoder and decoder, the design of the encoder cannot learn role and filler properly. For example, is there any restriction on the parameters in encoder and decoder respectively to reflect the property $UR I$ as in Section 2. Secondly, the design of the specific neural network cannot describe the theory behind proposed binding unbinding mechanism properly.<BRK>This paper considers the challenging problem of learning to generate programs from natural language descriptions: the inputs are sequences of words describing a task and the outputs are corresponding programs solving the task. Experiments on two datasets demonstrate the validity of the approach. The paper is very well written and easy to follow. Could that be a problem in practice? I am not very familiar with the literature but it seems some relevant work may be missing from the review.<BRK>In this paper, they propose a new encoder-decoder model based on Tensor Product Representations (TPRs) for Natural- to Formal-language generation, called TP-N2F. TP-N2F considerably outperforms LSTM-based Seq2Seq models, creating a new state of the art results on two benchmarks: the MathQA dataset for math problem solving, and the AlgoList dataset for program synthesis. Ablation studies show that improvements are mainly attributed to the use of TPRs in both the encoder and decoder to explicitly capture relational structure information for symbolic reasoning.
Reject. rating score: 3. rating score: 3. rating score: 3. rating score: 6. I thank the authors for their detailed response. Some of my questions have been addressed in the rebuttal, but as far as I can tell very few modifications have been made to the paper: mainly an additional super resolution experiment in the appendix, which goes in a good direction, but currently comes across as quite preliminary. So I think the paper still has most of the weaknesses I mentioned and thus it is not quite fit for publication. The paper proposes an approach to analyzing the properties of "perceptual metrics" used in deep learning image generation methods. Based on these responses, the paper proposes a "Perceptual Efficacy Score" that should measure the importance of certain feature in the feature maps for the performance of a perceptual metric. This i smainly because the experiments seem somewhat anecdotal and incomplete, see below for further details. 2) The proposed score seems to indeed correlate quite well with the importance of features for human judgement of image similarity. Is this based on some tuning? It would be interesting to see a comparison of frequency and orientation tuning of features in a CNN to human cells (as I understand, the latter should be available in prior works?). Do they lead to improved results?<BRK>This paper proposes an analysis of convolutional neural networks (CNNs) features the basis for making perceptual quality comparisons. The analysis is based on the proposed Perceptual Efficacy (PE) Score that measures spatial frequency and orientation selectivity of CNN features. The hypothesis put forward by the authors is that a CNN features with high PE score can be used to formulate a perceptual loss (Eq.1) that correlates well with human image quality judgement. So in the definition of the PE score we have embedded knowledge of human perceptual sensitivity. Experimental results: The scatterplot presented in Figure 4 does not say to me what the authors claim it should. I do not see a significant difference between the low PE features and the high PE features in terms of their correlation with human image quality judgement (as measures in this case by the DMOS). I would recommend an alternative method of presentation to make the desired point. Also, the description of visual masking in Sec.4.3 was confusing and difficult to follow.<BRK>The submission aims to analyze deep neural network (DNN) features in terms of how well they measure the perceptual severity of image distortions. It proposes to characterize each DNN feature in terms of two well known properties of the human visual system: a) sensitivity to changes in visual frequency and b) orientation selectivity. For that I believe it is necessary to demonstrate that the present empirical results can be used to improve results of an image generation task, e.g.super resolution.<BRK>In the article the authors propose to measure quality of CNN features by quantifying the orientation tuning and spatial frequency sensitivity of the features. The underlying hypothesis is that properties of features in the human visual cortex are also indicators for quality in CNNs. The authors devise an experiment similar to experiments performed on mammals to check which features are active under which types of basic patterns. I have a problem understanding some of the metrics used. I feel unable to connect this with the initial hypothesis, as it does not mention change of frequencies. in that case mu_2 would be the variance, which would be a natural measure for orientation selectivity. Is there a way to make Table 1 more pleasing for the human eye wrt the discussion of the results?<BRK>In this paper, to get more insight, they link basic human visual perception to characteristics of learned deep CNN representations as a novel and first attempt to interpret them. They observe that the behavior of CNN channels as spatial frequency and orientation selective filters can be used to link basic human visual perception models to their characteristics. They conclude that sensitivity to spatial frequencies that have lower contrast masking thresholds in human visual perception and a definite and strong orientation selectivity are important attributes of deep CNN channels that deliver better perceptual quality features.
Reject. rating score: 1. rating score: 1. rating score: 1. This paper introduces a method to detect cars from a single image. The method imposes several handcrafted constraints specific to the dataset in order to achieve higher improvement and efficiency. These constraints are quite strong and they not generalize to new situations (eg.a car in the sky, a car upside down a car with multiple wheels). I would suggest to emphasise the improvements over previous works and send this paper to a specialized journal or venue in vehicle detection.<BRK>The idea of using keypoints to carry pose estimation is is more than 15 years old, and for the car examples reported in this paper, I m wondering why not just you SURF or SIFT   these would certainly have been reasonable baselines. It is full of typos and far from ready for submission to ICLR. The equations (eg eqn 1) are impossible to parse.<BRK>The authors propose a set of model configurations on the waypoint detection, optimization techniques, a deep learning network topology and a data driven and domain knowledge based wheel detection mechanism. Including a quantitative discussion on the results and the corresponding uncertainties    Insert a quantitative  discussion on the experiments. Other: The paper is an application paper and does not offer novel advances for the ICLR community. However, from the material shown, especially the lack of the experimental description and its systematic shortcomings,  I cannot judge if the components of the method can contribute to improved vehicle detection and tracking.<BRK>They present a method to infer 3D location and orientation of vehicles on a single image. Here they also integrate three task priors, including a ground plane constraint and vehicle wheel grounding point position, as well as a small projection error from the image to the ground plane.
Reject. rating score: 1. rating score: 3. rating score: 3. This paper studies the two learning rates \alpha and \beta used in Model Agnostic Meta Learning (MAML) algorithm by [Finn et al., 2017]. This means that the setting is too simplified so that it is not even a meta learning setup. Under simplifications, the paper derives some necessary conditions on \alpha and \beta for the MAML iterates to converge to local minima, and then verifies the theory by experiments on synthetic and real world data. I also have concerns about the correctness of the analysis in the single task case.<BRK>For example, Behl et al.(2019) automatically tuning the learning rates during training definitely needs to be compared, since both aim to stabilize the training of MAML. Will the conclusion apply to such setups? This work theoretically discusses the relationship between the inner loop learning rate and the outer one, under a set of assumptions. Cons: 	Some of the simplifications for proving are empirical, so that the proof itself is not that rigorous.<BRK>The authors study a method to help tuning the two learning rates used in the MAML training algorithm. This condition is reminiscent of convergence criteria for gradient descent on quadratic objectives, and the authors make the interesting observation that the criteria for the exterior learning rate beta depends on the interior learning rate alpha. I’m not a MAML expert so I’ll let the other reviewers judge how this paper compares to the current literature. However, I think that the empirical work can be pushed further. The experiments on Omniglot and MiniImagenet are coherent with the theory, but I am not completely sure of their impact. As a result, I think the paper is exploring an interesting direction, but that the empirical work might be too preliminary for publication.<BRK>However, MAML is notorious for being hard to train because of the existence of two learning rates. Therefore, in this paper, they derive the conditions that inner learning rate $\alpha $and meta-learning rate $\beta $must satisfy for MAML to converge to minima with some simplifications.
Accept (Poster). rating score: 6. rating score: 6. rating score: 1. The paper studies self supervised learning from very few unlabeled images, down to the extreme case where only a single image is used for training. The experiments are carefully described and presented, and the paper is well written. [1] found that the network architecture for self supervised learning can matter a lot, and that by using a ResNet architecture, performance of SSL methods can be significantly improved. I appreciate the efforts of the authors into investigating the issues raised, the described experiments sound promising. Unfortunately, the new results are not presented in the revision. I will therefore keep my rating.<BRK>Overall, I think this paper is a nice sanity check on recent self supervision methods. The result holds for three state of the art self supervised methods, tested with two single image training examples. In my view, learning without labels is an important problem, and it is interesting what can be learned from a single image and simple data augmentation strategies. Could the authors please either correct this logic or provide the experiments? Further, it seems that the results in Table 4 might be a bit obscured by the size of the downstream task dataset. Finally, more images are needed to learn the deeper layers for the downstream task anyway. The paper is well written and clear.<BRK>This paper explores self supervised learning in the low data regime, comparing results to self supervised learning on larger datasets. BiGAN, RotNet, and DeepCluster serve as the reference self supervised methods. It argues that early layers of a convolutional neural network can be effectively learned from a single source image, with data augmentation. A performance gap exists for deeper layers, suggesting that larger datasets are required for self supervised learning of useful filters in deeper network layers. More importantly, it is already well established that it is possible to learn, from only a few images, filter sets that resemble the early layers of filters learned by CNNs.<BRK>They show that three different and representative methods, BiGAN, RotNet and DeepCluster, can learn the first few layers of a convolutional network from a single image as well as using millions of images and manual labels, provided that strong data augmentation is used. However, for deeper layers the gap with manual supervision cannot be closed even if millions of unlabelled images are used for training.
Accept (Poster). rating score: 3. rating score: 3. rating score: 1. Regarding its contribution, the paper seems to consider a training process which is done using the variational EM algorithm. The variational EM is used to optimize the ELBO term (motivation for this is the intractability of the computing the partition term). In the E step, they infer the posterior distribution and in the M step they learn the weights. Overall, the work of this paper seems technically sound but I don’t find the contributions particularly surprising or novel. Along with plogicnet, there have been many extensions and applications of Gnns, and I didn’t find that the paper expands this perspective in any surprising way.<BRK>In this paper the authors propose a system, called ExpressGNN, that combines MLNs and GNNs. This system is able to perform inference and learning the weights of the logic formulas. The proposed approach seems valid and really intriguing. I have just one concern and it is about the experiments for the knowledge graph completion task. In fact, this task was performed only on one KG. I think the proposed system should be evaluated on more KGs. For these reasons I think the paper, after an extension of the experimental results, should be accepted. [Minor]Page 3. “The equality holds” which equality are you talking about?<BRK>The main motivation seems to be that inference in traditional MLN is computationally inefficient. The paper is cryptic about precisely why this is the case. There is some allusion in the introduction as to grounding being exponential in the number of entities and the exponent being related to the number of variables in the clauses of the MLN but this should be more clearly stated (e.g., does inference being exponential in the number of entities hold for lifted BP?). This is not a paper where the related work section should be delegated to the appendix. Otherwise, its very difficult to read the paper and appreciate the results. Recently, a number of papers have been tried to quantify the expressive power of GNNs. MLN is fairly general, being able to incorporate any clause in first order logic. This question deserves an answer. If so, then the speedup isn t free and ExpressGNN would be a special case of MLN, albeit with the advantage of fast inference. At the very least, the paper should provide clear time complexities for each of the baselines. There are cheaper incarnations of MLN that the authors should compare against (or provide clear reasons as to why this is not needed). In ICDM)<BRK>However, inference in MLN is computationally intensive, making the industrial-scale application of MLN very difficult. In recent years, graph neural networks (GNNs) have emerged as efficient and effective tools for large-scale graph problems. Nevertheless, GNNs do not explicitly incorporate prior logic rules into the models, and may require many labeled examples for a target task. In this paper, they explore the combination of MLNs and GNNs, and use graph neural networks for variational inference in MLN. Their extensive experiments on several benchmark datasets demonstrate that ExpressGNN leads to effective and efficient probabilistic logic reasoning.
Reject. rating score: 1. rating score: 3. rating score: 6. rating score: 6. Some theoretical explanation of the method is provided. By seeing the section 4.1 and 4.2, it seems possible to access neurons in the intermediate layers. Concerns  Most of all, the information the experiments conveys is too small to convince the argument of the author. The author suggests that this method can be applied to various networks. Still, the reviewer couldn t find any clue that the method actually worked for various settings: different activations, convolutional networks, and so on. Assuming that the network was trained by MNIST and we infer the weight of the networks by the proposed method. Oh.et.al (https://arxiv.org/abs/1711.01768) proposed a blackbox reverse engineering method and provided experimental settings as well. The reviewer could not find much clue supporting the author s argument from the experiment section. inquiries  See the Concerns parts.<BRK>Actually, this would be a better proof of concept: Given a pre trained MNIST classifier, apply the proposed method, recover the weights and check if you get the same output as from the original network. This paper introduces an approach to recover weights of ReLU neural networks by querying the network with specifically constructed inputs. But in the end, one is interested in recovering the original weights of the network, not relative ones. Again, this analysis is very limited, it would be very interesting to see, how many more queries one needs for deeper layers of the network. But for this experiments with deeper than 2 layers networks are necessary. This paper tackles a very interesting and important problem that might have huge implications for security and many other aspects. The algorithm s description is either incomplete or unclear. At least a network that could classify MNIST reasonably well.<BRK>This paper introduces a procedure for reconstructing the architecture and weights of deep ReLU network, given only the ability to query the network (observe network outputs for a sequence of inputs). The method is currently limited to ReLU networks and does not account for any parameter sharing structure, such as that found in convolutional networks. Is the proposed approach practical to apply to networks used in actual applications? Page 7 states that the proposed algorithm also holds for ResNets, with slight modifications, but defers details to future work. If the modifications are indeed slight, it would better to include them here as this is an important special case and would increase the potential impact of the paper. I would appreciate clarification on concerns over practicality and the extension to ResNets.<BRK>In this paper, the authors showed that in many cases it is possible to reconstruct the architecture, weights, and biases of a deep ReLU network given the ability to query the network. The studied problem is very interesting. 2.How about the efficiency of the proposed method? I would like to see some analysis of the computational complexity and also some related experimental results. It would be very interesting to show a few example on reconstructing the input.<BRK>Here, they show that in many cases it is possible to reconstruct the architecture, weights, and biases of a deep ReLU network given the ability to query the network. Their algorithm uses this to identify the units in the network and weights connecting them (up to isomorphism). The fact that considerable parts of deep networks can be identified from their outputs has implications for security, neuroscience, and their understanding of neural networks.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. Besides, on the computation side, it would be complexity, an explicit comparison of complexity makes it easier to evaluate the performance when compared to other state of the art methods. It seems to me that the proposed framework also falls in this category, with a treatment from deep learning. Empirical evaluation on three benchmarks against other baselines suggested the advantage of the proposed method. [Decision]Overall, the paper addresses an important problem in computer vision (video action recognition) with an interesting.<BRK>The paper proposes 4d convolution and an enhanced inference strategy to improve the feature interaction for video classification. State of the art performance is achieved on several datasets. 4.Most importantly, for table4, authors are comparing to the ResNet18 ARNet instead of ResNet50? which is better than the proposed method. 5. lack of related work:  4D Spatio Temporal ConvNets: Minkowski Convolutional Neural Networks, CVPR 2019.<BRK>This paper presents 4D convolutional neural networks for video level representations. To learn long range evolution of spatio temporal representation of videos, the authors proposed V4D convolution layer. Same for action units. It makes sense, and I expect the proposed model may benefit from its design for long range spatio temporal feature learning. The paper presents an interesting idea, but there are some issues that need to be addressed before published on ICLR.<BRK>In this paper, they propose Video-level 4D Convolutional Neural Networks, namely V4D, to model the evolution of long-range spatio-temporal representation with 4D convolutions, as well as preserving 3D spatio-temporal representations with residual connections. They further introduce the training and inference methods for the proposed V4D. Extensive experiments are conducted on three video recognition benchmarks, where V4D achieves excellent results, surpassing recent 3D CNNs by a large margin.
Accept (Poster). rating score: 8. rating score: 8. rating score: 6. Either way, while this is an interesting result, it seems a bit misplaced in this paper. The method is also cost effective, a few hundred dollars depending on the dataset and the number of queries used to train the attacker model.<BRK>The paper is well written and easy to follow (the two exceptions/oddities are Figure 1 & Table 1, which appear one page before they are refered, which makes them initially hard to understand because they are out of context).<BRK>The question is: how easy is it for an adversary model to learn to imitate the victim model, only from novel inputs and the corresponding outputs? This paper is technically not very novel, but asks interesting questions.<BRK>They study the problem of model extraction in natural language processing, in which an adversary with only query access to a victim model attempts to reconstruct a local copy of that model. Finally, they study two defense strategies against model extraction—membership classification and API watermarking—which while successful against some adversaries can also be circumvented by more clever ones.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. The paper proposes an elegant method for task free continual learning problems. It has nicely pointed out that the conventional continual learning algorithms had the limitation of knowing the task boundaries. Summary: By applying DPM, the authors proposed a method of automatically determining whether to add a new expert for a new task or train the existing experts. While the Dirichlet Process Mixture (DPM) is not new, applying such nonparametric method to continual learning is new. The experimental results are impressive given the single epoch setting. Pros:1.Good experimental results for the task free setting, in which no information about task boundaries is given. I am not sure whether the proposed methods should work well for "all" cases. Would following the framework with known number of experts also excel other methods? 3.Can you apply this to the RL setting?<BRK>Summary: The paper proposes to use a Bayesian nonparametric mixture model for task free (without explicit task labels) continual learning. In fact, the paper claims its contribution is expansion based task free continual learning. However, this “task free characteristic” is the contribution of SVA based inference. Results show that the approach works well. The code has been released. Overall I am inclining towards voting for acceptance if the authors could address my following questions:  Could you comment on the creation of test data? This is important to distinguish since there are other methods that are fully Bayesian like Nguyen et.<BRK>This paper proposes Continual Neural Dirichlet Process Mixture Model (CN DPM) to solve task free continual learning. The core idea is to employ Dirichlet process mixture model to create novel experts in online fashion when task distributions change. Overall I find this paper to be well written and the experiments are conducted thoroughly. The main algorithm itself cannot be considered to be novel. DPM or other Bayesian nonparametric models have been extensively used for the problems requiring to adapt the model size according to the change of data. Nevertheless, the application of DPM in task free continual learning context seems to be considered as a contribution. In page 4 the authors stated that the generative model prevents catastrophic forgetting. Could you elaborate more on this? Another minor concern is the way the concentration parameter alpha is selected.<BRK>Despite the growing interest in continual learning, most of its contemporary works have been studied in a rather restricted setting where tasks are clearly distinguishable, and task boundaries are known during training. However, if their goal is to develop an algorithm that learns as humans do, this setting is far from realistic, and it is essential to develop a methodology that works in a task-free manner. In this work, they propose an expansion-based approach for task-free continual learning. CN-DPM expands the number of experts in a principled way under the Bayesian nonparametric framework.
Reject. rating score: 3. rating score: 6. rating score: 6. This paper focuses on the problem of developing deep learning systems that can prove theorems in a mathematical formalism   in this case, MetaMath. What sets this work apart from others is its focus on the instrumental task of generating data to train a prover, rather than directly training the prover on human theorems (via reinforcement learning) or human proofs (via imitation learning). Both approaches result in a policy that can be used to take proof steps, with the goal of producing new theorems which are similar to the human ones. The main result of the paper is that an extra 35/2720 (1.2%) of the test theorems are proven, a 6% improvement over the Holophrasm baseline of 539. It is difficult to judge how relevant of an improvement this is, and there is no analysis of the difficulty of the MetaMath problem set. In addition, due to the 10 1 1 train validation test split, the neural agents are likely shown relatively similar problems during training as at test time, including potentially stronger versions of the same theorems. There is also no comparison against non neural approaches, such as Z3, Vampire, or similar theorem provers. To accept this paper, I would like to see stronger evidence that the introduced method produces significant improvements in prover ability. For example, the same method could be applied to datasets such as HOList, Mizar, and CoqGym which have received more attention recently than MetaMath. Certainly not all 1M synthetic theorems could be generated in one graph. 3.Please include some more details about the training of the Holophrasm baseline. Does it simply do RL on the human theorems, or does it also do IL on human proofs?<BRK>This paper proposes a generative model for proofs in Metamath, a language for formalizing mathematics. The parameters of these networks are learned from existing proofs or theorem statements. The main purpose of this model is to generate synthetic theorems and proofs that can be used to train the neural networks of a data driven search based theorem prover. I think that the paper studies an important problem and contains interesting ideas. The idea of using a language model for theorem statements (so that a generated theorem can be meaningfully compared with a given theorem even when they are not the same) looks sensible. Also, the conjecture that a good proof generator is likely to lead to a good theorem prover sounds plausible. I find the description of the training of the generative model in the experiments slightly confusing. What theory is formalized by set.mm? Here are some minor comments.<BRK>This paper focuses on the task of automated theorem proving. To address the low availability of human written data and low sample efficiency in reinforcement learning, the authors propose to augment data by generating synthetic theorem data with a deep neural network based model. Experimental results show the usefulness of the generated synthetic theorem. This paper is well motivated and the proposed method is quite novel for automated theorem proving. The paper is well supported by theorems, however, the experimental analysis is a little weak. For the above reasons, I tend to accept this paper but wouldn t mind rejecting it. Maybe it s better if you can shorten section 3 and explain more about the problem setting (such as how to fit this problem in a graph?). 2.Can you show some examples of generated theorems? 3.You showed the prover has better performance with more synthetic data, but why is your model (generator) better? Can other generative models generate better proofs?<BRK>They consider the task of automated theorem proving, a key AI task. Deep learning has shown promise for training theorem provers, but there are limited human-written theorems and proofs available for supervised learning. To address this limitation, they propose to learn a neural generator that automatically synthesizes theorems and proofs for the purpose of training a theorem prover. Experiments on real-world tasks demonstrate that synthetic data from their approach significantly improves the theorem prover and advances the state of the art of automated theorem proving in Metamath.
Reject. rating score: 1. rating score: 3. rating score: 6. The paper prposes to learn an inverse network to predict x given a target y for optimisation, instead of the traditional way of optimisation (e.g.using Bayesian optimisation for the complex cases considered in the paper). However, unfortunately, this paper is too close in concept, and in my understanding lower in the solution quality to this recent paper:Nguyen, Phuoc, Truyen Tran, Sunil Gupta, Santu Rana, Matthew Barnett, and Svetha Venkatesh. In Proceedings of the 2019 SIAM International Conference on Data Mining, pp. Please let me know if I missed anything. Otherwise it is a reject from me.<BRK>This paper tackles the problem of solving a black box optimization problem where only some samples have been observed. Instead of learning only a single forward model of x  > y, this paper proposes to additionally use a mapping from y  > x. Optimizing in the space of z instead of x can be much simpler, and this should also act as a strong regularizer during training. Specifically, the paper uses a GAN that transforms [y,z]  > x, where z is stochastically sampled. Pros:   The proposed idea of using an inverse mapping is straightward but shown to be effective. Cons:   The proposed pieces were often difficult to follow, and there doesn t seem to be sufficient information regarding the reweighting and randomized labeling for understanding and reproducing this work (see Questions). Only a few ablation experiments were carried out, and the effect of reweighting seems to only appear as a visual comparison in Figure 1. Also, what were the original examples (are they closer to the F results, or the MIN results)? After (1), it wasn t yet clear why this is called "model based" optimization, as no model has been introduced yet. typo: "method to solve perform optimization"  typo: Figure 1 "Obsere"<BRK>In this paper, the authors present a method for model based blackbox optimization for cases where the design variables x are likely to lie on a manifold within a high dimensional space. Overall, I feel positively about the paper, largely because of the idea it introduces. First, the constructions in 3.2 and 3.3 are perhaps a little more ad hoc than they need to be. Additionally, two of the sections in the experimental results seemed somewhat rushed and need significant additional detail. This last paragraph is something of a shame and one of the main weaknesses I see with the paper, as these two section are among the only quantitative results for the authors  optimization algorithm.<BRK>In this work, they aim to solve data-driven optimization problems, where the goal is to find an input that maximizes an unknown score function given access to a dataset of input, score pairs. They propose model inversion networks (MINs) as an approach to solve such problems. They show that MINs can also be extended to the active setting, commonly studied in prior work, via a simple, novel and effective scheme for active data collection. Their experiments show that MINs act as powerful optimizers on a range of contextual/non-contextual, static/active problems including optimization over images and protein designs and learning from logged bandit feedback.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. This paper presents LayerDrop, a simple method for dropping groups of weights (typically layers) jointly. For me the most exciting thing about this approach is that this training regime allows to prune the trained network at test time *without finetuning*, effectively getting a smaller, more efficient network for free. Using dropout in the baseline model with a similar proportion as LayerDrop seems like an important baseline, and in particular it would be interesting to see whether the deep experiments (e.g., 40 layers on WT103) that are hard to train without LayerDrop could converge with regular dropout. I think this is a very strong submission and strongly advocate accepting it to ICLR.<BRK>The paper proposes a method, LayerDrop, for pruning layers in Transformer based models. The goal is to explore the stochastic depth of transformer models during training in order to do efficient layer pruning at inference time. The key idea is simple and easy to understand: randomly dropping transformer layers during training to make the model robust to subsequent pruning. + Strong results from the pruned networks without fine tuning on downstream tasks. It is also a sensible approach given the strong regularization effect of stochastic depth. Question: Similar to Pham et al.s work on applying stochastic depth to train very deep transformers for speech, do you expect LayerDrop to be helpful for training very deep transformer based models for NLP tasks assuming memory is not a big constraint?<BRK>This work explored the effect of LayerDrop training in efficient pruning at inference time. The authors showed that it is possible to have comparable performance from sub networks of smaller depth selected from one large network without additional finetuning. But the authors spent a lot of space showing improved results on many tasks, which are mainly from learning a larger network or with additional data compared to the baselines. On the other hand, I do not think it is adequate to argue the proposed method is a "novel approach to train over parameterized networks".<BRK>Overparametrized transformer networks have obtained state of the art results in various natural language processing tasks, such as machine translation, language modeling, and question answering. In this work, they explore LayerDrop, a form of structured dropout, which has a regularization effect during training and allows for efficient pruning at inference time. In particular, they show that it is possible to select sub-networks of any depth from one large network without having to finetune them and with limited impact on performance.
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. The key idea of the proposed method is to enable the network to represent syntactic and semantic knowledge separately. This allows the neural network to leverage compositionality for knowledge transfer while alleviating catastrophic forgetting. *Additional feedbackMy concern is about evaluation. But, this suggests that the datasets might be too artificial for this evaluation.<BRK>More specifically, the paper addresses the problem of growing vocabulary. The novelty of the proposed method is somewhat limited in my opinion. 3.I believe that f_predict here is parameterized by \theta and \theta is also frozen during the continual learning phase which contradicts the claim at the end of section 3.2 that only \phi is frozen. Otherwise, it’s hard to see why f_predict does not suffer from catastrophic forgetting. More on this in point 6.<BRK>The paper is about continual learning on NLP applications like natural language instruction learning or machine translation. The authors propose to exploit "compositionality" to separate semantics and syntax so as to facilitate the problem of interest. In summary, the current manuscript is clearly not ready for publication. The writing is not good, as I cannot see clearly the backbone of the paper. No preliminary background knowledge? What’s the main advantage/novelty of the presented method compared to that classic/naïve method? Please see the detailed comments below.<BRK>Most of the existing methods have been focusing on continual learning of label prediction tasks, which have fixed input and output sizes. In this paper, they propose a new scenario of continual learning which handles sequence-to-sequence tasks common in language learning. They further propose an approach to use label prediction continual learning algorithm for sequence-to-sequence continual learning by leveraging compositionality. Experimental results show that the proposed method has significant improvement over state-of-the-art methods. It enables knowledge transfer and prevents catastrophic forgetting, resulting in more than 85% accuracy up to 100 stages, compared with less than 50% accuracy for baselines in instruction learning task.
Reject. rating score: 1. rating score: 3. rating score: 6. The paper proposes an approach to exploration by utilizing an intrinsic reward based on distances in a learned, evolving abstract representation space. Learning the abstract representation space itself is based on a previous work, but the contribution of this paper is the utility of it to design the reward bonus for exploration by utilizing distances in this evolving representation space. If they are for the proposed method, are they for the competitors too? (2) I think the presentation of the bonus itself   novelty search (Section 4), which is the core of the paper, is rather unclear. Further, it would be helpful to mention that it is an estimate based on the learned model. I think it either needs more empirical validation, or a theoretical justification.<BRK>●	The description of the planning algorithm and Q learning in section 4 is a little sloppy, a clearer description would be appreciated. Pros: 1.Overall the paper is clear and the proposed method makes sense intuitively. This point should be at least acknowledged. As correctly stated by the authors, the L_2 norm will cease to be a good metric as state dimensionality increases. My main issue with the work in its current form is that the method is too light in terms of technical contribution. Does this happen in the limiting case   if not, is the margin acceptable.<BRK>This paper proposes a method of sample efficient exploration for RL agent. To solve this problem, the authors leverage novelty heuristics in a lower dimensional representation of a state, for which they propose a novelty measure. Authors propose a novel approach to the problem of exploration. They test their method by experiments conducted in two environments, where they use the same model architectures and model free methods for all types of novelty metrics, which shows the contribution of the proposed method in the results of learning. 1.The dependence of the quality of the dimensionality representational state is unclear.<BRK>They present a new approach for efficient exploration which leverages a low-dimensional encoding of the environment learned with a combination of model-based and model-free objectives. One key element of their approach is that they perform more gradient steps in-between every environment step in order to ensure the model accuracy.
Reject. rating score: 3. rating score: 3. rating score: 8. Novelty & Significance The paper definitely tackles an important problem (point cloud forecasting). I would imagine that data with outliers (whose values are unlike most others) would dramatically hurt performance, as the weights of D Conv would need to be shared equally by outliers and inliers. This work s contribution was a new "X Conv" operator, which also consumes point clouds and produces learned representations.<BRK>In order to achieve the convolution over point clouds by using both value features and the spatial features, given a data point, the convolution is conducted over its k nearest neighbors generated by CNN. Overall, this paper is interesting but needs some clarifications on 1. Given that the proposed convolution operator use KNN to choose the nearest neighbors. The choice of RNN, attention, or the new operator?<BRK>The approach is well situated in the literature, and the experiments are indicative that this method can improve on the current approaches. The D Conv operator is included in a LSTM architecture (CloudLSTM) to enable the spatio temporal modeling of point cloud data, and can be combined in standard neural network architectures such as a Seq2Seq with attention. I recommend acceptance of the paper and updated to 8.<BRK>This paper introduces CloudLSTM, a new branch of recurrent neural models tailored to forecasting over data streams generated by geospatial point-cloud sources. The D-Conv operator resolves the grid-structural data requirements of existing spatiotemporal forecasting models and can be easily plugged into traditional LSTM architectures with sequence-to-sequence learning and attention mechanisms.
Reject. rating score: 3. rating score: 3. rating score: 3. This paper proposes to use the GAN framework for adversarial training. After reading the comments and the updated draft, I am still not convinced that the proposed method should be superior to existing attacks.<BRK>This paper introduces a new adversarial training approach, where a generator is used to generate the most challenging adversarial examples and the classifier is trained to correctly classify the generated adversarial examples. In this way, the robustness of the classifier is expected to be improved. The general idea of improving the robustness of a classifier to by feeding adversarial examples to it is not a new idea.<BRK>The paper proposes to construct adversarial attacks by training a neural network to produce a distortion, rather than by constructing the distortion directly via PGD. Average distortion is also only meaningful if the method is finding the minimum norm attack point that changes the label, which most of the methods you consider do not do. In that case all the numbers should be zero for any reasonable attack.)<BRK>In addition to the classifier, their method adds another neural network that generates the most effective adversarial perturbation by finding the weakness of the classifier.
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. I would strongly encourage the authors to incorporate the baseline "(1)" as proposed by R3 in a future version of the paper as I agree with them that this is a relevant baseline. The motivation is that meta learning the noise allows to learn how to best perturb examples in order to improve generlization. This claim is supported by ample experimental evidence and comparisons against many baselines, as well as additional ablation studies w.r.t design choices of the algorithm itself.<BRK>This paper proposes meta dropout, which leverages adaptive dropout training for regularizing gradient based meta learning models, e.g., MAML and MetaSGD. I think it is ok to say that meta dropout tries to optimize a lower bound of log p(Y|X;\theta,\phi^*), but meta dropout does not regularize the variational framework because there is no variational inference framework. 2.Experiments on adversarial robustness can be further improved. (1) the settings and the analysis of adversarial robustness experiment can be discussed in details.<BRK>The paper proposes learning to add input dependent noise to improve the generalization of MAML style meta learning algorithm. The proposed method is evaluated on OmniGlot and miniImageNet. It is furthermore shown that meta dropout somewhat improves the model’s robustness against an adversarial attack. There is just too much confusion in the results, the improvements are not very robust.<BRK>However, obtaining such perturbation is not possible in standard machine learning frameworks as the distribution of the test data is unknown. They validate their method on few-shot classification datasets, whose results show that it significantly improves the generalization performance of the base model, and largely outperforms existing regularization methods such as information bottleneck, manifold mixup, and information dropout.
Reject. rating score: 3. rating score: 3. rating score: 6. The paper is concerned with few shot classification, both its benchmarks and method used to tackle it. The paper makes a number of contributions. This makes it unclear to what extent the proposed metric is general and predictive.<BRK>To validate this, the authors proposed clustering based meta learning method, called Centroid Network. The authors also proposed a new metric to quantify the difficulty of meta learning for few shot classification. <Details>* Strength + The paper is well organized. + A novel study on quantifying the difficulty of meta learning. + The proposed CentroidNet performs well in the experiments. The definition of CSCC is not convincing.<BRK>The authors propose a new metric for measuring the simplicity of a few shot learning benchmark and demonstrate that it is possible to achieve high performance on Omniglot and miniImageNet with their unsupervised method, resulting in a high value of this criterion, whereas the Meta Dataset is much more difficult. The paper is well written and generally very clear.<BRK>Because the class semantics are so similar, they propose a new method called Centroid Networks which can achieve surprisingly high accuracies on Omniglot and miniImageNet without using any labels at metaevaluation time. Their results suggest that those benchmarks are not adapted for supervised few-shot classification since the supervision itself is not necessary during meta-evaluation. Using their method, they derive a new metric, the Class Semantics Consistency Criterion, and use it to quantify the difficulty of Meta-Dataset.
Reject. rating score: 3. rating score: 3. rating score: 3. Furthermore, the early stopping criterion was not specified. (1.4) The details of the experiment in Figure 2 are not clear. Do you use a fixed number of SGD iterations during training and a variable number of iterations (determined by a stopping criterion) during testing? The paper also demonstrates that the EfficientNet architecture can be applied to segmentation.<BRK>(I did not see this split in the Appendix) and etc2. My main concern is novelty. In its current form I suggest to reject the paper and urge the authors to improve it according to the following points:1. how it is made sure that meta testing is done on a separate set of categories?<BRK>Strengths  Apart from the flaws mentioned under weaknesses, the paper is generally easy to follow. Maybe the statement could be framed better but in it’s current form it’s unclear what is being conveyed. WeaknessesThe paper has some major weaknesses that affect the clarity of the points being conveyed in several sections.<BRK>Finally, they show both theoretically and empirically that a key limitation of MAML-type algorithms is that when adapting to new tasks, a single update procedure is used that is not conditioned on the data.
Reject. rating score: 3. rating score: 6. rating score: 6. The paper proposes an improved extension of the Wasserstein auto encoder for anomaly detection. The novelty is in proposing a weighted reconstruction error that penalizes the mapping of data with high reconstruction errors (mostly anomalies) into high probability regions. The idea being that an outlier would have a higher reconstruction error, and hence should be mapped to low probability region of the latent distribution. I am not sure if, from anomaly detection perspective, this is any better than simply using the reconstruction score.<BRK>This work proposes an outlier detection method based on WAE framework. WAE is trained to ensure that 1) latent distribution follows a prior distribution 2) weighted reconstruction error is low where prior PDF is used to weight the reconstruction error. Post Rebuttal:Authors have partially addressed my concerns. I cannot recommend to accept the paper in its present condition. Outliers are  bad eggs  coming from the same class as normal data. Have other works published their results on this dataset? I believe reporting results on at least two datasets is necessary to demonstrate the generalizability of the method. What is the dimensionality used in the latent space?<BRK>This paper proposes a novel outlier detection approach, based on Wasserstein auto encoders. I will rely on the judgement of the other reviewers, whom I hope will have more experience and will better know the literature. In fig.2, the WAE acronym is defined only much later in the text. Indeed, the text also refers to fig. The text just below fig. It looks as if the symbols () and [] are inverted?<BRK>However, this assumption fails in practice, because the divergence penalty adopted for this purpose encourages mapping outliers into the same high-probability regions as inliers. To overcome this shortcoming, they introduce a novel deep learning outlier detection method, called Outlier Preserving Distribution Mapping Autoencoder (OP-DMA), which succeeds to map outliers to low probability regions in the latent space of an autoencoder. For this they leverage the insight that outliers are likely to have a higher reconstruction error than inliers. This weighting implies that outliers will result overall penalty if they are mapped to low-probability regions.
Reject. rating score: 3. rating score: 6. rating score: 6. The authors propose a joint/compositional embedding procedure where a single instance can be mapped/embedded to multiple classes while preserving the class specific information in the embedded representations. The authors look at class union and class query criteria for the composite embeddings. The proposed approach is evaluated appropriately. There are several issues with the work. Why is ML the best baseline? "... x_a containing objects in another image "   this statement is not making sense, is it objects in x_a also present in another image x_b?<BRK>This is done throughly jointly training embedding functions, a set union function and a query function. The paper reads well. While the approach is reasonable, the experiments seem to be quite incomplete and no explanation is given why a trivial solution cannot be used instead of the learnt functions. On the evaluation that utilises larger sets, e.g.COCO, there isn t any analysis of how performance of the technique scales with the size of the set since that would be one of the defining characteristics of a set union function. For example, f could be a function that maps an image to a binary representation of its classes (this could be a typical ResNet image classifier), g could be a function that does a binary OR of its two arguments and h could be a function that uses a binary AND and equality test on its two arguments. This may not be possible in the COCO experiment where the individual labels are not known but it seems quite unrealistic to have a dataset where only pairwise subset relationships are known. It also seems that the f is always different between that used with g and that used with h, is this the case? SimRef also doesn t do data augmentation but there s no explanation why it is done for the proposed method and not for this baseline.<BRK>Summary: This paper proposes compositional embeddings i.e.embeddings that can be used to infer multiple classes from the data. In particular, the paper deals with two types of composite functions for embeddings, one that computes union of the different classes represented by each embedding vector, and the other where the class of one of the embeddings is subsumed by the class of the other embedding. Comments: 1) This paper presents a welcome contribution to the saturated literature on embeddings. Further, the paper is very well written and puts itself nicely in context of previous work. I think this should inspire future work on other kinds of composite functions other than the two considered here. 3) The results on both the synthetic and real world omniglot and COCO datasets are impressive and mostly well executed and show significant improvement over the "most frequent" baseline. It seems contrived. Is it possible to do some ablation studies?<BRK>They explore the idea of compositional set embeddings that can be used to infer notjust a single class, but the set of classes associated with the input data (e.g., image, video, audio signal). This can be useful, for example, in multi-object detection inimages, or multi-speaker diarization (one-shot learning) in audio. Incontrast to prior work, these models must both perceive the classes associatedwith the input examples, and also encode the relationships between different classlabel sets. In experiments conducted on simulated data, OmniGlot, and COCOdatasets, the proposed composite embedding models outperform baselines basedon traditional embedding approaches.
Reject. rating score: 3. rating score: 3. rating score: 6. The paper has shown a lot experiment results on basic models, but the raising of Top K algorithm is not novel.<BRK>At the moment, if I understand correctly, the paper is based on theoretical compute savings.<BRK>2.For "Impact on Convergence" in section 3.2, it is not clear to me what the authors are using as a metric for the degree of convergence.<BRK>Training convolutional neural networks (CNNs) is time consuming. However, in the context of training a CNN, they find that eliminating small magnitude components of weight and activation vectors allows us to train deeper networks on more complex datasets versus eliminating small magnitude components of gradients.
Reject. rating score: 1. rating score: 1. rating score: 3. 4.The authors should carry out ablation study for different components of the model. Moreover, it would be much better if the authors can carry out experiments on some widely used recommendation datasets and use standard evaluation metrics for ranking. 2.It is an interesting idea to combine sample similarity together with feature co occurrence for better prediction accuracy. Many descriptions in the paper are not very clear. This should be the feature representation of sample i.<BRK>This paper proposes to combine the graph neural networks and factorization machines. My comments are as follows:  The idea of integrating the GNN and FMs is interesting and intuitive. Will the GNN technique improve the usage of this topological information of the features?<BRK>This paper tries to combine FMs and GNNs to capture both sample and feature interactions.<BRK>However, FMs assume each sample is independently observed and hence incapable of exploiting the interactions among samples. In this work, to leverage their complementary advantages and yet overcome their issues, they proposed a novel approach, namely Deep Relational Factorization Machines, which can capture both the feature interaction and the sample interaction. Finally, they demonstrate the effectiveness of the proposed approach with experiments on several real-world datasets.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. The paper proposes a regularizer for the output representation of transformer NNs, based on the singular value distribution to encourage learning of richer representations and avoid fast decay of singular values previously reported for NNs with softmax outputs. In particular, the embedding matrix is parametrized as the product of a matrix U, a diagonal matrix Sigma and a matrix V. U and V are encouraged towards orhogonality using additional penalties similar to Lagrangian augmentation. Finally, a desired singular value distribution (exponential or polynomial decay) is encouraged by adding an appropriate regularization penalty on the entries of Sigma. The authors present a generalization error bound that relates expected loss, training loss and singular value distribution to motiveate the choice of the regularizer. Experiments are provided for a machine translation and languate modeling, showing mild improvements of the proposed regularziaer over the state of the art baselines. The paper is well written, notation is clearly introduced and used in consistent manner, mathematical derivations are clear and easy to follow.<BRK>Summary: This paper deals with the representation degeneration problem in neural language generation, as some prior works have found that the singular value distribution of the (input output tied) word embedding matrix decays quickly. The authors proposed an approach that directly penalizes deviations of the SV distribution from the two prior distributions, as well as a few other auxiliary losses on the orthogonality of U and V (which are now learnable). The experiments were conducted on small and large scale language modeling datasets as well as the relatively small IWSLT 2014 De En MT dataset. Pros:+ The paper is well written with great clarity. The dimensionality of the involved matrices (and their decompositions) are clearly provided, and the approach is clearly described. The authors also did a great job providing the details of their experimental setup. + The experiments seem to show consistent improvements over the baseline methods (at least the ones listed by the authors) on a relatively extensive set of tasks (e.g., of both small and large scales, of two different NLP tasks). Via WT2 and WT103, the authors also showed that their method worked on both LSTM and Transformers (which it should, as the SVD on word embedding should be independent of the underlying architecture). + I think studying the expressivity of the output embedding matrix layer is a very interesting (and important) topic for NLP. (e.g., While models like BERT are widely used, the actual most frequently re used module of BERT is its pre trained word embeddings.)<BRK>Authors propose to apply Spectrum control regularization to the embedding of weight matrices in NLP problems such as language modeling and neural machine translation. Spectrum Control Regularization was originally proposed and applied to GANs (Jiang et al 2019)The author motivate the approach by showing that the singular values of embedding weight matrices, although I am not convinced that it is such a big issue. In terms of experimental results authors show a very slight improvement over strong baseline models, that further shows an evidence that regularization singular values of embedding matrices is not very important. Overall the paper is written well, however the contribution is very marginal.<BRK>Recent Transformer-based models such as Transformer-XL and BERT have achieved huge success on various natural language processing tasks. However, contextualized embeddings at the output layer of these powerful models tend to degenerate and occupy an anisotropic cone in the vector space, which is called the representation degeneration problem. In this paper, they propose a novel spectrum control approach to address this degeneration problem. The core idea of their method is to directly guide the spectra training of the output embedding matrix with a slow-decaying singular value prior distribution through a reparameterization framework. They show that their proposed method encourages isotropy of the learned word representations while maintains the modeling power of these contextual neural models.
Reject. rating score: 3. rating score: 6. The new graph kernels cover many existing graph kernels as well as their combination and composition as special cases. However, I do have some concerns of papers. 1.My major concern is the soundness of keeping eigenvectors unchanged in the evolution. That is to say, the evolution model does not have enough expressive power to recover the $L^\prime$. (if so, please add a reference)<BRK>This paper proposes a spectral graph neural network based on a graph kernel to predict graph evolution. The major drawback of the paper is the lack of experimentation with real datasets. Based on the results from four datasets they used, the efficacy of their proposed method is unclear. Note: I could not verify the theory in detail yet.<BRK>GSEN can effectively fit a wide range of existing graph kernels and their combinations and compositions with the theoretical guarantee and experimental verification. GSEN has outstanding efficiency in terms of time complexity ($O (n) $) and parameter complexity ($O (1) $), where $n $is the number of nodes of the graph.
Reject. rating score: 1. rating score: 3. rating score: 3. rating score: 6. The paper claims to improve the “clustering ability” of k means by measuring the similarity between samples and centroids by something known as Extreme Value Theory (EVT). The paper in its current form is extremely difficult to follow. I highlight some of the difficulty: 1. 2.The algorithm itself is not clear. Given the above, I am unable to evaluate the contribution of the paper. It will help the paper if the authors clearly formalise what is it that they are trying to achieve with their algorithm. After this, there needs to be a much more clear description of the algorithm. I think the above will also help decide whether section 3.2 on Extreme Value Theory is really required for the discussion.<BRK>The paper considers extending the k means algorithm to allow for finding clusters with non convex shapes. However, there still does not seem to be any statistical significance testing. Further, after reporting the mean and comparing with more methods, the method doesn t seem to perform as well as previously reported. This makes the concern in 2) more prevalent  if the method is not generally applicable, and are likely to help on a problem specific basis, it would be more informative to characterize *when* one might expect the method to perform better, and support this with empirical results. Based on this, I am maintaining my score, but think the work is interesting and encourage the authors to improve on the paper!<BRK>In this paper, the authors propose an improved k means clustering algorithm by using a new similarity measurement method based on extreme value theory. The paper is well written and easy to understand. However there are some concerns:1. There are many clustering methods based on geodesic distance of data points (manifold learning), which are supposed to be better at capturing the non convex data distribution.<BRK>The proposed idea is novel and the paper is well written. There are two concerns which I need the authors to address:1. Since the authors claim that they propose to speed up the computation of the Euclidean distances in k means. However, there is no time cost comparison in the experiments. 2.Some other variants of k means should be added for experimental comparison.<BRK>Unfortunately, it is generally non-trivial to extend k-means to cluster data points beyond Gaussian distribution, particularly, the clusters with non-convex shapes (Beliakov & King, 2006). They thus propose a novel algorithm called Extreme Value k-means (EV k-means), including GEV k-means and GPD k-means. In addition, they also introduce the tricks to accelerate Euclidean distance computation in improving the computational efficiency of classical k-means. Extensive experiments are conducted to validate their EV k-means and online EV k-means on synthetic datasets and real datasets. Experimental results show that their algorithms significantly outperform competitors in most cases.
Reject. rating score: 3. rating score: 3. rating score: 3. However, the experiments on the "novel domains" show that the proposed DoS and DoS Ch works worse than just averaging the outputs of the models in the model pool. The models are diversed by their own modulators. 3) The presentation of the proposed paper should be polished. The overall training process of the proposed method is:(1) Train the base embedding network f_E with the aggregated dataset from multiple domains. Many critical techniques used in this paper are not well explained, such as the ProtoNet. (3) Build a selection network through cross domain episodic training. I would like to discuss the final rating with other reviewers, ACs. The paper also surveys the few shot classification results on a varying number of source domains to show that DoA and DoA ch are robust to deal with different settings. I would recommend the authors to the following materials:a). 3) The paper is applicable to many practical scenarios since the data from the real world application is complicated. ###Cons###1) The critical component of the proposed method is the selection network.<BRK>In this paper, the authors proposed to extend meta learning for few shot classification to the multi domain setting. Specifically, there are three main components of the proposed method in training: 1) a base network, 2) a model pool, and 3) a selection network. The main technical contribution should lie on the selection network, where a key research issue is how to represent different tasks. However, in this work, the authors just simply used the mean of outputs of the base network over all instances of a specific task to represent the task. The authors failed to explain it. In summary, though the problem studied in this paper is interesting, the proposed method is incremental, and has limited novelty contributions.<BRK>In this paper, the authors proposed to address the few shot learning problem, especially for the cross domain setting where a newly coming task originates from a different distribution (or in this work implemented by sampling from an unseen dataset). Basically, the authors constructed a model zoo based on source datasets at hand, and learned an “argmax” meta selector which takes embedding of a task as input and outputs the model selection index. Also, when a new task arrives, will all models be trained from scratch to enforce the feature extractor to also be shared by this new task? In that case, the inference scheme of averaging in Eqn. The third concern comes from the empirical results. It is better to conduct ablation studies to consider those like autoencoder embeddings. o	Could you give more discussion on the results in Table 1? As the number of source datasets increases, an effective meta model of course should contribute more to the target dataset, while it is not in this work.<BRK>Although few-shot learning research has advanced rapidly with the help of meta-learning, its practical usefulness is still limited because most of the researches assumed that all meta-training and meta-testing examples came from a single domain. They propose a simple but effective way for few-shot classification in which a task distribution spans multiple domains including previously unseen ones during meta-training. The key idea is to build a pool of embedding models which have their own metric spaces and to learn to select the best one for a particular task through multi-domain meta-learning. This simplifies task-specific adaptation over a complex task distribution as a simple selection problem rather than modifying the model with a number of parameters at meta-testing time.
Reject. rating score: 3. rating score: 6. rating score: 6. Thus, the paper is not about imitation learning, but rather about an optimization method that reuses data generated from multiple tasks. If the motivation is to use trajectories from suboptimal policies from other tasks without expert knowledge, then I fail to see the motivation and the novelty of this paper. However, this was not realized. The sampled trajectories in the set could be suboptimal for reaching a goal, and there’s little evidence that optimizing J_GCSL(\pi) will learn an optimal policy based on these data. Also, it seems that the algorithm would require human knowledge to discern a trajectory as goal reaching or not, which is contrary to self supervision.<BRK>The intuition behind the algorithm is the goal of an observed trajectory can be identified after the fact, by simply looking at the states reached during that trajectory. GCSL treats each executed action as a sample from the expert policy conditioned on each of the states reached after that action is taken. Experimental results demonstrate superior performance against a base (non goal conditioned) RL algorithm (TRPO), and against another approach to learning goal conditioned polices (TD3 HER), on a relatively diverse set of control problems. A major issue is that the proof of the main theoretical result appears to be wrong. It might be useful to conduct some additional experiments where evaluation is based on the time required to solve a task, rather than just the accuracy of the final state.<BRK>This paper proposes a method to learn to reach goals in an RL environment. The method is based on principles of imitation learning. For instance, beginning with an arbitrary policy that samples a sequence of state action pairs, in the next iteration, the algorithm treats the previous policy as an expert by relabeling its ending state as a goal. I would then recommend it is on the positive side of the borderline. Comments:* The method is interesting but is still an "RL" method. So it is really learning to reach the goal via "RL".<BRK>In this paper, they ask: can they use imitation learning to train effective policies without any expert demonstrations? The key observation that makes this possible is that, in the multi-task setting, trajectories that are generated by a suboptimal policy can still serve as optimal examples for other tasks. Informed by this observation, they propose a very simple algorithm for learning behaviors without any demonstrations, user-provided reward functions, or complex reinforcement learning methods. Although related variants of this approach have been proposed previously in imitation learning settings with example demonstrations, they present the first instance of this approach as a method for learning goal-reaching policies entirely from scratch. They present a theoretical result linking self-supervised imitation learning and reinforcement learning, and empirical results showing that it performs competitively with more complex reinforcement learning methods on a range of challenging goal reaching problems.
Accept (Poster). rating score: 8. rating score: 8. rating score: 3. The paper tries to bring some theoretical foundation to the weakly supervised disentanglement. The authors propose two notions: consistency and restrictiveness, which they don t imply each other. Up until the experiment section, the paper is well written (although a bit verbose). Then are you suggesting this as a metric of evaluation?<BRK>This paper first discusses some concepts related to disentanglement. The authors propose to decompose disentanglement into two distinct concepts: consistency and restrictiveness. Then, a calculus of disentanglement is introduced to reveal the relationship between restrictiveness and consistency. The proposed concepts are applied to analyze weak supervision methods. This paper is well structured.<BRK>StrengthsThe framework uses two simple concepts, consistency and restrictiveness for both generator and decoder. It also gives rise to a calculus. WeaknessesThe paper does not propose effective methods for disentanglement in the weak supervision setting.<BRK>To address this issue, they provide a theoretical framework to assist in analyzing the disentanglement guarantees (or lack thereof) conferred by weak supervision when coupled with learning algorithms based on distribution matching. They empirically verify the guarantees and limitations of several weak supervision methods (restricted labeling, match-pairing, and rank-pairing), demonstrating the predictive power and usefulness of their theoretical framework.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. The paper is clearly written. It is also nice that the paper provides a good relation (with explanation) between this signal and the frequency of the value function.<BRK>Summary:This paper basically built upon [1]. What kind of norm are you using? The authors only evaluate their algorithm in one environment, MazeGridWorld.<BRK>What are your input features? I think this paper is currently one or two major revisions away from the acceptance threshold. Update Nov 17: most of my concerns have been addressed, and I have thus increased my rating.<BRK>Search-control is critical in improving learning efficiency. In this work, they propose a simple and novel search-control strategy by searching high frequency regions of the value function.
Reject. rating score: 3. rating score: 3. In this paper, the authors study the adversarial example generation problem, in the difficult case where the attacked model is a black box. For handling the sensibility to starting points, the authors propose a meta algorithm, which uses any iterative local update based attacks, and which maintains a set of solutions corresponding to different starting points. In the experiments, the meta algorithm uses SignOPT attack. It is compared with three decision based attacks, including SignOPT. However, I understand that this could be tricky.<BRK>This paper proposes a meta algorithm for the so called "decision based attack" problem, where a model that can be accessed only via label queries for a given input is attacked by a minimal perturbation to the input that changes the predicted label.<BRK>In this paper, they consider hard-label black- box attacks (a.k.a.decision-based attacks), which is a challenging setting that generates adversarial examples based on only a series of black-box hard-label queries. This type of attacks can be used to attack discrete and complex models, such as Gradient Boosting Decision Tree (GBDT) and detection-based defense models. To remedy this issue, they propose an efficient meta algorithm called BOSH-attack, which tremendously improves existing algorithms through Bayesian Optimization (BO) and Successive Halving (SH).
Accept (Poster). rating score: 8. rating score: 8. rating score: 3. The reviewer is well aware that a single non sparse column can kill the performance of the sparse matrix multiplication algorithm, and this is probably the main reason this algorithm has not found broad usage. For lack of space, one has to through quite few references to fully understand the experiments. Quality: there are only few equations in this paper, but they rely on excellent notation.<BRK>Summary This paper focuses on learning a representation that facilitates efficient content based retrieval. It is at first a bit unclear what the problem is for newcomers5. The novelty of this algorithm is its focus on minimizing the number of FLOPs in computing queries of instances, taking note as well of the role of the distribution of non zero values in determining the number of FLOPs. Based on experiments, the paper claims that the proposed algorithm yields a similar or better speed vs. recall tradeoff compared to baselines. The presentation of the distribution that minimizes FLOPs is convincing, and there is easy to follow buildup into the continuous relaxation of the FLOPs minimization problem. The recall/time trade off curves in figure 3 support the main empirical claim of the paper. Did it depend on the algorithm used? Minor comments that did not affect the rating1.<BRK>This paper proposes to learn sparse representation in neural networks for retrieval in large database of vectors. However, in general, sparsity leads to irregular memory access patterns, so it may not actually result in a speedup;" This has been my experience as well. Ideally one would hope to not need two inference runs (for sparse and dense) and keep two database (sparse and dense). For comparison dense ANN, Faiss’s IVF PQ is a relatively dated pipeline. Also dense ANN can also greatly benefit from the use of batching, which is not considered for this paper. But I still worry that the Megaface task somehow allow representation to be much more sparse than other NLP/CV tasks.<BRK>Deep representation learning has become one of the most widely adopted approaches for visual search, recommendation, and identification. In this work, in contrast to learning compact representations, they propose to learn high dimensional and sparse representations that have similar representational capacity as dense embeddings while being more efficient due to sparse matrix multiplication operations which can be much faster than dense multiplication. Their experiments show that their approach is competitive to the other baselines and yields a similar or better speed-vs-accuracy tradeoff on practical datasets.
Reject. rating score: 1. rating score: 3. rating score: 3. Summary:The paper presents an empirical study of causes of parameter divergence in federated learning. Federated learning is the setting where parameter updates (e.g.gradients) are computed separately on possibly non IID subsamples of the data and then aggregated by averaging. I recommend that the paper be rejected. (This is not my area so I don t know). As for network architectures, it would be valuable to look at a greater variety of standard architecture styles (e.g.ResNet, Inception, etc). I do realize this is a lot of experiments to do.<BRK>However, there are some weakness in this paper:1. 2.This paper only raises some issues in federated learning with non IID data, and discusses the potential causes. 3.Since this is nearly a pure empirical paper, I hope the authors can make the experiments thorough. The authors only studies Nesterov momentum in this paper. However, due to the heterogeneous setting, it is very likely that different workers have different numbers of local data samples, which could be another source of divergence.<BRK>The paper experimentally studies the reasons for the slow convergence of the Federated Averaging algorithm when the data are non iid distributed between workers in the multiclass classification case. The learning rate is constant throughout all of the experiments, depending only on the optimizer, but not on the neural network architecture. Concerns and questions that should be addressed:1. Where was this observed? 3.Why the divergence of parameters is considered only at the last layer? 8.Why for different experiments different baseline models are used?<BRK>However, despite the advantages, the federated learning-based methods still have a challenge in dealing with non-IID training data of local devices (i.e., learners). (ii) They then revisit the effects of optimizers, network depth/width, and regularization techniques; their observations show that the well-known advantages of the hyperparameter optimization strategies could rather yield diminishing returns with non-IID data.
Reject. rating score: 1. rating score: 1. rating score: 6. The authors propose an "uncertainty adaptation training scheme" (UATS) that describes the uncertainty of the neural network in the training process. Overall the quality of the presentation and the exposition in the paper is poor.<BRK>The paper is hard to read and there are syntactic errors as well as  issues with the grammar. From my point of view, this paper is a clear rejection. I would encourage the authors to be explicit about their contribution and the intellectual products of this work.<BRK>This paper proposes a way of training neural nets on analog circuit based chips, which are cursed with uncertainties. Such uncertainties are deeply rooted in the way neural nets are implemented on such chips. Overall, this paper touches upon an important research problem towards running neural nets on neuromorphic computing chips, which is how to deal with the underlying uncertainties. The proposed algorithm is reasonable and the experimental results look encouraging. However, I would like to ask a few clarification questions. Given authors’ response, I will be willing to adjust my score.<BRK>The crossbar-based neuromorphic computing chips, in which the computing is mainly performed by analog circuits, have the uncertainty and can be used to imitate the brain. However, most of the current deep neural networks have not taken the uncertainty of the neuromorphic computing chip into consideration. Therefore, their performances on the neuromorphic computing chips are not as good as on the original platforms (CPUs/GPUs). In this work, they proposed the uncertainty adaptation training scheme (UATS) that tells the uncertainty to the neural network in the training process.
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. This paper aims at exploring the properties of neural network training during the early phase. 2.The paper explores what is more important for the early phase of training: signs of the weights or magnitude of the weights. This paper studies the properties of deep neural networks. Some questions/suggestions to make the paper clearer:1. 2.The results of Fig.4 and Fig.6 can be inconsistent.<BRK>The paper performs exhaustive experiments in the early phase of network training. And it has some interesting implications for lottery tickets. The results in the paper are aligned with my intuition that the weights in the early stage are highly dependent and they share some similarities in the distribution level. They also show that the perturbations can be roughly be approximated by adding Gaussian noise to network weights.s3. This is a weak accept.<BRK>Pros+ This work provided a good summary of observations and network properties that worth studying during the early stage of network’s training. + The authors conducted extensive and detailed experiments to study the statistics of weights and their gradients. Most experiments are designed for previous observations and mostly for verification purpose.<BRK>Recent studies have shown that many important aspects of neural network learning take place within the very earliest iterations or epochs of training. They find that, within this framework, deep networks are not robust to reinitializing with random weights while maintaining signs, and that weight distributions are highly non-independent even after only a few hundred iterations. Despite this, pre-training with blurred inputs or an auxiliary self-supervised task can approximate the changes in supervised networks, suggesting that these changes are label-agnostic, though labels significantly accelerate this process. Together, these results help to elucidate the network changes occurring during this pivotal initial period of learning.
Reject. rating score: 1. rating score: 6. rating score: 6. The paper studies the mean and variance of the gradient norm at each layer for vanilla feedforward, ResNet and DenseNet, respectively, at the initialization step, which is related with Hanin & Ronick 2018 studying the mean and variance of forward activations. Thus I vote to reject this paper. 1 also reveals a surprising property of the gradients in general Relu networks. That is, when the weights are sampled from the same distribution in each layer, the gradient’s magnitude for each layer are equal in expectation, and depend only on the output statistics."<BRK>This paper studies the effects of residual and dense net type connections on the moments of per layer gradients at random initialization. In particular, using duality, bounds on the variance of the square norm of Jacobian (with respect to the randomness of random initialization) are derived for vanilla networks. In particular, with properly chosen initialization scales for each layer, the architectures with skip connections can be initialized so that the gradient norm does not explode with increasing depth. The results in this paper seem to be useful for guidance on choosing proper initialization schemes for different architectures at different depths.<BRK>This paper analyzes the statistics of activation norms and Jacobian norms for randomly initialized ReLU networks in the presence (and absence) of various types of residual connections. Whereas the variance of the gradient norm grows with depth for vanilla networks, it can be depth independent for residual networks when using the proper initialization.<BRK>In this work, they study the connection between the network's architecture and initialization parameters, to the statistical properties of the gradient in random fully connected ReLU networks, through the study of the the Jacobian. They compare three types of architectures: vanilla networks, ResNets and DenseNets. The later two, as they show, preserve the variance of the gradient norm through arbitrary depths when initialized properly, which prevents exploding or decaying gradients at deeper layers. In addition, they show that the statistics of the per layer gradient norm is a function of the architecture and the layer's size, but surprisingly not the layer's depth. This depth invariant result is surprising in light of the literature results that state that the norm of the layer's activations grows exponentially with the specific layer's depth.
Reject. rating score: 3. rating score: 3. rating score: 3. rating score: 6. Summary: This paper proposes AE GAN+sr, an auto encoder based GAN for equipping neural networks with better defenses against adversarial attacks. The authors evaluate their method on black box attacks, white box attacks, and gray box attacks on MNIST and Fashion MNIST, and show decent empirical results when compared to baselines. When you use gradient descent to find the best encoding for a corrupted input, how many steps do you need to take? Given that the BiGAN also incorporates an encoder (and they were also benchmarked against in the experiments), where do the advantages of the AE GAN+sr come from?<BRK>The paper combines an auto encoder (AE) based approach to correct the adversarially perturbed samples with a GAN based approach for the same task. First, the autoencoder is used to detect if an input sample is from the natural image distribution or adversarial distribution. In the latter case, Defense GAN is employed that uses gradient descent in the latent space, with encoder output as the initialization, to find a natural or non adversarial counterpart of the input sample. The autoencoder is also equipped with an adversarial loss such that the decoder produces realistic images. It is not clear why the proposed method performs better on stringer white box attacks compared to much weaker black box attacks.<BRK>The paper aims to refine DefenseGAN (ICLR 18), where an autoencoder is used to initialize the search for projecting an adversarial examples to the manifold of real examples. The main contribution is to reduce the computational cost of DefenseGAN, the claim is "by an order of magnitude".<BRK>The gradient based search obtains a latent code that corresponds to the reconstructed image that is closest to the input. The algorithm uses output of the decoder as a starting point in the search. Experiments show a reasonable performance against adversarial attacks on MNIST and F MNIST images. The ease of optimization in the proposed method comes at the cost of accuracy (Table 1).<BRK>In the inference time, when given an input, they will start a search process in the latent space which aims to find the closest reconstruction to the given image on the distribution of normal data. The encoder can provide a good start point for the searching process, which saves much computation cost. Experiments show that their method is robust against various attacks and can reach comparable even better performance to similar methods with much fewer computations.
Reject. rating score: 6. rating score: 6. rating score: 6. The paper proposes a meta learning approach to learn reward functions for reinforcement learning agents. The meta learning algorithm and the corresponding empirical investigation are the main contributions of the paper. Detailed remarks:* The main addition to the meta learning algorithm is the lifetime value function. Most of the approaches discussed in related work (e.g.shaping)  are aimed at learning/designing more informative reward functions. The intrinsic reward functions used in this paper map the full life time history of the agent to rewards. These examples do show potential benefits of meta learning intrinsic rewards, but I was somewhat disappointed that there was no more systematic investigation.<BRK>(Originally my score was a weak reject.) This paper aims to study whether a learned reward function can serve as a locus of knowledge about the environment, that can be used to accelerate training of new agents. However, given this motivation, it would be important to see comparisons between the proposed method of learning intrinsic rewards, and other methods for fast adaptation in the literature, such as MAML, which as I understand also has many of the properties highlighted in this paper. My main qualm with the paper is with its significance   the authors claim that the goal is to find out whether reward functions can be loci of knowledge, but we already know the answer is yes: the whole point of reward shaping is to improve training dynamics by building in knowledge into the reward function.<BRK>Summary The paper evaluates the intrinsic reward as a way of storing information about episodes. Interestingly, a state based reward function also generalizes to agents with perturbed action spaces, showing that this way of storing information is agnostic to the agent’s action space. I recommend marginal accept. Here, I will focus on scientific questions, answering which would significantly improve the quality of the paper. Metz’19, Meta Learning Update Rules for Unsupervised Representation Learning, is a conceptually relevant work that proposes to meta learn loss functions for unsupervised learning (and there is more recent related work on this topic too).<BRK>As far as the learning algorithm is concerned, these rewards are typically given and immutable. In this paper they instead consider the proposition that the reward function itself may be a good locus of knowledge. This is consistent with a common use, in the literature, of hand-designed intrinsic rewards to improve the learning dynamics of an agent.
Reject. rating score: 3. rating score: 3. rating score: 3. This paper proposes a model that learns to disentangle visual scene into objects (slots), and simultaneously learns a dynamics model to capture how these objects interact with each other. This paper is for sure studying an important problem. The major problem of this manuscript, to me, is its ignorance of related work and, therefore, overclaiming at a few places. I d like to see the authors  responses regarding the missing related work.<BRK>This paper introduces a model that learns a slot based representation, along with a transition model to predict the evolution of these representations in a sparse fashion, all in a fully unsupervised way. This work tackles an important problem, and is very well motivated and presented in a very clear fashion.<BRK>3b) Sec 4.2 argues that this slot based representation can help exploration, but this is in fact a chicken and egg problem, as one needs to have collected interesting transition samples for a good representation to emerge. This paper proposes to use a ‘slot based’ (factored) representation of a ‘scene’ s.t. However, while the overall approach is intuitive and seems to yield desirable results, I have concerns regarding the experiments, comparisons to prior work, and the exact contributions of this work. I feel any prediction model with some structure e.g.graph based forward model, convolutional forward model etc.<BRK>Learning an agent that interacts with objects is ubiquituous in many RL tasks. They introduce SPECTRA, a model for learning slot-structured transitions from raw visual observations that embodies this sparsity assumption. They show that learning a perception module jointly with a sparse slot-structured transition model not only biases the model towards more entity-centric perceptual groupings but also enables intrinsic exploration strategy that aims at maximizing the number of objects changed in the agent ’ s trajectory.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. It may help the presentation to more frequently demonstrate the general concepts with examples, though doing so may be in conflict with the general nature of the paper s theoretical contribution. This paper aims to1) define a general notion of compositionality as equivariance,2) build a model which is compositional in this general sense, and3) apply the model to SCAN. The proposed models are also new, interesting, and could be significant. Originality: The general notion of equivariant neural networks and good performance on SCAN are novel. Significance: As discussed in the Weaknesses section this could turn out to be very significant or not significant at all, but that s true for a lot of good research. While neither of these issues can really be solved, I think paper could be substantially better in both aspects.<BRK>However, effects requiring global equivariances like learning relationship between "twice" and "thrice", or learning relationships between different kinds of conjunctions are not handled in this work. While this paper does show that modelling effects of word substitution can be handled by the locally equivariant functions, it still cannot account for more complex generalization phenomena which are likely to be much more prevalent especially for domains dealing with natural language that are other than SCAN.<BRK>Section 4 is far less clear. This section should be significantly improved in order for me to increase my score. * Specific comments which I hope the authors address:1. 3.Is the whole model G equivariant? The authors might want to clearly state this. 4.It might be helpful for readers that are not familiar with deep learning for NLP tasks to provide a visualization of the full model (can be added to the appendix)5.<BRK>Humans understand novel sentences by composing meanings and roles of core language components. The main contribution of this paper is to hypothesize that language compositionality is a form of group-equivariance. Throughout a variety of experiments on the SCAN tasks, they analyze the behavior of existing models under the lens of equivariance, and demonstrate that their equivariant architecture is able to achieve the type compositional generalization required in human language understanding.
Reject. rating score: 1. rating score: 3. rating score: 3. The authors introduce a parameterized activation function to learn activation functions that have sigmoidal shapes that can be used in LSTMs. I am not familar with this stock prediction dataset, but the differences shown are often less than 1%. This paper is simply proposing another one and showing little to no improvement.<BRK>This paper proposed combined form of flexible activation functions with carefully designed principle of choosing activation functions. For example, "flexible is 0.032" does this parameter generalize to other dataset? (2) the paper has two benchmark, stock price prediction and CIFAR 10.<BRK>This paper presents a family of parameterized composite activation functions, and a regularization technique for parameterized activation functions in general. The regularization experiments use LeNet 5, which is not a compelling benchmark architecture with respect to contemporary practice. The effects of regularization techniques can be very different in different regimes of dataset and network size.<BRK>Based on this, they develop two novel flexible activation functions that can be implemented in LSTM cells and auto-encoder layers.
Reject. rating score: 3. rating score: 6. rating score: 6. The real benefit is the compressed model but that is also achievable by traditional pruning (Han et al.). The paper proposes a method to train a sparse network and achieve "dense level" performance. The method redistributes the sparsity according to momentum contribution of each layer during training after each epoch. 1.My biggest concern is about the experiments. The estimate of speedup might be flawed. After author rebuttal:Thank you for the response. The authors view that their method is not comparable with Han et al.15. because this work started training a sparse model while Han et al.15 trains a dense model and then do pruning. I m ok with other parts of the rebuttal and I will raise my score to weak reject. In their paper s Table 2, the VGG like model achieve ~0.3% error reduction at 5% weights while in this paper s Table 1 it s a 0.5% error increase. 3.The benefit of the method seems unclear. The test time real speedup is also limited according to Table 3.<BRK>2.I would request the authors to slightly rewrite certain parts of their paper so as not to imply that momentum decreases the variance of the gradients in general. I am, however, a bit confused with how the sparse momentum algorithm is discussed in this paper. The proposed algorithm looks interesting and seems to empirically work well. I do not agree that keeping the learning rate fixed across methods is the right approach.<BRK>This paper proposes an algorithm called Sparse Momentum for learning sparse neural networks. The provides a decent motivation for why sparse networks can be helpful. They compare their method with other methods that also maintain sparse neural networks throughout training and involve single training phase, which is fair. The authors compare speed up results for training in two ways: theoretical speedups which are proportional to reduction in number of FLOPS and practical speedups using dense convolutional algorithms corresponding to completely empty channels. Overall, I think the paper proposes an interesting idea of using momentum with promising results to learn sparse neural networks.<BRK>They demonstrate the possibility of what they call sparse learning: accelerated training of deep neural networks that maintain sparse weights throughout training while achieving dense performance levels. They accomplish this by developing sparse momentum, an algorithm which uses exponentially smoothed gradients (momentum) to identify layers and weights which reduce the error efficiently. Sparse momentum redistributes pruned weights across layers according to the mean momentum magnitude of each layer. Within a layer, sparse momentum grows weights according to the momentum magnitude of zero-valued weights. In their analysis, ablations show that the benefits of momentum redistribution and growth increase with the depth and size of the network.
Reject. rating score: 1. rating score: 1. rating score: 3. This papers proposed a solution to the problem of disease density estimation using satellite scene images. The authors claim that this approach alleviate the need of a post classification smoothing.<BRK>The discussion is not up to the level of ICLR and offers mostly guesswork. The method is applied to tasks of estimating crowding population, and diseases density, from satellite images. The paper have little novelty. This seems arbitrary and there are many competing approaches that could be applied.<BRK>The authors propose a method to extract features utilizing the adjacency between patches, for better classification/regression of satellite image patches. I have several significant concerns:  In the abstract, the authors claim that existing approaches such as post classification add computational overhead to the task, whereas the proposed method does not add significant overhead. I understand that the authors cannot compare to everything. But the authors should compare to representative baseline methods. Given the ICLR community s interest in general methods that can be applied to (or already been tested on) multiple applications, the paper would have been stronger if the methods applicabilityto other domains was discussed (and even better demonstrated).<BRK>Their approach utilizes a siamese network to improve the discriminative power of convolutional neural networks on a pairof neighboring scene images. For example, their model improved prediction accuracy by 1 percentage point and dropped the mean squared error value by 0.02 over the baseline, on a disease density estimation task.
Reject. rating score: 1. rating score: 6. rating score: 6. * I found the experiments either not related to the point of the paper or being very trivial not helping to backing up the arguments of the paper. * Paper summary: The paper proposes a framework to evaluate machine learning models in a hardware agnostic way. * Another crucial problem is that to allow a fair comparison especially in neural models, as shown in several studies(see [1] as a sample), this is important to account for random seeds and study how it impacts the model performance, to allow a fair evaluation of the models this is important to consider this factor, fair evaluation of models is argued to be the main point of this paper, however, the authors does not consider this factor in the paper, nor study it in the experiments.<BRK>The paper presents a unified approach to specify, evaluate and benchmark different ML methods. This benchmarking offers several insights on the evaluation of a given ML model, by stressing out the importance of aspects that can severely bias  the final outcome of the model (e.g., pre processing tasks, different hardware configurations or normalization of the data). To describe the workflow, authors use an image classifier on a given hardware as a running example, and play with different  preprocessing methods to measure their impact on the final accuracy of the model. Some details are not well specified/clear in the work:1) Data exploitation. Given that the deployment is run on Amazon instances, what are the requirements (e.g., data must be on S3 and so on). Is this server  part of the platform?<BRK>Generally, I believe that the work is well motivated and timely, the authors seem to have done a good job in citing related work (though admittedly I don t know much about this area), and the results are supportive of the claims of the system s usefulness. This paper studies the question of model evaluation and reproducibility in machine learning research, specifically deep learning research, and designs and tests an extensive system for evaluating and comparing models. Wouldn t this be most interesting at training time? In section 3.1, the authors write "The hardware details are not present in the manifest, but are user provided options when performing the evaluation."<BRK>The complicated procedures for evaluating innovations, along with the lack of standard and efficient ways of specifying and provisioning ML/DL evaluation, is a major "pain point" for the community. This paper proposes MLModelScope, an open-source, framework/hardware agnostic, extensible and customizable design that enables repeatable, fair, and scalable model evaluation and benchmarking.
Reject. rating score: 1. rating score: 3. rating score: 6. This activation function is constructed as a piecewise linear function that is learned concurrently with training, and, in the case of S APL, the activation function is forced to be symmetric. For these results to be interpretable, the authors should include a detailed description of the environment under which the experiments were performed, and present confidence intervals which demonstrate the significance of the improvement attained by S APL. 3.The adversarial evaluation section should be substantially revised to address several important flaws:(a) The evaluation for black box attacks is done in very non standard threat models (e.g.label only 3 pixel black box attacks) that do not seem to be relevant to the black box robustness of a system. Even under these threat models, the authors should also use more powerful label based black box attacks, such as the BoundaryAttack [1] or the label only attacks in [2] or [3]. (c) The open box (white box) setting only uses FGSM to evaluate the robustness, which is known to be a weak attack [4] and is recommended against for evaluating adversarial robustness [5]. (d) Overall, the results presented are not sufficient to evaluate the adversarial robustness of the S APL activation.<BRK>It is an extension of the APL activation, but is symmetric w.r.t.x axis.It also has more linear pieces (actually S pieces, where S can be arbitrarily large) than the existing activation functions like ReLU. The authors also show that neural networks with the proposed activation can be more robust to adversarial attacks. However, the gain is marginal as shown in Table 1. Secondly, the neural networks used in the experiments are quite outdated. The results could be more convincing. In a word, the proposed activation function is unnecessarily complicated and the gain is not justified with the latest models and not significant enough to convince people to adopt it.<BRK>This paper proposes a learnable piece wise linear activation unit whose hinges are placed symmetrically. This relates to the problem 1). 3) Experimental conditions are not clear. The robustness to adversarial attacks is also empirically examined. Please cite the papers which describe the architecture of the models used in the experiments. 4) On the sensitivity of optimization on the initial value. It would be interesting to see experimental results with "initialized with trained S APL" and "S APL positive with non zero initial value". However, there are several things to be addressed for acceptance. It is important to see comparisons with other activations such as the plain APL. However, there are some discussion after that which changes or restricts the equation. 2) Theorem 3.1 does not seem to prove the approximation ability of S APL. I think it needs to prove that h(x, S) can approximate arbitrary piecewise linear function (i.e., g(x, S)) if you want to prove the approximation ability of h(x, S).<BRK>The adaptability of learnable activation functions adds expressive power to the model which results in better performance. Here, they propose a new learnable activation function based on Adaptive Piecewise Linear units (APL), which 1) gives equal expressive power to both the positive and negative halves on the input space and 2) is able to approximate any zero-centered continuous non-linearity in a closed interval. Finally, they show that the use of Symmetric-APL activations can significantly increase the robustness of deep neural networks to adversarial attacks. Their experiments on both black-box and open-box adversarial attacks show that commonly-used architectures, namely Lenet, Network-in-Network, and ResNet-18 can be up to 51% more resistant to adversarial fooling by only using the proposed activation functions instead of ReLUs.
Reject. rating score: 1. rating score: 1. rating score: 1. I think this paper is not enough to accept in ICLR because  Lack of novelty. Other datasets, e.g., CIFAR100 and ImageNet, should be demonstrated. Need comparison with other augmentation methods, e.g., Mixup, CutMix, AutoAugment.<BRK>[1] Devries and Taylor. Improved regularization of convolutional neural networks with cutout, ArXiv 2017. The method description is not specific.<BRK>better exposition needed   it would have been helpful to include more examples of the original methods CutOut and Sample pairing.<BRK>CopyPairing reduces the test error rate by 11.97% compared with Cutout and 8.21% compared with SamplePairing. In this work they present two improvements of the state-of-the-art Cutout and SamplePairing techniques. The second technique they discovered is called CopyPairing.
Reject. rating score: 3. rating score: 6. rating score: 8. This paper proposes a fractional graph convolutional networks for semi supervised learning. Experimental results show that the proposed method (FGCN) shows the best accuracy compared to other recent graph based neural networks for all datasets except one. What is the intuition of the optimization of GSSL? However, this idea is too incremental and applying the classification function to graph filter is very trivial.<BRK>This paper presents a fractional generalized graph convolutional networks for semi supervised learning. Experimental results on multiple graph datasets are reported and discussed. Cons.1.The proposed method contains three major components: parallel FGS convolution, pooling, and residual block. Although some justifications are provided for such designs, it is difficult to justify the role of each component. In other words, it is unclear whether the performance gain is from the parallel structure, or the residual block. 2.Many recent methods on graph neural networks are not discussed or included as baselines, such as [a b].<BRK>It is based on a novel fractional filter for graph conv networks, which generalizes several previously employed graph semi supervised learning frameworks, by introducing a fractional hyperparameter (sigma in the paper), using fractional powers of the Laplace operator. The relevant previous work in the area seems to be cited, and the paper appropriately embedded in the previous workEmpirically, the method outperforms several established baseline models (classical and neural) on standard datasets in the node classification task. I believe the experimental results could justify an accept, but I would not claim I am an expert in semi supervised learning on graphs.<BRK>Due to high utility in many applications, from social networks to blockchain to power grids, deep learning on non-Euclidean objects such as graphs and manifolds continues to gain an ever increasing interest. In this paper they propose a new Fractional Generalized Graph Convolutional Networks (FGCN) method for semi-supervised learning, which casts the L\'evy Fights into random walks on graphs and, as a result, allows to more accurately account for the intrinsic graph topology and to substantially improve classification performance, especially for heterogeneous graphs.
Reject. rating score: 3. rating score: 6. In this paper, the authors aim to learn a locally interpretable model via the reinforcement learning approach, to address the fundamental challenge which is that the previous locally interpretable model has smaller representation capacity than black box models, and causes under fitting with conventional distillation techniques. With your given RL like objective function, it seems that the state transition is from features to features. However, there is no specific correlated explanation in your paper on why you make such an assumption. To sum up, I don’t think the proposed method is RL based, it would be more appropriate to define it as a MAB problem, and this paper should solve this problem before publishing.<BRK>In particular, they study local interpretable models, which are used to study interpretability at the level of one or a few data points. A key challenge is that in order for local models to be interpretable, they need to be simple in form and therefore lower in capacity (i.e.linear); thus, if they are trained on entire datasets they will underfit. In particular, they propose to use RL to learn to weight instances from datasets; RL is required as they make hard (i.e.non differentiable) decisions to select a subset of the dataset. This work is closely related to Ren et al.2018 [1], which proposes to meta learn how to weight samples in a batch so as to maximize performance on a validation set. The contribution of this paper is thus the use of RL for meta learning how to subsample a larger dataset in order to maximize some validation loss. * This is a minor issue, but this pushes the burden of interpretability further up to the black box sample weighting function.<BRK>However, developing globally interpretable models that explain the behavior of the entire model is challenging. An alternative approach is to explain black-box models through explaining individual prediction using a locally interpretable model. In this paper, they propose a novel method for locally interpretable modeling--Reinforcement Learning-based Locally Interpretable Modeling (RL-LIM). RL-LIM near-matches the overall prediction performance of black-box models while yielding human-like interpretability, and significantly outperforms state of the art locally interpretable models in terms of overall prediction performance and fidelity.
Accept (Talk). rating score: 8. rating score: 8. rating score: 8. The authors propose warped gradient descent (WarpGrad) an optimisation framework for facilitating gradient based meta learning.<BRK>This paper proposes a learning strategy to precondition gradients for meta learning.<BRK>Decision:This is a good paper which proposes an interesting generalization of previous gradient based meta learning methods like MAML and T Net, with an impressive number of experiments. It would help to develop this.<BRK>In this work, they propose Warped Gradient Descent (WarpGrad), a method that intersects these approaches to mitigate their limitations.
Reject. rating score: 3. rating score: 6. rating score: 6. The paper introduces foveabox, a method that performs "keypoint" like object detection   instead of "anchor" based detection (to be discussed later). The idea is simple: predict class labels for pixels that fall within (a reduced version) the GT boxes of the instance; and predict bounding box offsets for those positive pixels. The idea is built on top of the FPN backbone, where a set of feature maps (each representing a specific scale) are used to detect object boxes in multiple scales. The method is mainly compared against RetinaNet (which is "anchor" driven), and also compared against other more recent methods (ExtremeNet, CenterNet, FCOS) etc. + The paper is quite well written and structured, the illustrations are also clear;+ I have also read its previous version, and the new version has added a significant amount of work improving it   e.g.added feature alignment and group norm;+ I haven t fully checked the results section of other concurrent papers for full comparison, but the current results are among the state of the art for one stage detectors. I don t think the paper breaks away from the notion of "anchors".<BRK>This paper introduces an anchor free object detection framework that aims at simultaneously predicting the object position and the corresponding boundary. To achieve this, the proposed FoveaBox detector predicts category sensitive semantic maps for the object existing possibility, and  produces category agnostic bounding box for each position that is likely to contain an object. Experiments are performed on MS COCO detection benchmark. The paper is well written and easy to follow. Cons:The main issue with the paper is the main idea is similar to [1,2, 3, 4, 5]. Without a fair speed comparison and with similar detection performance, it is difficult to fully assess the merits of the proposed approach. However, a proper and detailed comparison with [1, 2, 3, 4, 5] is missing.<BRK>So this paper is solving an interesting problem and seems novel. ## Summary of the contribution:1. The paper proposed an object detection approach called FoveaBox that does not rely on anchors (sliding widows). 3.The paper shows that FoveaBox can also be used for object proposals by changing the classification target to class agnostic head. The problem seems not very well studied in the existing object detection literature. So my position is not strong. Figure 4 seems to be doing that illustration but giving only the tensor shape seems quite confusing. I also listed some questions below about the actual algorithm. 3.The scale is still discretized. How would the proposed approach address the issue when there are multiple bounding boxes around the same pixels? It seems the current approach is predicting a box per pixel?<BRK>While almost all state-of-the-art object detectors utilize predefined anchors to enumerate possible locations, scales and aspect ratios for the search of the objects, their performance and generalization ability are also limited to the design of anchors. Instead, FoveaBox directly learns the object existing possibility and the bounding box coordinates without anchor reference. This is achieved by: (a) predicting category-sensitive semantic maps for the object existing possibility, and (b) producing category-agnostic bounding box for each position that potentially contains an object. They demonstrate its effectiveness on standard benchmarks and report extensive experimental analysis. They believe the simple and effective approach will serve as a solid baseline and help ease future research for object detection.
Reject. rating score: 3. rating score: 3. rating score: 6. Originality:CCA is a generative model that learns a shared subspace based on  two (or multi) views of the data. 1).The novelty of the proposed approach is limited. 2).The experimental setup and results are sound but some of the tasks seem contrived to show the improved performance of TOCCA methods. What s the motivation for this setup? I hope I am not missing something. Quality:The paper is technically sound, though it is a trivial extension of a previous method. The experimental setup is somewhat contrived to show the superiority of the proposed method. Significance:The paper solves an important problem by infusing discriminative power into generative subspaces learned by CCA but the results are not that important in my eyes.<BRK>This is a interesting paper on an important topic, but it was a main weakness: it assumes that the reader is deeply familiar with the CCA. In order to make the paper more accessible to a general audience, the authors should:1) have at least one sentence in the abstract that explains in layman terms why is CCA important and how it works ("multi view learning" does not suffice); given that you have the term "multi view learning" in the title, you should explain what it is and how it can benefit from CCA2)  re organize the current intro, which reads more like related work, into a more traditional format       one intuitive paragraph on what is multi view learning (MVL), what is CCA, how does CCA help MVL        one intuitive paragraph on an illustrative example on how MVL & CCA help solving a problem       one intuitive paragraph on how the proposed approach works       one paragraph summarizing the main findings/results 3) ideally, add a section with an illustrative running example, which would have a huge impact on the paper s readability (far more than, say, than the current Appendix)<BRK>The problem is timely and important as it is challenging to perform CCA jointly with the task classification (see below) and hence previous work typically perform this in a pipeline   that is, first projecting the data using a pre trained CCA and then training a task classifier using the projected representation. As the authors note, this may be problematic as CCA may delete important information that is relevant for the classification, if training is not done jointly. As the authors note, the main challenge in developing a task optimal form of deep CCA that discriminates based on the CCA projection is in computing this projection within the network. This is an empirical paper in the sense that no guarantees are provided for the proposed techniques. The experiments are thorough, convincing and the span a range of applications. The results demonstrate the value of the proposed approximations, and I hence recommend a weak accept of the paper.<BRK>Canonical Correlation Analysis (CCA) is widely used for multimodal data analysis and, more recently, for discriminative tasks such as multi-view learning; however, it makes no use of class labels. Recent CCA methods have started to address this weakness but are limited in that they do not simultaneously optimize the CCA projection for discrimination and the CCA projection itself, or they are linear only. They address these deficiencies by simultaneously optimizing a CCA-based and a task objective in an end-to-end manner. Their method shows a significant improvement over previous state-of-the-art (including deep supervised approaches) for cross-view classification (8.5% increase), regularization with a second view during training when only one view is available at test time (2.2-3.2%), and semi-supervised learning (15%) on real data.
Reject. rating score: 3. rating score: 3. rating score: 6. The authors propose to learn an LSTM based decoder to output the action $a_t$ by greedily prediction one word at a time. They achieve this by training a sequence to sequence model on trajectories collected by running the game using a previously proposed exploration method (Go Explore). While the results are promising, there might be limited novelty beyond training a sequence to sequence model on pre collected trajectories. Pros:1.Nice idea for tackling the unbounded action space problem in text based games. Cons:1.The method depends on the assumption that we can get a set of trajectories with high rewards. Since the Go Explore method assumes access to extra trajectories at the start, this doesn t seem fair to the other baselines which may not observe the same high reward trajectories. Do you use the game rewards to train/finetune the seq2seq model or is it only trained in a supervised fashion on the trajectories?<BRK>It is shown to outperform existing solutions in solving text based games with better sample efficiency and stronger generalization ability to unseen games. And it is trained on the high reward trajectories obtained with Go Explore method using imitation learning. From this perspective, there is not much novelty in this paper. It seems to be just standard imitation learning of seq2seq model on the high reward trajectories collected in Phase 1. •	It seems to be unfair to compare the proposed method with advanced exploration strategy to other model free baselines that only have very simple exploration strategies (e.g., epsilon greedy). More baselines with better exploration strategies should be compared.<BRK>This paper applies the Go Explore algorithm to the domain of text based games and shows significant performance gains on Textworld s Coin Collector and Cooking sets of games. Additionally, the authors evaluate 3 different paradigms for training agents on (1) single games, (2) jointly on multiple games, and (3) training on a train set of games and testing on a held out set of games. In addition to better asymptotic performance Go Explore is also more efficient in terms of the number of environment interactions needed to reach a good policy. It is an algorithm that should not be ignored by the text based game playing community. The major drawback of the paper is a lack of novelty   the Go Explore algorithm is already well known, and this paper seems to be a direct application of Go Explore to text based games. While the results are both impressive and relevant for the text game playing community   it s my feeling that this work may not be of general interest to the broader ICLR community due to the lack of new insights in deep learning / representation discovery.<BRK>Text-based computer games describe their world to the player through natural language and expect the player to interact with the game using text. These games are of interest as they can be seen as a testbed for language understanding, problem-solving, and language generation by artificial agents. More specifically, in an initial exploration phase, they first extract trajectories with high rewards, after which they train a policy to solve the game by imitating these trajectories. Their experiments show that this approach outperforms existing solutions in solving text-based games, and it is more sample efficient in terms of the number of interactions with the environment. Moreover, they show that the learned policy can generalize better than existing solutions to unseen games without using any restriction on the action space.
Reject. rating score: 3. rating score: 3. rating score: 3. In this paper, the authors propose a new method to alleviate the effect of overfitting in the meta learning scenario. The method is based on network pruning. Empirical results demonstrate the effectiveness of the proposed method. Cons:  The proposed method is simple and lacks technical contributions. Post rebuttal:I have read other reviewers and the authors  responses.<BRK>The paper proposes to use the sparse network to mitigate the task overfitting. I hold the concerns about the rebuttal "this typo can be easily fixed with almost no impact on the technical proofs of the theoretical results". ############Thanks for the authors  feedback and I have read them. In general, this paper study an interesting problem in meta learning and the paper is written in a clear way.<BRK>All in all, I think it s a good paper but not completely ready for publication based on the following final assessment. what is the formal definition of meta generalization and meta overfitting? 10 pages seem excessive for the content of the paper. Figure 1.b: the green bars (proposed method) don’t seem to improve the generalization (testing accuracy).<BRK>However, the existing meta-learning models have been evidenced to overfit on meta-training tasks when using deeper and wider convolutional neural networks. This means that they cannot improve the meta-generalization performance by merely deepening or widening the networks. To remedy such a deficiency of meta-overfitting, they propose in this paper a sparsity constrained meta-learning approach to learn from meta-training tasks a subnetwork from which first-order optimization methods can quickly converge towards the optimal network in meta-testing tasks. Their theoretical analysis shows the benefit of sparsity for improving the generalization gap of the learned meta-initialization network.
Reject. rating score: 3. rating score: 3. rating score: 3. This paper introduces a feedback mechanism in the GAN framework which improves the quality of generated images in the context of image to image translation. After such a map is obtained, it is concatenated with the input image and fed iteratively to the generator.<BRK>[Summary]This paper proposes a GAN with an attention based discriminator for I2I translation, GuideGAN. The proposed discriminator provides the probability of real /fake and an attention map which reflects the salience for image generation. They apply their method to CycleGAN. The used datasets are not challenging. How large are additional parameters? How about the results of simple RGBA? [Minor]In Figure 1, M is used without the definition.<BRK>This paper proposes an extension of the conditional GAN objective, where the generator conditions on an attention map produced by the discriminator in addition to the input image. The paper should clarify what is done at test time, and clearly state the shortcomings as a result of this, i.e.different procedures are used for training and testing, which is not principled. Figure 2: Only the qualitative results for unsupervised image to image translation are available; qualitative results for supervised image to image translation should also be provided.<BRK>Namely, the relative model capacities of the generator and discriminator do not match, leading to mode collapse and/or diminished gradients. More specifically, they arm the discriminator with an attention mechanism so not only it estimates the probability that its input is real, but also does it create an attention map that highlights the critical features for such prediction. They extensively evaluate the proposed GuideGAN framework on a number of image transfer tasks. Both qualitative results and quantitative comparison demonstrate the superiority of their proposed approach.
Accept (Spotlight). rating score: 8. rating score: 6. rating score: 6. This work addresses the problem of causal inference in time dependent treatment regimes. To address the problem, the authors propose an extension of the balancing representation for causal inference framework that seeks render the current treatment independent from a representation of the history of treatment and confounders.<BRK>There is one problem with the illustrations in Figure 1: predictions on the potential outcomes before the first treatment was given should have stayed on the same path, and then depart when different treatment was initiated (at different time)<BRK>Especially when in the their Experiments section the model used for tumor growth that generates the synthetic data is Markov, and consequently, does not check if the proposed method works in a non Markovian world.<BRK>To handle the bias from time-varying confounders, covariates affecting the treatment assignment policy in the observational data, CRN uses domain adversarial training to build balancing representations of the patient history.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. rating score: 6. The novelity of the problem posed is that the time lag g(x(t)) is allowed to be non stationary and supposed to be unknown. The authors outlie a a Bayesian approach.<BRK>However, the novelty of the work is not presented clearly. For solving this regression problem, the authors use a standard Bayesian approach to fit the model. The application solved by authors is very important, and it is improved by the proposed method.<BRK>The approach of the paper to the problem is principled and it provides extensive theoretical and experimental results for further insight. However, the paper proceeds to make a modeling assumption of the same sort which bounds the time lag.<BRK>To avoid killing a good paper, I rated this paper as Weak Accept, and leave the decision to other reviewers and the AC. The topic is out of my scope. So I tried to understand this paper, and I found it is difficult for me.<BRK>This paper tackles a new regression problem, called Dynamic Time-Lag Regression (DTLR), where a cause signal drives an effect signal with an unknown time delay. A Bayesian approach is presented to tackle the specifics of the DTLR problem, with theoretical justifications based on linear stability analysis.
Reject. rating score: 6. rating score: 6. rating score: 6. This is essentially the situation presented in the illustrative example. If possible I think this paper would benefit significantly from a detailed explanation of how and when the proposed approach should be expected to improve on bootstrapping, including bootstrapping off a value function which uses an analogous architecture to v+. I found this example to be more illustrative than the one in the introduction of the paper, and I now feel that I have a better grasp of the motivation.<BRK>Are the number of parameters in the value function approximator the same between the hindsight RL algorithm and the baseline? Summary:The paper proposes a way to learn better representation for RL by employing a hindsight model based approach. What if learning a good \phi is as hard as predicting the return? I understood the setup of the problem and it seemed like it was very illustrative of an example where proposed approach will excel.<BRK>Value Driven Hindsight Modelling proposes a method to improve value function learning. While the Portal Choice experiments are informative and use Impala, it is a bit toy, and it would increase the reviewer’s confidence in the generality and robustness of the approach if improvements were also demonstrated for an actor critic method on a large environment suite. It requires significant context from definitions in the paper in order to understand.<BRK>The question of how to effectively learn predictors for value from data is one of the major problems studied by the RL community, and different approaches exploit structure in the problem domain in different ways. This provides us with tractable prediction targets that are directly relevant for a task, and can thus accelerate learning of the value function.
Reject. rating score: 3. rating score: 3. rating score: 3. This papers suggests new penalties to avoid the apparition of dead units and minimize the existence of dead points. DecisionI vote to reject this paper because the formulation of the penalties lacks clarity and because the experiments are incomplete and likely provide misleading results. Fourth paragraph of section 4.1 mentions results using convolutional layers, but it is not clear which of the provided results are on fully connected or convolutional networks.<BRK>In this paper, a problem of training DNNs having dead points and neurons was addressed. The constraints were augmented to the loss functions of DNNs as a regularizer. How do the proposed methods scale with larger datasets and networks? After the discussion:I checked the response of the authors and comments of the other reviewers.<BRK>This paper proposed two constrains to tackle the problems of dead points and dead neurons. Cons:  The experiments are not sufficient to validate the arguments.<BRK>They show very promising results on a toy, MNIST, and CIFAR-10 datasets. This problem is known to the academic community as \emph {dead neurons}. The other is a less studied problem, dead points. They accomplish that by alleviating two problems.
Reject. rating score: 3. rating score: 3. rating score: 8. Authors consider the 1 way few shot classification task. Argue that modeling it as a 2 way with a random negative sample is not efficient. Propose a novel technique applicable for prototypical networks. Their proposal is to use 0 as the prototype of null class. But in practice there is no benefit in doing so. Therefore, the main contribution is proposing to compare against norm of the embedding rather than a prototype for random negative samples. Probably because the prototype of 20 random images is 0 anyway. The contribution of this paper is intuitive and interesting.<BRK>This paper addresses a method of applying prototypical networks (which are popular for few shot learning problems) to few shot one classification problems where only one group of examples are available without any counter examples. The idea proposed in this paper is to introduce a null class which models the entire space of possible examples and queries are judged, compared to positive class as well as the null class.<BRK>This paper looks at the problem of few shot classification in the regime when only a single class is present. I have a few points that I believe should be clarified:  Point 1  Are you assuming that the trainable offset in the batch norm will be close to \vec{0}, or are you identically setting it to \vec{0}? The authors provide a large enough set of comparisons to existing algorithms, and explain their approach well.<BRK>This extension shows promising results when a higher number of support examples is available. Types of few-shot models include matching networks and prototypical networks. The difficulty here lies in the fact that no relative distance to other classes can be calculatedvia softmax. They solve this problem by introducing a “ null class ” centered around zero, and enforcing centering with batch normalization.
Reject. rating score: 3. rating score: 6. rating score: 6. This paper studies the training of overparametrized neural networks by gradient descent. That is, the weights are chosen sufficiently large and the neural network is sufficiently overparametrized. As this corresponds to learning lower degree polynomials first, the authors claim that this explains the "spectral bias" observed in previous papers. I think the paper is not yet ready for being published. Does that mean in a quantitative sense? To be honest, only considering Fig.1 I am not able to assess whether the convergence rates of the different components are truly linear. (Apart from that, results for the NTK regime are interesting in its own right.) The presentation of the paper needs to be improved.<BRK>So it is possible that there are technical flaws to this work that I did not notice. 2)  Given that the kernel depends on the loss function, and it s the eigenspectrum of the kernel s integrator operator that determines the convergence properties, can this work be applied to engineering better loss functions for practical applications? This provides a nice explanation for how ANNs can both: a) have enough capacity to memorize random data; yet b) generalize fairly well in many tasks with structured input data.<BRK>The paper aims to provide theoretical justification for a "spectral bias" that is observed in training of neural networks: a phenomenon recorded in literature (Rahaman et al.), where lower frequency components of a signal are fit faster than higher frequency ones. The contributions of the paper are as follows:1. 4.Some toy experiments are provided to exhibit the spectral bias phenomenon. Technical comments:  It is argued that the new bound of $O(\mathrm{min}(k^{ d 1}, d^{ k+1}))$ is better than the bound of $O(k^{ d 1})$ from the previous work of Bietti and Mairal, in the regime where $d \gg k$. Ideally, we should use freshly sampled points. If not, perhaps it is best to go with freshly sampled points.<BRK>An intriguing phenomenon observed during training neural networks is the spectral bias, where neural networks are biased towards learning less complex functions. The priority of learning functions with low complexity might be at the core of explaining generalization ability of neural network, and certain efforts have been made to provide theoretical explanation for spectral bias. In this work, they give a comprehensive and rigorous explanation for spectral bias and relate it with the neural tangent kernel function proposed in recent work.
Reject. rating score: 3. rating score: 3. rating score: 6. <Paper summary>The authors proposed a novel method for positive unlabeled learning. In the experiments, the proposed method achieves better performance compared with state of the art methods. * Minor concerns that do not have an impact on the score   Although the problem setting is quite different, the idea of this paper is partially similar to the importance weighting technique adopted in some recent domain adaptation methods [R1, R2]. Do you have any comment on that?<BRK>Experiments and comparisons with SOTA are provided. Pros:Their idea of making an adaption to GAN architecture by replacing the generator by a classifier to select P from U and using the discriminator to distinguish whether the selected data is from P or U for PU learning is interesting, and benefits from not relying on the class prior estimation. Can any explanation be given on this? Remarks:The clarity of the paper could be improved in multiple places.<BRK>Such a figure significantly improves the readability of the system. The idea of the paper is interesting; it is well motivated and well supported with a range of experiments from NLP and computer vision tasks. (vi) right after equation (2), please change x_s to \mathbf(x)^s for the consistency of your formulation. * The experiment section could have enjoyed from an ablation study in which a system that only implements terms I and II from Eq(3).<BRK>This paper shows that the GAN (Generative Adversarial Networks) style of adversarial training is quite suitable for PU learning. GAN learns a generator to generate data (e.g., images) to fool a discriminator which tries to determine whether the generated data belong to a (positive) training class.
Reject. rating score: 3. rating score: 3. rating score: 3. rating score: 6. The paper proposes to use Contrastive Predictive Coding (CPC), an unsupervised learning approach, to learn representations for further image classification. The authors show that using CPC for representation learning allows to achieve better results than other self supervised methods. All results are very impressive and is in line with current trends of using a linear classifier on top of a deep feature extractor (e.g., Nalisnick et al., "Hybrid Models with Deep and Invertible Features"). However, The whole idea of the paper is based on the original paper:* Oord, Aaron van den, Yazhe Li, and Oriol Vinyals. "Representation learning with contrastive predictive coding." arXiv preprint arXiv:1807.03748 (2018). Technically speaking, the paper is outstanding, but it lacks novelty in terms of new ideas. For instance, the authors explain CPC by mentioning about masked convolutional layers that is unnecessary at this point. I understand that from engineering perspective it is crucial information, but it does not help to understand CPC. The paper can be treated as an uptaded version of the original CPC paper.<BRK>This paper improves Contrastive Predictive Coding method and reaches a good performance in several downstream tasks. Strengths: + The experimental results seem good. And the performance of down stream tasks is comparable or better than the state of the art methods. + The paper is well written. Weaknesses:  The novelty and technical contributions are limited. This paper only proposes some minor improvements based on the original CPC method and use a deeper network to get better performance. The proposed method lacks of important insights for the research community. The capacity of network architecture is crucial for self supervised learning. Meanwhile, the network architectures of many compared methods are not listed in the tables, which may be misleading. For example, Unsupervised Data Augmentation (Xie et al., 2019) in table 2 and Instance Discrimination (Wu et al., 2018) in table 3 use ResNet50, which is much more shallow than ResNet 161 in this paper. In section 2.1, the paper doesn t describe clearly what s the input of masked convolutional network $g_{\phi}$ and how to calculate $c_{i, j}$.<BRK>Title: DATA EFFICIENT IMAGE RECOGNITION[Summary] This paper introduces Contrastive Predictive Coding (CPC) image recognition in the data efficient regime. Concretely, the authors improve CPC in terms of its architecture and training strategy. [Pros]  Although the CPC was proposed and evaluated in vision task in [1], a new implementation of CPC with dramatically improved ability is presented in this paper. [Cons]  In Sec.4.1, four axes are identified to upgrade CPC v1 to CPC v2. But they are not well motivated. More discussions about why this four axes are investigated in image recognition. It would be great if you provide more evidence that the improvement in low data classification results from the increased ‘predictability’. The pretrain model trained with different methods should be the same. I want to see how the four axes in Sec.4.1 are related to core motivation (more predictable) since they are major adjustments from CPP v1 to CPP v2. If the author provides a profound explanation of the problem, I would consider changing the rating.<BRK>Although the modifications to CPC aren t particularly original, the authors show first that these yield a significant improvement in linear classification accuracy. Pros:Owing to its generality (CPC assumes only a weak spatial prior in the input data), and cheap computational cost relative to earlier generative approaches, CPC is already a promising unsupervised representation learning technique. The paper gives more evidence of this usefulness for image data, yielding leading performance on several different image classification benchmarks. The authors also make the observation that linear separability, the standard benchmark for evaluating unsupervised representations, correlates poorly with efficient prediction in the presence of limited labeled data. Cons:The improvements given in the paper are quite useful within their stated domain (image data), but aren t directly applicable to other types of input data. In line with that, I think their work would be improved by some commentary on this, in particular by any concrete suggestions they have about how similar augmentations to CPC could be carried out in text, audio, and/or video data.<BRK>Human observers can learn to recognize new categories of objects from a handful of examples, yet doing so with machine perception remains an open challenge. They hypothesize that data-efficient recognition is enabled by representations which make the variability in natural signals more predictable, as suggested by recent perceptual evidence. They therefore revisit and improve Contrastive Predictive Coding, a recently-proposed unsupervised learning framework, and arrive at a representation which enables generalization from small amounts of labeled data. When provided with only 1% of ImageNet labels (i.e.13 per class), this model retains a strong classification performance, 73% Top-5 accuracy, outperforming supervised networks by 28% (a 65% relative improvement) and state-of-the-art semi-supervised methods by 14%. They also find this representation to serve as a useful substrate for object detection on the PASCAL-VOC 2007 dataset, approaching the performance of representations trained with a fully annotated ImageNet dataset.
Reject. rating score: 1. rating score: 3. rating score: 3. rating score: 8. This paper proposes two variants of Nesterov momentum that maintains a buffer of recent updates. The paper proves optimal convergence in the convex setting and makes nice connections to mirror descent and Katyusha. I’m not sure how useful “Test Accuracy STD%” is useful as a metric since it is influenced heavily by the learning rate and its schedule. Tail averaging schemes in general seem like they would increase “robustness.” In addition, there seem to be situations where a higher final variance is beneficial (just run the method for longer and you can find a better solution). I would recommend removing the grid lines and make the colors more differentiable. Whereas other boxes use equality Spacing looks a bit off in parts of the paper. Don t decay the learning rate, increase the batch size.<BRK>Since we are getting to issues surrounding the use of momentum with stochastic optimization, I would like to make a note that the performance of these algorithms more broadly aren t quite sketched out for their use in broader stochastic optimization. In particular, despite broad use in practice, it is unclear if standard variants of Nesterov acceleration/Heavy Ball method achieve "acceleration" in stochastic optimization. So the performance of the algorithm in practice may quite significantly be away from the bounds described in the paper. %%.%%This paper considers robustness issues faced by Nesterov’s Acceleration used with mini batch stochastic gradients for training Deep Models. The paper in general is well written and easy to follow. As long as this is not a reason/cause for worse terminal performance (which doesn’t seem to be the case), I am unable to see why large initial fluctuations are concerning.<BRK>The authors proposed Amortized Nesterov’s Momentum, a variant of Nesterov’s momentum that utilizes several past iterates, instead of one iterate, to provide the momentum. The goal is to have more robust iterates, faster convergence in the early stage and higher efficiency. I would like to know how much computation cost can be saved with this modification. It would be easier to follow if the side by side algorithm comparison can be shown early. The section 4’s theoretical analysis based on the convex composite problem is not quite convincing. In the experiment section, the comparison of AM1/2 SGD with other baselines seems not quite consistent. This makes the comparison not very meaningful, while AM1 SGD and AM2 SGD do not use learning rate restart.<BRK>This paper provides a new simple method to incorporate Nesterov momentum into standard SGD for deep learning, with good empirical and theoretical results. Overall I think this paper should be accepted, some minor comments follow. At no point does Polyak s heavy ball method get mentioned, even though the variant of Nesterov acceleration you are considering is very similar to it (since the momentum parameter is fixed, which is not the usual form of Nesterov except in the strongly convex case). Perhaps it suggests that a conference with a small page limit is not the best venue? It seems that SGD still has better convergence early on. The authors suggest their method fixes this (relative to standard nesterov SGD) but it doesn t seem to be quite as good as SGD.<BRK>However, due to the large stochasticity, SGD with Nesterov's momentum is not robust, i.e., its performance may deviate significantly from the expectation. In this work, they propose Amortized Nesterov's Momentum, a special variant of Nesterov's momentum which has more robust iterates, faster convergence in the early stage and higher efficiency. Their experimental results show that this new momentum achieves similar (sometimes better) generalization performance with little-to-no tuning.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. The authors propose a Neural Question Requirement module that extracts a list of condition from the question which should be met by the candidate answer in the question answering problems.<BRK>I’m wondering if this could be generalized to other MRC tasks, e.g.MSMARCO or DuReader. Evaluation is on SQuAD V2.0 which is constructed from Wikipedia and contains unanswerable questions generated by crowd sources. This is an interesting paper which focuses on the answer verification and validation, and shows the effectiveness of the proposed model. However, there are a few concerns detailed as follows:1.<BRK>It is different from existing answer verifiers in that NeurQuRI pinpoints where the mismatch occurs between the question and the candidate answer in unanswerable cases. The revision has satisfied my concerns, and I decided to increase the score of the paper (weak reject  > weak accept).<BRK>To address this, they propose a neural question requirement inspection model called NeurQuRI that extracts a list of conditions from the question, each of which should be satisfied by the candidate answer generated by an MRC model.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. 3.Showing that a model trained on a low resolution imagenet 32 dataset can generalize its compression capabilities to higher resolution datasets with convincing results. Improve the compression rate performance by adapting the discretization of latent space required for the entropy coder ANS. 2.Increasing compression speed by implementing a vectorized version of ANS, and heaving an ANS head in the shape of a pair of arrays matching that of the latent variable and the observed variable. It improves upon the previous bits back coding based hierarchical VAE [1].<BRK>This paper proposes a method for lossless image compression consisting of a VAE and using a bits back version of ANS. The results are very impressive on a ImageNet (but maybe not so impressive on the other benchmarks).<BRK>The authors propose a method for lossless image compression based on usingfully convolutional VAE models. These models are shown to generalize well whenthey are trained on small images (e.g.32x32 and 64x64) and then applied tomuch larger images. The method is based on a fully vectorized implementation ofbits back with asymetric numeral systems coding which is much faster thanprevious non vertorized implementations. The vectorizationapproach can be very useful in practice and the dynamic discretization can alsobe useful as shown by the experiments. One criticism could be that the authorsdo not achieve state of the art results, but I consider this a minor thing.<BRK>They make the following striking observation: fully convolutional VAE models trained on 32x32 ImageNet can generalize well, not just to 64x64 but also to far larger photographs, with no changes to the model. They use this property, applying fully convolutional models to lossless compression, demonstrating a method to scale the VAE-based 'Bits-Back with ANS' algorithm for lossless compression to large color photographs, and achieving state of the art for compression of full size ImageNet images.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. The paper studies the function space regularization behavior of learning with an infinite width ReLU network with a bound on the l2 norm of weights, in arbitrary dimension, extending the univariate study of Savarese et al.(2019).The authors show that the corresponding regularization function is more or less an L1 norm of the (weak) (d+1)st derivatives of the function, and provide a rigorous formal characterization in terms of the "R norm", which is expressed via duality through the Radon transform and powers of the Laplacian. In addition, the paper provides a number of implications of this study, such as approximation results through Sobolev spaces, an analysis of the norm of radial bump functions, and a new type of depth separation result in terms of norm as opposed to width.<BRK>They extend the work of Savarese et al.on the univariable function by introducing the Randon Transform and R norm to this problem. The authors finally prove that any function in Sobolev space could be (approximately) obtained by a bounded network.<BRK>This paper gives characterization of the norm required to approximate a given multivariate function by an infinite width two layer neural network. An important result is the relation between Radon transform and the $\mathcal{R}$ norm.<BRK>They give a tight characterization of the (vectorized Euclidean) norm of weights required to realize a function $f: \mathbb {R} \rightarrow \mathbb {R} ^d $as a single hidden-layer ReLU network with an unbounded number of units (infinite width), extending the univariate characterization of Savarese et al. (2019) to the multivariate case.
Reject. rating score: 1. rating score: 3. rating score: 3. Although the proposed algorithm is practically useful, I believe the submission is premature to be accepted at a conference due to (1) the lack of comparison with existing works on multi agent (reinforcement and imitation) learning and (2) the lack of novelty (It seems that the proposed method simply combines existing neural networks and applies it to multi agent behavior prediction.). In addition to them, there’s a paper on arXiv that uses GNN for MARL (https://arxiv.org/abs/1810.09202), which may be deeply related to this work as well.<BRK>The paper proposes a new time series model for learning a sequence of graphs. The GNN definition is expanded out in the pseudocode, but seems to be completely standard. You should cite some of that work. 3.Poor experimental evaluation. I would not call what the paper is doing "imitation learning".<BRK>While reading the paper, it appeared to me that the authors proposed a graph based method for solving MAS task and they compared with an old GNN method, which is not justified. 6.I am not quite satisfied with provided experimental evaluation of the paper. 7.Also I want to see a fair comparison in the paper. 4.Why does the proposed GNN model performs better than Kipf s GNN for predicting and imitating the motion behaviors?<BRK>They propose an implementation of GNN that predicts and imitates the motion be- haviors from observed swarm trajectory data. The network ’ s ability to capture interaction dynamics in swarms is demonstrated through transfer learning. They finally discuss the inherent availability and challenges in the scalability of GNN, and proposed a method to improve it with layer-wise tuning and mixing of data enabled by padding.
Reject. rating score: 1. rating score: 3. rating score: 6. The paper puts forward a new regularization based continual learning method that explicitly regularizes the optimization trajectory by constraining in the distribution space. The paper is well written, and preliminary empirical results are promising. [Model] After surveying previous work, I am not sure if the paper is really novel: First of all, adding KL based constrains to alleviate model forgetting has already been widely explored in prior arts, as the authors also acknowledged in the paper. The experiment results have shown that the co nature gradient method help from time to time. Understandably, the co nature gradient based optimization has the add on benefit for any continual learning tasks.<BRK>The paper proposed a novel regularization methods for continual learning. The authors introduce co natural gradients, which is an incremental development of natural gradient methods, note that co natural gradients use Fisher information to regularize the trajectory of the gradients which will be optimal on both tasks. I think that the performance of finetuning + co gradient is too natural, and not much meaningful. And it is required to apply on heterogeneous datasets to evaluate the performance when problems are really different.<BRK>This paper amends the gradient update rule for continual learning using a natural gradient style formulation in order to regularise the trajectory during learning to forget previous task(s) less. Although the idea seems reasonable and interesting, I feel like this paper needs work both in the theory and experiments. This corresponds to lower forgetting of previous tasks. This probably results in early stopping: the new tasks are not able to reach their new optimal points (with or without regularised trajectories). In fact, by adding another regularisation term, it is unsurprising that co natural gradient updates have less forgetting, as the extra regularisation term probably means the trained parameters are even closer to the previous parameters.<BRK>In this paper, they argue for the importance of regularizing optimization trajectories directly. They derive a new co-natural gradient update rule for continual learning whereby the new task gradients are preconditioned with the empirical Fisher information of previously learnt tasks. They show that using the co-natural gradient systematically reduces forgetting in continual learning. Moreover, it helps combat overfitting when learning a new task in a low resource scenario.
Reject. rating score: 3. rating score: 3. rating score: 3. I think the authors should instead focus on the discussion of generalization performance and the observation that training loss and test accuracy are independent of batch size in noise dominated regime. Particularly, the authors mixed their observations up with the results of published works, making it hard to identify the contributions of this paper.<BRK>This paper studies the properties of SGD as a function of batch size and learning rate. 2) Does momentum help in constant step budget (with sufficiently large steps so that training loss is small)? The critical contribution of this work appears to be the observation that large batch size can be worse than small under same number of steps demonstrating implicit regularization of small batch size.<BRK>This paper is an empirical contribution regarding SGD arguing that it presents two different behaviors which the authors name a noise dominated regimen, and a curvature dominated regime. They observe that the behaviors seem to arise in different batch sizes The authors derive empirical conclusions and perform experiments in different settings. It would be nice if there were some theoretical results to back up the observations.<BRK>These results confirm that the noise in stochastic gradients can introduce beneficial implicit regularization. In the curvature dominated regime, the optimal learning rate is independent of batch size, and the training loss and test accuracy degrade as the batch size rises.
Reject. rating score: 1. rating score: 3. rating score: 3. I think it is trying to devise a methodology that improves upon a given expert policy, but I am not confident whether if this is the objective. The main contribution of the paper is the algorithm POLISH on page 5. Do you mean that the algorithm diverges instead of the state distribution diverges?<BRK>This is not conclusive evidence that a sweet spot for t results in better performance. However, I am still not convinced by the evidence in this paper. It is hard to understand the process without a clear step by step description.<BRK>1) Were queries to the simulator used during MCTS accounted for when measuring sample complexity? My current guess is that this paper is primarily aimed towards policy optimization in a reinforcement learning setting. However, this point is not provided in the paper.<BRK>However, this can be computationally expensive, and can also end up training the policy on a state distribution that is far from the current policy's induced distribution. In this paper, they propose an algorithm that finds a middle ground by using Monte Carlo Tree Search (MCTS) to perform local trajectory improvement over rollouts from the policy. They provide theoretical justification for both the proposed local trajectory search algorithm and for their use of MCTS as a local policy improvement operator.
Reject. rating score: 3. rating score: 3. rating score: 6. This work proposes an alternative approach to non autoregressive translation (NAT) by predicting positions in addition to the word identities, such that the word order in the final prediction doesn t matter as long as the positions are correct. The length of the translation is predicted similar to Gu et al 2017, as well as smoothly copying the source sequence to decoder input. 2.The idea of modeling positions as a latent variable is interesting and might generalize to other tasks beyond NAT. Cons:1.This work should compare to later baselines such as FlowSeq (https://arxiv.org/pdf/1909.02480.pdf) which gets better performance with flow. However, empirically there s been better performance achieved by flow based models, so I am inclined to reject this work. However, I still have some concerns regarding the baseline flowseq.Flowseq large gets better results but is not reported in the main paper.<BRK>This work builds on the non autoregressive translation (NAT) by using position as a latent variable. It shows that predicting the position of the words improves the performance of the translation and paraphrase task. This paper uses a heuristic that the inputs positions and output positions of the decoder with close by embeddings are more likely to represent the position mapping. Questions:  While position prediction seems like a good idea, I am not fully convinced of the heuristic used    similarity between input (d_i) and output (y_j) is used to determine the position supervision. In addition to the previous point, is the model pertained before this heuristic is used? In Table 2 PNAT w/HSP seems to have amazing performance compared to other models.<BRK>This work proposes a non autoregressive model for conditioned text generation. Experiments on machine translation and paraphrase generation show strong result in comparison to other non autoregressive models. The idea of delegating generation order to the latent variables seems interesting, and the paper takes a reasonable approach when fleshing it out. The presentation does a good job in giving a very brief description of the model to readers familiar with non autoregressive generation; but for those who are not (like myself), much content needs to be clarified. If it is, how do the authors backprop through the argmax in the length? If it is some well known algo like the Hungarian algorithm, please specify.<BRK>Non-autoregressive generation is a new paradigm for text generation. Previous work hardly considers to explicitly model the positions of generated words. In this paper, they propose PNAT, which explicitly models positions of output words as latent variables in text generation. The proposed PNATis simple yet effective. Experimental results show that PNATgives very promising results in machine translation and paraphrase generation tasks, outperforming many strong baselines.
Reject. rating score: 3. rating score: 3. rating score: 6. Building on the split linearized bregman iteration strategy, the authors propose two practical algorithms to boost network, namely GT filters Alg and GT layers Alg. The proposed algorithms can simultaneously grow and train a network by progressively adding both convolutional filters and layers. Strengths:1 The authors introduce two simple but practical algorithms for augmenting the architectures of deep network. 2 The paper is clearly written and easy to follow. Thus contribution is incremental and novelty is limited.<BRK>This paper studies a very interesting topic: automatically grow filters and layers in neural networks and find an "optimal" width and depth for neural networks. The method is motivated by SPLITLBI, and its effectiveness is verified by experiments and comparison with AutoGrow. AutoGrow uses 4 blocks while this paper uses 5 blocks. 2.3.In the experiments of layer growing, please clarify if filter growth is also applied or not. If I read the AutoGrow paper correctly, efficiency is one of the their claims and they showed that the growing process is as fast as "training a single DNN", and they scaled to ImageNet, which is not covered in this paper. 2."To the best of our knowledge, this is the first algorithm for BoN that can simultaneously learn the network structures and parameters from training data."<BRK>This paper proposes an architecture search method for deep convolutional neural network models that progressively increases the number of filters per layer as well as the number of layers, and the authors refer to this general approach as boosting networks. The algorithm for increasing the number of filters is based on split linear Bregman iteration, and the algorithm for increasing the number of layers proceeds block by block, increasing the layers per block until the accuracy does not increase. Second, an additional effort should be made to compare to additional prior work in architecture search. Can a similar analysis be made for wall clock time, i.e., how long the models actually take to train? A thorough pass through the paper for spelling and grammar would be very useful.<BRK>Network structures are important to learning good representations of many tasks in computer vision and machine learning communities. In this paper, by virtue of an iterative sparse regularization path--Split Linearized Bregman Iteration (SplitLBI), they propose a simple yet effective boosting network method that can simultaneously grow and train a network by progressively adding both convolutional filters and layers. Extensive experiments with VGG and ResNets validate the effectiveness of their proposed algorithms.
Reject. rating score: 3. rating score: 3. rating score: 6. This paper presents a method to efficiently transfer pre trained english language model to bilingual language model. The obtained representations are evaluated on downstream NLP task (natural language inference and dependency parsing) with state of the art performances. Pros:  Experiments clearly show that, using the proposed method, stronger pre trained English embedding leads to stronger bilingual language model and thus to better performances for downstream foreign tasks. Cons: While it is generally  intelligible, some structural modifications could be done to  improved the clarity of the paper.<BRK>In this work, the authors propose a way to transfer a pre trained English BERT model to a new language within a short amount of time. The resulting bilingual LM is evaluated for zero shot transfer learning on two tasks: XNLI and dependency parsing. Pros:  The authors provide good details into their hyperparameter settings and about how the obtain the foreign language word embeddings. Cons:I find that a key comparison point in this paper is missing, which is Bilingual BERT trained on just the two languages that are being considered for their RAMEN system. This is not a fair comparison while mBERT which is trained on 100+ languages is not. The proposed system has an unfair advantage over mBERT since it’s initialized from BERT/RoBERTA and fine tuned only on two languages.<BRK>This paper proposes a method to adapt a pretrained BERT model from English to another languages with a limited time/GPU budget. Evaluation on 6 target languages shows good performance for natural language inference and dependency parsing. Concretely, the proposed approach consists of, starting from a pretrained English language model, first training language specific embeddings and then fine tuning the entire pretrained model on English *and* the target language, using those embeddings. The language specific embeddings are initialized based on the English embeddings (the authors propose two different ways for doing that). I like about the paper that the approach is simple and fast.<BRK>The availability of multilingual pre-trained models enables zero-shot transfer of NLP tasks from high resource languages to low resource ones. However, recent research in improving pre-trained models focuses heavily on English. While it is possible to train the latest neural architectures for other languages from scratch, it is undesirable due to the required amount of compute. In this work, they tackle the problem of transferring an existing pre-trained model from English to other languages under a limited computational budget. Furthermore, evaluating their models on six languages, they demonstrate that their models are better than multilingual BERT on two zero-shot tasks: natural language inference and dependency parsing.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. The motivation for this paper is straightforward. Because the author did not use the RNN network when implementing these algorithms? To this end, the author divides the action semantics into two categories, one is actions that only affect the environment and itself, and the other is actions that affects the other agents. Because it is a modification of the network structure, ASN can be combined with any type of multi agent reinforcement learning algorithm to improve its convergence speed and final performance.<BRK>This paper focused on the multi agent systems, and proposed a new network architecture (aka ASN). In the new architecture, the action set is  manually split into two subsets, each of which contains the actions that affects other agents or not. The authors focuses on an important issue for MAS, i.e.how to reduce model complexity when increasing the number of agents. It is interesting and  somehow convincing. Hence I am curious how such issue is handled in the proposed model?<BRK>This paper proposes a neural network architecture that provides an agent agent based embeddings that are used for actions that directly affect specific agent. observations of agents that it is not directly affecting. There is still a way to represent it, but only by bypassing the proposed architecture, and expressing this behaviour directly in e^i. Claiming "Experimental results on StarCraft II and Neural MMO show ASN significantly improves the performance of state of the art DRL approaches compared with several network architectures."<BRK>Moreover, the environmental stochasticity and uncertainties increase exponentially with the increase in the number of agents. In this paper, they propose a novel network architecture, named Action Semantics Network (ASN), that explicitly represents such action semantics between agents. ASN characterizes different actions' influence on other agents using neural networks based on the action semantics between them. ASN can be easily combined with existing deep reinforcement learning (DRL) algorithms to boost their performance.
Reject. rating score: 1. rating score: 1. rating score: 1. The paper suggest to use the previously proposed ScatterNet Hybrid Deep Learning  (SHDL) network in a continual learning setting. This is motivated by the fact that the SHDL needs less supervised data, so keeping a small replay buffer can be enough to maintain performance while avoiding catastrophic forgetting. My main doubt is the benchmark and evaluation of the proposed method. The metrics reported are all relative to a baseline value (which I could not find reported), and make it difficult to understand how the model is performing in absolute term. Also concerning the comparison with the previous literature, I could find no details about the architecture and the training algorithm used. Notice that this may in particular affect some the reported metrics, since they depend on the shape of the training curve (reporting the training curves for all methods may also be useful).<BRK>The author proposed a modular SHDL with skewed replay distributions to do continual learning and demonstrated the effectiveness of their model on the CIFAR 100 dataset. They made contributions in three aspects: (1) using a computationally efficient architecture SHDL which can learn rapidly with fewer labeled examples. (2)The comparison with other methods are too simple. When comparing with other methods, the author should introduce the parameter setting and the detailed training strategy. Otherwise, the evidence made in the experimental section is not convincing. Besides, the author should follow the evaluation paradigm used in other published papers to make a fairer comparison. After reading the paper thoroughly, I am still unclear about it.<BRK>I think there might be some interesting ideas in the work, but I think the authors somehow did not manage to position themselves well within the *recent* works on the topic or even with respect to what continual learning (CL) is understood to be in these recent works. This work seems limited to image classification. This potentially makes the CL problem much simpler because you are limiting yourself to the top layer only when dealing with CL, not the rest. Is it for the benefit for CL? The autoencoder being trained on all data feels like a cheat. I think the different metrics introduced are interesting and useful. What is the final performance on Cifar. How does this compare to a model that is not trained in a CL regime? What loss do you get from the proposed parametrizaton? Maybe provide a few more information of hyperparam used for this comparison. Overall I think the paper is not ready for being published. I have doubts that the work is reproducible without these details.<BRK>The ability to learn continuously is crucial for the effective functioning of agents interacting with the real world and processing continuous streams of information. Continuous learning has been a long-standing challenge for neural networks as the repeated acquisition of information from non-uniform data distributions generally lead to catastrophic forgetting or interference. This work proposes a modular architecture capable of continuous acquisition of tasks while averting catastrophic forgetting. Several experiments that substantiate the above-mentioned claims are demonstrated on the CIFAR-100 dataset.
Reject. rating score: 1. rating score: 3. rating score: 3. Much of the paper is devoted to analyzing the properties and performance of the Baikal loss. However, there are some important drawbacks of this work. 2) The experiments are largely on mnist, with a small study showing that the Baikal loss can be applied to cifar 10. It would be good to show that loss functions meta learned on mnist generalize to larger scale problems than cifar. This means that the loss will only be evaluated in certain regimes of its inputs. You only analyze one loss function that came from your EC. What if you run it multiple times?<BRK>The authors present a framework to perform meta learning on the loss used fortraining. The two datasetsused for evaluation are perfectly balanced, it might beneficial to see how itperforms in the unbalanced case. I have a couple of concerns about the method. I think it would be relevantto add a figure that shows how the fitness of the leader of each generationevolves over time. Also the factor"2.7279" seems to be equivalent to a change in learning rate. This may be anindication that the learning rate search was not done thoroughly. Also: Waslearning rate search was performed on the network trained with Cross Entropy? Itwas not entirely clear from the experiment details in Appendix A.2.1. About the Baikal loss itself, I fear that it could produce models that have verypoor calibration, it might be nice to evaluate that (even if it is only inthe appendix). While the paper does a great job at presenting the problem and its applicationsand propose a framework that generated a loss that can transfer to otherdatasets without any tuning required. Could you clarify this?<BRK>This paper proposes a very interesting idea of loss function optimization. However, the true goal of optimization is the final accuracy (for classification). And experiments show that GLO (Genetic Loss function Optimization) based loss function can achieve better results than cross entropy. Baikal loss is a form searched by GLO. The authors explain it as a regularizer which can prevent the model to be too confident. Experiments on MNIST and Cifar10 are conducted to show the effectiveness of the proposed method. Experiments on larger datasets such as ImageNet and more analysis about the optimization details are suggested to make this work more promising.<BRK>As the complexity of neural network models has grown, it has become increasingly important to optimize their design automatically through metalearning. This paper shows that loss functions can be optimized with metalearning as well, and result in similar improvements. The method, Genetic Loss-function Optimization (GLO), discovers loss functions de novo, and optimizes them for a target task. Networks trained with GLO loss functions are found to outperform the standard cross-entropy loss on standard image classification tasks.
Reject. rating score: 1. rating score: 3. rating score: 3. The problem starts at section 3 and continues after that. There is no explanation how one can show the corollary. It is not clear that all of these are actually helpful for the main purpose of the paper. 2) Related Work: I think authors need to do a more comprehensive literature review on generalization bounds. For example, authors heavily rely on Chen et al.(2019) for their generalization bounds while very similar results where shown before by [1] and [2]. 2018.3) Definition of generalization: I don t think the definition of generalization suggested in this work is much different than Arora et. 6) Experiments: Experiments can also be improved significantly.<BRK>In this paper, the authors study the generalization bound for GANs based on a new definition of generalization error where the distribution corresponding to the generator is assumed to be known for each generator (i.e., there is no empirical distribution for generators). In my opinion, most of the theoretical results seem follow directly from standard tools in statistical learning theory and existing results on capacity bounds of neural networks. It seems that the authors do not introduce new ideas or techniques in the analysis. The authors made comparisons with the related results in Arora et al (2017).<BRK>The authors give new generalization bounds for GANs. The argue for a new definition of generalization for GANs, which isolates the effect of sampling error arising from sampling fromthe real distribution. In my opinion, the mathematical writing is not up to the standard for publication in ICLR. There are many cases where the paper is unclear, and, in many other cases, I have to guess what they mean, and I have limited confidence in my guess. The fact that the c in this definition has a subscript of f led me to think that the bound can depend on f, but this does not make sense. When the compare their bound with previous work, they treat quantities as constants which can be large.<BRK>This paper focuses on the theoretical investigation of unsupervised generalization theory of generative adversarial networks (GANs). On top of that, they establish a bound for generalization error with a fixed generator in a general weight normalization context. Hence, they can explain how the complexity of discriminators and generators contribute to generalization error. Their numerical simulations also verify that their generalization bound is reasonable.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. This is the main impact of the paper, as I see it, and enough reason for acceptance.<BRK>What is meant with slight L2 regularization? Things to improve the paper that did not impact the score:  In equation (2) you could replace the x with a . Fig.4 shows the median error in meters for the different variants of the stereo network.<BRK>Overall, I am inclined to accept the paper.<BRK>Their code is available at https: //github.com/mileyan/Pseudo_Lidar_V2. In this paper they provide substantial advances to the pseudo-LiDAR framework through improvements in stereo depth estimation.
Reject. rating score: 3. rating score: 3. rating score: 6. “Best of Many Samples” Distribution matchingSummary:This paper proposes to a novel VAE GAN hybrid which, during training, draws multiple samples from the reparameterized latent distribution for each inferred q(z|x), and only backpropagates reconstruction error for the resulting G(z) which has the lowest reconstruction. While I think the idea is interesting, the change in results over Rosca et.<BRK>1.Using Discriminator to estimate the likelihood ratio is a commonly used approach, which was first proposed in [1]. This is also generalized as a reversed KL based GAN in [2] [3]. The authors failed to discuss this with these previous works in Section 3.3 and in Related works. 3. this paper is not well written.<BRK>This paper presents a new objective function for hybrid VAE GANs. To overcome a number of known issues with VAE GANs, this work uses multiple samples from the generator network to achieve a high data log likelihood and low divergence to the latent prior. Questions:  Considering the smaller gap between α GAN+SN and BMS VAE GAN, I was wondering how much of the improvement is due to spectral normalization vs using multiple samples.<BRK>Generative Adversarial Networks (GANs) can achieve state-of-the-art sample quality in generative modelling tasks but suffer from the mode collapse problem. Recent works have proposed hybrid VAE-GAN frameworks which integrate a GAN-based synthetic likelihood to the VAE objective to address both the mode collapse and sample quality issues, with limited success. This is because the VAE objective forces a trade-off between the data log-likelihood and divergence to the latent prior. This enables their hybrid VAE-GAN framework to achieve high data log-likelihood and low divergence to the latent prior at the same time and shows significant improvement over both hybrid VAE-GANS and plain GANs in mode coverage and quality.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. (3.1) ``$u(t, x)   0 \forall x \in \partial X$ and for all $t$" change to ``for all $t$ and $x \in \partial X$ 8. equation 1, write ``we restrict our exposition to Euclidean spaces $X   \mathbb{R}^d$" and remove ``$\forall x, x  \in \mathbb{R}^d, t \in \mathbb{R}^+$" 9. (3.1) ``for a local domain with Dirichlet condition $D$" change to ``for a local domain $D$ with Dirichlet condition" 11. (3.1) ``Definition 1" is more of a proposition/theorem. (3.1) the authors could elaborate more about how HKT contains all the information in the graph s spectrum. A figure of the architecture could be included under the ``putting IMD together" section<BRK>IMD is a lower bound to Gromov Wasserstein distance. Which implies that when IMD is small, this does not guarantee that GW will be small. It is interesting how large can be that gap (and when typically the gap increases). Experiments with language affinities are not convincing to me.<BRK>The paper propose a novel way to measure similarity between datasets, which e.g.is useful to determine if samples from a generative model resembles a test dataset. *) I found the geometric exposition to be rather confusing. Detailed questions and comments:*) It seems that the focus is on GANs and related models that do not come with a likelihood. Which guarantees are that?<BRK>The ability to represent and compare machine learning models is crucial in order to quantify subtle model changes, evaluate generative models, and gather insights on neural network architectures. Existing techniques for comparing data distributions focus on global data properties such as mean and covariance; in that sense, they are extrinsic and uni-scale. They develop a first-of-its-kind intrinsic and multi-scale method for characterizing and comparing data manifolds, using a lower-bound of the spectral variant of the Gromov-Wasserstein inter-manifold distance, which compares all data moments.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. This paper introduces two new adversarial attacks: one is generating adversarial examples by colouring the original images and the other is by changing textures of the original images. While the latter minimises the cross entropy as well as the loss that defines the texture differences. I think the general idea of going beyond perturbations of pixel values in this paper is interesting and the proposed approaches of attacking on colour and textures are intuitive and reasonable. The results seem to be promising with comprehensive experiments including whitebox attack, blackbox attack by transferring, and attacks on defences. The paper overall is well written and easy to follow.<BRK>This paper proposed to generate semantically meaningful adversarial examples in terms of color of texture. In order to make manipulated images photo realistic, colors to be replaced are chosen by energy values, while textures are replaced with style transfer technique. The paper is written clearly and organized well to understand. The idea of using color replacement and texture transfer is interesting and novel. A somewhat weakness is that the discriminator   a pretrained ResNet 50   is too weak for this scenario. What about a ResNet 50 trained on augmented datasets with color jittering?<BRK>The paper proposes cAdv and sAdv, two new unrestricted adversarial attack methods that manipulates either color or texture of an image. Experimental results show that the proposed methods are more robust on existing defense methods and more transferrable accross models. The paper also performs a user study to show that the generated examples are fairly imperceptible like the C&W attack. In overall, I agree that seeking a new way of attack is important, and the methods are clearly presented to claim a new message to the community: adversarial examples can be even found by exploiting semantic features that humans also utilize, since DNNs tend to overly utilize them, e.g.colors.These claims are supported by the experiments showing that the generated examples are more transferrable across robust classifiers. Adding an ablation of beta 0 case in the result would much help the understanding of the method. Eq 1: I think F should denote the classifier to attack, but the description tells it s the colorization network.<BRK>Such adversarial perturbations are usually restricted by bounding their $\mathcal {L} _p $norm such that they are imperceptible, and thus many current defenses can exploit this property to reduce their adversarial impact. In this paper, they instead introduce "unrestricted" perturbations that manipulate semantically meaningful image-based visual descriptors - color and texture - in order to generate effective and photorealistic adversarial examples. They also show that the proposed methods can effectively be applied to both image classification and image captioning tasks on complex datasets such as ImageNet and MSCOCO. In addition, they conduct comprehensive user studies to show that their generated semantic adversarial examples are photorealistic to humans despite large magnitude perturbations when compared to other attacks.
Reject. rating score: 3. rating score: 3. rating score: 6. rating score: 6. The authors build on the work by Ghorbani et al.in concept based interpretability methods by taking into account the "completeness" of the concepts. These could be shown to satisfy some reasonable properties such as efficiency (sum of importances equals total completeness value), symmetry, additivity, dummy (a concept that does not change the completeness universally should have zero importance) as stated in Prop. I was wondering how the completeness and importance measures change when the input is perturbed slightly such that the classifier output doesn t change?<BRK>I find the focus of this research on the completeness of conceptexplanations to be quite interesting and relevant to the interpretable machinelearning literature. However, I think they could do much more to motivate theirproposals by convincing the reader that (a) existing concept based explanationsare not complete in practice and (b) PCA in the activation space is notsufficient as a baseline method for proposing concepts. The synthetic experiments seem useful to the overall story (although the datagenerating process is somewhat hard to parse at first), and I like thatthere are many baseline methods in this study. It also seems that TCAV is notcomplete. My feeling is that a more comprehensive and thoroughexperimental study starting from a characterization of a problem with existingmethods would strengthen the paper considerably. Some superficial comments:S1* "This has thus lead to an increasing interest..." It would be good to add  references to this claim.<BRK>So the resulting value would not be affected by different magnitude of u_k or concept vectors c_j. It would strengthen the paper by somehow proving it. This helps readers understand the value of this method in the real data. However, it does provide a new concept of completeness, and I agree with R1 that it s interesting and relevant. Original evaluation:This paper can be seen as an extension to the paper Ghorbani et al., 2019 to try to extract "complete" unsupervised concepts as well as maintainting interpretability. Overall I feel the idea is interesting but a bit straightforward. I think the authors could strengthen the paper by experimenting with different coefficients and examine the stability of the resulting explanations, or at least indicate how results differ if the coefficients are set differently. the completeness score.<BRK>Summary The paper proposes metrics for evaluating concept based explanations in terms of ‘completeness’   characterized by (1) whether the set of presented concepts if sufficient to retain the predictive performance of the original model and (2) how is performance affected when all information useful to a complete set of concepts (as per (1)) is removed from features at a specific layer. I like the fact that the authors decided to capture both aspects of the completeness criterion   (1) projection to concept space should not hurt performance and (2) how does removing concept projected information from the features affect performance. Capturing both provides a holistic viewpoint of the features of the concerned layer   (1) can explain features/decisions with an associated metric based on the ‘imperfect’ set of concepts and (2) captures the effectiveness of the information present in the features if we remove all concept useful information. Furthermore, the observation that under certain conditions, the top k PCA vectors maximize the defined completeness scores is interesting as well. One possible experiment that can be used to capture the reliability aspect of the discovered concepts could be as follows   “Given the set of concepts (and representative patches) and SHAP values across all (or most relevant) classes, are humans able to predict the output of the model?” Is it possible to setup and experiment of this sort? I generally like the paper.<BRK>Understanding of this high-level intelligence can be enabled by deciphering the concepts they base their decisions on, as human-level thinking. In this paper, they study concept-based explainability for DNNs in a systematic framework. First, they define the notion of completeness, which quantifies how sufficient a particular set of concepts is in explaining a model's prediction behavior. Next, they propose a concept discovery method that considers two additional constraints to encourage the interpretability of the discovered concepts. On specifically-designed synthetic datasets and real-world text and image datasets, they validate the effectiveness of their framework in finding concepts that are complete in explaining the decision, and interpretable.
Reject. rating score: 3. rating score: 3. rating score: 3. This paper proposed an end to end phrase to phrase NMT model (NP2MT). I think the contribution of this paper is incremental and the idea is of less novelty. I also have a concerns about the experiments. The dataset used in this paper seems not convincing to me. The authors just tested the model performance on WMT test set.<BRK>This paper presents a phrase based encoder decoder model for machine translation. Another reason is that the computational cost of the proposed model is not really clear. The authors present a dynamic programming method for considering all possible segmentations in decoding. I like the idea of phrase to phrase translation and the relatively simple architecture proposed in the paper.<BRK>This submission belongs to the field of machine translation. The same for ${\bf y}_{<t}^{z_k}$, a*, d* and all other variables. This submission additionally describes how an external dictionary can be incorporated using a heuristic approach. In particular, it looks at the problem of phrase to phrase translation (previously used state of the art approach) using neural network approaches.<BRK>They also show how to incorporate a memory module derived from an external phrase dictionary to \nppmt {} to improve decoding. Experiment results demonstrate that \nppmt {} outperforms the best neural phrase-based translation model \citep {huang2018towards} both in terms of model performance and speed, and is comparable to a state-of-the-art Transformer-based machine translation system \citep {vaswani2017attention}.
Reject. rating score: 3. rating score: 3. This would still have a margin of \Omega(1/n). The main observation that the authors make is that the last layer weights are updated as in the Perceptron algorithm and as long as the first layer has learned a large margin representation, the first layer weights do not change much. Writing   Proofs are mostly clear however it would help to add more details in the proof of the main theorem (especially to argue about the use of the Perceptron convergence theorem for the changing representations). Also, in terms of learning with neural networks, as the authors point out, one can learn this class by training only the last layer.<BRK>Thus, one can first extract "cardinality features" $x \mapsto |x|$, after which learnability follows by standard generalization theory results (as the authors note in the proof of Theorem 1). MAJOR COMMENTS:1) The key property of symmetric functions is that their output depends only on $|x|$. On the other hand, the authors  experiments suggest that arbitrary symmetric functions are not learnable from random initialization. (a) First, I think it would be helpful to the reader if the authors could make this intuition more explicit.<BRK>This work provides an additional step in the theoretical understanding of neural networks. They empirically verify this and show that this does not hold when the initial conditions are chosen at random. The proof of convergence investigates the interaction between the two layers of the network.
Reject. rating score: 1. rating score: 1. rating score: 3. 7.The magenta model requires a citation as it is not obvious what is being referred to. 8.The Frechet distance should be introduced with a citation. For the evaluations, the Frechet distance is introduced without a reference/citation and the “models in Magenta” are referenced without a citation.<BRK>The reviewer would like to see how the results vary with respect to those parameters. The paper is interested in music generation, leveraging a 2D representation of the music data. * The authors did not justify the choice of the specifics of the architecture, such as the number of filters, layers or the presence / absence of batch normalization (part 3.1).<BRK>Strengths:  The paper is clearly written, and the authors have taken great care to describe the unique structures of the data they are modeling.<BRK>In this paper they present a method for algorithmic melody generation using a generative adversarial network without recurrent components. Here, they use DCGAN architecture with dilated convolutions and towers to capture sequential information as spatial image information, and learn long-range dependencies in fixed-length melody forms such as Irish traditional reel.
Reject. rating score: 1. rating score: 3. rating score: 6. The paper proposes an imitation learning algorithm that learns from state only demonstrations and assumes partial knowledge of the transition dynamics. The imitation policy is trained in an environment where the responsive state features are simulated and controllable by the agent, and the unresponsive state features are replayed from the demonstrations. Is it that the imitation policy is conditioned on the unresponsive features? For example, was it Euclidean distance in pixel space for the Atari games?<BRK>To use of this knowledge, the paper proposes an expert induced MDP (eMDP) model where the unknown part of transition probability is modeled as is from demonstrations. The paper compares the proposed method against behavior cloning, which is known to perform poorly under limited data. Based on eMDP, the paper proposes a reward function based on an integral probability metric between a state distribution of expert demonstrations under the target MDP and a state distribution of the agent under the eMDP. * Rating.The main contribution of the paper is the eMDP model, which enables utilizing prior knowledge about the target MDP for IL. "tildeF".etc.*** After authors  response. Also, it is unclear what happens when the transition of unresponsive state space is stochastic. There are many skipping contents in the paper.<BRK>This paper provides a theoretical basis called expert induced MDP (eMDP) for formulating imitation learning through RL when only partial information on the transition kernel is given. The paper s main contributions would be to provides error bounds (Lemma 5.2 in particular) for the convergence rate in terms of Lipschitz constants and the maximum error between eMDP and the demonstration set.<BRK>Model-based imitation learning methods require full knowledge of the transition kernel for policy evaluation. In this work, they introduce the Expert Induced Markov Decision Process (eMDP) model as a formulation of solving imitation problems using Reinforcement Learning (RL), when only partial knowledge about the transition kernel is available. The idea of eMDP is to replace the unknown transition kernel with a synthetic kernel that: a) simulate the transition of state components for which the transition kernel is known (s_r), and b) extract from demonstrations the state components for which the kernel is unknown (s_u).
Reject. rating score: 1. rating score: 1. rating score: 1. Thie paper proposes using an "imagination" module to provide safe exploration during RL learning. The imagination module is used to perform forward predictions, constructing a graph between possible states. If any action would lead to a "base state" that is an unsafe state that action will not be executed and another "safe" action is selected from the policy. There is a lot of repetitive content in the paper that can be discarded to condense down the paper and make it more readable. If you are using environments with discrete actions and performing prediction I am not sure if that can be called imaginative. Rather it should be called sampling. There needs to be much more detail on this process.<BRK>This paper presents a model based approach to safety in RL, where the agent uses a transition model to plan ahead to avoid actions that can lead it to unsafe states. They call the planning component an imaginative module. The agent takes the baseline state as input   that can be used to define either a safe or unsafe state, that is used in the planning component. This paper should be rejected because of the assumptions it makes goes against the very task they are trying to solve. The authors start with the assumption that the agent does not have access to the model (Sec 3) , and they explicitly learn the model. This is a very big assumption that essentially says the agent has access to the model, which defeats the purpose of the safe exploration problem. 2) The assumption about the baseline state(s).<BRK>The idea is to create a comprehensive connectivity graph of the states in the environment. Once done, an agent can avoid unsafe states by avoiding states that are unconnected to a specified safe state. A practitioner might also specify safe/unsafe states as an additional source of information about the reward. This paper suffers from poor and loose writing, incomplete specification of its experiments, unrealistic assumptions during evaluation (Sec 5.3 "we create the graph using rollouts from the actual environment" to avoid errors from learning a transition model). The paper does not address basic concerns with its approach: how is the model to be learned at all, if it is to be comprehensive in the way that is necessary for the connectivity graph (which this paper calls an "imaginative module")?<BRK>In this paper, they focus on ensuring the safety of the agent while making sure that the agent does not cause any unnecessary disruptions to its environment. This graph encapsulates all possible trajectories that can be followed by the agent, allowing the agent to efficiently traverse through the imagined environment without ever taking any action in reality. A baseline state, which can either represent a safe or an unsafe state (based on whichever is easier to define) is taken as a human input, and the imaginative module is used to predict whether the current actions of the agent can cause it to end up in dangerous states in the future.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 6. Summary of the paperThe authors propose a predictive model based on the energy based model that uses a Transformer architecture for the energy function. The model is reported to achieve a slightly worse but comparable performance to the Rosetta energy function, the state of the art method widely used in protein structure prediction and design. Strengths+ A very interesting contribution to structural biology with a non trivial application of deep learning. The paper is clear and well written. These claims should be downplayed and what was shown at most is that this predictor can be applied to predict energy for a restricted problem of the rotamer recovery task.<BRK>However the experimental results are always slightly worse than the Rosetta energy function and I (strongly) suggest that the authors rephrase those statements to reflect that. Score:Overall I think that the paper is solid and tackles an interesting problem of learning energy functions for physical systems from data. The experimental results are comprehensive and mostly supports the claims made in the paper.<BRK>The paper proposes an energy based model (learned using the transformer architecture) to learn protein conformation from data describing protein crystal structure (rather than one based on knowledge of physical and biochemical principles). To test the efficacy of their method, the authors perform the task of identifying rotamers (native side chain configurations) from crystal structures, and compare their method to the Rosetta energy function which is the current state of the art. + Presents an important and emerging application of neural networks+ Relatively clearly written, e.g.giving good background on protein conformation and related things to a machine learning audience The baselines that are compared to are set2set and Rosetta. Are there newer baselines the authors can compare to? For example, the related works section discusses some recent works.<BRK>They propose an energy-based model (EBM) of protein conformations that operates at atomic scale. The model is trained solely on crystallized protein data. By contrast, existing approaches for scoring conformations use energy functions that incorporate knowledge of physical principles and features that are the complex product of several decades of research and tuning. The model achieves performance close to that of the Rosetta energy function, a state-of-the-art method widely used in protein structure prediction and design.
Reject. rating score: 1. rating score: 1. rating score: 3. This paper proposes a modification of GANs where the latent space follows a distribution modelled by a Gaussian Mixture Model. The overall structure is clear, although writing can be improved. However, as the authors skip the sampling step to be able to back propagate through the model, this is not significant.<BRK>This paper proposes to use GMM as the latent prior distribution of GAN.<BRK>This paper considers the Gaussian mixture model at the latent space to have a better GAN training result. The softmax output is considered as the mixture weights of the GMM model and controls the loss function of the generator accordingly. It is not introduced. 1.This paper is hard to follow.<BRK>To this paper, they utilize the Generative Adversarial Network (GAN) framework to achieve an alternative plausible method to compute these probabilities at the data ’ s latent space z instead of x. When each of the modes is associated with a Gaussian distribution, they refer it as Gaussian MM, or GMM.
Reject. rating score: 1. rating score: 1. rating score: 3. Using this, it considers different adversarial smoothing distributions that yield some increase in certified adversarial accuracy. Proof of main theorem (strong duality) is incorrect. 2.The paper makes several references (in italics) to a "fundamental trade off between accuracy and robustness". 3.The justification for why the particular smoothing distributions are good ideas is sketchy. I elaborate on 1 and 3 below. #3 (sketchy justification): The paper justifies a smoothing distribution that concentrates more mass around the center as follows: "This phenomenon makes it problematic to use standard Gaussian distribution for adversarial certification, because one would expect that the smoothing distribution should concentrate around the center (the original image) in order to make the smoothed classifier close to the original classifier (and hence accurate)." This is at the very least too brief for justifying the main experimental innovation in the paper (here at least the empirical improvements are bigger, although still not huge).<BRK>Summary:This paper investigates the choice of noise distributions for smoothing an arbitrary classifier for defending against adversarial attacks. For the \ell_2 adversary, the paper argues that Gaussian distribution is not the right choice, because the distribution is concentrated on the spherical shell around the x. Instead, the authors propose using a new family of distributions, with the norm square  (p_{|z|_2^2}) following the scaled \chi^2 distribution with degree d k (Eq.8).This allows an extra degree of freedom, and setting k 0 recovers the Gaussian distribution. (2) For \ell_\infty distributions, the motivation of mixed norm distributions (Eq.9) over \ell_\infty based distributions (Eq.10) is not very clear. The distribution of the norm \|z\|_2 in Eq.(8) would be concentrated on a thin spherical shell of radius about \sqrt{d k}\sigma. In any case, the values reported for the proposed model in Table 3 are only a marginal improvement over Figure 1 (left) in Salman et al.(2019), just going by the trivial \ell_2 to \ell_\infty certificate. The paper contains numerous grammatical errors, confusing statements, and nonstandard phrases. It is not robustness but rather the lack thereof   say, sensitivity.<BRK>This paper presents a new method for adversarial certification using non Gaussian noise. A new framework for certification is proposed, which allows to use different distributions compared to previous work based on Gaussian noise. From this framework, a trade off between accuracy and robustness is identified and new distributions are proposed to obtain a better trade off than with Gaussian noise. The theoretical results are interesting, showing a clear trade off between robustness and accuracy with a new lower bound and deriving better smoothing distributions. However, the experimental results are lacking, and do not support much the proposed method. Main arguments:My main concern is about the experiments: Why were Cohen et al.’s models used instead of Salman et al.’s? Also, the reported certified accuracy for Salman et al.’s model for L_inf on CIFAR 10 reported in the original paper is 68.2 at 2/255, which is very far from the 58 in Table 3. These two different objectives cannot be compared in those terms. The term broken is used for defenses in which the claimed accuracy against stronger attacks were found to be much lower than what was claimed in the original paper. How would that affect the method?<BRK>Randomized classifiers have been shown to provide a promising approach for achieving certified robustness against adversarial attacks in deep learning. However, most existing methods only leverage Gaussian smoothing noise and only work for $\ell_2 $perturbation. They propose a general framework of adversarial certification with non-Gaussian noise and for more general types of attacks, from a unified functional optimization perspective. Their proposed methods achieve better results than previous works and provide a new perspective on randomized smoothing certification.
Reject. rating score: 3. rating score: 6. rating score: 6. The scope has to be broadened both in terms of the NN models and the hardware types. While I m not an expert in (G)NN acceleration on TPUs, I have experience with GNNs and approaches to accelerate CNNs in GPUs. It is for these reasons that I think the paper is not appropriate for ICLR.<BRK>Since theseoptimizations are not TPU specific and have not been applied in the GPU basedGNN libraries referenced in this paper reinforce my concerns that they areproblem specific. From myexperience, GPU devices *are* designed for and extremely efficient at dense linearalgebra. I was able to understand thechallenges and the solutions proposed, even without prior knowledge in thearchitecture this work focuses on.<BRK>This paper proposes a method to train graph neural networks on dense hardware such as TPUs. The method is motivated by an observation that connections in graphs have locality in some datasets. The overall score of this paper is slightly positive. The effectiveness of the proposed method is well supported by the experiments.<BRK>Graph neural networks have become increasingly popular in recent years due to their ability to naturally encode relational input data and their ability to operate on large graphs by using a sparse representation of graph adjacency matrices. In this work, they question this assumption by scaling up sparse graph neural networks using a platform targeted at dense computation on fixed-size data.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. The authors prove that discriminative approaches that are based on the partitionfunction approach suffer from high variance where mutual information is high(Theorem 2). This high variance problem is something that has previously been observedempirically and is the main theoretical point that is being made aboutlimitations of MI estimators. They prove that their clipping approach reduces variance and thereforeintroduces a bias variance tradeoff. Overall, variational MI approaches do not satisfy self consistency. Evaluation:I suggest to accept the paper. Page 3: "Obtain an density ratio estimate"  > Obtain a density ratio estimate<BRK>This paper relates most existing variational mutual information estimators to density ratio estimation and uses this to show high variance of certain categories of estimators; this motivates a new SMILE estimator that uses clipping to explicitly control the variance of the estimator (while potentially increasing bias). The self consistency tests are also sensible and important, although it would be nice to understand better which of these properties are truly important for various uses of mutual information in the literature. The proposed SMILE estimator is natural.<BRK>This work summarizes the existing methods of mutual information estimation in a variational inference framework and describes the limitations in terms of bias variance tradeoffs. Further, the authors care about the self consistency, namely, independence, data processing, and additivity, which are properties of both entropy and differential entropy. Further, density ratio clipping is proposed to lessen a high variance problem in estimating a partition function. The comparison with MINE and BA is also expected on the benchmark experiment.<BRK>Variational approaches based on neural networks are showing promise for estimating mutual information (MI) between high dimensional variables. However, they can be difficult to use in practice due to poorly understood bias/variance tradeoffs. They theoretically show that, under some conditions, estimators such as MINE exhibit variance that could grow exponentially with the true amount of underlying MI. They also empirically demonstrate that existing estimators fail to satisfy basic self-consistency properties of MI, such as data processing and additivity under independence. Based on a unified perspective of variational approaches, they develop a new estimator that focuses on variance reduction.
Reject. rating score: 3. rating score: 3. rating score: 6. This paper proposes to use a differentiable FFT layer to enforce hard constraints for results generated by a CNN. Ideally, other constraints than enforcing divergence freeness could be demonstrated to show the generality of employing an FFT projection in the loss function. The latter employs a curl formulation, and as such is less general, but probably faster than the FFT based method proposed here. In addition, the paper unfortunately contains only a single example.<BRK>This work develops a differentiable spectral projection layer to enforce spatial PDE constraints using spectral methods, to achieve the introduction of the physical constraints in the end to end network without damaging the intrinsic property of the network. The experimental comparison demonstrates the superiority of the proposed method. In my viewpoint, the novelty of this paper is somewhat novel. This paper focuses on designing the PDE layer to constrain the network output without additional loss. It is curious about the role and importance between FFT (IFFT) and spectral projection.<BRK>The paper describes a way to efficiently enforce physical constraints expressed by linear PDEs on the output of a neural network. For me, this is ok. Importantly, the proposed strategy is very general, and can indeed be used with any PDE constraint that is a linear combination of differential operators. On the negative side, the paper does not fully deliver on the promise to make physics constraints in deep networks usable in practice.<BRK>Enforcing physical constraints to solutions generatedby neural networks remains a challenge, yet it is essential to the accuracy and trustworthiness of such model predictions. To this end, they propose the use of a novel differentiable spectral projection layer for neural networks that efficiently enforcesspatial PDE constraints using spectral methods, yet is fully differentiable, allowing for its use as a layer in neural networks that supports end-to-end training.
Accept (Poster). rating score: 8. rating score: 6. rating score: 1. A bigger source of uncertainty is likely due to there being a limited amount of data to fit the coefficients to. I also liked that the paper is candid about its own limitations. This paper states 5 necessary criteria for any functional form for generalization error predictor that jointly considers dataset size and model size, then empirically verifies it with multiple datasets and architectures. The extrapolation section did provide evidence that there probably isn t /that/ much overfitting, but cross validation would directly address this concern.<BRK>Summary:This paper proposes a functional form to model the dependence of generalization error on a held out test set on model and dataset size. The parameters of the function are then fit using linear regression on observed data. The authors show that the regressed function \(\epsilon(m,n)\) is able to predict the generalization error for various \(m\) and \(n\) reasonably accurately. Overall, I think this is a well written paper and provides good insight into the behaviour of the error landscape as a function of model and dataset size.<BRK>This paper explores the relation among the generalization error of neural networks and the model and data scales empirically. The topic is interesting, while I was expecting to learn more from the paper, instead of some well known conclusions. For instance, how deep should a model be for a classification or regression task? What is the minimum/maximum size of the data set? What s the relation between the size of a model and that of a data set?<BRK>The dependency of the generalization error of neural networks on model and dataset size is of critical importance both in practice and for understanding the theory of neural networks. In this work, they present a functional form which approximates well the generalization error in practice. They show that the form both fits the observations well across scales, and provides accurate predictions from small- to large-scale models and data.
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. This paper improves the robustness of smoothed classifiers by maximizing the certified radius, which is more efficient than adversarially train the smoothed classifier and achieves higher average robust radius and better certified robustness when the radius is not much larger than the training sigma. It proposes a novel objective which is derived by decomposing the 0/1 certified loss into the sum of 0/1 classification error and 0/1 robustness error. It is not important but would be better if you could mention this point. Still, I am not sure about how much MACER improves upon the baselines, and would like to ask some questions.<BRK>One downside is that the paper does make fairly aggressive claims (e.g."performs better than all existing provable l_2 defenses"), but then only compares to two prior / baseline approaches in the experiments. This paper proposes a new approach to training models robust to perturbations (or  attacks ) within an l_2 radius, by maximizing a surrogate a soft randomized smoothing loss for the *certified radius* (a lower bound for the l_2 attack radius) of the classifier. The authors provide certain theoretical guarantees and also demonstrate strong empirical results relative to two baseline approaches. Since directly maximizing this robust radius is intractable, prior work seeks to derive a lower bound which the authors term the *certified radius*.<BRK>The paper presents a method for training a certified robust neural network, based on the certification method of cohen et al.I think this paper is quiet borderline, as it is a natural extension of cohen et al, but is incremental. It was nice that the authors gave a theoretical guarantee for the soft RS, but it is not clear if that is needed. You can train with soft RS and apply the hard RS at test time which has the standard guarantees  Fig.3 needs labels on x&y axis   The ablation study isn t really an ablation study, it is more testing the sensitivity of various parameters (which is good in itself).<BRK>In this paper, they propose the MACER algorithm, which learns robust models without using adversarial training but performs better than all existing provable l2-defenses. Recent work shows that randomized smoothing can be used to provide a certified l2 radius to smoothed classifiers, and their algorithm trains provably robust smoothed classifiers via MAximizing the CErtified Radius (MACER).
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. The work presents a novel and effective solution to the difficult task of learning reusable motor skills.<BRK>It is not clear to me how much the accuracy gain in the latent representation transfer to the accuracy of the actual recomposed task. The paper aims to learn middle level motor task primitives from unlabeled actions.<BRK>Although this paper approaches the problem very differently and the dimensionality of the primitives is lower, I still consider this paper very much related to what is presented by the authors.<BRK>In this paper, they present an approach to learn recomposable motor primitives across large-scale and diverse manipulation demonstrations. On the other hand, approaches in primitive discovery put restrictive assumptions on the complexity of a primitive, which limit applicability to narrow tasks.
Reject. rating score: 3. rating score: 3. rating score: 6. This paper proposed a variant of policy gradient algorithm with mirror descent update, which is a natural generalization of projected policy gradient descent. The authors also proposed a variance reduced policy gradient algorithm following the variance reduction techniques in optimization. The authors further proved the convergence of the proposed algorithms and some experiments are conducted to show the effectiveness of their algorithms. Note that $\epsilon_k$ is defined based on $\hat g_k$. Therefore, the proof is still problematic. Can the authors elaborate the differences discussed in the paragraph after eq (32) in more detail? Therefore, Lemma 3 does not hold in the setting of this paper. I think it would be more convincing to demonstrate the improvement in Table 1 by comparing the proposed VRMPO with at least one other variance reduced algorithm.<BRK>[Summary]This paper proposes MPO, a policy optimization method with convergence guarantees based on stochastic mirror descent that uses the average of previous gradients to update the policy parameters. Some terms like "projected gradient" and "baseline" (in the context of variance reduction) are not defined. The experiments are, however, limited and miss important baselines discussed in previous sections. The presentation is not clear.<BRK>This paper proposes a new policy gradient method that is based on stochastic mirror descent. It would also be more interesting after such reductions can be made, one can compare the sample efficiency of the proposed method with such state of the art policy gradient methods. The paper has good contributions in both theoretical and algorithmic aspects to policy optimization family. The empirical results are also very promising.<BRK>Improving sample efficiency has been a longstanding goal in reinforcement learning. In this paper, they propose the $\mathtt {VRMPO} $: a sample efficient policy gradient method with stochastic mirror descent. A novel variance reduced policy gradient estimator is the key of $\mathtt {VRMPO} $to improve sample efficiency.
Reject. rating score: 3. rating score: 3. rating score: 6. Summary:The authors propose augmenting VAEs with an additional latent variable to allow them to detect out of distribution (OOD) data. The authors attempt to justify the objective by writing out a variational lower bound for a VAE with a mixture prior where inliers and outliers are generated from different mixture components. Specifically, the authors  method proposes adding a term to the loss of the VAE that encourages the variational posterior (q) to distribute latent codes (z) for inliers and outliers differently. If it is the latter, then the equation is incorrect or at the very least not clear in the extreme.<BRK>This paper discusses the detection of out of distribution (OOD) samples for variational autoencoders (VAE). The idea is to train the encoder such that its output variational distribution q(z|\bar{x}) is pushed away from the prior of latent z. I think the paper needs more clarification and investigation for being published in the conference. 2) The use generated samples as negative samples is interesting but mysterious.<BRK>The paper proposes to counteract OOD problem in VAE by adding a regularization term to the ELBO. Unfortunately, I do not find the paper especially interesting. However, later they propose to skip a (negative) reconstruction error term for the negative data. This explanation is very vague and I do not see what it adds to the story. The Bernoulli distribution could be used only for binary random variables. In general, the results seem to partially confirm claims of the paper, however, they are quite vague. I really appreciate that the paper is updated and some concerns are solved.<BRK>Their model pushes latent images of negative samples away from the prior. Perhaps more surprisingly, they present a fully unsupervised variant that can also significantly improve detection performance: using the output of the generator as a source of negative samples results in a fully unsupervised model that can be interpreted as adversarially trained.
Reject. rating score: 3. rating score: 6. rating score: 8. This work focuses on credit assignment using a self attention module for transfer RL problems. WT is not explained in Fig.6. This can be helpful especially when the reward signal is sparse. Pros:  The writing is mostly great. Cons:  Some design choices are not well motivated or even problematic. Also what is "the rank of the rewards"? This is an important scenario to see whether SECRET will potentially create a negative transfer.<BRK>The paper introduces an interesting new direction in transfer learning for reinforcement learning, that is robust to the differences in the environtment dynamics. 1.The authors insist that their fous is on transfer and not competing on credit assignment.<BRK>Overall, this paper proposes an interesting general avenue for research in transfer learning in RL. Is there a reason why this is not demonstrated? They hypothesize that by learning how to assign credit, structural invariants can be learned which the agent can exploit to assign credit effectively and thus learn more efficiently in new environments (be it in domain or out of domain).<BRK>Despite the apparent promises, transfer in RL is still an open and little exploited research area. Their main contribution is Secret, a novel approach to transfer learning for RL that uses a backward-view credit assignment mechanism based on a self-attentive architecture. Consequently, it can be supplemented by transfer methods that do not modify the reward function and it can be plugged on top of any RL algorithm.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. For this reason, I may not know the details of technical terms, and I might not be the best person to review this work (when compared with the literature in this field). In this work, the authors applied value based reinforcement learning to learn an optimal policy for global parameter tuning in the parameter server (PS) that trains machine learning models in a distributed way using  stochastic gradient descent. Example parameters include SGD hyper parameters (such as learning rate) and system level parameters. While it is impressive to see that RL beats many of the SOTA baselines for parameter tuning, I also find that instead of using data in the real system to do RL training, the paper proposes generating "simulation" data by training a separate DNN. I wonder how the performance would differ if the RL policy is trained on the batch real data.<BRK>This paper studies how to improve the synchronization policy for parameter server based distributed SGD algorithm. A major challenge is to design the state and action spaces in the reinforcement learning setting. Compared to existing policies such as BSP, ASP and SSP, RL has an advantage to adapt to non stationary situations over the training process. The paper formalizes an RL problem by minimizing the total training time to reach a given validation accuracy. Although all the results are simulated in a controlled environment, Figure 4 gives a very interesting illustration showing the advantage of using the RL policy. I still have detailed comments (see below), but I find the paper well written, and the author(s) has obtained promising results. I still have some concern of the computation time obtain the RL state per step. In particular, the time cost to compute the loss L on different weights w. How do you address this issue? What is its size? It would be better to have more discussions in the paper or appendix.<BRK>This paper proposes to use deep RL to learn a policy for communication in the parameter server setup of distributed training. While it is a reasonable idea and the initial results are promising, the lack of an evaluation on a real cluster, or for training more computationally demanding models, is limiting. I fully appreciate the need to perform experiments in a controlled environment, such as the ones reported in the paper. Normally these are due to discrepancies between the assumed/simulated model, and real system behavior. Is it clear that deep RL is needed for this application, as opposed to more traditional RL approaches (either tabular, with suitably quantized actions, or a simpler form of function approximation?And to ask in the other direction, did you consider using a more complex policy architecture, e.g., involving an LSTM or other recurrent unit?<BRK>They apply a reinforcement learning (RL) based approach to learning optimal synchronization policies used for Parameter Server-based distributed training of machine learning models with Stochastic Gradient Descent (SGD). Utilizing a formal synchronization policy description in the PS-setting, they are able to derive a suitable and compact description of states and actions, allowing us to efficiently use the standard off-the-shelf deep Q-learning algorithm. As a result, they are able to learn synchronization policies which generalize to different cluster environments, different training datasets and small model variations and (most importantly) lead to considerable decreases in training time when compared to standard policies such as bulk synchronous parallel (BSP), asynchronous parallel (ASP), or stale synchronous parallel (SSP). To support their claims they present extensive numerical results obtained from experiments performed in simulated cluster environments. In their experiments training time is reduced by 44 on average and learned policies generalize to multiple unseen circumstances.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. This paper presents a black box style learning algorithm for Markov Random Fields (MRF). The approach doubles down on the variational approach with variational approximations for both the positive phase and negative phase of the log likelihood objective function. Is that the case? That said, it does seems like a fairly creative combination of existing approaches. More detail for this application of AdVIL would be nice. Experiments: The authors show the empirical advantages offered by the proposed method over the existing literature. For larger scale domains, I fear this could become an important obstacle to effective model training. Perhaps PCD 1 results in performance that is far better than AdVIL. It seems as though, in the application of AdVIL to the DBM, the authors are exploiting the structure of the model in how they define their sampling procedure.<BRK>The work proposes using variational distributions to model the model the inference of latent variables and model the partition function building on NVIL, thereby providing an algorithm that would work on general MRFs for both inference and learning. Since the two terms in the NLL are opposite in sign, it is a minimax operation and GAN like adversial training can be used. The paper shows providing tighter results to estimate the log partition function and comparisons on the digits dataset and Anneal importance sampling. The paper builds on NVIL by using two variational distributions for the NLL and how to solve the parameter estimation problem. I think this strategy can be tested more extensively on more types of general MRFs and more rigourous experimentation and that the community will benefit from reading from these ideas. Advil shows promise compared to the competing methods in some of the problems. 2.Inference can be done in general using approximate methods like variational message passing, QBPO among others that don t depend on graph structures either, how does this work compared when those algorithms are used with simple gradient descent while training the parameters? How would you define good approximation? The paper says the main comparison point is NVIL but different experiments either mention ALI, VCD or PCD which is confusing.<BRK>This manuscript proposes a new approach to fitting Markov Random Fields (MRFs). The general structure of the algorithm is amenable to many MRF structures and can be fairly straightforwardly applied to learning on a wide variety of problems. The theoretical analysis supports that the algorithm is reasonable. The approach, to me, seems novel in fitting MRFs. First, on the empirical results, there is a large literature on fitting RBM models, including many on scaling to much larger models. Also, to me, the classic MCMC+SGD is as much a black box as the proposed technique. Or succinctly, it should be made clear why I should use this over an MCMC approach. Second, the theoretical claims are nice, but the manuscript should be revised to address the limitations of the theory. It seems that as the optimization gets close to the solution, this is essentially the exact same condition. This also seems like it would get increasingly difficult as the number of hidden and visible units increases, so they should address how this Lemma holds as the theory scales.<BRK>They propose a black-box algorithm called {\it Adversarial Variational Inference and Learning} (AdVIL) to perform inference and learning on a general Markov random field (MRF). The two variational distributions provide an estimate of the negative log-likelihood of the MRF as a minimax optimization problem, which is solved by stochastic gradient descent. On the other hand, compared with existing black-box methods, AdVIL provides a tighter estimate of the log partition function and achieves much better empirical results.
Reject. rating score: 1. rating score: 6. rating score: 8. The work extends previous work (semi parametric topological memory, ref.[22]) in several ways. The authors’ claim that the proposed environment requires long term planning, but looking at the images this does not seem to be the case. The paper claims to perform zero shot generalization and to adapt to changes in the environment, like the slight changes in camera motion, variations in lightning, but it unclear how the solution solves this claim. As mentioned, the method is evaluated on an environment, which is too simple. This is known, and this information is buried in a dense set of equations which are difficult to decipher and do not add any further value to the paper. This image is responsible for the generalization to unseen environments, but it is a major drawback, as the image must be created beforehand.<BRK>The paper presents HTM, an extension of the semiparametric topological memory method that augments the approach with hallucinated nodes and an energy cost function. The paper is well written and clear. I believe such latent representations are an interesting approach to solving visual navigation and general planning. HTM provides an interesting and useful extension to SPTM, allowing both generalization to unseen environments and a more robust loss function. The original SPTM paper focuses on visual navigation from first person views. How does this method apply to such situations? Planning in real environments with real images, as done in [6].<BRK>The paper propose a novel visual planning approach which constructs explicit plans from "hallucinated" states of the environment. I vote for accepting this paper as it tackles two important problems: where to get subgoals for visual planning and what similarity function to use for zero shot planning. Furthermore, the paper is clearly written, the experiments are well conducted and analyzed. I would encourage the authors to discuss the following questions:1) Fidelity in Table 3   why is it lower for SPTM compared to HTM if both methods rely on the same generated samples? 3) Same question about fidelity/feasibility for HTM1/2?<BRK>In visual planning (VP), an agent learns to plan goal-directed behavior from observations of a dynamical system obtained offline, e.g., images obtained from self-supervised robot interaction. In addition, they learn a conditional VAE model that generates samples given a context image of the domain, and use these hallucinated samples for building the connectivity graph, allowing for zero-shot generalization to domain changes. A recent and promising approach to VP is the semi-parametric topological memory (SPTM) method, where image samples are treated as nodes in a graph, and the connectivity in the graph is learned using deep image classification. In this paper, they propose Hallucinative Topological Memory (HTM), which overcomes these shortcomings.
Reject. rating score: 3. rating score: 3. rating score: 3. Below please find my comments:Different from change pixel attack, the certification for rotation/brightness change in image classification, volume and pitch change in audio perturbations is much easier. To certify this attack, given a base classifier, we can simply do, for example, grid search, on the low dimensional space to find the worst case with very good accuracy. One contribution of this paper is in that: Theorem 3.2 is valid for random smoothing using gaussian with general covariance matrix.<BRK>In this paper, the authors generalize the randomized smoothing type of robustness certification to handle many types of attacks beyond norm based attacks, e.g., geometric perturbation, volume change, pitch shifts on audio data. At the core of the proposed generalization is using some interpolation which I think is quite straightforward. Graph Interpolating Activation Improves Both Natural and Robust Accuracies in Data Efficient Deep Learning, arXiv:1907.06800In sum, this paper studies the problem of certifying a broad class of adversarial attacks which is decent, however, the novelty is quite limited. Please address my questions during rebuttal.<BRK>Instead, motivated by the desire to certify robustness to other families of transformations, the authors propose to introduce a smoothing transform that operates in an additive way on the underlying parameter space (rather than on the raw input vectors, as in the case of \ell_2 randomized smoothing). The authors provide details for a few examples from image and audio data processing, and then show experimental results on ImageNet data. Comments:My assessment of this paper is that the level of novelty is relatively low.<BRK>They present a novel statistical certification method that generalizes prior work based on smoothing to handle richer perturbations. Concretely, their method produces a provable classifier which can establish statistical robustness against geometric perturbations (e.g., rotations, translations) as well as volume changes and pitch shifts on audio data. The generalization is non-trivial and requires careful handling of operations such as interpolation.
Reject. rating score: 3. rating score: 3. rating score: 6. This paper explores the role of the implicit alpha parameter when learning word embeddings. More concretely, word embeddings work by either implicitly or explicitly factorizing a co occurrence matrix, and the underlying parameter alpha controls how the singular values are weighted between the word and the context vectors. The authors provide theoretical insights on the role of alpha in relation with the original co occurrence matrix, and propose a new method to find its optimal value. I think that this is overall a solid work. In particular, the role of alpha in word embeddings was already studied empirically by Artetxe et al.(CoNLL 18, https://www.aclweb.org/anthology/K18 1028.pdf) for both word analogy and word similarity tasks, to the extent that Figure 2 in both papers is showing the exact same curves. However, the authors do not even cite it. As acknowledged in the paper, other authors like Levy et al.(TACL 15) also observed that the value of alpha was important in their experiments. I think that the right narrative for the paper should more in the line of "previous work showed that alpha behaves this and that way; we provide a theoretical explanation for this behavior, and derive a method to automatically find its optimal value". In other words, there is nothing in the training objective of word2vec that forces a symmetric factorization, and there is always an optimal solution with respect to this training objective that is arbitrarily asymmetric. It would make sense if the optimal alpha depended on the nature of the task (e.g.syntactic vs semantic), but I do not have any intuition (nor do the authors provide) as of why the vocabulary would be anyhow relevant. I also find the experimental evaluation to be somewhat weak. In particular, the proposed theory focuses in two phenomena (word similarity and word analogy) as stated in the abstract itself, but the empirical evaluation is limited to the word similarity task.<BRK>In particular, it looks at the set of embedding methods that explicitly or implicitly perform a matrix factorization and tries to understand why the word embeddings exhibit analogy structure and why words that are semantically similar get embedded close together. The mechanism it comes up with has to do with the alpha parameter that represents the powers of singular values of the matrix that was factorized to estimate the embeddings. It turns out that alpha controls the distance between the words in the embedding transformation process. Results are shown on several word similarity tasks. Comments: The paper offers fresh insights into the well studied problem of learning word embeddings. First, though the paper is well motivated and puts itself nicely in context of previous work, it needs a copy editor as there are many language/grammar issues some of which I highlight below. Second, and the main problem with the paper, is that the properties of the alpha parameter are intriguing but the experimental evaluation is underwhelming. The paper also needs to show the impact of the alpha parameter on the quality of embeddings learned for some downstream task e.g.NER, POS Tagging. Just showing results on word similarity tasks and computing correlations is not very insightful or useful.<BRK>In this paper, the authors study the word embedding, with a particular emphasize on the word2vec or similar strategies. To this end, the authors consider the matrix factorization framework, previously introduced in the literature, and also study the influence of an hyperparameter denoted by alpha. Roughly speaking, there are two major parts in the paper. On the other hand, they propose to choose optimally the hyperparameter alpha in order to ameliorate word embedding by better preserving the distance structure. Conducted experiments are convincing. We think that the major issue in this work is that it does not provide significant contributions with respect to the state of the art. Sci.Technol.(2016) 31: 624. https://doi.org/10.1007/s11390 016 1651 5Also available on ArXiv: https://arxiv.org/abs/1505.04891 Reply to RebuttalWe thank the authors for modifying the paper and the reply to out comments and suggestions. However, we still think that the paper is of low quality, due to straightforward extension to the paper of Levy et al from 2015. The authors have added a small section on related works, as recommended. However, they have removed the "Conclusion" section. The paper no longer has a conclusion and potential work that ends the paper.<BRK>As observed in previous works, many word embedding methods exhibit two interesting properties: (1) words having similar semantic meanings are embedded closely; (2) analogy structure exists in the embedding space, such that "emph {Paris} is to \emph {France} as \emph {Berlin} is to \emph {Germany}". They reveal how the relative distances between nodes change during this transforming process. Based on the analysis, they also provide the answer to a question whether the symmetrical factorization (e.g., \texttt {word2vec}) is better than traditional SVD method. They propose a method to improve the embedding further. The experiments on real datasets verify their analysis.
Reject. rating score: 1. rating score: 3. rating score: 3. The paper introduces a new type of layer, Farkas layers, that are designed to ameliorate the "dying ReLU problem". However, the paper doesn t have convincing baselines and doesn t dig deep enough into what the Farkas layer is actually doing. However, the reported results are for ResNets. 6.Our contributions, p2: “We empirically show an approximate 20% improvement on the first epoch over only using batch normalization.” I’m not sure what to make of this; improvements on the first epoch are only useful if they lead to overall improvements.<BRK>The authors propose a new `normalization  approach called Farkas layer for improving the training of neural networks. I am also not completely satisfied with the authors  explanation on why Farkas  layers work. This avoids the gradient becoming zero when all the units in a layer are dead. The empirical results show that this normalization method is effective, and improves the training of deep ResNets when no batch normalization is used.<BRK>This paper provides a new method to deal with the zero gradient problem of ReLU (all input values are smaller than zero) when BN is removed, called Farkas layer. The proposed Farkas layer is too simple and seems not work well. With BN, the FarkasNet does not show significant improvements than the traditional ResNet with BN on CIFAR 10, CIFAR 100 and ImageNet. Without BN, though FarkasNet shows significant improvements than ResNet.<BRK>Using elementary results from linear programming, they introduce Farkas layers: a method that ensures at least one neuron is active at a given layer. Focusing on residual networks with ReLU activation, they empirically demonstrate a significant improvement in training capacity in the absence of batch normalization or methods of initialization across a broad range of network sizes on benchmark datasets.
Reject. rating score: 1. rating score: 3. rating score: 3. This paper proposes a fine tune technique to help BERT models to learn & capture form and content information on textual data (without any form of structural parsing needed). They key addition to the classic BERT model is the introduction of the R and S embeddings. R &S are supposed to learn the information in text that is traditionally represented as the structural positions and the content bearing  symbols in those positions. The authors did a great job motivating the need for separating role and filler in the intro. F is not defined in either version the proposed HUBERT( Figure 2 or Figure 3). Did the authors mean S?<BRK>The paper considers two variants of the disentangling layer (TPR), one with LSTMs (figure 2) and the other with attention (figure 3). I read the paper eagerly and with excitement until I got to the results. I understand the authors are just trying to make a point that BERT does worse than their model in this case and that this is not good for transfer, but still I find this to be artificially constructed. The variations in the numbers seem small and possibly attributable to other factors. I hope the authors have the time to do this and consider the extra experiments.<BRK>More budget given to hyper parameter search for the models proposed in the paper. For each bert embedded token, the proposed method aims at disentangling semantic information of the word from its structural role. with several design choices such as: *  a regularization term to encourages the roles matrix to be orthogonal and hence each role carry independent information *  design the roles and symbols matrices so that the number of symbols is greater than the number of rolesIn evaluation authors design several experiments to show that: * Does transferring disentangled role & symbol embeddings improve transfer learning* the effectiveness of the TPR layer on performance? Conclusion: The paper introduces large claims and empirical results that correlate with, however the provided experiments are not done with enough control to attribute gains to the design choices provided in the paper.<BRK>They validate the effectiveness of their model on the GLUE benchmark and HANS dataset. They also show that there is shared structure between different NLP datasets which HUBERT, but not BERT, is able to learn and leverage. Extensive transfer-learning experiments are conducted to confirm this proposition.
Reject. rating score: 3. rating score: 6. rating score: 6. The gradients of the von Neumann divergence are provided for learning via backpropagation. Pros: 1.The use of von Neumann divergence as a loss for this task is perhaps novel. The paper should also include and perhaps compare to their datasets. I do not think the use of von Neumann divergence as a loss is the best choice one could have, esp. for a deep neural network learning setting. This divergence includes the matrix logarithm, which is perhaps computationally expensive. It is unclear why the paper decided to use von Neumann. 4.The experiments are not compelling, there are no comparisons to alternative models and the datasets used are small scale. Thus, it is unclear if the design choices in the paper have any strong bearing in the empirical performances. Overall, the paper makes an attempt at designing neural networks for learning SPD matrices. While, there are some components in the model that are perhaps new, the paper lacks any justifications for their choices, and as such these choices seem inferior to alternatives that have been proposed earlier.<BRK>This paper generalized neural networks into case where a semidefinite positive matrix is learned at the output. The paper presents theoretical derivations that look sound, and validating experiments on synthetic and real data. I must say my expertise does not really correspond to what is done in this paper, but I do not see any obvious flaws and the results look solid. I appreciated the discussion of limitations in section 6. I vote for acceptance with the weakest possible confidence level since it is likely I missed many important points.<BRK>While the field has had methods for years to estimate SPD matrices (such as the covariance matrix estimate in the reparameterization trick), this manuscript proposes a markedly different approach based on a different layer structure and repeated normalization steps based on Mercer Kernels. This loss appears to give significantly better solutions on synthetic data. While there is some interesting and potentially useful novelty in the approach, I have some concerns about the empirical evidence and modeling to truly determine the mMLP s utility. Why does mMLP/l_QRE outperform on E_quad? The network structure as a whole needs greater validation. Can the authors validate this structure versus the simpler structure of simply using left multiplications? I think that the heteroscedastic regression experiments don t evaluate on one of the key issues, which is uncertainty estimation. Also, heteroscedastic regression has a long history in neural networks, dating back to at least Nix and Wiegand in 1994. Please check Table 5(a), which states that you are only using a small number of training samples. How confident are you that the methods actually improve the prediction? How were the competing models tuned and optimized?<BRK>This includes neural networks such as the multilayer perceptron (MLP). Here, they introduce a new variant of the MLP, referred to as the matrix MLP, that is specialized at learning SPD matrices. Their construction not only respects the SPD constraint, but also makes explicit use of it. This translates into a model which effectively performs the task of SPD matrix learning even in scenarios where data are scarce.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. In this paper, the authors proposed a framework to generating a task scheduling for a compiler to reduce the execution cost of neural networks. The motivation is interesting, and the proposed method is technically reasonable.<BRK>SummaryThis paper proposes an ML based method to optimize TensorFlow Graph execution. Writing  The paper is well written and I enjoyed reading the paper. Method and ResultsSome confusion if the authors could answer:  I am very confused by one of the claims that “ the first work on learning a policy for jointly optimizing placement and scheduling”.<BRK>In this work the authors propose a deep RL approach to minimize the makespan and the peak memory usage of a computation graph as produced by MXNet/PyTorch/TensorFlow. Then the authors use a heuristic BRKGA to learn a policy for the placement of computation graphs, that actually works on unseen graphs. Overall this paper is well written, deals with an important practical problem.<BRK>They present a deep reinforcement learning approach to minimizing the execution cost of neural network computation graphs in an optimizing compiler.
Reject. rating score: 3. rating score: 3. rating score: 3. This paper studies an attack scenario, where the adversary trains a classifier in a way so that the learned model performs well on a main task, while after a certain permutation of the parameters specified by the adversary, the permuted model is also able to perform another secret task. For the attack scenario studied in this paper, it should be ideal to enable the model to perform both the main and the secret tasks at the same time. However, the permutation process could be very time consuming, especially when the number of model parameters goes large. For example, what happens if the main task is on a benchmark with a large label set? Post rebuttal commentsI thank the authors for the explanation of the threat model. However, I think my concerns are not addressed, and thus I keep my original assessment.<BRK>This paper proposed a novel an very interesting attacking scenario (the authors called it the Trojan horse attack) that aims to embed a secret model for solving a secret task into a public model for solving a different public task, through the use of weight permutations, where the permutations can be considered as a key in the crypto setting. Overall, the trojan horse attacking scenario considered in this paper is novel and provides new insights to the research in adversarial machine learning. While I agree with the authors  explanations on the difference between trojan horse attack versus multi task learning (shared data or not), my main concern is the lack of comparison and discussion to another secrecy based attack scheme, the "Adversarial Reprogramming of Neural Networks" published in 2019. In their adversarial reprogramming attack, the model weights also remain unchanged (and un permuted). In my perspective, they have the same threat model but adversarial reprogramming seems to be even stealthier as it does not use the secret data to jointly train the final model. *** Post rebuttal commentsI thank the authors for the clarification.<BRK>This paper proposes TrojanNet, a new threat model and corresponding attack in ML security. The paper demonstrates that it is possible for an adversary to train a network that performs well on some benign "base task," while also being able to perform a (potentially malicious) secret task when the weights are permuted in a specific manner. My main concern is with the validity of the threat model, as it seems to assume the ability to get arbitrary software (in particular, the program that applies the permutations) onto the victim s server, at which point permuting the weights of a deployed neural network is just one of endless malicious things an adversary can do. It would be interesting to explore whether the trojan nn attack can be executed in a scenario when the adversary does not have the ability to inject malicious code into the victim s server, just a standard model. (Note that these are just ideas and not requests for revisions.)<BRK>The complexity of large-scale neural networks can lead to poor understanding of their internal details. They show that this opaqueness provides an opportunity for adversaries to embed unintended functionalities into the network in the form of Trojan horse attacks. Their novel framework hides the existence of a malicious network within a benign transport network. Their attack is flexible, easy to execute, and difficult to detect. They prove theoretically that the malicious network's detection is computationally infeasible and demonstrate empirically that the transport network does not compromise its disguise.
Reject. rating score: 1. rating score: 3. In this paper, a tensor based anomaly detection system for satellites is proposed. The performance is evaluated with a real satellite data set. Though the results look interesting, I vote for rejection because the paper is not well fitted to the ICLR community.<BRK>The paper presents and evaluates an anomaly detection algorithm that is used for identifying anomalies in satellite data. E.g.how much influence does the adaptive thresholding have on the performance? Labels do not seem to be correct.<BRK>Although there have been several studies in detecting anomalies based on rule-based or machine learning-based approaches for satellite systems, a tensor-based decomposition method has not been extensively explored for anomaly detection. Because of the high risk and cost, detecting anomalies in a satellite system is crucial.
Reject. rating score: 3. rating score: 3. rating score: 3. Summary of the paper:          The paper proposes a Bayesian approach to make inference about latent variables such as un corrupted images. The authors use a GAN to estimate this prior distribution. That has already been done in the case of thevariational autoencoder. It is not clear how the HMC parameters are fixed. This questions the significance of the results. I have missed some references to related work on inverse problems.<BRK>In active learning, the proposed method should be compared with other methods such as Bayesian DNN using dropout, etc. In the first half of the paper, the general framework of the Bayes estimation is introduced. Then, The authors proposed how to incorporate GAN to the Bayesian inference. Though the Bayesian inference using GAN is a natural idea, learning algorithms proposed in this paper are simple and are not intensively developed.<BRK>This paper proposes to use a trained GAN model as the prior distribution for Bayesian inference to quantify the uncertainty. I do like the extension of applying the idea in physics problems. However, the paper demonstrates to us only the ability, not exactly “quantification”. There are some grammar issues in the paper. Given my major issue seems to be quite problematic, I currently would weakly reject this paper. But I don’t have a full picture over this area, I’ll read the rebuttal and see if I could raise the score.<BRK>Despite its many applications, Bayesian inference faces challenges when inferring fields that have discrete representations of large dimension, and/or have prior distributions that are difficult to characterize mathematically. In this work they demonstrate how the approximate distribution learned by a generative adversarial network (GAN) may be used as a prior in a Bayesian update to address both these challenges. They demonstrate the efficacy of this approach by inferring and quantifying uncertainty in inference problems arising in computer vision and physics-based applications.
Reject. rating score: 1. rating score: 3. rating score: 3. Instead of choosing the steps by heuristic, the authors propose to choose the key steps by augmenting the reward function with a penalty to decrease the ratio of attacks. I tend to vote rejection for this paper, given that the proposed algorithms seem incremental compared to the existing algorithms, and the experiments seem not sufficient enough to support the core claim proposed in the paper. Pros:  The paper is well written, with sufficient background and related work section for the paper to be self contained. The proposed framework is an interesting and practical framework for attacking RL agents. How sensitive it is to control the attack rate?<BRK>This paper proposes to learn the ‘key steps’ at which to to apply an adversarial attack on a reinforcement learning agent. The RL approach is compared with a random attack policy and two heuristic methods for attacking agents in games on the Atari benchmark. The setting addressed in this work, where the attacker only learns whether/when to attack or not is a greatly simplified version of the full problem. The results, currently, do not appear very significant because (1) the gap between the RL solution and the heuristics is very small and (2) these *appear* to be single runs without standard deviations displayed.<BRK>Contribution:This work presents a method for performing budgeted attacks on a RL agent where an attacker can apply a perturbation to a limited subset of the observations of the said agent. They then proceed to compare to to prior work that use heuristics based on the policy values to select the steps to attack. The paper is overall well written and easy to follow. Review:One major limitation of the work is that the attack rate is not readily modifiable. It uses a penalty term in the reward function with a tunable weight $\lambda$, but changing this weight seemingly requires retraining the opponent from scratch, which is unpractical. Overall, a rigorous way of comparing the methods need to be devised.<BRK>Deep reinforcement learning agents are known to be vulnerable to adversarial attacks. This paper introduces a novel reinforcement learning framework that learns more effective key steps through interacting with the agent. The proposed framework does not require any human heuristics nor knowledge, and can be flexibly coupled with any white-box or black-box adversarial attack scenarios. Experiments on benchmark Atari games across different scenarios demonstrate that the proposed framework is superior to existing methods for identifying more effective key steps.
Reject. rating score: 3. rating score: 3. rating score: 3. The authors do not provide any explanation for why this approach did not succeed in their settings. However, I have major concerns regarding the novelty of the proposed method and the theoretical rationale for the key design choices. Additionally, there is not much theoretical discussion about what the Gumbel Softmax adds to routing networks.<BRK>The paper proposes to learn the routing matrix in routing networks for multi task learning (MTL) using the gumbel softmax trick for binary random variables. However, I feel the experiments are not sufficient and I would encourage the authors to conduct more experiments and comparisons.<BRK>This paper applies the Gumbel softmax to optimizing task specific routing in deep multi task learning. Although the end results are good, and the approach is well motivated, I am leaning to reject, because the experiments have not made clear when the method works and how it behaves.<BRK>This paper proposes a novel per-task routing method for multi-task applications. However, this use of routing methods requires to address the challenge of learning the routing jointly with the parameters of a modular multi-task neural network.
Reject. rating score: 3. rating score: 3. rating score: 6. One might want to trade off understandability vs. mathematical rigor especially, if the paper does not rely on these concepts. However, the writeup and formatting is still very much sub standard and must be improved to make this paper worth publishing. All in all, the theoretical explanation and the bloated notation should be simplified and every equation should be embedded into an intuitive derivation.<BRK>In fact, even when compared against the approximate control ILQG by Li and Todorov (2007), it s unclear if the current method is better when given the same computation budget. In the same paper, Han et al.showed this method can solve an HJB equation in 100 dimensions. In general, the HJB equation and the FBSDEs can be much more difficult to work with.<BRK>The central idea is to use a recurrent network to transform the observations into a representation that can be used with solvers specifically tailored towards that class of problems. While it serves as an explicit mean to reduced the sample complexity of methods in RL, it appears to be about avoiding premature convergence in this work. **Decision:** I recommend to accept the paper for publication. The method appears to be theoretically founded and the experimental validation seems solid.<BRK>In this paper, they propose a deep learning based algorithm that leverages the second order Forward-Backward SDE representation and LSTM based recurrent neural networks to not only solve such Stochastic Optimal Control problems but also overcome the problems faced by previous approaches and scales well to high dimensional systems. The resulting control algorithm is tested on non-linear systems in robotics and biomechanics to demonstrate feasibility and out-performance against previous methods.
Accept (Spotlight). rating score: 6. rating score: 6. rating score: 3. Paper SummaryThis paper introduces a quantity termed the "one step generalization ratio" . The final section derives a relation which attempts to explain the correlation between the size of the expected gradient and the learning of features. Small Concerns On line 188 it is mentioned that GSNR is the "reason" for a particular phenomenon or explains a phenomenon. ReviewFirst, I should note that I am not well versed in this specific line of work (GSNR ,as other papers were cited with this concept), and thus I cannot readily assess whether the contributions are "well placed in the literature".<BRK>In this work, the authors suggest a new point of view on generalization through the lens of the distribution of the per sample gradients. The authors consider the variance and mean of the per sample gradients for each parameter of the model and define for each parameter the Gradient Signal to Noise ratio (GSNR). The GSNR of a parameter is the ratio between the mean squared of the gradient per parameter per sample (computed over the samples) and the variance of the gradient per parameter per sample (also computed over the samples). The GSNR is promising as a measure of generalization and the authors provide a nice leading order derivation of the GSNR as a proxy for the measure of the generalization gap in the model.<BRK>The paper defines the quantity of "gradient SNR" (GSNR), shows that larger GSNR leads to better generalization, and shows that SGD training of deep networks has large GSNR. However, I struggle to rate this paper, since I feel swamped with math. This is the reason I rate the paper Weak Reject. Some feedback points:Section 2.1:Eq.(1): It seems the common definition of SNR is the ratio of mean standard deviation. I think it would help the reader a lot to give some intuitive meaning to the GSNR value. What is the difference between \sigma and \rho? Seems one is on the data distribution and one on a sampled set.<BRK>In this paper, they provide a novel perspective on these issues using the gradient signal to noise ratio (GSNR) of parameters during training process of DNNs. The GSNR of a parameter is simply defined as the ratio between its gradient's squared mean and variance, over the data distribution. Based on several approximations, they establish a quantitative relationship between model parameters' GSNR and the generalization gap.
Accept (Talk). rating score: 8. rating score: 8. rating score: 8. This paper proposes to add a prior/objective to the standard MLE objective for training text generation models. The prior penalizes incorrect generations/predictions when they are close to the reference; thus, in contrast with standard MLE alone, the training objective does not equally penalize all incorrect predictions. In all cases, simply adding the proposed prior improves over a state of the art model. The authors show that it s effective to use a relatively simple fastText based prior, but it s possible to consider other priors based on large scale pre trained language models or learned models. I think it would be nice to discuss this paper and related works. My main criticism is that the writing was unfocused or unclear at times. It would have been more helpful to read more text in the intro that motivated the problem of negative diversity ignorance and the proposed solution. I would have also appreciated more analysis. How important is it to train fastText embeddings on the data itself?<BRK>The paper introduces a new Gaussian prior objective, "D2GPo", that addresses the fact that in sequence generative models, all incorrect predictions are penalized equally by MLE, a phenomenon which the authors refer to as the negative diversity ignorance drawback. The proposed objective is simple to implement and can easily be added on top of a regular cross entropy loss. The paper shows that the new objective shows consistent improvements across a wide range of tasks. I think this should really be clarified in the paper, and f needs to be defined formally. If I did not understand correctly, please point me out. What value did you use for the temperature T? Overall the paper tackles an interesting problem which I feel has received surprisingly little attention from the community.<BRK>This paper introduces the use of data dependent Gaussian prior, to overcome negative diversity ignorance problem that includes the exposure bias problem for sequence generation models. Experimental results show that the proposed method consistently improves the performance of the state of the art methods for neural machine translation, text summarization, storytelling, and image captioning. I lean to accept this paper. The proposed method is well motivated and shown to be effective in several tasks for language generation. The authors propose to define it as a Gaussian distribution. Another way to remedy the problem of KL divergence above is applying Wasserstein distance instead of KL divergence.<BRK>For typical sequence prediction problems such as language generation, maximum likelihood estimation (MLE) has commonly been adopted as it encourages the predicted sequence most consistent with the ground-truth sequence to have the highest probability of occurring. However, MLE focuses on once-to-all matching between the predicted sequence and gold-standard, consequently treating all incorrect predictions as being equally incorrect. To counteract this, they augment the MLE loss by introducing an extra Kullback--Leibler divergence term derived by comparing a data-dependent Gaussian prior and the detailed training prediction. The proposed data-dependent Gaussian prior objective (D2GPo) is defined over a prior topological order of tokens and is poles apart from the data-independent Gaussian prior (L2 regularization) commonly adopted in smoothing the training of MLE. Experimental results show that the proposed method makes effective use of a more detailed prior in the data and has improved performance in typical language generation tasks, including supervised and unsupervised machine translation, text summarization, storytelling, and image captioning.
Reject. rating score: 1. rating score: 6. rating score: 6. The authors present an approach to improve performance for retro synthesis of chemical targets in a seq2seq setting using transformers. It’s unclear how significant the results are.<BRK>The authors propose pre training models to help improve the model s generation to rare reactions. Both the pre training model and the mixture model are combining the specific domain knowledge to improve the generalization and diversity of retrosynthesis. There is not a huge algorithm novelty for the methods proposed in this paper, but they can well address the domain issues and improve the performance. My only concern is that the baseline model compared in the paper is Schwaller s work.<BRK>The authors provide a transformer based model to predict the reactants. Instead of just using token masking, they provide alternate proxy decompositions for a target molecule by randomly removing bond types that are likely to break and by transforming the target based on known templates. Overall, I think the community will benefit from this work.<BRK>They propose a new model for making generalizable and diverse retrosynthetic reaction predictions. Given a target compound, the task is to predict the likely chemical reactants to produce the target. On the 50k subset of reaction examples from the United States patent literature (USPTO-50k) benchmark dataset, their model greatly improves performance over the baseline, while also generating predictions that are more diverse.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. This paper studies the theoretical reasons why a randomly initialized decoder or autoencoder like architecture can prove useful for image denoising, by using early stopping. The paper is clearly written and the proofs are interesting. It will be of interest to the community.<BRK>The paper provides a theoretical study of regularization capabilities of over parameterized convolutional generators trained via gradient descent, in the context of denoising with an approach similar to the "deep image prior". I agree that the optimum is the same, but is the implicit bias from initialization the same? Whether or not this is a limitation of the study, it should be discussed further in the paper.<BRK>This paper studies the situation in which a two layer CNN with RELU nonlinearity is fit to a single image and the observation that it is able to fit a "natural" image in fewer iterations than a "noisy" image.<BRK>A surprising experiment that highlights this architectural bias towards natural images is that one can remove noise and corruptions from a natural image without using any training data, by simply fitting (via gradient descent) a randomly initialized, over-parameterized convolutional generator to the corrupted image. While this over-parameterized network can fit the corrupted image perfectly, surprisingly after a few iterations of gradient descent it generates an almost uncorrupted image.
Reject. rating score: 1. rating score: 1. rating score: 3. The paper is an empirical study that looks into the effect of neural network pruning on both the model accuracy as well as the generalization risk (defined as the difference between the training error and the test error). The authors argue that such discrepancy can be explained if we look into the impact of pruning on "stability". The first major issue I have with the paper is in their definition of stability. I don t believe that this definition adds any value. This makes the results nearly tautological (not very much different from claiming that the test accuracy changes if the test accuracy is going to change!). One part where this issue is particularly important is when the authors conclude that "instability" leads to an improved performance. So, the authors are saying that the test accuracy after pruning improves if it changes, which is another way of saying that pruning helps. If it is a contribution, the authors should include it in the main body of the paper. Third, there are major statements in the paper that are not well founded.<BRK>This paper studies a puzzling question: if larger parameter counts (over parameterization) leads to better generalization (less overfitting), how does pruning parameters improve generalization? To answer this question, the authors analyzed the behaviour of pruning over training and finally attribute the pruning s effect on generalization to the instability it introduces. I tend to vote for a rejection because (1) The explanation of instability and noise injection is not new. In this sense, the effect of over parameterization is on neural network training. However, pruning is typically conducted after training, so I don t think the fact that pruning parameters improves generalization contradicts the recent generalization theory of over parameterized networks. Particularly, these two phenomena can both be explained from Bayesian perspective.<BRK>From these experiments, the authors observe that pruning large score weights generates instable but high test accuracy and smaller generalization gap compared to pruning small score weights. The authors additionally study some other aspects of pruning (e.g., pruning as a noise injection) and conclude the paper. The authors proposed a new score E[BN] and all experiments are performed on this. The authors did not report the results for high sparsity. However, I think that it is quite trivial that the generalization gap is not a function of the number of parameters while it only provides the upper bound. It would be much more interesting if the same instability results same test accuracy even for different pruning algorithms.<BRK>To better understand this phenomenon, they analyze the behavior of pruning over the course of training, finding that pruning's effect on generalization relies more on the instability it generates (defined as the drops in test accuracy immediately following pruning) than on the final size of the pruned model. They demonstrate that even the pruning of unimportant parameters can lead to such instability, and show similarities between pruning and regularizing by injecting noise, suggesting a mechanism for pruning-based generalization improvements that is compatible with the strong generalization recently observed in over-parameterized networks.
Reject. rating score: 1. rating score: 3. rating score: 8. Review: This paper considers the problem of dropping neurons from a neural network. Notes:    Paper considers loss of function from loss of a few neurons.<BRK>This contribution studies the impact of deletions of random neurons on prediction accuracy of trained architecture, with the application to failure analysis and the specific context of neuromorphic hardware. Also, the results in the main part of the manuscript are presented to tersely: I do not understand where in table 2 dropout is varied.<BRK>This paper investigates the problem of fault teloranceon NN: basically how the predictions are affected byfailure of certain neurons at prediction time. Typos  "the error of the output of"  > "the error of the output"<BRK>In this paper, they address the fundamental question of the impact of the crash of a random subset of neurons on the overall computation of a neural network and the error in the output it produces. They give provable guarantees on the robustness of the network to these crashes.
Reject. rating score: 3. rating score: 3. rating score: 3. They tested the framework on both single objective and multi objective tasks. I don t mind if the paper gets rejected. The MCTS in LaNAS is one way that automatically focuses the search on the important regions. Currently, the proposed approach might limit the usability of the proposed method to other situations when accuracy is no longer that important. I would whether the author could visualize it in the next revision, which would be very cool. Why not use NasNet architecture for a fair comparison with other NAS papers? Do you only predict the mean and assume all variance is constant? This means that almost 3x more samples are needed, compared to what they claimed.<BRK>This paper provide a NAS algorithm using Bayesian Optimization with Graph Convolutional Network predictor. The paper is well written. In my opinion, the key point of the paper has nothing to do with GCN or multi objective. The important part is to use BO and EI to sample new architecture. However, no theoretical proof is provided to guarantee that the performance is getting better during while loop in Algorithm 1. 2.Eq.(9) focuses more on models with higher accuracy. However, those models with bad performance will be predicted inaccurately and may have a higher score than good models. 6.Algorithm 1 uses Pareto front, which does not exist when doing experiments on single objective search.<BRK>The authors emphasize that this method can be used for multi objective optimization and run experiments over NAS Bench, LSTM 12K and ResNet models. Graph embeddings:The authors have not considered other graph embeddings to use in their Bayesian regression setup. For large discrete combinatorial search spaces, this approach will not scale. Experiments The main claim of the paper is that this approach works well  for the multi objective case. The results in Sec 4.3 are using random as the only baseline. This is a pretty weak baseline.<BRK>Sample-based NAS is the most fundamental method aiming at exploring the search space and evaluating the most promising architecture. Inspired by the nature of the graph structure of a neural network, they propose BOGCN-NAS, a NAS algorithm using Bayesian Optimization with Graph Convolutional Network (GCN) predictor. Extensive experiments are conducted to verify the effectiveness of their method over many competing methods, e.g.128.4x more efficient than Random Search and 7.8x more efficient than previous SOTA LaNAS for finding the best architecture on the largest NAS dataset NasBench-101.
Reject. rating score: 3. rating score: 3. rating score: 6. 2)  Experimental results on CIFAR10 and ImageNet show accuracy degradation on clean test data. However, as authors are aware of, it needs to be validated experimentally. There are two concerns. 1) This paper needs to demonstrate the effectiveness of the input output Jacobian regularization over the input gradients regularization.<BRK>The main contribution of this paper is that it proposed an estimator of Jacobian regularization term for neural networks to reduce the computational cost reduced by orders of magnitude, and the estimator is mathematically proved unbiased. I have read the authors  response. Instead, efficient approximation should be emphasized. The authors should focus on the approximating algorithm rather than the merits of Jacobian regularization which has been discussed in [1].<BRK>Various experiments show that the behavior of Jacobian regularization and show that it is robust. Although I still feel the same for my score (6), in my opinion, the same issues exist for this paper, and the paper can be made stronger on those points. Each of these points seem to have important contributions for the field.<BRK>The stabilizing effect of the Jacobian regularizer leads to significant improvements in robustness, as measured against both random and adversarial input perturbations, without severely degrading generalization properties on clean data.
Reject. rating score: 1. rating score: 3. rating score: 6. The usage of chordal sparsity is interesting and innovative.<BRK>The authors emphasize that the proposed method is scalable.<BRK>Experiments on citation networks and the reddit network show that the proposed method is efficient. It may be interesting to explore different ways of making predictions based on this decomposition based inference.<BRK>Furthermore, they implement Chordal-GCN on large-scale datasets and demonstrate superior performance. The proposed model first decomposes a large-scale sparse network into several small dense subgraphs (called cliques), and constructs a clique tree.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. The paper proposes Mobile Transformer, an efficient machine translation model, which achieves state of the art results on IWSLT and WMT. The Mobile Transformer is base on long short range attention (LSRA) modules that combine a depthwise convolution branch to encode the local information and a self attention branch to capture the long range information. While LSRA is included in the search space of Evolved Transformer, surprisingly, their searching algorithm doesn t discover it. The paper is well written and easy to follow. The experiments are quite solid; however, it would be if the authors can report how Mobile Transformer performs on other language pairs or other NLP tasks. In my opinion, memory footprint or inference time on mobile devices can be more realistic. 5.In terms of inference latency, how much faster Mobile Transformer is compared to Transformer and LightConv?<BRK>The authors propose their method called "Long Short Range Attention (LSRA)," which separates the self attention layers into two different purposes, where some heads focus on the local context modeling while the others capture the long distance relationship. It also surpasses the recently developed comparative method called "Evolved Transformer" that requires a far costly architecture search under the mobile setting. This paper is basically well written and easy to follow what they have done. The experimental results look good. However, I have several concerns that I listed as follows. In fact, in the paper, they say, "To tackle the problem, instead of having one module for "general" information, we propose a more specialized architecture, Long Short Range Attention (LSRA), that captures the global and local information separately." There is a large mismatch (gap) between the main claim and what they have done. 2,I am not convinced of the condition of the so called "mobile setting (and also extremely efficient constraint)."<BRK>This paper presents a new technique (LSRA) improving Transformer for constrained scenarios (e.g., mobile settings). It combines two attention modules to provide both global and local information separately for a translation task. But the evaluation results are not so impressive and go in line with other previous efficient deep learning approaches for different domains. To summarize, the paper is addressing an important and interesting idea. Looking at the other comments and the feedback provided by the authors, I have a more positive feeling about the contributions of the paper which are now sufficiently demonstrated.<BRK>Notably, Lite Transformer outperforms the AutoML-based Evolved Transformer by 0.5 higher BLEU for the mobile NLP setting without the costly architecture search that requires more than 250 GPU years. In this paper, they present an efficient mobile NLP architecture, Lite Transformer to facilitate deploying mobile NLP applications on edge devices. The key primitive is the Long-Short Range Attention (LSRA), where one group of heads specializes in the local context modeling (by convolution) while another group specializes in the long-distance relationship modeling (by attention). Such specialization brings consistent improvement over the vanilla transformer on three well-established language tasks: machine translation, abstractive summarization, and language modeling.
Reject. rating score: 1. rating score: 1. rating score: 3. This paper proposes a so called self supervised method for learning from time series data in healthcare setting. The authors need to state what is the novelty of the proposed method compared with [1].<BRK>6) There is no comparison with recent work on the same datasets. There are major concerns that should be clarified or described in detail. 1) The overall architecture is not complete. 2) To this reviewer, the idea of self supervision is similar to the unsupervised learning for representation learning.<BRK>This paper propose an approach for self supervised learning on time series. On any of the applied problem it is not clear if the proposedapproach brings an improvement on the state of the art or if it sjust an illustration of the method disconnected from the literatureof the application. The paper is overall well written and addresses the relevant issueof learning from limited annotated data.<BRK>However, in many cases, particularly in healthcare, one may not have access to additional data (labeled or otherwise). In such settings, they hypothesize that self-supervision based solely on the structure of the data at-hand can help. They call this approach limited self-supervision, as they limit ourselves to only the data at-hand. They demonstrate the utility of limited self-supervision on three sequence-level classification tasks, two pertaining to real clinical data and one using synthetic data.
Accept (Talk). rating score: 8. rating score: 8. rating score: 6. The paper explores a critical divergence between theory and practice, emphasizing that while deep policy gradient algorithms seem to work in certain cases, they don t seem to be working foor the reasons underlying their derivations.<BRK>This is an interesting and important paper, it emphasizes and analyzes how policy gradient methods modify their objective functions and how this leads to training differences (and often errors w.r.t.the true objective). You ll notice that in the TRPO paper, it is called a surrogate objective not a surrogate reward: https://arxiv.org/pdf/1502.05477.pdf . + In fact, it was a bit unclear whether the comparisons were of the sampled/observed reward function R(s,a) (provided by the environment and sampling regime) or the objective function often the advantage A(s,a) (or the surrogate objective, GAE, etc.)<BRK>[Summary]This paper empirically studies the behavior of deep policy gradient algorithms during the optimization. To the best of my knowledge, the findings of this paper are new and not predictable by the current theory.<BRK>They study how the behavior of deep policy gradient algorithms reflects the conceptual framework motivating their development.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. I would actually expect the proposed approach to work better (or at least differently) but it would be interesting to see it confirmed. I find this approach interesting and like the paper overall. There is an interesting relation to the NIPS 2019 paper: https://arxiv.org/abs/1907.04944  They also rely on gradient descent to steer a pretrained language model. Their goal is to assess the degree of  steerability  rather than building a controlled generation model.<BRK>The authors describe a method for training plug and play language models, a way to incorporate control elements into pre trained LMs. This is exciting and a great research direction. However, there does not appear to be any evaluation with any existing work that performs controlled text generation. I imagine the authors will emphasize that that s not fair   because their method doesn t require retraining the language model   but it is relevant to demonstrate if there is a gap in performance or not. 2.Can the authors make a point or discuss the relationship of this work to neural style transfer?<BRK>The paper proposes a Plug and Play LM model for controlled natural language generation. In this sense, how useful these kinds of text generation techniques are not clear to me. Similarly for sentences in Table 1. In short, the idea in the paper is simple and seems effective. I am willing to increase my evaluation score if I will be convinced by other reviews and comments.<BRK>However, controlling attributes of the generated language (e.g.switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. They propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. Moreover this work do not provides significantly better (factor 8 improvement in the constants) convergence rate that the one provided on the literature (Tseng 1995, Mokthari et al 2019, Gidel et al.2019) to solve bilinear games. You should not claim it as a contribution. After rebuttal   I ve read the authors s response. To me, it is an accept.<BRK>In addition in their figures 4 and 5 they compare stochastic methods. I find the theoretical results of the paper interesting and promising, however i believe that the proposed analysis will be difficult to extend beyond bilinear games to more practical scenarios as the authors claim. This approach is not novel. The authors should be more clear (from the abstract) on what algorithms they study.<BRK>This should render the results reproducible. 5) EvaluationThe evaluation is on synthetic settings and the results on the convergence seem convincing but a subtantial optimization problem remains for future work. 6) Questions/Issues  A) Not sure about the implications, isn t the set of saddle points in eq (2.2) equivalent to x y 0?<BRK>As a first step, they restrict to bilinear zero-sum games and give a systematic analysis of popular gradient updates, for both simultaneous and alternating versions. They provide exact conditions for their convergence and find the optimal parameter setup and convergence rates.
Reject. rating score: 3. rating score: 3. rating score: 8. Although the novelty is limited, which is just to combines the DCT/DHWT with NN, the experiments are sufficient and I am glad to see that this simple idea works in practice. Concerns: This paper is based on the practical validation. However, the rebuttal does not address my concerns. The logic behind the authors response is that since there is much work that uses DCT / DHWT like features in computer vision tasks, we can also use them to replace the conventional PC layers. Unfortunately, I do not think that is the theory to support the idea described in this paper. Actually, the success of CNN architecture has already confirmed the advantage of conventional PC layer, it seems it is a backward step for the community. Moreover, I agree with Reviewer#4 that ImageNet should be used as benchmark to so the advantage of the paper.<BRK>This paper presents a new pointwise convolution (PC) method which applies conventional transforms such as DWHT and DCT. The proposed method aims to reduce the computational complexity of CNNs without degrading the performance. Although this paper is well organized and easy to follow, the novelty of the proposal seems limited and the performance improvement claimed by the author(s) is not very convincing due to the insufficiency of experiments. I wonder whether there is a more theoretical explanation for that. Moreover, the experiments are performed only on a small dataset CIFAR100. More experiments on larger scale datasets like ImageNet are recommended to make results more convincing.<BRK>Summary: This paper proposes using non parametric filters like Discrete Cosine Transform (DCT) and Discrete Walsh Hadamard Transform (DWHT) which have been widely used as feature extractors in vision and image processing before deep learning became prevalent as layers especially to replace pointwise convolution (PC) layers in deep network architectures like ShuffleNet v2 and MobileNet V1. They show experiments on cifar100 datasets. Comments:  The paper is overall easy to read although the writing and presentation can use some work. I think these aspects are being generally overlooked currently but wont be surprised to see more of these papers. (Just a suggestion for future work not asking for this in the rebuttal.)<BRK>Some conventional transforms such as Discrete Walsh-Hadamard Transform (DWHT) and Discrete Cosine Transform (DCT) have been widely used as feature extractors in image processing but rarely applied in neural networks. This paper firstly proposes to apply conventional transforms on pointwise convolution, showing that such transforms significantly reduce the computational complexity of neural networks without accuracy performance degradation. Especially for DWHT, it requires no floating point multiplications but only additions and subtractions, which can considerably reduce computation overheads.
Reject. rating score: 3. rating score: 3. rating score: 6. The reported accuracy gains are substantial. 3) This is similar to (2), but why Resizable NAS is better than the all len(L) models separately to find better architectures? The revision made the paper easier to understand. If the other two reviewers think that the revised paper is fairly well written and recommend acceptance, I will not challenge the decision. I guess it is partially because I did not understand why the number of sampling should be len(L) for each mini batch (it is actually due to fair sampling).<BRK>(+) The experimental results look promising. ( ) I recommend the authors redraw all the figures for clarity. As shown in the paper [1],  one can find that the author presented they could improve both accuracy and efficiency. One of the main problem I think is the training budget issue. Thus, it does not seem to be fair comparison in terms of the training budget. About rating)The authors provided a novel technique about the resizable approach and the experimental results look promising.<BRK>The results are impressive, but error bars would be appreciated if possible. I think this is a really neat idea, and as far as I m aware it is novel. For distillation, I recommend you cite https://arxiv.org/abs/1312.6184, as the Hinton paper is really just an extension of this. Do you given the performance as a result of naive sampling? I assume energy use is proportional to the number of FLOPS, which in turn depends on spatial resolution.<BRK>In this paper, they present a deep convolutional neural network (CNN) which performs arbitrary resize operation on intermediate feature map resolution at stage-level. Motivated by weight sharing mechanism in neural architecture search, where a super-network is trained and sub-networks inherit the weights from the super-network, they present a novel CNN approach. Such network, named as Resizable Neural Networks, are equivalent to training infinite single scale networks, but has no extra computational cost. Moreover, they present a training algorithm such that all sub-networks achieve better performance than individually trained counterparts.
Reject. rating score: 3. rating score: 6. rating score: 6. The authors propose a method that incorporates two types of graphs into a graph convolutional networks:(1) the given graph, which the authors refer to as node affinity graph, and (2) the graph that models an affinity between node attribute values. The main contribution of their work is a type of graph convolution that combines convolutions operating on these two graphs. Trying to provide some theoretical analysis of the proposed method (and standard graph convolutions) by showing that the intra class variance is reduced is laudable. The authors discuss related work sufficiently with one exception: there has been recent work on learning the structure of graph neural networks. They don t really add much to the core of the paper.<BRK>This work proposes a 2D graph convolution to combine the relational information encoded both in the nodes and in the edges. The basic idea is to reduce the intra class variance. The authors provide theorems and proofs to support this claim even though it is quite intuitive that smoothing with similar neighbours preserves higher variance with respect to dissimilar neighbours. It is not straightforward to understand the limitations on the size of graphs.<BRK>The paper proposes a new 2 D graph convolution method to aggregate information using both the node relation graph and the attribute graph generated using, e.g., PMI and KNN. Besides, the paper also includes a detailed discussion of intra class variance reduction. Overall, it is an interesting paper. The experiments seem to show that the performance is quite different. It would be good to have some discussion on this.<BRK>Graph convolutional neural networks have demonstrated promising performance in attributed graph learning, thanks to the use of graph convolution that effectively combines graph structures and node features for learning node representations. To address this problem, they propose to explore relational information among node attributes to complement node relations for representation learning. In particular, they propose to use 2-D graph convolution to jointly model the two kinds of relations and develop a computationally efficient dimensionwise separable 2-D graph convolution (DSGC). Theoretically, they show that DSGC can reduce intra-class variance of node features on both the node dimension and the attribute dimension to facilitate learning.
Reject. rating score: 3. rating score: 6. rating score: 6. Derive a generalization bound on the performance of adversarially robust networks  that depends on the margin between training examples and the decision boundary. Thus, I believe that the results could represent a significant contribution. Each of these papers should be mentioned along with their corresponding contributions. Moreover, it would also be useful to provide some intuition about how margins relate to the confidence of a classifier. It would greatly increase the clarity of the result if each term was explained intuitively, so that the readers can gain the main insight of the paper before reading the proofs. The plots in FIgure 3 and Figure 4 are very difficult to understand and unintuitive. Hence, the information from the other curves is mostly lost.<BRK>The paper proposes to explain a phenomenon that the increasing robustness for adversarial examples might lead to performance degradation on natural examples. Despite interesting, there are still some major concerns regarding the paper：1. 2.Only PGD was used as the adversarial perturbations in the experimental part of this paper. It would be more convincing if the authors could perform analysis on different adversarial training methods, e.g.FGSM and even the unified gradient perturbations developed in C2. In summary, it is good that a theoretical bound can be derived from the paper, but this paper s quality may need more enhancement particularly on its writing and experimental parts. I have carefully read the response as  well as the revised paper.<BRK>Summary:This paper focuses on analyzing the regularization of adversarial robustness (AR) on neural networks (NNs). The paper is well written with theoretically motivated experiments and detailed analysis. I d suggest accepting the paper. 2) Empirically examples are concentrates around decision boundaries. 3) The samples concentration around decision boundaries smooths sudden perturbation change, but also degrades model performance. It d nicer to further analyze on how to obtain AR without sacrificing performance on natural examples.<BRK>The problem of adversarial examples has shown that modern Neural Network (NN) models could be rather fragile. Among the most promising techniques to solve the problem, one is to require the model to be {\it $\epsilon $-adversarially robust} (AR); that is, to require the model not to change predicted labels when any given input examples are perturbed within a certain range. However, it is widely observed that such methods would lead to standard performance degradation, i.e., the degradation on natural examples.
Reject. rating score: 3. rating score: 3. rating score: 3. The authors study the sample complexity of adversarially robust learning with access to unlabeled samples. The work of Uesato et al.2019 is virtually a superset of the results in this manuscript. Both works collect additional images from an unlabeled and uncurated dataset (Tiny Images) and show that they can utilize them using their proposed approach to improve the state of the art robust accuracy on CIFAR10. Unfortunately, the paper is concurrent with two other works (which the authors acknowledge: Carmon et al.2019, Uesato et al.2019) which have already been accepted for publication at NeurIPS 2019.<BRK>The authors first consider the toy model presented in Schmidt et al.and show how the labeled sample complexity in the robust setting can be lowered to match the standard setting if sufficient unlabeled data is available. Comments: The problem the paper seeks to address (bridging the generalization gap in the adversarial setting) is an important one, and the paper is clear and well written. As the authors discuss, there have been three (other) independent papers that tackle the same problem (which were accepted at NeurIPS). Unlike the other papers, the algorithm discussed in the theoretical section (which is able to reduce sample complexity by leveraging unlabeled data) is entirely different from the one used in practice on MNIST/CIFAR. In particular, the authors report that VAT attains poor robustness (<2.5% for both 5k and 10k labeled). I could not find any difference between the two baselines except for the fact that Uesato et al.implement VAT with a KL divergence penalty (as suggested in the VAT paper) instead of cross entropy (as is used in this paper). The overlap with concurrent work is unfortunate, and it makes it hard to evaluate this paper.<BRK>This paper considers the problem of adversarial robustness. The paper shows that (Theorem 1) robust generalization error can be bounded in terms of the standard generalization error and a stability term, that does not depend on the labels. The paper suggests that we can use unlabeled data to improve the robust generalization. The exact implementation while can be different between these two, the paper does not currently compare with this and there is no evidence to prefer this regularizer over the existing one. Given the other parallel works studying the same setting, it is good to also include a comparison of the exact results (such as sample complexity) for this setup.<BRK>Neural network robustness has recently been highlighted by the existence of adversarial examples. In this paper, they theoretically and empirically show that with just more unlabeled data, they can learn a model with better adversarially robust generalization. The key insight of their results is based on a risk decomposition theorem, in which the expected robust risk is separated into two parts: the stability part which measures the prediction stability in the presence of perturbations, and the accuracy part which evaluates the standard classification accuracy. As the stability part does not depend on any label information, they can optimize this part using unlabeled data.
Reject. rating score: 3. rating score: 3. rating score: 3. This paper proposes Sparse Deep Predictive Coding (SDPC) to access the impact of the inter layer feedback, which is suggested by neuro scientific evidence. The SDPC model is compared with HILA on 2 different databases, and the experimental results show that SDPC achieved lower prediction error, faster converge rate. Moreover, the experiments are not sufficient to valid the advantage of this model. **updated comments**Thanks for the response and I really appreciate the hard work during the rebuttal. However, the STL 10 is still a small scale dataset, and the baselines provided in the rebuttal are still limited.<BRK>This paper presents a study that compares two techniques for Hierarchical Sparse Coding. It is found that the top down term is beneficial in terms of reducing predictive error and can learn faster. In terms of novelty the new term, the paper does not make a breakthrough contribution, but I consider this to be sufficient.<BRK>Instead of decomposing the HSC problem into independent subproblems, the proposed model added a new term to the loss function, which represents the influence of the latter layer on the current layer. (4) For section 2.2 and 2.3, the number/index of samples is not shown in the loss function for training.<BRK>In this study, a new model called Sparse Deep Predictive Coding (SDPC) is introduced to assess the impact of this inter-layer feedback connection. A 2-layered SDPC and a Hi-La networks are trained on 3 different databases and with different sparsity parameters on each layer. Second, they demonstrate that the inference stage of the SDPC is faster to converge than for the Hi-La model.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. In this paper the authors solve for the task of Raven Progressive Matrices (RPM) reasoning. The basic premise is a combination of object level representation that is obtained by a method similar to region proposal and combining them with graph network. The approach uses gated graph networks that also uses an aggregation function. This provides improved results over earlier WREN method. Over all while the contribution is useful, not much analysis is provided on the interpretability of the results. To conclude, I believe this paper provides a useful contribution by modeling the diagrammatic abstract reasoning as a graph based reasoning approach.<BRK>This paper proposes using a new version of graph networks – multiplex graph networks – which do object representation followed by some form of graph processing and reasoning to answer "IQ test" style diagrammatic reasoning, in particular including Raven Progressive Matrices that have been previously studied (a little). The paper shows very strong results on multiple datasets, much stronger than previous results (from strong groups) on these datasets. However, the structure and writing of the paper was very frustrating to me. The paper just didn t make much of an attempt to explain and then motivate/analyze the model used.<BRK>The paper proposes a novel, feedforward, end to end trainable, deep, neural network for abstract diagrammatic reasoning with significant improvements over the state of the art. However, the writing quality is poor and is the primary reason for my giving it a low score. Graphs are conceptual in the proposed approach – there doesn’t seem to be any graph algorithms or graph based processing. The proposed model is non interpretable. if interlayer connections are between objects in different layers (diagrams), what is this supposed to capture?<BRK>MXGNet combines three powerful concepts, namely, object-level representation, graph neural networks and multiplex graphs, for solving visual reasoning tasks. MXGNet first extracts object-level representations for each element in all panels of the diagrams, and then forms a multi-layer multiplex graph capturing multiple relations between objects across different diagram panels. MXGNet summarises the multiple graphs extracted from the diagrams of the task, and uses this summarisation to pick the most probable answer from the given candidates. They have tested MXGNet on two types of diagrammatic reasoning tasks, namely Diagram Syllogisms and Raven Progressive Matrices (RPM). For PGM and RAVEN, two comprehensive datasets for RPM reasoning, MXGNet outperforms the state-of-the-art models by a considerable margin.
Reject. rating score: 3. rating score: 3. rating score: 3. While K FAC has been developed as approximation to the exact natural gradient update, they come up with a different Riemannian metric, definition of space, etc., such that in the end, K FAC is the exact natural gradient for that. The paper uses very heavy math, well "over my head" and likely most ICLR attendees.<BRK>This paper analyzes the invariance properties of the K FAC algorithm by reconstructing the algorithm in a coordinate free way where the neural network is viewed as a series of affine mappings alternating with nonlinear activation functions. So K FAC can be viewed as the exact natural gradient under the new metric rather than an approximation under the Fisher metric. Without empirical studies, it is not easy to see the significance of this work.<BRK>The way paper is written has a high risk of causing confusion between coordinate dependent and coordinate free objects. This present paper offers a different approach: rather than approximating the exact NG directly as K FAC does, it views the approximate K FAC natural gradient as the exact "natural gradient" under the K FAC metric.<BRK>They explicitly construct a Riemannian metric under which the natural gradient matches the K-FAC update; invariance to affine transformations of the activations follows immediately. They extend their framework to analyze the invariance properties of K-FAC appied to convolutional networks and recurrent neural networks, as well as metrics other than the usual Fisher metric.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 6. rating score: 6. The algorithm quantizes matrices of linear operations, and, by generalization, also works on convolutional networks. Overall I find the paper interesting and enjoyable. Generally, it is not ovious or given that the current method would be able to compress general matrices well, as it implicitly assumes that weight W_{ij} has a high "correlation" with weights W_{i+kN/m,j} (which I call "vertical" correlation), W_{i,k+some_number} (which I call "horizontal" correlation) and W_{i+kN/m,k+some_number} (which I call "other" correlation). Matrices can have either "horizontal" or "vertical" redundancy (or "other" or neither). It would be very interesting to see which kind of redundancy their method managed to caprture. This can be a symptom of a redundant input. It would be interesting to see how this would affect compressibility.<BRK>The method can be expected to perform well empirically, which the experiments verify, and to have potential impact.<BRK>This paper proposes to use codes and codebooks to compress the weights. This paper is overall easy to follow. My main concern comes from the novelty of this paper. Clarification of the connections/differences, and comparison with these related methods should be made to show the efficacy of the proposed method. It is not clear how the compression ratio in table 1 is obtained.<BRK>This paper addresses to compress the network weights by quantizing their values to some fixed codeword vectors. Overall, I think that the proposed algorithm is easy to apply and the draft is relatively well written.<BRK>In this paper, they address the problem of reducing the memory footprint of convolutional network architectures. They introduce a vector quantization method that aims at preserving the quality of the reconstruction of the network outputs rather than its weights. The principle of their approach is that it minimizes the loss reconstruction error for in-domain inputs. They validate their approach by quantizing a high performing ResNet-50 model to a memory size of 5MB (20x compression factor) while preserving a top-1 accuracy of 76.1% on ImageNet object classification and by compressing a Mask R-CNN with a 26x factor.
Reject. rating score: 6. rating score: 6. rating score: 6. # 1.SummaryThe authors introduce a new pre training procedure for image text representations. The idea is to train the model on a huge collection of different image text datasets and the use the model for downstream tasks. Using two streams might also lead to learning context? This should be clarified in the main text of the paper. # 3.NoveltyThe novelty of the paper is quite limited since it is an extension of BERT to the visual domain. The authors propose an empirical analysis of different ways to mask the visual input, however this might not be a substantial extension of previous work. The evaluation on both pre training tasks and downstream tasks show that the method is working well in practice.<BRK>This paper presents a novel method for image text representations called UNITER. A detailed ablation study helps to understand the role of each pretrained task in the proposed model. Although the empirical results are nice, performing the intensive set of experiments on many different tasks is definitely time consuming and needs a lot of engineering efforts, the technical contribution does not seem significant to me. The paper modifies an existing pre training procedure by conditional masking (Section 2).<BRK>This is an impressive paper. LIke BERT, it proposes a tranformer based approach to derive a pre trained network for representing images and texts. The major limitation of this paper is why. Why does it happen? How this results can be achieved? Why the tasks used for pre training build a network that is so informative?<BRK>Joint image-text embedding is the bedrock for most Vision-and-Language (V+L) tasks, where multimodality inputs are jointly processed for visual and textual understanding. In this paper, they introduce UNITER, a UNiversal Image-TExt Representation, learned through large-scale pre-training over four image-text datasets (COCO, Visual Genome, Conceptual Captions, and SBU Captions), which can power heterogeneous downstream V+L tasks with joint multimodal embeddings. Different from concurrent work on multimodal pre-training that apply joint random masking to both modalities, they use Conditioned Masking on pre-training tasks (i.e., masked language/region modeling is conditioned on full observation of image/text). They also conduct a thorough ablation study to find an optimal combination of pre-training tasks for UNITER.
Reject. rating score: 3. rating score: 3. rating score: 8. In this paper, the authors propose a method for black box adversarial image generation. This ultimately leaves me with the question of what to do with this paper. My main source of questions is the experimental results section, which I currently view as somewhat weak and a little confusing   I would be more than happy to increase my score if my concerns are sufficiently addressed. The conclusion primarily focuses on the introduction of the method: does it have substantial merits past its empirical performance?<BRK>This paper deals with the problem of finding an adversarial examples when only the output of a model can be evaluated, but not its gradient. The key idea of the paper is building a Gaussian MRF (a Gaussian with a sparse inverse covariance matrix with a special band structure) to maintain a model for the gradients for predicting search directions. The evaluation of the idea is not complete While it is certainly interesting to see that such a simple heuristic can achieve comparable performance on standard datasets over other black box attack methods in the limited query budget regime, I would expect to see some experiments that illustrate the quality of the gradient estimator more closely (not only its final effectiveness for finding a search direction inside of an optimization method) and more strong justification of the proposed model.<BRK>This paper employs Markov random fields to exploit the input data structure and further model the covariance structure of the gradients. The numerics show effective for using fewer queries to obtain high attack accuracy. This paper is well written with clear derivations. I suggest the publication of the paper.<BRK>They study the problem of generating adversarial examples in a black-box setting, where they only have access to a zeroth order oracle, providing us with loss function evaluations. They employ Markov Random Fields (MRF) to exploit the structure of input data to systematically model the covariance structure of the gradients. The resulting method uses fewer queries than the current state of the art to achieve comparable performance.
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. The cost function is optimized via a loss function derived from side information, specifically subsets of the two datasets to be aligned that contain elements that should be matched in the optimal transport plan. I recommend that the paper be accepted.<BRK># SummaryThis paper proposes a new way to learn the optimal transport (OT) cost between two datasets that can utilize subset correspondence information. The prior work does not have a mechanism to utilize such information or requires more side information such as pair wise matching. # Quality  The idea of parameterizing the transport cost using a neural network is novel to my knowledge. Although the paper presents real world examples (RNAs), the paper could be stronger if it presented more real world examples/results and emphasized that this is an important problem to solve.<BRK>The authors formulate it as a transport cost learning in optimal transport framework with constraints giving by side information. The paper show good results of the proposed method for dataset alignment on several applications. It is better if the authors give more detail information and discussion.<BRK>Optimal transport (OT) is a principled approach to align datasets, but a key challenge in applying OT is that they need to specify a cost function that accurately captures how the two datasets are related. The side information they consider captures subset correspondence---i.e.certain subsets of points in the two data sets are known to be related.
Reject. rating score: 1. rating score: 3. rating score: 3. The authors should provide more evidences to justify the approximation. The paper is well written and the experiment section is extensive.<BRK>The paper did not really discuss this issue, and from my own thoughts I don’t think the task avoids this issue. Given this, I don’t think the task of comparing <f, \mu’_l, \nu’_l> with the POT results really says anything about their power in computing W. I would be glad though to hear back from the authors to see if my understanding is accurate, and adjust my evaluation from there.<BRK>I find the presentation of the different background work and models to be excellent, especially for someone who s not expert on WGANs like me. The contributions of the paper are experimental. However, the goodness of the approximation is measured with (24), which the authors called "subjective error".<BRK>In practice, the potential is approximated with a neural network and is called the discriminator. In this work, they study how well the methods, that are used in generative adversarial networks to approximate the Wasserstein metric, perform.
Reject. rating score: 1. rating score: 3. rating score: 6. Second, the paper addresses the high dimensional optimisation with simple upsampling technique like nearest neighbour, without even trying their hands dirty by using one of many many high dimensional Bayesian optimisation algorithm (a quick Google search will reveal them), The work  thus fail in thoroughness also. Third, adversarial perturbation are known to exist even around the image such that even a simple gradient descet optimisation starting from the target image would be able to provide perceptually small perturbation (it does not have to the smallest to be perceptually small). Thus accroding to me this paper is not good enough for acceptance.<BRK>Although the potential application of adversarial attack on deep learning model is interesting, the paper contribution and the novelty are limited giving the fact that there is another related paper published [1]. The authors in [1] consider using Bayesian optimization to make adversarial attack for model testing. In particular, they have considered the deep learning model. There is a big overlapping between the idea in [1] and the current paper. The paper presentation and writing is high quality although the paper is a bit over length.<BRK>The main contribution is to combine BO with dimension reduction, which leads to the effectiveness in generating black box adversarial examples in the regime of limited queries. However, I still have some concerns about this paper. Such a baseline is not clear in the paper, and the comparison with (Tu et al., 2019) is not provided in the paper. 4) Minor comment: In related work "Bayesian optimization has played a supporting role in several methods,including Tu et al.(2019), where ...." However,  Tu et al.(2019) does not seem using BO and ADMM. ############ Post feedback ##########Thanks for the clarification and the additional experiments.<BRK>They focus on the problem of black-box adversarial attacks, where the aim is to generate adversarial examples using information limited to loss function evaluations of input-output pairs. They use Bayesian optimization (BO) to specificallycater to scenarios involving low query budgets to develop query efficient adversarial attacks. Their proposed approach achieves performance comparable to the state of the art black-box adversarial attacks albeit with a much lower average query count.
Reject. rating score: 3. rating score: 3. rating score: 3. The essential idea is to enforce similarity between representations of multiple views obtained by subsetting channels from multiple co terminus sensor outputs. The paper does not discuss why this is a good task (is it challenging, is it representative of remote sensing applications, are these most frequent OSM classes). Indeed, existing work [1] (as cited in the paper) uses this hypothesis to create representations for satellite images. The paper also makes the claim that computing representation loss should be done with high level features rather than individual pixels. A more realistic or challenging setup would be required to evaluate the underlying ideas. Vol.33.2019.Update in response to rebuttal: The authors agreed to most of the points raised, but provided no clear suggestions in addressing them. I re emphasise that the empirical results on a single custom dataset based on OSM is limited. Further none of the reasonable baselines have been compared against, a point which the authors ignored in the rebuttal.<BRK>The method relies on the InfoNCE objective to contrast two different views of the data obtained by randomly cropping image patches, color jittering, channel dropping etc. In that regard I appreciate the direction explored by the paper. The authors evaluate on a single data set, that seems to not have been used previously. To make the evaluation more solid it would be good to compare on other data sets, for example on EuroSAT [1], and with other, possibly supervised classification methods, see, e.g., [1, 2]. Both forward and backward prediction losses are used, but it seems that the loss is symmetric. Deep learning classification of land cover and crop types using remote sensing data. Update after the rebuttal:Thanks to the authors for their detailed response. In particular, I do think evaluating the proposed method on prior benchmarks that were mostly used in the context of supervised methods (such as EuroSAT) is very important. Also, I still think comparison to from scratch training is crucial.<BRK>The basic idea is to generate, during training, two versions of the same patch by randomly dropping out channels, and to penalise the difference between the two corresponding embeddings, using the InfoNCE loss. Again, channels with really different wavelength would be more convincing here. I am not sure what suggests to the authors that there is much sensor invariance. If they are really very different (think RADAR images) this could in principle even hurt. Yes, they are known to work surprisingly well also for remote sensing data   but, unsurprisingly, not nearly as well as pre training on similar remote sensing images. A more natural baseline would be to pre train on the same type of imagery, with a proxy task for which it is easy to get labels automatically    actually the paper implicitly suggests that labels from OpenStreetMap might be a good candidate. Perhaps even more important is another natural baseline. Section 4.2 suggests the channels in Fig.4 were added in a fixed order   why?<BRK>In addition, most remote sensing applications currently use only a small subset of the multi-sensor, multi-channel information available, motivating the need for fused multi-sensor representations. They propose a new self-supervised training objective, Contrastive Sensor Fusion, which exploits coterminous data from multiple sources to learn useful representations of every possible combination of those sources. This method uses information common across multiple sensors and bands by training a single model to produce a representation that remains similar when any subset of its input channels is used. These representations outperform fully supervised ImageNet weights on a remote sensing classification task and improve as more sensors are fused.
Reject. rating score: 3. rating score: 3. rating score: 6. This paper propose a video instance embedding loss for jointly tackling the instance tracking and depth estimation from self supervised learning. Pros:1: I think the method is moving towards the right direction that 3d geometry and 2d instance representation should be considered jointly under the scenario of video learning. Cons: 1: I think the major argument I have is this method is lack of technical novelty, since it is straight forward to adopt the loss of  Brabandere et.al 2017 to video cases for including pixels in the same group under ground truth tracking, and the self supervised loss is exactly the same as previous methods.<BRK>SummaryThe paper presents a method to learn an embedding space for each pixel in a video that indicates the instance id of the objects. Strengths1) The proposed approach is simple and general and handles the problem of occluding objects in videos. 4) The paper is well written and well motivated. Weaknesses1) Comparisons to other video instance segmentation methods [1,2,3] are missing. This is a missed opportunity. Difference in depth might help in identifying instances.<BRK>This paper presents learning a spatio temporal embedding for video instance segmentation. With spatio temporal embedding loss, it is claimed to generate temporally consistent video instance segmentation. The authors show that the proposed method performs nicely on tracking and segmentation task, even when there are occlusions. Overall, this paper is well written. The authors mention that scenes are assumed to be mostly rigid, and appearance change is mostly due to the camera motion. The authors are encouraged to discuss more about the metrics and experimental results with the other metrics as well. Other than these, the experiment was well designed and conducted.<BRK>Understanding object motion is one of the core problems in computer vision. Significant progress has been made in instance segmentation, but such models cannot track objects, and more crucially, they are unable to reason in both 3D space and time. They propose a new spatio-temporal embedding loss on videos that generates temporally consistent video instance segmentation. Their model includes a temporal network that learns to model temporal context and motion, which is essential to produce smooth embeddings over time. Finally, they show that their model can accurately track and segment instances, even with occlusions and missed detections, advancing the state-of-the-art on the KITTI Multi-Object and Tracking Dataset.
Reject. rating score: 1. rating score: 1. rating score: 6. rating score: 8. This paper studies the problem of learning disentangled representation in a hierarchical manner. Specifically, common representations are captured at root level and unique representations are learned at lower hierarchical level. The HDN is trained in a generative adversarial network (GAN) manner, with additional hierarchical classification loss enforcing the disentanglement. Without such comparisons, it is unclear what is the value of hierarchical representation proposed here. What’s the interpolation parameter for each of the column? One strong baseline is to use the latent representation of a pre trained GAN model as comparison.<BRK>Experiments are conducted on four datasets to validate the method. No results on commonly used disentanglement metrics (e.g.see [1])2. No comparison with existing supervised/unsupervised methods on disentangled representations (e.g.[2][3])3.The needs for full supervision on each level and manually designed fixed hierarchy require labels for the full hierarchy and make it not applicable to many existing data. If not, is it a limitation of this method? For example, the authors mentioned that the proposed approach did not work well for ImageNet.<BRK>The claimed benefits are the improved generalizability and interpretability. Generalization to unseen categories tends to be a good proxy for real world performance and directly learning the high level categories is a useful idea for doing so. Although I am leaning towards weak accept, I think this paper is close to borderline because the findings do not seem experimentally well validated.<BRK>Therefore, this work appears novel and interesting to me. I will leave the assessment of the degree of novelty to other reviewers/AC who may be more familiar with the literature. What, according to the authors, is the difference between having a hierarchical structure and multi granularity? This would imply that the R s from different levels are randomly combined, and the number of representations combined is always L. Is this correct? Minor suggestions:  Please use parenthetical citations throughout the paper where appropriate (use \citep{}) to avoid breaking the flow of reading.<BRK>Disentangling such underlying primitives is the long objective of representation learning. Though they may obtain the primitives to constitute objects as the categories in that granularity, their results are obviously not efficient and complete. To simultaneously ensure the disentanglement and interpretability of the encoded representations, a novel hierarchical generative adversarial network (GAN) is elaborately designed.
Reject. rating score: 3. rating score: 3. rating score: 3. This is the first time I have considered this problem, but not sure whether any prior work exists in the specific subfield. The paper isolates one lifelong learning approach (A GEM) which is characteristic of one (of many) different approaches to lifelong learning, and investigates its robustness to standard adversarial attacks and a novel attack developed within this paper, which is stronger, but specific to episodic memory approaches. I cannot recommend acceptance at this point for the following reasons:1) I am not sure what I can generalize away from this paper to the immediate subfield and beyond. The paper claims that the investigated method is SOTA, but it s not clear this is the case, even in restricted class of similar episodic memory based models, see [1] for an independent evaluation of many such approaches. Are other methods in this class more susceptible to these attacks and can the proposed attack be applied to the whole class, or even other types of approaches?<BRK>The paper proposed a novel approach for robust continual learning model from the adversarial attack. But, I feel that some of the analysis are obvious which are not much meaningful to analyze the model,  and the overall contributions are suggested under A GEM model, while not to cover generic other episodic based continual learning. So, I hesitate to give the high score even the approach is interesting. Additional one question. What’s the reason that A GEM is robust for them?<BRK>General:The paper first proposes an adversarial attack on the exemplar based continual learning algorithm, A GEM. The attacker assumes to have access not only to the model but also to the episodic memory. I think that is quite a powerful assumption for the attacker and it is not very surprising that the attack would work. It is not clear whether the proposed method will also work well for other exemplar based methods like iCaRL or GEM (the simpler version than A GEM), etc. 2.What exactly is the practical scenario of this method?<BRK>Lifelong learning aims at avoiding the catastrophic forgetting problem of traditional supervised learning models. Episodic memory based lifelong learning methods such as A-GEM (Chaudhry et al., 2018b) are shown to achieve the state-of-the-art results across the benchmarks. While A-GEM has strong continual learning ability, it is not clear that if it can retain the performance in the presence of adversarial attacks. In this paper, they examine the robustness ofA-GEM against adversarial attacks to the examples in the episodic memory. they therefore propose a principled way for attacking A-GEM called gradient reversion (GREV) which is shown to be more effective.
Accept (Talk). rating score: 8. rating score: 8. rating score: 6. This is proved in two steps:  1. Assuming that gradient descent manages to find a set of network parameters that separate the data, thereafter gradient flow/descent monotonically increases the normalized margin (rather an approximation of it). While the main body of the paper presents a restricted set of results, the appendix generalizes this much further applying it to various kinds of loss functions (logistic/cross entropy, exponential), to multi class classification and to multi homogeneous models. The paper takes a significant step by unifying existing results on margin maximization and going beyond them. But a priori, $\|\theta\|_2^L$ is not the *only* choice; $\|\theta\|^L$ could also work for any norm $\|\cdot\|$.<BRK>This paper studies the trajectory induced by applying gradient descent/gradient flow for optimizing a homogeneous model with exponential tail loss functions, including logistic and cross entropy loss in particular. This is an important direction in recent theoretical studies on deep learning as we need to understand which global minimizer the training algorithm picks to analyze the generalization behavior. This paper rigorously proves gradient descent / gradient flow can maximize the L2 margin of homogeneous models. Existing works mostly focus on linear models or deep linear networks, and comparing with Nascon et al., 2019a, the assumptions in this paper are significantly weaker. It naturally decomposes the dynamics of the smoothed version into a radial component and a tangential velocity component.<BRK>This paper studies the implicit regularization phenomenon. More precisely, given separable data the authors ask whether homogenous functions (including neural networks) trained by gradient flow/descent converge to the max margin solution. The authors show that the limit points of gradient descent are KKT points of a constrained optimization problem. In this sense, the paper only studies the latter part of the training process. However, I will stand by me evaluation and will not change it. I agree though that assumption (A4) is indeed reasonable, although of course very strong.<BRK>In this paper, they study the implicit regularization of the gradient descent algorithm in homogeneous neural networks, including fully-connected and convolutional neural networks with ReLU or LeakyReLU activations. In particular, they study the gradient descent or gradient flow (i.e., gradient descent with infinitesimal step size) optimizing the logistic loss or cross-entropy loss of any homogeneous model (possibly non-smooth), and show that if the training loss decreases below a certain threshold, then they can define a smoothed version of the normalized margin which increases over time. They conduct several experiments to justify their theoretical finding on MNIST and CIFAR-10 datasets.
Reject. rating score: 3. rating score: 3. rating score: 6. This paper proposes to leverage the depth information for relation prediction, arguing that the depth information benefit the prediction of some predicates. To solve the lack of 3D data, an RGB to Depth model is trained on external available dataset and then applied to images from visual relation dataset. In the experiments, they investigate different strategies to extract features from depth maps and the explore effectiveness of depth information by comparing the model that only used depth map as input with those which use RGB information. +Strength:(1) The motivation is reasonable and what the authors make an attempt to explore is very meaningful. Visual relation especially the spatial relation is not likely to be predicted accurately without 3D information. Thus what the authors do is a good exploration for further extensions. (2) Comparisons with previous methods and the results show that the depth information is useful to some extent, but not so obvious. Is there any gap when it is used for VG or VRD dataset? (2) Although the depth map feature extraction seems to work well, it seems to be a little trivial. And why the AlexNet trained from scratch performs better than AlexNet pretrained on RGB images for object detection task and VGG net? If the author can give more explanations, this part will be more insightful.<BRK>OVERVIEW:The authors propose to use depth information to better predict the visual relation between objects in an image. They do this by incorporating a pre trained RGB to Depth model within existing frameworks. They claim the following contributions:1. 2.Discuss and empirically investigate different strategies to extract features from depth maps for relation detection. I liked the idea of using depth information to inform visual relationships but I am not sure if the proposed approach is the way to go. Direct reasoning in 3D should now be possible instead of going via deep networks as proposed in the paper. 2.The authors use a pre trained RGB to Depth network trained on NYU v2 to predict depth for the images of VRD and VG. There is very little discussion about the quality of predicted depth maps. Ideally, this needs to be quantified to convince the reader that the generated depth maps are "good" but at the very least the authors need to show qualitative examples (both good, typical and bad) to prove that the pre trained network generates meaningful depth maps. The AlexNet BN depth model is trained for relation detection using only depth. But it is not clear if it is using proposals/boxes generated by RGB detection model or using ground truth boxes. NOTE:I would like to mention that I have published in monocular object pose estimation and work in the object recognition. I am not as familiar with the visual relation detection field but I understand all the components proposed by the authors in this work.<BRK>3) the improvements due to the additional depth network are not significant or conclusive. This is especially important since the paper is outperforming prior works which could be a contribution if reproducible. The authors also argue that the related [Yang et al.2018] paper (point 4) should be considered a concurrent submission since the authors original submission was to AAAI18. The rebuttal also addresses other clarity or experimental issues which improves the quality of the revised work. All in all, *assuming that [Yang et al.2018] is considered a concurrent work* according to ICLR, I think the revised paper becomes slightly above borderline and thus I change my rating to "weak accept". ********* Summary ********* The paper poses the question of whether depth information is informative for visual relationship prediction using still images. As such it is important to see whether and to what extent depth information complements RGB information for visual relation detection. The paper proposes to use an off the shelf monocular depth estimation networks to augment the available RGB information towards better visual relation detection. It’s good to name the method in table 2 in the same fashion as table 1. The proposed model demonstrates improved results upon state of the art for visual relation prediction. It will read better if they are organized into subsections. However, it seems that the improvement is mainly coming from the new architecture as opposed to the inclusion of the depth information. For instance, it is good to discuss what is the advantage of the proposed (computationally more expensive) method over the following two simpler baselines:  Faster RCNN is used on RGBD input to produce a single feature vector  above case with RGB input but have the Faster RCNN predict the depth map as an auxiliary loss. It is not clearly motivated why one should use two separate networks for depth and RGB inputs in light of the additional complexity.<BRK>State of the art visual relation detection methods mostly rely on object information extracted from RGB images such as predicted class probabilities, 2D bounding boxes and feature maps. In this paper, they argue that the 3D positions of objects in space can provide additional valuable information about object relations. Since 3D information of a scene is not easily accessible, they propose incorporating a pre-trained RGB-to-Depth model within visual relation detection frameworks. They discuss different feature extraction strategies from depth maps and show their critical role in relation detection. Their experiments confirm that the performance of state-of-the-art visual relation detection approaches can significantly be improved by utilizing depth map information.
Reject. rating score: 3. rating score: 3. rating score: 6. This paper empirically examines an interesting relationship between mode connectivity and matching sparse subnetworks (lottery ticket hypothesis). The paper also focused on cases where matching subnetworks were found by IMP, but matching subnetworks can also be found by other pruning methods. This allowed the authors to find matching subnetworks for deeper networks and in cases where it could not be done without some intervention in learning schedule.<BRK>This paper works on empirically demonstrating the connection between model connectivity and the lottery ticket hypothesis, which are individually explored in the literature. These are not clear to me.<BRK>This paper empirically presents a very interesting connection between two also very interesting phenomena (mode connectivity and lottery ticket hypothesis), while removing a previous limitation of the lottery ticket hypothesis on larger networks. Though I must say the existing amount of experiments sufficiently validates the existence of the connection authors put forth and hence not required. Though it is unclear from the paper what are the immediate / straightforward applications, the findings do present interesting contributions.<BRK>They uncover a connection between two seemingly unrelated empirical phenomena: mode connectivity and sparsity. On the other hand, there is the lottery ticket hypothesis of Frankle & Carbin (2019), where dense, randomly initialized networks have sparse subnetworks capable of training in isolation to full accuracy.
Reject. rating score: 3. rating score: 3. rating score: 6. The paper is rather interesting and is able to solve some difficult tasks. A combination of different learning techniques for acquiring structure and learning with asymmetric data are used. This assumption seems rather strong. There is little sub task transfer in this task. In section 4.1 it is noted that the version that initialized with different policy means works best. How are these means initialized? There is no algorithm in the main paper which makes it a little difficult to understand the operation of the learning method.<BRK>While this paper has some interesting experiments. For Option Critic models the authors claim that "Rather than the additional inductive bias of temporal abstraction, we focus on the investigation of composition as type of hierarchy in the context of single and multitask learning while demonstratingthe strength of hierarchical composition to lie in domains with strong variation in the objectives such as in multitask domains." Additionally, it is well known that Option Critic approaches (when unregularized) tend to learn options that terminate every step [2]. The authors mention that Feudal approaches "employ different rewards for different levels of the hierarchy rather than optimizing a single objective for the entire model as we do."<BRK>This paper introduces a hierarchical policy structure for use in both single task and multitask reinforcement learning. The authors then assess the usefulness of such a structure in both settings on complex robotic tasks. Firstly, they assess the benefits of the hierarchical structure for single task settings in a simulated environment. While the experimental results are shown to support this, further discussion of why this is the case would have been welcome. I think that the paper was very well written. As such I recommend this paper to be weak accepted.<BRK>To this end, they investigate compositional inductive biases in the form of hierarchical policies as a mechanism for knowledge transfer across tasks in reinforcement learning (RL). They demonstrate that this type of hierarchy enables positive transfer while mitigating negative interference. Furthermore, they demonstrate the benefits of additional incentives to efficiently decompose task solutions. Their experiments show that these incentives are naturally given in multitask learning and can be easily introduced for single objectives. They design an RL algorithm that enables stable and fast learning of structured policies and the effective reuse of both behavior components and transition data across tasks in an off-policy setting.
Reject. rating score: 3. rating score: 6. rating score: 6. This paper studies the problem of differentially private optimization in the (strongly) convex setting. The authors focus on the gradient perturbation methods, i.e., DP GD and DP SGD, and provide the utility guarantees of DP GD and DP SGD under the so called expected curvature assumption. I summarize my main concerns as follows:1. All the theoretical results provided in this paper are based on the expected curvature assumption (Definition 3). 6.The contribution of the current paper is very incremental. All the proofs are just replacing the strongly convex condition with the expected curvature condition.<BRK>In this paper, the authors consider gradient perturbation and claim that it is more advantageous than other methods. Since other perturbation methods does not add noise at intermediate steps, expected curvature is the same with \mu and the utility advantage is not valid. They claim that they are the first study showing the advantage of gradient perturbation theoretically (I haven’t seen such a study either). Since they remove the dependency to minimum curvature \mu, they present utility order for both convex and strongly convex objective for DP GD and DP SGD. The learning rate of DP SGD is divided by 2 at the middle of training.<BRK>Comparing to minimum curvature, which was used in previous convergence analyses, expected curvature better captures the properties of the optimization problem, and thus offers an explanation for the advantage of gradient perturbation based methods over objective perturbation and output perturbation. I have the following questions. Is it similar to just replacing any \mu by \nu in the previous analysis? Yet I don’t see clearly how this can be used to show that they have more advantages than objective perturbation. You mentioned that “That is because DP makes the worst case assumption on query function and output/objective perturbation treat the whole learning algorithm as a single query to private dataset.<BRK>Previous work first determines the noise level that can satisfy the privacy requirement and then analyzes the utility of noisy gradient updates as in non-private case. In this paper, they explore how the privacy noise affects the optimization property. They show that for differentially private convex optimization, the utility guarantee of both DP-GD and DP-SGD is determined by an \emph {expected curvature} rather than the minimum curvature. By using the \emph {expected curvature}, their theory justifies the advantage of gradient perturbation over other perturbation methods and closes the gap between theory and practice.
Reject. rating score: 3. rating score: 3. rating score: 6. How the new datasets are collected, labeled and mixed are not clear. Based on the results, they made a few claims that challenges some of the previous believes in this field. How the “negative” is defined, since you got more than 1 nanotations for each image. “ Following the construction of synthetic datasets, we replace the training images in the original dataset”, refers to which original dataset?<BRK>The authors attempted to make the dataset represent "real world noise." The hamartia also makes their experimental findings and takeaways about "real noise" questionable, as it is not clear they are testing real noise but consequences of properties of convnet embeddings. However this paper still contributes a dataset, hence this paper still is some sort of contribution. However, with this flaw, it is not clear that it is enough for ICLR.<BRK>In this paper, the authorsestablish a large benchmark of controlled real world noise, which is one contribution of it. for example, DNNS generalize much better on real world noise, and DNNs may not learn paterns first on real world noisy data, and so on.<BRK>As real-world noise possesses unique properties, to understand the difference, they conduct a large-scale study across a variety of noise levels and types, architectures, methods, and training settings. Their study shows that: (1) Deep Neural Networks (DNNs) generalize much better on real-world noise. (4) Real-world noise appears to be less harmful, yet it is more difficult for robust DNN methods to improve.
Accept (Spotlight). rating score: 8. rating score: 6. This paper presents a theoretical study of ridge regression, focusing on the practical problems of correcting for the bias of the cross validation based estimate of the optimal regularisation parameter, and quantification of the asymptotic risk of sketching algorithms for ridge regression, both in the p / n  > gamma in (0, 1) regime (n   # data points, p   # dimensions). The whole study is complemented by a series of numerical experiments. I am recommending this paper to be accepted for publication at ICLR. Comments:  Most of the examples in the paper focus on the regime gamma < 1.<BRK>This paper deals with 3 theoretical properties of ridge regression. First, it proves that the ridge regression estimator is equivalent to a specific representation which is useful as for instance it can be used to derive the training error of the ridge estimator. The paper addresses an important problem and puts itself nicely in context of previous work.<BRK>They study the following three fundamental problems about ridge regression: (1) what is the structure of the estimator? (2) how to correctly use cross-validation to choose the regularization parameter? They study the bias of $K $-fold cross-validation for choosing the regularization parameter, and propose a simple bias-correction.
Reject. rating score: 1. rating score: 1. rating score: 6. This paper targets at a deep learning theory contribution based on information geometry. This contribution is tightly based on Zhang et al.(2018) and explains the generalization of deep learning from a Bayesian perspective. First of all, the writing (including language etc) is of poor quality, to the extent that the submission is very difficult to read and can be rejected merely based on this, with unusual expressions, missing  punctuations, super long sentenses, and wongly used words. As their results are largely based on the correctness of eq.(2), let s examine the derivations in appendix A.1. It is hard to observe anything new, given the poor writing and organization. The authors used information geometry and minimum description length to explain the generalization of deep learning. This is a small area. On the other hand, as the authors used the spectrum properties of the Fisher information matrix, there are some recent works by Amari which can be cited.<BRK>The authors test this conjecture on an artificial task using a small neural network, and investigate the sensitivity of the results to noise. I like the general idea of the paper and appreciate the very detailed exposition placing it in the context of other works. In particular, I enjoyed the summary showing the sometimes conflicting evidence for better generalization in either broader or sharper minima, and how it relates to the Jeffreys prior. In general I would like to see experiments on datasets and with architectures that are at least somewhat close to what people use in practice (at least in terms of the size of the task and the capacity of the net). While I appreciate your detailed theoretical exposition, I think the amount of empirical evidence you provide is insufficient to back the claims. Considering the explicit instruction to judge papers exceeding 8 pages with a higher standard, I believe that the lack of a greater amount of empirical evidence is a significant deficiency of your otherwise very interesting work.<BRK>The paper argues that the widest minimum in the loss landscape is not the best in terms of generalization. Synthetic simulations are presented to support these claims. The authors employ Fisher Information to characterize the optimal width or the curvature around the minimum. The fact that the determinant of the Fisher Information Matrix is invariant to parametrization, under certain conditions, serves as the motivation to design an objective Bayesian prior called Jeffrey s prior. The motivation and the theoretical arguments are interesting, but the paper lacks in presentation and sufficient empirical evidence is also lacking to get fully convinced by the claims. Fig 1 is not clear.<BRK>The efficacy of the width of the basin of attraction surrounding a minimum in parameter space as an indicator for the generalizability of a model parametrization is a point of contention surrounding the training of artificial neural networks, with the dominant view being that wider areas in the landscape reflect better generalizability by the trained model. In this work, however, they aim to show that this is only true for a noiseless system and in general the trend of the model towards wide areas in the landscape reflect the propensity of the model to overfit the training data. Utilizing the objective Bayesian (Jeffreys) prior they instead propose a different determinant of the optimal width within the parameter landscape determined solely by the curvature of the landscape.
Reject. rating score: 6. rating score: 6. rating score: 6. Im summary, I think this is an interesting experiment and it s nice to see that the MLP isn t doing a lot of heavy lifting (which also might be slightly counter to their hypothesis about the MLP containing a lot of entailment information). However, I find the transfer experiments unconvincing and the paper is short on analysis about when their model does better on transfer and when having an MLP helps, or how the learned sentence embeddings of the two models differ. This paper proposes a new way to train sentence embedding models using NLI data such that very few parameters are used for classification. This is in contrast to prior work where an MLP is used.<BRK>The paper proposes a few heuristic scorers to model entailment and contradiction, based on encoded sentence embeddings. It would be interesting to see if the well designed heuristic matching scores could ease the underlying model, so that it learns more generic sentence embeddings in general. The two similarity scores are not too novel, for example, sim_diff is the L1 distance between two vectors. Entailment, contradiction, and neutral scores are interesting, but hardly generalize to other sentence matching tasks (e.g., various IR applications).<BRK>This paper proposes an interesting approach towards learning NLI via parameter free operations over pairs of sentence embeddings. The authors propose entailment and contradiction operators that learn entailment and contradiction scores while training the parameters of the sentence encoders. The authors should have tried to an ablation type of approach in equation 1, to check if concatenation alone, element wise dot product alone or absolute difference alone or a combination of any two would work better with the scoring function. That being said, the reported results my be of some value after all. It is hard to narrow down on the exact contributions of this paper.<BRK>But in many such models the embeddings computed by the sentence encoder goes through an MLP-based interaction layer before predicting its label, and thus some of the information about textual entailment is encoded in the interpretation of sentence embeddings given by this parameterised MLP. In this work they propose a simple interaction layer based on predefined entailment and contradiction scores applied directly to the sentence embeddings. This parameter-free interaction model achieves results on natural language inference competitive with MLP-based models, demonstrating that the trained sentence embeddings directly represent the information needed for textual entailment, and the inductive bias of this model leads to better generalisation to other related datasets.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. Summary and Decision This paper studied an actor critic learning algorithm for solving a mean field game. linear quadratic setting, which is very unsatisfying. We call this class of games linear quadratic mean field games (LQ MFG). Therefore a theoretical convergence result in this setting is highly desired. This likely motivated the authors to make more simplifying LQ type assumptions to recover stronger results.<BRK>The authors begin by establishing the existence and uniqueness of the Nash equilibrium in such a setting, and then proposes an mean field actor critic algorithm with linear function approximation. Some minor suggestions. However, I have the following concerns and suggestions for this paper. 3) In (2.1), there should also be \sigma\in \mathbb{R}. Instead, the authors should add a problem called LQ SMFG (linear quadratic stationary MFG), which is basically problem 2.2 but the goal is to simultaneously find \mu^\star and \pi_{\pu}^\star.<BRK>SummaryThe present work is concerned with providing a provably convergent algorithm for linear quadratic mean field games. First, the authors show that under "standard" assumptions the map μ ↦ μ(π(μ)), μ) is contractive and hence, by the Banach fixed point theorem, has a unique solution, resulting in a unique Nash equilibrium of the mean field game. Second, they show that by using an actor critic method to approximate π(μ) for a given μ, this argument can be turned into an algorithm with provably linear convergence rate.<BRK>They consider the setting where the agents have identical linear state transitions and quadratic cost func- tions, while the aggregated effect of the agents is captured by the population mean of their states, namely, the mean-field state. Moreover, to find the Nash equilibrium, they propose a mean-field actor-critic algorithm with linear function approxima- tion, which does not require knowing the model of dynamics. In particular, they prove that their algorithm converges to the Nash equilibrium at a linear rate.
Reject. rating score: 3. rating score: 3. rating score: 6. Several heuristic methods are proposed for this search, and results are demonstrated in Atari domains. I am, however, concerned with the evaluation of the method and its practicality, as reflected by the following issues: 1. The method has many hyper parameters. 4.For the full ConQUR, there are many more hyper parameters, which I did not understand the intuition how to choose. How can I understand from this that ConQUR is really better?<BRK>Maybe, the paper benefits with some evidence answering this question. To summarize, I am mainly concerned about the marginal benefit at the cost of added complexity and computation for this paper. Another comment about experiments is that the paper uses pre trained DQN for the ConQur results, where only the last linear layer of the Q network is trained with ConQur.<BRK>Cons:The major experimental results of the paper are in Table 4 of Appendix D. This is not a good effort to save space by moving the most important results into appendix.<BRK>Delusional bias is a fundamental source of error in approximate Q-learning. They also propose a search framework that allows multiple Q-approximators to be generated and tracked, thus mitigating the effect of premature (implicit) policy commitments.
Reject. rating score: 6. rating score: 6. rating score: 6. The paper presents a new GLUE like dataset collection called DiscEval, which focuses on discourse and pragmatics. Like GLUE and SuperGLUE, datasets from existing works are collated and formated. Like other NLP benchmarks, the DiscEval benchmark would be a good resource for other researchers to hill climb their systems on, provided that the data format is standardized and the submission system is easy to use like GLUE. That said, the paper has some rooms for improvement:  With the information in Table 2, it is hard to judge the difficulty and headroom for each task. Contrast this to the GLUE and SuperGLUE papers which provide human baselines from actual humans. Providing the best single task result from previous work would also help give a more complete picture. With the result of fine tuned BERT almost matching the human performance in several tasks, the argument that BERT is not a universal representation (abstract + introduction) is weakened somewhat. As a valuable resource for other researchers, I am still leaning toward acceptance despite the issues above. It would be nice to be a bit more upfront about it.<BRK>  Overview: This work presents a benchmark for language understanding centered on understanding pragmatics and discourse, in contrast to existing NLP benchmarks which focus on mostly semantic understanding. I recommend (weak) accept. The paper would benefit a lot from having more in depth explanation of the tasks, what the classes are, and what the classes mean. Additionally, it d be nice to get the source of the data for each task. I understand that estimates of human performance, especially for tasks with a high number of classes, can be tricky to obtain. I do feel that they are especially important to have for this dataset. I think it s quite interesting that fine tuning on MNLI doesn t lead to good performance on DiscEval, as MNLI, as the authors point out, is commonly taken to be a useful pretraining task. It also doesn t look like BERT+Discovery is the best in any column here.<BRK>This paper presents a new benchmark for natural language understanding called DiscEval. The benchmark focuses on datasets that more directly measure a model s understanding of the discourse structure and relations in the text. Some comments and recommendations,  You say that for PDTB you "select the level 2 relations as categories," I believe these are the class level relations. Particularly given the data filtering and restructuring of some of the tasks, getting a rough estimate of human performance for all the datasets would be quite valuable  I think saying the MNLI does not help model performance on DiscEval is completely valid but claiming it hurts performance could be a stretch given the margins of error. I know tasks like PDTB and GUM have quite a few classes. Please include the number of classes for each of the datasets in the benchmark. I think this will be a useful resource to the research communityMinor things,  Page 5, last line the CoLA citation is wrong, should be Warstadt et al.(2019)<BRK>New models for natural language understanding have made unusual progress recently, leading to claims of universal text representations. However, current benchmarks are predominantly targeting semantic phenomena; they make the case that discourse and pragmatics need to take center stage in the evaluation of natural language understanding. They introduce DiscEval, a new benchmark for the evaluation of natural language understanding, that unites 11 discourse-focused evaluation datasets. DiscEval can be used as supplementary training data in a multi-task learning setup, and is publicly available, alongside the code for gathering and preprocessing the datasets.
Reject. rating score: 3. rating score: 6. rating score: 6. [Major Comments]My primary concern about this work is the scope of its applicability. I feel this paper makes strong assumptions on both the induction and inference stages, as well as the structure of the causal graph, which greatly limits the applicability of the approach. I feel this is an important detail to include. In Section 3.1, the authors said that "N is the number of actions in the environments," which is a bit confusing.<BRK>This is clearly a timely topic and I loved the motivations of the paper. Since this is supervised learning of the causal graph, I imagine that the semantics of the node is predetermined, which is a bit disappointing (but doing otherwise would be understandably much more challenging). What are the output nodes? Then in sec 3.2 the authors talk about a weighted sumn involving selected edges. post rebuttal  addition  The authors have satisfied most of my concerns and I have upgraded my rating to weak accept.<BRK>The experimental setting is an agent controlling 5 or 7 switches in a simulated environment and observing 32x32x3 images of the environment. * There seem to be strong assumptions on the structure of C that are only stated late in the paper.<BRK>In this work, they propose to endow an artificial agent with the capability of causal reasoning for completing goal-directed tasks. They leverage attention mechanisms in their causal induction model and goal-conditional policy, enabling us to incrementally generate the causal graph from the agent's visual observations and to selectively use the induced graph for determining actions.
Reject. rating score: 3. rating score: 6. rating score: 6. The authors propose R2D2 layers, which are trained to reduce and re use existing parameters of a neural network layer, and apply this to Transformer and LSTM architectures. The authors conduct experiments on various NLP tasks, including NLI and NMT. The main benefit of the proposed R2D2 layer is that the number of parameters can be reduced, as the existing parameters can be reused. To tie my points #1 and #2 together, I feel the authors did experiments on a variety of different tasks, but these style transfer and subject verb agreement tasks are not particularly interesting or realistic   instead this space should be devoted to discussions of the advantages of their method and analysis on its performance, which is quite lightly covered. This appears true if up sampling with a factor of 2 is used to make the models larger again. The authors should compare to factorized/quaternion baselines which have a larger quantity of parameters as well. 4.Table 3, where results are reported on the competitive WMT en de benchmark, lacks comparison for number of parameters and decoding speed. (As an aside, the technique should be applicable to the DynamicConv model, which is a Transformer variant?) 5.The related work section is quite light on other approaches to reducing model size, such as knowledge distillation or quantization? While the approach taken in this paper leverages parameter sharing, the motivation is similar and I feel acknowledging this entire area of work would be relevant. 6.I m not clear on why we see inference time decoding speed improvements based on the description of the method.<BRK>This paper proposes a new Reuse and Reduce with Dynamic weight Diffusion (R2D2) layer as an alternative to feed forward layers in neural networks. In extensive experiments on NLI, NMT, text style transfer and subject verb agreement, feed forward layers in LSTMs and Transformers are replaced with R2D2 layers. The modified models achieve similar performance to the originals, while being more than 50% smaller. Overall, the proposed method is presented clearly and the experiments are comprehensive and convincing. The proposed method is well explained. This is in contrast to some of the previous methods based on hypercomplex operations, which often seem harder to grasp and visualize. The main thing that I m missing is some analysis of the dynamics of the model, what it is learning (in comparison to using FC layers) or why a smaller number of parameters is still competitive with the standard FC layers. Overall, as the method seems straightforward enough to implement and achieves promising results, it has the potential to have some practical impact.<BRK>The experiments shows that models with less parameters yield comparable performance with their larger counterparts. The paper is however not clear on the ultimate objective of the method (speed/accuracy/generalisation?) I feel it would cleared if somewhere in the paper there was an equation with the element wise correspondence, i.e.H_{?,?}\sum_k A_i,k S_k,jIn that section, you should introduce that n is a hyperparameter before using it as well. The experimental section lack a validation/ablation study to help the reader understand the interplay between the number of blocks and the number of latent dimensions. Did you have to change other regularization parameters like dropout. To me the main weakness of the paper lies in the lack of comparison with alternatives. Replacing fully connected layers with alternative has a rich literature that the authors ignore. https://arxiv.org/abs/1812.08301 (squantizer) https://arxiv.org/abs/1802.08435 (block sparsity)...(iii) distillation of large models into smaller models. https://arxiv.org/abs/1503.02531 https://arxiv.org/abs/1702.01802(iv) it might not be necessary to compare, but at least mentioning approaches which predict weights from a meta network would be good.<BRK>They propose R2D2 layers, a new neural block for training efficient NLP models. Their method is inspired by recent Quaternion methods which share parameters via the Hamilton product. They conduct extensive experiments in the NLP domain, showing that R2D2 (i) enables a parameter savings of up to 2 times to 16 times with minimal degradation of performance and (ii) outperforms other parameter savings alternative such as low-rank factorization and Quaternion methods.
Reject. rating score: 3. rating score: 6. rating score: 6. The paper considers imitation learning from a set of demonstrations with diverse qualities. The paper addresses and interesting an important problem. However, although the experiments demonstrate good performance on a set of tasks they fail to provide convincing evidence about the generality of the approach. In particular, the model for generating diverse quality demonstrations is tightly coupled to the optimal policy through the assumed demonstrations. Some more comments:* I am missing some experimental details. Clearly, this could not be implemented be practice but could be facilitated in combination with an expert which can identify a good policy.<BRK>This paper proposes an imitation learning algorithm for the setting where the demonstration data consists of trajectories from sources of varying expertise. The authors proceed by defining a parameterized model of the (demonstration) trajectory distribution (Equation 2), which uses the MaxEnt RL model for the optimal policy, and a distribution (p_w) to model the level of expertise. Imitation learning is then reduced to maximum likelihood training under the provided demonstrations. I would like the authors to comment on the following:1. I understand the motivation in Section 3.4 that IS should help to improve the convergence rate, but for benchmarks like HalfCheetah, Walker, the performance seems to have saturated to a significantly lower value.<BRK>I like this paper: it tackles an interesting and relevant problem (imitation learning when demonstrations come from people with different levels of expertise), takes the natural approach of attempting to infer which expert produced which demonstration, and shows results compared against a large number of baselines. However, the experiments are set up to match VILD’s model, and it is not as clear what would happen in a more realistic setting where there will be misspecification. The authors do consider one type of misspecification: when instead of Gaussian noise, the true actions are generated with TSD noise. This gives me more hope that VILD will work in more realistic settings. While I would particularly appreciate experiments with real human data, in the absence of that I would like to see an experiment with misspecification of the number of demonstrators.<BRK>The goal of imitation learning (IL) is to learn a good policy from high-quality demonstrations. However, the quality of demonstrations in reality can be diverse, since it is easier and cheaper to collect demonstrations from a mix of experts and amateurs. Their work enables scalable and data-efficient IL under more realistic settings than before.
Reject. rating score: 3. rating score: 6. rating score: 6. The proposed method of mixture of experts variational autoencodersis valuable and insightful. On the other hand the work could be improved and clarified at some points:  in the abstract it is claimed that the method works for high dimensional data.However, it should be better explained why this is the case. a main shortcoming is that there is no discussion or experimental comparison with methods like spectral clustering and kernel spectral clustering. Given that the paper and the proposed method relates to similarity based representations it would be important to know how it compares to such methods.<BRK>The authors present an extension of variational autoencoders (VAEs), where Gaussian distribution of the latent variable is replaced by a mixture of Gaussians. The approach can be used for clustering and generation. The paper is well written and easy to read and understand. Specialized related work is discussed. However, the technical novelty together with the fine empirical evaluation are just good enough for ICLR, in my opinion.<BRK>Summary:The paper proposes to expand the VAE architecture with amixture of experts latent representation, with amixture component specific decoder that can specialize in a specificcluster. Overall, I recommend a weak accept. The method seems reasonable, andthe paper is well written, but the results are only marginally betterthan other methods, and there are several weaknesses with the proposedarchitecture and experimental setup. Please make an effort to move key results back in to  the main body of the paper.<BRK>MoE-Sim-VAE exhibits superior clustering performance on all these tasks in comparison to the baselines and they show that the MoE architecture in the decoder reduces the computational cost of sampling specific data modes with high fidelity. MoE-Sim-VAE is based on a Variational Autoencoder (VAE), where the decoder consists of a Mixture-of-Experts (MoE) architecture. This specific architecture allows for various modes of the data to be automatically learned by means of the experts.
Reject. rating score: 1. rating score: 1. rating score: 3. The main contribution of this paper are:1. 3.They run a set of experiments on the above settings. Two of the experiments are on simple synthetic settings, the third approximates a real world setting, although the approximation is quite rough and they don t convincingly argue for it being realistic, neither do they give convincing motivation of their objective which uses g +  standard deviation. It would also be worthwhile comparing to random search. * A better justification of the objective used in the experiments, using g +  the standard deviation appears fairly arbitrary, and there is no strong enough reason to believe this is a good approximation of what the cost is in the case of real world problems.<BRK>The modifications of EI to handle robustness and antifragility to aleatoric noise/uncertainty are simple and straightforward, one of which is similar to the augmented EI of [32]. Unfortunately, experimental results in such applications were not available in this paper to "close the loop" in supporting the motivation of this work, begging the question whether their proposed BO algorithm indeed works for these key applications. Fig.6a: Shouldn t the vertical axis be labeled as ... A sensitivity analysis would be useful here. The authors say that "A note on the form of this variance estimator is give in Appendix B." Can the authors give a detailed discussion why is the expression of f(x)   g(x) + s(x) in equation 9 the right one to be minimized in practice (e.g., in the context of materials and drug discovery)?<BRK>I would not count this as much in terms of novelty. The related background in Gaussian process is standard and could be omitted. The experimental section is weak. The experiments are demonstrated using low dimensional functions (1 2 dim). The reviewer suspects that the high/low performance is due to the high level of noise (?) The paper focuses on demonstrating the heteroscedasticity in the surrogate model using [1]. The reviewer is wondering what is the performance for other heteroscedastic GP approaches, such as [2,3,4] ?<BRK>Bayesian Optimisation is an important decision-making tool for high-stakes applications in drug discovery and materials design. In this paper, they propose a heteroscedastic Bayesian Optimisation scheme which both represents and optimises aleatoric noise in the suggestions. They consider cases such as drug discovery where they would like to minimise or be robust to aleatoric uncertainty but also applications such as materials discovery where it may be beneficial to maximise or be antifragile to aleatoric uncertainty. Both methods are capable of penalising or promoting aleatoric noise in the suggestions and yield improved performance relative to a naive implementation of homoscedastic Bayesian Optimisation on toy problems as well as a real-world optimisation problem.
Accept (Poster). rating score: 6. rating score: 3. rating score: 3. The paper describes new norm based generalization bounds that were specifically adapted to convolutional neural networks. Since convolutional neural networks do not explicitly depend on the input dimension, these bounds share the same property. Further additional improvement over Bartlett et al.‘17 bound, is that this new bound depends on the sum of the operator norms of the parameter matrices, rather than the product. I would be willing to increase my score if the authors added a comparison to Wei and Ma ‘19, and more evidence was provided that the bound is tighter for typical convolutional networks found in practice (please see detailed comments below). Detailed comments:I see Wei and Ma ‘19 cited in the beginning only, but there is no further comparison. They also proved bounds with similar dependencies. What is the dependence of the constant C on \eta in the bounds presented in Theorem 2.1? It is unclear what trade off comes with eta and how the empirical risk term is balanced with the complexity term, since \eta only appears next to the empirical risk term. The authors demonstrate via a concrete example that there exists a setting (depending on epsilon), under which this new bound (up to constants) is tighter than Bartlett et al.bound.Three things remain unclear to me:   How do the constants differ? Is the bound presented in the paper tighter in absolute terms? Is the bound tighter when the norms in the bounded are measured on typical trained neural network weights? In the introduction, the authors:    say that their bounds are size free, which refers to the bounds not having an explicit dependence on the input size. Also, I think that size free in the title is misleading, and should be replaced with input size free. mention that most recent bounds depend on the distance from the initialization instead of the size of the weights. This idea was first presented in Dziugate and Roy ‘17, which does not seem to be cited there. *** UPDATE ***I ve reread the rebuttals and feel that most of my concerns have been addressed. I increased my score to weak accept.<BRK>SummaryThis paper studied the generalization power of CNNs and showed several upper bounds of generalization errors. Their results have two characteristics. First, the bounds are in terms of the quantity that is independent of the input dimension (size free). These results improved the upper bounds that we can derive by naively applying the results of Bartlett et al.(2017) or Neushubar et al.(2017), because the dominant term of the existing upper bounds contained $l_{2, 1}$ or $l_2$ norms, which could depend on the input dimensions in the worst case. The authors empirically showed that there is a correlation between the generalization error of learned CNNs and the dominant term of the upper bound (i.e., the product of the parameter size and the distance from the set of initial parameters). DecisionTo the best of my knowledge, this is the first work that proved the size free generalization bound for multi layer CNNs. However, I think the assumption on the hypothesis class is very restrictive and significantly eases the problem, as I discuss in detail later. Therefore, I judge the technical contribution of the paper is moderate and recommend to reject the paper weakly. By the standard argument of the statistical learning theory (such as Theorem A.4), we can typically bound the generalization error by $O(B\sqrt{D/N})$ where $B$ is the infimum of Lipschitz constant of hypotheses, $D$ is the intrinsic dimension of the hypothesis class, and $N$ is the sample size. Therefore, we can derive the size free generalization bound if $B$ does not depend on the input dimension. Since the hypothesis class $F_\beta$ is defined via the spectral norm of CNNs, it is not surprising that we can derive the size freeness of $B$. The size free generalization bound has been already proven by Du et al., (2017), although it was the two layered case. They imposed a restricted eigenvalue assumption. I think it implies that we need more sophisticated analysis if we do not assume the size freeness of the hypothesis class. Comments  The authors claimed that Figure 3 is consistent with theorems because, according to the upper bound of theorems, the distance from the initialization point decreases when the generalization error is the same and the parameter size increases. Suggestions  Please add the conclusion section which summarizes the paper and discusses the possible research directions. page 3, section 2., theorem 2.1	  I think we should replace $\log(\lambda n)$ and $\log(\lambda)$ in equations with $\log(\beta \lambda n)$ and $\log(\beta \lambda)$, respectively.<BRK>The paper presents a generalization bound based on the number of parameters, the Lipschitz constant of the loss function and the distance of the final weights from the initialization, without dependence on the dimension of the input. The bound improves upon previous bound in some regimes when the size convolutional kernel is much less than the width of the network, which is a reasonable assumption. The paper also gives another bound which works for fully connected layers with an additional term that is linear with the depth of the network. The paper has some nice ideas, but the contribution of the paper is not clear for me. The main theorems are based on previous results (Lemma 2.3). And the remaining work of the paper is mainly deriving the Lipchitz bound to be used in the theorem for various kinds of networks. I think this should be clearly stated in the paper. The experiment part is not quite convincing. It is not clear from the figures that the norm decreases with the number of parameters in the network, which is claimed in the paper. Overall I would not recommend this paper for admission.<BRK>They prove bounds on the generalization error of convolutional networks. The bounds are in terms of the training loss, the number ofparameters, the Lipschitz constant of the loss and the distance fromthe weights to the initial weights. They are independent of thenumber of pixels in the input, and the height and width of hiddenfeature maps. They present experiments using CIFAR-10 with varyinghyperparameters of a deep convolutional network, comparing their boundswith practical generalization gaps.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. This work is focused on learning 3D object representations (decoders) that can be computed more efficiently than existing methods. They propose to solve this by learning an MLP that produces the output by recurrent application, and then composing subapplications of different networks as a type of interpolation. While the fast weights approach is not totally original, its application to this problem is novel and very well suited to it. The paper is also quite well written. I like that the authors are straightforward about the deficiency of the method (i.e.that you can t interpolate in latent space).<BRK>This paper presents a method for single image 3D reconstruction. Things to improve the paper that did not impact the score:  The tables will look a lot nicer if booktab is used in LaTeX It would be great if this paper could follow those recommendations to get better insights in the results. Overall, I am in favour of accepting this paper given some clarifications and improving the evaluations.<BRK>Summary:This paper describes a contextual encoding scheme for reconstruction of 3D pointclouds from 2D images. While most related work was covered well, I believe the authors could have a more up to date list of recent work that reconstructs triangle mesh representations from images [A C] (especially since several of these methods has an architecture that involves encoding and subsequent compositional refinement). Some of the reconstructions shown in this paper are quite impressive, and the quantitative results show outperforming 2 recent methods. Decision: Weak reject because the idea is quite interesting, but I believe a more thorough explanation and expanded experimental comparison would be of great help to ensure the community can appreciate this work. [B] MeshCNN: A Network with an Edge.<BRK>They present a new approach to 3D object representation where a neural network encodes the geometry of an object directly into the weights and biases of a second 'mapping' network. This mapping network can be used to reconstruct an object by applying its encoded transformation to points randomly sampled from a simple geometric space, such as the unit sphere. They find that the proposed approach can reconstruct encoded objects with accuracy equal to or exceeding state-of-the-art methods with orders of magnitude fewer parameters.
Accept (Poster). rating score: 8. rating score: 3. rating score: 3. This paper introduces CLAW, a complex but effective approach to continual learning with strong performance in the sequential task learning setting, as demonstrated on a number of standard benchmarks. I recommend acceptance because:  While conceptually similar to VCL, CLAW is convincingly shown to have superior performance across standard benchmarks and measures. Forward transfer is shown to be substantially better compared to other methods. Overall, the balance between not forgetting and still learning new tasks seems particularly favourable for the proposed method. This has been an elusive goal of continual learning research, hence the importance of accepting this work. The paper is well written but the method is rather complex and presumably non trivial to tune.<BRK>The authors propose a new continual learning method. The model is based on the probabilistic model and variational inference. To show the effectiveness of the proposed model, the authors do experiments on several benchmarks. The motivation of this paper is a task specific weight adaptation mechanism, which seems a simple version of [1]. It is better to explain more. I suggest the authors compare with [2], which also focuses on the performance of new tasks.<BRK>The paper proposed a novel probabilistic continual learning approach which automatically learn an optimal adaptation for arriving tasks while maintaining the performance of the past tasks. CLAW learns element wise weight masking per task task with respect to the several learnable parameters. However, the ablation study and analysis on the model is weak and authors only show experimental observations. Also, the experiments are performed on old architectures. I have several questions,  How about the training time / convergence rate of the CLAW compared to other methods? There might be used a wrong plots in Figure 1 (e).<BRK>Approaches to continual learning aim to successfully learn a set of related tasks that arrive in an online manner. Recently, several frameworks have been developed which enable deep learning to be deployed in this learning scenario. On the one hand, separately modelling each task avoids catastrophic forgetting but it does not support transfer learning and leads to large models. Ideally, the network should adaptively identify which parts of the network to share in a data driven way. Here they introduce such an approach called Continual Learning with Adaptive Weights (CLAW), which is based on probabilistic modelling and variational inference.
Accept (Poster). rating score: 6. rating score: 6. This work presents a method to use active learning to control the labeling process to gather training data in the context of a semantic segmentation application. The proposed strategy contains several novelties related to the model and the application domain. This because for this application the real object boundary is highly informative. A point that it is not clear to me is the initial training of the segmentation network. I rate the paper as weak accept.<BRK># Summary #The paper works on active learning for semantic segmentation, aiming to annotate as few "blocks/patches" as possible while training a strong model. Therefore, I would highly suggest the authors redoing the baseline methods; otherwise, future work that re splits the data (this is totally valid!) The authors deferred the state and action design entirely to the supplementary, while they are the main contributions to the paper. W2.The proposed algorithms seem to be highly time consuming. I have two questions. Can the authors provide more discussion?<BRK>Learning-based approaches for semantic segmentation have two inherent challenges. Second, realistic segmentation datasets are highly unbalanced: some categories are much more abundant than others, biasing the performance to the most represented ones. They present a new active learning strategy for semantic segmentation based on deep reinforcement learning (RL). The region selection decision is made based on predictions and uncertainties of the segmentation model being trained.
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. The paper studies the implicit bias of gradient based adversarial training for linear models on separable data with the exponential loss. These results are further illustrated by numerical experiments. The paper is also well written, and also has a nice numerical validation of the results.<BRK>Take away message of the paper: adversarial training accelerates convergence. Theoretical results are for the training empirical loss.<BRK>The paper applies theory on implicit bias of gradient descent to an adversarial training toy example. It attempts to utilize the theoretical results for deriving insights on how adversarial training establishes robustness.<BRK>Adversarial training is a principled approach for training robust neural networks. In this paper, they provide new theoretical insights of gradient descent based adversarial training by studying its computational properties, specifically on its implicit bias.
Reject. rating score: 3. rating score: 3. rating score: 3. Contributions: this submission proposes a generative framework which employs a hierarchical decomposition of scene, objects and parts. Assessment:  The proposed model and learning framework are closely related to previous work (AIR, SPAIR). In the proposed model, there is no constraint on the part object hierarchy, and it is unclear whether it can learn to discover such hierarchies directly by reconstructing single static images. It is also not evaluated in the submission. There are no comparisons with AIR or SPAIR on the two datasets the authors created. The authors are recommended to compare with previous work on hierarchical generative modelings with objects and parts, such as (Xu et al, ICLR 2019), and other related work, such as SPIRAL (Ganin et al.) Due to the limited contribution, lack of comparison with related work and limited empirical evaluation, I recommend rejection of the submission. The writing has indeed significantly improved and some questions (e.g.varying number of objects) have been addressed. After reading the rebuttal, my concerns remain that: (1) lack of empirical evaluation on more complex real / synthetic datasets; (2) it is still unclear to me how such hierarchy is inferred from single images.<BRK>The quality has improved but as mentioned in the original review, it would be good to see results on a more significant non synthetic task before I would argue for acceptance. The authors propose a generative model with a hierarchy of latent variables corresponding to a scene, objects, and object parts. The method can form bounding boxes around objects and parts in the examples that are shown, and the MSE is similar to that of a VAE. The idea of decomposing a probabilistic model hierarchically is potentially interesting, but this paper has drawbacks in terms of experimental quality, significance, and presentation. Experiments   The experiments are done on a manually constructed synthetic dataset, so it is not clear whether the proposed method would work in more realistic or more challenging settings. For instance, the dataset was constructed using a recursive process which probably does not resemble a realistic distribution of objects and their parts, and the parts are well separated within the object. The paper should more clearly demonstrate the benefits/tradeoffs of the hierarchical aspect.<BRK>The paper proposes an unsupervised approach to learning objects and their parts from images. The method is based on the "Attend, Infer, Repeat" (AIR) line of work and adds a new hierarchy level to the approach, corresponding to object parts. While the general topic of the paper is interesting, I do not think it is fit for publication. First and foremost, the experiments are very incomplete: the method is only evaluated on two custom synthetic datasets and not compared against any baselines or ablated versions of the method. 3) The paper shows generalization to a number of objects different from that seen during training. If this is described in another paper, it would be useful to point there, but still briefly summarize in this paper to make it self contained. Could it be applied to more realistic data, such as for instance ShapeNet objects? 1b) There are no comparisons to baselines. Generally, it is the job of the authors to come up with relevant baselines to show that the proposed model actually improves upon some simpler methods. 2) The novelty is somewhat limited: the method seems like a relatively straightforward extension of SPAIR by adding another hierarchy layer. It might be sufficient if the experimental results would be strong, but given that they are not, becomes somewhat concerning. If the method does indeed include significant technical innovation, it might be helpful to better highlight it. This is by no means a complete list.<BRK>Learning such representation via unsupervised learning can provide various benefits such as interpretability, compositionality, and transferability, which are important in many downstream tasks. In this paper, they propose the first hierarchical generative model for learning multiple latent part-whole relationships in a scene. During inference, taking top-down approach, their model infers the representation of more abstract concept (e.g., objects) and then infers that of more specific concepts (e.g., parts) by conditioning on the corresponding abstract concept. This makes the model avoid a difficult problem of routing between parts and whole. In experiments on images containing multiple objects with different shapes and part compositions, they demonstrate that their model can learn the latent hierarchical structure between parts and wholes and generate imaginary scenes.
Reject. rating score: 3. rating score: 3. rating score: 6. This paper considers efficiently producing adversarial examples for deep neural networks and proposes boundary projection (BP), which quickly searches an adversarial example around the classification boundary. The key idea of BP, searching on the class boundary manifold, is interesting and promising. While the operating characteristic of probability of success and distortion is mainly discussed, it is unclear which argument most demonstrate the improvement in speed distortion tradeoff.<BRK>This paper proposed an adversarial attack method based on optimization on the manifold. The authors claim it is a fast and effective attack even with quantization. I do not think the results in Table 3 are convincing or necessary. The state of the art adversarial training defense uses the adversarial examples obtained from PGD. Similar issues also exist in Table 2.<BRK>This paper introduces a parameterized approach to generate adversarial samples by balancing the speed distortion trade off. Having a limited number of iterations, the method reduces the fluctuations around the boundary and paves the classification manifold. The idea is novel, interesting and well formulated, while the intuition could be better explained. It is also interesting to discuss how the algorithm performs in classes that are linearly separable on a toy dataset.<BRK>In this work, they argue that speed is important as well, especially when considering that fast attacks are required by adversarial training. They investigate this speed-distortion trade-off in some depth and introduce a new attack called boundary projection BP that improves upon existing methods by a large margin. Their key idea is that the classification boundary is a manifold in the image space: they therefore quickly reach the boundary and then optimize distortion on this manifold.
Reject. rating score: 3. rating score: 3. rating score: 3. rating score: 3. The authors address an interesting and important problem of speeding up MRI scans and thus improving the subject s experience. While the paper considers an important problem and takes a novel approach to solve it (using a GAN generative model to estimate uncertainty), I found that it may be particularly inaccessible to non experts in the field of MRI image processing. Please consider making this more clear. This makes the work substantially less accessible to a wide audience with machine learning expertise as the common denominator. This is also potentially problematic for their estimator   in case of mode collapse their method would underestimate uncertainty. Some specific examples the authors should address follow. Previously it was v_i (vector). Incorrect double quotes are used throughout the text (both are right quotes). Could the authors please comment on what they mean by convergence in this case and how they guarantee that the generators they train converge. Perhaps “back” should not be there? al (2019)”. This should be elaborated in the text, and the comparison to Zhang et.<BRK>The paper is well written and easy to follow. The main contribution seems to be the combination of deep Bayesian inversion in Adler & Oktem, 2018 with an uncertainty driven sampling framework. The experimental results suggest that the technique can reduce the amount of time required to obtain good quality images from MRI scans which can potentially have a big financial impact. My main concerns with the paper are:1. The key idea of using uncertainty to guide sampling was also the main concept in Zhang et al.2019.This submitted paper highlights differences in the models but does not provide an experimental comparison. Since both papers share the same concepts, this reviewer considers that a comparison is critical. 3.Given the complexity of learning GANs and the sensitivity to initialization, results should contain more information such as the std of the MSE for several runs of the algorithm.<BRK>The paper proposes an uncertainty driven acquisition for MRI reconstruction. This is mostly an "application" paper that is evaluated on one dataset. My main concerns are as follows:The presentation of the paper could be improved. The experimental evaluation is rather limited and the dataset used in the experimental section is small. As acknowledged by the authors, this paper bears several similarities with the work of Zhang at al. 2019.However, the approach is not compared to Zhang et al.Including this comparison would make the paper stronger. It is interesting to see that CLUDAS outperforms CLOMDAS in terms of SSIM. Is it expected that CLUDAS would outperform CLOMDAS? Could the authors add a citation or explain this part in more detail?<BRK>The paper is quite well written and the idea is novel. However, the results are rather weak. The offline method performs better in terms of MSE, which is the loss it was trained for, meaning that the authors have not demonstrated a gain in adapting the sampling pattern to individual scans. The primary concern with the paper is not with the author’s contribution, but with serious flaws in [Adler 2018] that unfortunately snowball into this one. While the authors in [Adler 2018] do acknowledge issues with learning a variance, they misdiagnose the problem as mode collapse. The Fourier transform being orthogonal, norms and variances should be the same in both domains.<BRK>This work proposes a closed-loop, uncertainty-driven adaptive sampling frame- work (CLUDAS) for accelerating magnetic resonance imaging (MRI) via deep Bayesian inversion. By closed-loop, they mean that their samples adapt in real- time to the incoming data. They use this estimator to drive the sampling for accelerated MRI. Their numerical evidence demonstrates that the variance estimate strongly correlates with the expected MSE improvement for dif- ferent acceleration rates even with few posterior samples. Moreover, the resulting masks bring improvements to the state-of-the-art fixed and active mask designing approaches across MSE, posterior variance and SSIM on real undersampled MRI scans.
Reject. rating score: 1. rating score: 1. rating score: 1. The paper proposes to use an autoencoder, networkX, and node2Vec in succession to convert a Bitcoin transaction to a vector. Given the apparent lack of any technical contribution to machine learning theory or practice, the inconclusive empirical results, and the generally unpolished writing (e.g., long run on sentence in the conclusion, vague problem definition), I do not believe this paper is suitable for publication.<BRK>In this paper, the authors propose a new method for generating vector embeddings. My major concern is that the paper is a bit too short and is lack of some necessary information, for example:1 The authors are encouraged to provide sufficient background introduction, so that the reader can have a big picture of the problem and area.<BRK>Authors propose to apply the existing machine learning model to analyze bitcoin blockchain addresses. It uses autoencoder to extract the feature of the transaction and construct a transaction graph. The proposed method lack of detail description.<BRK>They propose a computationally efficient model to analyze bitcoin blockchain addresses and allow for their use with existing machine learning algorithms.
Accept (Talk). rating score: 8. rating score: 8. rating score: 6. Rewinding is a notion that was already explored in the Lotter Ticket Hypothesis (Frankle et al., 2019), so this paper seems to be more of an extension of that work.<BRK>As it is, I think this paper a worthy (if limited) contribution to the understanding of network pruning.<BRK>The empirical study conducted by this paper is useful and complements the results previously reported in [Frankle et al, 2019].<BRK>In this paper, they compare fine-tuning to alternative retraining techniques.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 3. This paper proposes word2ket   a space efficient form of storing word embeddings through tensor products. While this results in a time cost for each word lookup, the space savings are enormous and can potentially impact several applications where the vocabulary size is too large to fit into processor memory (CPU or GPU).<BRK>The experimental results seem to conflate the issues of the dimensionality of the word embeddings versus that of the higher layers. Overall the paper can be a strong contribution if the methods are stated with less quantum computing jargon, the overall parameter size and speed of the different models is specified in the experiments, and more specific connections to related work are made.<BRK>Their method, inspired by quantum entanglement, involves computing word embeddings on the fly (or by directly computing the output of the "word embedding" with the first linear layer of network). This paper is clearly written (with only a couple of typos) but does not yet reach publication standard. In general the related work and experimental sections are weak and brief, with only superficial analysis. There is  lack of careful analysis and insight into their results, as well as a careful comparisons to other work in this area. Here are some questions for the authors that come to mind when reviewing:How does your method compare to other published methods on your benchmarks?<BRK>Also, semantic relationships between words, learned from a text corpus, can be encoded in the relative configurations of the embedding vectors. Here, they used approaches inspired by quantum computing to propose two related methods, word2ket and word2ketXS, for storing word embedding matrix during training and inference in a highly efficient way.
Accept (Spotlight). rating score: 8. rating score: 6. rating score: 6. From my years of experience training neural networks, I have seen many scenarios in which higher learning rates result in worse performance, even after reducing the learning rate. The authors have addressed my concerns about the paper. Under what conditions does a higher learning rate lead to these effects on K and H and will it always lead to better model performance?<BRK>This work analyzes the optimization of deep neural networks from the point of view of the different learning trajectories obtained during different learning settings as brought about by different hyperparameters in optimization. The authors conduct their analysis for networks with and without BatchNorm. To get a quantitative understanding of the different trajectories of the optimization landscape the authors monitor and analyze the A.  Hessian of the network with respect to the parameters B. Overall the paper is well written and the numerous experimental results are quite impressive.<BRK>Regarding to conjecture 1 which states that “larger” learning rate yields lower \lamda_H^1 and \lamda_K^1, is there a limit for the range of learning rate? The further verified these predictions on BN networks. Comments:Understanding the optimization trajectory is an important topic and this work is one step towards better understanding. I would hope the authors could further improve the paper by well organizing the additional experiments and findings.<BRK>The early phase of training of deep neural networks is critical for their final performance. In particular, they demonstrate on multiple classification tasks that using a large learning rate in the initial phase of training reduces the variance of the gradient, and improves the conditioning of the covariance of gradients.
Reject. rating score: 3. rating score: 3. rating score: 3. rating score: 6. I think that this work has a lot of potential, and am especially impressed by the empirical results. However, the paper itself has significant weaknesses in its writing, analysis, and presentation of ideas. Based on these results, the authors have convinced that RAME is a state of the art algorithm. Taking the lower outcome of an ensemble of size 2 is equivalent to applying a penalty based on the stdev of the outcome distribution, with α 1. There are at least four factors that need to be teased apart: 1) deterministic vs stochastic model 2) lower confidence bound penalty 3) adaptive α for LCBP 4) STEVE reweighting.<BRK>They propose a novel Hybrid RL method, namely the Risk Averse Value Expansion(RAVE), that uses an ensemble of probabilistic dynamics models to generate imaginative rollouts and to model risk aversionof risks by estimating the lower confidence bound of the ensemble. I found this work interesting as the authors try to take the uncertainty of probabilistic dynamics model into account for estimating the value function and its confidence bounds in model free RL.<BRK>SummaryThis paper expands on previous work on hybrid model based and model free reinforcement learning. Specifically, it expands on the ideas in Model based Value Expansion (MVE) and Stochastic Ensemble Value Expansion (STEVE) with a dynamically scaled variance bias term to increase risk aversion over the course of learning, which the authors call Risk Averse Value Expansion (RAVE). RecommendationIn light of my comments above, I cannot currently recommend the acceptance of this paper at ICLR. The core baseline of the paper is DDPG, which is unnecessarily weak. The use of ensembling is another confounder in the experiments. Ensembling models almost always yields an improvement, so any technique that relies on some form of ensembling needs to additionally demonstrate that the gains presented are not solely due to ensembling.<BRK>The paper begins by showing that previous methods like model based value expansion (MVE) and stochastic ensemble value expansion (STEVE) can perform even worse than the pure model free DDPG algorithm in a rather noisy environment. Following the ideas of MVE and STEVE, It then proposes a risk averse value expansion (RAVE) to replace the target Q function in the actor critic algorithm, which is built upon an ensemble of probabilistic models (PE) and adopt the lower confidence bound as a surrogate of the target value as in the risk sensitive RL. The experiments show that RAVE does improve over the state of the art algorithms in several different environments, with a better draw down control. In general, this paper is well written and the idea of RAVE is novel as far as I know.<BRK>However, the previous methods do not take into account the stochastic character of the environment, thus still suffers from higher function approximation errors. In the proposed method, they use an ensemble of probabilistic models for environment modeling to generate imaginative rollouts, based on which they further introduce the aversion of risks by seeking the lower confidence bound of the estimation. Experiments on different environments including MuJoCo and robo-school show that RAVE yields state-of-the-art performance.
Reject. rating score: 3. rating score: 3. rating score: 6. It is easy to show that, when U is timed with a matrix R, where RR^T  I, as in U   UR, there exist a V  and \alpha that gives R^Tf(Vx)   \alpha f(V x) so that Uf(Vx)   URR^Tf(Vx)   U f(V x) for the hyperbolic tangent function. The paper presented a method for studying the landscape of the loss function w.r.t.parameters in a neural network from the perspective of weight space symmetry.<BRK>However, it seems to me this paper does not make enough contributions. List of contributions of the paper: 1. Prop.3 is about counting the number of permutation points. Why is this finding interesting and useful? (noting that the proof of the existence of such a path seems to be much easier than proving the existence of such a path for two general global minima). b)	The motivation of the algorithm is not clear.<BRK>However, there are a few points that I wasn’t entirely clear on. Conclusion   I like the paper and the idea in general. My main point of confusion relates to the connection between this work and the low loss connectivity between inequivalent optima found in literature, which (at least to me) seems to be the more interesting of the two connectivities.<BRK>In relation to the structure of the landscape, they study the permutation symmetry of neurons in each layer of a deep neural network, which gives rise not only to multiple equivalent global minima of the loss function but also to critical points in between partner minima. Moreover, they introduce higher-order permutation points by exploiting the hierarchical structure in the loss landscapes of neural networks, and find that the number of $K $-th order permutation points is much larger than the (already huge) number of equivalent global minima--at least by a polynomial factor of order $K $.
Reject. rating score: 1. rating score: 6. rating score: 6. After reading Sections 1 and 2, I believe that the authors have a misunderstanding of the relationship between Markov Games and Extensive Form Games. I did not read the proofs or experiments closely since I believe there are flaws in the central idea of this work. The authors make an artificial distinction that "turn based" games cannot be handled under the MG formalism. The fundamental difference between these two formalisms is that Markov Games aka Stochastic Games (https://en.wikipedia.org/wiki/Stochastic_game) are fully observed while EFGs in general are not fully observed.<BRK>In this work, a multi agent imitation learning algorithm for extensive Markov Games is proposed. This may be due to the asynchronous setting, but I think it should be mentioned in the paper. The contribution of this submission can be summarized as follows. Followed by Theorem1 and 2, authors define an extensive occupancy measure, a natural extension of occupancy measures in MGs, and cast a multi agent imitation learning problem into extensive occupancy measure matching problem in Theorem 3.<BRK>The submission extends the MARL◦MAIR to the extensive Markov game case, where the decisions are made asynchronously. To  this end, the submission takes advantage of the previous game theory results, to formulate the problem, and transform the model to a MAGAIL form. I believe the submission considers an interesting and challenging problem, and has extended the existing multi agent IRL methods to the extensive Markov game case.<BRK>Imitation learning aims to inversely learn a policy from expert demonstrations, which has been extensively studied in the literature for both single-agent setting with Markov decision process (MDP) model, and multi-agent setting with Markov game (MG) model. However, existing approaches for general multi-agent Markov games are not applicable to multi-agent extensive Markov games, where agents make asynchronous decisions following a certain order, rather than simultaneous decisions. They propose a novel framework for asynchronous multi-agent generative adversarial imitation learning (AMAGAIL) under general extensive Markov game settings, and the learned expert policies are proven to guarantee subgame perfect equilibrium (SPE), a more general and stronger equilibrium than Nash equilibrium (NE). The experiment results demonstrate that compared to state-of-the-art baselines, their AMAGAIL model can better infer the policy of each expert agent using their demonstration data collected from asynchronous decision-making scenarios (i.e., extensive Markov games).
Reject. rating score: 3. rating score: 3. rating score: 6. The authors make two claims1. M1 is better than M2 implies "VAE" is more powerful than "Glow0", which I doubt. Finally, the paper is somewhat incremental. Particularly comparing with VAE IAF, where this paper just adds flow layers to not only q but also p.Update After reading the rebuttal I found some of my concerns are unaddressed. The purpose of my initial question is for some in depth analysis and intuition / theory.<BRK>Unfortunately, the paper is not clearly written. A lot of details are missing that makes the paper impossible to reproduce and fully understand. The authors skipped many recent papers. The paper misses a lot of important details. Nevertheless, I would be more than interested in seeing a more thorough analysis of this phenomenon.<BRK>The authors also mentioned that jointly training resulted in images with poor qualities. Is it related to the size of the Glow used? My detailed comments and questions are as follows. 4.For the Glow used in experiments, how does its architecture compare with the one used in the original paper?<BRK>Due to the inherently restrictive design of architecture, however, it is necessary that their model are excessively deep in order to achieve effective training. In this paper they propose to combine Glow model with an underlying variational autoencoder in order to counteract this issue.
Reject. rating score: 3. rating score: 6. rating score: 6. rating score: 8. rating score: 8. Authors proposes an interesting statistical method to detect saliency in images. Authors provides a specific estimator that is fast to compute and characterize its performance w.r.t.parameters. My main concern is the experiment section. "For computational efficiency, we compute saliency maps on a 28 by 28 grid (i.e.γ˜ ∈ R 28×28) although the standard input for VGG 19 is 224 by 224. T".Shouldn t we do the same thing for all baselines? The seemingly good sailency results might stem from this artifact.<BRK>This work proposes a statistical framework for saliency estimation for black boxcomputer vision models, based on solving a convex program in (4). It also gives theoretical analysis on its consistency in Theorem 1, and run a few simulations to show the empirical performance of the proposed method. The method proposed seems to be novel and reasonable. However, I would like to remark that solving (4) might be empirically difficult (depending on the size of the problem) even though it is convex and can be solved in polynomial time theoretically. I wonder if the authors could clarify the setup of their experiments (instead of writing "The problem in equation 4 can besolved by any linear programming software, for which many open source implementations exist"), and if the author could remark on the empirical running time. Also, I am not sure if "Note that if L   0, then the TV penalization has no effect and the solution of the above procedure reduces to the empirical estimate," as the objective function is in L1 norm.<BRK>Author propose a statistical framework and a theoretically consistent procedure for saliency estimation that is close to the empirical solution and has sparse differences on the grid. I think the idea of the paper is quite interesting and the results are significant. Moreover, they have proposed a new perturbation scheme for estimation of gradients that works better than random perturbation schemes. I have some questions listed below:  Can the proposed approach be integrated with different types of Saliency map methods? Is there a quantitative way to assess the performance of the proposed approach?<BRK>ConclusionOverall, the paper provides a nice method along with analysis on convergence rates and other statisticalproperties. In general, I think more effort should be put into the flow and writing of the paper. Overall, this is an interesting contribution. ## After reading author responsesI believe the authors have clarified and improved the readability of the paper and clarified several of the questionsthat I had. Possible Improvements  The LEG method is not sufficiently motivated. While I believe this is a valuable contribution to the sea of attribution methods that have now been published, like the authors noted, it is still not clearly if attribution methods as a wholeare useful of decision making or understanding of a model by either a generic end user or the model developer. See the question section  for some of the issues I raised there. From figure 4, we see that the method passes the proposed sanity checks which seem like a key motivation for this work, however, the authors don t give an explanation for why this is thecase. This is a huge problem in this area that deserves significant attention. What are the constraints? For example, there should be a factor of 2 somewhere after taking the derivative wrt to $vec(g)$, but I don t see it. Further, What does it mean to take expectation wrt $F + x_0$.<BRK>The authors propose a nice framework for interpreting differentiable models without access to model details. The performance, in terms of log odds ratio, may not be as good as some of the comparing methods. There also seems to be a lot more to investigate for future work based on this framework. However, the paper provides a creative framework for incorporating structure into feature attribution scores in model interpretation. A typo: The first paragraph of Section 3: " less model evaluations." Overall, the paper proposes a nice framework for model interpretation. should be " fewer model evaluations." But there are two weaknesses. First, the authors should note that the proposed definition 2 is still a bit too straightforward without intuitive explanations. 3.It seems the experimental section of the paper is not satisfactory. Without such analysis, it is hard to see the advantage of the proposed method over other comparing saliency maps and model agnostic methods. For example, is the proposed method more sample efficient than LIME or SHAP, or other more efficient procedures such as L(C) Shapley? It is observed that LEG is able to select connected regions (Figure 6). The same phenomenon has been observed for C Shapley.<BRK>The use of deep learning for a wide range of data problems has increased the need for understanding and diagnosing these models, and deep learning interpretation techniques have become an essential tool for data analysts. Although numerous model interpretation methods have been proposed in recent years, most of these procedures are based on heuristics with little or no theoretical guarantees. They build a model-agnostic estimation procedure that is statistically consistent and passes the saliency checks of Adebayo et al. (2018).Our method requires solving a linear program, whose solution can be efficiently computed in polynomial time. Through their theoretical analysis, they establish an upper bound on the number of model evaluations needed to recover the region of importance with high probability, and build a new perturbation scheme for estimation of local gradients that is shown to be more efficient than the commonly used random perturbation schemes. Validity of the new method is demonstrated through sensitivity analysis.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. This paper proposed a new algorithm stochastic variance reduced policy gradient algorithms. This paper establishes better sample complexity compared with existing work. The key part of the proposed algorithm for variance reduction is to have step wise importance weights to deal with the inconsistency caused by varying trajectory distribution. In particular, there are a lot of discussions comparing this work with existing work. In section 3, it is not quite clear how the reference policy is defined, and the \theta^s is not clearly defined when s >  1.<BRK>The authors propose a new stochastic reduced variance policy gradient estimator, which combines a baseline GPOMDP estimator with a control variate integrating past gradients by importance re weighting. The authors establish the sample complexity of gradient descent using the proposed estimator, and further demonstrate its effectiveness through some simple empirical results. I believe this paper is a good contribution for ICLR. The result is relevant and interesting, and extends recent ideas around reduced variance policy gradient estimators. The empirical results presented are interesting, although I wish they were more comprehensive. Given that the theoretical results also apply to projected gradient descent, it would be interesting to see empirical results in that case.<BRK>Summary: The paper proposed a policy gradient method called SRVR PG, which based on stochastic recursive gradient estimator. It shows that the complexity is better than that of SVRPG. Since both papers are highly related, I would suggest the author(s) have some discussions to differentiate two papers. Basically, you adopt the existing estimator based on SARAH/SPIDER in optimization algorithms into RL problems. Notice that, the complexity result achieved in this paper is also matched the one for SARAH/SPIDER/SpiderBoost in nonconvex optimization. 5) In Papini et al., 2018, for the experiment part, they use a snapshot policy to sample in the inner loop, and use early stopping inner loop. Moreover, they also check variance to recover the backup policy when it is blowup.<BRK>In this work, they aim to reduce the sample complexity of existing policy gradient methods. This sample complexity improves the existing result $O (1/\epsilon^ {5/3}) $for stochastic variance reduced policy gradient algorithms by a factor of $O (1/\epsilon^ {1/6}) $. They conduct numerical experiments on classic control problems in reinforcement learning to validate the performance of their proposed algorithms.
Reject. rating score: 3. rating score: 3. rating score: 6. rating score: 6. The paper demonstrates an interesting application of progressive compression to reduce the disk I/O overhead of training deep neural networks. My major concern is that the paper should be clearer about the setting. That being said, I think this approach should be appealing when the I/O bandwidth is limited and dynamic. * Sec 2 paragraph 3. Does it have impact on the I/O efficiency?<BRK>How tie is the proposed record encoding with the image compression? Decoding a typical progressive JPEG image usually takes about 2 3 times as much time as decoding a non progressive JPEG, for full resolution, analyzing the time to read vs time to decode the images would be great. It is not clear how changing the number of total groups would affect the image size and the reading speed. Based on the current experiments it is not clear what is the impact of the batch size when creating PCRs and when reading the image blocks, or the impact of the batch size on the training speed.<BRK>This paper introduces Progressive Compressed Records (PCR) which is an on disk format for fetching and transporting training data in an attempt to reduce the overhead storage bandwidth for training large scale deep neural networks. My only concern is that although the related work section provides a thorough survey of the current methods in the literature, the authors did not demonstrate the performance of state of the art and compare their performance with them. I believe this is necessary to truly validate the superiority of their method over state of the art.<BRK>Summary: This paper introduces a new storage format for image datasets for machine learning training. Related work section is thorough. The experiments are limited to image classifications, and some of the datasets are subsampled (e.g.ImageNet and CelebA). This may not well represent real machine learning tasks, and practitioners may be unsure about the reliability of the compression.<BRK>Deep learning training accesses vast amounts of data at high velocity, posing challenges for datasets retrieved over commodity networks and storage devices. They introduce a way to dynamically reduce the overhead of fetching and transporting training data with a method they term Progressive Compressed Records (PCRs). They show that models can be trained on aggressively compressed representations of the training data and still retain high accuracy, and that PCRs can enable a 2x speedup on average over baseline formats using JPEG compression.
Accept (Spotlight). rating score: 8. rating score: 6. rating score: 6. This potential issue is partially supported by the results presented in the mauscript, where there is a gain only when the correct embedding is chosen.<BRK>The paper uses a mapping from nodes to an embedded space and introduces a second type of a neighborhood: a proximity in the embedded space. In the embedded space, a set of relations of nodes is defined. This approach allows one to overcome the issues described above.<BRK>This is very important for fully evaluating your methods. The method proposed in this work is novel and interesting.<BRK>The proposed aggregation scheme is permutation-invariant and consists of three modules, node embedding, structural neighborhood, and bi-level aggregation.
Reject. rating score: 1. rating score: 1. rating score: 3. This paper investigates how LSTM networks learn quantum optical experimental setup and predict characteristics of resulting quantum states. In the present form I argue for rejection. The paper could have been a nice opening towards new applications for ICLR but it would have to be written in a much more pedestrian manner.<BRK>This paper looks at the problem of predicting 2 properties of quantum states produced by an optical table consisting of a sequence of physical elements that modify a quantum state in a particular way, given the sequence of optical elements applied. The authors train 2 separate recurrent networks (LSTMs). I believe the results are promising, but more clarity and more work on generalizing beyond the restricted regime presented would greatly improve the paper. I think authors should be clearer about the actual problem their paper is addressing.<BRK>This paper proposed to use machine learning models to predict certain properties of complex quantum systems. In general, this paper is easy to follow and clearly presents the main idea and verifies the effectiveness of the proposed method. However, I still have some concerns about this paper. In my opinion, this paper should be submitted to a quantum physics journal or conference, rather than a machine learning conference.<BRK>Of particular interest are complex quantum states with more than two particles and a large number of entangled quantum levels. To search for interesting experiments, one thus has to randomly create millions of setups on a computer and calculate the respective output states.
Reject. rating score: 3. rating score: 3. rating score: 3. The paper is well written, and the experiments are well designed to support the claim.<BRK>Overall, I believe that this paper is below the acceptance threshold due to lack of 1) novelty, 2) significance and 3) depth of analysis. In comparison to prior work, the current paper has a more limited scope and significance.<BRK>The paper chooses a single method class of model based methods to do this comparison, namely dyna style algorithms that use the model to generate new data. The first is the presentation of the empirical results.<BRK>Furthermore, based on the outcomes of the study, they argue that the agent similar to the modified Rainbow DQN that is presented in this paper should be used as a baseline for any future work aimed at improving sample efficiency of deep reinforcement learning.
Reject. rating score: 3. rating score: 6. rating score: 6. This paper studies how to build semantic spatial maps for the purpose of navigation in 3D environments. The paper presents a differentiable policy network that pastes together semantic map predictions into a spatial map. This information is used to produce actions. Using learning to leverage semantic reasoning, and structuring the computation spatially makes a lot of sense. Maintaining and updating allocentric maps, and reading off egocentric maps. The central contribution of the paper is the design of the egocentric spatial memory, how to build and maintain it over time, and its use in deep RL. The paper does this by using components from previous papers and presents a very nice summary of this in Table 1. 2.Following on from point above, putting everything together and showing that it works, could also be a reasonable contribution, though it would warrant more extensive and systematic experiments for the different design choices, possibly in more realistic environments. b.Past works have demonstrated these ideas in visually realistic environments (similar to those in Gibson / Habitat, see semantic tasks in CogMap).<BRK>The paper proposes a novel architecture for spatially structured memory. The main idea is to incorporate inductive bias/invariance derived from projective geometry arguments. The experiments seem to clearly show that this new architecture improves previous approaches to tasks which require spatial reasoning and memory, and the ablations studies and visualizations provide useful insights into the workings of the agent. One thing I m missing is an experiment showing that this inductive bias also doesn t degrade performance on tasks where spatial reasoning is not necessary (as compared to vanilla GRU/LSTM).<BRK>It elegantly combines these ideas together by presenting a neural agent architecture that consists of:* a perception module (e.g.a convnet) that extracts coarse visual feature maps s_t from an RGBD image* a differentiable map canvas M_t that is rotated at each step based on affine egocentric velocity (dx_t, dy_y, d \phi_t)* differentiable inverse projection mapping, which uses known camera parameters, projective geometry and the depth channel of the image to project the visual feature vectors s_t onto a 2D map and add it to the existing canvas M_t* a recurrent module (GRU) for update a state h_t that is used for computing the policy distribution and value function* additional inputs to the policy and value function, that include a global map read r_t, as well as a query q_t (produced by the policy head) based retrieval of features from the map* position indexing of features retrieved from the mapThe algorithm is trained end to end, without extra supervision, using Advantage Actor Critic (A2C) RL. Criticism:The authors could justify better the choice of using the projective geometry inductive prior. This paper essentially combines existing ideas (see table 1): projective geometry, reward based learning of M_t, RL, multitask navigation, semantic features. What is disappointing, given that this is a combination paper, is that the environment is so simply, and that photorealistic environments were not tested. For example, the VizDoom environment uses 2D sprites for objects, making the visual feature extraction from objects much simpler. Would the method work equally well with the objects in DeepMind Lab, which are seen from multiple view points?<BRK>They present EgoMap, a spatially structured neural memory architecture. EgoMap augments a deep reinforcement learning agent ’ s performance in 3D environments on challenging tasks with multi-step objectives. The map is updated with ego-motion measurements through a differentiable affine transform. They show this architecture outperforms both standard recurrent agents and state of the art agents with structured memory. They demonstrate that incorporating these inductive biases into an agent ’ s architecture allows for stable training with reward alone, circumventing the expense of acquiring and labelling expert trajectories.
Reject. rating score: 3. rating score: 3. rating score: 6. For example: It does not allow continual adaptation. This is an important limitation of existing consistent meta learning methods and this paper does not address it. 2.The basic idea is to meta learn a model that can adapt to different MDPs using a small amount of data. Even though the authors have included hyper parameters in the appendix in the updated version of the paper, they still do no specify how these parameters were selected. This means that for with in distribution meta testing tasks, the policy can be used as it is (by giving it the right context vector which can be computed by adapting the model). ### Decision with reasons I vote for rejecting the paper in its current form for the following reasons:1  The paper assumes that it is possible to learn models for out of distribution tasks with a few samples that are accurate on all the previously stored data. It s hard to judge the importance of the experimental results because of this. At adaptation, only the context vector is being updated whereas model parameters (theta) are fixed.<BRK>Summary The authors propose an algorithm for meta rl which reduces the problem to one of model identification. The main idea is to meta train a fast adapting model of the environment and a shared policy, both conditioned on task specific context variables. At meta testing, only the model is adapted using environment data, while the policy simply requires simulated experience. Finally, the authors show experimentally that this procedure better generalizes to out of distribution tasks than similar methods.<BRK>Although the submission had a few typos (I am not a native English speaker, but I d encourage the authors to polish the writing of the paper), it s a very well written paper overall. Number of gradient steps is an important tuning parameter for MAML, it would be interesting to discuss number of gradient steps within the context of MIER. 2.It might be useful to conduct some qualitative results to understand the model learned with MIER against the baselines, e.g., how well MIER adapt to the out of distribution tasks with simulated data points (examples o such qualitative studies could be found, say, in Finn et al., 2017). 3.Given the fact that one major contribution of this paper was reformulating the meta RL problem as model identification, it would be useful to conduct some quantitative study to help the readers understand the effectiveness of learning the environment model p(s’, r|s,a) compared to ground truth, and how the quality of the learned environment model made an impact on the overall performance of the model. For example, the specification of both environment and policy models were not discussed in the paper. It might be useful to make it clearer.<BRK>While meta-reinforcement learning has enabled agents to leverage prior experience to adapt quickly to new tasks, the performance of these methods depends crucially on how close the new task is to the previously experienced tasks. Current approaches are either not able to extrapolate well, or can do so at the expense of requiring extremely large amounts of data due to on-policy training. In this work, they present model identification and experience relabeling (MIER), a meta-reinforcement learning algorithm that is both efficient and extrapolates well when faced with out-of-distribution tasks at test time based on a simple insight: they recognize that dynamics models can be adapted efficiently and consistently with off-policy data, even if policies and value functions cannot. These dynamics models can then be used to continue training policies for out-of-distribution tasks without using meta-reinforcement learning at all, by generating synthetic experience for the new task.
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. Summary: This paper proves a theoretical result, that for every continuous function f, there exists a ReLU network approximating f well, and for which interval propagation will result in tight bounds. The paper shows that approximating a continuous function with a network such that bound propagation works well on it is feasible. The proof is by construction: the function to approximate is decomposed into a sum of functions (slices) with bounded range, which will be approximable to the correct precision.<BRK>This paper proves the universal approximation property of interval certified ReLU networks. We are not getting any closer to improving the certified robustness with this paper.<BRK>The paper aims to show that there exist neural networks that can be certified by interval bound propagation.<BRK>Training neural networks to be certifiably robust is critical to ensure their safety against adversarial attacks. They prove that for every continuous function $f $, there exists a network $n $such that: (i) $n $approximates $f $arbitrarily close, and (ii) simple interval bound propagation of a region $B $through $n $yields a result that is arbitrarily close to the optimal output of $f $on $B $. Their result can be seen as a Universal Approximation Theorem for interval-certified ReLU networks.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. Paper ClaimsThe paper offers a new deep learning approach to symbolic reasoning over text. They propose using Neural Module Networks to perform explicit reasoning steps that are nevertheless differentiable. The proposed model s performance surpasses previous SOTA on several question types. Nevertheless, the NMN approach taken here can be a stepping stone to further understanding how to tackle symbolic reasoning in a deep neural network. The auxiliary supervision tasks appear to be essential to obtaining the results, most notably the unsupervised loss for IE. In particular, the writing of heuristics is a very specific solution targeting specific types of question and this will not scale to the full scope of natural language questions, and much less to all reasoning. This is truly great work that deserves to be published, discussed, and expanded upon.<BRK>These modules can be compositionally combined to perform complex reasoning. (2) To overcome the challenge of weak supervision, the authors proposed to use auxiliary loss (information extraction loss, parser supervision, intermediate output supervision). Strength:(1) The problem of applying symbolic reasoning over text is important and very challenging. This work has explored a promising direction that applies NMN, which achieved good results in VQA, to QA tasks that requires reasoning, specifically, a subset of the DROP dataset. However, results on the full dataset seems necessary for evaluating the potential of NMN approach over text. (2) There are several modules introduced in the paper, but there isn t much analysis of them during the experiments. However, the comparison with MTMSN using less training data seems a bit unfair since the proposed model is given more supervision (question parse supervision and intermediate module output supervision).<BRK>This paper proposes a model and a training framework for question answering which requires compositional reasoning over  the input text, by building executable neural modules and training based on additional auxiliary supervision signals. I really like this paper and the approach taken: tackling complex QA tasks is an important topic and current state of the art methods rely heavily on lexical similarities as mentioned in the paper. I think learning differentiable programs is a good direction to address this space. To be specific, the results in Table 2 are very close between MTMSN and the BERT based model proposed and it s not clear if the difference is because (1) the model is generally better; (2) this is a subset of the dataset that this model performs better; (3) this is because of the additional supervision signals provided (e.g.the results of Fig 2a without the aux sup is almost the same as MTMSN) and if we provided similar auxiliary supervision for other models they would equally do well; (4) due to lack of reporting variance and error bars across runs we see a small increase which may not be significant; ...Again, the paper is very interesting, but I don t think it s clear and thorough to experimentally prove that the overall approach is working better.<BRK>However, they find that it is challenging to learn these models for non-synthetic questions on open-domain text, where a model needs to deal with the diversity of natural language and perform a broader range of reasoning. They extend NMNs by: (a) introducing modules that reason over a paragraph of text, performing symbolic reasoning (such as arithmetic, sorting, counting) over numbers and dates in a probabilistic and differentiable manner; and (b) proposing an unsupervised auxiliary loss to help extract arguments associated with the events in text. Their proposed model significantly outperforms state-of-the-art models on a subset of the DROP dataset that poses a variety of reasoning challenges that are covered by their modules.
Reject. rating score: 3. rating score: 3. rating score: 3. In this paper, the authors uses the random matrix theory to study the spectrum distribution of the empirical Hessian and true Hessian for deep learning, and proposed an efficient spectrum visualization methods. The results obtained in the paper can shed some lights on the understanding of existing optimization algorithms (e.g., first order methods). 1)	The work is based on some assumptions, which is however not very reasonable. Although the authors made some discussions on generalizing the assumptions, it is unclear from the limited discussions whether the same theoretical results could be obtained in the generalized setting. A better way would be to design a simulation experiments, in which we know the data distribution (and thus can compute the true Hessian). So it is hard to generalize the experimental results to the entire space of “deep learning” (as indicated by the title).<BRK>This paper analyzes the spectrum of the Hessian matrix of large neural networks, both from the theoretical and the empirical perspective. Generally, analyzing the local curvature of (approximate) optima is an important problem for building a better understanding of neural network loss surfaces and for shedding light on how that geometry might be related to generalization performance. For these reasons, I think this paper is a bit below the bar for acceptance, and I d encourage the authors to add more content and depth to their analysis in a future submission. Some of the assumptions are very strong, especially the independence of the entries in \epsilon. While I actually suspect that the overall conclusion of spectral broadening may be fairly robust, the argument in section 4.3 did not convince me.<BRK>The paper compares the true hessian and the empirical hessian of the loss function, showing the spectrum of the Empirical Hessian is generally broadened, and proposes a way to visualize the spectrum. However, the reviewer has the following concerns:As to the assumption in this paper, it is too strong to assume independence between different elements of Hessian, Wigner ensemble seems to be a better model, since the only independence only comes from samples instead of the elements of the Hessian. Furthermore, it seems that Lemma 1 and Theorem 1 are asymptotic results.<BRK>The geometric properties of loss surfaces, such as the local flatness of a solution, are associated with generalization in deep learning. The Hessian is often used to understand these geometric properties. They investigate the differences between the eigenvalues of the neural network Hessian evaluated over the empirical dataset, the Empirical Hessian, and the eigenvalues of the Hessian under the data generating distribution, which they term the True Hessian. Under mild assumptions, they use random matrix theory to show that the True Hessian has eigenvalues of smaller absolute value than the Empirical Hessian.
Reject. rating score: 1. rating score: 3. rating score: 6. The paper addresses the challenge of hard exploration tasks. The approach taken is to apply self imitation to a diverse selection of trajectories from past experience   practice re doing the strangest things you ve ever done. This is claimed to drive more efficient exploration in sparse reward problems, leading to SOTA results for Montezuma s Revenge without certain common aides. The approach is incompletely motivated. Why trajectory conditioned policy over just goal conditioned policy? Why this use strategy specifically? In 2019 (post Go Explore), it s not clear Montezuma s revenge poses a significant exploration challenge   exploration doesn t even need to be interleaved with learning. This reviewer moves to reject the paper primarily for not balancing the high complexity of the solution to the lower difficulty of the problem. If the authors want to escape the shadow of this kind of technique which cheats by some framings of RL, more appropriate demonstration environments must be selected.<BRK>The authors identify and address the problem of sub optimal and myopic behaviors of self imitation learning in environments with sparse rewards. The authors propose DTSIL to learn a trajectory conditioned policy to imitate diverse trajectories from the agent’s own past experience. Overall, this paper is well written with comprehensive experimental results. Extensive experimental results demonstrated the effectiveness of the proposed DTSIL. However, it would be nice to compare it with traditional reinforcement learning (e.g., with \epsilon greedy policy for random exploration)? The authors did not explain how \delta_t was selected in their experiments. Choosing the right \delta_t may be hard, but it would be nice to introduce what “heuristics” the authors used and suggest to readers.<BRK>This paper proposes an approach for diverse self imitation for hard exploration problems. The idea is leverage recently proposed self imitation approaches for learning to imitate good trajectories generated by the policy itself. The authors view this approach as a generalization of Go Explore, since it does not rely on having a reset mechanism. For instance, if the environment is deterministic, then why not just do something like Go Explore, since state reset is just memorizing a deterministic action sequence? The results on stochastic environments (in the Appendix) seem pretty weak (but please correct me if I m mistaken here). For instance, one could conduct a systematic study (say of the Apple domain) where one varies the degree of stochasticity and measures how the performance the proposed algorithm changes, perhaps relative to Go Explore on the purely deterministic version of the environment.<BRK>However, it is very difficult to achieve similar success without relying on expert demonstrations. Recent works on self-imitation learning showed that imitating the agent's own past good experience could indirectly drive exploration in some environments, but these methods often lead to sub-optimal and myopic behavior. To address this issue, they argue that exploration in diverse directions by imitating diverse trajectories, instead of focusing on limited good trajectories, is more desirable for the hard-exploration tasks. They propose a new method of learning a trajectory-conditioned policy to imitate diverse trajectories from the agent's own past experiences and show that such self-imitation helps avoid myopic behavior and increases the chance of finding a globally optimal solution for hard-exploration tasks, especially when there are misleading rewards. Their method significantly outperforms existing self-imitation learning and count-based exploration methods on various hard-exploration tasks with local optima.
Reject. rating score: 1. rating score: 3. rating score: 8. The authors propose a neural architecture search (NAS) method to construct a Bayesian ensemble of deep learning models. The authors propose to use a differentiable architecture search method which model the architectural parameters using a concrete distribution. This idea was originally proposed by Xie et al.(2019) but this work was not discussed. In my opinion the novelty with respect to NAS is the WAIC objective function and its application to out of distribution detection. The idea of using ensemble to detect out of distribution examples is not new. Obvious baselines are missing. There is no experiment that proof that this way of searching architectures finds better suited ensembles. How about maximizing the cross entropy and train the discovered architecture multiple times from scratch and use these models in an ensemble to detect out of distribution examples? Concluding, the idea is nice but based on the current state of the paper it seems incremental.<BRK>This paper considers the neural architecture search (NAS) problem under the out of distribution (OoD) environment. As the OoD problem is not visited in the current NAS literature, this paper proposes replacements for each of the three standard components in NAS, i.e., the proxy task, the search space, and the optimization algorithm; and each replacement is built upon an ensemble of existing techniques. Overall, the novelty in this paper is limited and experiments are not very convincing. For the NAS problem, the architectural parameters must be guided by the validation set. If the validation set and the testing set have the same distribution, is it still a meaningful OoD problem? Q3.Except for WAIC, what other metrics can we consider? The authors should have a more comprehensive related work section, which includes discussion on this part. Specifically, why they are OoD problems. Based on the descriptions from the authors, these data sets seem to be standard ones. Q5.Variation is already considered in (1), I mean the second term there. Is it better to have a comparison with standard NAS in the experiments? While authors argue they are not applicable here, it is still good to demonstrate how not applicable they are. "First, optimizing p(α), a probability over ..., each network’s optimal parameters would need to be individually". Q8.What is the searching time of the proposed method? How s it compared with recent NAS methods?<BRK>Hendrycks et al.does assume access to OOD data, but not OOD data seen during evaluation; in this way, they do not assume data is "known ahead of time," which they reiterate throughout their paper. This paper throws neural architecture search at the problem of out of distribution detection. The approach appears to work on some of their cherry picked OOD datasets. I give this paper a 3 because they are quite possibly only showing their strongest results and not giving a complete picture, and there are numerous small errors throughout the paper. After a more thorough evaluation, the technique will likely not look strong on datasets such as CIFAR 10 vs CIFAR 100. I suggest just comparing against the Maximum Softmax Probability Baseline in Table 1 since the rest of this paper assumes OOD examples are not known ahead of time. Update: I have changed my score to an 8. If these issues are fixed, the paper is easily a 6 or an 8, depending on the results of currently unshown OOD datasets. Currently the paper leaves the impression that this is not only leapfrogging past previous density estimators, but that it also is beating multi class classifiers. This likely isn t true. We need to see performance on other OOD datasets. I am willing to increase my score from a 6 to an 8 even if the results are negative. In summary, I give this a 3 due to critical flaws, but if these are rectified, the paper will likely deserve a 6 or 8. For generative models, ensembles can help. I think they mean "with access to labels." Multi class classifiers are the most performant tool for OOD detection, and unsupervised generative models are around chance levels. > "Moreover, existing methods to calibrate model uncertainty estimates assume access to OoD data during training (Lee et al., 2018; Hendrycks et al., 2019).<BRK>Machine learning systems often encounter Out-of-Distribution (OoD) errors when dealing with testing data coming from a different distribution from the one used for training. With their growing use in critical applications, it becomes important to develop systems that are able to accurately quantify its predictive uncertainty and screen out these anomalous inputs. However, unlike standard learning tasks, there is currently no well established guiding principle for designing architectures that can accurately quantify uncertainty. Unlike standard neural architecture search methods which seek for a single best performing architecture, NADS searches for a distribution of architectures that perform well on a given task, allowing us to identify building blocks common among all uncertainty aware architectures. With this formulation, they are able to optimize a stochastic outlier detection objective and construct an ensemble of models to perform OoD detection.
Reject. rating score: 1. rating score: 3. rating score: 6. This paper adversarially trains models against l_p norms where p is of there different values. They make it seem surprising by suggesting that ABS suggested adversarial training is doomed and cannot provide robustness to l_1, l_2, l_\infty norms simultaneously. Overall the paper is deficient in creativity and generality, so I vote for rejection.<BRK>The paper proposes to do adversarial training on multiple L_p norm perturbation models simultaneously, to make the model robust against various types of attacks. Unfortunately this is not discussed in the paper. Thanks for the response.<BRK>Summary of the paper: The paper describes adversarial training aiming to build models that are robust to multiple adversarial attacks   with L_1, L_2 and L_inf norms. The method is a based on adversarial training against a union of adversaries. The experimental results demonstrate improvement over several baselines.<BRK>With this approach, they are able to train standard architectures which are robust against l_inf, l_2, and l_1 attacks, outperforming past approaches on the MNIST dataset and providing the first CIFAR10 network trained to be simultaneously robust against (l_inf, l_2, l_1) threat models, which achieves adversarial accuracy rates of (47.6%, 64.3%, 53.4%) for (l_inf, l_2, l_1) perturbations with epsilon radius = (0.03,0.5,12).
Accept (Talk). rating score: 8. rating score: 8. rating score: 8. I think the paper is a strong accept: the framework has some limitations in the current form (mostly in terms of what architectures are supported), however it still provides a very useful and extensible tool for researchers to efficiently experiment with a variety of more complex optimisation architectures.<BRK>This is a good paper. Issues of this kind have been discussed at length within the community, particularly on GitHub, and related issues with optimization of automatic differentiation code have motivated other software developments, such as Julia s Zygote package. * P4: please cite Baydin [3] who provides a very nice review of automatic differentiation. * The paper s title is not very informative about what the paper is about.<BRK>This paper adds a very interesting and useful feature to existing autodifferentiation for training neural networks. The second order information can be backprogated just as the first order ones, which can be used to accelerate training. This idea, although according to the paper, is developed upon existing works, still, strongly attracts as the second order information is crucial for training and perhaps visualizing the landscape of neural networks. I vote for an acceptance as this brings a significantly important feature to PyTorch, and the author s good experiments results and open sourced code.<BRK>Yet, other quantities such as the variance of the mini-batch gradients or many approximations to the Hessian can, in theory, be computed efficiently, and at the same time as the gradient.
Reject. rating score: 1. rating score: 3. rating score: 8. This paper explores methods to incorporate a pretrained language model into a dialog system. In terms of experiment results, the authors show that their approach improves over the basic GPT 2 and is competitive with baseline methods that rely on more supervision.A few clarification questions regarding the experiments:  Did the authors tune the GPT 2 model on each dataset, similar to ARDM as well? Or are the GPT 2 results shown in Table 1 after fine tuning? I think the writing of the model section could be improved.<BRK>Empirical contribution:My main concern with the paper is a lack of ablation, which makes it hard to understand where the improvement comes from. Considering that both TransferTransfo and ARDM are based on transformer and GPT 2, and are apparently fine tuned on the same data, I find it strange the gap between the two is so big on perplexity (10.1 vs TransferTransfo’s 19.9) while TransferTransfo is actually superior on BLEU. The second contribution is an alternating parameterization of the model that distinguishes between agent and user utterances. The paper is concerned with reducing the amount of manual dialog state or dialog act labeling in task oriented dialog, following the lead of Eric and Manning (2017) who did not require any such explicit annotation.<BRK>This paper proposes a pre trained language model architecture specifically used for task oriented dialogue systems. The basic idea is to alternate between the likelihood of two parties in a dialogue. Experiments are done throughly by comparing to BERT and GPT 2, which reflected the cutting edge research in pre trained language model.<BRK>ARDM models each speaker separately and takes advantage of the large pre-trained language model. It requires no supervision from human annotations such as belief states or dialog acts to achieve effective conversations. ARDM outperforms or is on par with state-of-the-art methods on two popular task-oriented dialog datasets: CamRest676 and MultiWOZ.
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. If this is correct, I would recommend making that intuition more clear early on in the paper, in order to improve upon the clarity of the paper. Minor comments and questions for the author:  I am slightly confused by the introduction of the rtop operator.<BRK>That said, there is quite a bit of room for improvement in terms of the writing of the paper. The paper appears to have way too many typos.<BRK>Numerical experiments show some advantage compared to SpiderBoost on some deep neural network architecture for some standard datasets MNIST, SVHN, and CIFAR 10. All of the experiments in this paper are focusing on deep learning problems.<BRK>Compared to SGD, these methods require at least double the number of operations per update to model parameters.
Reject. rating score: 1. rating score: 1. rating score: 3. Citations are misplaced (breaking the sentences, unclear when the paper of the authors are cited). The statement needs to be made much less vague. missing commas, which makes some sentences hard to follow. There are many (!) The authors evaluate their approximate bound and "best bound possible" empirically on MNIST and CIFAR. related work, and expand their theoretical and/or empirical results to produce a contribution of sufficient novelty/impact. Essentially, so what?<BRK>Also, because (4) is an approximation, it s a little disengenuous to call Theorems 4.2 and 5.2 "theorems", and it needs to be mentioned in the statements that they hold under some formalization of the approximation I described above. (2) Proves a lower bound on the PAC Bayes generalization objective. Evaluation: I found this paper extremely difficult to follow because it s sloppy in various places   both in terms of what claims are formal, and what are heuristic approximations   and in terms of properly defining crucial quantities.<BRK>This paper propose a second order approximation to the empirical loss in the PAC Bayes bound of random neural networks. * The theorems are easy algebras and better not presented as theorems. But the presentation in the paper could better be improved by explaining why doing this.<BRK>They conduct a theoretical analysis that links the random initialization, minimum, and curvature at the minimum of a deep neural network to limits on what is provable about generalization through PAC-Bayes.
Reject. rating score: 1. rating score: 3. rating score: 3. This will likely not generalize to new edges* Eq.6: What is the motivation for adding a vector of ones? There are many interesting ideas in this paper and I believe that the overall approach/idea has merit and is interesting for the community. This feedback is mostly meant as an encouragement to “polish” the ideas outlined in the paper and their presentation, as I believe that these can be quite impactful, if presented well and once all technical issues have been addressed. A derivation would be helpful.<BRK>The idea is new (to my understanding) and interesting. On the other hand, the main concerns are:*Only a rough idea is introduced and discussed, then followed by some nice practical results. In the middle, there is lacking of concrete execution of the proposed idea. The devil could lie in the details. *many details are missing. Throughout the paper, it is not clear at all how the message functions and the inference procedure functions (or called reasoning agents) are realized in practice.<BRK>This paper proposes Policy Message Passing, a Bayesian GNN which models edge message passing as a mixture distribution with corresponding coefficient generated by a learnable prior defined on graph state. But the writing and other experiments can be further improved, as detailed below. Sec 3.1: What does ```````+1 mean in Eq.(6)?Sec 3.2: The choice for the variational distribution q is not mentioned. I think this paper should discuss the connections to previous Bayesian GNNs and compare their performance with PMP at least in Sec 4.2. It seems that the number of parameters of PMP is much larger than GCN and GAT. Could you provide details for the model size and the training/inference time cost?<BRK>In this paper, they present the Policy Message Passing algorithm, which takes a probabilistic perspective and reformulates the whole information aggregation as stochastic sequential processes. They apply their algorithm to multiple complex graph reasoning and prediction tasks and show that their algorithm consistently outperforms state-of-the-art graph-structured models by a significant margin.
Reject. rating score: 3. rating score: 3. rating score: 6. Strengths of the paper:  The paper is well written and easy to follow.<BRK>T   0.2 shows the best performance on Fig.2 (b), but its seems sensitive, and generality of this setting is not clear.<BRK>The paper is well written with clear descriptions, fairly comprehensive analysis and empirical exploration, and good results, and in general I agree that learning quantization so as to minimize quantization related errors on task at hand is a good strategy. If this was considered please comment on reasons for not including / discussing this in the paper, otherwise perhaps this’ll be good to discuss.<BRK>However, in most traditional quantization works, the objective is to minimize the reconstruction error for datapoints to be searched. In this work, they focus directly on minimizing error in inner product approximation and derive a new class of quantization loss functions. They conduct experiments on public benchmarking datasets \url {http: //ann-benchmarks.com} to demonstrate that their method using the new objective outperforms other state-of-the-art methods.
Accept (Spotlight). rating score: 8. rating score: 6. rating score: 3. The distribution of singular values in the layer to layer Jacobian matrices for pruned networks becomes increasingly pathological as the depth increases. This observation motivates the concept of  layerwise dynamical isometry  (LDI), a slight generalization of the concept of  dynamical isometry  that has been studied in prior work. As such, I found the contributions of this paper to be novel and believe the results will be of interest to practitioners and theorists alike. An important contribution of this paper is in identifying that a main difficulty in pruning networks at initialization comes from degradation of signal propagation, leading to poor or impossible training. The numerous well thought out experiments provide compelling evidence that the trainability of pruned networks is highly correlated with spectral measures of the networks  Jacobians.<BRK>In this paper, the authors studied and formalized the effect of initialization to connection sensitivity based pruning. The authors first pointed out that a previously studied pruning criterion   connection sensitivity (CS)   is a normalized magnitude of gradients. Based on signal propagation theory, to achieve a  faithful  (with minimal amplification) CS, the gradients must be also faithful. Then by using relation of Jacobians and gradient, the authors proved that orthogonally initial weights guarantees faithful on linear networks and certain distribution property on nonlinear network can achieve layerwise dynamic isometry, which is to ensure faithful signal propagation. Based on these findings, the authors proposed an initialization setup for improving pruning performance, with the goal to ensure dynamic isometry by orthogonal initialization and approximation. How is training performance when relu is used since relu is more commonly used in modern deep architectures.<BRK>The paper introduces a signal propagation perspective for single shot network pruning. Particularly, by achieving layerwise dynamical isometry with proper initialization scheme, the paper ensure the reliability of connection sensitivity pruning criteria, thereby improving the pruning results. The main argument in the paper of dynamical isometry doesn’t has a direct connection with performance of a pruned network since it basically concerns the trainability of very deep neural networks. In terms of potential improvements, I would suggest the authors to first carefully study the relationship between trainability and generalization and also analyze signal propagation of networks with batch norm. Finally, for such an empirical paper, I expect more large scale experiments conducted.<BRK>A typical approach to pruning starts by training a model and then removing redundant parameters while minimizing the impact on what is learned. However, it remains unclear exactly why pruning an untrained, randomly initialized neural network is effective. Moreover, they analyze the signal propagation properties of the resulting pruned networks and introduce a simple, data-free method to improve their trainability. Their modifications to the existing pruning at initialization method lead to improved results on all tested network models for image classification tasks. Furthermore, they empirically study the effect of supervision for pruning and demonstrate that their signal propagation perspective, combined with unsupervised pruning, can be useful in various scenarios where pruning is applied to non-standard arbitrarily-designed architectures.
Reject. rating score: 1. rating score: 1. rating score: 3. The paper deals with the understanding of deep learning under a physical point of view, related to entanglement entropy. As far as i understand the paper explains how the computation of the entanglement entropy may be performed and how this measure may be used to design the neural network architecture. The paper cites papers from physical letters and physical reviews without introducing all necessary background. From this point of view the paper is not self content enough for the audience of the conference. In addition to the fundamental contribution that i could not summarize well, the paper includes an experimental validation of the contribution on the question answering problem on benchmark datasets, showing very relevant results outperforming the baselines, which look like state of the art methods in the field. But is it a fair comparison with the baselines ?<BRK>Specifically, the paper aims at solving two problems: 1) to study the theoretical analysis of entanglement entropy for the matching of two objects (question answering pairs), and (2) to qualitatively calculate the matching matrix. The main goal of the paper is to show that a low dimensional attention matrix can be derived from a high dimensional matching matrix. Overall it is not clear what the actual contribution of this work really is. The paper is not well written and difficult to follow. It would help to clearly state what the contribution of this work is, instead of repeating sentences.<BRK>or any of the subsequent improvements to it. This paper would benefit from a clearer exposition minus the quantum mechanics jargon and from experiments on the above stated benchmarks to be more convincing. I am not an expert on Quantum physics, hence I am unable to judge the merits of the quantum entanglement approach proposed in the paper. Besides, this approach should also work for SQuAD, GLUE, SuperGLUE and all the other established NLP benchmarks that benefit from improved language modeling capabilities.<BRK>The formal understanding of deep learning has made great progress based on quantum many-body physics. For example, the entanglement entropy in quantum many-body systems can interpret the inductive bias of neural network and then guide the design of network structure and parameters for certain tasks. However, there are two unsolved problems in the current study of entanglement entropy, which limits its application potential. In this paper, they are trying to address these two problem by investigating the fundamental connections between the entanglement entropy and the attention matrix.
Reject. rating score: 3. rating score: 3. rating score: 6. The authors propose an algorithm called differentially private adversarial learning (DPAL) to achieve such goal. To apply functional mechanism, the authors use the 1st order polynomial approximation of both objective functions (by Taylor Expansion). Overall, this paper studies an interesting setting when privacy guarantee and adversarial robustness are both needed in machine learning model and proposes an algorithm to achieve such goal. The paper is well organized. 1.The approximation is necessary to derive the privacy guarantee for the objective function. But the paper does not provide either theoretical analysis on the approximation error or empirical result of the distance between the approximate and real function values. Another way is to use the approximate loss as the objective function to train the models and see how the model performance changes.<BRK>This paper propose an algorithm with DP preservation to train adversarially robust neural networks. Certified robustness of the smoothed classifier is also given, which depends on the privacy budget of each compositing mechanism. However, the current version is quite difficult to follow for people without DP background, with some settings even conflict with other papers on adversarial robustness, and I do have some doubts about the experimental results. Perhaps I missed something, but the authors have not shown how it is related to certified robustness in the paper. This distinction should be addressed, since only one model can be chosen at deployment.<BRK>In this submission, the authors address a challenging task, in which the goals are three fold: 1) preserve the privacy of training data in terms of DP; 2) both provably and practically robust to adversarial examples; and 3) maintain high model utility. The authors demonstrate the advantage of the proposed method both theoretically and experimentally. Overall, this is a solid work. However, I have the following concerns. If the authors can clarify them during the rebuttal, I am willing to increase my review score. 1) On page 9, the authors mentioned several existing DP preserving algorithms with provable robustness. 2) How about the scalability of the proposed method when applying to real world scenarios? In the experiment part, the authors show the results on MNIST and CIFAR 10; however, the scale of these two datasets are kind of small compared to real world applications. Can the authors experimentally show the scalability of the proposed method by increasing the number of samples? 3) This question will not be counted towards the review: Do the authors try other datasets? MNIST and CIFAR 10 are relatively simple image datasets, how about real images? But the main results are actually in appendix, and the reviewers are forced to read more than 10 pages.<BRK>In this paper, they aim to develop a novel mechanism to preserve differential privacy (DP) in adversarial learning for deep neural networks, with provable robustness to adversarial examples. They leverage the sequential composition theory in DP, to establish a new connection between DP preservation and provable robustness. To address the trade-off among model utility, privacy loss, and robustness, they design an original, differentially private, adversarial objective function, based on the post-processing property in DP, to tighten the sensitivity of their model. An end-to-end theoretical analysis and thorough evaluations show that their mechanism notably improves the robustness of DP deep neural networks.
Reject. rating score: 1. rating score: 1. rating score: 3. This is not true,there exist online approaches such as https://arxiv.org/abs/1706.09563or distributed methods https://arxiv.org/abs/1901.09235. Finally the latex formatting suffers from many issues and typos.<BRK>This paper presents a single image super resolution method. The only slight variation is that the activation function ReLU for the residual layers are arranged before the convolutional layers.<BRK>[Strengths]  This paper is well written and easy to follow. It seems CISTA may be the main novelty of this work.<BRK>In this paper, they exploit the natural connection between CSC and Convolutional Neural Networks (CNN) to address CSC based image SR. Specifically, Convolutional Iterative Soft Thresholding Algorithm (CISTA) is introduced to solve CSC problem and it can be implemented using CNN architectures.
Reject. rating score: 3. rating score: 3. rating score: 3. The paper proposes a framework for learning with rejection using ideas from adversarial examples. So, the algorithm can be simply summarized as,1. Having a close adversarial example is same as saying that the current point is very close to the decision boundary. 2.The proposed approach is not novel. For example, [1] uses adversarial example style detection to augment their training data and improve their end to end model. 3.There have been approaches which attempt to learn rejection function [2], so it would have been good to at least do a comparison of the proposed approach with such methods. I have raised my scores, but I still believe that this paper falls short of acceptance.<BRK>This paper wants to study the problem of “learning with rejection under adversarial attacks”. Necessary references are needed for these places. Both the problem setting and the technique does not have novelty. The paper also has problems in writing. Finally, there is no comparison with any baseline. Only empirical results of the proposed methods are shown. Due to all these reasons, there is still a long way to go before the paper can be published. In the last equation of Page 3, there is no definition of \tilde L. Actually, according to Figure 1, x’s is more close to the decision boundary, it is an example more hard to classify, which could also be “suspicious”. In the definition of “suspicious example” at the beginning of Sec.3.1, is both x and x’ defined as suspicious examples in this way? In the last equation of page 2, there is a rejection function, so minimizing this loss is a “separation based approach”. Any comment on the inconsistency? The motivated problem is not new. The current paper has not discussed any related work of cost sensitive learning although they want to study a problem in its field.<BRK>There is still no universal method to deal with adversarial examples, and introducing a reject option to flag potential attacks seems a sensitive choice for many applications. However, I think there are different dimensions along which the paper could be improved:  My understanding of classification with a reject option is that the rejection cost c(x) is a design choice that can depend on the specific application. While c(x) is introduced as part of the framework it is then derived in a very specific way, removing the design aspect or at least not explaining very clearly how one would design it. The authors state that they use a 30 step of the PGD algorithm to find z^*. How sensitive is the rejection function to different initializations? I think it is important to investigate this aspect. In Tables 1 and 2, I don’t understand why the precision is the same for all rows corresponding to the same attack (strength), while the other metrics vary. Overall, I think the paper explores an interesting direction, but would greatly benefit from a revision along the lines outlined above. ###Reply to rebuttal:I thank the reviewers for their detailed reply.<BRK>Recent developments in machine learning have opened new opportunities for industrial innovations such as self-driving cars. However, many machine learning models are vulnerable to adversarial attacks and industrial practitioners are concerned about accidents arising from misclassification. To avoid critical misclassifications, they define a sample that is likely to be mislabeled as a suspicious sample. Their main idea is to apply a framework of learning with rejection and adversarial examples to assist in the decision making for such suspicious samples. They demonstrate the effectiveness of the proposed method in experiments.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. The proposed method is a modification of the local SGD which updates models distributed to several workers in a parallel way and synchronize the model parameters at every few epochs. The proposed method is simple and easy to implement. The authors conducted thorough experiments to investigate the performance of the proposed method.<BRK>This paper proposes a variant of local SGD, post local SGD, for distributed training of deep neural networks. The authors are encouraged to report experimental results on distributed training of large LM models. The idea is straightforward and easy to understand  start the training with standard mini batch SGD and later switch to local SGD.<BRK>In this paper, the authors propose a variant of local SGD: post local SGD, which improves the generalization performance compared to large batch SGD. However, the authors claim that the learning rates for mini batch SGD are fine tuned (in Figure 3), which makes the empirical results questionable. Thus, the contribution of this paper is limited. The post local SGD is a simple extension of local SGD.<BRK>Mini-batch stochastic gradient methods (SGD) are state of the art for distributed training of deep neural networks. As a remedy, they propose a \emph {post-local} SGD and show that it significantly improves the generalization performance compared to large-batch training on standard benchmarks while enjoying the same efficiency (time-to-accuracy) and scalability.
Reject. rating score: 1. rating score: 3. rating score: 3. This paper is motivated by the unstable performance of Transformer in reinforcement learning, and tried several variants of Transformer to see whether some of them can stabilize the Transformer. The experimental results look good, however, I have problems in understanding the motivation, the intuition of the proposed methods, the experimental design, and the general implication to the research community that is using the Transformer in their day to day research. For another example, why replacing the residual connection with the gating layer can make the Transformer more stable?<BRK>Still, I think the paper is well written and could be improved upon. Their results show that transformers are able to learn in memory intensive environments, with some gating combinations surpassing LSTM. The hyperparameter studies on Memory Maze also show improvements in memory related tasks, but do not help in understanding of the proposed work.<BRK>The paper explores a transformer for reinforcement learning. The authors demonstrate that Canonical Transformer is unstable. The authors introduce two modifications to the Canonical Transformer. The authors have evaluated the overall performance, as well as hyperparameters, seeds, and ablations. This could be an interesting finding. Which modification contributes more? (4) Have you experimentally validated the proposed hypothesis as to why the Identity Map Reordering, such as recording the evolution of the produced values in the submodules?<BRK>Harnessing the transformer's ability to process long time horizons of information could provide a similar performance boost in partially-observable reinforcement learning (RL) domains, but the large-scale transformers used in NLP have yet to be successfully applied to the RL setting. In this work they demonstrate that the standard transformer architecture is difficult to optimize, which was previously observed in the supervised learning setting but becomes especially pronounced with RL objectives. They propose architectural modifications that substantially improve the stability and learning speed of the original Transformer and XL variant.
Reject. rating score: 6. rating score: 6. rating score: 6. This paper presents a detailed replication study of the BERT pre training model considering alternative design choices such as dynamic masking, removal of next sentence prediction loss, longer training time, larger batch sizes, additional training data, training on longer single document or cross document sequences etc. to demonstrate their efficacy on several benchmark datasets and tasks by achieving the new state of the art results. However, I am not sure if the paper presents a case of adequate novelty in terms of ideas as many of them are rather obvious and the current state of the art models could also improve considerably using similar experimental setups, which authors also acknowledged in footnote 2.<BRK>This paper presents a replication study of BERT pretraining and carefully measures the impact of many key hyperparameters and training data size. It shows that BERT was significantly undertrained and propose an improved training recipe called RoBERTa. The proposed RoBERTa achieves/matches state of the art performance on many standard NLU downstream tasks. However, the BERT analysis results provided in this paper should also be valuable to the community.<BRK>This paper is a replication study of BERT for training large language models. Pros:+ The paper incorporates robust optimization into BERT training with more data, and shows that together it significantly improves BERT s performance on downstream tasks. Cons:  While the replication study is well appreciated, the novelty contribution of the paper is marginally incremental as the model structure is largely unchanged from BERT. The other techniques applied also are somewhat trivial. Very little can be deduced from the experiments, as performance is often improved by training over more data. Overall, I believe this paper comes at the right time and is addressing an interesting problem.<BRK>They present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. They find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Their best model achieves state-of-the-art results on GLUE, RACE, SQuAD, SuperGLUE and XNLI. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements.
Accept (Spotlight). rating score: 6. rating score: 6. rating score: 6. The paper presents a new graph representation learning method for the whole graph under both unsupervised and semi supervised setting. Different from existing ones using graph kernel, or graph2vec, the proposed InfoGraph is able to extract graph level representation with fixed length features that are generalized well. Experiments on both unsupervised and semi supervised experiments on popular benchmarks demonstrate the effectiveness of InfoGraph and InfoGraph**  The paper is well written and easy to follow, and the research problem is of great value in different fields. This may undermine the novelty of this paper somehow.<BRK>In this paper, the authors propose a graph level representation, which extends the existing node level representation learning mechanism. Besides, both unsupervised and semi supervised learning are leveraged for InfoGraph and InfoGraph*, receptively. The authors naturally apply Deep Graph Infomax, a contrastive representation learning method, for the whole graph level instead of the previous node embedding learning. The learned graph level representation looks good to me. Overall speaking, the paper is well written.<BRK>The paper presents an unsupervised method for graph embedding. The authors seek to obtain graph representations by maximizing the mutual information between graph level and patch levelrepresentations. They also consider a semi supervised task when the Mutual Information based criterion has an additional term which quantifies a classification error, obtained when constructing a classifier based on the obtained graph representations. The differences, listed by the authors, are only of technical nature. There is no comparison with GNN models such as [2].<BRK>This paper studies learning the representations of whole graphs in both unsupervised and semi-supervised scenarios. Experimental results on the tasks of graph classification and molecular property prediction show that InfoGraph is superior to state-of-the-art baselines and InfoGraph * can achieve performance competitive with state-of-the-art semi-supervised models. Inspired by recent progress of unsupervised representation learning, in this paper they proposed a novel method called InfoGraph for learning graph-level representations. By doing so, the graph-level representations encode aspects of the data that are shared across different scales of substructures.
Reject. rating score: 6. rating score: 6. rating score: 8. The paper is well motivated and the experiments (although I have some reservations about the setup) demonstrate efficacy of the proposed method. Review: > IntroductionI do not agree with the statement that value function based algorithms are restricted to discrete action domains, especially when you rely on  “ignoring function approximation errors” for some of your claims. Again in switching from value function to PG is true for traditional RL/Control theory but this is not valid here. ( your methods rely on Q(s, a) which is action value function, or the constraint in equation 3 is integral of Q over all actions which would be value function in traditional definition )Note: this is explained very well towards the end in the Appendix B, but this is a review of the paper and not Appendix B or C.  > Section 2section 2.3 I would strongly advise the authors to rewrite this, this section reads like it was copied as is from the reference [Chow et al 2018]. especially the way Lyapunov function is defined. It is not clear to me how the feasibility of initial pi_0 is ensured ? → Section 3Section 3 is pleasant to read and very easy to understand, however, same cannot be said of thesection 3.1. Again PPO is more heuristic than TRPO which makes it hard to compare like for like. PPO might give higher rewards but constraint violations may increase as well. Figure 6 Can you be more specific as what the figure 6 is showing ? What do you mean by actions are velocity ? Minor points (Language, Typos):page 3, last paragraph,  Chow et al.is repeated, I can see why this happens there but suggest editing to avoid this. [This is also in intro paragraph, there it is just a typo and should be rectified]Figure 6: Captions labels are incorrect.<BRK>This is a very complete submission. There is a novel analysis,simulations, as well as some results on real data. The authors proposeLyapunov based safe RL algorithms that can handleproblems with large or infinite action spaces, and return safepolicies both during training and atconvergence. I was impressed with themethod and the analysis behind the method. The incorporation of theLyapunov idea from control theory makes a great deal of sense in thisapplication. However, it is not trivial to get from using this tool toa working method.<BRK>The paper presents a technique for extending existing reinforcement learning algorithms to ensure safety of generated trajectories in terms of not violating given constraints. I have very little knowledge of this area and as a result was not able to evaluate the paper thoroughly. However, the problem addressed is certainly a very important one and based on my high level understanding of the concepts involved the approach seems sensible. The experiments are clear and well designed, showing the trade off between performance and safety.<BRK>They study continuous action reinforcement learning problems in which it is crucial that the agent interacts with the environment only through safe policies, i.e., ~policies that keep the agent in desirable situations, both during training and at convergence. Their algorithms can use any standard policy gradient (PG) method, such as deep deterministic policy gradient (DDPG) or proximal policy optimization (PPO), to train a neural network policy, while guaranteeing near-constraint satisfaction for every policy update by projecting either the policy parameter or the selected action onto the set of feasible solutions induced by the state-dependent linearized Lyapunov constraints. Compared to the existing constrained PG algorithms, their are more data efficient as they are able to utilize both on-policy and off-policy data. Moreover, their action-projection algorithm often leads to less conservative policy updates and allows for natural integration into an end-to-end PG training pipeline. They evaluate their algorithms and compare them with the state-of-the-art baselines on several simulated (MuJoCo) tasks, as well as a real-world robot obstacle-avoidance problem, demonstrating their effectiveness in terms of balancing performance and constraint satisfaction.
Reject. rating score: 1. rating score: 3. rating score: 8. Reviewer has concern about the scope and application value of the problem as a research paper. The problem is separated into two steps. 2) Since the rendering process could be costly, it s useful to discuss the speed of convergence in the attribute value adjustment iterations. While this is indeed the majority, this prior knowledge need to be speciifed clearly by saying it s tailored towards such applications (or use more examples to proof that s not the case).<BRK>How about a direct comparison to some of the work listed in the second para on page 2? Novel elements include the attribute refinement using imitation learning, and the authors show the effect of this step, but the improvement is small. Thus, the limited novelty and not very convincing results make the question the potential impact of this paper. None of them seem to be from recent prior work.<BRK>This is an important information, but I couldn t find it in the paper. Typically, I was surprised to see in the first row of Table 2 that the model with a random attribute initialization can reach such a high performance. The model is trained on synthetic datasets generated by a black box rendering engine, and generalizes well to real world datasets. Overall, the method is sensible and elegant, and could easily be applied to other domains.<BRK>To achieve this, they take a black box rendering engine and a set of attributes it supports (e.g., colors, border radius, shadow or text properties), use it to generate a suitable synthetic training dataset, and then train specialized neural models to predict each of the attribute values. To improve pixel-level accuracy, they also use imitation learning to train a neural policy that refines the predicted attribute values by learning to compute the similarity of the original and rendered images in their attribute space, rather than based on the difference of pixel values.
Reject. rating score: 3. rating score: 3. rating score: 6. 2.Experimental results are not convincing: in the paper, only grayscale datasets, such as MNIST and FMNIST, are considered to evaluate the proposed method and I think it is not enough. Moreover, I think the authors should evaluate their methods on more realistic cases. In ICLR, 2019. [Lee  18] Lee, K., Lee, H., Lee, K. and Shin, J., Training confidence calibrated classifiers for detecting out of distribution samples.<BRK>I suggest the authors to do more experiments on larger datasets to avoid this criticism. 1.Though the analysis is interesting, it is not applicable to both benchmark datasets and real world cases. > If you are talking about the comparison in MNIST variants, please note that experimental results on MNIST cannot be seriously taken unless there is a strong theoretical background; especially, MNIST variants are too small to talk about the scalability of the method. I think Appendix E in the Lee et al.s paper corresponds to this reasoning, but Lee et al.actually didn t generate OOD samples but simply optimized the confidence loss with a "seen OOD."<BRK>The paper proposes an algorithm to generate boundary OOD positive/negative samples to train a classifier for OOD samples. Experiments are conducted on MNIST and Fashon MNIST datasets, which are used as OOD and in distribution, and vice versa. Overall the paper is well written and well organized. The proposed method is based on the idea from theoretical analysis, and is reasonable and valid. However, I like the method and could be accepted as an ICLR paper.<BRK>Therefore, detecting out-of-distribution (OOD) samples is very important to avoid classification errors. In the context of OOD detection for image classification, one of the recent approaches proposes training a classifier called “ confident-classifier ” by minimizing the standard cross-entropy loss on in-distribution samples and minimizing the KLdivergence between the predictive distribution of OOD samples in the low-density “ boundary ” of in-distribution and the uniform distribution (maximizing the entropy of the outputs). Thus, the samples could be detected as OOD if they have low confidence or high entropy. Overall the proposed approach consistently performs better than others across most of the experiments.
Reject. rating score: 3. rating score: 3. rating score: 6. Summary:   The authors consider a transfer learning problem where the source distribution is P(X,Y) while the target distribution is P*(X,Y) and classifier is trained on data from the source distribution. Does it mean samples from two different domains ?? This is used to prove that the causal ones have tighter differential privacy guarantees than the associative ones c) Using the differential privacy results, they also show that optimal causal classifiers are more resistant to membership attacks. Does it mean you add noise to the model parameters ?? This is confusing at best. This connection is very different from the scope of the current paper. However, the statement by the authors is strictly not true. Consider the following counter example   Suppose all the features are in the Markov Blanket of Y (even a simpler case where all features are causal parents of Y). Suppose the true P (Y|X) is a logistic model with weights w_1 on one part of the domain D_1 and with weights w_2 for another part of the domain D_2. relevant when clearly you have distribution P (X,Y) and P*(X,Y) ?? Authors could clarify. and would be causal. That brings into question the validity of many (if not all) the theoretical results in the paper. Authors must address this. 2.Issues regarding definition of certain quantities.<BRK>The paper proposes using causal learning models for alleviating privacy attacks, i.e.membership inference attacks. The paper proves that causal models trained on sufficiently large samples are robust to membership inference attacks; they confirm the theories with experiments on 4 synthetic data. The main concern of this paper is the results are only confirmed on synthetic data, where all the 4 datasets are generated from known Bayesian networks (i.e., causal graphs). Another question is about the ‘causal models are known to be invariant to the training distribution and hence generalize well to shifts between samples from the same distribution and across different distributions.’  More explanations about ‘invariance’ is needed. For example, in Figure 2a and Figure 3a, causal models have similar performance (except Alarm data) with DNN models on test 2, where test samples are generated from different distributions than training samples.<BRK>Overview: This paper discusses the risk of membership inference attacks that deep neural networks might face when used in a practical manner on real world datasets. The authors demonstrate how attack accuracy goes up when one dataset is used for training while another altogether is used for testing. They propose the use of causal learning approaches in order to negate risk of membership inference attacks. Causal models can handle distribution shifts across datasets because they learn using a causal structure. You mentioned that the datasets used in the experimental section were used in order to avoid errors in learning causal structure. Your experimental results suggest that the causal model can learn on smaller amounts of data than the DNN. Are these out of the box solutions from libraries, or something more custom built? I give this paper a borderline acceptance, based upon the fact that the above questions need to be addressed.<BRK>Machine learning models, especially deep neural networks have been shown to reveal membership information of inputs in the training data. Their results confirm the value of the generalizability of causal models in reducing susceptibility to privacy attacks. Further, they show that the attack accuracy amplifies when the model is used to predict samples that come from a different distribution than the training set, which is often the case in real world applications. Causal models are known to be invariant to the training distribution and hence generalize well to shifts between samples from the same distribution and across different distributions.
Reject. rating score: 1. rating score: 3. rating score: 6. The paper addresses the problem of using multiple modalities for learning from demonstration. It would also be interesting to see an ablation study in the sense of replacing or removing aspects of the architecture to see its relative effect on the overall model performance. Doing the same with multiple modalities involved, in particular vision, language and motion, has only been recently considered, so this is a timely paper. The ablation study is setup somewhat differently than what I would have expected.<BRK>*Summary The paper describes a new end to end imitation learning method combining language, vision, and motion. The experiments demonstrate the generalization performance of the method. The reason is as follows. Related to this point, the problem was not identified in the Introduction. The authors might assume that introducing language into behavioral cloning itself is qualitatively new work.<BRK>The work focuses on a robotic pick and place task, where the instruction indicates which of the available bins an item should be placed in. This approach would have advantages and disadvantages. It also means that the network outputs a distinct goal configuration, which the DMP should reach (assuming the goal is feasible) regardless of the other motion parameters. It is likely that the approach presented in this work is better suited to the specific problem of robot control, but it would be helpful to see if learning a low level control policy directly can be successful in this context. The paper suggests that this data was not used directly to train the model, but was instead used to build a template for generating natural language instructions.<BRK>In this work they propose a novel end-to-end imitation learning approach which combines natural language, vision, and motion information to produce an abstract representation of a task, which in turn can be used to synthesize specific motion controllers at run-time.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 8. This paper suggests using the activation function k Winners Take All (k WTA) in deep neural networks to enhance the performance of adversarial defense. Their experiments show that the robustness is improved when they simply change the activation function to k WTA. They also give reasonable theoretical analysis for their approach. I find that the idea is simple and elegant. Their experiments show that the robust accuracies are significantly improved on all evaluated methods when they use the k WTA activation function. They show that k WTA makes the network very discontinuous with respect to the input x, and thus the adversary could not get useful gradient information. This is why the network is still trainable though itself is not continuous. The paper is also well written and easy to follow. I recommend the acceptance of the paper.<BRK>This paper addresses the important question of improving the robustness of deep neural networks against adversarial attacks. The presented effect is backed up by extensive theoretical investigations that relate the increased robustness to the dense introduction of discontinuities, which makes gradient based adversarial attacks harder. Empirical evaluations in CIFAR and SVHN for a variety of attacks and defense mechanisms demonstrate the desired effects, and illustrate the loss landscapes due to using k WTA. There is code available, and the idea should be simple to implement in practice, so I would expect this paper to have large impact on the study of adversarial robustness. 3.Since small changes have a big effect in k WTA, it should be investigated how robust the k WTA networks are with respect to more natural perturbations, e.g.noisy input, blurring, translations, rotations, occlusions, etc.<BRK>A k WTA activation functions outputs the k highest activations in a layer while setting all other activations to zero. The reasoning given by the authors is that k WTA activation functions have many discontinuities with respect to the input space. This makes it more difficult for attacks to use gradient information. They use the CIFAR10 and SVNH datasets. ** After Author Response **Changing from weak accept to acceptThe authors have addressed my concerns and I believe the paper can provide significant value to those interested in adversarial robustness.<BRK>They propose a simple change to existing neural network structures for better defending against gradient-based adversarial attacks. Instead of using popular activation functions (such as ReLU), they advocate the use of k-Winners-Take-All (k-WTA) activation, a C0 discontinuous function that purposely invalidates the neural network model ’ s gradient at densely distributed input data points. The proposed k-WTA activation can be readily used in nearly all existing networks and training methods with no significant overhead. This understanding is also empirically backed. They test k-WTA activation on various network structures optimized by a training method, be it adversarial training or not.
Reject. rating score: 3. rating score: 3. rating score: 3. The paper introduces the high variance policies challenge in domain randomization for reinforcement learning. Should minimizing the maximum difference be more proper? Overall, the paper is well written and the ideas are novel. I will consider raising my score according to the rebuttal. The authors have addressed some of my concerns but why minimizing the expected difference is not convincing. I think the paper should receive a borderline score between 3 and 6.<BRK>Summary:To improve the generalization ability of deep RL agents across the tasks with different visual patterns, this paper proposed a simple regularization technique for domain randomization. The authors showed that the proposed method can be useful to improve the generalization ability using CartPole and Car Racing environments. Detailed comments:I d like to recommend "weak reject" due to the following reasons:1. Even though this paper provides more justification and analysis for this part (Proposition 1 in the draft), the contributions are not enough as the ICLR publications. 3.For domain randomization, it has been observed that finding a good distribution of simulation parameters is a key component [Ramos  19, Mozifian  19, Chebotar  19], but the authors did not consider training the distribution of simulation parameters in the paper.<BRK>That figure shows the proposed regularization slightly hinders the performance for the environments near l 1, g 50 (that region is a darker shade of green on the left subfigure). * The title, introduction and conclusions do not reflect the scope of the paper. I recommend this paper to not be accepted until the following issues are addressed.<BRK>Producing agents that can generalize to a wide range of environments is a significant challenge in reinforcement learning. One method for overcoming this issue is domain randomization, whereby at the start of each training episode some parameters of the environment are randomized so that the agent is exposed to many possible variations. They conduct experiments that demonstrate that their technique leads to more efficient and robust learning than standard domain randomization, while achieving equal generalization scores.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 6. This paper proposes an operation for removing the pixel wise and channel wise correlations of input features. In the experimental analysis, the authors demonstrate a good performance of the proposed method compared with batch normalization. Also, the authors provide the CPU time of the proposed method, which is very appreciated. Overall, this paper has reasonable contributions to the learning algorithm of the deep neural network, so I recommend the AC to accept this paper.<BRK>This paper proposes "network deconvolution", a neural network primitive aimed at whitening the activations of each layer of the network. The method is a generalization of batch normalization that not only whitens per channel, but also removes correlations between channels and across spatial locations. Experiments show that the proposed methods improves training speed and predictive accuracy on a number of image classification models. I d like to see more discussion and experiments on:  What s the dependence on batch size? Does the method work better for larger batches? The only training curves shown in the main paper are on Fashion MNIST which I don t think is very interesting. How does this method interact with regularization methods? Does network deconvolution still improve final accuracy if the baseline uses more extensive data augmentation or other regularization?<BRK>This paper addresses the correlation present in the input data, which may affect learning kernels with redundancy. Therefore, they introduce  a deconvolution mechanism on the input features to remove spatial and channel wise correlation,  before these features are fed to network layers. They draw analogy to batch normalization, and show superiority in terms of  convergence and accuracy. However, it seems that the deconvolution to avoid correlation is helping for classification tasks, but may not be applicable for other related tasks such as semantic segmentation, where simply using BN may work. One of the questions that comes to mind is considering PCA normalization for the same purpose. Essentially, the step of computing the covariance is the same and decorrelating the data by  using the orthonormal basis vectors, pretty much with a similar motivation. I think this discussion should be added to the normalization and whitening section of related works. Minor note: The recommended page limit is 8 pages, the paper has now 10 pages.<BRK>Convolution is a central operation in Convolutional Neural Networks (CNNs), which applies a kernel to overlapping regions shifted across the image. Network deconvolution can be efficiently calculated at a fraction of the computational cost of a convolution layer. They also show that the deconvolution filters in the first layer of the network resemble the center-surround structure found in biological neurons in the visual regions of the brain. Filtering with such kernels results in a sparse representation, a desired property that has been missing in the training of neural networks. Learning from the sparse representation promotes faster convergence and superior results without the use of batch normalization.
Accept (Poster). rating score: 8. rating score: 8. rating score: 6. The paper proposed a novel sample efficient pretraining task. One inefficiency of BERT is that only 15% tokens are used for training in each example. This method looks as only adding the discrimination task after BERT pretraining task. All these observations are interesting. It will be helpful if the authors provide more empirical analysis why the adversarial ELECTRA perform worse or failed. Overall, I think this is a good paper. The studied problem is important, the idea is new and the experimental results are positive. But, it will be a big plus if the authors can show ELECTRA can outperform RoBERTa with the same amount of training time.<BRK>The principle advantage of the approach is that, in contrast with the standard masked language model (MLM) objective used by BERT and derivatives, there is a training signal for all tokens of the input (rather than a small fraction, when 10 20% of the input tokens are masked and then reconstructed under the MLM objective). ELECTRA matches the performance of RobBERTa on the popular GLUE NLP task, with just 1/4 of the training compute. Strengths: Simple but novel self supervised task for learning text representations, strong results, adequate ablation. Limitations: The authors limit their investigation of downstream performance to the GLUE set of tasks, which are classification tasks. This is a significant limitation of the current version of the paper, as it may be that replaced token detection is more suitable for these tasks, but inferior to MLM (a higher precision self supervised task) for more involved tasks like question answering.<BRK>Summary: Authors offer an alternative for masked LM pretraining that s more sample efficient called replaced token detection. Their method basically replaces certain input tokens with alternatives which are sampled from a generator and train a discriminative model to determine whether its generated or real. The work shows empirical success getting better results than GPT with a fraction of the compute on GLUE and others. Positives: Idea is simple and makes sense intuitively, but not something one would think immediately would work better with a such a small fraction of the compute. I think the formulations of the experiments and ideas to develop this are adequate. Whats the degradation in performance? I think the paper merits acceptance.<BRK>While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. Then, instead of training a model that predicts the original identities of the corrupted tokens, they train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the task is defined over all input tokens rather than just the small subset that was masked out. The gains are particularly strong for small models; for example, they train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Their approach also works well at scale, where it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute.
Reject. rating score: 1. rating score: 6. rating score: 6. This paper proposes to mix reinforcement learning and imitation learning to boost the learning of an actor critic architecture. The authors use a regularized reward function that minimizes the divergence between the policy of the expert and the one followed by the agent. I have many concerns about this paper. First, The state of the art is missing important pre deep learning references such as: 1. Also, the proposed solution here is equivalent to regularizing the MDP with a KL divergence  w.r.t.to an initial policy that would be the one of the expert. It is generally studied in 4. In addition to not be very novel, I think the method has some flaws. Especially if it comes from an RL agent using similar deep RL algorithms (which is the case here). I would be more impressed by experiments on stochastic environments and sparse rewards.<BRK>It combines both an augmented reward for minimizing the KL between the policy and the expert actions as well as directly minimizing that KL in the policy. The motivation for the paper is difficult to follow. The end of the related work section is not very clear, you say these methods are problematic because "the adopted shaping reward yields no direct dependence on the current policy" but there s no explanation or motivation for why that would be a problem. Assumption 1 seems like a very strong assumption that would not be true for many human experts. There was nothing specific in your algorithm that meant it should specifically address sparse reward tasks. The motivation and related work need to be made clearer to situate this work with the other related works. And the experiments should go beyond these tasks that have been modified to have sparse rewards.<BRK>Given a set of expert demonstrations, this work provides a policy dependent reward shaping objective thatcan utilize demonstration information and preserves policy optimality, policy improvement,and the convergence of policy iteration at the same time, under the assumptionthat expert policy is optimal and stochastic. The main advantage of the proposed method is that the reward shaping functionis related to the current policy. A practical algorithm based on theoretical derivation is provided. The authors conducted sufficient experiments to demonstratethe effectiveness of the proposed method, compared with the state of the art in RLfD. 1 can be contradictedwith that expert policy is optimal in policy invariance proof. This paper works on a problem of infinite horizon discounted MDP. It is not clear that in what type of MDPs the optimal stochastic policy exists andit can satisfy Asm. Since p(s) > 0 for all s, following strong stochasticity policy. The proof of Theorem 2 is similar to the proof in Proposition 1, [1], though in adifferent context. It would be better to have a citation?<BRK>They study the problem of \textit {Reinforcement learning from demonstrations (RLfD)}, where the learner is provided with both some expert demonstrations and reinforcement signals from the environment. Another approach uses demonstration data for reward shaping. But existing algorithms in the latter one adopt shaping reward which is not directly dependent on current policy, limiting the algorithms to treat demonstrated states the same as other states, failing to directly exploit supervision signal in demonstration data. In this paper, they propose a novel objective function with policy-dependent shaping reward, so as to get the best of both worlds. They present a convergence proof for policy iteration of the proposed objective, under the tabular setting.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. rating score: 6. This paper proposes an alternative loss function for classification models that they claim leads to improved adversarial robustness even under strong adaptive attacks. It also attempts to analyze how the softmax cross entropy loss discourages robustness by considering the problem in terms of local density in the pre logit feature space. Although as an emergency reviewer I haven t had time for a thorough verification, the overall idea seems sound, as do their theoretical and experimental results. I appreciate the careful analysis of the sample densities induced by each method; the N/L^2 vs. N/L result is especially nice.<BRK>The MMC loss is essentially minimizing the distance between the feature and the pre fixed class center. This acutally reminds me of a number of works in angular margin based softmax loss. This paper considers to minimize the maximum inner product. I appreciate the authors provide many theoretical justifications, which is inspiring. Intuitively speaking, I can understand that shrinking the feature space (i.e., make feature distribution more compact) can improve the adversarial robustness. As a result, I think this paper is naturally motivated and is also theoretically sound. The experiments can be further improved.<BRK>The paper compares between SCE loss,  large margin Gaussian Mixture (L GM) loss and proposes the Max Mahalanobis center (MMC) loss as an alternative to explicitly learn more structured representations and induce high density regions in the feature space. Overall the paper is well written, with sufficient theoretical reasoning and experiments. Also in practice it is hard to justify whether certain loss function really behaves like a Gaussian distribution, which makes the application of the theorem more limited. In fact, if the samples are concentrated (which can be common in practice), is the proposed method still able to induce high density sample region? Are the experiment results sensitive to the choice of parameters C_MM and L? I have updated my score.<BRK>To remedy this, the authors then proposes the max mahalanobis center (MMC) loss. Then when looking back at the sample density definition in eq.(2).I wonder how the Vol is defined to guarantee eq.2 is a valid density function? I find the paper fairly well written in general, and the arguments are well supported by the development of the theoretical results. In terms of experiments, since it is claimed that the proposed loss adds little computation cost compared to the SCE loss, I think it is better to include running time comparison in the results.<BRK>Since collecting new training data could be costly, they focus on better utilizing the given data by inducing the regions with high sample density in the feature space, which could lead to locally sufficient samples for robust learning. This inspires us to propose the Max-Mahalanobis center (MMC) loss to explicitly induce dense feature regions in order to benefit robustness. They empirically demonstrate that applying the MMC loss can significantly improve robustness even under strong adaptive attacks, while keeping state-of-the-art accuracy on clean inputs with little extra computation compared to the SCE loss.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. This paper proposes locally constant network (LCN), which is implemented via the gradient of piece wise linear networks such as ReLU networks. The experiments conducted in the paper disclose that training LCN outperforms other methods using decision trees. The detailed comments are as follows:1) The idea of LCN is very interesting, and the equivalence to decision trees is also very valuable, as it provides interpretability and shines light on new training algorithms.<BRK>Connecting the locally constant network with oblique decision trees looks interesting. The idea is based on the fact that DNN consisting of only linear transformations and ReLU activations is piecewise linear. Specifically, they proved that these two models are in some sense equivalent, and one can transform one model to another. I think the paper is well written and the idea is clear.<BRK>*Summary*This paper leverages the piecewise linearity of predictions in ReLU neural networks to encode and learn piecewise constant predictors akin to oblique decision trees (trees with splits made on linear combinations of features instead of axis aligned splits). Overall, the idea is clever, the presentation could be improved slightly, and the experiments raise existential questions for this kind of work.<BRK>They show how neural models can be used to realize piece-wise constant functions such as decision trees. They formally establish the equivalence between the classes of locally constant networks and decision trees. They demonstrate that their method outperforms alternative techniques for training oblique decision trees in the context of molecular property classification and regression tasks.
Reject. rating score: 3. rating score: 6. rating score: 6. This paper extends the prediction error based model by Pathak et. al., 2019. Experiments on VizDoom point navigation tasks show that the proposed model does better than baselines as rewards get sparser. The paper does a good job at clearly explaining their model, presenting results on relevant experiments and baseline comparisons for their model and justifying each modeling choice with ablations. 2.The novelty in their contribution is moderate   the idea of long term future prediction is not new (e.g.: Ke et.al., 2019) (but using it for giving a curiosity bonus is new), the architecture choice is not significantly new. 3.I expected a more detailed ablation for the choice of K, given that “multi step” has major emphasis in the paper, but Figure 7(a) only shows ablations for 3 vs 1 step predictions. Note that the version of their model with 1 step predictions does not completely reduce to Pathak et. al.2019’s ICM model, as RND state features are used. This hypothesis needs to be verified by the authors with more experiments   I would like to see all the main experiments have an additional baseline of 1 step predictions. Only two values of K are tested   1 and 3, what happens with larger values of K? Random features imply that a random “hash” of the observations is being computed which has no reason to have similar features for two nearby states. If there is any small amount of noise in the state transitions, this would mean that predicting far into the future is practically impossible given that the random feature of slightly incorrect states will be very different. Can the authors give reasons/motivations as to why such a model would work in the case of stochastic transitions and K is large or will it be brittle to stochasticity? References:All references are same as those cited in paper.<BRK>The authors tackle the exploration problem by introducing SIM (Sequence level Intrinsic exploration Module). In most existing literature, intrinsic motivation bonuses are scored based on individual states or transitions, and not over multi step trajectories. The error between this feature vector and the RND embedding of the true observation is used as a novelty bonus. Overall, the paper is easy to follow and well motivated. The main experimental results show a reasonable improvement over baselines (RND, ICM). It would be nice if experiments could be performed on a more popular benchmark such as Atari, but overall I think this is an interesting paper with a reasonable contribution. For additional motivation, "Why is posterior sampling better than optimism for reinforcement learning" (Osband & Van Roy 2016) offers some justification on the downsides of modeling novelty bonuses for each state/action independently.<BRK>Sequence level intrinsic exploration model for partial observable domainsThis paper tackles the problem of RL in partially observable domains with sparse rewards. To address the sparse rewards issue, it proposes a sequence level intrinsic novelty model to guide policy learning. The sequence model is based on a dual LSTM architecture. In general, this paper is well written as easily accessible. Comprehensive experiments are provided to validate the effectiveness of the proposed methods. The main issue with the paper is lacking discussions regarding the effective of the biased incur by the intrinsic reward. Specifically, 1)	How is the scaling factor beta determined? It would be nice if some discussions or experimental comparisons can be provided. I wonder how the proposed method perform will in non sparse rewards cases. My main concern is that in the non sparse reward cases, the intrinsic reward will cause bias, which may not guarantee good final performance.<BRK>Training reinforcement learning policies in partially observable domains with sparse reward signal is an important and open problem for the research community. In this paper, they introduce a new sequence-level intrinsic novelty model to tackle the challenge of training reinforcement learning policies in sparse rewarded partially observable domains. To evaluate the efficiency of their proposed approach, they conduct extensive experiments on several challenging 3D navigation tasks from ViZDoom and DeepMind Lab. They also present results on two hard-exploration domains from Atari 2600 series in Appendix to demonstrate their proposed approach could generalize beyond partially observable navigation tasks. Overall, the experiment results reveal that their proposed intrinsic novelty model could outperform several state-of-the-art curiosity baselines with considerable significance in the testified domains.
Reject. rating score: 3. rating score: 3. A definition/example of a regular ECG signal, one presenting arrhythmia, various types of arrhythmia signal forms, would be welcome for the paper to fit this audience and be self contained. This is a multi scale approach.<BRK>This paper describes a large scale ECG dataset that the authors intend to publish. I think this is an interesting dataset for the ECG machine learning community. Is that information part of the to be released dataset?<BRK>They release the largest public ECG dataset of continuous raw signals for representation learning containing over 11k patients and 2 billion labelled beats. Their goal is to enable semi-supervised ECG models to be made as well as to discover unknown subtypes of arrhythmia and anomalous ECG signal events. To this end, they propose an unsupervised representation learning task, evaluated in a semi-supervised fashion.
Reject. rating score: 1. rating score: 3. rating score: 8. This paper presents several models for visual recognition in the presence of image degradation (e.g., low resolution, noise, compression artifacts). In the models, an image enhancement network is placed in front of a recognition model and trained together with the recognizer to improve the recognition accuracy as well as to enhance the image quality. Overall, it was hard to find interesting ideas that future readers may learn from the paper.<BRK>The goal in this work is to improve machine interpretability of images. The authors main claims are: 	Their proposed approach improves image recognition accuracy even without knowing subsequent recognition tasks and recognition models used to perform them (transferable model to different recognition models/tasks). This also ensures the recognition model is not harmed on natural images.” Care to explain? o	: “to achieve better recover the face identity from low resolution images”, Typo? In 1.Introduction/Paragraph 3:o	“.. of great importance that the processed images be recognizable”  Should explain the concept of image recognition! “Image processing” in the context of the paper is intended only as “image enhancement for recognition”. I also have some trouble with the terminology:  o	In 1.<BRK>Claims: The paper presents a concept of "recognition aware (RA) image processing": when one enhances image in a some way, not only human judjement should be taken into account, but also performance of various computer vision application using that image. As an example of processing tasks, authors take super resolution, denoising and JPEG artifacts removal. Authors propose a several training schemas to solve this problem and discuss a limitations of each one:    "simple" preprocessing, when the only image enhancement loss is optimized   "RA" joint optimization of recognition and enhancement loss (supervised and unsupervised)   a variant when two images are created: one for human and one for machine. Paper carefully studies this aspect as well. Overall paper is well written and is pleasure to read.<BRK>Recent progress in image recognition has stimulated the deployment of vision systems (e.g.image search engines) at an unprecedented scale. In this work, they propose simple approaches to improve machine interpretability of processed images: optimizing the recognition loss directly on the image processing network or through an intermediate transforming model, a process which they show can also be done in an unsupervised manner. Interestingly, the processing model's ability to enhance the recognition performance can transfer when evaluated on different recognition models, even if they are of different architectures, trained on different object categories or even different recognition tasks. This makes the solutions applicable even when they do not have the knowledge about future downstream recognition models, e.g., if they are to upload the processed images to the Internet. They conduct comprehensive experiments on three image processing tasks with two downstream recognition tasks, and confirm their method brings substantial accuracy improvement on both the same recognition model and when transferring to a different one, with minimal or no loss in the image processing quality.
Reject. rating score: 3. rating score: 3. rating score: 6. To do this, the authors built a  tilt bot , which tilts a box and the object within to collect data (sound & vision) of object interactions. The problem of integrating audio for perception is interesting and has been quite widely explored; however, this paper extends the setup to also explore the effect of audios on dynamics modeling. This is relatively new and may lead to many potential future developments in this direction. Technically, however, this paper mostly builds on existing technicals on learning forward and inverse models, except that the input is now audio in addition to video. The experimental results are also very limited. They are restricted to a single domain, a fixed collection of objects, and there are no comparisons with published, SOTA algorithms. I also wonder how the authors think of the related work from Zhang et al: http://sound.csail.mit.edu/ , as they ve also studied the effect of auditory and visual data in shape and material recognition.<BRK>This paper presents audio visual object classification and motion prediction work on a novel dataset of 60 different objects rolling around in a bin tilted to and fro by a robot, with video and 4 channel audio recordings of the object impacts. The data is rather novel, is large enough to do ML (around 17 hours of eventful audio/video) and is to be publicly released. The model architectures  are not of theoretical novelty. However, the experiments are somewhat interesting. It was found that the audio contains significant object classification information. The audio was also good for predicting the trajectory of the object. Overall the experiments are rather thin with only a few experimental results.<BRK>This paper explores the interesting connections between action and sound, by building a sound action vision dataset with a tilt bot. This is a good paper overall, I appreciate the efforts on the dataset, and this direction of research is worth pursuing. Regarding experiments, I like the way it is set up, especially the four microphones, and the action space of the robot. IA couple of questions:(1) In the inverse model learning, Fig 3(a) bottom, why are images used as input as well? Don t we want to predict action purely from sound? (2) In forward model prediction, how are the ground truth locations defined and labeled?<BRK>Truly intelligent agents need to capture the interplay of all their senses to build a rich physical understanding of their world. This is primarily due to lack of data that captures the interplay of action and sound. In this work, they perform the first large-scale study of the interactions between sound and robotic action. To do this, they create the largest available sound-action-vision dataset with 15,000 interactions on 60 objects using their robotic platform Tilt-Bot. Finally, object representations derived from audio embeddings are indicative of implicit physical properties.
Accept (Poster). rating score: 8. rating score: 8. rating score: 1. I think the paper should be accepted. The experiments are detailed an elaborate.<BRK>It resolves my concerns. Given these clarifications in an author response, I would be willing to increase the score.<BRK>Are these algorithms badly written and need to be improved? I found the paper fairly hard to read and follow.<BRK>They also demonstrate how learning in the space of algorithms can yield new opportunities for positive transfer between tasks---showing how learning a shortest-path algorithm can be substantially improved when simultaneously learning a reachability algorithm.
Reject. rating score: 3. rating score: 3. rating score: 3. This paper tackles the Image to Image translation task via a simplified yet more effective training procedure. Although this appears to be a simple modification, the empirical performance for generalization and reconstruction qualities prove the effectiveness of the proposal. For instance, I m interested in seeing with some toy distributions, what is the training progress (measured quantitatively) comparing the proposed method and traditional BicycleGAN.<BRK>In this paper, the authors tackle the problem of multi modal image to image translation by pre training a style based encoder. The encoder is not expected to be invertible. On the other hand, I am convinced that the proposed model is better than BicycleGAN, and the approach is somehow novel. My overall rating is borderline.<BRK>Summary:The authors propose to use a non end to end approach to the problem of multi modal I2I. Overall, this is an incremental work in the field of supervised I2I. Is it true that the proposed approach requires semantically aligned datasets, as the style of the whole image is described with a comparatively low dimensional vector, and the GAN objective is applied to paired outputs only?<BRK>Alternatively, they study a simple, yet powerful pre-training strategy for multi-modal I2I translation. They first pre-train an encoder, using a proxy task, to encode the style of an image, such as color and texture, into a low-dimensional latent style vector. Their generator achieves state-of-the-art results on several benchmarks with a training objective that includes just a GAN loss and a reconstruction loss, which simplifies and speeds up the training significantly compared to competing approaches. They further study the contribution of different loss terms to learning the task of multi-modal I2I translation, and finally they show that the learned style embedding is not dependent on the target domain and generalizes well to other domains.
Reject. rating score: 3. rating score: 3. rating score: 6. Summary:The paper studies the sensitivity of a neural network with respect to quantizing its weights and activations. However, I think there is considerable room for improvement, and that more details are needed in order to assess the significance of the results, as I detail in the rest of my review. In my opinion, the main contribution of the paper is the experimental findings, and in particular that the sensitivity of the training loss with respect to the precision of the weights and activations correlates with the accuracy of the network. The method is a straightforward application of Monte Carlo Arithmetic (MCA) to neural networks. "The number of trials is an important consideration because [...] it can produce adverse effects on results". I give some more specific suggestions on what to improve later on. In the baseline method, was the model to be quantized selected based on validation performance before quantization or after quantization?<BRK>The authors propose a scalable method based on Monte Carlo arithmetic for quantifying the sensitivity of trained neural networks to floating point rounding errors. They demonstrate that the loss of significance metric K estimated from the process can be used for selecting networks that are more robust to quantization, and compare popular architectures (AlexNet, ResNet etc.) Other Comments:  How is the second bullet point in Section 4.1 addressed in the proposed method? This could significantly improve the impact of the paper.<BRK>The premise of this paper is that quantization plays an important role in the deployment of deep neural networks; ie in the inference stage. This definition enables the authors to apply Monte Carlo methods to obtain network predictions as shown in equation (10) and figure 2, and subsequently carry out sensitivity analysis. The experiments show that a measure of sensitivity (K) is indeed a good augmentation to cross validation for model selection for the purpose of trading off accuracy and resource consumption when launching deep neural networks with floating point rounding errors.<BRK>Quantization is a crucial technique for achieving low-power, low latency and high throughput hardware implementations of Deep Neural Networks. They present a novel technique, Monte Carlo Deep Neural Network Arithmetic (MCA), for determining the sensitivity of Deep Neural Networks to quantization in floating point arithmetic.We do this by applying Monte Carlo Arithmetic to the inference computation and analyzing the relative standard deviation of the neural network loss. The method makes no assumptions regarding the underlying parameter distributions. For the same network topology and dataset, they demonstrate the ability to gain the equivalent of bits of precision by simply choosing weight parameter sets which demonstrate a lower loss of significance from the Monte Carlo trials. Additionally, they can apply MCA to compare the sensitivity of different network topologies to quantization effects.
Reject. rating score: 6. rating score: 6. rating score: 6. The authors showed that, when the curvature (Hessian) of the network is bounded, improved certificate can be achieved by convex optimization. After rebuttal:Thanks for the response, my concerns have been addressed, rating upgraded to 6: weak accept. While the proposed approaches are theoretically sound, I have several concerns, mostly on the experiments. 1.Existing certification methods should be compared in more details, in terms of different assumptions, activation functions, etc.<BRK>This paper gives a global curvature bound for general neural networks, and used this bound for certifying robustness of neural networks. Despite some concerns, the main contribution of giving global curvature bounds of neural networks is valid. 2.The paper mentions the Attack problem and proposes an attack algorithm (Algorithm 2), however I am not able to find any experiments on the attack.<BRK>This paper develops computationally efficient convex relaxations for robustness certification and adversarial attack problems given the classifier has a bounded curvature. However all empirical results are only from MNIST. The result is a consequence of a closed form expression that the paper derived for the Hessian of a deep network.<BRK>In this paper, they provide computationally-efficient robustness certificates for deep classifiers with differentiable activation functions in two steps. Second, they derive a computationally-efficient differentiable upper bound on the curvature of a deep network.
Reject. rating score: 1. rating score: 3. rating score: 3. rating score: 3. In the absence of any theoretical or empirical result to justify the merits of the proposed algorithm the contribution of this paper to the literature is not clear. Unfortunately the paper doesn’t provide any comparison with those methods.<BRK>The evaluation is limited to tasks of which the state space is small, and the proposed method is not compared with existing methods. Due to the unclear novelty and limited empirical results, I give weak reject to the paper in the current form. My concern is that the scalability of the proposed method. I m curious about the performance of the proposed method in high dimensional tasks.<BRK>The authors go over existing algorithms and talks a bit how their proposal conceptually differs in how it performs said backups. I come away from this work not fully appreciating the impact it is trying to sell me on. First, I would like to see better comparisons between this method and existing policy iteration methods.<BRK>Besides, there is no baseline method in the literature to be presented to compare with the proposed method. The paper does not clearly state the motivation of the proposed method.<BRK>This method performs policy updates which integrate reward information over all states at all horizons simultaneously thus sequentially maximizing the expected reward obtained per algorithmic iteration. An exact formula is derived which finitely parametrizes these path gradients in terms of action preferences. They are quadratic in successor representation entries and afford natural generalizations to higher-order gradient techniques.
Reject. rating score: 3. rating score: 6. rating score: 8. The proposed method is straightforward (which is good): a convolution window with size n is used to calculate representation for an n gram, which then replaces the token representation in a standard attention model. The paper implements the multihead version of the proposed phrase based attention, more specifically, in a transformer model. Experiments with machine translation and language modeling show that it outperforms the token attention counterpart. My main concerns are about the experiments:   For the translation experiments, the transformer numbers are lower than those reported by Vaswani et al.(2017) across the board, for both the "base" and the "big" settings. But please indicate this instead of putting a citation in the table, which can be misleading. I do not recommend the that the paper is accepted, until the authors address my concerns on the baselines.<BRK>This paper proposes an extension of the attention module that explicitly incorporates phrase information. Transformer models with the proposed phrase attention are evaluated on multiple translation tasks, as well as on language modelling, generally obtaining better results than by simply increasing model size. I find the idea interesting. On 3 WMT 14 translation tasks, the proposed approach leads to improvements between 0.7 and 1.8 BLEU with respect to Transformer Base. Using phrasal attention appears to be more efficient than simply increasing model size. Some claims made in the paper may be too strong. As such, at a given layer, it is not guaranteed the the i^{th} vector is a representation i^{th} token. As such, neighbouring representations may not represent n grams. However, this is very recent work (September 5 on ArXiv), so it would be understandable for the authors not to know about it.<BRK>This work proposes an attention mechanism that directly reflects the phrasal correspondence by employing convolution. The n gram aware attention is incorporated into Transformer and shows gains in translation and language modeling tasks. The gains reported in this paper are meaningful, thought might not be SOTA. The analysis on attention is also interesting in that the phrasal relation is employed in lower layers, but not in the higher layer. Did you completely remove the multi head attention, but used only n gram attentions? It is unfortunate that this work does not report empirical results when applied to a big model configuration.<BRK>They propose a novel phrase-based attention method to model n-grams of tokens as the basic attention entities, and design multi-headed phrasal attentions within the Transformer architecture to perform token-to-token and token-to-phrase mappings. Their approach yields improvements in English-German, English-Russian and English-French translation tasks on the standard WMT'14 test set. Furthermore, their phrasal attention method shows improvements on the one-billion-word language modeling benchmark.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 6. However these section are  important to understand the  rest paper.<BRK>At least the last paper should be cited and discussed in related work.<BRK>Section 4.2 was really difficult for me to parse and is too compressed (so is the rest of the paper but this one was worse).<BRK>nan
Reject. rating score: 1. rating score: 1. rating score: 3. The paper is well written, and the background is introduced clearly. As I understand it, the goal of *out of distribution sample detection* is to train a deep network that simultaneously generalizes well and also be discriminative to outliers. However, it’s not clear to me why the proposed method server this purpose; empirical results are not convincing either. It is not clear to me how to avoid degenerate solutions at convergence while maintaining good testing performance with the proposed training strategy. In table 2, the reported testing accuracy on CIFAR 10 using Dense BC is 92.4.<BRK>The paper considers the problem of out of distribution (OOD) sample detection while solving a classification task. “because they will not be relevant to the classification accuracy” – who are they? Until the authors can clarify and justify the objective, I will vote for rejection only based on this ground. It seems that the paper requires a lot of polishing. More about the clarity issues belowFor strong evaluation, comparison with Malinin & Gales (2018) work seems to be important since it was the only work also using uncertainties for OOD detection in related work. Therefore, the claim that “Therefore, the uncertainties of the features will be larger when the inputs are in distribution samples” requires more elaboration and arguments2. Conceptually, this means that the output of the corresponding layers is considered to be stochastic rather than deterministic.<BRK>However, I feel that this paper is somewhat under evaluated initially, so I hope the authors have an opportunity in another venue with their revision. At least, if \mu is proven to have no effect on OOD detection by some experiment, then it can be a clue. Experimental results on several OOD benchmarks with different backbone networks show that their method outperforms ODIN (Liang et al., 2017). In ICML, 2015. Again, I am not sure why \mu and \sigma should be split, and why \mu should be discarded for the OOD detection part. 4.Comparison with more state of the art methods is required.<BRK>In this paper, they tackle the problem of detecting samples that are not drawn from the training distribution, i.e., out-of-distribution (OOD) samples, in classification. This problem arises because their approaches use only the features close to the output layer and disregard the uncertainty of the features. Therefore, they propose a method that extracts the uncertainties of features in each layer of DNNs using a reparameterization trick and combines them. In experiments, their method outperforms the existing methods by a large margin, achieving state-of-the-art detection performance on several datasets and classification models.
Accept (Poster). rating score: 8. rating score: 8. rating score: 3. The main contribution of this paper is to learn a universal policy that is able to perform near optimally on test tasks with transition dynamics that were never observed during training. This is achieved by using a "probe policy" to generate short trajectories that are then used to learn a latent encoding to categorise the transition dynamics of the current task. The universal policy is then conditions on both the state and this encoding so that the learned policy can perform well on tasks with different dynamics. Overall I really like this paper.<BRK>An environment s transition dynamics is assumed to depend on a hidden parameter that is not observed by the agent, in contrast to some previous work which assumes observability. The idea of the main algorithm is as follows. Did you try using an LSTM as a baseline to directly learn the policy? A "probing" policy is run for a specified number of time steps. From the trajectory generated, a VAE is used to estimate the hidden parameter governing the transition dynamics. 10.Why different probe length settings for 2D navigation vs. Acrobot and HIV? (https://link.springer.com/article/10.1007/s10514 017 9666 5)EDIT: After the comments below which addressed my concerns, I have raised the score for this paper from a weak reject to an accept. Only 3 runs were made for the test episode, which is not enough to be able to make empirical claims with non trivial confidence (see https://arxiv.org/abs/1709.06560). Some additional questions about the experiments:1.<BRK>This paper presents a strategy for single trajectory transfer of a reinforcement learned policy. In this case, the latent variable is ‘Z’. At test time, they first run an exploitative algorithm (they call this the probe) to rapidly infer the value of Z, and thereby the optimal policy which this indexes, before switching to running the policy alone. I think that this paper (if indeed novel) is interesting, and I do agree that few trajectory transfer in RL is a potentially impactful area in which to be working. To improve the paper, I would propose a slight rewrite to focus only on the core claim (i.e.why this model outperforms anything else for wall clock competitive single episode RL policy transfer).<BRK>Transfer and adaptation to new unknown environmental dynamics is a key challenge for reinforcement learning (RL). To achieve single episode transfer in a family of environments with related dynamics, they propose a general algorithm that optimizes a probe and an inference model to rapidly estimate underlying latent variables of test dynamics, which are then immediately used as input to a universal control policy.
Reject. rating score: 3. rating score: 3. The paper proposes to condition the architecture on the input instances by introducing a "selection network" that learns to retain a subset of branches in the architecture during each inference pass. [2] Tan, Mingxing, et al."Mnasnet: Platform aware neural architecture search for mobile." The method resembles sparsely gated mixture of experts [1] at a high level, but has been implemented in a way that better fits the context of architecture search (which is still technically interesting). Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.<BRK>This paper proposes an instance aware dynamic network, ISBNet, for efficient image classification. The methods used in this paper are similar to previous works on neural architecture search (NAS), but this paper can be seen as a meaningful extension to NAS. Overall, the idea of the paper is clearly presented.<BRK>Recent years have witnessed growing interests in designing efficient neural networks and neural architecture search (NAS). Inference with a fixed model that processes all instances through the same transformations would incur computational resources unnecessarily. For example, ISBNet takes only 8.70% parameters and 31.01% FLOPs of the efficient network MobileNetV2 with comparable accuracy on CIFAR-10.
Reject. rating score: 1. rating score: 3. rating score: 6. On this test set, the model in the paper performs essentially the same as on the homologous dataset, while the Deng model performs substantially worse. Can you give examples of scenarios where your model got things right and Deng s model did not, and vice versa? They find performance of their two stage model is a bit better than Deng s model on this test set.<BRK>Hence, the proposed work does not add a significant insight in solving the problem. The authors compare their method against Image2Latex approach (2016) that is an end to end pipeline and show that there is significant improvement compared to this approach.<BRK>The paper is written well, and easy to read. 2) In formula (4), the reviewer did not see the explanation of "v_{att}^T" and "u_T".<BRK>The experiment demonstrates that the two-stage method significantly outperforms the end-to-end method.
Reject. rating score: 1. rating score: 1. rating score: 1. Pros:This paper proposed a new method for zero shot transfer learning under the reinforcement learning setting. As a result, the novelty of the proposed method was very incremental and limited from a technology perspective. The only difference was an added attention layer to the learning of latent states.<BRK>The paper proposes adding an attention mechanism to the DARLA beta VAE approach to transfer learning. I cannot at this stage recommend acceptance for the following reasons:1) The paper augments an existing method with a well understood attention mechanism, so the novelty of the approach is relatively low.<BRK>The paper proposes a new method for zero shot visual transfer for RL, SADALA. However, I d encourage the authors to try this. Reject.The experiments of the paper were particularly weak.<BRK>Domain adaptation is an open problem in deep reinforcement learning (RL). The gap between visual observations of the source and target environments often causes the agent to fail in the target environment. They present a new RL agent, SADALA (Soft Attention DisentAngled representation Learning Agent).
Reject. rating score: 3. rating score: 3. rating score: 8. 1.I had hard time to understand latent canonicalization. Perhaps an example in linear algebra is needed. 2.The learning of the proposed model relies on meta data description such that the learning is supervised. Can the method be applicable to situations where no meta data and no class labels are available? The experiments were only done on simple image datasets. I am wondering this method can be applied to other complex datasets whose latent factors are unknown.<BRK>This paper proposes to relax the assumption of disentangled representation and encourage the model to learn linearly manipulable representations. I find these assumptions too strong for the task of learning disentangled representation. In fact, this is a very difficult part of learning disentangled representation. Although experiments show that learning such representations are beneficial for low shot setting of SVHN, it is not clear whether such improvement generalizes to more realistic datasets such as ImageNet. If the goal is to learn representation for low shot setting, the method needs to be compared with other representation learning methods such as jigsaw[1], colorization[2] and rotation[3]. European conference on computer vision.<BRK>This paper proposes a method for unsupervised learning  of data representations that can be manipulated to remove factors of variation via linear transformations. A couple things I thought were missing in the paper:Did you try applying the classification loss to both the encoded representation and the canonicalized representation at the same time? I think this paper should be accepted as it proposes a novel idea, which does not seem too difficult to reproduce, describes a simulator for synthetic data for digit recognition, and proposes it as a benchmark for learning representations, and provides experimental results that help in better understanding the representation learned by the model.<BRK>In this work, they seek the generalization power of disentangled representations, but relax the requirement of explicit latent disentanglement and instead encourage linearity of individual factors of variation by requiring them to be manipulable by learned linear transformations. Assuming a source domain with access to meta-labels specifying the factors of variation within an image, they demonstrate experimentally that their method helps reduce the number of observations needed to generalize to a similar target domain when compared to a number of supervised baselines.
Reject. rating score: 3. rating score: 3. rating score: 6. [1] Frankle & Carbin. "The Lottery Ticket Hypothesis." It is a novel contribution to both our knowledge of pruning and of finding winning lottery tickets. ICLR 2019. The paper presents no concrete data on the comparative costs of performing CS and IMP even though the core claim is that CS is more efficient. The paper does not disclose enough detail to compute these costs, and it seems like CS is more expensive than IMP for standard workflows. Moreover, the current presentation of the data through "pareto curves" is misleadingly favorable to CS. I also believe that the paper needs experiments on ImageNet and needs a more thorough evaluation as a pruning technique beyond the lottery ticket hypothesis.<BRK>This work propose a new iterative pruning methods named Continuous Sparsification. It will continuously prune the current weight until it reaches the target ratio instead of iterative prune the weight to specific ratio. The author gives a good analysis but the experiment is not yet convincing enough. 1) This work actually presents compression algorithm with little connection with lottery ticket. As a pruning method, it does not show the results on common models like VGG and DenseNet with different depth. It also does not give the experiment on ImageNet.<BRK>This paper proposes a novel objective function that can be used to jointly optimize a classification objective while at the same time encourage sparsification in a network. The lottery ticket hypothesis and associated work shows that the iterative pruning of a network can lead to a sparse network that performs with high accuracy. On the other hand, the work of Zhou et al.shows that sparse masks (dubbed "supermasks") may be learned without training the parameters of the network. In a sense, this paper tries to combine these ideas by simultaneously training a network while also optimizing the mask. I think this paper serves as a reasonable contribution to the ever growing "lottery ticket hypothesis" body of work. The paper is mostly clear, and the idea for joint optimization is very reasonable. It s not tremendously original (in that it basically combines two ideas that are already in the literature), but in spite of that, I still think this paper warrants being accepted to ICLR. In particular, the fact that continuous sparsification can find winning tickets without any parameter rewinding is fascinating and deserves further investigation. Do the authors have any sense for why this works, when prior work suggests that rewinding is necessary for sufficiently complicated models and datasets?<BRK>The Lottery Ticket Hypothesis from Frankle & Carbin (2019) conjectures that, for typically-sized neural networks, it is possible to find small sub-networks which train faster and yield superior performance than their original counterparts. In this paper, they propose a new algorithm to search for winning tickets, Continuous Sparsification, which continuously removes parameters from a network during training, and learns the sub-network's structure with gradient-based methods instead of relying on pruning strategies. They show empirically that their method is capable of finding tickets that outperforms the ones learned by Iterative Magnitude Pruning, and at the same time providing up to 5 times faster search, when measured in number of training epochs.
Reject. rating score: 3. rating score: 3. rating score: 6. This work studies the predictive uncertainty issue of deep learning models. In particular, this work focuses on the distributional uncertainty which is caused by distributional mismatch between training and test examples. Instead, this paper proposes a new loss function for DPN, which consists of the commonly used cross entropy loss term and a regularization term. The final objective function is a weighted combination of the two loss functions. Actually, ODIN s setting seems to be more practical and more challenging than the setting used by the propose methods. 5.The experimental study can have more comparison on challenging datasets with more classes since it is indicated that DPN has difficulty in dealing with a large number of classes. Otherwise,  \lambda_{out} < \lambda_{in} may not make sense.<BRK>This paper proposes an improved DPN framework with a novel loss function, which uses the standard cross entropy loss along with a regularization term to control the sharpness of the output Dirichlet distributions from the network.The proposed loss function aims to improve the training efficiency of the DPN framework for challenging classification tasks with large number of classes The proposed improved DPN is very incremental. It only adds a simple regularization term to the standard cross entropy loss. The regularization term is the precision of the Dirichlet from the DPN. The technical novelty and contribution is not significant.<BRK>The paper proposes a novel loss function using the standard cross entropy loss along with a regularization term on logits for training the Dirichlet prior network. The Dirichlet prior network is proposed by Malinin & Gales (2018), the new method in this paper overcomes the challenge of training the network based on the KL divergence which cannot work well for dataset with large number of classes. The paper is well written and easy to follow.<BRK>Determining the source of uncertainties in the predictions of AI systems are important. It allows the users to act in an informative manner to improve the safety of such systems, applied to the real-world sensitive applications. In this paper, they present an improved DPN framework by proposing a novel loss function using the standard cross-entropy loss along with a regularization term to control the sharpness of the output Dirichlet distributions from the network. Their proposed loss function aims to improve the training efficiency of the DPN framework for challenging classification tasks with large number of classes. In their experiments using synthetic and real datasets, they demonstrate that their DPN models can distinguish the distributional uncertainty from other uncertainty types.
Reject. rating score: 3. rating score: 3. rating score: 6. The paper proposes a novel way to formulate intrinsic reward based on optical flow prediction error. * "We demonstrated the proposed methodology and compared it against a number of baselines on Atari games, Super Mario Bros., and ViZDoom." please state more clearly that only 5 out of 57 Atari games are considered, here and in the abstract. Suggestions on improving the paper:1) Better motivating the approach in the paper would help. I am leaning towards rejecting this paper. 2) Better motivating the choice of the environments and conducting experiments on more environments would be important for evaluating the impact of the paper. Why would those weaknesses drive the agent to new locations? Second, the choice of tasks where the largest improvement is shown (i.e.5 Atari games) seems not well motivated and rather crafted for the proposed method. Those 5 Atari games are not established hard exploration games. I understand that every game becomes hard exploration if the rewards are omitted but then there is a question why those particular games. This is a very important question and I hope the authors will address this. * "These games are characterized by moving objects that require the agents to concentrate on and interact with." * Figure 6   those results are not great compared to the results of Episodic Curiosity: https://arxiv.org/abs/1810.02274 .<BRK>Well motivated paperThe authors study the problem of exploration and exploitation in deep reinforcement learning. The authors propose a new intrinsic curiosity based method that deploys the methods developed in optical flow. Following this algorithm, the agents utilize the reconstruction error in the optical flow network to come up with intrinsic rewards. The authors show that this approach boosts up the behavior of the RL agents and improves the performance on a set of test environments. A few comments that I hope might help the authors to improve the clarity of their paper. 1) While the paper is nicely written, I would encourage the authors, of course, if they think necessary, to make the paper slightly more self contained by explaining the optical flow problem, FlowNet, and warping approach. While a cruise reader might be required to either know literature in optical flow or go and study them along with this paper, it might be helpful for a bit more general readers to have these tools and approaches in access. For example the empirical study in Fig 5. 6) While I find this study interesting and valuable, the novelty of the approach might fall short to be published at a conference like ICLR with a low acceptance rate. This does not mean that there is anything unscientific about this paper, in fact, the scientific value of this work is appreciated and this work adds a lot to the community. And also, if there is a shortcoming, what are those.<BRK>ProsSolid technical innovation/contribution:   The paper proposed a novel method FICM that bridged the intrinsic reward in DRL with optical flow loss in CV to encourage exploration in an environment with sparse rewards. To the best of my knowledge, this was the first paper proposed to use moving patterns in two consecutive observations to motivate agent exploration. Clarity:    In general this was a very well written paper, I had no difficulty in following the paper throughout. The proposed method (FICM) was clearly motivated, and the authors provided good coverage of related works. ConsExperiments:  Experiments were conducted only using a few recent results as baselines (ICM, forward dynamics, RND). I’d also like to see more extensive comparisons between FICM and ICM across different datasets, for example, Super Mario Bros. and the Atari games, instead of only comparing FICM against ICM on ViZDoom. Reproducibility:  Although the authors discussed the experiment setting in detail in supplements, I believe open sourcing the code / software used to conduct the experiments would be greatly help with the reproducibility of the proposed method for researchers or practitioners.<BRK>Exploration bonuses derived from the novelty of observations in an environment have become a popular approach to motivate exploration for reinforcement learning (RL) agents in the past few years. In this paper, they introduce the concept of optical flow estimation from the field of computer vision to the RL domain and utilize the errors from optical flow estimation to evaluate the novelty of new observations. They introduce a flow-based intrinsic curiosity module (FICM) capable of learning the motion features and understanding the observations in a more comprehensive and efficient fashion. They evaluate their method and compare it with a number of baselines on several benchmark environments, including Atari games, Super Mario Bros., and ViZDoom. Their results show that the proposed method is superior to the baselines in certain environments, especially for those featuring sophisticated moving patterns or with high-dimensional observation spaces.
Reject. rating score: 1. rating score: 6. rating score: 6. The idea is to "regularize" the target policy being learned to generate a trajectory distribution similar to the one induced by the source optimal policy in the respective MDP. Major comments The problem of transfer in RL is fundamental for scaling RL algorithms to real domains. Lemma 4.3 is also a know result in the literature, but what does it tell us about the proposed approach? The paper is well written and easy to read. The proposed algorithm is not computing any explicit transition model. However, I believe there might be some technical issues with the derivations that overall make me doubt about the significance of this work. The definition of the KL divergence requires that p is absolutely continuous w.r.t.q.However, under Ass. 2.(12) seems to be wrong too. Theorem 4.1 is a standard supervised learning bound.<BRK>This paper addresses the actively studied problem of efficiently transferring policies across domains in reinforcement learning. The proposed algorithm is based on a policy adaptation mechanism, with the idea that provided that a source optimal policy of a task is available, that policy is adapted to derive the optimal policy of the target task at a low sample complexity. The paper is well written and the proposed algorithm is novel and original. Cons:  Some important previous  works are missing from the related work section. Assumption 2 should be discussed; any suggestion on how to model the KL approximation if the assumption does not hold?<BRK>The idea is to add an additional cost that is the KL divergence between the trajectory likelihood under target policy (being learned) and target dynamics and the trajectory likelihood under the source policy (assumed optimal and deterministic) and source dynamics. I think the problem of transferring knowledge from one task to another in RL is very important for RL to be applicable to more real world scenarios. Concerns / QuestionsLine 7 of Alg1 is confusing because it refers to a “target task model”, but in Assumption 2, it says only a model of the source transition function is needed. I think (b) is a special case of transfer learning   a lot of transfer learning is concerned with changing reward functions as well, which this method wouldn’t apply to. I think the related work section is missing important areas of research in imitation learning and meta reinforcement learning.<BRK>Efficient and robust policy transfer remains a key challenge in reinforcement learning. They introduce a principled mechanism that can \textbf {"Adapt-to-Learn"}, that is adapt the source policy to learn to solve a target task with significant transition differences and uncertainties. They show through theory and experiments that their method leads to a significantly reduced sample complexity of transferring the policies between the tasks.
Reject. rating score: 3. rating score: 3. rating score: 6. 3.The notation of the distribution is confusing. SummaryIn this paper, the author introduces a new privacy notation for the attribute attacks. Based on this view, the author has presented a comprehensive analysis of the trade off between privacy preserve and modelutility.<BRK>+ Which DP Laplacian mechanism is used is not specified. + The most impressive part of this paper is the analysis of the trade off between privacy and utility from which the upper bound is quantified. + The major weakness is the experiments.<BRK>Besides, there were two epsilon values used in the experiments for DP. It also provides an analysis of the intrinsic privacy utility tradeoff. As for the comments and experiments with differential privacy, I’m not sure if I follow the argument correctly.<BRK>In light of the current gap between theory and practice, they develop a novel theoretical framework for privacy-preservation under the attack of attribute inference. On the other hand, it is clear that privacy constraint may cripple utility when the protected attribute is correlated with the target variable. Empirically, they extensively conduct experiments to corroborate their privacy guarantee and validate the inherent trade-offs in different privacy preservation algorithms.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. The paper proposes an alternative to the truncated back propagation through time (BPTT) algorithm for training RNNs. It maintains a buffer of the last N RNN states that can be updated. The states s_i and s_{i T}, as well as the RNN parameters are updated based on this loss function. The novel idea of the paper is therefore a modifiable state buffer for the RNN states. I confess that I did not read the theory; I don’t think it’s super relevant because in practice convergence to fixed point will require too many updates.<BRK>The authors note that RNNs are trained using BPTT, which prevents them from being trained in an online fashion. Proposed Method: The authors propose to learn the state of the RNNs explicitly by improving the prediction accuracy at each time step as well as predicting the "next" state of the RNN. The authos note that the constraint of predicting the next state  is a fixed point formula for the states underthe given RNN dynamics. Clarity of the paper: The paper is clearly written. I like it.<BRK>In this paper, the authors reformulate the RNN training objective to explicitly learn the state vectors, and propose an algorithm called Fixed Point Propagation (FPP Algorithm 1). In general, this paper is interesting and well written. The experiment results in Section 5 seem to be very strong. I think the paper suffers from the following limitations:1) Theorem 1 shows that the FPP algorithm converges to a stationary point. My understanding is that \lambda is a key meta parameter of FPP.<BRK>Recurrent neural networks (RNNs) allow an agent to construct a state-representation from a stream of experience, which is essential in partially observable problems. There are variety of strategies to improve training in RNNs, the mostly notably Backprop Through Time (BPTT) and by Real-Time Recurrent Learning. They show that for a fixed buffer of data, their algorithm---called Fixed Point Propagation (FPP)---is sound: it converges to a stationary point of the new objective.
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. CycleGAN has attracted a lot of attention in unpaired image to image translation. This paper provides a nice answer the the first question. The addressed issue is important, the investigation is reasonable, and the results are intuitive and plausible, with clear practical implications. I think it is a good paper.<BRK>This paper focuses on CycleGAN method to show theoretically when the exact solution space is invariant with respect to authomorphisms of the underlying probability spaces for unpaired image to image translation. I strongly suggest the authors to expand and provide more experimental evaluations. The experimental results are interesting, however, they are very limited.<BRK>I have read the rebuttal of the authors . or by using a radically new approach  for cycle gan such as the Gromov Wasserstein distance as done in " Learning Generative Models Across Incomparable Spaces" Review of the paper: The notations and the formalism  in the paper are heavy and cumbersome and don t come with any surprising result, since the transforms between unpaired spaces will be found always up to   symmetries since we have the composition of one map with another. The use of the identity loss is also shown to not to help either in fixing this invariance issue.<BRK>It is known that the CycleGAN problem might admit multiple solutions, and their goal in this paper is to analyze the space of exact solutions and to give perturbation bounds for approximate solutions. They show theoretically that the exact solution space is invariant with respect to automorphisms of the underlying probability spaces, and, furthermore, that the group of automorphisms acts freely and transitively on the space of exact solutions. In order to demonstrate that these results are applicable, they show that under mild conditions nontrivial smooth automorphisms exist.
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. The key idea in this paper is to generate a feature vector based on a Fisher information idea for intermediary levels of a deep neural network. The connection to tangent plane ideas in terms of robustness also made sense. Is there a way to argue via a perturbation analysis of the variational problem of what makes the most sense. I feel the paper hints at this but does not make this idea explicit.<BRK>The method is theoretically backed by an appeal to the recently proposed neural tangent kernel and seems like it could be practically useful. Specifically, I would like further discussion of the choice of the layers to use as gradient features and an ablation study on supervised trained networks. Significance: The approach is a quite nice merging of theoretical insights with a neat practical implementation. A quick paste of PyTorch pseudo code would be sufficient here. Similarly, I’d like to see the features themselves used as a linear classifier (no network forwards passed) in the same two ablation studies. That is, could the authors use w_1^T J w_2 as the features for their linear classifier.<BRK>Summary: This paper considers the use of a neural network s Jacobian as additional features for semi supervised, unsupervised, and transfer learning of representations. The idea is simple and the authors motivate this choice by connecting it to the literature on the neural tangent kernel (although nothing is proven in this paper). The ablation study could be more compelling. The discussion of the connection to theory added very little to the paper.<BRK>They address the challenging problem of deep representation learning--the efficient adaption of a pre-trained deep network to different tasks. Specifically, they propose to explore gradient-based features. Their key innovation is the design of a linear model that incorporates both gradient and activation of the pre-trained network. They demonstrate that their model provides a local linear approximation to an underlying deep model, and discuss important theoretical insights.
Reject. rating score: 3. rating score: 3. rating score: 6. rating score: 6. The abstract and the introduction were set up nicely and I was quite intrigued to see a theoretical analysis and practical implementation of a neural network using analogue hardware. All in all, I believe this is a very promising direction to invest, but the paper is not quite ready for ICLR.<BRK>The manuscript illustrates how a noisy neural network can reduce the learning capacity. To mitigate this loss, the authors propose a method that combines the method of "noise injection and "knowledge distillation". However, from a conceptual point of view,  their contribution (i.e.(10) in Section 5,) is unclear to me.<BRK>The experimental results show that the combination of distillation and noise injection outperforms pure noise injection on all networks, as well as noisy inference without retraining. * Evaluation *Overall I like this paper and think it is suitable to accept for ICLR, because it addresses an important practical problem of implementing deep networks on efficient hardware. Relevant literature in this domain is cited. What I am mainly missing are two points:1.<BRK>The authors of the manuscript study how inherent noise in the analog neural networks affects its accuracy. However, in heavy use the system may warm up, and then there could be an effect that is correlated accross different weights. The noise model used would not be able to ensure proper inference in these conditions.<BRK>The success of deep learning has brought forth a wave of interest in computer hardware design to better meet the high demands of neural network inference. However, these proposed analog accelerators suffer from the intrinsic noise generated by their physical components, which makes it challenging to achieve high accuracy on deep neural networks. Hence, for successful deployment on analog accelerators, it is essential to be able to train deep neural networks to be robust to random continuous noise in the network weights, which is a somewhat new challenge in machine learning. In this paper, they advance the understanding of noisy neural networks.
Reject. rating score: 3. rating score: 3. rating score: 3. If the results are not equivalent, this implies that the discriminator does not reach the optimum. The paper proposes a pure implicit likelihood approach that uses three discriminator models to estimate the KL divergences. Moreover, the paper uses LPIPS to measure reconstruction quality   but this measure is a deep neural network. So the explicit solution of theorem 1 can be written down and another ablation study would be training the method with the explicit formulation for this KL term(i.e.only training two discriminator models).<BRK>The use of a weighted sum of the forward & KL divergences to train a generative model is hardly new, and has already been presented a few times (Larsen et al.2015, Dosovitskiy & Brox 2016). In this context the paper does not present the impact of its main contribution alone.<BRK>The meaning of the units of the axes is a bit unclear. In Figures 5 and 7 the reconstruction of IJAE sometimes seems to be pretty far from the original image (i.e., it s not that it s blurry as for VAEs, it s that the model seems to be reconstructing a completely different image).<BRK>It is known that GAN can produce very realistic samples while VAE does not suffer from mode collapsing problem. One of these parts can be optimized by using the standard adversarial training, and the second one is the very objective of the VAE model.
Reject. rating score: 1. rating score: 1. rating score: 1. The paper describes a dependency parser for Amharic text trained on the Yimam et al.2018 treebank. The proposed method is an unlabelled arc eager transition based dependency parser followed by a dependency label classifier.<BRK>The paper claims to build a transition based dependency parser for Amharic. The major problem of the paper is precision.<BRK>* overview   This paper describes a model for transition based dependency parser, and tested on the Amharic treebank. Generally, the architecture of the model is poorly explained.<BRK>In this study, a dependency parser for Amharic language is implemented using arc-eager transition system and LSTM network. The study introduced another way of building labeled dependency structure by using a separate network model to predict dependency relation.
Reject. rating score: 1. rating score: 3. rating score: 8. This paper presents a multi frame super resolution method applied to satellite imagery. Besides, I am not convinced that pair wise fusion can handle significant translational fusion as the filters have shared parameters. A major concern is the estimation of a single translational motion for the SR image at the end of the network after all multiple images are already fused.<BRK>The paper proposes a framework including recursive fusion to co registration and registration loss to solve the problem that the super resolution results and the high resolution labels are not pixel aligned. 3) Registration loss is important in this paper and it can solve the problem the output SR is not pixel wise aligned to the HR ground truth. However, I have some concerns about this paper:1) This paper lacks many references. Recently, many works focus on multi frame super resolution containing video super resolution and stereo image super resolution via deep learning.<BRK>This paper proposes an end to end multi frame super resolution algorithm, that relies on a pair wise co registrations and fusing blocks (convolutional residual blocks), embedded in a encoder decoder network  HighRes net  that estimates the super resolution image. Overall, I found this paper interesting, and the method described is both clever and efficient. While I am not an expert on super resolution, I do see a clever algorithm, that can be for example used with different number of input views. Lastly, the results are good wrt to the state of the art, as the algorithm was proposed during a 2019 challenge and was in the top ones on the private leaderboard. In that case, is that possible to have a super resolution of the type of the input LR images? 1 that it has a very large number of parameters (34M).<BRK>Generative deep learning has sparked a new wave of Super-Resolution (SR) algorithms that enhance single images with impressive aesthetic results, albeit with imaginary details. Multi-frame Super-Resolution (MFSR) offers a more grounded approach to the ill-posed problem, by conditioning on multiple low-resolution views. This is important for satellite monitoring of human impact on the planet--from deforestation, to human rights violations--that depend on reliable imagery. To this end, they present HighRes-net, the first deep learning approach to MFSR that learns its sub-tasks in an end-to-end fashion: (i) co-registration, (ii) fusion, (iii) up-sampling, and (iv) registration-at-the-loss. They introduce a registered loss, by learning to align the SR output to a ground-truth through ShiftNet.
Reject. rating score: 1. rating score: 1. This paper proposes modification to Deep CFR and introduce a simplified baseball environment to evaluate the modifications. If I have interpreted the authors results correctly, this demonstrates that the initial policy is not a Nash equilibrium. Why is this more of an issue in fully competitive environments than it is in general sum games? Section 3.1. closes by stating "In principle, their average strategies will gradually converge to the Nash equilibrium strategy." Setting aside the issue regarding whether a Nash equilibrium has been learnt, this is a subjective opinion not an rigorous empirical observation. All results presented should include quantification of variation as well as average values and the number of repeats these averages are taken from should be clearly documented.<BRK>The paper studies adaptation of agent policies in a simplified baseball game, which is designed as a zero sum two agent game between a batter (B) and a pitcher (P), each of which has 5 discreet actions. The introduced game is fully observable but stochastic, which the authors argue is a challenging setup. The authors propose a Bayesian style adaptation of the agent strategies (where each agent models the probability of the actions of the opponent by computing the posterior give a prior and evidence from the past observations), which seems to be computable analytically, from an initialization learned with counterfactual regret minimization (CFR) that approximates the Nash of the considered game. Could the authors argue (preferably, formally theoretically or at least quantitatively) why adaptation is necessary? Does the asymmetry of the game have to do something with this? There are typos throughout.<BRK>In this paper, to discuss whether the adaptation capability can help a learning agent to improve its competitiveness in a multi-agent environment, they construct a simplified baseball game scenario to develop and evaluate the adaptation capability of learning agents. They purpose a modified Deep CFR algorithm to learn a strategy that approximates the Nash equilibrium strategy. They also form several teams, with different teams adopting different playing strategies, trying to analyze (1) whether an adaptation mechanism can help in increasing the winning percentage and (2) what kind of initial strategies can help a team to get a higher winning percentage. Besides, with the proposed strategy adaptation mechanism, the winning percentage can be increased for the team with a Nash-equilibrium initial strategy. Nevertheless, based on the same adaptation mechanism, those teams with deterministic initial strategies actually become less competitive.
Reject. rating score: 3. rating score: 3. rating score: 6. rating score: 6. This work presents a model for text based video clip (video moments or text to clip) retrieval. The goal is to identify a video segment within a longer video that is most relevant to an input sentence. Authors propose a new model based on a weakly supervised training approach. The main contribution of the paper is incremental (specially respect to Mithun et al., 2019), I do not see a ground breaking contribution. One of the main novelties with respect to previous text to clip models is the use of co attention schemes at the level of words and frames. However, the idea of co attention at different grain levels have been proposed before. Similarly, for DiDeMo dataset, results in Table 3 are for the test set, while the ablation study in Table 4 is for the validation set. I rate the paper as borderline, but there is not such a rating at ICLR 2020, so I will lean to weak reject.<BRK>Overview: The authors proposed a weakly supervised method to localize video moments given text queries. Then the attentive features are used to localize the sentence query in videos by calculating the similarity of words and frames. In summary, the proposed weakly supervised Moment Alignment Network (wMAN) utilizes a multi level co attention mechanism to learn richer multimodal representations for language based video retrieval..Pros:1. So in this way, the novelty is only marginal. 2.Paper writing can be improved. Figure 2 shows the overall structure of the model, however, the caption doesn t explain all the notations in the figure, such as WCVG, and the equations. Additionally, the reference is very far away from Figure 2, which makes the whole paper hard to read. This eval is important, as it shows the necessity of using "multi level" attention. 2.There is a "word conditioned" visual graph network, why not the other way, "frame conditioned" semantic graph net and iterate over it?<BRK>Summary:This paper proposes a method for aligning an input text with the frames in a video that correspond to what the text describes in a weakly supervised way. The proposed method makes multiple comparisons while computing the attention weights over all words and frames. Determining the size of the sliding window:From reading the paper, it looks like the sliding window used for computing the word / frame relationships has to be manually defined. This seems a bit suboptimal for the generalizability of this method. Or at least perform the same? It would be nice if the authors successfully answer / address the questions / concerns mentioned above in the rebuttal.<BRK>The paper proposed a weakly supervised wMAN model for moment localization in untrimmed videos. The proposed model was evaluated on two publicly available dataset and achieved reasonable results. Pros:  Weakly supervised method for video moment localization is a reasonable and important direction. wMAN is evaluated with two publicly available datasets, and is compared with state of the art methods and other "oracle" baselines. The performance is impressive and could be a better baseline for the future work. PE seems to be important for wMAN, and the authors provides few sentences analysis about this, but I don t think I fully understand this part. Less technical comments: The paper writing is fine to me, but I don t like the typesetting. Overall, I think the paper is marginal above the accept line.<BRK>Given a video and a sentence, the goal of weakly-supervised video moment retrieval is to locate the video segment which is described by the sentence without having access to temporal annotations during training. The aforementioned mechanism is comprised of a Frame-By-Word interaction module as well as a novel Word-Conditioned Visual Graph (WCVG). Their approach also incorporates a novel application of positional encodings, commonly used in Transformers, to learn visual-semantic representations that contain contextual information of their relative positions in the temporal sequence through iterative message-passing.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. The paper proposed a pre training method for strengthening the contextual embeddings alignment. The authors also proposed to use the an regulation that prevents the learned embedding from drift too far. The authors evaluated the proposed pre training on the contextual alignment metric and show the BERT has variable accuracy depends on the language. The proposed method improved significantly on zero shot XNLI compares to the base model. The authors did a good job of analyzing the bert for multi lingual. I wonder is there any ablations study with respect to how the word pairs affect the pretraining?<BRK>This paper presents a new method to further align multilingual BERT by learning a transformation to minimize distances in a parallel corpus. I think that this is overall a solid work. From my point of view, the real "non contextual word retrieval" task would be bilingual lexicon induction (i.e.dictionary induction), which is more interesting as a task (as the induced dictionaries can have practical applications) and has been widely studied in the literature.<BRK>This paper conducts a series of experiments on the multilingual BERT model of Devlin et al., aiming to inject stronger bilingual knowledge into the model for improved  Aligned BERT . The knowledge originating from parallel (Europarl) data improves the model significantly as shown on tasks such as contextual and non contextual word retrieval as well as in zero shot XNLI task. The authors mention that it may not hold for  contextual pre trained models given their increased complexity . I would like to see more experiments in this space. Another experiment which would contribute to the paper is the analysis of the importance of parallel corpora size.<BRK>They propose procedures for evaluating and strengthening contextual embedding alignment and show that they are useful in analyzing and improving multilingual BERT. Further, to measure the degree of alignment, they introduce a contextual version of word retrieval and show that it correlates well with downstream zero-shot transfer.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. The authors propose a Graph Inference Learning (GIL) framework to learn node labels on graph topology. The node labeling is based of three aspects: 1) node representation to measure the similarity between the centralized subgraph around the unlabeled node and reference node; 2) structure relation that measures the similarity between node attributes; and 3) the reachability between unlabeled query node and reference node. The authors propose to use graph convolution to learn the node representation and random work on graph to evaluate the reachability from query node to reference node. The idea of the paper seems straightforward and the experimental results seems promising in semi supervised classification for nodes in graph data.<BRK>The paper suggests a learning architecture for graph based semi supervised learning. The input graph is given with only some labeled, and the goal is to label the rest. The architecture is trained on a validation set to learn features that optimize the accuracy of inference on the unlabeled node. All parts of the architecture seem reasonable, and the experiments report advantage over prior work, which counts in favor of the paper.<BRK>(The paper used different terms about the "attention".) The major technical novelty of this paper seems to lie in the introduction of this idea on graphs and the use of between node path information for the "attention mechanism". But only the results on Cora is given and results on all other datasets are missing. And it can easily achieve a test accuracy of 0.75+, which is better than the performance of the proposed method reported in the paper. In summary, this paper shows some technical novelty but the motivation behind the proposed method is not strong enough.<BRK>In this work, they address the semi-supervised classification of graph data, where the categories of those unlabeled nodes are inferred from labeled nodes as well as graph structures. Here they propose a Graph Inference Learning (GIL) framework to boost the performance of node classification by learning the inference of node labels on graph topology. To bridge the connection of two nodes, they formally define a structure relation by encapsulating node attributes, between-node paths and local topological structures together, which can make inference conveniently deduced from one node to another node. Comprehensive evaluations on four benchmark datasets (including Cora, Citeseer, Pubmed and NELL) demonstrate the superiority of their GIL when compared with other state-of-the-art methods in the semi-supervised node classification task.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. This paper proposes an approach to perform image translation called U GAT IT. Transgaga: Geometry aware unsupervised image to image translation. The experiments are thorough and the paper is well written. The paper is clearly organized and feels polished. There is a thorough description of model architecture, dataset and tuning parameters in the appendix. Weak points include novelty and significance.<BRK>* The paper proposes a new image to image GAN based translator that uses attention and a new normalization that learns a proper ratio between instance and layer normalization. Weak Accept* The paper was well written and the method and contributions are clearly explained. * There is clear novelty in this paper, even if slightly limited.<BRK>This paper proposes a new attention mechanism for unsupervised image to image translation task. I wonder why the proposed method only consists of instance norm and layer norm?<BRK>They propose a novel method for unsupervised image-to-image translation, which incorporates a new attention module and a new learnable normalization function in an end-to-end manner. Experimental results show the superiority of the proposed method compared to the existing state-of-the-art models with a fixed network architecture and hyper-parameters.
Reject. rating score: 3. rating score: 3. Also it is unclear to me how this method performs in the deep ensemble/Bayesian RNN case. The main arguments are the following:1. current metrics on dataset level cannot reveal uncertainty in prediction on personal level;2. when evaluated on personal level, deterministic NNs with different random initialisations can produce very different predictions (thus require consideration of model uncertainty)I am not exactly sure if ICLR is the best venue for this submission, as there is quite little innovation in modelling methodology, and the empirical analysis is domain specific.<BRK>I am not an expert in the field of model uncertaintySummary / contributions:This paper discusses the important problem of model uncertainty in the output of ML models developed for clinical applications. The other advantage is the fewer number of parameters that need to be stored to get such statistics. Is the point of the paper to make it more obvious? It is certainly the case that medicine practioners are not as aware of these issues, but to reach that audience this paper would do better in a venue that caters to that community.<BRK>Recent work has shown that changing just the random seed is enough for otherwise well-tuned deep neural networks to vary in their individual predicted probabilities. In light of this, they investigate the role of model uncertainty methods in the medical domain. Meanwhile, the presence of significant variability in patient-specific predictions and optimal decisions motivates the need for capturing model uncertainty. Understanding the uncertainty for individual patients is an area with clear clinical impact, such as determining when a model decision is likely to be brittle.
Reject. rating score: 3. rating score: 3. rating score: 3. So this contribution seems not practically useful according to the empirical result. The paper then uses an existing result from Liao & Berg (2017) to show that the gap can be bounded by the variance of prediction probability.<BRK>Moreover it can be even negative. Therefore, the Eq.10 for the gap from this paper is misleading. From this equality the authors derive that reducing the variance leads to decreasing the gap.<BRK>Deterministic seems to be good just for MNIST. Did the authors try other values of alphas? The entire paragraph is extremely convoluted. Introducing a conditional model and showing that the dropout objective is akin to MAP estimation of the parameters of this model is interesting.<BRK>The deterministic subvariant's bound is equal to its objective, and the highest amongst these models. Together, these results suggest that the predominant view of deterministic dropout as a good approximation to MC averaging is misleading.
Accept (Spotlight). rating score: 8. rating score: 6. The paper is also well written and the results are clearly presented. Overall, this is a nice contribution to spectral graph theory and so I recommend acceptance.<BRK>The paper explains through a block model the impact of the complete graph regularization, intended as adding to all the entries of the adjacency matrix a constant.<BRK>In this paper, they explain on a simple block model the impact of the complete graph regularization, whereby a constant is added to all entries of the adjacency matrix.
Reject. rating score: 3. rating score: 3. rating score: 8. Paper Summary: The paper studies what an embodied agent that is trained using RL learns in the context of the hide and seek game. There are two agents in a simple environment (one hider and one seeker). The paper attempts to analyze how the learned representation differs with different capabilities of the agents or environment structure. For example, it is strange that "visibilityreward" performs worst for "Awareness of Self Visibility". There are conclusions about "temporal events" (e.g., the hider learns to first turn away from the seeker then run away) just based on a single frequency number (Figure 4). There are many other possibilities that can result in the same frequency. If the authors draw conclusions about temporal events, they should show how values change over time. Significance:I expected a more rigorous analysis since this is an analysis paper and there is no new methodology in the paper. More thorough analysis based on quantitative results are required to justify the claims and provide generalizable conclusions. It is claimed that "When the model has a weakness, the model learned to overcome it by instead learning better features". To justify that, we need to know what the dots are. What is the standard deviation?<BRK>Visual Hide and SeekIn this paper, the authors propose a two agent hide and seek environment, and uses reinforcement learning to train the hider. The authors interpret the learned representation by using the learnt features to do classification. There is still a lot of potential improvement available for this project, which is summarized below. Pros:  The environment itself, once open sourced, can be quite valuable to the community. I think it is fair to say that the proposed environment in this project is better than the hide and seek environment from OpenAI in that it provides the visual input. By the way, I believe OpenAI environment is also partially observable, where unseen agents’ information is masked out. It is an interesting finding on how meaningful features and performance correlate with each other. Cons:  Findings in the project are very practical and interesting, but they do not seem to provide valuable information for future research. For example, to improve the quality of the paper, is it possible to utilize the features and performance correlation to improve unsupervised visual feature learning? There are a lot more potential to improve the environments. It will greatly increase the impact of the paper by, for example, considering multi agent training, self play, unsupervised training.<BRK>Comments from other reviewers and revisions have deliberately not been taken into account. Fig.2 is very interesting. ## Overall**Summary**The authors introduce a new RL environment and task, "Visual Hide and Seek", in which they analyze how the agent s learned visual representations are impacted by its speed, auxiliary rewards, and opponent behavior. I d highly recommend this paper get accepted since I believe the analysis carried out here and the conclusions reached are quite novel and the paper is overall well written. However, at the same time, the work of [Baker et al., 2019][1] was published with significantly more fanfare. I hope their work does not overshadow this one since they are only related in the general task concept. "We summarize representative cases, and put the full results for all combinations in the Appendix"   no you didn t.  Fig.6: What is going on in the left third of this diagram? There are some sections of the paper where the order of paragraphs is confusing. If this is by any chance indicating a change over time, do you maybe want to spread a single, very colorful plot of distance over time into multiple less colorful plots? The explanation is only given in the second paragraph. So I d suggest rotating the second paragraph upwards before the first. Also maybe add reward over time plots, as is common in DRL, to show that your policies converged after 8 mil. Why not 6 or 8 or a ResNet? Would you think the features would be stronger/weaker in an 8 layer CNN? [3]: https://www.cc.gatech.edu/~athomaz/classes/CS8803 HRI Spr08/MayaChandan/Site/Affordance_Learning.html### Experiments  4.1 "... learned this play game"  > "... learned this game". Tab.3: This is averaged over how many frames of rollout?<BRK>They train embodied agents to play Visual Hide and Seek where a prey must navigate in a simulated environment in order to avoid capture from a predator. Although they train the model to play this game from scratch without any prior knowledge of its visual world, experiments and visualizations show that a representation of other agents automatically emerges in the learned representation. Their results suggest that, although agent weaknesses make the learning problem more challenging, they also cause useful features to emerge in the representation.
Reject. rating score: 1. rating score: 3. rating score: 3. SummaryThis paper discusses the value of creating more challenging environments for training reinforcement learning agents. This paper makes many strong claims about the nature of intelligence that are neither supported in the work or are accepted in the community. The first of these properties is stochasticity in the environment transitions, specifically stochasticity that is independent of the action taken by the agent. The paper s intended contribution was different than I had realized during the initial review phase. I appreciate the effort the author put into the response and the changes made to the literature review section of the paper. To give a small example, imagine an agent wandering in a tabular gridworld. The continual learning and life long learning communities are focused exclusively on the problem of non episodic learning. There were a few key issues with the experiments discussed in this paper. To make claims about the state of a field the lit review should be rather extensive.<BRK>The paper discussed several properties of environments used in reinforcement learning research experiments. The main conclusion is the environment should be dynamic and non episodic, and the environment shaping is introduced to be effective. However, I have some concerns about the proposed method and experiments: 1. In general, I think the arguments in the paper are still a bit vague to be demonstrated using only experimental results. For instance, the paper can formulate the environment shaping and reward shaping concretely and prove that environment shaping could replace reward shaping.<BRK>The authors study what they refer to as ecological reinforcement learning, defined as the interaction between properties of the environment and the reinforcement learning agent. They introduce environments with characteristics that reflect natural environments: non episodic learning, uninformative reward signals, and natural dynamics that cause the environment to change. These factors are shown to significantly affect the learning progress of RL agents and, unexpectedly, the agents can sometimes learn more efficiently in these more challenging conditions. Clarity:The paper seems to be clearly written. While the experimental results show some light about the performance of existing methods in the proposed environment, the paper does not contain any methodological contributions. Because of this, it is hard to assess the novelty of the work.<BRK>Understanding the impact of specific environmental properties on the learning dynamics of reinforcement learning algorithms is important as they want to align the environments in which they develop their algorithms with the real world, and this is strongly coupled with the type of intelligence which can be learned. In this work, they study what they refer to as ecological reinforcement learning: the interaction between properties of the environment and the reinforcement learning agent. They show these factors can have a profound effect on the learning progress of reinforcement learning algorithms. Surprisingly, they find that these seemingly more challenging learning conditions can often make reinforcement learning agents learn more effectively.
Reject. rating score: 3. rating score: 6. rating score: 8. It s ok to have some contrived experiments, but it seems like all experimentson which you were able to provide evidence that your technique was helpful are contrived. It also seems like the baselines you used are not obviously the right choice for theseexperiments? Re: your CIFAR experiments:I think this supports my earlier claim that the extra parameters and bells and whistlesin modern GAN techniques *already implicitly do what your method is proposing to do*. Detailed comments on draft:It s worth noting that conditional GANs also sample from multiple disconnected `manifolds . I think a lot of people miss this point when writing papers about GANs. Then it seems like it could learn to assign 70, 50, and 30 of those modesto each one of the gaussians in your underlying data? I guess I haven t tried this myself, but it seems like a more fair comparison. > We next demonstrateWould be nice to have a new subesection here. See fig 3 of [2], in which it looks bad but not nearly as bad as you ve shown it. Again, I have a feeling that this is because of the small generator you use. I m pretty sure that a motivated person could get a normal GAN to model thedistribution in fig 5 reasonably well.<BRK>This paper proposes to improve GAN by learning a function that splits the latent space in several parts and then feed each part to a different generator, this enables GAN to model distribution with disconnected support. They also show how the method is robust to the choice of number of generators used. I m slightly in favour to accept the paper. I think the idea is well motivated and shows real advantage over other methods on the toy experiments. The major downside of the paper is that the proposed method doesn t seem to improve that much in more realistic setting. Main Argument:+ The idea is well motivated and the paper precisely explain that they try to address the problem of modelling data when the manifold is disconnected and the class are imbalanced. The author point to the fact that this might be due to the fact that the metrics we used are not sensible to outliers. I found the explanation of the proposed algorithm a bit confusing, it would be nicer if the final loss was clearly defined in the paper and the derivation of the loss explained.<BRK>One thing that I d like to see is a variant where the "different" generators share almost all of their parameters, but have different batch norm "mean/sigma" parameters. Review: This paper presents a simple yet well motivated new method for helping GANs to model disconnected manifolds. There are a few more explorations that I d like to see (discussed in comments), but I still appreciate this paper for directly addressing an important challenge. The 2D toy experiments do a good job of illustrating why the method helps and also the improvements on the "disconnected face/landscape" dataset are quite good. It is a bit disappointing that it doesn t help on CIFAR10, although I think it s reasonable to leave this for future work.<BRK>Recent work has demonstrated that GANs are consequently sensitive to, and limited by, the shape of the noise distribution. They address this problem by learning to generate from multiple models such that the generator's output is actually the combination of several distinct networks. They contribute a novel formulation of multi-generator models where they learn a prior over the generators conditioned on the noise, parameterized by a neural network. Thus, this network not only learns the optimal rate to sample from each generator but also optimally shapes the noise received by each generator.
Reject. rating score: 1. rating score: 3. rating score: 3. This paper considers the problem of text classification, especially the settings in which the number of labeled sentences is very small. However, authors assume, annotations of rationales behind the label, i.e.highlighting tokens in a sentence which are important in deciding its label. The idea proposed in the paper, even in the specific problem context considered, are incremental. I don t think that this kind of work aligns with the theme of learning representations. This paper may be suitable for publication in an NLP workshop as the baseline model.<BRK>It needs a through review of few shot learning. However, this is still not reflected in the paper in any way. If the answer is yes, it should be. I disagree with the claim that the model is interpretable. The authors should discuss the existing few shot learning mechanisms. I am assuming it is done using full gradient descent over all parameters. In the mean time, It is not ready for publication.<BRK>While neural networks can also be analyzed in different ways, I agree with the authors that this is nice to have. Overall, the paper seems solid. The main motivation to use PARCUS is that it works better in a low resource setting than recent state of the art models for the high resource case. Furthermore, for training, PARCUS makes use of rationales.<BRK>They propose a model to tackle classification tasks in the presence of very little training data. To this aim, they introduce a novel matching mechanism to focus on elements of the input by using vectors that represent semantically meaningful concepts for the task at hand. In addition, the model is interpretable, as it allows for human inspection of the learned weights.
Reject. rating score: 1. rating score: 3. rating score: 6. In this setting, the authors propose a neural architecture inspired by message passing operations in deep probabilistic graphical models. The internal layers of the architecture consist of propagation, decimation and prediction steps. Overall, the paper is relatively well written and well positioned with respect to related work. In the SAT problem, we have a CNF formula, say $F$, and the goal is to predict whether $F$ is satisfiable or not. However, unless I missed something, the PDP architecture returns as output a set of $T$ “soft assignments” for each input SAT instance, which leads to two major concerns: * There is no final decision (SAT/UNSAT), so how can we predict the satisfiability of an instance $F$ with just a set of $T$ assignments? * Furthermore, the output set consists of “soft” assignments, as defined by (4). Are the authors using a rounding method? For the sake of reproducibility, this should be mentioned in the revised version of the paper. Furthermore, it seems that at first sight, PDP is competitive with Glucose, as illustrated in the left part of Figure 1.<BRK>The paper presents an approach, PDP, to solve Boolean satisfiability (SAT) by decomposing it into Propagation, Decimation and Prediction, where each can be learned with a neural network. Surprisingly, NeuroSAT cannot handle the problems studied in this work. To allow a better comparison to prior work, I am wondering why the author did not compare in a setting and dataset prior work evaluated SAT solvers. 1.2.It would be interest to know if and how the proposed model performs on the problems evaluated in Selsam 2019. I am borderline on this paper, also given my limited knowledge of the field.<BRK>The method consists of an energy based loss function which is optimized by a three stage architecture that performs propagation, decimation, and prediction (PDP). The authors show that on uniform random 4 SAT problems, their PDP system outperforms two classical methods, a prior neural method, and performs favorably in comparison to a heavily developed industrial solver. This well written paper introduces an appealing unsupervised method for learning solvers for an important class of problems. Their results seem to be much stronger than the prior neural state of the art, which also has the downside of requiring labelled data. Overall, this seems to me like a useful contribution. Currently my accept recommendation is weak only because I m not familiar enough with this area to verify that there are not other prior work comparisons that should have been included.<BRK>On the other hand, by fixing the search strategy (e.g.greedy search), one would effectively deprive the neural models of learning better strategies than those given. In this paper, they propose a generic neural framework for learning SAT solvers (and in general any CSP solver) that can be described in terms of probabilistic inference and yet learn search strategies beyond greedy search. Their framework is based on the idea of propagation, decimation and prediction (and hence the name PDP) in graphical models, and can be trained directly toward solving SAT in a fully unsupervised manner via energy minimization, as shown in the paper.
Reject. rating score: 1. rating score: 1. rating score: 1. rating score: 3. However, both Reviewer #2 and Reviewer #3 mentioned the same issues, saying "much of the technical leverage exploited in this paper comes from earlier work...results a lot like Proposition 2 can be found in [Cavazza et al.]" (#2), and "I am surprised that [Cavazza et al.] Similarly, the generalization bounds are not shown to be useful. However, the authors have not posted a revised draft or any sample text , leaving me to keep my recommendation at  reject. Due to the paper s lack of discussion and, at times, mischaracterization of previous work, the text needs to be significantly revised before it can be accepted. The paper first focuses on matrix sensing, showing an explicit regularizer with connections to trace norm regularization and proving a generalization bound. In effect, this equates Cavazza et al.[AIStats 2018]’s work with much less related work (e.g.Bayesian interpretations). This work is not cited another significant oversight. In general, the paper makes several claims that are at best ungenerous to previous work. Yet this essentially reduces the results to a study of dropout in linear models (except perhaps in Prop 4) and therefore I don’t see how one could claim the previous work of Wager et al.[NeurIPS 2013], which also studies dropout for linear models, doesn t address similar questions.<BRK>They conduct experiments showing that dropout improves over SGD without dropout, and plotting generalization gaps and their bounds. Much of the technical leverage exploited in this paper comes from earlier work. The authors  claim that "changing the learning rate or the batch size does not significantly improve theperformance of any of these algorithms" is a little hard to believe. My impression is that these choicesaffect the implicit regularization of SGD (along with the initialization). Some more detail about whatthey tried would be helpful. There is some interesting new content in the paper, even if, on the whole, it is a bit conceptually and technicallyincremental.<BRK>For both matrix sensing task and linear regression task, authors derive an explicit regularization term due to dropout. Authors then give a generalization bound for matrix completion with dropout. In the context of deep neural networks, under assumptions on the input distribution authors show that the explicit regularizer associated with dropout is exactly the squared l2 path norm of the network. Following are my concerns:1. I believe that this is not true for most of the neural networks with RELU nonlinearity for the output units. In particular, the theorems derived in the paper has assumptions on the sample complexity. These assumptions are not verified for the datasets used in the experiment section and I suspect that lower bound on n can be too large for the bounds to be meaningful.<BRK>Firstly, the idea of using drop out for matrix sensing seems to be a somewhat trivial extension of the work on dropout for matrix factorization   http://proceedings.mlr.press/v84/cavazza18a/cavazza18a.pdf. I am surprised that this work is not cited with its due credit in the writing. The derivations also suggest so and the authors deserve credit for attempting these derivations. Overall, the paper appeared quite promising in the beginning, but the claims in the introduction are not well supported through the rest of the paper.<BRK>First, they study dropout for matrix sensing, where it induces a data-dependent regularizer that, in expectation, equals the weighted trace-norm of the product of the factors. In deep learning, they show that the data-dependent regularizer due to dropout directly controls the Rademacher complexity of the underlying class of deep neural networks. These developments enable us to give concrete generalization error bounds for the dropout algorithm in both matrix completion as well as training deep neural networks.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. The authors propose in this paper a variant of Deep SVDD which brings semi supervision to this model. But then the distribution of phi(x,W) cannot be approximated by a Gaussian for anomalies and thus the bound on the entropy is not valid.<BRK>[Comments]The paper is well written and easy to follow (the presentation is especially pleasant to read). The solution is implemented by the encoder of a pre trained autoencoder that is further fine tuned to enforce entropy assumption on all types of training data. Will this help to achieve a better result?<BRK>I think the paper is well written and the experiment seems to support the authors argument.<BRK>Deep approaches to anomaly detection have recently shown promising results over shallow methods on large and complex datasets. In practice however, one may have---in addition to a large set of unlabeled samples---access to a small pool of labeled samples, e.g.a subset verified by some domain expert as being normal or anomalous. In this work they present Deep SAD, an end-to-end deep methodology for general semi-supervised anomaly detection.
Reject. rating score: 1. rating score: 1. rating score: 3. This paper proposed a method to improve the image to image translation. By utilizing the CG based Synthia and embedded edge maps, it has shown some effects on the Cityscapes dataset. This paper is not novel. It is difficult to generalize to another dataset/domain as the CG based set is not easy to get. The model side has very limited novelty. The pipeline and motivation are similar to previous work for domain adaptation for generative models and bridging domains:1. Adaptation Across Extreme Variations using Unlabeled Domain BridgesThe experimental study is weak. It is good to see if this method can work on others, like ADE20k and COCO stuff.<BRK>SUMMARY: Use HED to convert semantic maps to edge maps, then use GAN to generate realistic images, by adding several losses together. Paper could be written much better, it is slightly confusing to follow. The only innovation is to have a middle step of converting these semantic maps to edge maps, and then from edge maps to realistic images. This is not altogether a big change, I was hoping to see it goes in a good direction. As far as I understand, the DNED model the authors use is a slight change to the HED model the authors cite, in that they sample different weights to combine to form different outputs. Since architecturally there is not much difference in the already existing models, maybe focus on more use cases of edge maps. What is the advantage of using edge maps as an intermediate step, as opposed to some other representation? The video results look good, and this is one example of exploiting the fact that edge maps are an intermediate step. However, more work needs to be done to support claims of diversity.<BRK>Authors use edge maps to generate more realistic images and videos in GANs. They train a GAN in a conditional way togenerate a photo realistic version of a given scene. In my opinion, the novelty of the proposed approach is not high and the improvements are incremental. I have some concerns about the method as well: what happens if the quality of edge maps is not good for some datasets? Is it realistic to assume that the edge maps of real images are provided with high quality? Regarding the video generation, authors should compare their approach with recent video GAN papers such as https://arxiv.org/abs/1907.06571There are several typos in the paper as well.<BRK>Their work offers a new method for domain translation from semantic label mapsand Computer Graphic (CG) simulation edge map images to photo-realistic im-ages. They train a Generative Adversarial Network (GAN) in a conditional way togenerate a photo-realistic version of a given CG scene. Existing architectures ofGANs still lack the photo-realism capabilities needed to train DNNs for computervision tasks, they address this issue by embedding edge maps, and training it in anadversarial mode. They also offer an extension to their model that uses their GANarchitecture to create visually appealing and temporally coherent videos.
Reject. rating score: 3. rating score: 3. This paper introduces a simple measure of tunability that allows to compare optimizers under varying resource constraints. If I understand correctly, they are not the number of trails. In addition, the proposed stability metric seems not quite related with the above intuitions, as the illustrations (1.a and 1b) define the tunability to be the flatness of hyperparameter space around the best configurations, but the proposed definition is a weighted sum of the incumbents in terms of the HPO budgets. The definition of the tuning budgets is not clear, is it the number of trials or the time/computation budgets? My concern is that is highly dependent  on the order of hyperparameter searched, which could impact the tunability significantly. Could the authors be more specific on the hyperparameters searched? Is this process counted in the tunability measurement?<BRK>The metric is defined as a weighted average of the performance after tuning with i random trials, with i that goes from 1 to K. The weights of this weighted average and K are "hyper parameters" of the metric itself. The paper appears to treat 2. as the main contribution. However, I do not think the metric they introduce is good enough to be recommended in future work, when comparing tunability of optimizers (or other algorithms with hyperparameters). Comparisons which, while mentioned, should perhaps have been discussed and compared more in detail in this work. I enjoyed reading the submission, which is very clearly written, but due to the relatively limited value of the contributions, and excessive focus on the tunability metric which I do not feel is giustified, I slightly lean against acceptance here at ICLR. I think it would be worth discussing this more.<BRK>In this work, they fill in the important, yet ambiguous concept of ‘ ease-of-use ’ by defining an optimizer ’ s tunability: How easy is it to find good hyperparameter configurations using automatic random hyperparameter search? Evaluating a variety of optimizers on an extensive set of standard datasets and architectures, they find that Adam is the most tunable for the majority of problems, especially with a low budget for hyperparameter tuning.
Reject. rating score: 3. rating score: 3. rating score: 6. The paper proposes learning NN to correct for inaccuracies in numerical solvers of PDEs, with experimental focus on fluid flow simulation. The paper tackles an important problem of using NN to speed up expensive simulation computations. This paper alone does not seem to have enough novel ML contributions for acceptance. It seems to be based on v_R and v_B but would benefit from being explicit. Is it a typo? How is the assumption different from Sections 3.1 and 3.2?<BRK>Clarity  I think this paper is well written in most places, but the paper is not intended to make it easy for general machine learning researchers to understand and follow the problem thoroughly. In this paper, the authors propose the model that assists PDE solver by correcting residuals in a data driven way. Novelty  I think this approach is somewhat novel since it introduces new direction in the field where the model assists PDE solver to improve performance with learned correction function.<BRK>The authors aim at improving the accuracy of numerical solvers (e.g.for simulations of partial differential equations) by training a neural network on simulated reference data. The neural network is used to correct the numerical solver. This is then trained in a supervised manner.<BRK>This paper presents a data-driven approach that learns to improve the accuracy of numerical solvers.
Reject. rating score: 1. rating score: 3. rating score: 3. rating score: 8. The paper proposes an approach to generalization for deep networks based on kolmogorov complexity. The normalized information distance and its approximation via a compression algorithm were developed in earlier work, as noted by the authors. So the main contribution seems to be framing the deep learning classifier as a source code and developing a method to minimize the proposed information distance to improve generalization. If the C f  is a map from X_i  > y, X_i is an image, and y is a scalar, the source code as defined is not encoding X_i, it is simply encoding a part of X_i that is relevant to the classification task. 1.The claim " Because a sufficiently high capacity neural network can memorize its input samples the Kolmogorov complexity of the true source code is larger than that of the learned source code: i.e., K(C) > K(C~)." If K(C~) is increased, then I think one needs to show that K(C~)< K(C) even with the encodings added? So, the insight about minimizing information distance computed between C and C~ applies when the learned and the true "source code" correspond to the new task. 3.For the adversarial robustness claims, I don t believe that the justification provided in the paper is the necessarily only one.<BRK>The main idea is to cast the problem of improving generalization as the problem of minimizing the normalized information distance between the learned source code and the true source code, defined using the Kolmogorov complexity. Built upon this framework, the method of using extended encodings of an input sample is presented, which empirically seems to lead to better generalization in the settings of adversarial attack and corruptions. For example, in Equation (3), the authors claim that "a necessary and sufficient condition ensuring that ... the learned source code C_0 is more general than the learned source code C1 is ...". This, to me, is more like some intuition or conjecture, rather than rigorous mathematical proof. As a result, I am skeptical about the theoretical claims made in the paper.<BRK>Specifically, the inputs are augmented with additional features formed by encoding the image (using a source code). (In particular, evaluating the Kolmogorov complexity, which is required to compute the normalized information distance, is not tractable.) The existing experiments do make clear that the encoded inputs are more robust to perturbations and aversarial attacks than uncoded inputs. I believe that this paper contains interesting and novel ideas, but there are also some inconsistencies and some points that are vague. Is the encoding assumed to be performed on the perturbed input, or on an unperturbed input (i.e., before it is perturbed)? The intro also mentions briefly about adversarial examples, and this is the focus of the experiments. Minor: Please clarify the definition of "inference accuracy" in the paper. Also, what loss criterion is used for training? This isn t clear from the discussion in Sec 3.<BRK>This paper provides a very interesting viewpoint for understanding the generalization in deep learning, where the concept of generalization is defined as “the difference between training error and inference error”, and covers the concept of adversarial robustness. The author treats the deep model as a source code, and provides some theoretic analysis based on the Kolmogorov complexity. 3.The experiments show that using additional encodings improve robustness in several settings, including in the sense of adversarial robustness. 3.The empirical method that the authors proposed is simple, and computationally friendly. The normalization seems important for the theoretical analysis, so it would be better to explain where this definition comes from more clearly.<BRK>Deep artificial neural networks can achieve an extremely small difference between training and test accuracies on identically distributed training and test sets, which is a standard measure of generalization. To address this problem, they first reformulate a classification algorithm as a procedure for searching for a source code that maps input features to classes. They then derive a necessary and sufficient condition for generalization using a universal cognitive similarity metric, namely information distance, based on Kolmogorov complexity. Using this condition, they formulate an optimization problem to learn a more general classification function. As an illustration of this idea, they focus on image classification, where they use channel codes on the input features as a systematic way to improve the degree to which the training and test sets are representative of the empirical sample set.
Reject. rating score: 1. rating score: 3. rating score: 3.   SummaryThis paper proposes to learn a visual tracking network for an object detection loss as well as the ordinary tracking objective for enhancing the reliability of the tracking network.<BRK>This paper investigates representations learned by Siamese trackers. An auxiliary detection task is proposed to induce stronger target representations in order to improve tracking performance. However, the proposed solution of just integrating an additional detection task branch within the Siamese tracking architecture is naive. Some recent works, such as [2, 3] have also investigated a similar problem of richer object representations for deep visual tracking. The paper shows some qualitative analysis. Such analysis is missing in the paper. For instance, the main argument of this paper is that current approaches rely on center saliency and likely struggle in the presence of occlusion. Although the model doesn’t outperform state of the art, it attains competitive performance."<BRK>I understand that training a state of the art tracker with the additional object detection branch could possibly require resources beyond what’s available to the authors. The only real effect the detector shows in Figs. 6–8 not very helpful/conclusive  No test case where capturing appearance is important for tracking and authors’ approach helps  No comparison of tracker trained without detection objective on real vs. random targetsThe paper is well motivated and has a clear hypothesis. The same quantitative analysis could also be done for existing state of the art trackers, as it does not require training.<BRK>Fully convolutional deep correlation networks are integral components of state-of-the-art approaches to single object visual tracking. It is commonly assumed thatthese networks perform tracking by detection by matching features of the objectinstance with features of the entire frame. Ouranalysis shows that despite being a useful prior, salience detection can prevent theemergence of more robust tracking strategies in deep networks. This leads us tointroduce an auxiliary detection task that encourages more discriminative objectrepresentations that improve tracking performance.
Reject. rating score: 3. rating score: 8. rating score: 8. Authors of this paper present architecture to produce valid Euclidean distance matrices. Generating molecular structures in a one shot fashion is conducted using the produced distance matrices in 3 d embedding. In Section 2, the constraint on L makes M symmetric and positive semi definite. This seems to be equivalent to treating M as a kernel matrix, and D is the pairwise distance between the kernel function induced by M. So, learning a valid Euclidian distance matrix is same as learning a kernel function. Although this paper is an application oriented paper, the comparisons with baseline methods are preferred, such as some simple and straightforward baselines.<BRK>The paper is extremely interesting, solid and very well written. The idea is simple but nonetheless developed in a smart and effective fashion. The main issue related to the manuscript is very narrow target of the experimental part, limited to the isomers of a given compound   it would have been interesting to check its potentialities in generating more different structures and distance matrices, and thus to compare its effectiveness versus alternative generative approaches.<BRK>	This paper addresses the important problem of molecular structures generation, and more generally of efficient point cloud distributions learning in d dimensional space. But currently I believe that the authors are a bit too humble about their results (a rather uncommon phenomenon). One is that of sampling valid (Euclidean) distances matrices (EDM). The paper uses these EDM to train a generator G directly in EDM space, against a Critic network C (the architecture of which is taken from existing literature). Some of the output configurations have to be discarded due to incorrect bond types assignment (this is the part that is still obscure to me). I think this test and the corresponding result should be emphasized more. At least in the appendix. However, discovery of new structures (and their conformations) is in itself a big topic. For instance, it would seem rather natural to me to include some equivalent of OpenBAbel and Energy estimates within the learning loop, so that the generator directly generates valid (open babel wise) and reasonable (energy level wise) structures.<BRK>Meanwhile, neural networksutilizing symmetry invariant layers have been shown to be able to optimize theirtraining objective in a data-efficient way. Motivated by the goal to generate molecular structures in Cartesian space, they usethis architecture to construct a Wasserstein GAN utilizing a permutation invariant critic network. This makes it possible to generate molecular structures in aone-shot fashion by producing Euclidean distance matrices which have a three-dimensional embedding.
Reject. rating score: 3. rating score: 3. rating score: 6. This paper talks about the problem of off policy or batch learning in the contextual bandit setting without the complete support assumption. While the problem being solved is very relevant and their approach compares three different approaches to the deficient support problem, I am not sure how this work is positioned with respect to approaches solving similar problems in the reinforcement learning land. For example, Batch constrained Q learning ([1]) restricts the set of actions that can be used, Bootstrapping Error Accumulation ([2]) and SPIBB ([3]) restrict the policy class in batch reinforcement learning.<BRK>This paper considers a new off policy contextual bandit method that can learn even when the logging policy has deficient support. Three approaches are explored, namely restricting the action space, reward extrapolation, and restricting the policy space. This paper is well written and it considers an important problem of deficient support. How does the proposed method compare to more recent state of the art off policy bandit approaches (Liu et al.(2019), Xie et al.(2019), Tang et al.(2019)) in the experiments? The work by Liu et al.(2019) also considered the setting of deficient support. arXiv:1904.08473, 2019.<BRK>This work addresses the problem of off policy evaluation in the presence of positivity violations, i.e.some actions are not observed in the logged policy. The authors propose three methods to deal with this problem. I found a few pieces of this paper confusing. In section 3.2 it is proposed that a surrogate reward function be used for actions with unknown support, but the left hand side of the equation would seem to imply that the ratio still needs to be known in order to get an estimate. Perhaps an indicator function is missing? It is also not made plain what assumptions are being employed in order to allow for extrapolation. Overall, I think this is a promising approach (the empirical results certainly bare that outO but to my eyes it lacks sufficient detail and specificity.<BRK>They therefore develop new off-policy contextual-bandit methods that can controllably and robustly learn even when the logging policy has deficient support. To this effect, they explore three approaches that provide various guarantees for safe learning despite the inherent limitations of support deficient data: restricting the action space, reward extrapolation, and restricting the policy space. They analyze the statistical and computational properties of these three approaches, and empirically evaluate their effectiveness in a series of experiments. They find that controlling the policy space is both computationally efficient and that it robustly leads to accurate policies.
Reject. rating score: 3. rating score: 3. rating score: 6. This paper proposes a technique to generate audio adversarial examples based on iterative proportional clipping. Overall, the work is interesting but I have the following concerns. Thanks for the samples.<BRK>This paper introduces an adversarial attack mechanism, Iterative Proportional Clipping (IPC), based on a differentiable MFCC feature extractor. The authors claim about perceptual properties of the method seem to be somewhat anecdotal and, while I think the method is sensible, just showing the power spectrum is not very informative. In contrast to other audio attacks, the authors claim that the perturbations are well masked and are much less audible.<BRK>This paper proposed a new method for generating adversarial examples for the task of automatic speech recognition. Overall this paper is an incremental research work. The idea is intuitive and well presented. However, the changes to the signal are highly noticeable.<BRK>Audio adversarial examples, imperceptible to humans, have been constructed to attack automatic speech recognition (ASR) systems. This paper proposes a new approach to generate adversarial audios using Iterative Proportional Clipping (IPC), which exploits temporal dependency in original audios to significantly limit human-perceptible noise. They show that the proposed approach can successfully attack the latest state-of-the-art ASR model Wav2letter+, and only requires a few minutes to generate an audio adversarial example.
Reject. rating score: 1. rating score: 1. rating score: 1. This paper argues that BLEU and ROUGE, two metrics that are used for the evaluation of machine translation and text summarization systems, are flawed, and proposes a new JAUNE metric to replace it. The authors train a regressor on the STS B dataset, and show that their model (which is using sentence embeddings from RoBERTa) corresponds better to the ground truth similarity labels than then scaled (but otherwise unchanged) BLEU scores. I could agree with many of the problems that the authors describe, but the proposed solution seems to be a very specific solution that works on a given dataset (for which supervised training data is available), but I do not think it will generalize well to unseen test data in different domains. I also do not understand how the BLEU score can simply be rescaled from 0 5   how do you determine the maximum BLEU score before rescaling?<BRK>  Summary  The authors motivate the development of new (automatic) metrics to evaluate language generation by using similarity with a given reference: standard metrics like BLEU, ROUGE or METEOR have been shown to have poor correlation with human judgment on a number of tasks and are vulnerable to changes in word re ordering, semantics changing word replacement, and syntactic transformations. They then propose a multi dimensional evaluation criteria to evaluate sentence similarity based on semantic similarity (something that correlates with human judgments of the same), logical equivalence and fluency. Decision  The problem this paper seeks to tackle is clearly one of greatimportance in the field, but I find it hard to argue that this papersignificantly contributes to the existing body of work (more on thisbelow) and as a result I vote to reject this paper. There are two possible contributions for this paper: a set of criteria for what makes a good evaluation metric and the concrete proposals to implement these criteria. For the first, I find the proposed criteria to be overly generic and not helpful at providing additional clarity on what makes for a good evaluation: for example, how is semantic similarity different from logical consistency?<BRK>Observing shortcomings of BLEU and ROUGE, the paper proposes, JAUNE, a set of criteria for a good evaluation metric. The paper, as its current form, is not ready for publishing. Some suggestions and comments:  Please carefully check the paper and fix typos and confusing sentences. I was collecting these errors but eventually stopped. Is your criticism only about BLEU and ROUGE, or the state of the arts in NLP evaluation in general? While the authors suggest a data driven metric, it reads to me like a model driven metric (RoBERTAa specifically). Doesn t it systematically bias towards a certain family of metrics? Better and more comprehensive experimental results are highly desired.<BRK>They review the limitations of BLEU and ROUGE--the most popular metrics used to assess reference summaries against hypothesis summaries, and introduce JAUNE: a set of criteria for what a good metric should behave like and propose concrete ways to use recent Transformers-based Language Models to assess reference summaries against hypothesis summaries.
Reject. rating score: 3. rating score: 3. rating score: 3. This paper proposed a new query efficient black box attack algorithm using better evolution strategies. after the rebuttalI thank the authors for their response but I still feel that there is a lot more to improve for this paper in terms of intuition and experiments. The experimental results show that the proposed method achieves state of the art attack efficiency in black box setting.<BRK>This paper proposed a DFO framework to generate black box adversarial examples. I am Okay with the design of $\ell_\infty$ attack. This is also not clear to me. ########### Post feedback ##############Thanks for the response and the additional experiments to address my first question.<BRK>This paper proposes a black box adversarial attacks to deep neural networks. The experimental results look quite promising, i.e., revealing the vulnerability of the deep neural network against black box adversarial attacks. A possible weakness in the experimental design is that the authors haven t apply any defense methodology to the classification models to be attacked. The current paper simply replaces the bandit with evolution strategies. Nonetheless, I believe the combination of these constraint handling technique and evolutionary approaches are not new.<BRK>They introduce a new black-box attack achieving state of the art performances. Their approach is based on a new objective function, borrowing ideas from $\ell_\infty $-white box attacks, and particularly designed to fit derivative-free optimization requirements. Not only they introduce a new objective function, they extend previous works on black box adversarial attacks to a larger spectrum of evolution strategies and other derivative-free optimization methods.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. The paper presents a case study of training a video classifier using convolutional networks, and on how the learned features related to previous, hand designed ones. The particular domain considered is of importance for biologists/medical doctors/neuroscientists: zebra fish swim bout classification. In order to identify which particular features the neural networks are paying attention to, the paper used Deep Taylor Decomposition, which allowed the authors to identify "clever hans" type phenomena (network attending to meaningless experimental setup differences that actually gave a way the ground truth classes). This allowed for the authors to mask out such features and make the network attend to more meaningful ones. Overall the paper is well written, the experiments are well designed; everything seems very rigorous and  well executed. It makes for a very good quality practitioner level case study on video understanding, which may also be useful for people studying zebra fish or related simple life forms.<BRK>SUMMARY: explore the use of CNN in a binary task on images of zebrafishIt is important to note that researchers in the field of AI and deep learning are themselves aware of the fallacies of deep learning, and are striving everyday to overcome these themselves. This is evident in this paper with the authors calling CNNs "black box" and the learnings of a neural network "cheating". Perhaps the authors are also not aware that the fallacies that causes CNNs to overfit on some characteristics in the input data are also present in other machine learning tools such as SVMs. Perhaps the intention of the authors is to bring more relevance to the dangers of spurious correlations, especially when the applications are critical. BACKGROUND:Hypothesis: Prey movements in zebrafish are characterized by specific motions, that are triggered by a specific pathway involving an area called AF7. The level of detail in the training procedure is very helpful to reproduce the setting as well as establish a reference for any future work in this direction. Furthermore, the authors conduct one good analysis (DTD) to explain the results of their CNN.<BRK>the paper uses model interpretation techniques to understand blackbox CNN fit of zebrafish videos. it is also able to detect the use of experimental artifacts, whose removal improves predictive performance. the idea of a case study about the usefulness of model interpretation techniques is interesting. while the experimental studies rely on our belief that the interpretation technique indeed interprets, the result that removing experimental features and improving predictive performance is convincing and interesting. it illustrates how model interpretability and human intuition and domain knowledge can be useful.<BRK>Semmelhack et al. (2014) have achieved high classification accuracy in distinguishing swim bouts of zebrafish using a Support Vector Machine (SVM). Reaching better transparency helps to build trust in their classifications and makes learned features interpretable to experts. Using a recently developed technique called Deep Taylor Decomposition, they generated heatmaps to highlight input regions of high relevance for predictions. They find that their CNN makes predictions by analyzing the steadiness of the tail's trunk, which markedly differs from the manually extracted features used by Semmelhack et al. (2014).We further uncovered that the network paid attention to experimental artifacts. Removing these artifacts ensured the validity of predictions.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. This work proposes to detect adversarial examples or otherwise corrupted images with the reconstruction network, which is used to regularise CapsNets. The comprehensive experiments are conducted. Although the idea is not very novel, the paper makes enough contributions to get accepted. Especially, Section 6 diagnoses the adversarial examples for CapsNets and shows the relationship between the success of the attack and the visual similarity between the source and target class. We have the following question for authors about this work:1. It is a more comprehensive evaluation metric for such a problem.<BRK>This paper studies the problem of detecting and generating adversarial images using class conditional capsule networks. Specifically, this paper first introduced a novel method that detects adversarial examples by class conditional image reconstruction. Results demonstrate the effectiveness of the proposed defense and the novel reconstructive attack method. Second, the proposed proxy based on l_2 image distance might not be effective at all for higher resolution images. (3) It looks like the proposed method is not specific to generative models use class labels as condition.<BRK>This paper proposed a new defense method for capsule networks. The visualizations of adversarial examples generated by the CapNets are more aligned with the human perception which is very insightful. The author should provide more visualizations on other datasets such as CIFAR 10 to support the contribution that the features captured by CapsNets are more aligned with human perception than CNNs. Overall the paper is clearly written and easy to follow.<BRK>Adversarial examples raise questions about whether neural network models are sensitive to the same visual features as humans. In this paper, they first detect adversarial examples or otherwise corrupted images based on a class-conditional reconstruction of the input. Then, they diagnose the adversarial examples for CapsNets and find that the success of the reconstructive attack is highly related to the visual similarity between the source and target class. This suggests that CapsNets use features that are more aligned with human perception and have the potential to address the central issue raised by adversarial examples.
Reject. rating score: 1. rating score: 3. rating score: 6. They have justified a new sender receiver game that can be tuned for various levels of competition which then allows them to analyze the effects of various levels of cooperation and competition. But I am concerned that the new setting proposed in this paper seems like a  toy setting  to investigate if emergent communication would happen. It is difficult for me to assess the significance of these results since the authors have not presented real world scenarios and experiments that demonstrate the importance of selfish communication. The introduction of the circular game is suspect.<BRK>"we can look to extant results"   s/extant/extent? > So?It would still be interesting to see what learning agents do in this setting. [1]: "Learning with Opponent Learning Awareness", Foerster et al.[update: I have updated the score based on the discussion with the authors]. While the paper lacks execution and conceptual clarity, I believe the game itself is interesting and could serve as a starting point for more thorough investigation. Also, note that the loss is also differentiable with respect to the action of the 1st agent.<BRK>This ICLR submission deals with a problem of whether selfish agents can learn to use an emergent communication channel, using a sender receiver game as a case study. This review is delivered with the caveat that I am not an expert in this particulat field. The investigation seems relevant and the paper is well written and structured, being within the scope of the conference. The literature review is up to date and seems overall relevant.<BRK>They introduce a new sender-receiver game to study emergent communication for this spectrum of partially-competitive scenarios and put special care into evaluation. They find that communication can indeed emerge in partially-competitive scenarios, and they discover three things that are tied to improving it. Second, that stability and performance are improved by using LOLA (Foerster et al, 2018), especially in more competitive scenarios.
Accept (Spotlight). rating score: 8. rating score: 6. I think the paper is written quite well, and the approach makes a lot of sense. I think the idea of replacing generalizing the KL to sliced Cramer distance is quite interested. I think overall this is a great paper, very informative. It is though widely used in the continual learning community, though maybe it should not anymore.<BRK>[Summary]This paper proposes a new method for overcoming catastrophic forgetting in continual learning, based on distribution based regularization using the sliced Cramer distance, i.e.Sliced Cramer Preservation (SCP). Unlike previous work on catastrophic forgetting, this paper tackles unsupervised learning scenarios as well as supervised learning.<BRK>Deep neural networks suffer from the inability to preserve the learned data representation (i.e., catastrophic forgetting) in domains where the input data distribution is non-stationary, and it changes during training. Various selective synaptic plasticity approaches have been recently proposed to preserve network parameters, which are crucial for previously learned tasks while learning new tasks. They propose the sliced Cram\' {e} r distance as a suitable choice for such preservation and evaluate their Sliced Cramer Preservation (SCP) algorithm through extensive empirical investigations on various network architectures in both supervised and unsupervised learning settings.
Reject. rating score: 1. rating score: 1. rating score: 1. [updated rating due to supervision of $c_i$, which was not made clear enough and would require other baseline models]This paper proposes a modification of the usual parameterization of the encoder in VAEs, to more allow representing an embedding $z$ through an explicit basis $M_B$, which will be pushed to be orthogonal (and hence could correspond to a fully factorised disentangled representation). There is still an interesting philosophical discussion to be had about when one would like to obtain a “global basis” for the latent space (i.e.Figure 3 (b)), or when one would prefer more local ones. 1.The main question I have, which may be rather trivial, is “are the c_i supervised in any way?”. I.e.how was “5_o_clock_shadow” attached to that particular image at the top left? 2.There is not enough details about the architecture, hyperparameter and baselines in the current version of the paper.<BRK>This paper proposes BasisVAE for acquiring a disentangled representation of VAE. In particular, the following points need justified and clarified. 1) Theorem 1 is difficult to follow. The claim of the theorem is unclear. I suppose it says ELBO can be written as a sum with respect to z_i given p(z) \prod_i p(z_i), but the statement is not clear enough from the text. For computing this term, the output of encoder c f(x) should be converted into z. Is this an expected result?<BRK>Summary:This paper claims to achieve disentanglement by encouraging an orthogonal latent space. I found the paper difficult to read and the theoretical claims problematic. It would suggest thatp(x | a b)   p(x | a) p(x | b) / p(x). In particular the object p(x | z_i) is the integral of p(x, z_not_i | z_i) d z_not_i, which is quite non trivial. I do not think the authors provided a sufficient justification for how this model relates back to Theorem 1. Then M f(x)   g(x), which reduces to training a beta VAE (if using Eq 12). Issue 3: The ExperimentsExperimentally, the main question is whether the authors convincingly demonstrate that BasisVAE achieves better disentanglement (independent of whether BasisVAE is theoretically well understood). They are surprisingly small.<BRK>The proposed model not only defines the latent space to be separated by the generative factors, but also shows the better quality of the generated and reconstructed images. The disentangled representation is verified with the generated images and the simple classifier trained on the output of the encoder. In this paper, they propose a method to decompose the latent space into basis, and reconstruct it by linear combination of the latent bases.
Reject. rating score: 3. rating score: 6. rating score: 8. Strength:This paper proposes a new reweighted RNN by unfolding a reweighted L1 L1 minimization problem. This paper provides the generalization error bound for deep RNNs and shows that the proposed reweighted RNN has a lower generalization error bound. It is not clear why the reweighted L1 L1 regularization is better than the L1 L1 regularization. The authors should compare the baseline method which uses the  L1 L1 regularization in their framework instead of directly comparing the proposed algorithm with [Le et al., 2019] as there exist differences in the algorithm design. The authors should clarify whether the performance gains due to the only use of large model parameters. Overall, this paper proposes an effective reweighted RNN model based on the solver of a reweighted L1 L1 minimization. I would be willing to increase the score if these problems are solved in the authors’ response.<BRK>Authors proposed a deep RNN via unfolding reweighted l1 l1 minimization, where reweighted l1 l1 minimization algorithms are applied to a video task. Overall, the paper is well explained in a theoretical part and exhibits a good result compared with other conventional RNN methods in the experiment. In Section 3, authors formulate Rademacher complexities for both conventional and proposed method, which shows the generalization performance of the proposed method when d increases. just focus on single dataset like moving MNIST, I believe testing on language data is also quite important (this is a full paper and exhaustive experiments should be mandatory). It should be better to compare with these methods. For example, visualizing features from each RNN model would be beneficial.<BRK>This paper proposes a novel method to solve the sequential signal reconstruction problem. The method is based on the deep unfolding methods and incorporates the reweighting mechanism. Additionally, they derive the generalization error bound and show how their over parameterized reweighting RNNs ensure good generalization. I recommend the paper to be accepted for mainly two reasons. First, they derive a tighter generalization bound for deep RNNs; Second, the experiment results align with the theory and show the continuous improvements when increasing the depth of RNNs. For example, if the depth continues increasing, will the proposed method suffer the similar problem as other methods (performance does not improve or even degrade)? 4.Are there any known limitations of the proposed method?<BRK>In this line of research, this paper develops a novel deep recurrent neural network (coined reweighted-RNN) by unfolding a reweighted l1-l1 minimization algorithm and applies it to the task of sequential signal reconstruction. To the best of their knowledge, this is the first deep unfolding method that explores reweighted minimization. Furthermore, it has higher network expressivity than existing deep unfolding RNN models due to the over-parameterizing weights. Moreover, they establish theoretical generalization error bounds for the proposed reweighted-RNN model by means of Rademacher complexity. The bounds reveal that the parameterization of the proposed reweighted-RNN ensures good generalization.
Reject. rating score: 3. rating score: 3. rating score: 8. This paper proposes to iteratively reformulate questions in the latent space for multi hop question answering. "The web as a knowledge base for answering complex questions." The authors experiment their model on the HotpotQA dataset and achieve the state of the art performance. But, it s important to experiment with some other datasets.<BRK>This paper proposes a model for multi hop question answering, specifically in a closed domain setting where the relevant paragraphs are present within a few distractor paragraphs. Empricially, this paper shows gain in the distractor setting for HotpotQA dataset. Each module of the network seems to be carefully designed and the ablation results and analysis are helpful. I think that should be fixed.s2. The paper should consider testing on the open domain setting of the HotpotQA dataset. Moreover, it has been also shown recently that the distractor setting can easily be fooled and is not a great benchmark for testing the reasoning capabilities. That is going to be computationally intensive and the authors should strongly consider other approaches such as replacing bi grus with transformers that can encode sequences in parallel, and parameter sharing.<BRK>The authors propose a multi hop latent question reformulation system that performs well in the question answering setup. The system achieves best current published result on the HotpotQA dataset. The system achieves that using question aware representation of the document. It seems that the visualization of these intermediate forms would allow to understand the model better.<BRK>Their model achieves competitive results on the public leaderboard and outperforms the best current \textit {published} models in terms of Exact Match (EM) and $F_1 $score. In this paper, they propose a novel architecture, called the Latent Question Reformulation Network (LQR-net), a multi-hop and parallel attentive network designed for question-answering tasks that require reasoning capabilities. This updated question is then passed to the following hop.
Reject. rating score: 1. rating score: 3. rating score: 3. It is concerned with the examination of pruning experiments for a LeNet on the MNIST dataset. I have to give this paper a reject as the experiments conducted are far too weak, and there is little evidence anything found here will, say, generalise to a ResNet/DenseNet on ImageNet. What should they take account of when performing network pruning?<BRK>Texts in Figure 7 are too small to read. Explicitly noting the meaning of color in the figure would be better. The authors perform experiments mainly on LeNet with the MNIST dataset and analyze the observations. Second, the observations are only presented for LeNet and MNIST and it is non trivial whether they extend to large scale models.<BRK>*Summary*This paper compares network pruning masks learned via different iterative pruning methods. First, a clarification on the figures: are lines for pruned weights terminated where they are pruned? If so, this would be helpful to state. (4) *Presentation*: Figure is too small throughout to read from a printed copy (or even on a screen without significant zooming). Several results could be presented with less ambiguity in tabular form, as noted above. (5) *Replications*: The paper presents results only a single set of experiments using the MNIST dataset with the LeNet architecture. Fig 8: Perhaps scale vertically by the standard deviation of the weights?<BRK>They examine how recently documented, fundamental phenomena in deep learn-ing models subject to pruning are affected by changes in the pruning procedure. Specifically, they analyze differences in the connectivity structure and learning dynamics of pruned models found through a set of common iterative pruning techniques, to address questions of uniqueness of trainable, high-sparsity sub-networks, and their dependence on the chosen pruning method.
Reject. rating score: 3. rating score: 6. rating score: 6. In this paper, the authors propose a distributional robust risk minimization method using Lipschitz regularization and give an approach to approximate the Lipschitz constant for product kernels efficiently. My major concerns are as follows. 1.The result in Theorem 1 is similar to Theorem 3 in [1]. However, the applicability of the method is still limited if it can be used only for product kernels. Certifying some distributional robustness with principled adversarial  training.<BRK>Through the lens of Distributional Robust Risk (DRR), this work draws a link between adversarial robustness and Lipschitz constant regularisation. More consistency in the notation would be greatly appreciated. The authors then apply this idea to kernel methods and aim to minimise the true risk under a Lipschitz constant constraint. I have concerns about the novelty of the two main contributions of this work, which are Theorems 1 and 2:* Theorem 1 is a direct implication of Kantorovich duality, well known in optimal transport.<BRK>7) There is some disparity between the experimental results presented and the theory in the paper. This is a reasonable setting for validating the theoretical contributions of this work but is not a realistic interpretation of the adversarial threat model considered. The Lipschitz constant of the embedding function (the pre trained ResNet) is not known and will likely lead to vacuous bounds (for robustness with respect to the input space). Minor:  In Theorem 1 statement, "If additionally if $\ell_f$ is convex".<BRK>However existing methods are restricted to either linear models or very small perturbations, and cannot find the globally optimal solution for restricted nonlinear models such as kernel methods. In this paper they resolved these limitations by upper bounding DRRs with an empirical risk regularised by the Lipschitz constant of the model, including deep neural networks and kernel methods. As an application, they showed that it also provides a certificate for adversarial training, and global solutions can be achieved on product kernel machines in polynomial time.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. The authors propose a new algorithm for unbiased stochastic gradient estimation for use in reinforcement learning of sequence generation tasks (specifically neural program synthesis and image captioning). The method consists in performing correlated Monte Carlo rollouts starting from each token in the generated sequence, and using the multiple rollouts to reduce gradient variance. The proposed algorithm is novel, and the results are promising. The paper is clearly written.<BRK>The paper presents a novel reinforcement learning based algorithm for contextual sequence generation. The algorithm is evaluated on the Karel dataset for neural program synthesis and the MS COCO dataset for image captioning. 3.I understand that the ARSM estimator should be unbiased for V   2. 4.In the experiments, the variance is shown to reduce significantly which is nice. Bunel et al (2018) report higher generalization on the Karel dataset. Can the same experiments be performed with this checker on or are there any constraints of the ARSM based method?<BRK>The paper presents experimental results on the application of the gradient ARSM estimator of Yin et al.(2019) to challenging structured prediction problems (neural program synthesis and image captioning).<BRK>Sequence generation models are commonly refined with reinforcement learning over user-defined metrics. To stabilize this method, they adapt to contextual generation of categorical sequences a policy gradient estimator, which evaluates a set of correlated Monte Carlo (MC) rollouts for variance control. Due to the correlation, the number of unique rollouts is random and adaptive to model uncertainty; those rollouts naturally become baselines for each other, and hence are combined to effectively reduce gradient variance. They evaluate their methods on both neural program synthesis and image captioning.
Reject. rating score: 1. rating score: 3. rating score: 8. This paper proposed a framework based on a mathematical tool of tropical geometry to characterize the decision boundary of neural networks. The analysis is applied to network pruning, lottery ticket hypothesis and adversarial attacks. More specifically, from my perspective, tropical semiring, tropical polynomials and tropical rational functions all can be represented with the standard mathematical tools. Thanks the authors for the response. What is the application of this setting? They are not clear to me.<BRK>The authors take advantage of a new perspective on deep neural network i.e., tropical geometry. Then, they use the theoretical results of the theorem for two applications i.e., network pruning and adversarial examples generation. 1) This paper needs to be placed properly among several important missing references on the decision boundary of deep neural networks [1][2]. Also, the authors imply that two networks performing similarly (in terms of the accuracy) on a particular dataset have similar decision boundaries. Please discuss how the introduced method in this paper is placed among these methods.<BRK>1.Summary of the paperThis paper describes the decision boundaries of a certain class ofneural networks (piecewise linear, non linear activation functions)through the lens of tropical geometry. Similar to this earlier work, the network is shown to be represented asa tropical rational function. This characterisation is used to explain different phenomena of neuralnetwork training, viz. the  lottery ticket hypothesis , networkpruning, and adversarial attacks.<BRK>This work tackles the problem of characterizing and understanding the decision boundaries of neural networks with piece-wise linear non-linearity activations. They use tropical geometry, a new development in the area of algebraic geometry, to provide a characterization of the decision boundaries of a simple neural network of the form (Affine, ReLU, Affine). The generators of the zonotopes are precise functions of the neural network parameters. In doing so, they propose a new tropical perspective for the lottery ticket hypothesis, where they see the effect of different initializations on the tropical geometric representation of the decision boundaries.
Accept (Spotlight). rating score: 8. rating score: 6. rating score: 6. The paper presents DeepSphere, a method for learning over spherical data via a graphical representation and graph convolutions. DeepSphere is then demonstrated on several problems as well as shown how it applies to non uniform data. The experiments performed are thorough and interesting. The approach both outperforms baselines in inference time and accuracy. The unevenly sampled data is a nice extension showing the generality of the approach.<BRK>The paper studies the problem of designing a convolution for a spherical neural network. Pros: 1.The application and combination of different techniques in this paper are smart. 2.The experiments show that the proposed method outperforms other baseline methods. Cons:1.It is a good application of known techniques, but the novelty is limited. 2.It is suggested to add more baselines in the experiments.<BRK>In this paper, CNNs specialized for spherical data are studied. The proposed architecture is a combination of existing frameworks based on the discretization of a sphere as a graph. The experiments show the proposed model achieves a good tradeoff between the prediction performance and the computational cost. Although the theoretical result is not strong enough, the empirical results show the proposed approach is promising. It is nice that the authors try to mitigate from overclaiming of the analysis.<BRK>Designing a convolution for a spherical neural network requires a delicate tradeoff between efficiency and rotation equivariance. DeepSphere, a method based on a graph representation of the discretized sphere, strikes a controllable balance between these two desiderata. This contribution is twofold. Experiments show state-of-the-art performance and demonstrates the efficiency and flexibility of this formulation.
Accept (Spotlight). rating score: 8. rating score: 6. rating score: 6. There is a list of noteworthy contributions of this work:1) They demonstrate that their approach achieves SOTA on various (well chosen) standard CL benchmarks (notably P MNIST for CL, Split MNIST) and also does reasonably well on Split CIFAR 10/100 benchmark.<BRK>This paper proposes to use hypernetwork to prevent catastrophic forgetting. For the rehearsal objective in 2, L2 penalty is used.<BRK>Paper 1872Paper proposes a method for CL. The method is based on hypernetworks. Preventing forgetting in the main network is now, replaced by preventing forgetting in the hypernetwork. CONCLUSIONOverall, I like the idea of the paper and it is well explained. If the authors insist in using Eq 2, I would like to see it compared with the proposed version. Comparison with the closest methods like HAT and PackNet should be included. For CIFAR 10/100 groups of 20 classes are added in 5 steps ?<BRK>Artificial neural networks suffer from catastrophic forgetting when they are sequentially trained on multiple tasks. To overcome this problem, they present a novel approach based on task-conditioned hypernetworks, i.e., networks that generate the weights of a target model based on task identity. Besides achieving state-of-the-art performance on standard CL benchmarks, additional experiments on long task sequences reveal that task-conditioned hypernetworks display a very large capacity to retain previous memories. Notably, such long memory lifetimes are achieved in a compressive regime, when the number of trainable hypernetwork weights is comparable or smaller than target network size.
Reject. rating score: 1. rating score: 1. rating score: 3. This work tries to predict the protein functional activation on a tissue by combining the information from amino acid sequence, and tissue specific protein protein interaction network. For classification, what exactly the linear model is? And is there any regularization? What s more, since validation set is used for tuning, it would be better to report the results on test set. What s more, what s exactly the 13 tissues this paper is using? Why they are chosen? In conclusion, I find this is an interesting paper, that the authors tries to combine amino acid sequence representation and tissue information to predict the activation of protein on specific tissue. What s more, comparing results with the start of art methods on the same task setting is important, too.<BRK>(3) Run more experiments on various tasks instead of one: protein function prediction (GO term), enzyme function prediction (EC number), protein secondary structure prediction, protein contact map prediction(4) Refine the representation of the paper. (2) The simple combination of the results from two published articles is not that interesting(3) the presentation of the paper and idea is not in an acceptable form (the authors should at least draw a figure to show the big idea of the paper).<BRK>The method consists in training a linear classifier on the output of two existing embedding methods, UniRep/SeqVec and OhmNet, respectively embedding the amino acid sequences and the tissue specific protein protein interaction networks. timely and important topic (prediction of protein function), where ML is likely to have an big impact.<BRK>Given that protein function is an emergent property of all these levels of interactions in this work, they learn joint representations from both amino acid sequence and multilayer networks representing tissue-specific protein-protein interactions. Using these representations, they train machine learning models that outperform existing methods on the task of tissue-specific protein function prediction on 10 out of 13 tissues.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. This paper introduces a method to prune networks at initialization in a way that (mostly) preserves the gradient flow through the resulting pruned network. The writing is excellent for most of the paper.<BRK>This paper proposes a novel one shot pruning algorithm which improves the training of sparse networks by maximizing the norm of the gradient at initialization. Though I find the proposed method intriguing and well motivated, experiments section of the paper misses some important sparse training baselines and needs some improvement. Though, I found the results reported on Imagenet to be limited.<BRK>The paper proposes a new prunning criterion that performs better than Single shot Network Pruning (SNIP) in prunning a network at the initalization. However, I am not convinced by the theoretical explanation and some of the experimental results (see detailed comment below). I agree with Reviewer #4 that increasing gradient norm at initialization is a promising direction on its own, which warrants acceptance. Could you add training accuracy to the Tables (maybe in the Supplement)?<BRK>They aim to prune networks at initialization, thereby saving resources at training time as well. Specifically, they argue that efficient training requires preserving the gradient flow through the network.
Reject. rating score: 3. rating score: 3. rating score: 3. This paper proposes to specify the first layer of a CNN for audio applications with predefined filterbanks from the signal processing community. Some accuracy improvements are obtained on non trivial datasets. However, I find that this study would benefit of more careful comparisons to understand which particular component is responsible for some of their success! Also, I think some relevant papers are missing in the introduction. Cons :  Several attempts to employ hybrid architectures (as defined in the text) have been already proposed. I agree some of those references are only considering images, but those methods are definitely not specific to them. As stated in the text, many works propose to initialize the CNN with a specific filter bank. Have the authors tried to compare their performances if the first layer is simply initialized with those filters and then freely evolve? I feel this is missing and would make the claim of the paper stronger. If this has been already done, please highlight it in the text. Section 3: Sometimes(e.g., AudioMnist), the hybrid training pipeline is quite different from the original implementation, for instance, because of the use of ADAM when the original implementation was using SGD. Post discussion:R1 made several relevant comments about the technical novelty and my concerns weren t fully solved.<BRK>The main benefit is that the layer can be specified with a small number of parameters (+ filter configurations that are typically fixed beforehand get to be tuned to improve inductive bias). While this is interesting, I do not see any conceptual novelty. The authors might argue that the conceptual difference compared to [1] is cosine modulation (see Remark 2 on page 4). Well, cosine modulated filters were considered in [4] as Parzen filters (v1 was on arXiv in June 2019). The latter work has not even been cited by the authors. Moreover, the paper does not discuss the consequences of using cosine modulations instead of exponentials. This lack of baselines and reference to related work makes the experiments inadequate. Learning filterbanks from raw speech for phone recognition.<BRK>The paper presents an interesting signal processing based extension of CNNs, where the first layer convolution is replaced by some pre defined filter banks. Since those filter banks are parameterized with a smaller number of parameters, while they have been proven to be effective in audio processing, I was convinced that this approach could produce better performance than a generic CNN with no such consideration. I figure the proposed method in this paper is more flexible as it does not use the pre defined filterbanks; instead it tries to learn the parameters to specify the only necessary filters for the particular problem. But I think the authors may need to address the difference from this previous work done by Mallat s group, because they at least share a similar philosophy. I think, if there is an optimal filter length depending on the problem, which has to be found to guarantee the performance, it has to be better investigated in the paper.<BRK>They propose and investigate the design of a new convolutional layer where kernels are parameterized functions. This layer aims at being the input layer of convolutional neural networks for audio applications. The kernels are defined as functions having a band-pass filter shape, with a limited number of trainable parameters. They show that networks having such an input layer can achieve state-of-the-art accuracy on several audio classification tasks. Furthermore, the learned filters bring additional interpretability and a better understanding of the data properties exploited by the network.